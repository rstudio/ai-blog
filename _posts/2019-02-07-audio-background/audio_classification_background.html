<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="radix" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

  <!--radix_placeholder_meta_tags-->
  <title>Audio classification with Keras: Looking closer at the non-deep learning parts</title>
  
  <meta property="description" itemprop="description" content="Sometimes, deep learning is seen - and welcomed - as a way to avoid laborious preprocessing of data. However, there are cases where preprocessing of sorts does not only help improve prediction, but constitutes a fascinating topic in itself. One such case is audio classification. In this post, we build on a previous post on this blog, this time focusing on explaining some of the non-deep learning background. We then link the concepts explained to updated for near-future releases TensorFlow code."/>
  
  
  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2019-02-07"/>
  <meta property="article:created" itemprop="dateCreated" content="2019-02-07"/>
  <meta name="article:author" content="Sigrid Keydana"/>
  
  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="Audio classification with Keras: Looking closer at the non-deep learning parts"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="Sometimes, deep learning is seen - and welcomed - as a way to avoid laborious preprocessing of data. However, there are cases where preprocessing of sorts does not only help improve prediction, but constitutes a fascinating topic in itself. One such case is audio classification. In this post, we build on a previous post on this blog, this time focusing on explaining some of the non-deep learning background. We then link the concepts explained to updated for near-future releases TensorFlow code."/>
  <meta property="og:locale" content="en_US"/>
  
  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="Audio classification with Keras: Looking closer at the non-deep learning parts"/>
  <meta property="twitter:description" content="Sometimes, deep learning is seen - and welcomed - as a way to avoid laborious preprocessing of data. However, there are cases where preprocessing of sorts does not only help improve prediction, but constitutes a fascinating topic in itself. One such case is audio classification. In this post, we build on a previous post on this blog, this time focusing on explaining some of the non-deep learning background. We then link the concepts explained to updated for near-future releases TensorFlow code."/>
  
  <!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=&lt;span class=&quot;nocase&quot;&gt;Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition&lt;/span&gt;;citation_publication_date=2018;citation_author=P. Warden"/>
  <!--radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","slug","date","categories","bibliography","output","preview"]}},"value":[{"type":"character","attributes":{},"value":["Audio classification with Keras: Looking closer at the non-deep learning parts"]},{"type":"character","attributes":{},"value":["Sometimes, deep learning is seen - and welcomed - as a way to avoid laborious preprocessing of data. However, there are cases where preprocessing of sorts does not only help improve prediction, but constitutes a fascinating topic in itself. One such case is audio classification. In this post, we build on a previous post on this blog, this time focusing on explaining some of the non-deep learning background. We then link the concepts explained to updated for near-future releases TensorFlow code.\n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","affiliation_url"]}},"value":[{"type":"character","attributes":{},"value":["Sigrid Keydana"]},{"type":"character","attributes":{},"value":["RStudio"]},{"type":"character","attributes":{},"value":["https://www.rstudio.com/"]}]}]},{"type":"character","attributes":{},"value":["keydana2019audiobackground"]},{"type":"character","attributes":{},"value":["02-07-2019"]},{"type":"character","attributes":{},"value":["Keras","Introductions","Audio"]},{"type":"character","attributes":{},"value":["bibliography.bib"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["radix::radix_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"character","attributes":{},"value":["images/seven2.png"]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["audio_classification_background_files/bowser-1.9.3/bowser.min.js","audio_classification_background_files/distill-2.2.21/template.v2.js","audio_classification_background_files/jquery-1.11.3/jquery.min.js","audio_classification_background_files/webcomponents-2.0.0/webcomponents.js","bibliography.bib","images/aliasing.png","images/bandwidth_1_2.png","images/bandwidth_2_2.png","images/seven_16000_2.png","images/seven_30_2.png","images/seven2.png","images/sin8_16_32_64_2.png","images/waves2.png","images/windows2.png"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->
  
  <style type="text/css">
  
  body {
    background-color: white;
  }
  
  .pandoc-table {
    width: 100%;
  }
  
  .pandoc-table>caption {
    margin-bottom: 10px;
  }
  
  .pandoc-table th:not([align]) {
    text-align: left;
  }
  
  .pagedtable-footer {
    font-size: 15px;
  }
  
  .html-widget {
    margin-bottom: 2.0em;
  }
  
  .l-screen-inset {
    padding-right: 16px;
  }
  
  .l-screen .caption {
    margin-left: 10px;
  }
  
  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .shaded-content {
    background: white;
  }
  
  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }
  
  .hidden {
    display: none !important;
  }
  
  d-article {
    padding-bottom: 30px;
  }
  
  d-appendix {
    padding-top: 30px;
  }
  
  d-article>p>img {
    width: 100%;
  }
  
  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }
  
  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  /* CSS for table of contents */
  
  .d-toc {
    color: rgba(0,0,0,0.8);
    font-size: 0.8em;
    line-height: 1em;
  }
  
  .d-toc-header {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    text-transform: uppercase;
    margin-top: 0;
    margin-bottom: 1.3em;
  }
  
  .d-toc a {
    border-bottom: none;
  }
  
  .d-toc ul {
    padding-left: 0;
  }
  
  .d-toc li>ul {
    padding-top: 0.8em;
    padding-left: 16px;
    margin-bottom: 0.6em;
  }
  
  .d-toc ul,
  .d-toc li {
    list-style-type: none;
  }
  
  .d-toc li {
    margin-bottom: 0.9em;
  }
  
  .d-toc-separator {
    margin-top: 20px;
    margin-bottom: 2em;
  }
  
  .d-article-with-toc {
    border-top: none;
    padding-top: 0;
  }
  
  
  
  /* Tweak code blocks (note that this CSS is repeated above in an injection
     into the d-code shadow dom) */
  
  d-code {
    overflow-x: auto !important;
  }
  
  pre.d-code code.d-code {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }
  
  pre.text-output {
  
    font-size: 12px;
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  @media(min-width: 768px) {
  
  d-code {
    overflow-x: visible !important;
  }
  
  pre.d-code code.d-code  {
      padding-left: 18px;
      font-size: 14px;
  }
  pre.text-output {
    font-size: 14px;
  }
  }
  
  /* Figure */
  
  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }
  
  .figure img {
    width: 100%;
  }
  
  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }
  
  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }
  
  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }
  
  
  
  /* Tweak 1000px media break to show more text */
  
  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }
  
    .grid {
      grid-column-gap: 16px;
    }
  
    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }
  
  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }
  
    .grid {
      grid-column-gap: 32px;
    }
  }
  
  
  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */
  
  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }
  
  
  /* Social footer */
  
  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }
  
  .disqus-comments {
    margin-right: 30px;
  }
  
  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }
  
  #disqus_thread {
    margin-top: 30px;
  }
  
  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }
  
  .article-sharing a:hover {
    border-bottom: none;
  }
  
  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }
  
  .subscribe p {
    margin-bottom: 0.5em;
  }
  
  
  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }
  
  
  /* Improve display for browsers without grid (IE/Edge <= 15) */
  
  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }
  
  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }
  
  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }
  
  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }
  
  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }
  
  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }
  
  
  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }
  
  .downlevel .footnotes ol {
    padding-left: 13px;
  }
  
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }
  
  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }
  
  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }
  
  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  </style>
  
  <script type="application/javascript">
  
  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }
  
  // show body when load is complete
  function on_load_complete() {
  
    // set body to visible
    document.body.style.visibility = 'visible';
  
    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }
  
    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }
  
  function init_distill() {
  
    init_common();
  
    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);
  
    // create d-title
    $('.d-title').changeElementType('d-title');
  
    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);
  
    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();
  
    // move posts container into article
    $('.posts-container').appendTo($('d-article'));
  
    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');
  
    // create d-bibliography
    var bibliography = $('<d-bibliography></d-bibliography>');
    $('#distill-bibliography').wrap(bibliography);
  
    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;
  
    // replace citations with <d-cite>
    $('.citation').each(function(i, val) {
      appendix = true;
      var cites = $(this).attr('data-cites').split(" ");
      var dt_cite = $('<d-cite></d-cite>');
      dt_cite.attr('key', cites.join());
      $(this).replaceWith(dt_cite);
    });
    // remove refs
    $('#refs').remove();
  
    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();
  
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-toc a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });
  
    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');
  
    // replace code blocks with d-code
    $('pre>code').each(function(i, val) {
      var code = $(this);
      var pre = code.parent();
      var clz = "";
      var language = pre.attr('class');
      if (language) {
        // map unknown languages to "clike" (without this they just dissapear)
        if ($.inArray(language, ["bash", "clike", "css", "go", "html",
                                 "javascript", "js", "julia", "lua", "markdown",
                                 "markup", "mathml", "python", "svg", "xml"]) == -1)
          language = "clike";
        language = ' language="' + language + '"';
        var dt_code = $('<d-code block' + language + clz + '></d-code>');
        dt_code.text(code.text());
        pre.replaceWith(dt_code);
      } else {
        code.addClass('text-output').unwrap().changeElementType('pre');
      }
    });
  
    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {
  
      // capture layout
      var layout = $(this).attr('data-layout');
  
      // apply layout to markdown level block elements
      var elements = $(this).children().not('d-code, pre.text-output, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });
  
  
      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });
  
    // load distill framework
    load_distill_framework();
  
    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {
  
      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;
  
      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');
  
      // table of contents
      if (have_authors) // adjust border if we are in authors
        $('.d-toc').parent().addClass('d-article-with-toc');
  
      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');
  
      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }
  
      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');
  
      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");
  
      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }
  
       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }
  
      // inject pre code styles (can't do this with a global stylesheet b/c a shadow root is used)
      $('d-code').each(function(i, val) {
        var style = document.createElement('style');
        style.innerHTML = 'pre code { padding-left: 10px; font-size: 12px; border-left: 2px solid rgba(0,0,0,0.1); } ' +
                          '@media(min-width: 768px) { pre code { padding-left: 18px; font-size: 14px; } }';
        if (this.shadowRoot)
          this.shadowRoot.appendChild(style);
      });
  
      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();
  
      // clear polling timer
      clearInterval(tid);
  
      // show body now that everything is ready
      on_load_complete();
    }
  
    var tid = setInterval(distill_post_process, 50);
    distill_post_process();
  
  }
  
  function init_downlevel() {
  
    init_common();
  
     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));
  
    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
  
    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();
  
    // remove toc
    $('.d-toc-header').remove();
    $('.d-toc').remove();
    $('.d-toc-separator').remove();
  
    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });
  
  
    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);
  
    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);
  
    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();
  
    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();
  
    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));
  
    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });
  
    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));
  
    $('body').addClass('downlevel');
  
    on_load_complete();
  }
  
  
  function init_common() {
  
    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};
  
        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });
  
        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);
  
    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});
  
    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });
  
      }
    });
  
    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });
  
    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.radix-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
      });
    }
  
    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');
  
    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");
  
    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();
  
    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }
  
  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });
  
  </script>
  
  <!--/radix_placeholder_distill-->
  <script src="audio_classification_background_files/jquery-1.11.3/jquery.min.js"></script>
  <script src="audio_classification_background_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="audio_classification_background_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="audio_classification_background_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Audio classification with Keras: Looking closer at the non-deep learning parts","description":"Sometimes, deep learning is seen - and welcomed - as a way to avoid laborious preprocessing of data. However, there are cases where preprocessing of sorts does not only help improve prediction, but constitutes a fascinating topic in itself. One such case is audio classification. In this post, we build on a previous post on this blog, this time focusing on explaining some of the non-deep learning background. We then link the concepts explained to updated for near-future releases TensorFlow code.","authors":[{"author":"Sigrid Keydana","authorURL":"#","affiliation":"RStudio","affiliationURL":"https://www.rstudio.com/"}],"publishedDate":"2019-02-07T00:00:00.000+01:00","citationText":"Keydana, 2019"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Audio classification with Keras: Looking closer at the non-deep learning parts</h1>
<p>Sometimes, deep learning is seen - and welcomed - as a way to avoid laborious preprocessing of data. However, there are cases where preprocessing of sorts does not only help improve prediction, but constitutes a fascinating topic in itself. One such case is audio classification. In this post, we build on a previous post on this blog, this time focusing on explaining some of the non-deep learning background. We then link the concepts explained to updated for near-future releases TensorFlow code.</p>
</div>

<div class="d-byline">
  Sigrid Keydana  (RStudio)<a href="https://www.rstudio.com/" class="uri">https://www.rstudio.com/</a>
  
<br/>02-07-2019
</div>

<div class="d-article">
<p>About half a year ago, this blog featured a post, written by Daniel Falbel, on how to use Keras to classify pieces of spoken language. The article got a lot of attention and not surprisingly, questions arose how to apply that code to different datasets. We’ll take this as a motivation to explore in more depth the preprocessing done in that post: If we know why the input to the network looks the way it looks, we will be able to modify the model specification appropriately if need be.</p>
<p>In case you have a background in speech recognition, or even general signal processing, for you the introductory part of this post will probably not contain much news. However, you might still be interested in the code part, which shows how to do things like creating spectrograms with current versions of TensorFlow.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> If you don’t have that background, we’re inviting you on a (hopefully) fascinating journey, slightly touching on one of the greater mysteries of this universe.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p>We’ll use the same dataset as Daniel did in his post, that is, <a href="https://storage.cloud.google.com/download.tensorflow.org/data/speech_commands_v0.01.tar.gz">version 1 of the Google speech commands dataset</a><span class="citation" data-cites="speechcommandsv2">(Warden <a href="#ref-speechcommandsv2">2018</a>)</span> The dataset consists of ~ 65,000 WAV files, of length one second or less. Each file is a recording of one of thirty words, uttered by different speakers.</p>
<p>The goal then is to train a network to discriminate between spoken words. How should the input to the network look? The WAV files contain amplitudes of sound waves over time. Here are a few examples, corresponding to the words <em>bird</em>, <em>down</em>, <em>sheila</em>, and <em>visual</em>:<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<p><img src="images/waves2.png" /></p>
<h1 id="time-domain-and-frequency-domain">Time domain and frequency domain</h1>
<p>A sound wave is a signal extending in <em>time</em>, analogously to how what enters our visual system extends in <em>space</em>. At each point in time, the current signal is dependent on its past. The obvious architecture to use in modeling it thus seems to be a recurrent neural network.</p>
<p>However, the information contained in the sound wave can be represented in an alternative way: namely, using the <em>frequencies</em> that make up the signal.</p>
<p>Here we see a sound wave (top) and its frequency representation (bottom).</p>
<p><img src="images/sin8_16_32_64_2.png" /></p>
<p>In the time representation (referred to as the <em>time domain</em>), the signal is composed of consecutive amplitudes over time. In the frequency domain, it is represented as magnitudes of different frequencies. It may appear as one of the greatest mysteries in this world that you can convert between those two without loss of information, that is: Both representations are essentially equivalent!</p>
<p>Conversion from the time domain to the frequency domain is done using the <em>Fourier transform</em>; to convert back, the <em>Inverse Fourier Transform</em> is used. There exist different types of Fourier transforms depending on whether time is viewed as continuous or discrete, and whether the signal itself is continuous or discrete. In the “real world”, where usually for us, real means virtual as we’re working with digitized signals, the time domain as well as the signal are represented as discrete and so, the <em>Discrete Fourier Transform</em> (DFT) is used. The DFT itself is computed using the FFT (<em>Fast Fourier Transform</em>) algorithm, resulting in significant speedup over a naive implementation.</p>
<p>Looking back at the above example sound wave, it is a compound of four sine waves, of frequencies 8Hz, 16Hz, 32Hz, and 64Hz, whose amplitudes are added and displayed over time. The compound wave here is assumed to extend infinitely in time. Unlike speech, which changes over time, it can be characterized by a single enumeration of the magnitudes of the frequencies it is composed of. So here the <em>spectrogram</em>, the characterization of a signal by magnitudes of constituent frequencies varying over time, looks essentially one-dimensional.</p>
<p>However, when we ask <em>Praat</em> to create a spectrogram of one of our example sounds (a <em>seven</em>), it could look like this:</p>
<p><img src="images/seven2.png" /></p>
<p>Here we see a two-dimensional <em>image</em> of frequency magnitudes over time (higher magnitudes indicated by darker coloring). This two-dimensional representation may be fed to a network, in place of the one-dimensional amplitudes. Accordingly, if we decide to do so we’ll use a convnet instead of an RNN.</p>
<p>Spectrograms will look different depending on how we create them. We’ll take a look at the essential options in a minute. First though, let’s see what we <em>can’t</em> always do: ask for <em>all</em> frequencies that were contained in the analog signal.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<h1 id="sampling">Sampling</h1>
<p>Above, we said that both representations, <em>time domain</em> and <em>frequency domain</em>, were essentially equivalent. In our virtual real world, this is only true if the signal we’re working with has been digitized correctly, or as this is commonly phrased, if it has been “properly sampled”.</p>
<p>Take speech as an example: As an analog signal, speech per se is continuous in time; for us to be able to work with it on a computer, it needs to be converted to happen in discrete time. This conversion of the independent variable (time in our case, space in e.g. image processing) from continuous to discrete is called <em>sampling</em>.</p>
<p>In this process of discretization, a crucial decision to be made is the <em>sampling rate</em> to use. The sampling rate has to be at least double the highest frequency in the signal. If it’s not, loss of information will occur. The way this is most often put is the other way round: To preserve all information, the analog signal may not contain frequencies above one-half the sampling rate. This frequency - half the sampling rate - is called the <em>Nyquist rate</em>.</p>
<p>If the sampling rate is too low, <em>aliasing</em> takes place: Higher frequencies <em>alias</em> themselves as lower frequencies. This means that not only can’t we get them, they also corrupt the magnitudes of corresponding lower frequencies they are being added to. Here’s a schematic example of how a high-frequency signal could alias itself as being lower-frequency. Imagine the high-frequency wave being sampled at integer points (grey circles) only:</p>
<p><img src="images/aliasing.png" /></p>
<p>In the case of the speech commands dataset, all sound waves have been sampled at 16 kHz. This means that when we ask <em>Praat</em> for a spectogram, we should not ask for frequencies higher than 8kHz. Here is what happens if we ask for frequencies up to 16kHz instead - we just don’t get them:</p>
<p><img src="images/seven_16000_2.png" /></p>
<p>Now let’s see what options we <em>do</em> have when creating spectrograms.</p>
<h1 id="spectrogram-creation-options">Spectrogram creation options</h1>
<p>In the above simple sine wave example, the signal stayed constant over time. However in speech utterances, the magnitudes of constituent frequencies change over time. Ideally thus, we’d have an exact frequency representation for every point in time. As an approximation to this ideal, the signal is divided into overlapping windows, and the Fourier transform is computed for each time slice separately. This is called the <em>Short Time Fourier Transform</em> (STFT).</p>
<p>When we compute the spectrogram via the STFT, we need to tell it what size windows to use, and how big to make the overlap. The longer the windows we use, the better the resolution we get in the frequency domain. However, what we gain in resolution there, we lose in the time domain, as we’ll have fewer windows representing the signal. This is a general principle in signal processing: Resolution in the time and frequency domains are inversely related.</p>
<p>To make this more concrete, let’s again look at a simple example. Here is the spectrogram of a synthetic sine wave, composed of two components at 1000 Hz and 1200 Hz. The window length was left at its (<em>Praat</em>) default, 5 milliseconds:</p>
<p><img src="images/bandwidth_1_2.png" /></p>
<p>We see that with a short window like that, the two different frequencies are mangled into one in the spectrogram. Now enlarge the window to 30 milliseconds, and they are clearly differentiated:</p>
<p><img src="images/bandwidth_2_2.png" /></p>
<p>The above spectrogram of the word “seven” was produced using Praats default of 5 milliseconds. What happens if we use 30 milliseconds instead?</p>
<p><img src="images/seven_30_2.png" /></p>
<p>We get better frequency resolution, but at the price of lower resolution in the time domain. The window length used during preprocessing is a parameter we might want to experiment with later, when training a network.</p>
<p>Another input to the STFT to play with is the type of window used to weight the samples in a time slice. Here again are three spectrograms of the above recording of <em>seven</em>, using, respectively, a Hamming, a Hann, and a Gaussian window:</p>
<p><img src="images/windows2.png" /></p>
<p>While the spectrograms using the Hann and Gaussian windows don’t look much different, the Hamming window seems to have introduced some artifacts.</p>
<h1 id="beyond-the-spectrogram-mel-scale-and-mel-frequency-cepstral-coefficients-mfccs">Beyond the spectrogram: Mel scale and Mel-Frequency Cepstral Coefficients (MFCCs)</h1>
<p>Preprocessing options don’t end with the spectrogram. A popular transformation applied to the spectrogram is conversion to <em>mel scale</em>, a scale based on how humans actually perceive differences in pitch. We don’t elaborate further on this here,<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> but we do briefly comment on the respective TensorFlow code below, in case you’d like to experiment with this. In the past, coefficients transformed to Mel scale have sometimes been further processed to obtain the so-called Mel-Frequency Cepstral Coefficients (MFCCs). Again, we just show the code. For excellent reading on Mel scale conversion and MFCCs (including the reason why MFCCs are less often used nowadays) see <a href="https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html">this post</a> by Haytham Fayek.</p>
<p>Back to our original task of speech classification. Now that we’ve gained a bit of insight in what is involved, let’s see how to perform these transformations in TensorFlow.</p>
<h1 id="preprocessing-for-audio-classification-using-tensorflow">Preprocessing for audio classification using TensorFlow</h1>
<p>Code will be represented in snippets according to the functionality it provides, so we may directly map it to what was explained conceptually above. A complete example is available <a href="https://github.com/skeydan/audio_classification/blob/master/audio_classification_tf.R">here</a>. The complete example builds on Daniel’s <a href="https://blogs.rstudio.com/tensorflow/posts/2018-06-06-simple-audio-classification-keras/">original code</a> as much as possible,<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> with two exceptions:</p>
<ul>
<li><p>The code runs in eager as well as in static graph mode. If you decide you only ever need eager mode, there are a few places that can be simplified. This is partly related to the fact that in eager mode, TensorFlow operations in place of tensors return values, which we can directly pass on to TensorFlow functions expecting values, not tensors. In addition, less conversion code is needed when manipulating intermediate values in R.</p></li>
<li><p>With TensorFlow 1.13 being released any day, and preparations for TF 2.0 running at full speed, we want the code to necessitate as few modifications as possible to run on the next major version of TF. One big difference is that there will no longer be a <code>contrib</code> module. In the original post, <code>contrib</code> was used to read in the <code>.wav</code> files as well as compute the spectrograms. Here, we will use functionality from <code>tf.audio</code> and <code>tf.signal</code> instead.</p></li>
</ul>
<p>All operations shown below will run inside <code>tf.dataset</code> code, which on the R side is accomplished using the <code>tfdatasets</code> package. To explain the individual operations, we look at a single file, but later we’ll also display the data generator as a whole.</p>
<p>For stepping through individual lines, it’s always helpful to have eager mode enabled, independently of whether ultimately we’ll execute in eager or graph mode:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
library(keras)
use_implementation(&quot;tensorflow&quot;)

library(tensorflow)
tfe_enable_eager_execution(device_policy = &quot;silent&quot;)</code></pre>
</div>
<p>We pick a random <code>.wav</code> file and decode it using <code>tf$audio$decode_wav</code>.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a>This will give us access to two tensors: the samples themselves, and the sampling rate.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
fname &lt;- &quot;data/speech_commands_v0.01/bird/00b01445_nohash_0.wav&quot;
wav &lt;- tf$audio$decode_wav(tf$read_file(fname))</code></pre>
</div>
<p><code>wav$sample_rate</code> contains the sampling rate. As expected, it is 16000, or 16kHz:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
sampling_rate &lt;- wav$sample_rate %&gt;% as.numeric()
sampling_rate</code></pre>
</div>
<pre><code>
16000</code></pre>
<p>The samples themselves are accessible as <code>wav$audio</code>, but their shape is (16000, 1), so we have to transpose the tensor to get the usual (<em>batch_size</em>, <em>number of samples</em>) format we need for further processing.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
samples &lt;- wav$audio
samples &lt;- samples %&gt;% tf$transpose(perm = c(1L, 0L))
samples</code></pre>
</div>
<pre><code>
tf.Tensor(
[[-0.00750732  0.04653931  0.02041626 ... -0.01004028 -0.01300049
  -0.00250244]], shape=(1, 16000), dtype=float32)</code></pre>
<h4 id="computing-the-spectogram">Computing the spectogram</h4>
<p>To compute the spectrogram, we use <code>tf$signal$stft</code> (where <em>stft</em> stands for <em>Short Time Fourier Transform</em>). <code>stft</code> expects three non-default arguments: Besides the input signal itself, there are the window size, <code>frame_length</code>, and the stride to use when determining the overlapping windows, <code>frame_step</code>. Both are expressed in units of <code>number of samples</code>. So if we decide on a window length of 30 milliseconds and a stride of 10 milliseconds …</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
window_size_ms &lt;- 30
window_stride_ms &lt;- 10</code></pre>
</div>
<p>… we arrive at the following call:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
samples_per_window &lt;- sampling_rate * window_size_ms/1000 
stride_samples &lt;-  sampling_rate * window_stride_ms/1000 

stft_out &lt;- tf$signal$stft(
  samples,
  frame_length = as.integer(samples_per_window),
  frame_step = as.integer(stride_samples)
)</code></pre>
</div>
<p>Inspecting the tensor we got back, <code>stft_out</code>, we see, for our single input wave, a matrix of 98 x 257 complex values:</p>
<pre><code>
tf.Tensor(
[[[ 1.03279948e-04+0.00000000e+00j -1.95371482e-04-6.41121820e-04j
   -1.60833192e-03+4.97534114e-04j ... -3.61620914e-05-1.07343149e-04j
   -2.82576875e-05-5.88812982e-05j  2.66879797e-05+0.00000000e+00j] 
   ... 
   ]],
shape=(1, 98, 257), dtype=complex64)</code></pre>
<p>Here 98 is the number of periods, which we can compute in advance, based on the number of samples in a window and the size of the stride:<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a></p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
n_periods &lt;- length(seq(samples_per_window/2, sampling_rate - samples_per_window/2, stride_samples))</code></pre>
</div>
<p>257 is the number of frequencies we obtained magnitudes for. By default, <code>stft</code> will apply a Fast Fourier Transform of size <em>smallest power of 2 greater or equal to the number of samples in a window</em>,<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a> and then return the <em>fft_length / 2 + 1</em> unique components of the FFT: the zero-frequency term and the positive-frequency terms.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a></p>
<p>In our case, the number of samples in a window is 480. The nearest enclosing power of 2 being 512, we end up with 512/2 + 1 = 257 coefficients. This too we can compute in advance:<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a></p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
fft_size &lt;- as.integer(2^trunc(log(samples_per_window, 2)) + 1) </code></pre>
</div>
<p>Back to the output of the STFT. Taking the elementwise magnitude of the complex values, we obtain an energy spectrogram:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
magnitude_spectrograms &lt;- tf$abs(stft_out)</code></pre>
</div>
<p>If we stop preprocessing here, we will usually want to log transform the values to better match the sensitivity of the human auditory system:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
log_magnitude_spectrograms = tf$log(magnitude_spectrograms + 1e-6)</code></pre>
</div>
<h4 id="mel-spectrograms-and-mel-frequency-cepstral-coefficients-mfccs">Mel spectrograms and Mel-Frequency Cepstral Coefficients (MFCCs)</h4>
<p>If instead we choose to use Mel spectrograms, we can obtain a transformation matrix that will convert the original spectrograms to Mel scale:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
lower_edge_hertz &lt;- 0
upper_edge_hertz &lt;- 2595 * log10(1 + (sampling_rate/2)/700)
num_mel_bins &lt;- 64L
num_spectrogram_bins &lt;- magnitude_spectrograms$shape[-1]$value

linear_to_mel_weight_matrix &lt;- tf$signal$linear_to_mel_weight_matrix(
  num_mel_bins,
  num_spectrogram_bins,
  sampling_rate,
  lower_edge_hertz,
  upper_edge_hertz
)</code></pre>
</div>
<p>Applying that matrix, we obtain a tensor of size <em>(batch_size, number of periods, number of Mel coefficients)</em> which again, we can log-compress if we want:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
mel_spectrograms &lt;- tf$tensordot(magnitude_spectrograms, linear_to_mel_weight_matrix, 1L)
log_mel_spectrograms &lt;- tf$log(mel_spectrograms + 1e-6)</code></pre>
</div>
<p>Just for completeness’ sake, finally we show the TensorFlow code used to further compute MFCCs. We don’t include this in the complete example as with MFCCs, we would need a different network architecture.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
num_mfccs &lt;- 13
mfccs &lt;- tf$signal$mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[, , 1:num_mfccs]</code></pre>
</div>
<h4 id="accommodating-different-length-inputs">Accommodating different-length inputs</h4>
<p>In our complete example, we determine the sampling rate from the first file read, thus assuming all recordings have been sampled at the same rate. We do allow for different lengths though. For example in our dataset, had we used this file, just 0.65 seconds long, for demonstration purposes:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
fname &lt;- &quot;data/speech_commands_v0.01/bird/1746d7b6_nohash_0.wav&quot;</code></pre>
</div>
<p>we’d have ended up with just 63 periods in the spectrogram. As we have to define a fixed <code>input_size</code> for the first conv layer, we need to pad the corresponding dimension to the maximum possible length, which is <code>n_periods</code> computed above. The padding actually takes place as part of dataset definition. Let’s quickly see dataset definition as a whole, leaving out the possible generation of Mel spectrograms.<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a></p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
data_generator &lt;- function(df,
                           window_size_ms,
                           window_stride_ms) {
  
  # assume sampling rate is the same in all samples
  sampling_rate &lt;-
    tf$audio$decode_wav(tf$read_file(tf$reshape(df$fname[[1]], list()))) %&gt;% .$sample_rate
  
  samples_per_window &lt;- (sampling_rate * window_size_ms) %/% 1000L  
  stride_samples &lt;-  (sampling_rate * window_stride_ms) %/% 1000L   
  
  n_periods &lt;-
    tf$shape(
      tf$range(
        samples_per_window %/% 2L,
        16000L - samples_per_window %/% 2L,
        stride_samples
      )
    )[1] + 1L
  
  n_fft_coefs &lt;-
    (2 ^ tf$ceil(tf$log(
      tf$cast(samples_per_window, tf$float32)
    ) / tf$log(2)) /
      2 + 1L) %&gt;% tf$cast(tf$int32)
  
  ds &lt;- tensor_slices_dataset(df) %&gt;%
    dataset_shuffle(buffer_size = buffer_size)
  
  ds &lt;- ds %&gt;%
    dataset_map(function(obs) {
      wav &lt;-
        tf$audio$decode_wav(tf$read_file(tf$reshape(obs$fname, list())))
      samples &lt;- wav$audio
      samples &lt;- samples %&gt;% tf$transpose(perm = c(1L, 0L))
      
      stft_out &lt;- tf$signal$stft(samples,
                                 frame_length = samples_per_window,
                                 frame_step = stride_samples)
      
      magnitude_spectrograms &lt;- tf$abs(stft_out)
      log_magnitude_spectrograms &lt;- tf$log(magnitude_spectrograms + 1e-6)
      
      response &lt;- tf$one_hot(obs$class_id, 30L)

      input &lt;- tf$transpose(log_magnitude_spectrograms, perm = c(1L, 2L, 0L))
      list(input, response)
    })
  
  ds &lt;- ds %&gt;%
    dataset_repeat()
  
  ds %&gt;%
    dataset_padded_batch(
      batch_size = batch_size,
      padded_shapes = list(tf$stack(list(
        n_periods, n_fft_coefs,-1L
      )),
      tf$constant(-1L, shape = shape(1L))),
      drop_remainder = TRUE
    )
}</code></pre>
</div>
<p>The logic is the same as described above, only the code has been generalized to work in eager as well as graph mode. The padding is taken care of by <em>dataset_padded_batch()</em>, which needs to be told the maximum number of periods and the maximum number of coefficients.</p>
<h4 id="time-for-experimentation">Time for experimentation</h4>
<p>Building on the <a href="https://github.com/skeydan/audio_classification/blob/master/audio_classification_tf.R">complete example</a>, now is the time for experimentation: How do different window sizes affect classification accuracy? Does transformation to the mel scale yield improved results?<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a> You might also want to try passing a non-default <code>window_fn</code> to <code>stft</code> (the default being the Hann window) and see how that affects the results. And of course, the straightforward definition of the network leaves a lot of room for improvement.</p>
<h1 id="wrapping-up">Wrapping up</h1>
<p>Speaking of the network: Now that we’ve gained more insight into what is contained in a spectrogram, we might start asking, is a convnet really an adequate solution here? Normally we use convnets on images: two-dimensional data where both dimensions represent the same kind of information. Thus with images, it is natural to have square filter kernels. In a spectrogram though, the time axis and the frequency axis represent fundamentally different types of information, and it is not clear at all that we should treat them equally. Also, whereas in images, the translation invariance of convnets is a desired feature, this is not the case for the frequency axis in a spectrogram.</p>
<p>Closing the circle, we discover that due to deeper knowledge about the subject domain, we are in a better position to reason about (hopefully) successful network architectures. We leave it to the creativity of our readers to continue the search…</p>
<div id="refs" class="references">
<div id="ref-speechcommandsv2">
<p>Warden, P. 2018. “Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.” <em>ArXiv E-Prints</em>, April. <a href="https://arxiv.org/abs/1804.03209">https://arxiv.org/abs/1804.03209</a>.</p>
</div>
</div>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>As well as TensorFlow 2.0, more or less.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>Referring to the Fourier Transform. To cite an authority for this characterization, it is e.g. found in Brad Osgood’s lecture on <a href="https://www.youtube.com/playlist?list=PLB24BC7956EE040CD">The Fourier Transform and its Applications</a>.<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>To display these sound waves, and later on to create spectrograms, we use <a href="http://www.fon.hum.uva.nl/praat/">Praat</a>, a speech analysis and synthesis program that has a <em>lot</em> more functionality than what we’re making use of here.<a href="#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>In practice, working with datasets created for speech analysis, there will be no problems due to low sampling rates (the topic we talk about below). However, the topic is too essential - and interesting! - to skip over in an introductory post like this one.)<a href="#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>Cf. <a href="https://en.wikipedia.org/wiki/Mel_scale">discussions about the validity of the original experiments</a>.<a href="#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>In particular, we’re leaving the convnet and training code itself nearly unchanged.<a href="#fnref6" class="footnote-back">↩</a></p></li>
<li id="fn7"><p>As of this writing, <code>tf.audio</code> is only available in the TensorFlow nightly builds. If the <code>decode_wav</code> line fails, simply replace <code>tf$audio</code> by <code>tf$contrib$framework$python$ops$audio_ops</code>.<a href="#fnref7" class="footnote-back">↩</a></p></li>
<li id="fn8"><p>This code, taken from the original post, is applicable when executing eagerly or when working “on the R side”. The complete example, which dynamically determines the sampling rate and performs all operations so they work inside a static TensorFlow graph, has a more intimidating-looking equivalent that essentially does the same thing.<a href="#fnref8" class="footnote-back">↩</a></p></li>
<li id="fn9"><p>In the former case, the samples dimension in the time domain will be padded with zeros.<a href="#fnref9" class="footnote-back">↩</a></p></li>
<li id="fn10"><p>For real signals, the negative-frequency terms are redundant.<a href="#fnref10" class="footnote-back">↩</a></p></li>
<li id="fn11"><p>Again, the code in the full example looks a bit more involved because it is supposed to be runnable on a static TensorFlow graph.<a href="#fnref11" class="footnote-back">↩</a></p></li>
<li id="fn12"><p>Which is contained in the complete example code, though.<a href="#fnref12" class="footnote-back">↩</a></p></li>
<li id="fn13"><p>For us, it didn’t.<a href="#fnref13" class="footnote-back">↩</a></p></li>
</ol>
</section>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>


<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom"></div>
<script id="distill-bibliography" type="text/bibtex">
@article{speechcommandsv2,
   author = {{Warden}, P.},
    title = "{Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1804.03209},
 primaryClass = "cs.CL",
 keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
     year = 2018,
    month = apr,
    url = {https://arxiv.org/abs/1804.03209},
}

</script>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
