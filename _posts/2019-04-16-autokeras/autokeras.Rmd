---
title: "Auto-Keras: An R easily accessible deep learning library"
description: |
  Witnessing the age of Data Science it is essential to be at least interested in Deep Learning (DL). However, DL tasks, such as featurization, hyperparameters tunning, or network design, are by no means easy for people without computer science skills. In this post, we introduce the **Auto-Keras** R package. Given a tagged dataset and a task, in a few lines of code, Auto-Keras allows to train DL models and get the best. Here, we present Auto-Keras by training a model and predicting the well-known MNIST dataset.
author:
  - name: Juan Cruz Rodriguez
    url: https://jcrodriguez.rbind.io
    affiliation: FAMAF, Universidad Nacional de CÃ³rdoba
    affiliation_url: https://www.famaf.unc.edu.ar/
bibliography: bibliography.bib
date: "`r Sys.Date()`"
creative_commons: CC BY
repository_url: https://github.com/jcrodriguez1989/tf_blog_autokeras
categories:
  - AutoML
  - Keras
  - Releases
  - Tools
  - Examples
  - Images
  - Introductions
output: distill::distill_article
---

```{r}
knitr::opts_chunk$set(echo = TRUE)
```

In the past few years, artificial intelligence has been a subject of intense media hype. Machine learning, deep learning, and artificial intelligence come up in countless articles, often outside of technology-minded publications [@chollet2017deep]. Transiting the age of data, it results fundamentally for any researcher with large amounts of information, to consider the application of deep learning models.

With a brief search on the web, dozens of texts are found where it is suggested to apply one or another deep learning model. However, tasks such as featurization, hyperparameters tunning, or network design, by no means, are easy for people without a rich computer science background. In this context, research work began to emerge in the area of what is known as Neural Architecture Search (NAS) [@baker2016designing; @pham2018efficient; @zoph2016neural; @luo2018neural; @liu2017hierarchical; @real2018regularized; @jin2018efficient]. The main goal of NAS algorithms is, given a specific tagged dataset, to search for the most optimal neural network to perform a certain task on that dataset. In this sense, NAS algorithms allow the user to not have to worry about any task related to Data Science engineering. In other words, given a tagged dataset, and a task, e.g., Image Regression, Text Classification, among others, the NAS algorithm will train several high-performance deep learning models and return the one that outperforms the rest.

Several NAS algorithms were developed on different platforms (e.g. [Google Cloud AutoML](https://cloud.google.com/automl/)), or as libraries of certain programming languages (e.g. [Auto-Keras](https://autokeras.com/), [TPOT](https://epistasislab.github.io/tpot/), [Auto-Sklearn](https://www.automl.org/automl/auto-sklearn/)). However, for a language that brings together experts from such diverse disciplines as is the R programming language, to the best of our knowledge, there is no NAS tool to this day. In this post, we present the Auto-Keras R package, an interface from R to the [Auto-Keras Python library](https://autokeras.com/) [@jin2018efficient]. Thanks to the use of Auto-Keras, R programmers with few lines of code will be able to train several deep learning models for their data and get the one that outperforms.

Let's dive into Auto-Keras!

## Auto-Keras

**Note:** the Python Auto-Keras library is only compatible with Python 3.6. So make sure this version is currently installed, and correctly set to be used by the [`reticulate`](https://rstudio.github.io/reticulate/) R library.

### Installation

To begin, install the autokeras R package from GitHub as follows:

```{r eval = FALSE}
if (!require("remotes")) {
  install.packages("remotes")
}
remotes::install_github("jcrodriguez1989/autokeras")
```

The Auto-Keras R interface uses the Keras and TensorFlow backend engines by default. To install both the core Auto-Keras library as well as the Keras and TensorFlow backends use the `install_autokeras()` function:

```{r eval = FALSE}
library("autokeras")
install_autokeras()
```

This will provide you with default CPU-based installations of Keras and TensorFlow. If you want a more customized installation, e.g. if you want to take advantage of NVIDIA GPUs, see the documentation for [`install_keras()`](https://keras.rstudio.com/reference/install_keras.html) from the `keras` R library.

### MNIST Example

We can learn the basics of Auto-Keras by walking through a simple example: recognizing handwritten digits from the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset. MNIST consists of 28 x 28 grayscale images of handwritten digits like this:

```{r mnistJpg, echo = FALSE, fig.align = "center", out.width = "120px"}
knitr::include_graphics("images/img_1.jpg")
```

The dataset also includes labels for each image, telling us which digit it is. For example, the label for the above image is 2.

#### Loading the Data

The MNIST dataset is included with Keras and can be accessed using the [`dataset_mnist()`](https://keras.rstudio.com/reference/index.html#section-datasets) function from the `keras` R library. Here we load the dataset then create variables for our test and training data:

```{r}
library("keras")
mnist <- dataset_mnist() # load mnist dataset
c(x_train, y_train) %<-% mnist$train # get train
c(x_test, y_test) %<-% mnist$test # and test data
```

The `x` data is a 3-d array `(images,width,height)` of grayscale integer values ranging between 0 to 255.

```{r}
x_train[1, 14:20, 14:20] # show some pixels from the first image
```

The `y` data is an integer vector with values ranging from 0 to 9.

```{r}
n_imgs <- 8
head(y_train, n = n_imgs) # show first 8 labels
```

Each of these images can be plotted in R:

```{r}
library("ggplot2")
library("tidyr")

# get each of the first n_imgs from the x_train dataset and
# convert them to wide format
mnist_to_plot <-
  do.call(rbind, lapply(seq_len(n_imgs), function(i) {
    samp_img <- x_train[i, , ] %>%
      as.data.frame()
    colnames(samp_img) <- seq_len(ncol(samp_img))
    data.frame(
      img = i,
      gather(samp_img, "x", "value", convert = TRUE),
      y = seq_len(nrow(samp_img))
    )
  }))

ggplot(mnist_to_plot, aes(x = x, y = y, fill = value)) + geom_tile() +
  scale_fill_gradient(low = "black", high = "white", na.value = NA) +
  scale_y_reverse() + theme_minimal() + theme(panel.grid = element_blank()) +
  theme(aspect.ratio = 1) + xlab("") + ylab("") + facet_wrap(~img, nrow = 2)
```

While we are going to use the pre-loaded MNIST dataset, Keras also provides [Image Preprocessing](https://keras.rstudio.com/reference/index.html#section-image-preprocessing) functions to easily load images. For instance, if we have an image in the file path `"images/img_1.jpg"`, then we can load it by doing (and then plot it as shown above):

```{r}
sample_img <-
  image_load("images/img_1.jpg", grayscale = TRUE) %>%
  image_to_array() %>%
  as.data.frame()
```

```{r, echo = FALSE, fig.align = "center", out.width = "240px"}
colnames(sample_img) <- seq_len(ncol(sample_img))
sample_img_tidy <- data.frame(
  gather(sample_img, "x", "value", convert = TRUE),
  y = seq_len(nrow(sample_img))
)

ggplot(sample_img_tidy, aes(x = x, y = y, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "black", high = "white", na.value = NA) +
  scale_y_reverse() +
  theme_minimal() +
  theme(panel.grid = element_blank()) +
  theme(aspect.ratio = 1) +
  xlab("") +
  ylab("")
```

#### Data ready, let's get the model!

Data pre-processing? Model definition? Metrics, epochs definition, any? No, none of them are required by Auto-Keras. For Image Classification tasks, it results enough for Auto-Keras to have the `x_train` and `y_train` objects as defined above.


From now on, to train several deep learning models for two hours, it results enough to run:

```{r, eval = FALSE}
# train an Image Classifier for two hours
clf <- model_image_classifier(verbose = TRUE) %>%
  fit(x_train, y_train, time_limit = 2 * 60 * 60)
```

    Saving Directory: /tmp/autokeras_ZOG76O
    Preprocessing the images.
    Preprocessing finished.
    
    Initializing search.
    Initialization finished.
    
    
    +----------------------------------------------+
    |               Training model 0               |
    +----------------------------------------------+
    
    No loss decrease after 5 epochs.
    
    
    Saving model.
    +--------------------------------------------------------------------------+
    |        Model ID        |          Loss          |      Metric Value      |
    +--------------------------------------------------------------------------+
    |           0            |  0.19463148526847363   |   0.9843999999999999   |
    +--------------------------------------------------------------------------+
    
    
    +----------------------------------------------+
    |               Training model 1               |
    +----------------------------------------------+
    
    No loss decrease after 5 epochs.
    
    
    Saving model.
    +--------------------------------------------------------------------------+
    |        Model ID        |          Loss          |      Metric Value      |
    +--------------------------------------------------------------------------+
    |           1            |   0.210642946138978    |         0.984          |
    +--------------------------------------------------------------------------+


Evaluate it:

```{r, eval = FALSE}
clf %>% evaluate(x_test, y_test)
```

    [1] 0.9866

And then just get the best-trained model with:

```{r, eval = FALSE}
clf %>% final_fit(x_train, y_train, x_test, y_test, retrain = TRUE)
```

    No loss decrease after 30 epochs.

Evaluate the final model:

```{r, eval = FALSE}
clf %>% evaluate(x_test, y_test)
```

    [1] 0.9918

And the model can be saved to take it into production with:

```{r, eval = FALSE}
clf %>% export_autokeras_model("./myMnistModel.pkl")
```

### Conclusions

In this post, the Auto-Keras R package was presented. It was shown that, with almost no deep learning knowledge, it is possible to train models and get the one that returns the best results for the desired task. Here we trained models for two hours, however, we have also tried training for 24 hours, and so, it trained 15 models and got a model with Accuracy of 0.9928. Although Auto-Keras will not return a model as efficient as the one generated manually by an expert, this new library results as an excellent starting point in the world of deep learning. Auto-Keras is an open-source R package and is freely available in [https://github.com/jcrodriguez1989/autokeras/](https://github.com/jcrodriguez1989/autokeras/).

Although the Python Auto-Keras library is currently in a pre-release version and has few types of training tasks developed, it was recently added to the keras-team working group. This leads to an imminent advancement of this library.

### Reproducibility

To correctly reproduce the results of this post, we recommend using the Auto-Keras docker image by typing:

```{bash eval = FALSE}
docker pull jcrodriguez1989/r-autokeras:0.1.0
docker run -it jcrodriguez1989/r-autokeras:0.1.0 /bin/bash
```
