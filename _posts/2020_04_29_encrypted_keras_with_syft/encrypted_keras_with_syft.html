<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

  <!--radix_placeholder_meta_tags-->
  <title>Towards privacy: Encrypted deep learning with Syft and Keras</title>

  <meta property="description" itemprop="description" content="Deep learning need not be irreconcilable with privacy protection. Federated learning enables on-device, distributed model training; encryption keeps model and gradient updates private; differential privacy prevents the training data from leaking. As of today, private and secure deep learning is an emerging technology. In this post, we introduce Syft, an open-source framework that integrates with PyTorch as well as TensorFlow. In an example use case, we obtain private predictions from a Keras model."/>


  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2020-04-29"/>
  <meta property="article:created" itemprop="dateCreated" content="2020-04-29"/>
  <meta name="article:author" content="Sigrid Keydana"/>

  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="Towards privacy: Encrypted deep learning with Syft and Keras"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="Deep learning need not be irreconcilable with privacy protection. Federated learning enables on-device, distributed model training; encryption keeps model and gradient updates private; differential privacy prevents the training data from leaking. As of today, private and secure deep learning is an emerging technology. In this post, we introduce Syft, an open-source framework that integrates with PyTorch as well as TensorFlow. In an example use case, we obtain private predictions from a Keras model."/>
  <meta property="og:locale" content="en_US"/>

  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="Towards privacy: Encrypted deep learning with Syft and Keras"/>
  <meta property="twitter:description" content="Deep learning need not be irreconcilable with privacy protection. Federated learning enables on-device, distributed model training; encryption keeps model and gradient updates private; differential privacy prevents the training data from leaking. As of today, private and secure deep learning is an emerging technology. In this post, we introduce Syft, an open-source framework that integrates with PyTorch as well as TensorFlow. In an example use case, we obtain private predictions from a Keras model."/>

  <!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=Federated learning of deep networks using model averaging;citation_publication_date=2016;citation_volume=abs/1602.05629;citation_author=H. Brendan McMahan;citation_author=Eider Moore;citation_author=Daniel Ramage;citation_author=Blaise Agüera Arcas"/>
  <meta name="citation_reference" content="citation_title=Differential privacy;citation_publication_date=2006;citation_publisher=Springer Verlag;citation_volume=4052;citation_author=Cynthia Dwork"/>
  <meta name="citation_reference" content="citation_title=Calibrating noise to sensitivity in private data analysis;citation_publication_date=2006;citation_publisher=Springer-Verlag;citation_doi=10.1007/11681878_14;citation_author=Cynthia Dwork;citation_author=Frank McSherry;citation_author=Kobbi Nissim;citation_author=Adam Smith"/>
  <!--radix_placeholder_rmarkdown_metadata-->

  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","slug","bibliography","date","categories","output","preview"]}},"value":[{"type":"character","attributes":{},"value":["Towards privacy: Encrypted deep learning with Syft and Keras"]},{"type":"character","attributes":{},"value":["Deep learning need not be irreconcilable with privacy protection. Federated learning enables on-device, distributed model training; encryption keeps model and gradient updates private; differential privacy prevents the training data from leaking. As of today, private and secure deep learning is an emerging technology. In this post, we introduce Syft, an open-source framework that integrates with PyTorch as well as TensorFlow. In an example use case, we obtain private predictions from a Keras model.  \n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","affiliation_url"]}},"value":[{"type":"character","attributes":{},"value":["Sigrid Keydana"]},{"type":"character","attributes":{},"value":["RStudio"]},{"type":"character","attributes":{},"value":["https://www.rstudio.com/"]}]}]},{"type":"character","attributes":{},"value":["keydanaprivacysyftkeras"]},{"type":"character","attributes":{},"value":["bibliography.bib"]},{"type":"character","attributes":{},"value":["04-29-2020"]},{"type":"character","attributes":{},"value":["R","Privacy & Security","TensorFlow/Keras"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"character","attributes":{},"value":["images/thumb.jpg"]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["bibliography.bib","encrypted_keras_with_syft_files/bowser-1.9.3/bowser.min.js","encrypted_keras_with_syft_files/distill-2.2.21/template.v2.js","encrypted_keras_with_syft_files/header-attrs-2.1/header-attrs.js","encrypted_keras_with_syft_files/jquery-1.11.3/jquery.min.js","encrypted_keras_with_syft_files/webcomponents-2.0.0/webcomponents.js","images/thumb.jpg"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

  <style type="text/css">

  body {
    background-color: white;
  }

  .pandoc-table {
    width: 100%;
  }

  .pandoc-table>caption {
    margin-bottom: 10px;
  }

  .pandoc-table th:not([align]) {
    text-align: left;
  }

  .pagedtable-footer {
    font-size: 15px;
  }

  .html-widget {
    margin-bottom: 2.0em;
  }

  .l-screen-inset {
    padding-right: 16px;
  }

  .l-screen .caption {
    margin-left: 10px;
  }

  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .shaded-content {
    background: white;
  }

  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }

  .hidden {
    display: none !important;
  }

  d-article {
    padding-bottom: 30px;
  }

  d-appendix {
    padding-top: 30px;
  }

  d-article>p>img {
    width: 100%;
  }

  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }

  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  /* CSS for table of contents */

  .d-toc {
    color: rgba(0,0,0,0.8);
    font-size: 0.8em;
    line-height: 1em;
  }

  .d-toc-header {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    text-transform: uppercase;
    margin-top: 0;
    margin-bottom: 1.3em;
  }

  .d-toc a {
    border-bottom: none;
  }

  .d-toc ul {
    padding-left: 0;
  }

  .d-toc li>ul {
    padding-top: 0.8em;
    padding-left: 16px;
    margin-bottom: 0.6em;
  }

  .d-toc ul,
  .d-toc li {
    list-style-type: none;
  }

  .d-toc li {
    margin-bottom: 0.9em;
  }

  .d-toc-separator {
    margin-top: 20px;
    margin-bottom: 2em;
  }

  .d-article-with-toc {
    border-top: none;
    padding-top: 0;
  }



  /* Tweak code blocks (note that this CSS is repeated above in an injection
     into the d-code shadow dom) */

  d-code {
    overflow-x: auto !important;
  }

  pre.d-code code.d-code {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }

  pre.text-output {

    font-size: 12px;
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  @media(min-width: 768px) {

  d-code {
    overflow-x: visible !important;
  }

  pre.d-code code.d-code  {
      padding-left: 18px;
      font-size: 14px;
  }
  pre.text-output {
    font-size: 14px;
  }
  }

  /* Figure */

  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }

  .figure img {
    width: 100%;
  }

  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }

  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }

  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }



  /* Tweak 1000px media break to show more text */

  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }

    .grid {
      grid-column-gap: 16px;
    }

    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }

  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }

    .grid {
      grid-column-gap: 32px;
    }
  }


  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */

  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }


  /* Social footer */

  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }

  .disqus-comments {
    margin-right: 30px;
  }

  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }

  #disqus_thread {
    margin-top: 30px;
  }

  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }

  .article-sharing a:hover {
    border-bottom: none;
  }

  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }

  .subscribe p {
    margin-bottom: 0.5em;
  }


  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }


  /* Improve display for browsers without grid (IE/Edge <= 15) */

  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }

  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }

  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }

  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }

  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }

  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }

  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }


  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }

  .downlevel .footnotes ol {
    padding-left: 13px;
  }

  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }

  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }

  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }

  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  </style>

  <script type="application/javascript">

  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }

  // show body when load is complete
  function on_load_complete() {

    // set body to visible
    document.body.style.visibility = 'visible';

    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }

    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }

  function init_distill() {

    init_common();

    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);

    // create d-title
    $('.d-title').changeElementType('d-title');

    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);

    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();

    // move posts container into article
    $('.posts-container').appendTo($('d-article'));

    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');

    // create d-bibliography
    var bibliography = $('<d-bibliography></d-bibliography>');
    $('#distill-bibliography').wrap(bibliography);

    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;

    // replace citations with <d-cite>
    $('.citation').each(function(i, val) {
      appendix = true;
      var cites = $(this).attr('data-cites').split(" ");
      var dt_cite = $('<d-cite></d-cite>');
      dt_cite.attr('key', cites.join());
      $(this).replaceWith(dt_cite);
    });
    // remove refs
    $('#refs').remove();

    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();

    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-toc a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });

    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');

    // replace code blocks with d-code
    $('pre>code').each(function(i, val) {
      var code = $(this);
      var pre = code.parent();
      var clz = "";
      var language = pre.attr('class');
      if (language) {
        // map unknown languages to "clike" (without this they just dissapear)
        if ($.inArray(language, ["bash", "clike", "css", "go", "html",
                                 "javascript", "js", "julia", "lua", "markdown",
                                 "markup", "mathml", "python", "svg", "xml"]) == -1)
          language = "clike";
        language = ' language="' + language + '"';
        var dt_code = $('<d-code block' + language + clz + '></d-code>');
        dt_code.text(code.text());
        pre.replaceWith(dt_code);
      } else {
        code.addClass('text-output').unwrap().changeElementType('pre');
      }
    });

    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {

      // capture layout
      var layout = $(this).attr('data-layout');

      // apply layout to markdown level block elements
      var elements = $(this).children().not('d-code, pre.text-output, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });


      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });

    // load distill framework
    load_distill_framework();

    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {

      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;

      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');

      // table of contents
      if (have_authors) // adjust border if we are in authors
        $('.d-toc').parent().addClass('d-article-with-toc');

      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }

      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");

      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }

       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }

      // inject pre code styles (can't do this with a global stylesheet b/c a shadow root is used)
      $('d-code').each(function(i, val) {
        var style = document.createElement('style');
        style.innerHTML = 'pre code { padding-left: 10px; font-size: 12px; border-left: 2px solid rgba(0,0,0,0.1); } ' +
                          '@media(min-width: 768px) { pre code { padding-left: 18px; font-size: 14px; } }';
        if (this.shadowRoot)
          this.shadowRoot.appendChild(style);
      });

      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();

      // clear polling timer
      clearInterval(tid);

      // show body now that everything is ready
      on_load_complete();
    }

    var tid = setInterval(distill_post_process, 50);
    distill_post_process();

  }

  function init_downlevel() {

    init_common();

     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));

    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;

    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();

    // remove toc
    $('.d-toc-header').remove();
    $('.d-toc').remove();
    $('.d-toc-separator').remove();

    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });


    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);

    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();

    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });

    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));

    $('body').addClass('downlevel');

    on_load_complete();
  }


  function init_common() {

    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};

        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });

        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);

    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});

    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });

      }
    });

    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });

    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.distill-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
      });
    }

    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');

    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");

    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();

    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }

  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });

  </script>

  <!--/radix_placeholder_distill-->
  <script src="encrypted_keras_with_syft_files/header-attrs-2.1/header-attrs.js"></script>
  <script src="encrypted_keras_with_syft_files/jquery-1.11.3/jquery.min.js"></script>
  <script src="encrypted_keras_with_syft_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="encrypted_keras_with_syft_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="encrypted_keras_with_syft_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Towards privacy: Encrypted deep learning with Syft and Keras","description":"Deep learning need not be irreconcilable with privacy protection. Federated learning enables on-device, distributed model training; encryption keeps model and gradient updates private; differential privacy prevents the training data from leaking. As of today, private and secure deep learning is an emerging technology. In this post, we introduce Syft, an open-source framework that integrates with PyTorch as well as TensorFlow. In an example use case, we obtain private predictions from a Keras model.","authors":[{"author":"Sigrid Keydana","authorURL":"#","affiliation":"RStudio","affiliationURL":"https://www.rstudio.com/"}],"publishedDate":"2020-04-29T00:00:00.000+02:00","citationText":"Keydana, 2020"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Towards privacy: Encrypted deep learning with Syft and Keras</h1>
<p><p>Deep learning need not be irreconcilable with privacy protection. Federated learning enables on-device, distributed model training; encryption keeps model and gradient updates private; differential privacy prevents the training data from leaking. As of today, private and secure deep learning is an emerging technology. In this post, we introduce Syft, an open-source framework that integrates with PyTorch as well as TensorFlow. In an example use case, we obtain private predictions from a Keras model.</p></p>
</div>

<div class="d-byline">
  Sigrid Keydana  (RStudio)<a href="https://www.rstudio.com/" class="uri">https://www.rstudio.com/</a>
  
<br/>04-29-2020
</div>

<div class="d-article">
<p>The word <em>privacy</em>, in the context of deep learning (or machine learning, or “AI”), and especially when combined with things like <em>security</em>, sounds like it could be part of a catch phrase: <em>privacy, safety, security</em> – like <em>liberté, fraternité, égalité</em>. In fact, there should probably be a mantra like that. But that’s another topic, and like with the other catch phrase just cited, not everyone interprets these terms in the same way.</p>
<p>So let’s think about privacy, narrowed down to its role in training or using deep learning models, in a more technical way. Since privacy – or rather, its violations – may appear in various ways, different violations will demand different countermeasures. Of course, in the end, we’d like to see them all integrated – but re privacy-related technologies, the field is really just starting out on a journey. The most important thing we can do, then, is to learn about the concepts, investigate the landscape of implementations under development, and – perhaps – decide to join the effort.</p>
<p>This post tries to do a tiny little bit of all of those.</p>
<h2 id="aspects-of-privacy-in-deep-learning">Aspects of privacy in deep learning</h2>
<p>Say you work at a hospital, and would be interested in training a deep learning model to help diagnose some disease from brain scans. Where you work, you don’t have many patients with this disease; moreover, they tend to mostly be affected by the same subtypes: Your training set, were you to create one, would not reflect the overall distribution very well. It would, thus, make sense to cooperate with other hospitals; but that isn’t so easy, as the data collected is protected by privacy regulations. So, the first requirement is: The data has to stay where it is; e.g., it may not be sent to a central server.</p>
<h4 id="federated-learning">Federated learning</h4>
<p>This first <em>sine qua non</em> is addressed by <a href="https://blogs.rstudio.com/tensorflow/posts/2020-04-08-tf-federated-intro/">federated learning</a> <span class="citation" data-cites="McMahanMRA16">(McMahan et al. <a href="#ref-McMahanMRA16" role="doc-biblioref">2016</a>)</span>. Federated learning is not “just” desirable for privacy reasons. On the contrary, in many use cases, it may be the only viable way (like with smartphones or sensors, which collect gigantic amounts of data). In federated learning, each participant receives a copy of the model, trains on their own data, and sends back the gradients obtained to the central server, where gradients are averaged and applied to the model.</p>
<p>This is good insofar as the data never leaves the individual devices; however, a lot of information can still be extracted from plain-text gradients. Imagine a smartphone app that provides trainable auto-completion for text messages. Even if gradient updates from many iterations are averaged, their distributions will greatly vary between individuals. Some form of encryption is needed. But then how is the server going to make sense of the encrypted gradients?</p>
<p>One way to accomplish this relies on <em>secure multi-party computation</em> (SMPC).</p>
<h4 id="secure-multi-party-computation">Secure multi-party computation</h4>
<p>In SMPC, we need a system of several agents who collaborate to provide a result no single agent could provide alone: “normal” computations (like addition, multiplication …) on “secret” (encrypted) data. The assumption is that these agents are “honest but curious” – honest, because they won’t tamper with their share of data; curious in the sense that if they <em>were</em> (curious, that is), they wouldn’t be able to inspect the data because it’s encrypted.</p>
<p>The principle behind this is <em>secret sharing</em>. A single piece of data – a salary, say – is “split up” into meaningless (hence, encrypted) parts which, when put together again, yield the original data. Here is an example.</p>
<p>Say the parties involved are Julia, Greg, and me. The below function encrypts a single value, assigning to each of us their “meaningless” share:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# a big prime number
# all computations are performed in a finite field, for example, the integers modulo that prime
Q &lt;- 78090573363827
 
encrypt &lt;- function(x) {
  # all but the very last share are random 
  julias &lt;- runif(1, min = -Q, max = Q)
  gregs &lt;- runif(1, min = -Q, max = Q)
  mine &lt;- (x - julias - gregs) %% Q
  list (julias, gregs, mine)
}

# some top secret value no-one may get to see
value &lt;- 77777

encrypted &lt;- encrypt(value)
encrypted</code></pre>
</div>
<pre><code>
[[1]]
[1] 7467283737857

[[2]]
[1] 36307804406429

[[3]]
[1] 34315485297318</code></pre>
<p>Once the three of us put our shares together, getting back the plain value is straightforward:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
decrypt &lt;- function(shares) {
  Reduce(sum, shares) %% Q  
}

decrypt(encrypted)</code></pre>
</div>
<pre><code>
77777</code></pre>
<p>As an example of how to compute on encrypted data, here’s addition. (Other operations will be a lot less straightforward.) To add two numbers, just have everyone add their respective shares:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
add &lt;- function(x, y) {
  list(
    # julia
    (x[[1]] + y[[1]]) %% Q,
    # greg
    (x[[2]] + y[[2]]) %% Q,
    # me
    (x[[3]] + y[[3]]) %% Q
  )
}
  
x &lt;- encrypt(11)
y &lt;- encrypt(122)

decrypt(add(x, y))</code></pre>
</div>
<pre><code>
133</code></pre>
<p>Back to the setting of deep learning and the current task to be solved: Have the server apply gradient updates without ever seeing them. With secret sharing, it would work like this:</p>
<p>Julia, Greg and me each want to train on our own private data. Together, we will be responsible for gradient averaging, that is, we’ll form a <em>cluster</em> of <em>workers</em> united in that task. Now, the model owner <em>secret shares</em> the model, and we start training, each on their own data. After some number of iterations, we use secure averaging to combine our respective gradients. Then, all the server gets to see is the mean gradient, and there is no way to determine our respective contributions.</p>
<h4 id="beyond-private-gradients">Beyond private gradients</h4>
<p>Amazingly, it is even possible to <em>train</em> on encrypted data – amongst others, using that same technique of secret sharing. Of course, this has to negatively affect training speed. But it’s good to know that if one’s use case were to demand it, it would be feasible. (One possible use case is when training on one party’s data alone doesn’t make any sense, but data is sensitive, so others won’t let you access their data unless encrypted.)</p>
<p>So with encryption available on an all-you-need basis, are we completely safe, privacy-wise? The answer is no. The model can still leak information. For example, in some cases it is possible to perform <em>model inversion</em> [@abs-1805-04049], that is, with just black-box access to a model, train an <em>attack model</em> that allows reconstructing some of the original training data. Needless to say, this kind of leakage has to be avoided. <a href="https://blogs.rstudio.com/tensorflow/posts/2019-12-20-differential-privacy/">Differential privacy</a> <span class="citation" data-cites="Dwork2006">(Dwork et al. <a href="#ref-Dwork2006" role="doc-biblioref">2006</a>)</span>, <span class="citation" data-cites="dwork2006differential">(Dwork <a href="#ref-dwork2006differential" role="doc-biblioref">2006</a>)</span> demands that results obtained from querying a model be independent from the presence or absence, in the dataset employed for training, of a single individual. In general, this is ensured by adding noise to the answer to every query. In training deep learning models, we add noise to the gradients, as well as clip them according to some chosen norm.</p>
<p>At some point, then, we will want all of those in combination: federated learning, encryption, and differential privacy.</p>
<p><em>Syft</em> is a very promising, very actively developed framework that aims for providing all of them. Instead of “aims for”, I should perhaps have written “provides” – it depends. We need some more context.</p>
<h2 id="introducing-syft">Introducing Syft</h2>
<p>Syft – also known as <a href="https://github.com/OpenMined/PySyft">PySyft</a>, since as of today, its most mature implementation is written in and for Python – is maintained by <a href="https://www.openmined.org/">OpenMined</a>, an open source community dedicated to enabling privacy-preserving AI. It’s worth it reproducing their mission statement here:</p>
<blockquote>
<p>Industry standard tools for artificial intelligence have been designed with several assumptions: data is centralized into a single compute cluster, the cluster exists in a secure cloud, and the resulting models will be owned by a central authority. We envision a world in which we are not restricted to this scenario - a world in which AI tools treat privacy, security, and multi-owner governance as first class citizens. […] The mission of the OpenMined community is to create an accessible ecosystem of tools for private, secure, multi-owner governed AI.</p>
</blockquote>
<p>While far from being the only one, PySyft is their most maturely developed framework. Its role is to provide secure federated learning, including encryption and differential privacy. For deep learning, it relies on existing frameworks.</p>
<p>PyTorch integration seems the most mature, as of today; with PyTorch, encrypted and differentially private training are already available. Integration with TensorFlow is a bit more involved; it does not yet include TensorFlow Federated and TensorFlow Privacy. For encryption, it relies on <a href="https://github.com/tf-encrypted/tf-encrypted">TensorFlow Encrypted</a> (TFE), which as of this writing is not an official TensorFlow subproject.</p>
<p>However, even now it is already possible to <em>secret share</em> Keras models and administer private predictions. Let’s see how.</p>
<h2 id="private-predictions-with-syft-tensorflow-encrypted-and-keras">Private predictions with Syft, TensorFlow Encrypted and Keras</h2>
<p>Our introductory example will show how to use an externally-provided model to classify private data – without the model owner ever seeing that data, <em>and</em> without the user ever getting hold of (e.g., downloading) the model. (Think about the model owner wanting to keep the fruits of their labour hidden, as well.)</p>
<p>Put differently: The model is encrypted, and the data is, too. As you might imagine, this involves a cluster of agents, together performing secure multi-party computation.</p>
<p>This use case presupposing an already trained model, we start by quickly creating one. There is nothing special going on here.</p>
<h4 id="prelude-train-a-simple-model-on-mnist">Prelude: Train a simple model on MNIST</h4>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# create_model.R

library(tensorflow)
library(keras)

mnist &lt;- dataset_mnist()
mnist$train$x &lt;- mnist$train$x/255
mnist$test$x &lt;- mnist$test$x/255

dim(mnist$train$x) &lt;- c(dim(mnist$train$x), 1)
dim(mnist$test$x) &lt;- c(dim(mnist$test$x), 1)

input_shape &lt;- c(28, 28, 1)

model &lt;- keras_model_sequential() %&gt;%
  layer_conv_2d(filters = 16, kernel_size = c(3, 3), input_shape = input_shape) %&gt;%
  layer_average_pooling_2d(pool_size = c(2, 2)) %&gt;%
  layer_activation(&quot;relu&quot;) %&gt;%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3)) %&gt;%
  layer_average_pooling_2d(pool_size = c(2, 2)) %&gt;%
  layer_activation(&quot;relu&quot;) %&gt;%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3)) %&gt;%
  layer_average_pooling_2d(pool_size = c(2, 2)) %&gt;%
  layer_activation(&quot;relu&quot;) %&gt;%
  layer_flatten() %&gt;%
  layer_dense(units = 10, activation = &quot;linear&quot;)
  

model %&gt;% compile(
  loss = &quot;sparse_categorical_crossentropy&quot;,
  optimizer = &quot;adam&quot;,
  metrics = &quot;accuracy&quot;
)

model %&gt;% fit(
    x = mnist$train$x,
    y = mnist$train$y,
    epochs = 1,
    validation_split = 0.3,
    verbose = 2
)

model$save(filepath = &quot;model.hdf5&quot;)</code></pre>
</div>
<h4 id="set-up-cluster-and-serve-model">Set up cluster and serve model</h4>
<p>The easiest way to get all required packages is to install the ensemble OpenMined put together for their <a href="https://www.udacity.com/course/secure-and-private-ai--ud185">Udacity Course</a> that introduces federated learning and differential privacy with PySyft. This will install TensorFlow 1.15 and TensorFlow Encrypted, amongst others.</p>
<p>The following lines of code should all be put together in a single file. I found it practical to “source” this script from an R process running in a console tab.</p>
<p>To begin, we again define the model, two things being different now. First, for technical reasons, we need to pass in <code>batch_input_shape</code> instead of <code>input_shape</code>. Second, the final layer is “missing” the softmax activation. This is not an oversight – SMPC <code>softmax</code> has not been implemented yet. (Depending on when you read this, that statement may no longer be true.) Were we training this model in <em>secret sharing</em> mode, this would of course be a problem; for classification though, all we care about is the maximum score.</p>
<p>After model definition, we load the actual weights from the model we trained in the previous step. Then, the action begins. We create an ensemble of TFE workers that together run a distributed TensorFlow cluster. The model is <em>secret</em> <em>shared</em> with the workers, that is, model weights are split up into shares that, each inspected alone, are unusable. Finally, the model is <em>served</em>, i.e., made available to clients requesting predictions.</p>
<p>How can a Keras model be <em>shared</em> and <em>served</em>? These are not methods provided by Keras itself. The magic comes from Syft <em>hooking</em> into Keras, extending the <code>model</code> object: cf. <code>hook &lt;- sy$KerasHook(tf$keras)</code> right after we import Syft.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# serve.R
# you could start R on the console and &quot;source&quot; this file

# do this just once
reticulate::py_install(&quot;syft[udacity]&quot;)

library(tensorflow)
library(keras)

sy &lt;- reticulate::import((&quot;syft&quot;))
hook &lt;- sy$KerasHook(tf$keras)

batch_input_shape &lt;- c(1, 28, 28, 1)

model &lt;- keras_model_sequential() %&gt;%
 layer_conv_2d(filters = 16, kernel_size = c(3, 3), batch_input_shape = batch_input_shape) %&gt;%
 layer_average_pooling_2d(pool_size = c(2, 2)) %&gt;%
 layer_activation(&quot;relu&quot;) %&gt;%
 layer_conv_2d(filters = 32, kernel_size = c(3, 3)) %&gt;%
 layer_average_pooling_2d(pool_size = c(2, 2)) %&gt;%
 layer_activation(&quot;relu&quot;) %&gt;%
 layer_conv_2d(filters = 64, kernel_size = c(3, 3)) %&gt;%
 layer_average_pooling_2d(pool_size = c(2, 2)) %&gt;%
 layer_activation(&quot;relu&quot;) %&gt;%
 layer_flatten() %&gt;%
 layer_dense(units = 10) 
 
pre_trained_weights &lt;- &quot;model.hdf5&quot;
model$load_weights(pre_trained_weights)

# create and start TFE cluster
AUTO &lt;- TRUE
julia &lt;- sy$TFEWorker(host = &#39;localhost:4000&#39;, auto_managed = AUTO)
greg &lt;- sy$TFEWorker(host = &#39;localhost:4001&#39;, auto_managed = AUTO)
me &lt;- sy$TFEWorker(host = &#39;localhost:4002&#39;, auto_managed = AUTO)
cluster &lt;- sy$TFECluster(julia, greg, me)
cluster$start()

# split up model weights into shares 
model$share(cluster)

# serve model (limiting number of requests)
model$serve(num_requests = 3L)</code></pre>
</div>
<p>Once the desired number of requests have been served, we can go to this R process, stop model sharing, and shut down the cluster:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# stop model sharing
model$stop()

# stop cluster
cluster$stop()</code></pre>
</div>
<p>Now, on to the client(s).</p>
<h4 id="request-predictions-on-private-data">Request predictions on private data</h4>
<p>In our example, we have one client. The client is a TFE worker, just like the agents that make up the cluster.</p>
<p>We define the cluster here, client-side, as well; create the client; and connect the client to the model. This will set up a queueing server that takes care of <em>secret sharing</em> all input data before submitting them for prediction.</p>
<p>Finally, we have the client asking for classification of the first three MNIST images.</p>
<p>With the server running in some different R process, we can conveniently run this in RStudio:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# client.R

library(tensorflow)
library(keras)

sy &lt;- reticulate::import((&quot;syft&quot;))
hook &lt;- sy$KerasHook(tf$keras)

mnist &lt;- dataset_mnist()
mnist$train$x &lt;- mnist$train$x/255
mnist$test$x &lt;- mnist$test$x/255

dim(mnist$train$x) &lt;- c(dim(mnist$train$x), 1)
dim(mnist$test$x) &lt;- c(dim(mnist$test$x), 1)

batch_input_shape &lt;- c(1, 28, 28, 1)
batch_output_shape &lt;- c(1, 10)

# define the same TFE cluster
AUTO &lt;- TRUE
julia &lt;- sy$TFEWorker(host = &#39;localhost:4000&#39;, auto_managed = AUTO)
greg &lt;- sy$TFEWorker(host = &#39;localhost:4001&#39;, auto_managed = AUTO)
me &lt;- sy$TFEWorker(host = &#39;localhost:4002&#39;, auto_managed = AUTO)
cluster &lt;- sy$TFECluster(julia, greg, me)

# create the client
client &lt;- sy$TFEWorker()

# create a queueing server on the client that secret shares the data 
# before submitting a prediction request
client$connect_to_model(batch_input_shape, batch_output_shape, cluster)

num_tests &lt;- 3
images &lt;- mnist$test$x[1: num_tests, , , , drop = FALSE]
expected_labels &lt;- mnist$test$y[1: num_tests]

for (i in 1:num_tests) {
  res &lt;- client$query_model(images[i, , , , drop = FALSE])
  predicted_label &lt;- which.max(res) - 1
  cat(&quot;Actual: &quot;, expected_labels[i], &quot;, predicted: &quot;, predicted_label)
}</code></pre>
</div>
<pre><code>
Actual:  7 , predicted:  7 
Actual:  2 , predicted:  2 
Actual:  1 , predicted:  1 </code></pre>
<p>There we go. Both model and data did remain secret, yet we were able to classify our data.</p>
<p>Let’s wrap up.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Our example use case has not been too ambitious – we started with a trained model, thus leaving aside federated learning. Keeping the setup simple, we were able to focus on underlying principles: <em>Secret sharing</em> as a means of encryption, and setting up a Syft/TFE cluster of workers that together, provide the infrastructure for encrypting model weights as well as client data.</p>
<p>In case you’ve read our previous post on <a href="https://blogs.rstudio.com/tensorflow/posts/2019-12-20-differential-privacy/">TensorFlow Federated</a> – that, too, a framework under development – you may have gotten an impression similar to the one I got: Setting up Syft was a lot more straightforward, concepts were easy to grasp, and surprisingly little code was required. As we may gather from a <a href="https://blog.openmined.org/introducing-pysyft-tensorflow/">recent blog post</a>, integration of Syft with TensorFlow Federated and TensorFlow Privacy are on the roadmap. I am looking forward <em>a lot</em> for this to happen.</p>
<p>Thanks for reading!</p>
<h2 class="unnumbered" id="section"></h2>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-dwork2006differential">
<p>Dwork, Cynthia. 2006. “Differential Privacy.” In <em>33rd International Colloquium on Automata, Languages and Programming, Part Ii (Icalp 2006)</em>, 33rd International Colloquium on Automata, Languages and Programming, part II (ICALP 2006), 4052:1–12. Lecture Notes in Computer Science. Springer Verlag. <a href="https://www.microsoft.com/en-us/research/publication/differential-privacy/">https://www.microsoft.com/en-us/research/publication/differential-privacy/</a>.</p>
</div>
<div id="ref-Dwork2006">
<p>Dwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. “Calibrating Noise to Sensitivity in Private Data Analysis.” In <em>Proceedings of the Third Conference on Theory of Cryptography</em>, 265–84. TCC’06. Berlin, Heidelberg: Springer-Verlag. <a href="https://doi.org/10.1007/11681878_14">https://doi.org/10.1007/11681878_14</a>.</p>
</div>
<div id="ref-McMahanMRA16">
<p>McMahan, H. Brendan, Eider Moore, Daniel Ramage, and Blaise Agüera y Arcas. 2016. “Federated Learning of Deep Networks Using Model Averaging.” <em>CoRR</em> abs/1602.05629. <a href="http://arxiv.org/abs/1602.05629">http://arxiv.org/abs/1602.05629</a>.</p>
</div>
</div>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>


<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom"></div>
<script id="distill-bibliography" type="text/bibtex">
@article{McMahanMRA16,
  author    = {H. Brendan McMahan and
               Eider Moore and
               Daniel Ramage and
               Blaise Agüera y Arcas},
  title     = {Federated Learning of Deep Networks using Model Averaging},
  journal   = {CoRR},
  volume    = {abs/1602.05629},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.05629},
  archivePrefix = {arXiv},
  eprint    = {1602.05629},
  timestamp = {Mon, 13 Aug 2018 16:48:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/McMahanMRA16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{dwork2006differential,
author = {Dwork, Cynthia},
title = {Differential Privacy},
series = {Lecture Notes in Computer Science},
booktitle = {33rd International Colloquium on Automata, Languages and Programming, part II (ICALP 2006)},
year = {2006},
month = {July},
abstract = {In 1977 Dalenius articulated a desideratum for statistical databases: nothing about an individual should be learnable from the database that cannot be learned without access to the database. We give a general impossibility result showing that a formalization of Dalenius’ goal along the lines of semantic security cannot be achieved. Contrary to intuition, a variant of the result threatens the privacy even of someone not in the database. This state of affairs suggests a new measure, differential privacy, which, intuitively, captures the increased risk to one’s privacy incurred by participating in a database. The techniques developed in a sequence of papers [8, 13, 3], culminating in those described in [12], can achieve any desired level of privacy under this measure. In many cases, extremely accurate information about the database can be provided while simultaneously ensuring very high levels of privacy.},
publisher = {Springer Verlag},
url = {https://www.microsoft.com/en-us/research/publication/differential-privacy/},
pages = {1-12},
volume = {4052},
isbn = {3-540-35907-9},
edition = {33rd International Colloquium on Automata, Languages and Programming, part II (ICALP 2006)},
}

@inproceedings{Dwork2006,
 author = {Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi and Smith, Adam},
 title = {Calibrating Noise to Sensitivity in Private Data Analysis},
 booktitle = {Proceedings of the Third Conference on Theory of Cryptography},
 series = {TCC'06},
 year = {2006},
 isbn = {3-540-32731-2, 978-3-540-32731-8},
 location = {New York, NY},
 pages = {265--284},
 numpages = {20},
 url = {http://dx.doi.org/10.1007/11681878_14},
 doi = {10.1007/11681878_14},
 acmid = {2180305},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
} 

@article{abs-1805-04049,
  author    = {Luca Melis and
               Congzheng Song and
               Emiliano De Cristofaro and
               Vitaly Shmatikov},
  title     = {Inference Attacks Against Collaborative Learning},
  journal   = {CoRR},
  volume    = {abs/1805.04049},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.04049},
  archivePrefix = {arXiv},
  eprint    = {1805.04049},
  timestamp = {Mon, 13 Aug 2018 16:47:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1805-04049.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
</script>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
