---
title: "Chat with LLMs using chattr"
description: >
  TODO  
author:
  - name: Edgar Ruiz
    affiliation: Posit
    affiliation_url: https://www.posit.co/
slug: llms-with-chattr
date: 2024-04-04
categories:
  - Generenative Models
  - Packages/Releases  
  - R
output:
  distill::distill_article:
    self_contained: false
    toc: true
preview: images/app.png
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  eval = FALSE, 
  fig.width = 6,
  fig.height = 6
  )
```


## Getting started 



```{r}
# Install from GitHub
remotes::install_github("mlverse/chattr")

# Run the app
chattr::chattr_app()

#> ── chattr - Available models 
#> Select the number of the model you would like to use:
#>
#> 1: GitHub - Copilot Chat -  (copilot) 
#>
#> 2: OpenAI - Chat Completions - gpt-3.5-turbo (gpt35) 
#>
#> 3: OpenAI - Chat Completions - gpt-4 (gpt4) 
#>
#> 4: LlamaGPT - ~/ggml-gpt4all-j-v1.3-groovy.bin (llamagpt) 
#>
#>
#> Selection:
>
```

`chattr` will try and identify which models you have setup in your laptop, 
and will include only those in the prompt. For example, if you have only have 
OpenAI setup, then the prompt will look something like this:

```r
#> ── chattr - Available models 
#> Select the number of the model you would like to use:
#>
#> 2: OpenAI - Chat Completions - gpt-3.5-turbo (gpt35) 
#>
#> 3: OpenAI - Chat Completions - gpt-4 (gpt4) 
#>
#> Selection:
>
```

For Copilot and OpenAI, `chattr` confirms that there is an available 
authentication token in order to show them in the prompt.  After your selection
the Shiny app will run in your RStudio Viewer pane:

![](images/app.png)

## Works with local LLMs

Open-source, trained models, that are able to run in your laptop are widely
available today. Instead of integrating with each individually, `chattr` works
with **LlamaGPTJ-chat**. This is a lightweight application that communicates 
with a variety of local models. At this time, LlamaGPTJ-chat integrates with the
following family models:

- **GPT-J** (ggml and gpt4all models)
- **LLaMA** (ggml Vicuna models from Meta)
- **Mosaic Pretrained Transformers (MPT)** 

LlamaGPTJ-chat works right off the terminal. `chattr` integrates with the
application by starting an 'hidden' terminal session. There it initializes the
selected model, and makes it available to start chatting with it. 

To get started, you need to install LlamaGPTJ-chat, and download a compatible
model. More detailed instructions are found
[here](https://mlverse.github.io/chattr/articles/backend-llamagpt.html#installation).

## Integrating with `chattr`

The idea for `chattr` is to make it easier for new LLM APIs to be added. `chattr`
is basically split in two sections, the user-interface (Shiny app and 
`chattr()` function), and the included back-ends (GPT, Copilot, LLamaGPT). 
New back-ends do not need to be added directly in `chattr`.  If you are a package
developer, and would like to take advantage of the `chattr` UI, you will only 
need to include a `ch_submit()` method in your package. 

The two output requirements for `ch_submit()` are: 

- As the final return value, send the full response from the model you are 
integrating into `chattr`

- If streaming (`stream` is TRUE), output the current output as it is occurring. 
Generally through a `cat()` function call. 

For more detail, please visit the function's reference page, link 
[here](https://mlverse.github.io/chattr/reference/ch_submit.html).



## Next steps

