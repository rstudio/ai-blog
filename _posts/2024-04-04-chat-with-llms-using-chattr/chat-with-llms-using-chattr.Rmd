---
title: "Chat with LLMs using chattr"
description: >
  TODO  
author:
  - name: Edgar Ruiz
    affiliation: Posit
    affiliation_url: https://www.posit.co/
slug: llms-with-chattr
date: 2024-04-04
categories:
  - Generenative Models
  - Packages/Releases  
  - R
output:
  distill::distill_article:
    self_contained: false
    toc: true
preview: images/app.png
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  eval = FALSE, 
  fig.width = 6,
  fig.height = 6
  )
```


## Works with Copilot and ChatGPT

## Works with local LLMs

Open-source, trained models that are able to run in your laptop are widely
available today. Instead of integrating with each individually, `chattr` works
with **LlamaGPTJ-chat**. This is a lightweight application that communicates 
with a variety of local models. At this time, LlamaGPTJ-chat integrates with the
following family models:

- **GPT-J** (ggml and gpt4all models)
- **LLaMA** (ggml Vicuna models from Meta)
- **Mosaic Pretrained Transformers (MPT)** 

LlamaGPTJ-chat works right off the terminal. `chattr` integrates with the
application by starting an 'hidden' terminal session, where it initializes the
selected model, and makes it available to start chatting with it. 

To get started, you need to install LlamaGPTJ-chat, and download a compatible
model. More detailed instructions are found [here](https://mlverse.github.io/chattr/articles/backend-llamagpt.html#installation).


## Integrating with `chattr`

## Next steps

