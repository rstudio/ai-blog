---
title: "Chat with LLMs using chattr"
description: >
  TODO  
author:
  - name: Edgar Ruiz
    affiliation: Posit
    affiliation_url: https://www.posit.co/
slug: llms-with-chattr
date: 2024-04-04
categories:
  - Generenative Models
  - Packages/Releases  
  - R
output:
  distill::distill_article:
    self_contained: false
    toc: true
preview: images/app.png
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  eval = FALSE, 
  fig.width = 6,
  fig.height = 6
  )
```


## Works with Copilot and ChatGPT

## Works with local LLMs

Open-source, trained models that are able to run in your laptop are widely
available today. Instead of integrating with each individually, `chattr` works
with **LlamaGPTJ-chat**. This is a lightweight application that communicates 
with a variety of local models. At this time, LlamaGPTJ-chat integrates with the
following family models:

- **GPT-J** (ggml and gpt4all models)
- **LLaMA** (ggml Vicuna models from Meta)
- **Mosaic Pretrained Transformers (MPT)** 

LlamaGPTJ-chat works right off the terminal. `chattr` integrates with the
application by starting an 'hidden' terminal session, where it initializes the
selected model, and makes it available to start chatting with it. 

To get started, you need to install LlamaGPTJ-chat, and download a compatible
model. More detailed instructions are found [here](https://mlverse.github.io/chattr/articles/backend-llamagpt.html#installation).

## Integrating with `chattr`

The idea for `chattr` is to make it easier for new LLM APIs to be added. `chattr`
is basically split in two sections, the user-interface (Shiny app and 
`chattr()` function), and the included back-ends (GPT, Copilot, LLamaGPT). 
New back-ends do not need to be added directly in `chattr`.  If you are a package
developer, and would like to take advantage of `chattr` UI, you will only 
need to include a `ch_submit()` method in your package. 

The two output requirements for `ch_submit()` are: 

- As the final return value, send the full response from the model you are 
integrating into `chattr`

- If streaming (`stream` is TRUE), output the current output as it is occurring. 
Generally through a `cat()` function call. 

For more detail, please visit the function's reference page, link 
[here](https://mlverse.github.io/chattr/reference/ch_submit.html).



## Next steps

