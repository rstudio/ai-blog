---
title: "News from the sparkly-verse"
description: >
  TODO: Add description
author:
  - name: Edgar Ruiz
    affiliation: Posit
    affiliation_url: https://www.posit.co/
slug: sparklyr-updates-q1-2024
date: 2024-04-22
categories:
  - Packages/Releases  
  - Spark
  - R
output:
  distill::distill_article:
    self_contained: false
    toc: true
preview: images/sparklyr.png
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  eval = FALSE
  )
```

## Highlights

* Databricks Connect v2 now supports running native R code in Spark, via
`pysparklyr`. 

* `sparkxgb` is coming back to life

* `sparklyr` is leaner and... nicer. It now has less package dependencies, 
and it now only supports Spark 2.4 and above

## pysparklyr 0.1.4

`spark_apply()` now works on Databricks Connect v2. The latest `pysparklyr` 
release uses the `rpy2` Python library as the backbone of the integration.

Databricks Connect v2, is based on Spark Connect. At this time, it supports 
Python user-defined functions (UDFs), but not R user-defined functions. 
Using `rpy2` circumvents this limitation. As shown in the diagram, `sparklyr`
sends the the R code to the locally installed `rpy2`, which in turn sends it
to Spark. Then the `rpy2` installed in the remote Databricks cluster will run 
the R code.


```{r, echo=FALSE, eval=TRUE, out.width="600px", fig.cap="R code via rpy2", fig.alt="Diagram that shows how sparklyr transmits the R code via the rpy2 python package, and how Spark uses it to run the R code"}
knitr::include_graphics("images/r-udfs.png")
```

A big advantage of this approach, is that `rpy2` supports Arrow. In fact it
is the recommended Python library to use when integrating [Spark, Arrow and
R](https://arrow.apache.org/docs/python/integration/python_r.html).
This means that the data exchange between the three languages will be much 
faster!

As in its original implementation, schema inferring works, and as with the
original implementation, it has a performance cost. But unlike the original,
this implementation will return a 'columns' specification that you can use
for the next time you run the call.

A full article about this new capability is available here: 
[Run R inside Databricks Connect](https://spark.posit.co/deployment/databricks-connect-udfs.html)
  
## sparklyr 1.8.5

### Fixes

- Fixes quoting issue with `dbplyr` 2.5.0 (#3429)

- Fixes Windows OS identification (#3426)

### Package improvements

- Removes dependency on `tibble`, all calls are now redirected to `dplyr` (#3399)

- Removes dependency on `rapddirs` (#3401): 
  - Backwards compatibility with `sparklyr` 0.5 is no longer needed
  - Replicates selection of cache directory 

- Converts `spark_apply()` to a method (#3418)

## sparkxgb 

- Avoids sending two deprecated parameters to XGBoost. The default arguments in
the R function are NULL, and it will return an error message if the call intends
to use them:

  - Sketch EPS - No longer supported since XGBoost version 1.6
  
  - Timeout Request Updates - No long supported since XGBoost version 1.7

- Modernizes the entire `testthat` suite, it also expands it to provide more
coverage


- Modernizes and expands CI testing. The single CI job is now expanded to three:

  - R package check, with no testing against the three major OS's
  - `testthat` tests against Spark version 3.5 
  - Coverage testing, also against Spark version 3.5
  
- Removes `forge` dependency 

- Improves download, preparation and building of the JAR

- Updates and cleans up the call that sets the Maven package to be used in the
Spark session

- Updates Roxygen and `testthat` versions

- Edgar Ruiz (https://github.com/edgararuiz) will be the new maintainer of this
  package moving forward.


