---
title: "Introducing mall for R...and Python"
description: >
  We are proud to introduce the {mall} project. With {mall}, you can use a 
  local LLM to run NLP operations recursively over a data frame. (sentiment, 
  summarization, translation, etc). {mall} has been simultaneusly released
  to CRAN and PyPi (as an extension to Polars).
  
author:
  - name: Edgar Ruiz
    affiliation: Posit
    affiliation_url: https://www.posit.co/
slug: edgarmallintro
date: 2024-10-30
output:
  distill::distill_article:
    self_contained: false
    toc: true
categories:
  - Python
  - R
  - LLM
  - Polars
  - Natural Language Processing
  - Tabular Data
preview: images/article.png
---

## A bit of history

A few months ago, I worked on a workshop to teach how to use Databricks with R.
And while working on it, I stumbled into custom SQL functions that have the 
["ai_" prefix](https://docs.databricks.com/en/large-language-models/ai-functions.html). 
These functions lets you run NLP against a table by simply using SQL:

```sql
> SELECT ai_analyze_sentiment('I am happy');
  positive

> SELECT ai_analyze_sentiment('I am sad');
  negative
```

It was mind-blowing to find a new way to integrate LLM's in out daily 
work as data analysts. I have been used to the chatting, or code completion. 
Those interfaces focus on helping us with code and development. 
This is the first time I found an integration  that lets you run use 
the **LLM against our actual data!**.

At that time, my first reaction was to try and access those functions via R.
[`dbplyr`](https://github.com/tidyverse/dbplyr) lets you access SQL functions
in R, and it was great to see them work:

```r
orders |>
  head() |> 
  select(o_comment) |> 
  mutate(
    sentiment = ai_analyze_sentiment(o_comment)
    )
#> # Source:   SQL [6 x 2]
#>   o_comment                   sentiment
#>   <chr>                        <chr>    
#> 1 ", pending theodolites …    neutral  
#> 2 "uriously special foxes …   neutral  
#> 3 "sleep. courts after the …  neutral  
#> 4 "ess foxes may sleep …      neutral  
#> 5 "ts wake blithely unusual … mixed    
#> 6 "hins sleep. fluffily …     neutral
```

The downside is that even though accessible through R, we still need
a live connection to Databricks in order to use it. This limits the number
of people that can benefit from this new integration 

As per their documentation, Databricks is using Llama 3.1 70B. This is a great
LLM but, it is really big. Its sheer size puts it out of the 
realm of possibility for most user's machines to handle.

### Enter Ollama

LLM development has been moving at lightning speed. Initially, only LLM's 
available online were viable for daily work. This has raised issues for companies
that are reticent to uploading their data to a system that resides
outside their network or their cloud. Additionally, there are monetary costs, the per-token
charges could make using something such as this "batch scoring" rather expensive.

The ideal solution works on locally in our computers. And for that, you need an
LLM model that is small enough to fit in-memory, and that it is accurate enough
for it to viable. Additionally, you will need a way to easily interact with the
local model itself. A bit over a year ago, that last requirement was actually
really hard to accomplish for a casual user.











