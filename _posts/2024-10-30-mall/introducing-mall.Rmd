---
title: "Introducing mall for R...and Python"
description: >
  We are proud to introduce the {mall} project. With {mall}, you can use a 
  local LLM to run NLP operations recursively over a data frame. (sentiment, 
  summarization, translation, etc). {mall} has been simultaneusly released
  to CRAN and PyPi (as an extension to Polars).
  
author:
  - name: Edgar Ruiz
    affiliation: Posit
    affiliation_url: https://www.posit.co/
slug: edgarmallintro
date: 2024-10-30
output:
  distill::distill_article:
    self_contained: false
    toc: true
categories:
  - Python
  - R
  - LLM
  - Polars
  - Natural Language Processing
  - Tabular Data
preview: images/article.png
---

## A bit of history

A few months ago, I worked on a workshop about using Databricks with R.
During that time, I stumbled into custom SQL functions that have the 
["ai_" prefix](https://docs.databricks.com/en/large-language-models/ai-functions.html).
I learned that these functions run NLP with a simple SQL call:

```sql
> SELECT ai_analyze_sentiment('I am happy');
  positive

> SELECT ai_analyze_sentiment('I am sad');
  negative
```

It was mind-blowing to find a new way to integrate LLM's in our daily 
work as data analysts. Historically, we have only used LLM's to help us with
code completion and development. But this was the **first time an integration
helps us by directly interacting with the data!**

The downside is that even though accessible through R, we still need
a live connection to Databricks in order to use it. This limits the number
of people that can benefit from this new integration 

As per their documentation, Databricks is using Llama 3.1 70B. This is a great
LLM but, it is really big. Its sheer size puts it out of the 
realm of possibility for most user's machines to handle.

### Enter Ollama

LLM development has been moving at lightning speed. Initially, only LLM's 
available online were viable for daily work. This has raised issues for companies
that are reticent to uploading their data to a system that resides
outside their network or their cloud. Additionally, there are monetary costs, the per-token
charges could make using something such as this "batch scoring" rather expensive.

The ideal solution works on locally in our computers. And for that, you need an
LLM model that is small enough to fit in-memory, and that it is accurate enough
for it to viable. Additionally, you will need a way to easily interact with the
local model itself. A bit over a year ago, that last requirement was actually
really hard to accomplish for a casual user.











