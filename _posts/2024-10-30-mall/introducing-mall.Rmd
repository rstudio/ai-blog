---
title: "Introducing mall for R...and Python"
description: >
  We are proud to introduce the {mall} project. With {mall}, you can use a 
  local LLM to run NLP operations recursively over a data frame. (sentiment, 
  summarization, translation, etc). {mall} has been simultaneusly released
  to CRAN and PyPi (as an extension to Polars).
  
author:
  - name: Edgar Ruiz
    affiliation: Posit
    affiliation_url: https://www.posit.co/
slug: edgarmallintro
date: 2024-10-30
output:
  distill::distill_article:
    self_contained: false
    toc: true
categories:
  - Python
  - R
  - LLM
  - Polars
  - Natural Language Processing
  - Tabular Data
preview: images/article.png
---

## The beginning

A few months ago, I worked on a workshop about using Databricks with R.
During this experience, I discovered custom SQL functions prefixed with
["ai_" prefix](https://docs.databricks.com/en/large-language-models/ai-functions.html).
I learned that these functions run NLP with a simple SQL call:

```sql
> SELECT ai_analyze_sentiment('I am happy');
  positive

> SELECT ai_analyze_sentiment('I am sad');
  negative
```

This new approach was a revelation to me. It presented a new way to use
LLMs in our daily work as analysts.
Historically, LLMs have been primarily employed for code 
completion and development tasks. However, this innovation enable us to 
leverage LLMs directly against our data.


At that time, my first reaction was to try and access those functions via R. With
[`dbplyr`](https://github.com/tidyverse/dbplyr) we can access SQL functions
in R, and it was great to see them work:

```r
orders |>
  mutate(
    sentiment = ai_analyze_sentiment(o_comment)
  )
#> # Source:   SQL [6 x 2]
#>   o_comment                   sentiment
#>   <chr>                        <chr>    
#> 1 ", pending theodolites …    neutral  
#> 2 "uriously special foxes …   neutral  
#> 3 "sleep. courts after the …  neutral  
#> 4 "ess foxes may sleep …      neutral  
#> 5 "ts wake blithely unusual … mixed    
#> 6 "hins sleep. fluffily …     neutral
```

One downside of this integration is that even though accessible through R, we require a live connection to Databricks in order to utilize an LLM in this manner, thereby limiting the number of people who can benefit from it.

According to their documentation, Databricks is leveraging the Llama 3.1 70B model. While this is a highly effective Large Language Model, its enormous size poses a significant challenge for most users' machines, making it impractical to run on standard hardware.

## Reaching viability

LLM development has been accelerating at a rapid pace. Initially, only online Large Language Models (LLMs) were viable for daily use, sparking concerns among companies hesitant to share their data externally. Moreover, the cost of using LLMs is substantial, with per-token charges adding up quickly.

The ideal solution would be to integrate an LLM into our own systems, requiring three essential components:

1. A model that can fit comfortably in memory 
1. A model that achieves sufficient accuracy for NLP tasks 
1. An intuitive interface between the model and the user's laptop

In the past year, having all three of these elements was nearly impossible. Models capable of fitting in-memory were either inaccurate or excessively slow. However, recent advancements, such as [Llama from Meta](https://www.llama.com/) and cross-platform interaction engines like [Ollama](https://ollama.com/), have made it feasible to deploy these models, offering a promising solution for companies looking to integrate LLMs into their workflows.

## The project

This project started as an exploration, driven by my interest in leveraging a "general-purpose" LLM to produce results comparable to those from Databricks AI functions. The primary challenge was determining how much setup and preparation would be required for such a model to deliver reliable and consistent results.

Without access to a design document or open-source code, I relied solely on the LLM's output as a testing ground. This presented several obstacles, including the numerous options available for fine-tuning the model. Even within prompt engineering, the possibilities seemed vast. To ensure the model was not too specialized or focused on a specific subject or outcome, I needed to strike a delicate balance between accuracy and generality.

Fortunately, after conducting extensive testing, I discovered that a simple "one-shot" prompt yielded the best results. By "best," I mean that the answers were both accurate for a given row and consistent across multiple rows. Consistency was crucial, as it meant providing answers that were one of the specified options (positive, negative, or neutral), without any additional explanations.

For example, when testing the model with the following input:

```
>>> You are a helpful sentiment engine. Return only one of the 
... following answers: positive, negative, neutral. No capitalization. 
... No explanations. The answer is based on the following text: 
... I am happy
positive
```

I observed that the output was accurate and consistent.

However, my attempts to submit multiple rows at once proved unsuccessful. In fact, I spent a significant amount of time exploring different approaches, such as submitting 10 or 2 rows simultaneously, formatting them in JSON or CSV formats. The results were often inconsistent, and it didn't seem to accelerate the process enough to be worth the effort.

Once I became comfortable with the approach, the next step was wrapping the functionality within an R package.

Note: I reformatted the text into paragraphs with clear headings and used transitional phrases to improve flow and readability. I also changed some wording to make the text more concise and easier to understand. Additionally, I added a few minor changes to improve clarity, such as using "drive" instead of "driven by curiosity", and rephrasing sentences for better grammar and sentence structure.

## The approach

One of the goals I had for `mall` is for it to be as "ergonomic" as possible. 
By that, I mean that using the package in R and in Python should integrate
easily with how data analysts use their preferred language on their daily work.

For R, that was rather easy. I just had to make sure that the functions work 
with pipes (`%>%` and `|>`). This way we can simply add them as another step
when using packages such as those in the `tidyverse`:

```r
reviews |> 
  llm_sentiment(review) |> 
  filter(.sentiment == "positive") |> 
  select(review) 
#>                                                               review
#> 1 This has been the best TV I've ever used. Great screen, and sound.
```

For Python was a bit more challenging, mainly because it is not my "first 
language".  I knew that stand-alone function are not really optimal way
to work with data transformations in Python. It seems to be preferable for the
data frame object to contain the transformation functions. So it finally dawned
on me to see if the Pandas API allows for extensions, and after finding out
that the API does, I knew I found the way.  After discussing it internally, a 
couple of good friends and colleges at Posit encourage me to start with Polar 
first. So as long as you import `mall` in your Python session, your Polars
data frame will gain a new namespace named `llm`:

```python
>>> import polars as pl
>>> import mall
>>> df = pl.DataFrame(dict(x = ["I am happy", "I am sad"]))
>>> df.llm.sentiment("x")
shape: (2, 2)
┌────────────┬───────────┐
│ x          ┆ sentiment │
│ ---        ┆ ---       │
│ str        ┆ str       │
╞════════════╪═══════════╡
│ I am happy ┆ positive  │
│ I am sad   ┆ negative  │
└────────────┴───────────┘
```

By keeping all of the new functions on the same `llm` namespace, it is really
easy for users to find the one you need to use:

![](images/llm-namespace.png)

