---
title: "Introducing mall for R...and Python"
description: >
  We are proud to introduce the {mall} project. With {mall}, you can use a 
  local LLM to run NLP operations recursively over a data frame. (sentiment, 
  summarization, translation, etc). {mall} has been simultaneusly released
  to CRAN and PyPi (as an extension to Polars).
  
author:
  - name: Edgar Ruiz
    affiliation: Posit
    affiliation_url: https://www.posit.co/
slug: edgarmallintro
date: 2024-10-30
output:
  distill::distill_article:
    self_contained: false
    toc: true
categories:
  - Python
  - R
  - LLM
  - Polars
  - Natural Language Processing
  - Tabular Data
preview: images/article.png
---

## A bit of history

A few months ago, I worked on a workshop about using Databricks with R.
During that time, I stumbled into custom SQL functions that have the 
["ai_" prefix](https://docs.databricks.com/en/large-language-models/ai-functions.html).
I learned that these functions run NLP with a simple SQL call:

```sql
> SELECT ai_analyze_sentiment('I am happy');
  positive

> SELECT ai_analyze_sentiment('I am sad');
  negative
```

This was mind-blowing, a new way to bring LLM's in our daily 
work as data analysts. Historically, we have only used LLM's to help us with
code completion or development. But this was the **first we can use LLM directly 
against the data!**


At that time, my first reaction was to try and access those functions via R. With
[`dbplyr`](https://github.com/tidyverse/dbplyr) we can access SQL functions
in R, and it was great to see them work:

```r
orders |>
  mutate(
    sentiment = ai_analyze_sentiment(o_comment)
  )
#> # Source:   SQL [6 x 2]
#>   o_comment                   sentiment
#>   <chr>                        <chr>    
#> 1 ", pending theodolites …    neutral  
#> 2 "uriously special foxes …   neutral  
#> 3 "sleep. courts after the …  neutral  
#> 4 "ess foxes may sleep …      neutral  
#> 5 "ts wake blithely unusual … mixed    
#> 6 "hins sleep. fluffily …     neutral
```

The downside is that even though accessible through R, we still need
a live connection to Databricks in order to use an LLM in this manner. Thus 
limiting the number of people that can benefit from this new integration 

As per their documentation, Databricks is using Llama 3.1 70B. This is a great
LLM but, it is really big. Its sheer size puts it out of the 
realm of possibility for most user's machines to handle.

### Reaching viability

LLM development has been moving at lightning speed. In the beginning, only online
LLM's were viable to use in daily work. That raised concerns for companies
that are reticent to sharing their data externally. Additionally, there are 
monetary costs, the per-token charges could easily add-up for batch scoring. 

The ideal solution would be to run the LLM in our computers. And for that, you 
need three things:

1. A model that is small enough to fit in-memory 
1. A model that is is accurate enough to run the NLP tasks
1. An easy-to-use interface between the model and your laptop

As recently as a year ago, having all three was not possible. Models that fit 
in-memory were essentially unusable, they were either inaccurate or extremely
slow. But thanks to recent improvements, such as latest generation LLM's,
such as [Llama from Meta](https://www.llama.com/), and a cross-platform model
interaction engines that are basically plug-and-play such as 
[Ollama](https://ollama.com/).

## Introducing `mall`

This project started with an exploration. First, I wanted to see what if it was 
possible to have a "general purpose" LLM to return results comparable to what we 
get from Databricks AI functions.  The second thing that needed to be determined
it was how much setup and preparation would it take to have the LLM return
reliable and consistent results.  






