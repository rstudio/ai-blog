---
title: "Introducing mall for R...and Python"
description: >
  We are proud to introduce the {mall} project. With {mall}, you can use a 
  local LLM to run NLP operations recursively over a data frame. (sentiment, 
  summarization, translation, etc). {mall} has been simultaneusly released
  to CRAN and PyPi (as an extension to Polars).
  
author:
  - name: Edgar Ruiz
    affiliation: Posit
    affiliation_url: https://www.posit.co/
slug: edgarmallintro
date: 2024-10-30
output:
  distill::distill_article:
    self_contained: false
    toc: true
categories:
  - Python
  - R
  - LLM
  - Polars
  - Natural Language Processing
  - Tabular Data
preview: images/article.png
---

## The beginning

A few months ago, I worked on a workshop about using Databricks with R.
During that time, I stumbled into custom SQL functions that have the 
["ai_" prefix](https://docs.databricks.com/en/large-language-models/ai-functions.html).
I learned that these functions run NLP with a simple SQL call:

```sql
> SELECT ai_analyze_sentiment('I am happy');
  positive

> SELECT ai_analyze_sentiment('I am sad');
  negative
```

This was mind-blowing, a new way to bring LLM's in our daily 
work as data analysts. Historically, we have only used LLM's to help us with
code completion or development. But this was the **first we can use LLM directly 
against the data!**


At that time, my first reaction was to try and access those functions via R. With
[`dbplyr`](https://github.com/tidyverse/dbplyr) we can access SQL functions
in R, and it was great to see them work:

```r
orders |>
  mutate(
    sentiment = ai_analyze_sentiment(o_comment)
  )
#> # Source:   SQL [6 x 2]
#>   o_comment                   sentiment
#>   <chr>                        <chr>    
#> 1 ", pending theodolites …    neutral  
#> 2 "uriously special foxes …   neutral  
#> 3 "sleep. courts after the …  neutral  
#> 4 "ess foxes may sleep …      neutral  
#> 5 "ts wake blithely unusual … mixed    
#> 6 "hins sleep. fluffily …     neutral
```

The downside is that even though accessible through R, we still need
a live connection to Databricks in order to use an LLM in this manner. Thus 
limiting the number of people that can benefit from this new integration 

As per their documentation, Databricks is using *Llama 3.1 70B*. This is a great
LLM but, it is really big. Its sheer size puts it out of the 
realm of possibility for most user's machines to handle.

## Reaching viability

LLM development has been moving at lightning speed. In the beginning, only online
LLM's were viable to use in daily work. That raised concerns for companies
that are reticent to sharing their data externally. Additionally, there are 
monetary costs, the per-token charges could easily add-up for batch scoring. 

The ideal solution would be to run the LLM in our computers. And for that, you 
need three things:

1. A model that is small enough to fit in-memory 
1. A model that is is accurate enough to run  NLP tasks
1. An easy-to-use interface between the model and your laptop

As recently as a year ago, having all three was not possible. Models that fit 
in-memory were essentially unusable, they were either inaccurate or extremely
slow. But thanks to recent improvements, such as latest generation LLM's,
such as [Llama from Meta](https://www.llama.com/), and a cross-platform model
interaction engines that are basically plug-and-play such as 
[Ollama](https://ollama.com/), it was worth trying it out.

## The project

This project started as an exploration. First, I wanted to see what if it was 
possible to have a "general purpose" LLM to return results comparable to what we 
get from Databricks AI functions.  The second thing that needed to be determined
it was how much setup and preparation would it take to have the LLM return
reliable and consistent results.  Without a design document, or open-source
code I could see how Databricks implemented their solution, I had only the
LLM output as a way to test.

There are so many options to have the LLM produce the desired output. We have
prompt engineering, 'RAG', 'embeddings', etc. Even within prompt engineering the 
options are vast. Whatever fine tuning we use, it also needed to be general
enough for it not to be too specialized or focused on a subject, or on obtaining
a specific output. 

Thankfully after some testing, the best results I got were
from a simple 'one-shot' prompt. Again, by best we mean results that are both
accurate for a given row, and also consistent across multiple rows. By 
consistent, I mean to make sure that the answers for a sentiment analysis are
'positive', 'negative' or 'neutral', and not something like 'radish', or to
get a long winded explanation such as: 'the person feels positive'. Here
is an sample of something I tested:

```
>>> You are a helpful sentiment engine. Return only one of the 
... following answers: positive, negative, neutral. No capitalization. 
... No explanations. The answer is based on the following text: 
... I am happy
positive
```

As a side note, I did try testing submitting multiple
rows at a time. In fact, I spent a lot of time trying out
different avenues, such as: sending 10 rows, sending 2 rows, submitting them
in a JSON format, or CSV, etc. The resulting output very inconsistent. 
Additionally, it didn't seem to accelerate the process enough to be worth it.

After getting comfortable with the approach, the rest was a matter of wrapping
the functionality inside an R package.

## The approach

One of the goals I had for `mall` is for it to be as "ergonomic" as possible. 
By that, I mean that using the package in R and in Python should integrate
easily with how data analysts use their preferred language on their daily work.

For R, that was rather easy. I just had to make sure that the functions work 
with pipes (`%>%` and `|>`). This way we can simply add them as another step
when using packages such as those in the `tidyverse`:

```r
reviews |> 
  llm_sentiment(review) |> 
  filter(.sentiment == "positive") |> 
  select(review) 
#>                                                               review
#> 1 This has been the best TV I've ever used. Great screen, and sound.
```

For Python was a bit more challenging, mainly because it is not my "first 
language".  I knew that stand-alone function are not really optimal way
to work with data transformations in Python. It seems to be preferable for the
data frame object to contain the transformation functions. So it finally dawned
on me to see if the Pandas API allows for extensions, and after finding out
that the API does, I knew I found the way.  After discussing it internally, a 
couple of good friends and colleges at Posit encourage me to start with Polar 
first. So as long as you import `mall` in your Python session, your Polars
data frame will gain a new namespace named `llm`:

```python
>>> import polars as pl
>>> import mall
>>> df = pl.DataFrame(dict(x = ["I am happy", "I am sad"]))
>>> df.llm.sentiment("x")
shape: (2, 2)
┌────────────┬───────────┐
│ x          ┆ sentiment │
│ ---        ┆ ---       │
│ str        ┆ str       │
╞════════════╪═══════════╡
│ I am happy ┆ positive  │
│ I am sad   ┆ negative  │
└────────────┴───────────┘
```

By keeping all of the new functions on the same `llm` namespace, it is really
easy for users to find the one you need to use:

![](images/llm-namespace.png)
