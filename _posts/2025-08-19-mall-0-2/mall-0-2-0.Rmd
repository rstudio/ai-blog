---
title: "mall 0.2"
description: >
  The latest version of {mall} is now available in CRAN and PyPi. You can now use
  external LLM providers such as OpenAI, Gemini and Anthropic. 
  Use 
author:
  - name: Edgar Ruiz
    affiliation: Posit
    affiliation_url: https://www.posit.co/
slug: edgarmall02
date: 2025-08-19
output:
  distill::distill_article:
    self_contained: false
    toc: true
categories:
  - Python
  - R
  - LLM
  - Polars
  - Natural Language Processing
  - Tabular Data
preview: images/article.png
---

[mall](https://mlverse.github.io/mall/) uses Large Language Models (LLM) to run 
Natural Language Processing (NLP) operations against your data. This package
is available for both R, and Python. Version 0.2.0 has been released to
[CRAN](https://cran.r-project.org/web/packages/mall/index.html) and 
[PyPi](https://pypi.org/project/mlverse-mall/) respectively.

In R, you can install the latest version with:

```r
install.packages("mall")
```

In Python, with:

``` python
pip install mlverse-mall
```

This release expands the number of LLM providers you can use with {mall}. Also,
in Python it introduces the option to run the NLP operations over string vectors,
and in R, it enables support for parallelized requests. 

## More LLM providers

The biggest highlight of this release is the the ability to use external LLM 
providers such as [OpenAI](https://openai.com/), [Gemini](https://gemini.google.com/) 
and [Anthropic](https://www.anthropic.com/). Instead of writing integration for
each provider one by one, `mall` uses specialized integration packages to act as 
intermediates. 

In R, `mall` uses the [`ellmer`](https://ellmer.tidyverse.org/index.html) package
to integrate with [a variety of LLM providers](https://ellmer.tidyverse.org/reference/index.html#chatbots).
To access the new feature, first create a chat connection, and then pass that 
connection to `llm_use()`. Here is an example of connecting and using OpenAI: 

```r
install.packages("ellmer")

library(mall)
library(ellmer)

chat <- chat_openai()
llm_use(chat)
```

In Python, `mall` uses [`chatlas`](https://posit-dev.github.io/chatlas/) as 
the integration point with the LLM. `chatlas` also integrates with
[several LLM providers](https://posit-dev.github.io/chatlas/reference/#chat-model-providers).
To use, first instantiate a `chatlas` chat connection class, and then pass that
to the [Polars](https://pola.rs/) data frame via the `<DF>.llm.use()` function: 


``` python
pip install chatlas

import mall
from chatlas import ChatOpenAI

chat = ChatOpenAI()

data = mall.MallData
reviews = data.reviews

reviews.llm.use(chat)
```

Connecting `mall` to external LLM providers introduces a consideration of cost.
Most providers charge for the use of their API, so there is a potential that a 
large table, with long texts, could be an expensive operation. 

## Parallel requests (R only)

A new feature introduced in [`ellmer` 0.3.0](https://www.tidyverse.org/blog/2025/07/ellmer-0-3-0)
enables the access to submit multiple prompts in parallel, rather than in sequence.
This makes it faster, and potentially cheaper, to process a table. If the provider
supports this feature, `ellmer` will take automatically advantage of it if we use 
[`parallel_chat()`](https://ellmer.tidyverse.org/reference/parallel_chat.html)
to submit the prompts. Gemini and OpenAI support the feature.

In the new release of `mall`, the integration with `ellmer` has been specially 
written to take advantage of parallel chat. The internals have been re-written to
submit the NLP specific part of the instructions as a system message in order
reduce the size of each prompt. Additionally, the cache system has also been
re-tooled to support batched requests.


