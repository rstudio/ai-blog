<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>RStudio AI Blog</title>
    <link>https://blogs.rstudio.com/tensorflow/</link>
    <atom:link href="https://blogs.rstudio.com/tensorflow/index.xml" rel="self" type="application/rss+xml"/>
    <description>News, concepts, and applications as regards deep learning, probabilistic computation, distributed computing and machine learning automation from R.
</description>
    <image>
      <title>RStudio AI Blog</title>
      <url>https://blogs.rstudio.com/tensorflow/images/favicon.png</url>
      <link>https://blogs.rstudio.com/tensorflow/</link>
    </image>
    <generator>Distill</generator>
    <lastBuildDate>Tue, 02 Feb 2021 00:00:00 +0000</lastBuildDate>
    <item>
      <title>An introduction to weather forecasting with deep learning</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-09-01-weather-prediction</link>
      <description>


&lt;p&gt;With all that is going on in the world these days, is it frivolous to talk about weather prediction? Asked in the 21st century, this is bound to be a rhetorical question. In the 1930s, when German poet Bertolt Brecht wrote the famous lines:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Was sind das für Zeiten, wo&lt;br /&gt;
Ein Gespräch über Bäume fast ein Verbrechen ist&lt;br /&gt;
Weil es ein Schweigen über so viele Untaten einschließt!&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;(“What kind of times are these, where a conversation about trees is almost a crime, for it means silence about so many atrocities!”),&lt;/p&gt;
&lt;p&gt;he couldn’t have anticipated the responses he would get in the second half of that century, with trees symbolizing, as well as literally falling victim to, environmental pollution and climate change.&lt;/p&gt;
&lt;p&gt;Today, no lengthy justification is needed as to why prediction of atmospheric states is vital: Due to global warming, frequency and intensity of severe weather conditions – droughts, wildfires, hurricanes, heatwaves – have risen and will continue to rise. And while accurate forecasts don’t change those events per se, they constitute essential information in mitigating their consequences. This goes for atmospheric forecasts on all scales: from so-called “nowcasting” (operating on a range of about six hours), over medium-range (three to five days) and sub-seasonal (weekly/monthly), to climate forecasts (concerned with years and decades). Medium-range forecasts especially are extremely important in acute disaster prevention.&lt;/p&gt;
&lt;p&gt;This post will show how deep learning (DL) methods can be used to generate atmospheric forecasts, using a newly published &lt;a href="https://github.com/pangeo-data/WeatherBench"&gt;benchmark dataset&lt;/a&gt;&lt;span class="citation"&gt;(Rasp et al. 2020)&lt;/span&gt;. Future posts may refine the model used here and/or discuss the role of DL (“AI”) in mitigating climate change – and its implications – more globally.&lt;/p&gt;
&lt;p&gt;That said, let’s put the current endeavor in context. In a way, we have here the usual &lt;em&gt;dejà vu&lt;/em&gt; of using DL as a black-box-like, magic instrument on a task where &lt;em&gt;human knowledge&lt;/em&gt; used to be required. Of course, this characterization is overly dichotomizing; many choices are made in creating DL models, and performance is necessarily constrained by available algorithms – which may, or may not, fit the domain to be modeled to a sufficient degree.&lt;/p&gt;
&lt;p&gt;If you’ve started learning about image recognition rather recently, you may well have been using DL methods from the outset, and not have heard much about the rich set of feature engineering methods developed in pre-DL image recognition. In the context of atmospheric prediction, then, let’s begin by asking: How in the world did they do that &lt;em&gt;before&lt;/em&gt;?&lt;/p&gt;
&lt;h2 id="numerical-weather-prediction-in-a-nutshell"&gt;Numerical weather prediction in a nutshell&lt;/h2&gt;
&lt;p&gt;It is not like machine learning and/or statistics are not already used in numerical weather prediction – on the contrary. For example, every model has to start from somewhere; but raw observations are not suited to direct use as initial conditions. Instead, they have to be assimilated to the four-dimensional &lt;em&gt;grid&lt;/em&gt;&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; over which model computations are performed. At the other end, namely, model output, statistical post-processing is used to refine the predictions. And very importantly, ensemble forecasts are employed to determine uncertainty.&lt;/p&gt;
&lt;p&gt;That said, the model &lt;em&gt;core&lt;/em&gt;, the part that extrapolates into the future atmospheric conditions observed today, is based on a set of differential&lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; equations, the so-called &lt;a href="https://en.wikipedia.org/wiki/Primitive_equations"&gt;primitive equations&lt;/a&gt;, that are due to the conservation laws of &lt;a href="https://en.wikipedia.org/wiki/Conservation_of_momentum"&gt;momentum&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Conservation_of_energy"&gt;energy&lt;/a&gt;, and &lt;a href="https://en.wikipedia.org/wiki/Conservation_of_mass"&gt;mass&lt;/a&gt;. These differential equations cannot be solved analytically; rather, they have to be solved numerically, and that on a grid of resolution as high as possible. In that light, even deep learning could appear as just “moderately resource-intensive” (dependent, though, on the model in question). So how, then, could a DL approach look?&lt;/p&gt;
&lt;h2 id="deep-learning-models-for-weather-prediction"&gt;Deep learning models for weather prediction&lt;/h2&gt;
&lt;p&gt;Accompanying the benchmark dataset they created, Rasp et al.&lt;span class="citation"&gt;(Rasp et al. 2020)&lt;/span&gt; provide a set of notebooks, including one demonstrating the use of a simple convolutional neural network to predict two of the available atmospheric variables, &lt;em&gt;500hPa geopotential&lt;/em&gt; and &lt;em&gt;850hPa temperature&lt;/em&gt;. Here &lt;em&gt;850hPa temperature&lt;/em&gt; is the (spatially varying) temperature at a fix atmospheric height of 850hPa (~ 1.5 kms)&lt;a href="#fn4" class="footnote-ref" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; ; &lt;em&gt;500hPa geopotential&lt;/em&gt; is proportional to the (again, spatially varying) altitude associated with the pressure level in question (500hPa).&lt;/p&gt;
&lt;p&gt;For this task, two-dimensional convnets, as usually employed in image processing, are a natural fit: Image width and height map to longitude and latitude of the spatial grid, respectively; target variables appear as channels. In this architecture, the time series character of the data is essentially lost: Every sample stands alone, without dependency on either past or present. In this respect, as well as given its size and simplicity, the convnet presented below is only a toy model, meant to introduce the approach as well as the application overall. It may also serve as a &lt;em&gt;deep learning baseline&lt;/em&gt;, along with two other types of baseline commonly used in numerical weather prediction introduced below.&lt;/p&gt;
&lt;p&gt;Directions on how to improve on that baseline are given by recent publications. Weyn et al.&lt;span class="citation"&gt;(Weyn, Durran, and Caruana, n.d.)&lt;/span&gt;, in addition to applying more geometrically-adequate spatial preprocessing, use a U-Net-based architecture instead of a plain convnet. Rasp and Thuerey &lt;span class="citation"&gt;(Rasp and Thuerey 2020)&lt;/span&gt;, building on a fully convolutional, high-capacity ResNet architecture, add a key new procedural ingredient: pre-training on climate models. With their method, they are able to not just compete with physical models, but also, show evidence of the network learning about physical structure and dependencies. Unfortunately, compute facilities of this order are not available to the average individual, which is why we’ll content ourselves with demonstrating a simple toy model. Still, having seen a simple model in action, as well as the type of data it works on, should help a lot in understanding how DL can be used for weather prediction.&lt;/p&gt;
&lt;h2 id="dataset"&gt;Dataset&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/pangeo-data/WeatherBench"&gt;Weatherbench&lt;/a&gt; was explicitly created as a benchmark dataset and thus, as is common for this species, hides a lot of preprocessing and standardization effort from the user. Atmospheric data are available on an hourly basis, ranging from 1979 to 2018, at different spatial resolutions. Depending on resolution, there are about 15 to 20 measured variables, including temperature, geopotential, wind speed, and humidity. Of these variables, some are available at several pressure levels. Thus, our example makes use of a small subset of available “channels”. To save storage, network and computational resources, it also operates at the smallest available resolution.&lt;/p&gt;
&lt;p&gt;This post is accompanied by executable code on &lt;a href="https://colab.research.google.com/drive/1URSjWfcnhgesHWCqd7Un5GHECYblY04r?usp=sharing"&gt;Google Colaboratory&lt;/a&gt;, which should not just render unnecessary any copy-pasting of code snippets but also, allow for uncomplicated modification and experimentation.&lt;/p&gt;
&lt;p&gt;To read in and extract the data, stored as &lt;a href="https://www.unidata.ucar.edu/software/netcdf/"&gt;NetCDF&lt;/a&gt; files, we use &lt;a href="https://docs.ropensci.org/tidync/"&gt;tidync&lt;/a&gt;, a high-level package built on top of &lt;a href="https://cran.r-project.org/package=ncdf4"&gt;ncdf4&lt;/a&gt; and &lt;a href="https://cran.r-project.org/package=RNetCDF"&gt;RNetCDF&lt;/a&gt;. Otherwise, availability of the usual “TensorFlow family” as well as a subset of tidyverse packages is assumed.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(reticulate)
library(tensorflow)
library(keras)
library(tfdatasets)
library(tfautograph)

library(dplyr)
library(ggplot2)
library(tidyr)
library(purrr)
library(lubridate)

library(tidync)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As already alluded to, our example makes use of two spatio-temporal series: 500hPa geopotential and 850hPa temperature. The following commands will download and unpack the respective sets of by-year files, for a spatial resolution of 5.625 degrees:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;download.file(&amp;quot;https://dataserv.ub.tum.de/s/m1524895/download?path=%2F5.625deg%2Ftemperature_850&amp;amp;files=temperature_850_5.625deg.zip&amp;quot;,
              &amp;quot;temperature_850_5.625deg.zip&amp;quot;)
unzip(&amp;quot;temperature_850_5.625deg.zip&amp;quot;, exdir = &amp;quot;temperature_850&amp;quot;)

download.file(&amp;quot;https://dataserv.ub.tum.de/s/m1524895/download?path=%2F5.625deg%2Fgeopotential_500&amp;amp;files=geopotential_500_5.625deg.zip&amp;quot;,
              &amp;quot;geopotential_500_5.625deg.zip&amp;quot;)
unzip(&amp;quot;geopotential_500_5.625deg.zip&amp;quot;, exdir = &amp;quot;geopotential_500&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inspecting one of those files’ contents, we see that its data array is structured along three dimensions, longitude (64 different values), latitude (32) and time (8760). The data itself is &lt;code&gt;z&lt;/code&gt;, the geopotential.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;tidync(&amp;quot;geopotential_500/geopotential_500hPa_2015_5.625deg.nc&amp;quot;) %&amp;gt;% hyper_array()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Class: tidync_data (list of tidync data arrays)
Variables (1): &amp;#39;z&amp;#39;
Dimension (3): lon,lat,time (64, 32, 8760)
Source: /[...]/geopotential_500/geopotential_500hPa_2015_5.625deg.nc&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Extraction of the data array is as easy as telling &lt;code&gt;tidync&lt;/code&gt; to read the first in the list of arrays:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;z500_2015 &amp;lt;- (tidync(&amp;quot;geopotential_500/geopotential_500hPa_2015_5.625deg.nc&amp;quot;) %&amp;gt;%
                hyper_array())[[1]]

dim(z500_2015)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 64 32 8760&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While we delegate further introduction to &lt;code&gt;tidync&lt;/code&gt; to a comprehensive &lt;a href="https://ropensci.org/blog/2019/11/05/tidync/"&gt;blog post&lt;/a&gt; on the ROpenSci website, let’s at least look at a quick visualization, for which we pick the very first time point. (Extraction and visualization code is analogous for 850hPa temperature.)&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;image(z500_2015[ , , 1],
      col = hcl.colors(20, &amp;quot;viridis&amp;quot;), # for temperature, the color scheme used is YlOrRd 
      xaxt = &amp;#39;n&amp;#39;,
      yaxt = &amp;#39;n&amp;#39;,
      main = &amp;quot;500hPa geopotential&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The maps show how pressure&lt;a href="#fn5" class="footnote-ref" id="fnref5"&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; and temperature strongly depend on latitude. Furthermore, it’s easy to spot the &lt;a href="https://en.wikipedia.org/wiki/Atmospheric_wave"&gt;atmospheric waves&lt;/a&gt;:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-09-01-weather-prediction/images/viz.png" alt="Spatial distribution of 500hPa geopotential and 850 hPa temperature for 2015/01/01 0:00h." width="726" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-6)Spatial distribution of 500hPa geopotential and 850 hPa temperature for 2015/01/01 0:00h.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;For training, validation and testing, we choose consecutive years: 2015, 2016, and 2017, respectively.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;z500_train &amp;lt;- (tidync(&amp;quot;geopotential_500/geopotential_500hPa_2015_5.625deg.nc&amp;quot;) %&amp;gt;% hyper_array())[[1]]

t850_train &amp;lt;- (tidync(&amp;quot;temperature_850/temperature_850hPa_2015_5.625deg.nc&amp;quot;) %&amp;gt;% hyper_array())[[1]]

z500_valid &amp;lt;- (tidync(&amp;quot;geopotential_500/geopotential_500hPa_2016_5.625deg.nc&amp;quot;) %&amp;gt;% hyper_array())[[1]]

t850_valid &amp;lt;- (tidync(&amp;quot;temperature_850/temperature_850hPa_2016_5.625deg.nc&amp;quot;) %&amp;gt;% hyper_array())[[1]]

z500_test &amp;lt;- (tidync(&amp;quot;geopotential_500/geopotential_500hPa_2017_5.625deg.nc&amp;quot;) %&amp;gt;% hyper_array())[[1]]

t850_test &amp;lt;- (tidync(&amp;quot;temperature_850/temperature_850hPa_2017_5.625deg.nc&amp;quot;) %&amp;gt;% hyper_array())[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since geopotential and temperature will be treated as channels, we concatenate the corresponding arrays. To transform the data into the format needed for images, a permutation is necessary:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_all &amp;lt;- abind::abind(z500_train, t850_train, along = 4)
train_all &amp;lt;- aperm(train_all, perm = c(3, 2, 1, 4))
dim(train_all)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 8760 32 64 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All data will be standardized according to mean and standard deviation as obtained from the training set:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;level_means &amp;lt;- apply(train_all, 4, mean)
level_sds &amp;lt;- apply(train_all, 4, sd)

round(level_means, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;54124.91  274.8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In words, the mean geopotential height (see footnote 5 for more on this term), as measured at an isobaric surface of 500hPa, amounts to about 5400 metres&lt;a href="#fn6" class="footnote-ref" id="fnref6"&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;, while the mean temperature at the 850hPa level approximates 275 Kelvin (about 2 degrees Celsius).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train &amp;lt;- train_all
train[, , , 1] &amp;lt;- (train[, , , 1] - level_means[1]) / level_sds[1]
train[, , , 2] &amp;lt;- (train[, , , 2] - level_means[2]) / level_sds[2]

valid_all &amp;lt;- abind::abind(z500_valid, t850_valid, along = 4)
valid_all &amp;lt;- aperm(valid_all, perm = c(3, 2, 1, 4))

valid &amp;lt;- valid_all
valid[, , , 1] &amp;lt;- (valid[, , , 1] - level_means[1]) / level_sds[1]
valid[, , , 2] &amp;lt;- (valid[, , , 2] - level_means[2]) / level_sds[2]

test_all &amp;lt;- abind::abind(z500_test, t850_test, along = 4)
test_all &amp;lt;- aperm(test_all, perm = c(3, 2, 1, 4))

test &amp;lt;- test_all
test[, , , 1] &amp;lt;- (test[, , , 1] - level_means[1]) / level_sds[1]
test[, , , 2] &amp;lt;- (test[, , , 2] - level_means[2]) / level_sds[2]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll attempt to predict three days ahead.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;lead_time &amp;lt;- 3 * 24 # 3d&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now all that remains to be done is construct the actual &lt;em&gt;datasets&lt;/em&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;batch_size &amp;lt;- 32

train_x &amp;lt;- train %&amp;gt;%
  tensor_slices_dataset() %&amp;gt;%
  dataset_take(dim(train)[1] - lead_time)

train_y &amp;lt;- train %&amp;gt;%
  tensor_slices_dataset() %&amp;gt;%
  dataset_skip(lead_time)

train_ds &amp;lt;- zip_datasets(train_x, train_y) %&amp;gt;%
  dataset_shuffle(buffer_size = dim(train)[1] - lead_time) %&amp;gt;%
  dataset_batch(batch_size = batch_size, drop_remainder = TRUE)

valid_x &amp;lt;- valid %&amp;gt;%
  tensor_slices_dataset() %&amp;gt;%
  dataset_take(dim(valid)[1] - lead_time)

valid_y &amp;lt;- valid %&amp;gt;%
  tensor_slices_dataset() %&amp;gt;%
  dataset_skip(lead_time)

valid_ds &amp;lt;- zip_datasets(valid_x, valid_y) %&amp;gt;%
  dataset_batch(batch_size = batch_size, drop_remainder = TRUE)

test_x &amp;lt;- test %&amp;gt;%
  tensor_slices_dataset() %&amp;gt;%
  dataset_take(dim(test)[1] - lead_time)

test_y &amp;lt;- test %&amp;gt;%
  tensor_slices_dataset() %&amp;gt;%
  dataset_skip(lead_time)

test_ds &amp;lt;- zip_datasets(test_x, test_y) %&amp;gt;%
  dataset_batch(batch_size = batch_size, drop_remainder = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s proceed to defining the model.&lt;/p&gt;
&lt;h2 id="basic-cnn-with-periodic-convolutions"&gt;Basic CNN with periodic convolutions&lt;/h2&gt;
&lt;p&gt;The model is a straightforward convnet, with one exception: Instead of plain convolutions, it uses slightly more sophisticated ones that “wrap around” longitudinally.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;periodic_padding_2d &amp;lt;- function(pad_width,
                                name = NULL) {
  
  keras_model_custom(name = name, function(self) {
    self$pad_width &amp;lt;- pad_width
    
    function (x, mask = NULL) {
      x &amp;lt;- if (self$pad_width == 0) {
        x
      } else {
        lon_dim &amp;lt;- dim(x)[3]
        pad_width &amp;lt;- tf$cast(self$pad_width, tf$int32)
        # wrap around for longitude
        tf$concat(list(x[, ,-pad_width:lon_dim,],
                       x,
                       x[, , 1:pad_width,]),
                  axis = 2L) %&amp;gt;%
          tf$pad(list(
            list(0L, 0L),
            # zero-pad for latitude
            list(pad_width, pad_width),
            list(0L, 0L),
            list(0L, 0L)
          ))
      }
    }
  })
}

periodic_conv_2d &amp;lt;- function(filters,
                             kernel_size,
                             name = NULL) {
  
  keras_model_custom(name = name, function(self) {
    self$padding &amp;lt;- periodic_padding_2d(pad_width = (kernel_size - 1) / 2)
    self$conv &amp;lt;-
      layer_conv_2d(filters = filters,
                    kernel_size = kernel_size,
                    padding = &amp;#39;valid&amp;#39;)
    
    function (x, mask = NULL) {
      x %&amp;gt;% self$padding() %&amp;gt;% self$conv()
    }
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For our purposes of establishing a deep-learning baseline that is fast to train, CNN architecture and parameter defaults are chosen to be simple and moderate, respectively:&lt;a href="#fn7" class="footnote-ref" id="fnref7"&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;periodic_cnn &amp;lt;- function(filters = c(64, 64, 64, 64, 2),
                         kernel_size = c(5, 5, 5, 5, 5),
                         dropout = rep(0.2, 5),
                         name = NULL) {
  
  keras_model_custom(name = name, function(self) {
    
    self$conv1 &amp;lt;-
      periodic_conv_2d(filters = filters[1], kernel_size = kernel_size[1])
    self$act1 &amp;lt;- layer_activation_leaky_relu()
    self$drop1 &amp;lt;- layer_dropout(rate = dropout[1])
    self$conv2 &amp;lt;-
      periodic_conv_2d(filters = filters[2], kernel_size = kernel_size[2])
    self$act2 &amp;lt;- layer_activation_leaky_relu()
    self$drop2 &amp;lt;- layer_dropout(rate =dropout[2])
    self$conv3 &amp;lt;-
      periodic_conv_2d(filters = filters[3], kernel_size = kernel_size[3])
    self$act3 &amp;lt;- layer_activation_leaky_relu()
    self$drop3 &amp;lt;- layer_dropout(rate = dropout[3])
    self$conv4 &amp;lt;-
      periodic_conv_2d(filters = filters[4], kernel_size = kernel_size[4])
    self$act4 &amp;lt;- layer_activation_leaky_relu()
    self$drop4 &amp;lt;- layer_dropout(rate = dropout[4])
    self$conv5 &amp;lt;-
      periodic_conv_2d(filters = filters[5], kernel_size = kernel_size[5])
    
    function (x, mask = NULL) {
      x %&amp;gt;%
        self$conv1() %&amp;gt;%
        self$act1() %&amp;gt;%
        self$drop1() %&amp;gt;%
        self$conv2() %&amp;gt;%
        self$act2() %&amp;gt;%
        self$drop2() %&amp;gt;%
        self$conv3() %&amp;gt;%
        self$act3() %&amp;gt;%
        self$drop3() %&amp;gt;%
        self$conv4() %&amp;gt;%
        self$act4() %&amp;gt;%
        self$drop4() %&amp;gt;%
        self$conv5()
    }
  })
}

model &amp;lt;- periodic_cnn()&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="training"&gt;Training&lt;/h2&gt;
&lt;p&gt;In that same spirit of “default-ness”, we train with MSE loss and Adam optimizer.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;loss &amp;lt;- tf$keras$losses$MeanSquaredError(reduction = tf$keras$losses$Reduction$SUM)
optimizer &amp;lt;- optimizer_adam()

train_loss &amp;lt;- tf$keras$metrics$Mean(name=&amp;#39;train_loss&amp;#39;)

valid_loss &amp;lt;- tf$keras$metrics$Mean(name=&amp;#39;test_loss&amp;#39;)

train_step &amp;lt;- function(train_batch) {

  with (tf$GradientTape() %as% tape, {
    predictions &amp;lt;- model(train_batch[[1]])
    l &amp;lt;- loss(train_batch[[2]], predictions)
  })

  gradients &amp;lt;- tape$gradient(l, model$trainable_variables)
  optimizer$apply_gradients(purrr::transpose(list(
    gradients, model$trainable_variables
  )))

  train_loss(l)

}

valid_step &amp;lt;- function(valid_batch) {
  predictions &amp;lt;- model(valid_batch[[1]])
  l &amp;lt;- loss(valid_batch[[2]], predictions)
  
  valid_loss(l)
}

training_loop &amp;lt;- tf_function(autograph(function(train_ds, valid_ds, epoch) {
  
  for (train_batch in train_ds) {
    train_step(train_batch)
  }
  
  for (valid_batch in valid_ds) {
    valid_step(valid_batch)
  }
  
  tf$print(&amp;quot;MSE: train: &amp;quot;, train_loss$result(), &amp;quot;, validation: &amp;quot;, valid_loss$result()) 
    
}))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Depicted graphically, we see that the model trains well, but extrapolation does not surpass a certain threshold (which is reached early, after training for just two epochs).&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-09-01-weather-prediction/images/history.png" alt="MSE per epoch on training and validation sets." width="363" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-16)MSE per epoch on training and validation sets.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This is not too surprising though, given the model’s architectural simplicity and modest size.&lt;/p&gt;
&lt;h2 id="evaluation"&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;Here, we first present two other baselines, which – given a highly complex and chaotic system like the atmosphere – may sound irritatingly simple and yet, be pretty hard to beat. The metric used for comparison is &lt;em&gt;latitudinally weighted root-mean-square error&lt;/em&gt;. Latitudinal weighting up-weights the lower latitudes and down-weights the upper ones.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;deg2rad &amp;lt;- function(d) {
  (d / 180) * pi
}

lats &amp;lt;- tidync(&amp;quot;geopotential_500/geopotential_500hPa_2015_5.625deg.nc&amp;quot;)$transforms$lat %&amp;gt;%
  select(lat) %&amp;gt;%
  pull()

lat_weights &amp;lt;- cos(deg2rad(lats))
lat_weights &amp;lt;- lat_weights / mean(lat_weights)

weighted_rmse &amp;lt;- function(forecast, ground_truth) {
  error &amp;lt;- (forecast - ground_truth) ^ 2
  for (i in seq_along(lat_weights)) {
    error[, i, ,] &amp;lt;- error[, i, ,] * lat_weights[i]
  }
  apply(error, 4, mean) %&amp;gt;% sqrt()
}&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="baseline-1-weekly-climatology"&gt;Baseline 1: Weekly climatology&lt;/h4&gt;
&lt;p&gt;In general, climatology refers to long-term averages computed over defined time ranges. Here, we first calculate weekly averages based on the training set. These averages are then used to forecast the variables in question for the time period used as test set.&lt;/p&gt;
&lt;p&gt;Step one makes use of &lt;code&gt;tidync&lt;/code&gt;, &lt;code&gt;ncmeta&lt;/code&gt;, &lt;code&gt;RNetCDF&lt;/code&gt; and &lt;code&gt;lubridate&lt;/code&gt; to compute weekly averages for 2015, following the &lt;a href="https://en.wikipedia.org/wiki/ISO_week_date"&gt;ISO week date system&lt;/a&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_file &amp;lt;- &amp;quot;geopotential_500/geopotential_500hPa_2015_5.625deg.nc&amp;quot;

times_train &amp;lt;- (tidync(train_file) %&amp;gt;% activate(&amp;quot;D2&amp;quot;) %&amp;gt;% hyper_array())$time

time_unit_train &amp;lt;- ncmeta::nc_atts(train_file, &amp;quot;time&amp;quot;) %&amp;gt;%
  tidyr::unnest(cols = c(value)) %&amp;gt;%
  dplyr::filter(name == &amp;quot;units&amp;quot;)

time_parts_train &amp;lt;- RNetCDF::utcal.nc(time_unit_train$value, times_train)

iso_train &amp;lt;- ISOdate(
  time_parts_train[, &amp;quot;year&amp;quot;],
  time_parts_train[, &amp;quot;month&amp;quot;],
  time_parts_train[, &amp;quot;day&amp;quot;],
  time_parts_train[, &amp;quot;hour&amp;quot;],
  time_parts_train[, &amp;quot;minute&amp;quot;],
  time_parts_train[, &amp;quot;second&amp;quot;]
)

isoweeks_train &amp;lt;- map(iso_train, isoweek) %&amp;gt;% unlist()

train_by_week &amp;lt;- apply(train_all, c(2, 3, 4), function(x) {
  tapply(x, isoweeks_train, function(y) {
    mean(y)
  })
})

dim(train_by_week)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;53 32 64 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Step two then runs through the test set, mapping dates to corresponding ISO weeks and associating the weekly averages from the training set:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;test_file &amp;lt;- &amp;quot;geopotential_500/geopotential_500hPa_2017_5.625deg.nc&amp;quot;

times_test &amp;lt;- (tidync(test_file) %&amp;gt;% activate(&amp;quot;D2&amp;quot;) %&amp;gt;% hyper_array())$time

time_unit_test &amp;lt;- ncmeta::nc_atts(test_file, &amp;quot;time&amp;quot;) %&amp;gt;%
  tidyr::unnest(cols = c(value)) %&amp;gt;%
  dplyr::filter(name == &amp;quot;units&amp;quot;)

time_parts_test &amp;lt;- RNetCDF::utcal.nc(time_unit_test$value, times_test)

iso_test &amp;lt;- ISOdate(
  time_parts_test[, &amp;quot;year&amp;quot;],
  time_parts_test[, &amp;quot;month&amp;quot;],
  time_parts_test[, &amp;quot;day&amp;quot;],
  time_parts_test[, &amp;quot;hour&amp;quot;],
  time_parts_test[, &amp;quot;minute&amp;quot;],
  time_parts_test[, &amp;quot;second&amp;quot;]
)

isoweeks_test &amp;lt;- map(iso_test, isoweek) %&amp;gt;% unlist()

climatology_forecast &amp;lt;- test_all

for (i in 1:dim(climatology_forecast)[1]) {
  week &amp;lt;- isoweeks_test[i]
  lookup &amp;lt;- train_by_week[week, , , ]
  climatology_forecast[i, , ,] &amp;lt;- lookup
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this baseline, the latitudinally-weighted RMSE amounts to roughly 975 for geopotential and 4 for temperature.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;wrmse &amp;lt;- weighted_rmse(climatology_forecast, test_all)
round(wrmse, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;974.50   4.09&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="baseline-2-persistence-forecast"&gt;Baseline 2: Persistence forecast&lt;/h4&gt;
&lt;p&gt;The second baseline commonly used makes a straightforward assumption: Tomorrow’s weather is today’s weather, or, in our case: In three days, things will be just like they are now.&lt;/p&gt;
&lt;p&gt;Computation for this metric is almost a one-liner. And as it turns out, for the given lead time (three days), performance is not too dissimilar from obtained by means of weekly climatology:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;persistence_forecast &amp;lt;- test_all[1:(dim(test_all)[1] - lead_time), , ,]

test_period &amp;lt;- test_all[(lead_time + 1):dim(test_all)[1], , ,]

wrmse &amp;lt;- weighted_rmse(persistence_forecast, test_period)

round(wrmse, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;937.55  4.31&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="baseline-3-simple-convnet"&gt;Baseline 3: Simple convnet&lt;/h4&gt;
&lt;p&gt;How does the simple deep learning model stack up against those two?&lt;/p&gt;
&lt;p&gt;To answer that question, we first need to obtain predictions on the test set.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;test_wrmses &amp;lt;- data.frame()

test_loss &amp;lt;- tf$keras$metrics$Mean(name = &amp;#39;test_loss&amp;#39;)

test_step &amp;lt;- function(test_batch, batch_index) {
  predictions &amp;lt;- model(test_batch[[1]])
  l &amp;lt;- loss(test_batch[[2]], predictions)
  
  predictions &amp;lt;- predictions %&amp;gt;% as.array()
  predictions[, , , 1] &amp;lt;- predictions[, , , 1] * level_sds[1] + level_means[1]
  predictions[, , , 2] &amp;lt;- predictions[, , , 2] * level_sds[2] + level_means[2]
  
  wrmse &amp;lt;- weighted_rmse(predictions, test_all[batch_index:(batch_index + 31), , ,])
  test_wrmses &amp;lt;&amp;lt;- test_wrmses %&amp;gt;% bind_rows(c(z = wrmse[1], temp = wrmse[2]))

  test_loss(l)
}

test_iterator &amp;lt;- as_iterator(test_ds)

batch_index &amp;lt;- 0
while (TRUE) {
  test_batch &amp;lt;- test_iterator %&amp;gt;% iter_next()
  if (is.null(test_batch))
    break
  batch_index &amp;lt;- batch_index + 1
  test_step(test_batch, as.integer(batch_index))
}

test_loss$result() %&amp;gt;% as.numeric()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;3821.016&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus, average loss on the test set parallels that seen on the validation set. As to latitudinally weighted RMSE, it turns out to be higher for the DL baseline than for the other two:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;apply(test_wrmses, 2, mean) %&amp;gt;% round(2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      z    temp 
1521.47    7.70 &lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;At first glance, seeing the DL baseline perform worse than the others might feel anticlimactic. But if you think about it, there is no need to be disappointed.&lt;/p&gt;
&lt;p&gt;For one, given the enormous complexity of the task, these heuristics are not as easy to outsmart. Take persistence: Depending on lead time - how far into the future we’re forecasting - the wisest guess may actually be that everything will stay the same. What would you guess the weather will look like in five minutes? — Same with weekly climatology: Looking back at how warm it was, at a given location, that same week two years ago, does not in general sound like a bad strategy.&lt;/p&gt;
&lt;p&gt;Second, the DL baseline shown is as basic as it can get, architecture- as well as parameter-wise. More sophisticated and powerful architectures have been developed that not just by far surpass the baselines, but can even compete with physical models (cf. especially Rasp and Thuerey &lt;span class="citation"&gt;(Rasp and Thuerey 2020)&lt;/span&gt; already mentioned above). Unfortunately, models like that need to be trained on &lt;em&gt;a lot&lt;/em&gt; of data.&lt;/p&gt;
&lt;p&gt;However, other weather-related applications (other than medium-range forecasting, that is) may be more in reach for individuals interested in the topic. For those, we hope we have given a useful introduction. Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-weatherbench"&gt;
&lt;p&gt;Rasp, Stephan, Peter D. Dueben, Sebastian Scher, Jonathan A. Weyn, Soukayna Mouatadid, and Nils Thuerey. 2020. “WeatherBench: A benchmark dataset for data-driven weather forecasting.” &lt;em&gt;arXiv E-Prints&lt;/em&gt;, February, arXiv:2002.00469. &lt;a href="http://arxiv.org/abs/2002.00469"&gt;http://arxiv.org/abs/2002.00469&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-rasp2020purely"&gt;
&lt;p&gt;Rasp, Stephan, and Nils Thuerey. 2020. “Purely Data-Driven Medium-Range Weather Forecasting Achieves Comparable Skill to Physical Models at Similar Resolution.” &lt;a href="http://arxiv.org/abs/2008.08626"&gt;http://arxiv.org/abs/2008.08626&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-weyn"&gt;
&lt;p&gt;Weyn, Jonathan A., Dale R. Durran, and Rich Caruana. n.d. “Improving Data-Driven Global Weather Prediction Using Deep Convolutional Neural Networks on a Cubed Sphere.” &lt;em&gt;Journal of Advances in Modeling Earth Systems&lt;/em&gt; n/a (n/a): e2020MS002109. &lt;a href="https://doi.org/10.1029/2020MS002109"&gt;https://doi.org/10.1029/2020MS002109&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;&lt;em&gt;An die Nachgeborenen, 1934-38.&lt;/em&gt; The atrocities referred to are those of Nazi Germany.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;Four, because in addition to three spatial dimensions, there is the time dimension.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;mostly&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;Pressure and altitude are related by the &lt;a href="https://en.wikipedia.org/wiki/Barometric_formula"&gt;barometric equation&lt;/a&gt;. On weather maps, pressure is often used to represent the vertical dimension.&lt;a href="#fnref4" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn5"&gt;&lt;p&gt;Whereas we normally might think of atmospheric pressure as varying at a fixed height (for example, at sea level), meteorologists like to display things the other way round, displaying variable heights at fixed constant-pressure (isobaric) surfaces. Still, intuitively these may be read in the same way: High pressure at a given location means that some predefined pressure level is attained at &lt;em&gt;higher altitude&lt;/em&gt; than in some low-pressure location. To be precise, these kinds of “inverted pressure maps” normally display &lt;em&gt;geopotential height&lt;/em&gt;, measured in metres, not &lt;em&gt;geopotential&lt;/em&gt;, the variable we’re dealing with here. &lt;em&gt;Geopotential&lt;/em&gt; (without the “height”) refers to gravitational potential energy per unit mass; it is obtained by multiplying by gravitational acceleration, and is measured in metres squared per second squared. The measures are not a hundred percent equivalent, because gravitational acceleration varies with latitude and longitude as well as elevation.&lt;a href="#fnref5" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn6"&gt;&lt;p&gt;As explained in the previous footnote, geopotential height is geopotential divided by standard gravitational acceleration, roughly, 9.8 metres per seconds squared.&lt;a href="#fnref6" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn7"&gt;&lt;p&gt;These are the same filter and kernel sizes as employed in &lt;a href="https://github.com/pangeo-data/WeatherBench/blob/master/notebooks/3-cnn-example.ipynb"&gt;Rasp et al.'s simple CNN example on github&lt;/a&gt;.&lt;a href="#fnref7" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">47cf15552b82b021f49c79654e45691e</distill:md5>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-09-01-weather-prediction</guid>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-09-01-weather-prediction/images/thumb.png" medium="image" type="image/png" width="600" height="332"/>
    </item>
    <item>
      <title>Training ImageNet with R</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Javier Luraschi</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-08-24-training-imagenet-with-r</link>
      <description>


&lt;p&gt;&lt;a href="http://www.image-net.org/"&gt;ImageNet&lt;/a&gt; &lt;span class="citation"&gt;(Deng et al. 2009)&lt;/span&gt; is an image database organized according to the &lt;a href="http://wordnet.princeton.edu/"&gt;WordNet&lt;/a&gt; &lt;span class="citation"&gt;(Miller 1995)&lt;/span&gt; hierarchy which, historically, has been used in computer vision benchmarks and research. However, it was not until AlexNet &lt;span class="citation"&gt;(Krizhevsky, Sutskever, and Hinton 2012)&lt;/span&gt; demonstrated the efficiency of deep learning using convolutional neural networks on GPUs that the computer-vision discipline turned to deep learning to achieve state-of-the-art models that revolutionized their field. Given the importance of ImageNet and AlexNet, this post introduces tools and techniques to consider when training ImageNet and other large-scale datasets with R.&lt;/p&gt;
&lt;p&gt;Now, in order to process ImageNet, we will first have to &lt;em&gt;divide and conquer&lt;/em&gt;, partitioning the dataset into several manageable subsets. Afterwards, we will train ImageNet using AlexNet across multiple GPUs and compute instances. &lt;a href="#preprocessing-imagenet"&gt;Preprocessing ImageNet&lt;/a&gt; and &lt;a href="#distributed-training"&gt;distributed training&lt;/a&gt; are the two topics that this post will present and discuss, starting with preprocessing ImageNet.&lt;/p&gt;
&lt;h2 id="preprocessing-imagenet"&gt;Preprocessing ImageNet&lt;/h2&gt;
&lt;p&gt;When dealing with large datasets, even simple tasks like downloading or reading a dataset can be much harder than what you would expect. For instance, since ImageNet is roughly 300GB in size, you will need to make sure to have at least 600GB of free space to leave some room for download and decompression. But no worries, you can always borrow computers with huge disk drives from your favorite cloud provider. While you are at it, you should also request compute instances with multiple GPUs, Solid State Drives (SSDs), and a reasonable amount of CPUs and memory. If you want to use the exact configuration we used, take a look at the &lt;a href="https://github.com/mlverse/imagenet"&gt;mlverse/imagenet&lt;/a&gt; repo, which contains a Docker image and configuration commands required to provision reasonable computing resources for this task. In summary, make sure you have access to sufficient compute resources.&lt;/p&gt;
&lt;p&gt;Now that we have resources capable of working with ImageNet, we need to find a place to download ImageNet from. The easiest way is to use a variation of ImageNet used in the &lt;a href="http://www.image-net.org/challenges/LSVRC/"&gt;ImageNet Large Scale Visual Recognition Challenge (ILSVRC)&lt;/a&gt;, which contains a subset of about 250GB of data and can be easily downloaded from many &lt;a href="https://kaggle.com"&gt;Kaggle&lt;/a&gt; competitions, like the &lt;a href="https://www.kaggle.com/c/imagenet-object-localization-challenge"&gt;ImageNet Object Localization Challenge&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you’ve read some of our previous posts, you might be already thinking of using the &lt;a href="https://pins.rstudio.com"&gt;pins&lt;/a&gt; package, which you can use to: cache, discover and share resources from many services, including Kaggle. You can learn more about data retrieval from Kaggle in the &lt;a href="http://pins.rstudio.com/articles/boards-kaggle.html"&gt;Using Kaggle Boards&lt;/a&gt; article; in the meantime, let’s assume you are already familiar with this package.&lt;/p&gt;
&lt;p&gt;All we need to do now is register the Kaggle board, retrieve ImageNet as a pin, and decompress this file. Warning, the following code requires you to stare at a progress bar for, potentially, over an hour.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(pins)
board_register(&amp;quot;kaggle&amp;quot;, token = &amp;quot;kaggle.json&amp;quot;)

pin_get(&amp;quot;c/imagenet-object-localization-challenge&amp;quot;, board = &amp;quot;kaggle&amp;quot;)[1] %&amp;gt;%
  untar(exdir = &amp;quot;/localssd/imagenet/&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we are going to be training this model over and over using multiple GPUs and even multiple compute instances, we want to make sure we don’t waste too much time downloading ImageNet every single time.&lt;/p&gt;
&lt;p&gt;The first improvement to consider is getting a faster hard drive. In our case, we locally-mounted an array of SSDs into the &lt;code&gt;/localssd&lt;/code&gt; path. We then used &lt;code&gt;/localssd&lt;/code&gt; to extract ImageNet and configured R’s temp path and pins cache to use the SSDs as well. Consult your cloud provider’s documentation to configure SSDs, or take a look at &lt;a href="https://github.com/mlverse/imagenet"&gt;mlverse/imagenet&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Next, a well-known approach we can follow is to partition ImageNet into chunks that can be individually downloaded to perform distributed training later on.&lt;/p&gt;
&lt;p&gt;In addition, it is also faster to download ImageNet from a nearby location, ideally from a URL stored within the same data center where our cloud instance is located. For this, we can also use pins to register a board with our cloud provider and then re-upload each partition. Since ImageNet is already partitioned by category, we can easily split ImageNet into multiple zip files and re-upload to our closest data center as follows. Make sure the storage bucket is created in the same region as your computing instances.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;board_register(&amp;quot;&amp;lt;board&amp;gt;&amp;quot;, name = &amp;quot;imagenet&amp;quot;, bucket = &amp;quot;r-imagenet&amp;quot;)

train_path &amp;lt;- &amp;quot;/localssd/imagenet/ILSVRC/Data/CLS-LOC/train/&amp;quot;
for (path in dir(train_path, full.names = TRUE)) {
  dir(path, full.names = TRUE) %&amp;gt;%
    pin(name = basename(path), board = &amp;quot;imagenet&amp;quot;, zip = TRUE)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now retrieve a subset of ImageNet quite efficiently. If you are motivated to do so and have about one gigabyte to spare, feel free to follow along executing this code. Notice that ImageNet contains &lt;em&gt;lots&lt;/em&gt; of JPEG images for each WordNet category.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;board_register(&amp;quot;https://storage.googleapis.com/r-imagenet/&amp;quot;, &amp;quot;imagenet&amp;quot;)

categories &amp;lt;- pin_get(&amp;quot;categories&amp;quot;, board = &amp;quot;imagenet&amp;quot;)
pin_get(categories$id[1], board = &amp;quot;imagenet&amp;quot;, extract = TRUE) %&amp;gt;%
  tibble::as_tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1,300 x 1
   value                                                           
   &amp;lt;chr&amp;gt;                                                           
 1 /localssd/pins/storage/n01440764/n01440764_10026.JPEG
 2 /localssd/pins/storage/n01440764/n01440764_10027.JPEG
 3 /localssd/pins/storage/n01440764/n01440764_10029.JPEG
 4 /localssd/pins/storage/n01440764/n01440764_10040.JPEG
 5 /localssd/pins/storage/n01440764/n01440764_10042.JPEG
 6 /localssd/pins/storage/n01440764/n01440764_10043.JPEG
 7 /localssd/pins/storage/n01440764/n01440764_10048.JPEG
 8 /localssd/pins/storage/n01440764/n01440764_10066.JPEG
 9 /localssd/pins/storage/n01440764/n01440764_10074.JPEG
10 /localssd/pins/storage/n01440764/n01440764_1009.JPEG 
# … with 1,290 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When doing distributed training over ImageNet, we can now let a single compute instance process a partition of ImageNet with ease. Say, 1/16 of ImageNet can be retrieved and extracted, in under a minute, using parallel downloads with the &lt;a href="https://callr.r-lib.org/"&gt;callr&lt;/a&gt; package:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;categories &amp;lt;- pin_get(&amp;quot;categories&amp;quot;, board = &amp;quot;imagenet&amp;quot;)
categories &amp;lt;- categories$id[1:(length(categories$id) / 16)]

procs &amp;lt;- lapply(categories, function(cat)
  callr::r_bg(function(cat) {
    library(pins)
    board_register(&amp;quot;https://storage.googleapis.com/r-imagenet/&amp;quot;, &amp;quot;imagenet&amp;quot;)
    
    pin_get(cat, board = &amp;quot;imagenet&amp;quot;, extract = TRUE)
  }, args = list(cat))
)
  
while (any(sapply(procs, function(p) p$is_alive()))) Sys.sleep(1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can wrap this up partition in a list containing a map of images and categories, which we will later use in our AlexNet model through &lt;a href="https://tensorflow.rstudio.com/guide/tfdatasets/introduction/"&gt;tfdatasets&lt;/a&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;data &amp;lt;- list(
    image = unlist(lapply(categories, function(cat) {
        pin_get(cat, board = &amp;quot;imagenet&amp;quot;, download = FALSE)
    })),
    category = unlist(lapply(categories, function(cat) {
        rep(cat, length(pin_get(cat, board = &amp;quot;imagenet&amp;quot;, download = FALSE)))
    })),
    categories = categories
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great! We are halfway there training ImageNet. The next section will focus on introducing distributed training using multiple GPUs.&lt;/p&gt;
&lt;h2 id="distributed-training"&gt;Distributed Training&lt;/h2&gt;
&lt;p&gt;Now that we have broken down ImageNet into manageable parts, we can forget for a second about the size of ImageNet and focus on training a deep learning model for this dataset. However, any model we choose is likely to require a GPU, even for a 1/16 subset of ImageNet. So make sure your GPUs are properly configured by running &lt;code&gt;is_gpu_available()&lt;/code&gt;. If you need help getting a GPU configured, the &lt;a href="https://www.youtube.com/watch?v=i5Bjm3jG_d8"&gt;Using GPUs with TensorFlow and Docker&lt;/a&gt; video can help you get up to speed.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tensorflow)
tf$test$is_gpu_available()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now decide which deep learning model would best be suited for ImageNet classification tasks. Instead, for this post, we will go back in time to the glory days of AlexNet and use the &lt;a href="https://github.com/r-tensorflow/alexnet"&gt;r-tensorflow/alexnet&lt;/a&gt; repo instead. This repo contains a port of AlexNet to R, but please notice that this port has not been tested and is not ready for any real use cases. In fact, we would appreciate PRs to improve it if someone feels inclined to do so. Regardless, the focus of this post is on workflows and tools, not about achieving state-of-the-art image classification scores. So by all means, feel free to use more appropriate models.&lt;/p&gt;
&lt;p&gt;Once we’ve chosen a model, we will want to me make sure that it properly trains on a subset of ImageNet:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;remotes::install_github(&amp;quot;r-tensorflow/alexnet&amp;quot;)
alexnet::alexnet_train(data = data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch 1/2
 103/2269 [&amp;gt;...............] - ETA: 5:52 - loss: 72306.4531 - accuracy: 0.9748&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So far so good! However, this post is about enabling large-scale training across multiple GPUs, so we want to make sure we are using as many as we can. Unfortunately, running &lt;code&gt;nvidia-smi&lt;/code&gt; will show that only one GPU currently being used:&lt;/p&gt;
&lt;pre class="bash"&gt;&lt;code&gt;nvidia-smi&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.152.00   Driver Version: 418.152.00   CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 00000000:00:05.0 Off |                    0 |
| N/A   48C    P0    89W / 149W |  10935MiB / 11441MiB |     28%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K80           Off  | 00000000:00:06.0 Off |                    0 |
| N/A   74C    P0    74W / 149W |     71MiB / 11441MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to train across multiple GPUs, we need to define a distributed-processing strategy. If this is a new concept, it might be a good time to take a look at the &lt;a href="https://tensorflow.rstudio.com/tutorials/advanced/distributed/distributed_training_with_keras/"&gt;Distributed Training with Keras&lt;/a&gt; tutorial and the &lt;a href="https://www.tensorflow.org/guide/distributed_training"&gt;distributed training with TensorFlow&lt;/a&gt; docs. Or, if you allow us to oversimplify the process, all you have to do is define and compile your model under the right scope. A step-by-step explanation is available in the &lt;a href="https://www.youtube.com/watch?v=DQyLTlD1IBc"&gt;Distributed Deep Learning with TensorFlow and R&lt;/a&gt; video. In this case, the &lt;code&gt;alexnet&lt;/code&gt; model &lt;a href="https://github.com/r-tensorflow/alexnet/blob/57546/R/alexnet_train.R#L92-L94"&gt;already supports&lt;/a&gt; a strategy parameter, so all we have to do is pass it along.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tensorflow)
strategy &amp;lt;- tf$distribute$MirroredStrategy(
  cross_device_ops = tf$distribute$ReductionToOneDevice())

alexnet::alexnet_train(data = data, strategy = strategy, parallel = 6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice also &lt;code&gt;parallel = 6&lt;/code&gt; which configures &lt;code&gt;tfdatasets&lt;/code&gt; to make use of multiple CPUs when loading data into our GPUs, see &lt;a href="https://tensorflow.rstudio.com/guide/tfdatasets/introduction/#parallel-mapping"&gt;Parallel Mapping&lt;/a&gt; for details.&lt;/p&gt;
&lt;p&gt;We can now re-run &lt;code&gt;nvidia-smi&lt;/code&gt; to validate all our GPUs are being used:&lt;/p&gt;
&lt;pre class="bash"&gt;&lt;code&gt;nvidia-smi&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.152.00   Driver Version: 418.152.00   CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 00000000:00:05.0 Off |                    0 |
| N/A   49C    P0    94W / 149W |  10936MiB / 11441MiB |     53%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K80           Off  | 00000000:00:06.0 Off |                    0 |
| N/A   76C    P0   114W / 149W |  10936MiB / 11441MiB |     26%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;MirroredStrategy&lt;/code&gt; can help us scale up to about 8 GPUs per compute instance; however, we are likely to need 16 instances with 8 GPUs each to train ImageNet in a reasonable time (see Jeremy Howard’s post on &lt;a href="https://www.fast.ai/2018/08/10/fastai-diu-imagenet/"&gt;Training Imagenet in 18 Minutes&lt;/a&gt;). So where do we go from here?&lt;/p&gt;
&lt;p&gt;Welcome to &lt;code&gt;MultiWorkerMirroredStrategy&lt;/code&gt;: This strategy can use not only multiple GPUs, but also multiple GPUs across multiple computers. To configure them, all we have to do is define a &lt;code&gt;TF_CONFIG&lt;/code&gt; environment variable with the right addresses and run the exact same code in each compute instance.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tensorflow)

partition &amp;lt;- 0
Sys.setenv(TF_CONFIG = jsonlite::toJSON(list(
    cluster = list(
        worker = c(&amp;quot;10.100.10.100:10090&amp;quot;, &amp;quot;10.100.10.101:10090&amp;quot;)
    ),
    task = list(type = &amp;#39;worker&amp;#39;, index = partition)
), auto_unbox = TRUE))

strategy &amp;lt;- tf$distribute$MultiWorkerMirroredStrategy(
  cross_device_ops = tf$distribute$ReductionToOneDevice())

alexnet::imagenet_partition(partition = partition) %&amp;gt;%
  alexnet::alexnet_train(strategy = strategy, parallel = 6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please note that &lt;code&gt;partition&lt;/code&gt; must change for each compute instance to uniquely identify it, and that the IP addresses also need to be adjusted. In addition, &lt;code&gt;data&lt;/code&gt; should point to a different partition of ImageNet, which we can retrieve with &lt;code&gt;pins&lt;/code&gt;; although, for convenience, &lt;code&gt;alexnet&lt;/code&gt; contains similar code under &lt;code&gt;alexnet::imagenet_partition()&lt;/code&gt;. Other than that, the code that you need to run in each compute instance is exactly the same.&lt;/p&gt;
&lt;p&gt;However, if we were to use 16 machines with 8 GPUs each to train ImageNet, it would be quite time-consuming and error-prone to manually run code in each R session. So instead, we should think of making use of cluster-computing frameworks, like Apache Spark with &lt;a href="https://blog.rstudio.com/2020/01/29/sparklyr-1-1/#barrier-execution"&gt;barrier execution&lt;/a&gt;. If you are new to Spark, there are many resources available at &lt;a href="https://sparklyr.ai"&gt;sparklyr.ai&lt;/a&gt;. To learn just about running Spark and TensorFlow together, watch our &lt;a href="https://www.youtube.com/watch?v=Zm20P3ADa14"&gt;Deep Learning with Spark, TensorFlow and R&lt;/a&gt; video.&lt;/p&gt;
&lt;p&gt;Putting it all together, training ImageNet in R with TensorFlow and Spark looks as follows:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sparklyr)
sc &amp;lt;- spark_connect(&amp;quot;yarn|mesos|etc&amp;quot;, config = list(&amp;quot;sparklyr.shell.num-executors&amp;quot; = 16))

sdf_len(sc, 16, repartition = 16) %&amp;gt;%
  spark_apply(function(df, barrier) {
      library(tensorflow)

      Sys.setenv(TF_CONFIG = jsonlite::toJSON(list(
        cluster = list(
          worker = paste(
            gsub(&amp;quot;:[0-9]+$&amp;quot;, &amp;quot;&amp;quot;, barrier$address),
            8000 + seq_along(barrier$address), sep = &amp;quot;:&amp;quot;)),
        task = list(type = &amp;#39;worker&amp;#39;, index = barrier$partition)
      ), auto_unbox = TRUE))
      
      if (is.null(tf_version())) install_tensorflow()
      
      strategy &amp;lt;- tf$distribute$MultiWorkerMirroredStrategy()
    
      result &amp;lt;- alexnet::imagenet_partition(partition = barrier$partition) %&amp;gt;%
        alexnet::alexnet_train(strategy = strategy, epochs = 10, parallel = 6)
      
      result$metrics$accuracy
  }, barrier = TRUE, columns = c(accuracy = &amp;quot;numeric&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We hope this post gave you a reasonable overview of what training large-datasets in R looks like – thanks for reading along!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-deng2009imagenet"&gt;
&lt;p&gt;Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. “Imagenet: A Large-Scale Hierarchical Image Database.” In &lt;em&gt;2009 Ieee Conference on Computer Vision and Pattern Recognition&lt;/em&gt;, 248–55. Ieee.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-krizhevsky2012imagenet"&gt;
&lt;p&gt;Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. “Imagenet Classification with Deep Convolutional Neural Networks.” In &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;, 1097–1105.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-miller1995wordnet"&gt;
&lt;p&gt;Miller, George A. 1995. “WordNet: A Lexical Database for English.” &lt;em&gt;Communications of the ACM&lt;/em&gt; 38 (11): 39–41.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">957d307b9483c9dd27777f72c326943d</distill:md5>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Distributed Computing</category>
      <category>Data Management</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-08-24-training-imagenet-with-r</guid>
      <pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-08-24-training-imagenet-with-r/images/fishing-net.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>FNN-VAE for noisy time series forecasting</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-07-31-fnn-vae-for-noisy-timeseries</link>
      <description>


&lt;p&gt;This post did not end up quite the way I’d imagined. A quick follow-up on the recent &lt;a href="https://blogs.rstudio.com/ai/posts/2020-07-20-fnn-lstm/"&gt;Time series prediction with FNN-LSTM&lt;/a&gt;, it was supposed to demonstrate how &lt;em&gt;noisy&lt;/em&gt; time series (so common in practice) could profit from a change in architecture: Instead of FNN-LSTM, an LSTM autoencoder regularized by false nearest neighbors (FNN) loss, use FNN-VAE, a variational autoencoder constrained by the same. However, FNN-VAE did not seem to handle noise better than FNN-LSTM. No plot, no post, then?&lt;/p&gt;
&lt;p&gt;On the other hand – this is not a scientific study, with hypothesis and experimental setup all preregistered; all that really matters is if there’s something useful to report. And it looks like there is.&lt;/p&gt;
&lt;p&gt;Firstly, FNN-VAE, while on par performance-wise with FNN-LSTM, is far superior in that other meaning of “performance”: Training goes a &lt;em&gt;lot&lt;/em&gt; faster for FNN-VAE.&lt;/p&gt;
&lt;p&gt;Secondly, while we don’t see much difference between FNN-LSTM and FNN-VAE, we &lt;em&gt;do&lt;/em&gt; see a clear impact of using FNN loss. Adding in FNN loss strongly reduces mean squared error with respect to the underlying (denoised) series – especially in the case of VAE, but for LSTM as well. This is of particular interest with VAE, as it comes with a regularizer out-of-the-box – namely, Kullback-Leibler (KL) divergence.&lt;/p&gt;
&lt;p&gt;Of course, we don’t claim that similar results will always be obtained on other noisy series; nor did we tune any of the models “to death”. For what could be the intent of such a post but to show our readers interesting (and promising) ideas to pursue in their own experimentation?&lt;/p&gt;
&lt;h2 id="the-context"&gt;The context&lt;/h2&gt;
&lt;p&gt;This post is the third in a mini-series.&lt;/p&gt;
&lt;p&gt;In &lt;a href="https://blogs.rstudio.com/ai/posts/2020-06-24-deep-attractors/"&gt;Deep attractors: Where deep learning meets chaos&lt;/a&gt;, we explained, with a substantial detour into chaos theory, the idea of FNN loss, introduced in &lt;span class="citation"&gt;(Gilpin 2020)&lt;/span&gt;. Please consult that first post for theoretical background and intuitions behind the technique.&lt;/p&gt;
&lt;p&gt;The subsequent post, &lt;a href="https://blogs.rstudio.com/ai/posts/2020-07-20-fnn-lstm/"&gt;Time series prediction with FNN-LSTM&lt;/a&gt;, showed how to use an LSTM autoencoder, constrained by FNN loss, for forecasting (as opposed to reconstructing an attractor). The results were stunning: In multi-step prediction (12-120 steps, with that number varying by dataset), the short-term forecasts were drastically improved by adding in FNN regularization. See that second post for experimental setup and results on four very different, non-synthetic datasets.&lt;/p&gt;
&lt;p&gt;Today, we show how to replace the LSTM autoencoder by a – convolutional – VAE. In light of the experimentation results, already hinted at above, it is completely plausible that the “variational” part is not even so important here – that a convolutional autoencoder with just MSE loss would have performed just as well on those data. In fact, to find out, it’s enough to remove the call to &lt;code&gt;reparameterize()&lt;/code&gt; and multiply the KL component of the loss by 0. (We leave this to the interested reader, to keep the post at reasonable length.)&lt;/p&gt;
&lt;p&gt;One last piece of context, in case you haven’t read the two previous posts and would like to jump in here directly. We’re doing time series forecasting; so why this talk of autoencoders? Shouldn’t we just be comparing an LSTM (or some other type of RNN, for that matter) to a convnet? In fact, the necessity of a latent representation is due to the very idea of FNN: The latent code is supposed to reflect the true attractor of a dynamical system. That is, if the attractor of the underlying system is roughly two-dimensional, we hope to find that just two of the latent variables have considerable variance. (This reasoning is explained in a lot of detail in the previous posts.)&lt;/p&gt;
&lt;h2 id="fnn-vae"&gt;FNN-VAE&lt;/h2&gt;
&lt;p&gt;So, let’s start with the code for our new model.&lt;/p&gt;
&lt;p&gt;The encoder takes the time series, of format &lt;code&gt;batch_size x num_timesteps x num_features&lt;/code&gt; just like in the LSTM case, and produces a flat, 10-dimensional output: the latent code, which FNN loss is computed on.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tensorflow)
library(keras)
library(tfdatasets)
library(tfautograph)
library(reticulate)
library(purrr)

vae_encoder_model &amp;lt;- function(n_timesteps,
                               n_features,
                               n_latent,
                               name = NULL) {
  keras_model_custom(name = name, function(self) {
    self$conv1 &amp;lt;- layer_conv_1d(kernel_size = 3,
                                filters = 16,
                                strides = 2)
    self$act1 &amp;lt;- layer_activation_leaky_relu()
    self$batchnorm1 &amp;lt;- layer_batch_normalization()
    self$conv2 &amp;lt;- layer_conv_1d(kernel_size = 7,
                                filters = 32,
                                strides = 2)
    self$act2 &amp;lt;- layer_activation_leaky_relu()
    self$batchnorm2 &amp;lt;- layer_batch_normalization()
    self$conv3 &amp;lt;- layer_conv_1d(kernel_size = 9,
                                filters = 64,
                                strides = 2)
    self$act3 &amp;lt;- layer_activation_leaky_relu()
    self$batchnorm3 &amp;lt;- layer_batch_normalization()
    self$conv4 &amp;lt;- layer_conv_1d(
      kernel_size = 9,
      filters = n_latent,
      strides = 2,
      activation = &amp;quot;linear&amp;quot; 
    )
    self$batchnorm4 &amp;lt;- layer_batch_normalization()
    self$flat &amp;lt;- layer_flatten()
    
    function (x, mask = NULL) {
      x %&amp;gt;%
        self$conv1() %&amp;gt;%
        self$act1() %&amp;gt;%
        self$batchnorm1() %&amp;gt;%
        self$conv2() %&amp;gt;%
        self$act2() %&amp;gt;%
        self$batchnorm2() %&amp;gt;%
        self$conv3() %&amp;gt;%
        self$act3() %&amp;gt;%
        self$batchnorm3() %&amp;gt;%
        self$conv4() %&amp;gt;%
        self$batchnorm4() %&amp;gt;%
        self$flat()
    }
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The decoder starts from this – flat – representation and decompresses it into a time sequence. In both encoder and decoder (de-)conv layers, parameters are chosen to handle a sequence length (&lt;code&gt;num_timesteps&lt;/code&gt;) of 120, which is what we’ll use for prediction below.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;vae_decoder_model &amp;lt;- function(n_timesteps,
                               n_features,
                               n_latent,
                               name = NULL) {
  keras_model_custom(name = name, function(self) {
    self$reshape &amp;lt;- layer_reshape(target_shape = c(1, n_latent))
    self$conv1 &amp;lt;- layer_conv_1d_transpose(kernel_size = 15,
                                          filters = 64,
                                          strides = 3)
    self$act1 &amp;lt;- layer_activation_leaky_relu()
    self$batchnorm1 &amp;lt;- layer_batch_normalization()
    self$conv2 &amp;lt;- layer_conv_1d_transpose(kernel_size = 11,
                                          filters = 32,
                                          strides = 3)
    self$act2 &amp;lt;- layer_activation_leaky_relu()
    self$batchnorm2 &amp;lt;- layer_batch_normalization()
    self$conv3 &amp;lt;- layer_conv_1d_transpose(
      kernel_size = 9,
      filters = 16,
      strides = 2,
      output_padding = 1
    )
    self$act3 &amp;lt;- layer_activation_leaky_relu()
    self$batchnorm3 &amp;lt;- layer_batch_normalization()
    self$conv4 &amp;lt;- layer_conv_1d_transpose(
      kernel_size = 7,
      filters = 1,
      strides = 1,
      activation = &amp;quot;linear&amp;quot;
    )
    self$batchnorm4 &amp;lt;- layer_batch_normalization()
    
    function (x, mask = NULL) {
      x %&amp;gt;%
        self$reshape() %&amp;gt;%
        self$conv1() %&amp;gt;%
        self$act1() %&amp;gt;%
        self$batchnorm1() %&amp;gt;%
        self$conv2() %&amp;gt;%
        self$act2() %&amp;gt;%
        self$batchnorm2() %&amp;gt;%
        self$conv3() %&amp;gt;%
        self$act3() %&amp;gt;%
        self$batchnorm3() %&amp;gt;%
        self$conv4() %&amp;gt;%
        self$batchnorm4()
    }
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that even though we called these constructors &lt;code&gt;vae_encoder_model()&lt;/code&gt; and &lt;code&gt;vae_decoder_model()&lt;/code&gt;, there is nothing variational to these models per se; they are really just an encoder and a decoder, respectively. Metamorphosis into a VAE will happen in the training procedure; in fact, the only two things that will make this a VAE are going to be the reparameterization of the latent layer and the added-in KL loss.&lt;/p&gt;
&lt;p&gt;Speaking of training, these are the routines we’ll call. The function to compute FNN loss, &lt;code&gt;loss_false_nn()&lt;/code&gt;, can be found in both of the abovementioned predecessor posts; we kindly ask the reader to copy it from one of these places.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# to reparameterize encoder output before calling decoder
reparameterize &amp;lt;- function(mean, logvar = 0) {
  eps &amp;lt;- k_random_normal(shape = n_latent)
  eps * k_exp(logvar * 0.5) + mean
}

# loss has 3 components: NLL, KL, and FNN
# otherwise, this is just normal TF2-style training 
train_step_vae &amp;lt;- function(batch) {
  with (tf$GradientTape(persistent = TRUE) %as% tape, {
    code &amp;lt;- encoder(batch[[1]])
    z &amp;lt;- reparameterize(code)
    prediction &amp;lt;- decoder(z)
    
    l_mse &amp;lt;- mse_loss(batch[[2]], prediction)
    # see loss_false_nn in 2 previous posts
    l_fnn &amp;lt;- loss_false_nn(code)
    # KL divergence to a standard normal
    l_kl &amp;lt;- -0.5 * k_mean(1 - k_square(z))
    # overall loss is a weighted sum of all 3 components
    loss &amp;lt;- l_mse + fnn_weight * l_fnn + kl_weight * l_kl
  })
  
  encoder_gradients &amp;lt;-
    tape$gradient(loss, encoder$trainable_variables)
  decoder_gradients &amp;lt;-
    tape$gradient(loss, decoder$trainable_variables)
  
  optimizer$apply_gradients(purrr::transpose(list(
    encoder_gradients, encoder$trainable_variables
  )))
  optimizer$apply_gradients(purrr::transpose(list(
    decoder_gradients, decoder$trainable_variables
  )))
  
  train_loss(loss)
  train_mse(l_mse)
  train_fnn(l_fnn)
  train_kl(l_kl)
}

# wrap it all in autograph
training_loop_vae &amp;lt;- tf_function(autograph(function(ds_train) {
  
  for (batch in ds_train) {
    train_step_vae(batch) 
  }
  
  tf$print(&amp;quot;Loss: &amp;quot;, train_loss$result())
  tf$print(&amp;quot;MSE: &amp;quot;, train_mse$result())
  tf$print(&amp;quot;FNN loss: &amp;quot;, train_fnn$result())
  tf$print(&amp;quot;KL loss: &amp;quot;, train_kl$result())
  
  train_loss$reset_states()
  train_mse$reset_states()
  train_fnn$reset_states()
  train_kl$reset_states()
  
}))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To finish up the model section, here is the actual training code. This is nearly identical to what we did for FNN-LSTM before.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;n_latent &amp;lt;- 10L
n_features &amp;lt;- 1

encoder &amp;lt;- vae_encoder_model(n_timesteps,
                         n_features,
                         n_latent)

decoder &amp;lt;- vae_decoder_model(n_timesteps,
                         n_features,
                         n_latent)
mse_loss &amp;lt;-
  tf$keras$losses$MeanSquaredError(reduction = tf$keras$losses$Reduction$SUM)

train_loss &amp;lt;- tf$keras$metrics$Mean(name = &amp;#39;train_loss&amp;#39;)
train_fnn &amp;lt;- tf$keras$metrics$Mean(name = &amp;#39;train_fnn&amp;#39;)
train_mse &amp;lt;-  tf$keras$metrics$Mean(name = &amp;#39;train_mse&amp;#39;)
train_kl &amp;lt;-  tf$keras$metrics$Mean(name = &amp;#39;train_kl&amp;#39;)

fnn_multiplier &amp;lt;- 1 # default value used in nearly all cases (see text)
fnn_weight &amp;lt;- fnn_multiplier * nrow(x_train)/batch_size

kl_weight &amp;lt;- 1

optimizer &amp;lt;- optimizer_adam(lr = 1e-3)

for (epoch in 1:100) {
  cat(&amp;quot;Epoch: &amp;quot;, epoch, &amp;quot; -----------\n&amp;quot;)
  training_loop_vae(ds_train)
 
  test_batch &amp;lt;- as_iterator(ds_test) %&amp;gt;% iter_next()
  encoded &amp;lt;- encoder(test_batch[[1]][1:1000])
  test_var &amp;lt;- tf$math$reduce_variance(encoded, axis = 0L)
  print(test_var %&amp;gt;% as.numeric() %&amp;gt;% round(5))
}&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="experimental-setup-and-data"&gt;Experimental setup and data&lt;/h2&gt;
&lt;p&gt;The idea was to add white noise to a deterministic series. This time, the &lt;a href="https://en.wikipedia.org/wiki/R%C3%B6ssler_attractor"&gt;Roessler system&lt;/a&gt; was chosen, mainly for the prettiness of its attractor, apparent even in its two-dimensional projections:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/roessler.png" alt="Roessler attractor, two-dimensional projections." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-5)Roessler attractor, two-dimensional projections.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Like we did for the Lorenz system in the first part of this series, we use &lt;code&gt;deSolve&lt;/code&gt; to generate data from the Roessler equations.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(deSolve)

parameters &amp;lt;- c(a = .2,
                b = .2,
                c = 5.7)

initial_state &amp;lt;-
  c(x = 1,
    y = 1,
    z = 1.05)

roessler &amp;lt;- function(t, state, parameters) {
  with(as.list(c(state, parameters)), {
    dx &amp;lt;- -y - z
    dy &amp;lt;- x + a * y
    dz = b + z * (x - c)
    
    list(c(dx, dy, dz))
  })
}

times &amp;lt;- seq(0, 2500, length.out = 20000)

roessler_ts &amp;lt;-
  ode(
    y = initial_state,
    times = times,
    func = roessler,
    parms = parameters,
    method = &amp;quot;lsoda&amp;quot;
  ) %&amp;gt;% unclass() %&amp;gt;% as_tibble()

n &amp;lt;- 10000
roessler &amp;lt;- roessler_ts$x[1:n]

roessler &amp;lt;- scale(roessler)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, noise is added, to the desired degree, by drawing from a normal distribution, centered at zero, with standard deviations varying between 1 and 2.5.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# add noise
noise &amp;lt;- 1 # also used 1.5, 2, 2.5
roessler &amp;lt;- roessler + rnorm(10000, mean = 0, sd = noise)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here you can compare effects of not adding any noise (left), standard deviation-1 (middle), and standard deviation-2.5 Gaussian noise:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/roessler_noise.png" alt="Roessler series with added noise. Top: none. Middle: SD = 1. Bottom: SD = 2.5." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-8)Roessler series with added noise. Top: none. Middle: SD = 1. Bottom: SD = 2.5.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Otherwise, preprocessing proceeds as in the previous posts. In the upcoming results section, we’ll compare forecasts not just to the “real”, after noise addition, test split of the data, but also to the underlying Roessler system – that is, the thing we’re really interested in. (Just that in the real world, we can’t do that check.) This second test set is prepared for forecasting just like the other one; to avoid duplication we don’t reproduce the code.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;n_timesteps &amp;lt;- 120
batch_size &amp;lt;- 32

gen_timesteps &amp;lt;- function(x, n_timesteps) {
  do.call(rbind,
          purrr::map(seq_along(x),
                     function(i) {
                       start &amp;lt;- i
                       end &amp;lt;- i + n_timesteps - 1
                       out &amp;lt;- x[start:end]
                       out
                     })
  ) %&amp;gt;%
    na.omit()
}

train &amp;lt;- gen_timesteps(roessler[1:(n/2)], 2 * n_timesteps)
test &amp;lt;- gen_timesteps(roessler[(n/2):n], 2 * n_timesteps) 

dim(train) &amp;lt;- c(dim(train), 1)
dim(test) &amp;lt;- c(dim(test), 1)

x_train &amp;lt;- train[ , 1:n_timesteps, , drop = FALSE]
y_train &amp;lt;- train[ , (n_timesteps + 1):(2*n_timesteps), , drop = FALSE]

ds_train &amp;lt;- tensor_slices_dataset(list(x_train, y_train)) %&amp;gt;%
  dataset_shuffle(nrow(x_train)) %&amp;gt;%
  dataset_batch(batch_size)

x_test &amp;lt;- test[ , 1:n_timesteps, , drop = FALSE]
y_test &amp;lt;- test[ , (n_timesteps + 1):(2*n_timesteps), , drop = FALSE]

ds_test &amp;lt;- tensor_slices_dataset(list(x_test, y_test)) %&amp;gt;%
  dataset_batch(nrow(x_test))&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="results"&gt;Results&lt;/h2&gt;
&lt;p&gt;The LSTM used for comparison with the VAE described above is identical to the architecture employed in the previous post. While with the VAE, an &lt;code&gt;fnn_multiplier&lt;/code&gt; of 1 yielded sufficient regularization for all noise levels, some more experimentation was needed for the LSTM: At noise levels 2 and 2.5, that multiplier was set to 5.&lt;/p&gt;
&lt;p&gt;As a result, in all cases, there was one latent variable with high variance and a second one of minor importance. For all others, variance was close to 0.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In all cases&lt;/em&gt; here means: In all cases where FNN regularization was used. As already hinted at in the introduction, the main regularizing factor providing robustness to noise here seems to be FNN loss, not KL divergence. So for all noise levels, besides FNN-regularized LSTM and VAE models we also tested their non-constrained counterparts.&lt;/p&gt;
&lt;h4 id="low-noise"&gt;Low noise&lt;/h4&gt;
&lt;p&gt;Seeing how all models did superbly on the original deterministic series, a noise level of 1 can almost be treated as a baseline. Here you see sixteen 120-timestep predictions from both regularized models, FNN-VAE (dark blue), and FNN-LSTM (orange). The noisy test data, both input (&lt;code&gt;x&lt;/code&gt;, 120 steps) and output (&lt;code&gt;y&lt;/code&gt;, 120 steps) are displayed in (blue-ish) grey. In green, also spanning the whole sequence, we have the original Roessler data, the way they would look had no noise been added.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/noise_1.png" alt="Roessler series with added Gaussian noise of standard deviation 1. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from FNN-LSTM. Dark blue: Predictions from FNN-VAE." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-10)Roessler series with added Gaussian noise of standard deviation 1. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from FNN-LSTM. Dark blue: Predictions from FNN-VAE.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Despite the noise, forecasts from both models look excellent. Is this due to the FNN regularizer?&lt;/p&gt;
&lt;p&gt;Looking at forecasts from their unregularized counterparts, we have to admit these do not look any worse. (For better comparability, the sixteen sequences to forecast were initiallly picked at random, but used to test all models and conditions.)&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/noise_1_nofnn.png" alt="Roessler series with added Gaussian noise of standard deviation 1. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from unregularized LSTM. Dark blue: Predictions from unregularized VAE." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-11)Roessler series with added Gaussian noise of standard deviation 1. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from unregularized LSTM. Dark blue: Predictions from unregularized VAE.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;What happens when we start to add noise?&lt;/p&gt;
&lt;h4 id="substantial-noise"&gt;Substantial noise&lt;/h4&gt;
&lt;p&gt;Between noise levels 1.5 and 2, something changed, or became noticeable from visual inspection. Let’s jump directly to the highest-used level though: 2.5.&lt;/p&gt;
&lt;p&gt;Here first are predictions obtained from the unregularized models.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/noise_2.5_nofnn.png" alt="Roessler series with added Gaussian noise of standard deviation 2.5. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from unregularized LSTM. Dark blue: Predictions from unregularized VAE." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-12)Roessler series with added Gaussian noise of standard deviation 2.5. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from unregularized LSTM. Dark blue: Predictions from unregularized VAE.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Both LSTM and VAE get “distracted” a bit too much by the noise, the latter to an even higher degree. This leads to cases where predictions strongly “overshoot” the underlying non-noisy rhythm. This is not surprising, of course: They were &lt;em&gt;trained&lt;/em&gt; on the noisy version; predict fluctuations is what they learned.&lt;/p&gt;
&lt;p&gt;Do we see the same with the FNN models?&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/noise_2.5.png" alt="Roessler series with added Gaussian noise of standard deviation 2.5. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from FNN-LSTM. Dark blue: Predictions from FNN-VAE." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-13)Roessler series with added Gaussian noise of standard deviation 2.5. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from FNN-LSTM. Dark blue: Predictions from FNN-VAE.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Interestingly, we see a much better fit to the underlying Roessler system now! Especially the VAE model, FNN-VAE, surprises with a whole new smoothness of predictions; but FNN-LSTM turns up much smoother forecasts as well.&lt;/p&gt;
&lt;p&gt;“Smooth, fitting the system…” – by now you may be wondering, when are we going to come up with more quantitative assertions? If quantitative implies “mean squared error” (MSE), and if MSE is taken to be some divergence between forecasts and the true target from the test set, the answer is that this MSE doesn’t differ much between any of the four architectures. Put differently, it is mostly a function of noise level.&lt;/p&gt;
&lt;p&gt;However, we could argue that what we’re really interested in is how well a model forecasts the underlying process. And there, we see differences.&lt;/p&gt;
&lt;p&gt;In the following plot, we contrast MSEs obtained for the four model types (grey: VAE; orange: LSTM; dark blue: FNN-VAE; green: FNN-LSTM). The rows reflect noise levels (1, 1.5, 2, 2.5); the columns represent MSE in relation to the noisy(“real”) target (left) on the one hand, and in relation to the underlying system on the other (right). For better visibility of the effect, &lt;em&gt;MSEs have been normalized as fractions of the maximum MSE in a category&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;So, if we want to predict &lt;em&gt;signal plus noise&lt;/em&gt; (left), it is not extremely critical whether we use FNN or not. But if we want to predict the signal only (right), with increasing noise in the data FNN loss becomes increasingly effective. This effect is far stronger for VAE vs. FNN-VAE than for LSTM vs. FNN-LSTM: The distance between the grey line (VAE) and the dark blue one (FNN-VAE) becomes larger and larger as we add more noise.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/mses.png" alt="Normalized MSEs obtained for the four model types (grey: VAE; orange: LSTM; dark blue: FNN-VAE; green: FNN-LSTM). Rows are noise levels (1, 1.5, 2, 2.5); columns are MSE as related to the real target (left) and the underlying system (right)." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-14)Normalized MSEs obtained for the four model types (grey: VAE; orange: LSTM; dark blue: FNN-VAE; green: FNN-LSTM). Rows are noise levels (1, 1.5, 2, 2.5); columns are MSE as related to the real target (left) and the underlying system (right).
&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="summing-up"&gt;Summing up&lt;/h2&gt;
&lt;p&gt;Our experiments show that when noise is likely to obscure measurements from an underlying deterministic system, FNN regularization can strongly improve forecasts. This is the case especially for convolutional VAEs, and probably convolutional autoencoders in general. And if an FNN-constrained VAE performs as well, for time series prediction, as an LSTM, there is a strong incentive to use the convolutional model: It trains significantly faster.&lt;/p&gt;
&lt;p&gt;With that, we conclude our mini-series on FNN-regularized models. As always, we’d love to hear from you if you were able to make use of this in your own work!&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-gilpin2020deep"&gt;
&lt;p&gt;Gilpin, William. 2020. “Deep Reconstruction of Strange Attractors from Time Series.” &lt;a href="http://arxiv.org/abs/2002.05909"&gt;http://arxiv.org/abs/2002.05909&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">14d4183bce425fc7eaa5ddba400775d1</distill:md5>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-07-31-fnn-vae-for-noisy-timeseries</guid>
      <pubDate>Fri, 31 Jul 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/kb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Time series prediction with FNN-LSTM</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm</link>
      <description>


&lt;p&gt;Today, we pick up on the plan alluded to in the conclusion of the recent &lt;a href="https://blogs.rstudio.com/ai/posts/2020-06-24-deep-attractors/"&gt;Deep attractors: Where deep learning meets chaos&lt;/a&gt;: employ that same technique to generate &lt;em&gt;forecasts&lt;/em&gt; for empirical time series data.&lt;/p&gt;
&lt;p&gt;“That same technique”, which for conciseness, I’ll take the liberty of referring to as FNN-LSTM, is due to William Gilpin’s 2020 paper “Deep reconstruction of strange attractors from time series” &lt;span class="citation"&gt;(Gilpin 2020)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In a nutshell&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, the problem addressed is as follows: A system, known or assumed to be nonlinear and highly dependent on initial conditions, is observed, resulting in a scalar series of measurements. The measurements are not just – inevitably – noisy, but in addition, they are – at best – a projection of a multidimensional state space onto a line.&lt;/p&gt;
&lt;p&gt;Classically in nonlinear time series analysis, such scalar series of observations are augmented by supplementing, at every point in time, delayed measurements of that same series – a technique called &lt;em&gt;delay coordinate embedding&lt;/em&gt; &lt;span class="citation"&gt;(Sauer, Yorke, and Casdagli 1991)&lt;/span&gt;. For example, instead of just a single vector &lt;code&gt;X1&lt;/code&gt;, we could have a matrix of vectors &lt;code&gt;X1&lt;/code&gt;, &lt;code&gt;X2&lt;/code&gt;, and &lt;code&gt;X3&lt;/code&gt;, with &lt;code&gt;X2&lt;/code&gt; containing the same values as &lt;code&gt;X1&lt;/code&gt;, but starting from the third observation, and &lt;code&gt;X3&lt;/code&gt;, from the fifth. In this case, the &lt;em&gt;delay&lt;/em&gt; would be 2, and the &lt;em&gt;embedding dimension&lt;/em&gt;, 3. Various &lt;a href="https://en.wikipedia.org/wiki/Takens&amp;#39;s_theorem"&gt;theorems&lt;/a&gt; state that if these parameters are chosen adequately, it is possible to reconstruct the complete state space. There is a problem though: The theorems assume that the dimensionality of the true state space is known, which in many real-world applications, won’t be the case.&lt;/p&gt;
&lt;p&gt;This is where Gilpin’s idea comes in: Train an autoencoder, whose intermediate representation encapsulates the system’s attractor. Not just any MSE-optimized autoencoder though. The latent representation is regularized by &lt;em&gt;false nearest neighbors&lt;/em&gt; (FNN) loss, a technique commonly used with delay coordinate embedding to determine an adequate embedding dimension. False neighbors are those who are close in &lt;code&gt;n&lt;/code&gt;-dimensional space, but significantly farther apart in &lt;code&gt;n+1&lt;/code&gt;-dimensional space. In the aforementioned introductory &lt;a href="https://blogs.rstudio.com/ai/posts/2020-06-24-deep-attractors/"&gt;post&lt;/a&gt;, we showed how this technique allowed to reconstruct the attractor of the (synthetic) Lorenz system. Now, we want to move on to prediction.&lt;/p&gt;
&lt;p&gt;We first describe the setup, including model definitions, training procedures, and data preparation. Then, we tell you how it went.&lt;/p&gt;
&lt;h2 id="setup"&gt;Setup&lt;/h2&gt;
&lt;h3 id="from-reconstruction-to-forecasting-and-branching-out-into-the-real-world"&gt;From reconstruction to forecasting, and branching out into the real world&lt;/h3&gt;
&lt;p&gt;In the previous post, we trained an LSTM autoencoder to generate a compressed code, representing the attractor of the system. As usual with autoencoders, the target when training is the same as the input, meaning that overall loss consisted of two components: The FNN loss, computed on the latent representation only, and the mean-squared-error loss between input and output. Now for prediction, the target consists of future values, as many as we wish to predict. Put differently: The architecture stays the same, but instead of reconstruction we perform prediction, in the standard RNN way. Where the usual RNN setup would just directly chain the desired number of LSTMs, we have an LSTM encoder that outputs a (timestep-less) latent code, and an LSTM decoder that starting from that code, repeated as many times as required, forecasts the required number of future values.&lt;/p&gt;
&lt;p&gt;This of course means that to evaluate forecast performance, we need to compare against an LSTM-only setup. This is exactly what we’ll do, and comparison will turn out to be interesting not just quantitatively, but &lt;em&gt;qualitatively&lt;/em&gt; as well.&lt;/p&gt;
&lt;p&gt;We perform these comparisons on the four datasets Gilpin chose to demonstrate &lt;a href="https://github.com/williamgilpin/fnn/blob/master/exploratory.ipynb"&gt;attractor reconstruction on observational data&lt;/a&gt;. While all of these, as is evident from the images in that notebook, exhibit nice attractors, we’ll see that not all of them are equally suited to forecasting using simple RNN-based architectures – with or without FNN regularization. But even those that clearly demand a different approach allow for interesting observations as to the impact of FNN loss.&lt;/p&gt;
&lt;h3 id="model-definitions-and-training-setup"&gt;Model definitions and training setup&lt;/h3&gt;
&lt;p&gt;In all four experiments, we use the same model definitions and training procedures, the only differing parameter being the number of timesteps used in the LSTMs (for reasons that will become evident when we introduce the individual datasets).&lt;/p&gt;
&lt;p&gt;Both architectures were chosen to be straightforward, and about comparable in number of parameters – both basically consist of two LSTMs with 32 units (&lt;code&gt;n_recurrent&lt;/code&gt; will be set to 32 for all experiments).&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id="fnn-lstm"&gt;FNN-LSTM&lt;/h4&gt;
&lt;p&gt;FNN-LSTM looks nearly like in the previous post, apart from the fact that we split up the encoder LSTM into two, to uncouple capacity (&lt;code&gt;n_recurrent&lt;/code&gt;) from maximal latent state dimensionality (&lt;code&gt;n_latent&lt;/code&gt;, kept at 10 just like before).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# DL-related packages
library(tensorflow)
library(keras)
library(tfdatasets)
library(tfautograph)
library(reticulate)

# going to need those later
library(tidyverse)
library(cowplot)

encoder_model &amp;lt;- function(n_timesteps,
                          n_features,
                          n_recurrent,
                          n_latent,
                          name = NULL) {
  
  keras_model_custom(name = name, function(self) {
    
    self$noise &amp;lt;- layer_gaussian_noise(stddev = 0.5)
    self$lstm1 &amp;lt;-  layer_lstm(
      units = n_recurrent,
      input_shape = c(n_timesteps, n_features),
      return_sequences = TRUE
    ) 
    self$batchnorm1 &amp;lt;- layer_batch_normalization()
    self$lstm2 &amp;lt;-  layer_lstm(
      units = n_latent,
      return_sequences = FALSE
    ) 
    self$batchnorm2 &amp;lt;- layer_batch_normalization()
    
    function (x, mask = NULL) {
      x %&amp;gt;%
        self$noise() %&amp;gt;%
        self$lstm1() %&amp;gt;%
        self$batchnorm1() %&amp;gt;%
        self$lstm2() %&amp;gt;%
        self$batchnorm2() 
    }
  })
}

decoder_model &amp;lt;- function(n_timesteps,
                          n_features,
                          n_recurrent,
                          n_latent,
                          name = NULL) {
  
  keras_model_custom(name = name, function(self) {
    
    self$repeat_vector &amp;lt;- layer_repeat_vector(n = n_timesteps)
    self$noise &amp;lt;- layer_gaussian_noise(stddev = 0.5)
    self$lstm &amp;lt;- layer_lstm(
      units = n_recurrent,
      return_sequences = TRUE,
      go_backwards = TRUE
    ) 
    self$batchnorm &amp;lt;- layer_batch_normalization()
    self$elu &amp;lt;- layer_activation_elu() 
    self$time_distributed &amp;lt;- time_distributed(layer = layer_dense(units = n_features))
    
    function (x, mask = NULL) {
      x %&amp;gt;%
        self$repeat_vector() %&amp;gt;%
        self$noise() %&amp;gt;%
        self$lstm() %&amp;gt;%
        self$batchnorm() %&amp;gt;%
        self$elu() %&amp;gt;%
        self$time_distributed()
    }
  })
}

n_latent &amp;lt;- 10L
n_features &amp;lt;- 1
n_hidden &amp;lt;- 32

encoder &amp;lt;- encoder_model(n_timesteps,
                         n_features,
                         n_hidden,
                         n_latent)

decoder &amp;lt;- decoder_model(n_timesteps,
                         n_features,
                         n_hidden,
                         n_latent)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The regularizer, FNN loss, is unchanged:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;loss_false_nn &amp;lt;- function(x) {
  
  # changing these parameters is equivalent to
  # changing the strength of the regularizer, so we keep these fixed (these values
  # correspond to the original values used in Kennel et al 1992).
  rtol &amp;lt;- 10 
  atol &amp;lt;- 2
  k_frac &amp;lt;- 0.01
  
  k &amp;lt;- max(1, floor(k_frac * batch_size))
  
  ## Vectorized version of distance matrix calculation
  tri_mask &amp;lt;-
    tf$linalg$band_part(
      tf$ones(
        shape = c(tf$cast(n_latent, tf$int32), tf$cast(n_latent, tf$int32)),
        dtype = tf$float32
      ),
      num_lower = -1L,
      num_upper = 0L
    )
  
  # latent x batch_size x latent
  batch_masked &amp;lt;-
    tf$multiply(tri_mask[, tf$newaxis,], x[tf$newaxis, reticulate::py_ellipsis()])
  
  # latent x batch_size x 1
  x_squared &amp;lt;-
    tf$reduce_sum(batch_masked * batch_masked,
                  axis = 2L,
                  keepdims = TRUE)
  
  # latent x batch_size x batch_size
  pdist_vector &amp;lt;- x_squared + tf$transpose(x_squared, perm = c(0L, 2L, 1L)) -
    2 * tf$matmul(batch_masked, tf$transpose(batch_masked, perm = c(0L, 2L, 1L)))
  
  #(latent, batch_size, batch_size)
  all_dists &amp;lt;- pdist_vector
  # latent
  all_ra &amp;lt;-
    tf$sqrt((1 / (
      batch_size * tf$range(1, 1 + n_latent, dtype = tf$float32)
    )) *
      tf$reduce_sum(tf$square(
        batch_masked - tf$reduce_mean(batch_masked, axis = 1L, keepdims = TRUE)
      ), axis = c(1L, 2L)))
  
  # Avoid singularity in the case of zeros
  #(latent, batch_size, batch_size)
  all_dists &amp;lt;-
    tf$clip_by_value(all_dists, 1e-14, tf$reduce_max(all_dists))
  
  #inds = tf.argsort(all_dists, axis=-1)
  top_k &amp;lt;- tf$math$top_k(-all_dists, tf$cast(k + 1, tf$int32))
  # (#(latent, batch_size, batch_size)
  top_indices &amp;lt;- top_k[[1]]
  
  #(latent, batch_size, batch_size)
  neighbor_dists_d &amp;lt;-
    tf$gather(all_dists, top_indices, batch_dims = -1L)
  #(latent - 1, batch_size, batch_size)
  neighbor_new_dists &amp;lt;-
    tf$gather(all_dists[2:-1, , ],
              top_indices[1:-2, , ],
              batch_dims = -1L)
  
  # Eq. 4 of Kennel et al.
  #(latent - 1, batch_size, batch_size)
  scaled_dist &amp;lt;- tf$sqrt((
    tf$square(neighbor_new_dists) -
      # (9, 8, 2)
      tf$square(neighbor_dists_d[1:-2, , ])) /
      # (9, 8, 2)
      tf$square(neighbor_dists_d[1:-2, , ])
  )
  
  # Kennel condition #1
  #(latent - 1, batch_size, batch_size)
  is_false_change &amp;lt;- (scaled_dist &amp;gt; rtol)
  # Kennel condition 2
  #(latent - 1, batch_size, batch_size)
  is_large_jump &amp;lt;-
    (neighbor_new_dists &amp;gt; atol * all_ra[1:-2, tf$newaxis, tf$newaxis])
  
  is_false_neighbor &amp;lt;-
    tf$math$logical_or(is_false_change, is_large_jump)
  #(latent - 1, batch_size, 1)
  total_false_neighbors &amp;lt;-
    tf$cast(is_false_neighbor, tf$int32)[reticulate::py_ellipsis(), 2:(k + 2)]
  
  # Pad zero to match dimensionality of latent space
  # (latent - 1)
  reg_weights &amp;lt;-
    1 - tf$reduce_mean(tf$cast(total_false_neighbors, tf$float32), axis = c(1L, 2L))
  # (latent,)
  reg_weights &amp;lt;- tf$pad(reg_weights, list(list(1L, 0L)))
  
  # Find batch average activity
  
  # L2 Activity regularization
  activations_batch_averaged &amp;lt;-
    tf$sqrt(tf$reduce_mean(tf$square(x), axis = 0L))
  
  loss &amp;lt;- tf$reduce_sum(tf$multiply(reg_weights, activations_batch_averaged))
  loss
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Training is unchanged as well, apart from the fact that now, we continually output latent variable variances in addition to the losses. This is because with FNN-LSTM, we have to choose an adequate weight for the FNN loss component. An “adequate weight” is one where the variance drops sharply after the first &lt;code&gt;n&lt;/code&gt; variables, with &lt;code&gt;n&lt;/code&gt; thought to correspond to attractor dimensionality. For the Lorenz system discussed in the previous post, this is how these variances looked:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;     V1       V2        V3        V4        V5        V6        V7        V8        V9       V10
 0.0739   0.0582   1.12e-6   3.13e-4   1.43e-5   1.52e-8   1.35e-6   1.86e-4   1.67e-4   4.39e-5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we take variance as an indicator of &lt;em&gt;importance&lt;/em&gt;, the first two variables are clearly more important than the rest. This finding nicely corresponds to “official” estimates of Lorenz attractor dimensionality. For example, the correlation dimension is estimated to lie around 2.05 &lt;span class="citation"&gt;(Grassberger and Procaccia 1983)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Thus, here we have the training routine:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_step &amp;lt;- function(batch) {
  with (tf$GradientTape(persistent = TRUE) %as% tape, {
    code &amp;lt;- encoder(batch[[1]])
    prediction &amp;lt;- decoder(code)
    
    l_mse &amp;lt;- mse_loss(batch[[2]], prediction)
    l_fnn &amp;lt;- loss_false_nn(code)
    loss &amp;lt;- l_mse + fnn_weight * l_fnn
  })
  
  encoder_gradients &amp;lt;-
    tape$gradient(loss, encoder$trainable_variables)
  decoder_gradients &amp;lt;-
    tape$gradient(loss, decoder$trainable_variables)
  
  optimizer$apply_gradients(purrr::transpose(list(
    encoder_gradients, encoder$trainable_variables
  )))
  optimizer$apply_gradients(purrr::transpose(list(
    decoder_gradients, decoder$trainable_variables
  )))
  
  train_loss(loss)
  train_mse(l_mse)
  train_fnn(l_fnn)
  
  
}

training_loop &amp;lt;- tf_function(autograph(function(ds_train) {
  for (batch in ds_train) {
    train_step(batch)
  }
  
  tf$print(&amp;quot;Loss: &amp;quot;, train_loss$result())
  tf$print(&amp;quot;MSE: &amp;quot;, train_mse$result())
  tf$print(&amp;quot;FNN loss: &amp;quot;, train_fnn$result())
  
  train_loss$reset_states()
  train_mse$reset_states()
  train_fnn$reset_states()
  
}))


mse_loss &amp;lt;-
  tf$keras$losses$MeanSquaredError(reduction = tf$keras$losses$Reduction$SUM)

train_loss &amp;lt;- tf$keras$metrics$Mean(name = &amp;#39;train_loss&amp;#39;)
train_fnn &amp;lt;- tf$keras$metrics$Mean(name = &amp;#39;train_fnn&amp;#39;)
train_mse &amp;lt;-  tf$keras$metrics$Mean(name = &amp;#39;train_mse&amp;#39;)

# fnn_multiplier should be chosen individually per dataset
# this is the value we used on the geyser dataset
fnn_multiplier &amp;lt;- 0.7
fnn_weight &amp;lt;- fnn_multiplier * nrow(x_train)/batch_size

# learning rate may also need adjustment
optimizer &amp;lt;- optimizer_adam(lr = 1e-3)

for (epoch in 1:200) {
 cat(&amp;quot;Epoch: &amp;quot;, epoch, &amp;quot; -----------\n&amp;quot;)
 training_loop(ds_train)
 
 test_batch &amp;lt;- as_iterator(ds_test) %&amp;gt;% iter_next()
 encoded &amp;lt;- encoder(test_batch[[1]]) 
 test_var &amp;lt;- tf$math$reduce_variance(encoded, axis = 0L)
 print(test_var %&amp;gt;% as.numeric() %&amp;gt;% round(5))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On to what we’ll use as a baseline for comparison.&lt;/p&gt;
&lt;h4 id="vanilla-lstm"&gt;Vanilla LSTM&lt;/h4&gt;
&lt;p&gt;Here is the vanilla LSTM, stacking two layers, each, again, of size 32. Dropout and recurrent dropout were chosen individually per dataset, as was the learning rate.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;lstm &amp;lt;- function(n_latent, n_timesteps, n_features, n_recurrent, dropout, recurrent_dropout,
                 optimizer = optimizer_adam(lr =  1e-3)) {
  
  model &amp;lt;- keras_model_sequential() %&amp;gt;%
    layer_lstm(
      units = n_recurrent,
      input_shape = c(n_timesteps, n_features),
      dropout = dropout, 
      recurrent_dropout = recurrent_dropout,
      return_sequences = TRUE
    ) %&amp;gt;% 
    layer_lstm(
      units = n_recurrent,
      dropout = dropout,
      recurrent_dropout = recurrent_dropout,
      return_sequences = TRUE
    ) %&amp;gt;% 
    time_distributed(layer_dense(units = 1))
  
  model %&amp;gt;%
    compile(
      loss = &amp;quot;mse&amp;quot;,
      optimizer = optimizer
    )
  model
  
}

model &amp;lt;- lstm(n_latent, n_timesteps, n_features, n_hidden, dropout = 0.2, recurrent_dropout = 0.2)&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="data-preparation"&gt;Data preparation&lt;/h3&gt;
&lt;p&gt;For all experiments, data were prepared in the same way.&lt;/p&gt;
&lt;p&gt;In every case, we used the first 10000 measurements available in the respective &lt;code&gt;.pkl&lt;/code&gt; files &lt;a href="https://github.com/williamgilpin/fnn/tree/master/datasets"&gt;provided by Gilpin in his GitHub repository&lt;/a&gt;. To save on file size and not depend on an external data source, we extracted those first 10000 entries to &lt;code&gt;.csv&lt;/code&gt; files downloadable directly from this blog’s repo:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;geyser &amp;lt;- download.file(
  &amp;quot;https://raw.githubusercontent.com/rstudio/ai-blog/master/docs/posts/2020-07-20-fnn-lstm/data/geyser.csv&amp;quot;,
  &amp;quot;data/geyser.csv&amp;quot;)

electricity &amp;lt;- download.file(
  &amp;quot;https://raw.githubusercontent.com/rstudio/ai-blog/master/docs/posts/2020-07-20-fnn-lstm/data/electricity.csv&amp;quot;,
  &amp;quot;data/electricity.csv&amp;quot;)

ecg &amp;lt;- download.file(
  &amp;quot;https://raw.githubusercontent.com/rstudio/ai-blog/master/docs/posts/2020-07-20-fnn-lstm/data/ecg.csv&amp;quot;,
  &amp;quot;data/ecg.csv&amp;quot;)

mouse &amp;lt;- download.file(
  &amp;quot;https://raw.githubusercontent.com/rstudio/ai-blog/master/docs/posts/2020-07-20-fnn-lstm/data/mouse.csv&amp;quot;,
  &amp;quot;data/mouse.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Should you want to access the complete time series (of considerably greater lengths), just download them from Gilpin’s repo and load them using &lt;code&gt;reticulate&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# e.g.
geyser &amp;lt;- reticulate::py_load_object(&amp;quot;geyser_train_test.pkl&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the data preparation code for the first dataset, &lt;code&gt;geyser&lt;/code&gt; - all other datasets were treated the same way.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# the first 10000 measurements from the compilation provided by Gilpin
geyser &amp;lt;- read_csv(&amp;quot;geyser.csv&amp;quot;, col_names = FALSE) %&amp;gt;% select(X1) %&amp;gt;% pull() %&amp;gt;% unclass()

# standardize
geyser &amp;lt;- scale(geyser)

# varies per dataset; see below 
n_timesteps &amp;lt;- 60
batch_size &amp;lt;- 32

# transform into [batch_size, timesteps, features] format required by RNNs
gen_timesteps &amp;lt;- function(x, n_timesteps) {
  do.call(rbind,
          purrr::map(seq_along(x),
                     function(i) {
                       start &amp;lt;- i
                       end &amp;lt;- i + n_timesteps - 1
                       out &amp;lt;- x[start:end]
                       out
                     })
  ) %&amp;gt;%
    na.omit()
}

n &amp;lt;- 10000
train &amp;lt;- gen_timesteps(geyser[1:(n/2)], 2 * n_timesteps)
test &amp;lt;- gen_timesteps(geyser[(n/2):n], 2 * n_timesteps) 

dim(train) &amp;lt;- c(dim(train), 1)
dim(test) &amp;lt;- c(dim(test), 1)

# split into input and target  
x_train &amp;lt;- train[ , 1:n_timesteps, , drop = FALSE]
y_train &amp;lt;- train[ , (n_timesteps + 1):(2*n_timesteps), , drop = FALSE]

x_test &amp;lt;- test[ , 1:n_timesteps, , drop = FALSE]
y_test &amp;lt;- test[ , (n_timesteps + 1):(2*n_timesteps), , drop = FALSE]

# create tfdatasets
ds_train &amp;lt;- tensor_slices_dataset(list(x_train, y_train)) %&amp;gt;%
  dataset_shuffle(nrow(x_train)) %&amp;gt;%
  dataset_batch(batch_size)

ds_test &amp;lt;- tensor_slices_dataset(list(x_test, y_test)) %&amp;gt;%
  dataset_batch(nrow(x_test))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’re ready to look at how forecasting goes on our four datasets.&lt;/p&gt;
&lt;h2 id="experiments"&gt;Experiments&lt;/h2&gt;
&lt;h3 id="geyser-dataset"&gt;Geyser dataset&lt;/h3&gt;
&lt;p&gt;People working with time series may have heard of &lt;a href="https://en.wikipedia.org/wiki/Old_Faithful"&gt;Old Faithful&lt;/a&gt;, a geyser in Wyoming, US that has continually been erupting every 44 minutes to two hours since the year 2004. For the subset of data Gilpin extracted&lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;geyser_train_test.pkl&lt;/code&gt; corresponds to detrended temperature readings from the main runoff pool of the Old Faithful geyser in Yellowstone National Park, downloaded from the &lt;a href="https://geysertimes.org/"&gt;GeyserTimes database&lt;/a&gt;. Temperature measurements start on April 13, 2015 and occur in one-minute increments.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Like we said above, &lt;code&gt;geyser.csv&lt;/code&gt; is a subset of these measurements, comprising the first 10000 data points. To choose an adequate timestep for the LSTMs, we inspect the series at various resolutions:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/geyser_ts.png" alt="Geyer dataset. Top: First 1000 observations. Bottom: Zooming in on the first 200." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-8)Geyer dataset. Top: First 1000 observations. Bottom: Zooming in on the first 200.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;It seems like the behavior is periodic with a period of about 40-50; a timestep of 60 thus seemed like a good try.&lt;/p&gt;
&lt;p&gt;Having trained both FNN-LSTM and the vanilla LSTM for 200 epochs, we first inspect the variances of the latent variables on the test set. The value of &lt;code&gt;fnn_multiplier&lt;/code&gt; corresponding to this run was &lt;code&gt;0.7&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;test_batch &amp;lt;- as_iterator(ds_test) %&amp;gt;% iter_next()
encoded &amp;lt;- encoder(test_batch[[1]]) %&amp;gt;%
  as.array() %&amp;gt;%
  as_tibble()

encoded %&amp;gt;% summarise_all(var)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   V1     V2        V3          V4       V5       V6       V7       V8       V9      V10
0.258 0.0262 0.0000627 0.000000600 0.000533 0.000362 0.000238 0.000121 0.000518 0.000365&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is a drop in importance between the first two variables and the rest; however, unlike in the Lorenz system, &lt;code&gt;V1&lt;/code&gt; and &lt;code&gt;V2&lt;/code&gt; variances also differ by an order of magnitude.&lt;/p&gt;
&lt;p&gt;Now, it’s interesting to compare prediction errors for both models. We are going to make an observation that will carry through to all three datasets to come.&lt;/p&gt;
&lt;p&gt;Keeping up the suspense for a while, here is the code used to compute per-timestep prediction errors from both models. The same code will be used for all other datasets.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;calc_mse &amp;lt;- function(df, y_true, y_pred) {
  (sum((df[[y_true]] - df[[y_pred]])^2))/nrow(df)
}

get_mse &amp;lt;- function(test_batch, prediction) {
  
  comp_df &amp;lt;- 
    data.frame(
      test_batch[[2]][, , 1] %&amp;gt;%
        as.array()) %&amp;gt;%
        rename_with(function(name) paste0(name, &amp;quot;_true&amp;quot;)) %&amp;gt;%
    bind_cols(
      data.frame(
        prediction[, , 1] %&amp;gt;%
          as.array()) %&amp;gt;%
          rename_with(function(name) paste0(name, &amp;quot;_pred&amp;quot;)))
  
  mse &amp;lt;- purrr::map(1:dim(prediction)[2],
                        function(varno)
                          calc_mse(comp_df,
                                   paste0(&amp;quot;X&amp;quot;, varno, &amp;quot;_true&amp;quot;),
                                   paste0(&amp;quot;X&amp;quot;, varno, &amp;quot;_pred&amp;quot;))) %&amp;gt;%
    unlist()
  
  mse
}

prediction_fnn &amp;lt;- decoder(encoder(test_batch[[1]]))
mse_fnn &amp;lt;- get_mse(test_batch, prediction_fnn)

prediction_lstm &amp;lt;- model %&amp;gt;% predict(ds_test)
mse_lstm &amp;lt;- get_mse(test_batch, prediction_lstm)

mses &amp;lt;- data.frame(timestep = 1:n_timesteps, fnn = mse_fnn, lstm = mse_lstm) %&amp;gt;%
  gather(key = &amp;quot;type&amp;quot;, value = &amp;quot;mse&amp;quot;, -timestep)

ggplot(mses, aes(timestep, mse, color = type)) +
  geom_point() +
  scale_color_manual(values = c(&amp;quot;#00008B&amp;quot;, &amp;quot;#3CB371&amp;quot;)) +
  theme_classic() +
  theme(legend.position = &amp;quot;none&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And here is the actual comparison. One thing especially jumps to the eye: FNN-LSTM forecast error is significantly lower for initial timesteps, first and foremost, for the very first prediction, which from this graph we expect to be pretty good!&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/geyser_mses.png" alt="Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-11)Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Interestingly, we see “jumps” in prediction error, for FNN-LSTM, between the very first forecast and the second, and then between the second and the ensuing ones, reminding of the similar jumps in variable importance for the latent code! After the first ten timesteps, vanilla LSTM has caught up with FNN-LSTM, and we won’t interpret further development of the losses based on just a single run’s output.&lt;/p&gt;
&lt;p&gt;Instead, let’s inspect actual predictions. We randomly pick sequences from the test set, and ask both FNN-LSTM and vanilla LSTM for a forecast. The same procedure will be followed for the other datasets.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;given &amp;lt;- data.frame(as.array(tf$concat(list(
  test_batch[[1]][, , 1], test_batch[[2]][, , 1]
),
axis = 1L)) %&amp;gt;% t()) %&amp;gt;%
  add_column(type = &amp;quot;given&amp;quot;) %&amp;gt;%
  add_column(num = 1:(2 * n_timesteps))

fnn &amp;lt;- data.frame(as.array(prediction_fnn[, , 1]) %&amp;gt;%
                    t()) %&amp;gt;%
  add_column(type = &amp;quot;fnn&amp;quot;) %&amp;gt;%
  add_column(num = (n_timesteps  + 1):(2 * n_timesteps))

lstm &amp;lt;- data.frame(as.array(prediction_lstm[, , 1]) %&amp;gt;%
                     t()) %&amp;gt;%
  add_column(type = &amp;quot;lstm&amp;quot;) %&amp;gt;%
  add_column(num = (n_timesteps + 1):(2 * n_timesteps))

compare_preds_df &amp;lt;- bind_rows(given, lstm, fnn)

plots &amp;lt;- 
  purrr::map(sample(1:dim(compare_preds_df)[2], 16),
             function(v) {
               ggplot(compare_preds_df, aes(num, .data[[paste0(&amp;quot;X&amp;quot;, v)]], color = type)) +
                 geom_line() +
                 theme_classic() +
                 theme(legend.position = &amp;quot;none&amp;quot;, axis.title = element_blank()) +
                 scale_color_manual(values = c(&amp;quot;#00008B&amp;quot;, &amp;quot;#DB7093&amp;quot;, &amp;quot;#3CB371&amp;quot;))
             })

plot_grid(plotlist = plots, ncol = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are sixteen random picks of predictions on the test set. The ground truth is displayed in pink; blue forecasts are from FNN-LSTM, green ones from vanilla LSTM.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/geyser_preds.png" alt="60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-13)60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;What we expect from the error inspection comes true: FNN-LSTM yields significantly better predictions for immediate continuations of a given sequence.&lt;/p&gt;
&lt;p&gt;Let’s move on to the second dataset on our list.&lt;/p&gt;
&lt;h3 id="electricity-dataset"&gt;Electricity dataset&lt;/h3&gt;
&lt;p&gt;This is a dataset on power consumption, aggregated over 321 different households and fifteen-minute-intervals.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;electricity_train_test.pkl&lt;/code&gt; corresponds to average power consumption by 321 Portuguese households between 2012 and 2014, in units of kilowatts consumed in fifteen minute increments. This dataset is from the &lt;a href="http://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014"&gt;UCI machine learning database&lt;/a&gt;.&lt;a href="#fn4" class="footnote-ref" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here, we see a very regular pattern:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/electricity_ts.png" alt="Electricity dataset. Top: First 2000 observations. Bottom: Zooming in on 500 observations, skipping the very beginning of the series." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-14)Electricity dataset. Top: First 2000 observations. Bottom: Zooming in on 500 observations, skipping the very beginning of the series.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;With such regular behavior, we immediately tried to predict a higher number of timesteps (&lt;code&gt;120&lt;/code&gt;) – and didn’t have to retract behind that aspiration.&lt;/p&gt;
&lt;p&gt;For an &lt;code&gt;fnn_multiplier&lt;/code&gt; of &lt;code&gt;0.5&lt;/code&gt;, latent variable variances look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;V1          V2            V3       V4       V5            V6       V7         V8      V9     V10
0.390 0.000637 0.00000000288 1.48e-10 2.10e-11 0.00000000119 6.61e-11 0.00000115 1.11e-4 1.40e-4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We definitely see a sharp drop already after the first variable.&lt;/p&gt;
&lt;p&gt;How do prediction errors compare on the two architectures?&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/electricity_mses.png" alt="Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-15)Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Here, FNN-LSTM performs better over a long range of timesteps, but again, the difference is most visible for immediate predictions. Will an inspection of actual predictions confirm this view?&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/electricity_predictions.png" alt="60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-16)60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;It does! In fact, forecasts from FNN-LSTM are very impressive on all time scales.&lt;/p&gt;
&lt;p&gt;Now that we’ve seen the easy and predictable, let’s approach the weird and difficult.&lt;/p&gt;
&lt;h3 id="ecg-dataset"&gt;ECG dataset&lt;/h3&gt;
&lt;p&gt;Says Gilpin,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;ecg_train.pkl&lt;/code&gt; and &lt;code&gt;ecg_test.pkl&lt;/code&gt; correspond to ECG measurements for two different patients, taken from the &lt;a href="https://physionet.org/content/qtdb/1.0.0/"&gt;PhysioNet QT database&lt;/a&gt;.&lt;a href="#fn5" class="footnote-ref" id="fnref5"&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;How do these look?&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/ecg_ts.png" alt="ECG dataset. Top: First 1000 observations. Bottom: Zooming in on the first 400 observations." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-17)ECG dataset. Top: First 1000 observations. Bottom: Zooming in on the first 400 observations.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;To the layperson that I am, these do not look nearly as regular as expected. First experiments showed that both architectures are not capable of dealing with a high number of timesteps. In every try, FNN-LSTM performed better for the very first timestep.&lt;/p&gt;
&lt;p&gt;This is also the case for &lt;code&gt;n_timesteps = 12&lt;/code&gt;, the final try (after &lt;code&gt;120&lt;/code&gt;, &lt;code&gt;60&lt;/code&gt; and &lt;code&gt;30&lt;/code&gt;). With an &lt;code&gt;fnn_multiplier&lt;/code&gt; of &lt;code&gt;1&lt;/code&gt;, the latent variances obtained amounted to the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;     V1        V2          V3        V4         V5       V6       V7         V8         V9       V10
  0.110  1.16e-11     3.78e-9 0.0000992    9.63e-9  4.65e-5  1.21e-4    9.91e-9    3.81e-9   2.71e-8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There &lt;em&gt;is&lt;/em&gt; a gap between the first variable and all other ones; but not much variance is explained by &lt;code&gt;V1&lt;/code&gt; either.&lt;/p&gt;
&lt;p&gt;Apart from the very first prediction, vanilla LSTM shows lower forecast errors this time; however, we have to add that this was not consistently observed when experimenting with other timestep settings.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/ecg_mses.png" alt="Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-18)Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Looking at actual predictions, both architectures perform best when a persistence forecast is adequate – in fact, they produce one even when it is &lt;em&gt;not&lt;/em&gt;.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/ecg_predictions.png" alt="60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-19)60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;On this dataset, we certainly would want to explore other architectures better able to capture the presence of high &lt;em&gt;and&lt;/em&gt; low frequencies in the data, such as mixture models. But – were we forced to stay with one of these, and could do a one-step-ahead, rolling forecast, we’d go with FNN-LSTM.&lt;/p&gt;
&lt;p&gt;Speaking of mixed frequencies – we haven’t seen the extremes yet …&lt;/p&gt;
&lt;h3 id="mouse-dataset"&gt;Mouse dataset&lt;/h3&gt;
&lt;p&gt;“Mouse”, that’s spike rates recorded from a mouse thalamus.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;mouse.pkl&lt;/code&gt; A time series of spiking rates for a neuron in a mouse thalamus. Raw spike data was obtained from &lt;a href="http://crcns.org/data-sets/thalamus/th-1/about-th-1"&gt;CRCNS&lt;/a&gt; and processed with the authors' code in order to generate a spike rate time series.&lt;a href="#fn6" class="footnote-ref" id="fnref6"&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/mouse_ts.png" alt="Mouse dataset. Top: First 2000 observations. Bottom: Zooming in on the first 500 observations." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-20)Mouse dataset. Top: First 2000 observations. Bottom: Zooming in on the first 500 observations.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Obviously, this dataset will be very hard to predict. How, after “long” silence, do you know that a neuron is going to fire?&lt;/p&gt;
&lt;p&gt;As usual, we inspect latent code variances (&lt;code&gt;fnn_multiplier&lt;/code&gt; was set to &lt;code&gt;0.4&lt;/code&gt;):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;     V1       V2        V3         V4       V5       V6        V7      V8       V9        V10
 0.0796  0.00246  0.000214    2.26e-7   .71e-9  4.22e-8  6.45e-10 1.61e-4 2.63e-10    2.05e-8
&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, we don’t see the first variable explaining much variance. Still, interestingly, when inspecting forecast errors we get a picture very similar to the one obtained on our first, &lt;code&gt;geyser&lt;/code&gt;, dataset:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/mouse_mses.png" alt="Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-22)Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;So here, the latent code definitely seems to help! With every timestep “more” that we try to predict, prediction performance goes down &lt;em&gt;continuously&lt;/em&gt; – or put the other way round, short-time predictions are expected to be pretty good!&lt;/p&gt;
&lt;p&gt;Let’s see:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/mouse_predictions.png" alt="60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-23)60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In fact on this dataset, the difference in behavior between both architectures is striking. When nothing is “supposed to happen”, vanilla LSTM produces “flat” curves at about the mean of the data, while FNN-LSTM takes the effort to “stay on track” as long as possible before also converging to the mean. Choosing FNN-LSTM – had we to choose one of these two – would be an obvious decision with this dataset.&lt;/p&gt;
&lt;h2 id="discussion"&gt;Discussion&lt;/h2&gt;
&lt;p&gt;When, in timeseries forecasting, would we consider FNN-LSTM? Judging by the above experiments, conducted on four very different datasets: Whenever we consider a deep learning approach. Of course, this has been a casual exploration – and it was meant to be, as – hopefully – was evident from the nonchalant and bloomy (sometimes) writing style.&lt;/p&gt;
&lt;p&gt;Throughout the text, we’ve emphasized &lt;em&gt;utility&lt;/em&gt; – how could this technique be used to improve predictions? But, looking at the above results, a number of interesting questions come to mind. We already speculated (though in an indirect way) whether the number of high-variance variables in the latent code was relatable to how far we could sensibly forecast into the future. However, even more intriguing is the question of how characteristics of the &lt;em&gt;dataset itself&lt;/em&gt; affect FNN efficiency.&lt;/p&gt;
&lt;p&gt;Such characteristics could be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;How nonlinear is the dataset? (Put differently, how incompatible, as indicated by some form of test algorithm, is it with the hypothesis that the data generation mechanism was a linear one?)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To what degree does the system appear to be sensitively dependent on initial conditions? In other words, what is the value of its (estimated, from the observations) highest &lt;a href="https://en.wikipedia.org/wiki/Lyapunov_exponent"&gt;Lyapunov exponent&lt;/a&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is its (estimated) dimensionality, for example, in terms of &lt;a href="https://en.wikipedia.org/wiki/Correlation_dimension"&gt;correlation dimension&lt;/a&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While it is easy to obtain those estimates, using, for instance, the &lt;a href="https://cran.r-project.org/web/packages/nonlinearTseries/"&gt;nonlinearTseries&lt;/a&gt; package explicitly modeled after practices described in Kantz &amp;amp; Schreiber’s classic &lt;span class="citation"&gt;(Kantz and Schreiber 2004)&lt;/span&gt;, we don’t want to extrapolate from our tiny sample of datasets, and leave such explorations and analyses to further posts, and/or the interested reader’s ventures :-). In any case, we hope you enjoyed the demonstration of practical usability of an approach that in the preceding post, was mainly introduced in terms of its conceptual attractivity.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-gilpin2020deep"&gt;
&lt;p&gt;Gilpin, William. 2020. “Deep Reconstruction of Strange Attractors from Time Series.” &lt;a href="http://arxiv.org/abs/2002.05909"&gt;http://arxiv.org/abs/2002.05909&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-GRASSBERGER1983189"&gt;
&lt;p&gt;Grassberger, Peter, and Itamar Procaccia. 1983. “Measuring the Strangeness of Strange Attractors.” &lt;em&gt;Physica D: Nonlinear Phenomena&lt;/em&gt; 9 (1): 189–208. &lt;a href="https://doi.org/https://doi.org/10.1016/0167-2789(83)90298-1"&gt;https://doi.org/https://doi.org/10.1016/0167-2789(83)90298-1&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Kantz"&gt;
&lt;p&gt;Kantz, Holger, and Thomas Schreiber. 2004. &lt;em&gt;Nonlinear Time Series Analysis&lt;/em&gt;. Cambridge University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-embedology"&gt;
&lt;p&gt;Sauer, Tim, James A. Yorke, and Martin Casdagli. 1991. “Embedology.” &lt;em&gt;Journal of Statistical Physics&lt;/em&gt; 65 (3-4): 579–616. &lt;a href="https://doi.org/10.1007/BF01053745"&gt;https://doi.org/10.1007/BF01053745&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Please refer to the aforementioned predecessor post for a detailed introduction.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;“Basically” because FNN-LSTM technically has three LSTMs – the third one, with &lt;code&gt;n_latent = 10&lt;/code&gt; units, being used to store the latent code.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;see dataset descriptions in the &lt;a href="https://github.com/williamgilpin/fnn"&gt;repository's README&lt;/a&gt;&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;again, citing from Gilpin’s repository’s &lt;a href="https://github.com/williamgilpin/fnn"&gt;README&lt;/a&gt;.&lt;a href="#fnref4" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn5"&gt;&lt;p&gt;again, citing from Gilpin’s repository’s &lt;a href="https://github.com/williamgilpin/fnn"&gt;README&lt;/a&gt;.&lt;a href="#fnref5" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn6"&gt;&lt;p&gt;again, citing from Gilpin’s repository’s &lt;a href="https://github.com/williamgilpin/fnn"&gt;README&lt;/a&gt;.&lt;a href="#fnref6" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">6611909d56171d2558567c7b24918a6f</distill:md5>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm</guid>
      <pubDate>Mon, 20 Jul 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm/images/old_faithful.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Deep attractors: Where deep learning meets chaos</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors</link>
      <description>


&lt;p&gt;For us deep learning practitioners, the world is – not flat, but – linear, mostly. Or piecewise linear.&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; Like other linear approximations, or maybe even more so, deep learning can be incredibly successful at making predictions. But let’s admit it – sometimes we just miss the thrill of the nonlinear, of good, old, deterministic-yet-unpredictable chaos. Can we have both? It looks like we can. In this post, we’ll see an application of deep learning (DL) to nonlinear time series prediction – or rather, the essential step that predates it: reconstructing the attractor underlying its dynamics. While this post is an introduction, presenting the topic from scratch, further posts will build on this and extrapolate to observational datasets.&lt;/p&gt;
&lt;h3 id="what-to-expect-from-this-post"&gt;What to expect from this post&lt;/h3&gt;
&lt;p&gt;In his 2020 paper &lt;em&gt;Deep reconstruction of strange attractors from time series&lt;/em&gt; &lt;span class="citation"&gt;(Gilpin 2020)&lt;/span&gt;, William Gilpin uses an autoencoder architecture, combined with a regularizer implementing the &lt;em&gt;false nearest neighbors&lt;/em&gt; statistic &lt;span class="citation"&gt;(Kennel, Brown, and Abarbanel 1992)&lt;/span&gt;, to reconstruct attractors from univariate observations of multivariate, nonlinear dynamical systems. If you feel you completely understand the sentence you just read, you may as well directly jump to the paper – come back for the code though&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. If, on the other hand, you’re more familiar with the chaos on your desk (extrapolating … apologies) than &lt;em&gt;chaos theory chaos&lt;/em&gt;, read on. Here, we’ll first go into what it’s all about&lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;, and then, show an example application, featuring Edward Lorenz’s famous butterfly attractor. While this initial post is primarily supposed to be a fun introduction to a fascinating topic, we hope to follow up with applications to real-world datasets in the future.&lt;/p&gt;
&lt;h2 id="rabbits-butterflies-and-low-dimensional-projections-our-problem-statement-in-context"&gt;Rabbits, butterflies, and low-dimensional projections: Our problem statement in context&lt;/h2&gt;
&lt;p&gt;In curious misalignment with how we use “chaos” in day-to-day language, chaos, the technical concept, is very different from stochasticity, or randomness. Chaos may emerge from purely deterministic processes - very simplistic ones, even. Let’s see how; with rabbits.&lt;/p&gt;
&lt;h3 id="rabbits-or-sensitive-dependence-on-initial-conditions"&gt;Rabbits, or: Sensitive dependence on initial conditions&lt;/h3&gt;
&lt;p&gt;You may be familiar with the &lt;em&gt;logistic&lt;/em&gt; equation, used as a toy model for population growth. Often it’s written like this – with &lt;span class="math inline"&gt;\(x\)&lt;/span&gt; being the size of the population, expressed as a fraction of the maximal size (a fraction of possible rabbits, thus), and &lt;span class="math inline"&gt;\(r\)&lt;/span&gt; being the growth rate (the rate at which rabbits reproduce):&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
x_{n + 1} = r \ x_n \ (1 - x_n)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This equation describes an &lt;em&gt;iterated map&lt;/em&gt; over discrete timesteps &lt;span class="math inline"&gt;\(n\)&lt;/span&gt;. Its repeated application results in a &lt;em&gt;trajectory&lt;/em&gt; describing how the population of rabbits evolves. Maps can have &lt;em&gt;fixed points&lt;/em&gt;, states where further function application goes on producing the same result forever. Example-wise, say the growth rate amounts to &lt;span class="math inline"&gt;\(2.1\)&lt;/span&gt;, and we start at two (pretty different!) initial values, &lt;span class="math inline"&gt;\(0.3\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(0.8\)&lt;/span&gt;. Both trajectories arrive at a fixed point – the same fixed point – in fewer than 10 iterations. Were we asked to predict the population size after a hundred iterations, we could make a very confident guess, whatever the of starting value. (If the initial value is &lt;span class="math inline"&gt;\(0\)&lt;/span&gt;, we stay at &lt;span class="math inline"&gt;\(0\)&lt;/span&gt;, but we can be pretty certain of that as well.)&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-06-24-deep-attractors/images/single_fixedpoint.png" alt="Trajectory of the logistic map for r = 2.1 and two different initial values." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-1)Trajectory of the logistic map for r = 2.1 and two different initial values.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;What if the growth rate were somewhat higher, at &lt;span class="math inline"&gt;\(3.3\)&lt;/span&gt;, say? Again, we immediately compare trajectories resulting from initial values &lt;span class="math inline"&gt;\(0.3\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(0.9\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-06-24-deep-attractors/images/2cycle.png" alt="Trajectory of the logistic map for r = 3.3 and two different initial values." width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-2)Trajectory of the logistic map for r = 3.3 and two different initial values.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This time, don’t see a single fixed point, but a &lt;em&gt;two-cycle&lt;/em&gt;: As the trajectories stabilize, population size inevitably is at one of two possible values – either too many rabbits or too few, you could say. The two trajectories are phase-shifted, but again, the attracting values – the &lt;em&gt;attractor&lt;/em&gt; – is shared by both initial conditions. So still, predictability is pretty high. But we haven’t seen everything yet.&lt;/p&gt;
&lt;p&gt;Let’s again enhance the growth rate some. Now &lt;em&gt;this&lt;/em&gt; (literally) is chaos:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-06-24-deep-attractors/images/chaos.png" alt="Trajectory of the logistic map for r = 3.6 and two different initial values, 0.3 and 0.9." width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-3)Trajectory of the logistic map for r = 3.6 and two different initial values, 0.3 and 0.9.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Even after a hundred iterations, there is no set of values the trajectories recur to. We can’t be confident about any prediction we might make.&lt;/p&gt;
&lt;p&gt;Or can we? After all, we have the governing equation, which is deterministic. So we should be able to calculate the size of the population at, say, time &lt;span class="math inline"&gt;\(150\)&lt;/span&gt;? In principle, yes; but this presupposes we have an accurate measurement for the starting state.&lt;/p&gt;
&lt;p&gt;How accurate? Let’s compare trajectories for initial values &lt;span class="math inline"&gt;\(0.3\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(0.301\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-06-24-deep-attractors/images/chaos2.png" alt="Trajectory of the logistic map for r = 3.6 and two different initial values, 0.3 and 0.301." width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-4)Trajectory of the logistic map for r = 3.6 and two different initial values, 0.3 and 0.301.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;At first, trajectories seem to jump around in unison; but during the second dozen iterations already, they dissociate more and more, and increasingly, all bets are off. What if initial values are &lt;em&gt;really&lt;/em&gt; close, as in, &lt;span class="math inline"&gt;\(0.3\)&lt;/span&gt; vs. &lt;span class="math inline"&gt;\(0.30000001\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;It just takes a bit longer for the disassociation to surface.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-06-24-deep-attractors/images/chaos3.png" alt="Trajectory of the logistic map for r = 3.6 and two different initial values, 0.3 and 0.30000001." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-5)Trajectory of the logistic map for r = 3.6 and two different initial values, 0.3 and 0.30000001.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;What we’re seeing here is &lt;em&gt;sensitive dependence on initial conditions&lt;/em&gt;, an essential precondition for a system to be chaotic. In an nutshell: Chaos arises when a &lt;em&gt;deterministic&lt;/em&gt; system shows &lt;em&gt;sensitive dependence on initial conditions&lt;/em&gt;. Or as Edward Lorenz &lt;a href="https://en.wikipedia.org/wiki/Chaos_theory"&gt;is said to have put it&lt;/a&gt;,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When the present determines the future, but the approximate present does not approximately determine the future.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now if these unstructured, random-looking point clouds constitute chaos, what with the all-but-amorphous butterfly (to be displayed very soon)?&lt;/p&gt;
&lt;h3 id="butterflies-or-attractors-and-strange-attractors"&gt;Butterflies, or: Attractors and strange attractors&lt;/h3&gt;
&lt;p&gt;Actually, in the context of chaos theory, the term butterfly may be encountered in different contexts.&lt;/p&gt;
&lt;p&gt;Firstly, as so-called “butterfly effect”, it is an instantiation of the templatic phrase “the flap of a butterfly’s wing in _________ profoundly affects the course of the weather in _________.”&lt;a href="#fn4" class="footnote-ref" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; In this usage, it is mostly a metaphor for sensitive dependence on initial conditions.&lt;/p&gt;
&lt;p&gt;Secondly, the existence of this metaphor led to a Rorschach-test-like identification with two-dimensional visualizations of attractors of the Lorenz system. The Lorenz system is a set of three first-order differential equations designed to describe &lt;a href="https://en.wikipedia.org/wiki/Atmospheric_convection"&gt;atmospheric convection&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{aligned}
&amp;amp; \frac{dx}{dt} = \sigma (y - x)\\
&amp;amp; \frac{dy}{dt} = \rho x - x z - y\\
&amp;amp; \frac{dz}{dt} = x y - \beta z
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This set of equations is nonlinear, as required for chaotic behavior to appear. It also has the required dimensionality, which for smooth, continuous systems, is at least 3&lt;a href="#fn5" class="footnote-ref" id="fnref5"&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;. Whether we actually see chaotic attractors – among which, the butterfly – depends on the settings of the parameters &lt;span class="math inline"&gt;\(\sigma\)&lt;/span&gt;, &lt;span class="math inline"&gt;\(\rho\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt;. For the values conventionally chosen, &lt;span class="math inline"&gt;\(\sigma=10\)&lt;/span&gt;, &lt;span class="math inline"&gt;\(\rho=28\)&lt;/span&gt;, and &lt;span class="math inline"&gt;\(\beta=8/3\)&lt;/span&gt; , we see it when projecting the trajectory on the &lt;span class="math inline"&gt;\(x\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(z\)&lt;/span&gt; axes:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-06-24-deep-attractors/images/lorenz_attractors.png" alt="Two-dimensional projections of the Lorenz attractor for sigma = 10, rho = 28, beta = 8 / 3. On the right: the butterfly." width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-6)Two-dimensional projections of the Lorenz attractor for sigma = 10, rho = 28, beta = 8 / 3. On the right: the butterfly.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The butterfly is an &lt;em&gt;attractor&lt;/em&gt; (as are the other two projections), but it is neither a point nor a cycle. It is an attractor in the sense that starting from a variety of different initial values, we end up in some sub-region of the state space, and we don’t get to escape no more. This is easier to see when watching evolution over time, as in this animation:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-06-24-deep-attractors/images/x_z.gif" alt="How the Lorenz attractor traces out the famous &amp;quot;butterfly&amp;quot; shape."  /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-7)How the Lorenz attractor traces out the famous “butterfly” shape.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now, to plot the attractor in two dimensions, we threw away the third. But in “real life”, we don’t usually have too &lt;em&gt;much&lt;/em&gt; information (although it may sometimes seem like we had). We might have a lot of measurements, but these don’t usually reflect the actual state variables we’re interested in. In these cases, we may want to actually &lt;em&gt;add&lt;/em&gt; information.&lt;/p&gt;
&lt;h3 id="embeddings-as-a-non-dl-term-or-undoing-the-projection"&gt;Embeddings (as a non-DL term), or: Undoing the projection&lt;/h3&gt;
&lt;p&gt;Assume that instead of all three variables of the Lorenz system, we had measured just one: &lt;span class="math inline"&gt;\(x\)&lt;/span&gt;, the rate of convection. Often in nonlinear dynamics, the technique of delay coordinate embedding &lt;span class="citation"&gt;(Sauer, Yorke, and Casdagli 1991)&lt;/span&gt; is used to enhance a series of univariate measurements.&lt;/p&gt;
&lt;p&gt;In this method – or family of methods – the univariate series is augmented by time-shifted copies of itself. There are two decisions to be made: How many copies to add, and how big the delay should be. To illustrate, if we had a scalar series,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1 2 3 4 5 6 7 8 9 10 11 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;a three-dimensional embedding with time delay 2 would look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1 3 5
2 4 6
3 5 7
4 6 8
5 7 9
6 8 10
7 9 11
...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of the two decisions to be made – number of shifted series and time lag – the first is a decision on the dimensionality of the reconstruction space. Various theorems, such as &lt;a href="https://en.wikipedia.org/wiki/Takens%27s_theorem"&gt;Taken's theorem&lt;/a&gt;, indicate bounds on the number of dimensions required, provided the dimensionality of the true state space is known – which, in real-world applications, often is not the case.The second has been of little interest to mathematicians, but is important in practice. In fact, Kantz and Schreiber &lt;span class="citation"&gt;(Kantz and Schreiber 2004)&lt;/span&gt; argue that in practice, it is the product of both parameters that matters, as it indicates the time span represented by an embedding vector.&lt;/p&gt;
&lt;p&gt;How are these parameters chosen? Regarding reconstruction dimensionality, the reasoning goes that even in chaotic systems, points that are close in state space at time &lt;span class="math inline"&gt;\(t\)&lt;/span&gt; should still be close at time &lt;span class="math inline"&gt;\(t + \Delta t\)&lt;/span&gt;, provided &lt;span class="math inline"&gt;\(\Delta t\)&lt;/span&gt; is very small. So say we have two points that are close, by some metric, when represented in two-dimensional space. But in three dimensions, that is, if we don’t “project away” the third dimension, they are a lot more distant. As illustrated in &lt;span class="citation"&gt;(Gilpin 2020)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-06-24-deep-attractors/images/fnn.png" alt="In the two-dimensional projection on axes x and y, the red points are close neighbors. In 3d, however, they are separate. Compare with the blue points, which stay close even in higher-dimensional space. Figure from Gilpin (2020)." width="380" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-8)In the two-dimensional projection on axes x and y, the red points are close neighbors. In 3d, however, they are separate. Compare with the blue points, which stay close even in higher-dimensional space. Figure from Gilpin (2020).
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;If this happens, then projecting down has eliminated some essential information. In 2d, the points were &lt;em&gt;false neighbors&lt;/em&gt;. The &lt;em&gt;false nearest neighbors&lt;/em&gt; (FNN) statistic can be used to determine an adequate embedding size, like this:&lt;/p&gt;
&lt;p&gt;For each point, take its closest neighbor in &lt;span class="math inline"&gt;\(m\)&lt;/span&gt; dimensions, and compute the ratio of their distances in &lt;span class="math inline"&gt;\(m\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(m+1\)&lt;/span&gt; dimensions. If the ratio is larger than some threshold &lt;span class="math inline"&gt;\(t\)&lt;/span&gt;, the neighbor was false. Sum the number of false neighbors over all points. Do this for different &lt;span class="math inline"&gt;\(m\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(t\)&lt;/span&gt;, and inspect the resulting curves.&lt;/p&gt;
&lt;p&gt;At this point, let’s look ahead at the autoencoder approach. The autoencoder will use that same FNN statistic as a regularizer, in addition to the usual autoencoder reconstruction loss. This will result in a new heuristic regarding embedding dimensionality that involves fewer decisions.&lt;/p&gt;
&lt;p&gt;Going back to the classic method for an instant, the second parameter, the time lag, is even more difficult to sort out &lt;span class="citation"&gt;(Kantz and Schreiber 2004)&lt;/span&gt;. Usually, mutual information is plotted for different delays and then, the first delay where it falls below some threshold is chosen. We don’t further elaborate on this question as it is rendered obsolete in the neural network approach. Which we’ll see now.&lt;/p&gt;
&lt;h2 id="learning-the-lorenz-attractor"&gt;Learning the Lorenz attractor&lt;/h2&gt;
&lt;p&gt;Our code closely follows the architecture, parameter settings, and data setup used in the &lt;a href="https://github.com/williamgilpin/fnn"&gt;reference implementation&lt;/a&gt; William provided. The loss function, especially, has been ported one-to-one.&lt;/p&gt;
&lt;p&gt;The general idea is the following. An autoencoder – for example, an LSTM autoencoder as presented here – is used to compress the univariate time series into a latent representation of some dimensionality, which will constitute an upper bound on the dimensionality of the learned attractor. In addition to mean squared error between input and reconstructions, there will be a second loss term, applying the FNN regularizer. This results in the latent units being roughly ordered by &lt;em&gt;importance&lt;/em&gt;, as measured by their variance. It is expected that somewhere in the listing of variances, a sharp drop will appear. The units before the drop are then assumed to encode the &lt;em&gt;attractor&lt;/em&gt; of the system in question.&lt;/p&gt;
&lt;p&gt;In this setup, there is still a choice to be made: how to weight the FNN loss. One would run training for different weights &lt;span class="math inline"&gt;\(\lambda\)&lt;/span&gt; and look for the drop. Surely, this could in principle be automated, but given the newness of the method – the paper was published this year – it makes sense to focus on thorough analysis first.&lt;/p&gt;
&lt;h3 id="data-generation"&gt;Data generation&lt;/h3&gt;
&lt;p&gt;We use the &lt;code&gt;deSolve&lt;/code&gt; package to generate data from the Lorenz equations.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(deSolve)
library(tidyverse)

parameters &amp;lt;- c(sigma = 10,
                rho = 28,
                beta = 8/3)

initial_state &amp;lt;-
  c(x = -8.60632853,
    y = -14.85273055,
    z = 15.53352487)

lorenz &amp;lt;- function(t, state, parameters) {
  with(as.list(c(state, parameters)), {
    dx &amp;lt;- sigma * (y - x)
    dy &amp;lt;- x * (rho - z) - y
    dz &amp;lt;- x * y - beta * z
    
    list(c(dx, dy, dz))
  })
}

times &amp;lt;- seq(0, 500, length.out = 125000)

lorenz_ts &amp;lt;-
  ode(
    y = initial_state,
    times = times,
    func = lorenz,
    parms = parameters,
    method = &amp;quot;lsoda&amp;quot;
  ) %&amp;gt;% as_tibble()

lorenz_ts[1:10,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 10 x 4
      time      x     y     z
     &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
 1 0        -8.61 -14.9  15.5
 2 0.00400  -8.86 -15.2  15.9
 3 0.00800  -9.12 -15.6  16.3
 4 0.0120   -9.38 -16.0  16.7
 5 0.0160   -9.64 -16.3  17.1
 6 0.0200   -9.91 -16.7  17.6
 7 0.0240  -10.2  -17.0  18.1
 8 0.0280  -10.5  -17.3  18.6
 9 0.0320  -10.7  -17.7  19.1
10 0.0360  -11.0  -18.0  19.7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ve already seen the attractor, or rather, its three two-dimensional projections, in figure 6 above. But now our scenario is different. We only have access to &lt;span class="math inline"&gt;\(x\)&lt;/span&gt;, a univariate time series. As the time interval used to numerically integrate the differential equations was rather tiny, we just use every tenth observation.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;obs &amp;lt;- lorenz_ts %&amp;gt;%
  select(time, x) %&amp;gt;%
  filter(row_number() %% 10 == 0)

ggplot(obs, aes(time, x)) +
  geom_line() +
  coord_cartesian(xlim = c(0, 100)) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-06-24-deep-attractors/images/obs.png" alt="Convection rates as a univariate time series." width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-11)Convection rates as a univariate time series.
&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id="preprocessing"&gt;Preprocessing&lt;/h3&gt;
&lt;p&gt;The first half of the series is used for training. The data is scaled and transformed into the three-dimensional form expected by recurrent layers.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
library(tfdatasets)
library(tfautograph)
library(reticulate)
library(purrr)

# scale observations
obs &amp;lt;- obs %&amp;gt;% mutate(
  x = scale(x)
)

# generate timesteps
n &amp;lt;- nrow(obs)
n_timesteps &amp;lt;- 10

gen_timesteps &amp;lt;- function(x, n_timesteps) {
  do.call(rbind,
          purrr::map(seq_along(x),
             function(i) {
               start &amp;lt;- i
               end &amp;lt;- i + n_timesteps - 1
               out &amp;lt;- x[start:end]
               out
             })
  ) %&amp;gt;%
    na.omit()
}

# train with start of time series, test with end of time series 
x_train &amp;lt;- gen_timesteps(as.matrix(obs$x)[1:(n/2)], n_timesteps)
x_test &amp;lt;- gen_timesteps(as.matrix(obs$x)[(n/2):n], n_timesteps) 

# add required dimension for features (we have one)
dim(x_train) &amp;lt;- c(dim(x_train), 1)
dim(x_test) &amp;lt;- c(dim(x_test), 1)

# some batch size (value not crucial)
batch_size &amp;lt;- 100

# transform to datasets so we can use custom training
ds_train &amp;lt;- tensor_slices_dataset(x_train) %&amp;gt;%
  dataset_batch(batch_size)

ds_test &amp;lt;- tensor_slices_dataset(x_test) %&amp;gt;%
  dataset_batch(nrow(x_test))&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="autoencoder"&gt;Autoencoder&lt;/h3&gt;
&lt;p&gt;With newer versions of TensorFlow (&amp;gt;= 2.0, certainly if &amp;gt;= 2.2), autoencoder-like models are best coded as custom models, and trained in an “autographed” loop.&lt;a href="#fn6" class="footnote-ref" id="fnref6"&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The encoder is centered around a single LSTM layer, whose size determines the maximum dimensionality of the attractor. The decoder then undoes the compression – again, mainly using a single LSTM.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# size of the latent code
n_latent &amp;lt;- 10L
n_features &amp;lt;- 1

encoder_model &amp;lt;- function(n_timesteps,
                          n_features,
                          n_latent,
                          name = NULL) {
  
  keras_model_custom(name = name, function(self) {
    
    self$noise &amp;lt;- layer_gaussian_noise(stddev = 0.5)
    self$lstm &amp;lt;-  layer_lstm(
      units = n_latent,
      input_shape = c(n_timesteps, n_features),
      return_sequences = FALSE
    ) 
    self$batchnorm &amp;lt;- layer_batch_normalization()
    
    function (x, mask = NULL) {
      x %&amp;gt;%
        self$noise() %&amp;gt;%
        self$lstm() %&amp;gt;%
        self$batchnorm() 
    }
  })
}

decoder_model &amp;lt;- function(n_timesteps,
                          n_features,
                          n_latent,
                          name = NULL) {
  
  keras_model_custom(name = name, function(self) {
    
    self$repeat_vector &amp;lt;- layer_repeat_vector(n = n_timesteps)
    self$noise &amp;lt;- layer_gaussian_noise(stddev = 0.5)
    self$lstm &amp;lt;- layer_lstm(
        units = n_latent,
        return_sequences = TRUE,
        go_backwards = TRUE
      ) 
    self$batchnorm &amp;lt;- layer_batch_normalization()
    self$elu &amp;lt;- layer_activation_elu() 
    self$time_distributed &amp;lt;- time_distributed(layer = layer_dense(units = n_features))
    
    function (x, mask = NULL) {
      x %&amp;gt;%
        self$repeat_vector() %&amp;gt;%
        self$noise() %&amp;gt;%
        self$lstm() %&amp;gt;%
        self$batchnorm() %&amp;gt;%
        self$elu() %&amp;gt;%
        self$time_distributed()
    }
  })
}


encoder &amp;lt;- encoder_model(n_timesteps, n_features, n_latent)
decoder &amp;lt;- decoder_model(n_timesteps, n_features, n_latent)&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="loss"&gt;Loss&lt;/h3&gt;
&lt;p&gt;As already explained above, the loss function we train with is twofold. On the one hand, we compare the original inputs with the decoder outputs (the reconstruction), using mean squared error:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;mse_loss &amp;lt;- tf$keras$losses$MeanSquaredError(
  reduction = tf$keras$losses$Reduction$SUM)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In addition, we try to keep the number of false neighbors small, by means of the following regularizer.&lt;a href="#fn7" class="footnote-ref" id="fnref7"&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;loss_false_nn &amp;lt;- function(x) {
 
  # original values used in Kennel et al. (1992)
  rtol &amp;lt;- 10 
  atol &amp;lt;- 2
  k_frac &amp;lt;- 0.01
  
  k &amp;lt;- max(1, floor(k_frac * batch_size))
  
  tri_mask &amp;lt;-
    tf$linalg$band_part(
      tf$ones(
        shape = c(n_latent, n_latent),
        dtype = tf$float32
      ),
      num_lower = -1L,
      num_upper = 0L
    )
  
   batch_masked &amp;lt;- tf$multiply(
     tri_mask[, tf$newaxis,], x[tf$newaxis, reticulate::py_ellipsis()]
   )
  
  x_squared &amp;lt;- tf$reduce_sum(
    batch_masked * batch_masked,
    axis = 2L,
    keepdims = TRUE
  )

  pdist_vector &amp;lt;- x_squared +
  tf$transpose(
    x_squared, perm = c(0L, 2L, 1L)
  ) -
  2 * tf$matmul(
    batch_masked,
    tf$transpose(batch_masked, perm = c(0L, 2L, 1L))
  )

  all_dists &amp;lt;- pdist_vector
  all_ra &amp;lt;-
    tf$sqrt((1 / (
      batch_size * tf$range(1, 1 + n_latent, dtype = tf$float32)
    )) *
      tf$reduce_sum(tf$square(
        batch_masked - tf$reduce_mean(batch_masked, axis = 1L, keepdims = TRUE)
      ), axis = c(1L, 2L)))
  
  all_dists &amp;lt;- tf$clip_by_value(all_dists, 1e-14, tf$reduce_max(all_dists))

  top_k &amp;lt;- tf$math$top_k(-all_dists, tf$cast(k + 1, tf$int32))
  top_indices &amp;lt;- top_k[[1]]

  neighbor_dists_d &amp;lt;- tf$gather(all_dists, top_indices, batch_dims = -1L)
  
  neighbor_new_dists &amp;lt;- tf$gather(
    all_dists[2:-1, , ],
    top_indices[1:-2, , ],
    batch_dims = -1L
  )
  
  # Eq. 4 of Kennel et al. (1992)
  scaled_dist &amp;lt;- tf$sqrt((
    tf$square(neighbor_new_dists) -
      tf$square(neighbor_dists_d[1:-2, , ])) /
      tf$square(neighbor_dists_d[1:-2, , ])
  )
  
  # Kennel condition #1
  is_false_change &amp;lt;- (scaled_dist &amp;gt; rtol)
  # Kennel condition #2
  is_large_jump &amp;lt;-
    (neighbor_new_dists &amp;gt; atol * all_ra[1:-2, tf$newaxis, tf$newaxis])
  
  is_false_neighbor &amp;lt;-
    tf$math$logical_or(is_false_change, is_large_jump)
  
  total_false_neighbors &amp;lt;-
    tf$cast(is_false_neighbor, tf$int32)[reticulate::py_ellipsis(), 2:(k + 2)]
  
  reg_weights &amp;lt;- 1 -
    tf$reduce_mean(tf$cast(total_false_neighbors, tf$float32), axis = c(1L, 2L))
  reg_weights &amp;lt;- tf$pad(reg_weights, list(list(1L, 0L)))
  
  activations_batch_averaged &amp;lt;-
    tf$sqrt(tf$reduce_mean(tf$square(x), axis = 0L))
  
  loss &amp;lt;- tf$reduce_sum(tf$multiply(reg_weights, activations_batch_averaged))
  loss
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;MSE and FNN are added , with FNN loss weighted according to the essential hyperparameter of this model:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;fnn_weight &amp;lt;- 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This value was experimentally chosen as the one best conforming to our &lt;em&gt;look-for-the-highest-drop&lt;/em&gt; heuristic.&lt;/p&gt;
&lt;h3 id="model-training"&gt;Model training&lt;/h3&gt;
&lt;p&gt;The training loop closely follows the aforementioned &lt;a href="https://tensorflow.rstudio.com/tutorials/advanced/"&gt;recipe&lt;/a&gt; on how to train with custom models and &lt;code&gt;tfautograph&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_loss &amp;lt;- tf$keras$metrics$Mean(name=&amp;#39;train_loss&amp;#39;)
train_fnn &amp;lt;- tf$keras$metrics$Mean(name=&amp;#39;train_fnn&amp;#39;)
train_mse &amp;lt;-  tf$keras$metrics$Mean(name=&amp;#39;train_mse&amp;#39;)

train_step &amp;lt;- function(batch) {
  
  with (tf$GradientTape(persistent = TRUE) %as% tape, {
    
    code &amp;lt;- encoder(batch)
    reconstructed &amp;lt;- decoder(code)
    
    l_mse &amp;lt;- mse_loss(batch, reconstructed)
    l_fnn &amp;lt;- loss_false_nn(code)
    loss &amp;lt;- l_mse + fnn_weight * l_fnn
    
  })
  
  encoder_gradients &amp;lt;- tape$gradient(loss, encoder$trainable_variables)
  decoder_gradients &amp;lt;- tape$gradient(loss, decoder$trainable_variables)
  
  optimizer$apply_gradients(
    purrr::transpose(list(encoder_gradients, encoder$trainable_variables))
  )
  optimizer$apply_gradients(
    purrr::transpose(list(decoder_gradients, decoder$trainable_variables))
  )
  
  train_loss(loss)
  train_mse(l_mse)
  train_fnn(l_fnn)
}

training_loop &amp;lt;- tf_function(autograph(function(ds_train) {
  
  for (batch in ds_train) {
    train_step(batch)
  }
  
  tf$print(&amp;quot;Loss: &amp;quot;, train_loss$result())
  tf$print(&amp;quot;MSE: &amp;quot;, train_mse$result())
  tf$print(&amp;quot;FNN loss: &amp;quot;, train_fnn$result())
  
  train_loss$reset_states()
  train_mse$reset_states()
  train_fnn$reset_states()
  
}))

optimizer &amp;lt;- optimizer_adam(lr = 1e-3)

for (epoch in 1:200) {
  cat(&amp;quot;Epoch: &amp;quot;, epoch, &amp;quot; -----------\n&amp;quot;)
  training_loop(ds_train)  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After two hundred epochs, overall loss is at 2.67, with the MSE component at 1.8 and FNN at 0.09.&lt;/p&gt;
&lt;h3 id="obtaining-the-attractor-from-the-test-set"&gt;Obtaining the attractor from the test set&lt;/h3&gt;
&lt;p&gt;We use the test set to inspect the latent code:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;test_batch &amp;lt;- as_iterator(ds_test) %&amp;gt;% iter_next()
predicted &amp;lt;- encoder(test_batch) %&amp;gt;%
  as.array(predicted) %&amp;gt;%
  as_tibble()

predicted&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 6,242 x 10
      V1    V2         V3        V4        V5         V6        V7        V8       V9       V10
   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
 1 0.439 0.401 -0.000614  -0.0258   -0.00176  -0.0000276  0.000276  0.00677  -0.0239   0.00906 
 2 0.415 0.504  0.0000481 -0.0279   -0.00435  -0.0000970  0.000921  0.00509  -0.0214   0.00921 
 3 0.389 0.619  0.000848  -0.0240   -0.00661  -0.000171   0.00106   0.00454  -0.0150   0.00794 
 4 0.363 0.729  0.00137   -0.0143   -0.00652  -0.000244   0.000523  0.00450  -0.00594  0.00476 
 5 0.335 0.809  0.00128   -0.000450 -0.00338  -0.000307  -0.000561  0.00407   0.00394 -0.000127
 6 0.304 0.828  0.000631   0.0126    0.000889 -0.000351  -0.00167   0.00250   0.0115  -0.00487 
 7 0.274 0.769 -0.000202   0.0195    0.00403  -0.000367  -0.00220  -0.000308  0.0145  -0.00726 
 8 0.246 0.657 -0.000865   0.0196    0.00558  -0.000359  -0.00208  -0.00376   0.0134  -0.00709 
 9 0.224 0.535 -0.00121    0.0162    0.00608  -0.000335  -0.00169  -0.00697   0.0106  -0.00576 
10 0.211 0.434 -0.00129    0.0129    0.00606  -0.000306  -0.00134  -0.00927   0.00820 -0.00447 
# … with 6,232 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a result of the FNN regularizer, the latent code units should be ordered roughly by decreasing variance, with a sharp drop appearing some place (if the FNN weight has been chosen adequately).&lt;/p&gt;
&lt;p&gt;For an &lt;code&gt;fnn_weight&lt;/code&gt; of 10, we do see a drop after the first two units:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;predicted %&amp;gt;% summarise_all(var)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1 x 10
      V1     V2      V3      V4      V5      V6      V7      V8      V9     V10
   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
1 0.0739 0.0582 1.12e-6 3.13e-4 1.43e-5 1.52e-8 1.35e-6 1.86e-4 1.67e-4 4.39e-5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the model indicates that the Lorenz attractor can be represented in two dimensions. If we nonetheless want to plot the complete (reconstructed) state space of three dimensions, we should reorder the remaining variables by magnitude of variance&lt;a href="#fn8" class="footnote-ref" id="fnref8"&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;. Here, this results in three projections of the set &lt;code&gt;V1&lt;/code&gt;, &lt;code&gt;V2&lt;/code&gt; and &lt;code&gt;V4&lt;/code&gt;:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-06-24-deep-attractors/images/predicted_attractors.png" alt="Attractors as predicted from the latent code (test set). The three highest-variance variables were chosen." width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-20)Attractors as predicted from the latent code (test set). The three highest-variance variables were chosen.
&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="wrapping-up-for-this-time"&gt;Wrapping up (for this time)&lt;/h2&gt;
&lt;p&gt;At this point, we’ve seen how to reconstruct the Lorenz attractor from data we did not train on (the test set), using an autoencoder regularized by a custom &lt;em&gt;false nearest neighbors&lt;/em&gt; loss. It is important to stress that at no point was the network presented with the expected solution (attractor) – training was purely unsupervised.&lt;/p&gt;
&lt;p&gt;This is a fascinating result. Of course, thinking practically, the next step is to obtain predictions on heldout data. Given how long this text has become already, we reserve that for a follow-up post. And again &lt;em&gt;of course&lt;/em&gt;, we’re thinking about other datasets, especially ones where the true state space is not known beforehand. What about measurement noise? What about datasets that are not completely deterministic&lt;a href="#fn9" class="footnote-ref" id="fnref9"&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;? There is a lot to explore, stay tuned – and as always, thanks for reading!&lt;/p&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-gilpin2020deep"&gt;
&lt;p&gt;Gilpin, William. 2020. “Deep Reconstruction of Strange Attractors from Time Series.” &lt;a href="http://arxiv.org/abs/2002.05909"&gt;http://arxiv.org/abs/2002.05909&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Kantz"&gt;
&lt;p&gt;Kantz, Holger, and Thomas Schreiber. 2004. &lt;em&gt;Nonlinear Time Series Analysis&lt;/em&gt;. Cambridge University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-PhysRevA.45.3403"&gt;
&lt;p&gt;Kennel, Matthew B., Reggie Brown, and Henry D. I. Abarbanel. 1992. “Determining Embedding Dimension for Phase-Space Reconstruction Using a Geometrical Construction.” &lt;em&gt;Phys. Rev. A&lt;/em&gt; 45 (6): 3403–11. &lt;a href="https://doi.org/10.1103/PhysRevA.45.3403"&gt;https://doi.org/10.1103/PhysRevA.45.3403&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-embedology"&gt;
&lt;p&gt;Sauer, Tim, James A. Yorke, and Martin Casdagli. 1991. “Embedology.” &lt;em&gt;Journal of Statistical Physics&lt;/em&gt; 65 (3-4): 579–616. &lt;a href="https://doi.org/10.1007/BF01053745"&gt;https://doi.org/10.1007/BF01053745&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Gilbert"&gt;
&lt;p&gt;Strang, Gilbert. 2019. &lt;em&gt;Linear Algebra and Learning from Data&lt;/em&gt;. Wellesley Cambridge Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Strogatz"&gt;
&lt;p&gt;Strogatz, Steven. 2015. &lt;em&gt;Nonlinear Dynamics and Chaos: With Applications to Physics, Biology, Chemistry, and Engineering&lt;/em&gt;. Westview Press.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;For many popular activation functions at least (such as ReLU). See e.g. &lt;span class="citation"&gt;(Strang 2019)&lt;/span&gt;.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;The paper is also accompanied by a &lt;a href="https://github.com/williamgilpin/fnn"&gt;Python implementation&lt;/a&gt;.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;To people who want to learn more about this topic, the usual recommendation is &lt;span class="citation"&gt;(Strogatz 2015)&lt;/span&gt;. Personally I prefer another source, which I can’t recommend highly enough: Santa Fe Institute’s &lt;a href="https://www.complexityexplorer.org/courses/100-nonlinear-dynamics-mathematical-and-computational-approaches"&gt;Nonlinear Dynamics: Mathematical and Computational Approaches&lt;/a&gt;, taught by Liz Bradley.&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;See e.g. &lt;a href="https://en.wikipedia.org/wiki/Butterfly_effect"&gt;Wikipedia&lt;/a&gt; for some history and links to sources.&lt;a href="#fnref4" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn5"&gt;&lt;p&gt;In discrete systems, like the logistic map, a single dimension is enough.&lt;a href="#fnref5" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn6"&gt;&lt;p&gt;See the &lt;a href="https://tensorflow.rstudio.com/tutorials/advanced/"&gt;custom training tutorial&lt;/a&gt; for a blueprint.&lt;a href="#fnref6" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn7"&gt;&lt;p&gt;See the appendix of &lt;span class="citation"&gt;(Gilpin 2020)&lt;/span&gt; for a pseudocode-like documentation.&lt;a href="#fnref7" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn8"&gt;&lt;p&gt;As per author recommendation (personal communication).&lt;a href="#fnref8" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn9"&gt;&lt;p&gt;See &lt;span class="citation"&gt;(Kantz and Schreiber 2004)&lt;/span&gt; for detailed discussions on using methodology from nonlinear deterministic systems analysis for noisy and/or partly-stochastic data.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;a href="#fnref9" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">9ffa8ff9d7baa88fa6d32d9bdc6c28d4</distill:md5>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors</guid>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors/images/x_z.gif" medium="image" type="image/gif"/>
    </item>
    <item>
      <title>Easy PixelCNN with tfprobability</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn</link>
      <description>


&lt;p&gt;We’ve seen quite a few examples of unsupervised learning (or self-supervised learning, to choose the more correct but less popular term) on this blog.&lt;/p&gt;
&lt;p&gt;Often, these involved &lt;em&gt;Variational Autoencoders (VAEs)&lt;/em&gt;, whose appeal lies in them allowing to model a &lt;em&gt;latent space&lt;/em&gt; of underlying, independent (preferably) factors that determine the visible features. A possible downside can be the inferior quality of generated samples. Generative Adversarial Networks (GANs) are another popular approach. Conceptually, these are highly attractive due to their game-theoretic framing. However, they can be difficult to train. &lt;em&gt;PixelCNN&lt;/em&gt; variants, on the other hand – we’ll subsume them all here under PixelCNN – are generally known for their good results. They seem to involve some more alchemy&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; though. Under those circumstances, what could be more welcome than an easy way of experimenting with them? Through TensorFlow Probability (TFP) and its R wrapper, &lt;a href="https://github.io/tfprobability"&gt;tfprobability&lt;/a&gt;, we now have such a way.&lt;/p&gt;
&lt;p&gt;This post first gives an introduction to PixelCNN, concentrating on high-level concepts (leaving the details for the curious to look them up in the respective papers). We’ll then show an example of using &lt;code&gt;tfprobability&lt;/code&gt; to experiment with the TFP implementation.&lt;/p&gt;
&lt;h2 id="pixelcnn-principles"&gt;PixelCNN principles&lt;/h2&gt;
&lt;h3 id="autoregressivity-or-we-need-some-order"&gt;Autoregressivity, or: We need (some) order&lt;/h3&gt;
&lt;p&gt;The basic idea in PixelCNN is autoregressivity. Each pixel is modeled as depending on all prior pixels. Formally:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[p(\mathbf{x}) = \prod_{i}p(x_i|x_0, x_1, ..., x_{i-1})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now wait a second - what even &lt;em&gt;are&lt;/em&gt; prior pixels? Last I saw one images were two-dimensional. So this means we have to impose an &lt;em&gt;order&lt;/em&gt; on the pixels. Commonly this will be &lt;em&gt;raster scan&lt;/em&gt; order: row after row, from left to right. But when dealing with color images, there’s something else: At each position, we actually have &lt;em&gt;three&lt;/em&gt; intensity values, one for each of red, green, and blue. The original PixelCNN paper&lt;span class="citation"&gt;(Oord, Kalchbrenner, and Kavukcuoglu 2016)&lt;/span&gt; carried through autoregressivity here as well, with a pixel’s intensity for red depending on just prior pixels, those for green depending on these same prior pixels but additionally, the current value for red, and those for blue depending on the prior pixels as well as the current values for red and green.&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[p(x_i|\mathbf{x}&amp;lt;i) = p(x_{i,R}|\mathbf{x}&amp;lt;i)\ p(x_{i,G}|\mathbf{x}&amp;lt;i, x_{i,R})\ p(x_{i,B}|\mathbf{x}&amp;lt;i, x_{i,R}, x_{i,G})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, the variant implemented in TFP, PixelCNN++&lt;span class="citation"&gt;(Salimans et al. 2017)&lt;/span&gt; , introduces a simplification; it factorizes the joint distribution in a less compute-intensive way.&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Technically, then, we know how autoregressivity is realized; intuitively, it may still seem surprising that imposing a raster scan order “just works” (to me, at least, it is). Maybe this is one of those points where compute power successfully compensates for lack of an equivalent of a cognitive prior.&lt;/p&gt;
&lt;h3 id="masking-or-where-not-to-look"&gt;Masking, or: Where not to look&lt;/h3&gt;
&lt;p&gt;Now, PixelCNN ends in “CNN” for a reason – as usual in image processing, convolutional layers (or blocks thereof) are involved. But – is it not the very nature of a convolution that it computes an average of some sorts, looking, for each output pixel, not just at the corresponding input but also, at its spatial (or temporal) surroundings? How does that rhyme with the look-at-just-prior-pixels strategy?&lt;/p&gt;
&lt;p&gt;Surprisingly, this problem is easier to solve than it sounds. When applying the convolutional kernel, just multiply with a mask that zeroes out any “forbidden pixels” – like in this example for a 5x5 kernel, where we’re about to compute the convolved value for row 3, column 3:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\left[\begin{array}
{rrr}
1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1\\
1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1\\
1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
\end{array}\right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This makes the algorithm honest, but introduces a different problem: With each successive convolutional layer consuming its predecessor’s output, there is a continuously growing &lt;em&gt;blind spot&lt;/em&gt; (so-called in analogy to the blind spot on the retina, but located in the top right) of pixels that are never &lt;em&gt;seen&lt;/em&gt; by the algorithm. Van den Oord et al. (2016)&lt;span class="citation"&gt;(Oord et al. 2016)&lt;/span&gt; fix this by using two different convolutional stacks, one proceeding from top to bottom, the other from left to right&lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-29-pixelcnn/images/stacks.png" alt="" /&gt;
&lt;p class="caption"&gt;Fig. 1: Left: Blind spot, growing over layers. Right: Using two different stacks (a vertical and a horizontal one) solves the problem. Source: van den Oord et al., 2016.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id="conditioning-or-show-me-a-kitten"&gt;Conditioning, or: Show me a kitten&lt;/h3&gt;
&lt;p&gt;So far, we’ve always talked about “generating images” in a purely generic way. But the real attraction lies in creating samples of some specified type – one of the classes we’ve been training on, or orthogonal information fed into the network. This is where PixelCNN becomes &lt;em&gt;Conditional PixelCNN&lt;/em&gt;&lt;span class="citation"&gt;(Oord et al. 2016)&lt;/span&gt;, and it is also where that feeling of magic resurfaces. Again, as “general math” it’s not hard to conceive. Here, &lt;span class="math inline"&gt;\(\mathbf{h}\)&lt;/span&gt; is the additional input we’re conditioning on:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[p(\mathbf{x}| \mathbf{h}) = \prod_{i}p(x_i|x_0, x_1, ..., x_{i-1}, \mathbf{h})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;But how does this translate into neural network operations? It’s just another matrix multiplication (&lt;span class="math inline"&gt;\(V^T \mathbf{h}\)&lt;/span&gt;) added to the convolutional outputs (&lt;span class="math inline"&gt;\(W \mathbf{x}\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\mathbf{y} = tanh(W_{k,f} \mathbf{x} + V^T_{k,f} \mathbf{h}) \odot \sigma(W_{k,g} \mathbf{x} + V^T_{k,g} \mathbf{h})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;(If you’re wondering about the second part on the right, after the Hadamard product sign – we won’t go into details, but in a nutshell, it’s another modification introduced by &lt;span class="citation"&gt;(Oord et al. 2016)&lt;/span&gt;, a transfer of the “gating” principle from recurrent neural networks, such as GRUs and LSTMs, to the convolutional setting.)&lt;/p&gt;
&lt;p&gt;So we see what goes into the decision of a pixel value to sample. But how is that decision actually &lt;em&gt;made&lt;/em&gt;?&lt;/p&gt;
&lt;h3 id="logistic-mixture-likelihood-or-no-pixel-is-an-island"&gt;Logistic mixture likelihood , or: No pixel is an island&lt;/h3&gt;
&lt;p&gt;Again, this is where the TFP implementation does not follow the original paper, but the latter PixelCNN++ one. Originally, pixels were modeled as discrete values, decided on by a softmax over 256 (0-255) possible values. (That this actually worked seems like another instance of deep learning magic. Imagine: In this model, 254 is as far from 255 as it is from 0.)&lt;/p&gt;
&lt;p&gt;In contrast, PixelCNN++ assumes an underlying continuous distribution of color intensity, and rounds to the nearest integer. That underlying distribution is a mixture of logistic distributions, thus allowing for multimodality:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\nu \sim \sum_{i} \pi_i \ logistic(\mu_i, \sigma_i)\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id="overall-architecture-and-the-pixelcnn-distribution"&gt;Overall architecture and the PixelCNN distribution&lt;/h3&gt;
&lt;p&gt;Overall, PixelCNN++, as described in &lt;span class="citation"&gt;(Salimans et al. 2017)&lt;/span&gt;, consists of six blocks. The blocks together make up a UNet-like structure, successively downsizing the input and then, upsampling again:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-29-pixelcnn/images/high-level.png" alt="" /&gt;
&lt;p class="caption"&gt;Fig. 2: Overall structure of PixelCNN++. From: Salimans et al., 2017.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In TFP’s PixelCNN distribution, the number of blocks is configurable as &lt;code&gt;num_hierarchies&lt;/code&gt;, the default being 3.&lt;/p&gt;
&lt;p&gt;Each block consists of a customizable number of layers, called &lt;em&gt;ResNet layers&lt;/em&gt; due to the residual connection (visible on the right) complementing the convolutional operations in the horizontal stack:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-29-pixelcnn/images/layer.png" alt="" /&gt;
&lt;p class="caption"&gt;Fig. 3: One so-called "ResNet layer", featuring both a vertical and a horizontal convolutional stack. Source: van den Oord et al., 2017.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In TFP, the number of these layers per block is configurable as &lt;code&gt;num_resnet&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;num_resnet&lt;/code&gt; and &lt;code&gt;num_hierarchies&lt;/code&gt; are the parameters you’re most likely to experiment with, but there are a few more you can check out in the &lt;a href="https://rstudio.github.io/tfprobability/reference/tfd_pixel_cnn.html"&gt;documentation&lt;/a&gt;. The number of logistic distributions in the mixture is also configurable, but from my experiments it’s best to keep that number rather low to avoid producing &lt;code&gt;NaN&lt;/code&gt;s during training.&lt;/p&gt;
&lt;p&gt;Let’s now see a complete example.&lt;/p&gt;
&lt;h2 id="end-to-end-example"&gt;End-to-end example&lt;/h2&gt;
&lt;p&gt;Our playground will be &lt;a href="https://github.com/googlecreativelab/quickdraw-dataset"&gt;QuickDraw&lt;/a&gt;, a dataset – still growing – obtained by asking people to draw some object in at most twenty seconds, using the mouse. (To see for yourself, just check out the &lt;a href="https://quickdraw.withgoogle.com/"&gt;website&lt;/a&gt;). As of today, there are more than a fifty million instances, from 345 different classes.&lt;/p&gt;
&lt;p&gt;First and foremost, these data were chosen to take a break from MNIST and its variants. But just like those (and many more!), QuickDraw can be obtained, in &lt;code&gt;tfdatasets&lt;/code&gt;-ready form, via &lt;a href="https://github.com/rstudio/tfds"&gt;tfds&lt;/a&gt;, the R wrapper to TensorFlow datasets. In contrast to the MNIST “family” though, the “real samples” are themselves highly irregular, and often even missing essential parts. So to anchor judgment, when displaying generated samples we always show eight actual drawings with them.&lt;/p&gt;
&lt;h3 id="preparing-the-data"&gt;Preparing the data&lt;/h3&gt;
&lt;p&gt;The dataset being gigantic, we instruct &lt;code&gt;tfds&lt;/code&gt; to load the first 500,000 drawings “only”.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(reticulate)

# &amp;gt;= 2.2 required
library(tensorflow)
library(keras)

# make sure to use at least version 0.10
library(tfprobability)

library(tfdatasets)
# currently to be installed from github
library(tfds)

# load just the first 500,000 images
# nonetheless, initially the complete dataset will be downloaded and unpacked
# ... be prepared for this to take some time
train_ds &amp;lt;- tfds_load(&amp;quot;quickdraw_bitmap&amp;quot;, split=&amp;#39;train[:500000]&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To speed up training further, we then zoom in on twenty classes. This effectively leaves us with ~ 1,100 - 1,500 drawings per class.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# bee, bicycle, broccoli, butterfly, cactus,
# frog, guitar, lightning, penguin, pizza,
# rollerskates, sea turtle, sheep, snowflake, sun,
# swan, The Eiffel Tower, tractor, train, tree
classes &amp;lt;- c(26, 29, 43, 49, 50,
             125, 134, 172, 218, 225,
             246, 255, 258, 271, 295,
             296, 308, 320, 322, 323
)

classes_tensor &amp;lt;- tf$cast(classes, tf$int64)

train_ds &amp;lt;- train_ds %&amp;gt;%
  dataset_filter(
    function(record) tf$reduce_any(tf$equal(classes_tensor, record$label), -1L)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The PixelCNN distribution expects values in the range from 0 to 255 – no normalization required. Preprocessing then consists of just casting pixels and labels each to &lt;code&gt;float&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;preprocess &amp;lt;- function(record) {
  record$image &amp;lt;- tf$cast(record$image, tf$float32) 
  record$label &amp;lt;- tf$cast(record$label, tf$float32)
  list(tuple(record$image, record$label))
}

batch_size &amp;lt;- 32

train &amp;lt;- train_ds %&amp;gt;%
  dataset_map(preprocess) %&amp;gt;%
  dataset_shuffle(10000) %&amp;gt;%
  dataset_batch(batch_size)&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="creating-the-model"&gt;Creating the model&lt;/h3&gt;
&lt;p&gt;We now use &lt;a href="https://rstudio.github.io/tfprobability/reference/tfd_pixel_cnn.html"&gt;tfd_pixel_cnn&lt;/a&gt; to define what will be the loglikelihood used by the model.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;dist &amp;lt;- tfd_pixel_cnn(
  image_shape = c(28, 28, 1),
  conditional_shape = list(),
  num_resnet = 5,
  num_hierarchies = 3,
  num_filters = 128,
  num_logistic_mix = 5,
  dropout_p =.5
)

image_input &amp;lt;- layer_input(shape = c(28, 28, 1))
label_input &amp;lt;- layer_input(shape = list())
log_prob &amp;lt;- dist %&amp;gt;% tfd_log_prob(image_input, conditional_input = label_input)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This custom loglikelihood is added as a loss to the model, and then, the model is compiled with just an optimizer specification only. During training, loss first decreased quickly, but improvements from later epochs were smaller.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model(inputs = list(image_input, label_input), outputs = log_prob)
model$add_loss(-tf$reduce_mean(log_prob))
model$compile(optimizer = optimizer_adam(lr = .001))

model %&amp;gt;% fit(train, epochs = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To jointly display real and fake images:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;for (i in classes) {
  
  real_images &amp;lt;- train_ds %&amp;gt;%
    dataset_filter(
      function(record) record$label == tf$cast(i, tf$int64)
    ) %&amp;gt;% 
    dataset_take(8) %&amp;gt;%
    dataset_batch(8)
  it &amp;lt;- as_iterator(real_images)
  real_images &amp;lt;- iter_next(it)
  real_images &amp;lt;- real_images$image %&amp;gt;% as.array()
  real_images &amp;lt;- real_images[ , , , 1]/255
  
  generated_images &amp;lt;- dist %&amp;gt;% tfd_sample(8, conditional_input = i)
  generated_images &amp;lt;- generated_images %&amp;gt;% as.array()
  generated_images &amp;lt;- generated_images[ , , , 1]/255
  
  images &amp;lt;- abind::abind(real_images, generated_images, along = 1)
  png(paste0(&amp;quot;draw_&amp;quot;, i, &amp;quot;.png&amp;quot;), width = 8 * 28 * 10, height = 2 * 28 * 10)
  par(mfrow = c(2, 8), mar = c(0, 0, 0, 0))
  images %&amp;gt;%
    purrr::array_tree(1) %&amp;gt;%
    purrr::map(as.raster) %&amp;gt;%
    purrr::iwalk(plot)
  dev.off()
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From our twenty classes, here’s a choice of six, each showing real drawings in the top row, and fake ones below.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-29-pixelcnn/images/draw_29.png" alt="" /&gt;
&lt;p class="caption"&gt;Fig. 4: Bicycles, drawn by people (top row) and the network (bottom row).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-29-pixelcnn/images/draw_43.png" alt="" /&gt;
&lt;p class="caption"&gt;Fig. 5: Broccoli, drawn by people (top row) and the network (bottom row).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-29-pixelcnn/images/draw_49.png" alt="" /&gt;
&lt;p class="caption"&gt;Fig. 6: Butterflies, drawn by people (top row) and the network (bottom row).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-29-pixelcnn/images/draw_134.png" alt="" /&gt;
&lt;p class="caption"&gt;Fig. 7: Guitars, drawn by people (top row) and the network (bottom row).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-29-pixelcnn/images/draw_218.png" alt="" /&gt;
&lt;p class="caption"&gt;Fig. 8: Penguins, drawn by people (top row) and the network (bottom row).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-29-pixelcnn/images/draw_246.png" alt="" /&gt;
&lt;p class="caption"&gt;Fig. 9: Roller skates, drawn by people (top row) and the network (bottom row).&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We probably wouldn’t confuse the first and second rows, but then, the actual human drawings exhibit enormous variation, too. And no one ever said PixelCNN was an architecture for concept learning. Feel free to play around with other datasets of your choice – TFP’s PixelCNN distribution makes it easy.&lt;/p&gt;
&lt;h2 id="wrapping-up"&gt;Wrapping up&lt;/h2&gt;
&lt;p&gt;In this post, we had &lt;code&gt;tfprobability&lt;/code&gt; / TFP do all the heavy lifting for us, and so, could focus on the underlying concepts. Depending on your inclinations, this can be an ideal situation – you don’t lose sight of the forest for the trees. On the other hand: Should you find that changing the provided parameters doesn’t achieve what you want, you have a reference implementation to start from. So whatever the outcome, the addition of such higher-level functionality to TFP is a win for the users. (If you’re a TFP developer reading this: Yes, we’d like more :-)).&lt;/p&gt;
&lt;p&gt;To everyone though, thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-OordKK16"&gt;
&lt;p&gt;Oord, Aaron van den, Nal Kalchbrenner, and Koray Kavukcuoglu. 2016. “Pixel Recurrent Neural Networks.” &lt;em&gt;CoRR&lt;/em&gt; abs/1601.06759. &lt;a href="http://arxiv.org/abs/1601.06759"&gt;http://arxiv.org/abs/1601.06759&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-OordKVEGK16"&gt;
&lt;p&gt;Oord, Aaron van den, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. 2016. “Conditional Image Generation with Pixelcnn Decoders.” &lt;em&gt;CoRR&lt;/em&gt; abs/1606.05328. &lt;a href="http://arxiv.org/abs/1606.05328"&gt;http://arxiv.org/abs/1606.05328&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Salimans2017PixeCNN"&gt;
&lt;p&gt;Salimans, Tim, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. 2017. “PixelCNN++: A Pixelcnn Implementation with Discretized Logistic Mixture Likelihood and Other Modifications.” In &lt;em&gt;ICLR&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Alluding to Ali Rahimi’s (in)famous “deep learning is alchemy” &lt;a href="https://www.youtube.com/watch?v=Qi1Yry33TQE"&gt;talk&lt;/a&gt; at NeurIPS 2017. I would suspect that to some degree, that statement resonates with many DL practitioners – although one need not agree that more mathematical rigor is the solution.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;For details, see &lt;span class="citation"&gt;(Salimans et al. 2017)&lt;/span&gt;.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;.For details, see &lt;span class="citation"&gt;(Oord et al. 2016)&lt;/span&gt;.&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">c31e1544cc97c983f93b63438fe1899f</distill:md5>
      <category>R</category>
      <category>Image Recognition &amp; Image Processing</category>
      <category>TensorFlow/Keras</category>
      <category>Probabilistic ML/DL</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn</guid>
      <pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn/images/thumb.png" medium="image" type="image/png" width="400" height="203"/>
    </item>
    <item>
      <title>Hacking deep learning: model inversion attack by example</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks</link>
      <description>


&lt;p&gt;How private are individual data in the context of machine learning models? The data used to train the model, say. There are types of models where the answer is simple. Take k-nearest-neighbors, for example. There &lt;em&gt;is not&lt;/em&gt; even a model without the complete dataset. Or support vector machines. There is no model without the support vectors. But neural networks? They’re just some composition of functions, – no data included.&lt;/p&gt;
&lt;p&gt;The same is true for data fed to a deployed deep-learning model. It’s pretty unlikely one could invert the final softmax output from a big ResNet and get back the raw input data.&lt;/p&gt;
&lt;p&gt;In theory, then, “hacking” a standard neural net to spy on input data sounds illusory. In practice, however, there is always some real-world &lt;em&gt;context&lt;/em&gt;. The context may be other datasets, publicly available, that can be linked to the “private” data in question. This is a popular showcase used in advocating for differential privacy&lt;span class="citation"&gt;(Dwork et al. 2006)&lt;/span&gt;: Take an “anonymized” dataset, dig up complementary information from public sources, and de-anonymize records ad libitum. Some context in that sense will often be used in “black-box” attacks, ones that presuppose no insider information about the model to be hacked.&lt;/p&gt;
&lt;p&gt;But context can also be structural, such as in the scenario demonstrated in this post. For example, assume a distributed model, where sets of layers run on different devices – embedded devices or mobile phones, for example. (A scenario like that is sometimes seen as “white-box”&lt;span class="citation"&gt;(Wu et al. 2016)&lt;/span&gt;, but in common understanding, white-box attacks probably presuppose some more insider knowledge, such as access to model architecture or even, weights. I’d therefore prefer calling this white-ish at most.) — Now assume that in this context, it is possible to intercept, and interact with, a system that executes the deeper layers of the model. Based on that system’s intermediate-level output, it is possible to perform &lt;em&gt;model inversion&lt;/em&gt;&lt;span class="citation"&gt;(Fredrikson et al. 2014)&lt;/span&gt;, that is, to reconstruct the input data fed into the system.&lt;/p&gt;
&lt;p&gt;In this post, we’ll demonstrate such a model inversion attack, basically porting the approach given in a &lt;a href="https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/advanced/privacy_attacks/Tutorial%201%20-%20Black%20box%20model%20inversion.ipynb%20Sy"&gt;notebook&lt;/a&gt; found in the &lt;a href="https://github.com/OpenMined/PySyft"&gt;PySyft&lt;/a&gt; repository. We then experiment with different levels of &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt;-privacy, exploring impact on reconstruction success. This second part will make use of TensorFlow Privacy, introduced in a &lt;a href="https://blogs.rstudio.com/ai/posts/2019-12-20-differential-privacy/"&gt;previous blog post&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="part-1-model-inversion-in-action"&gt;Part 1: Model inversion in action&lt;/h2&gt;
&lt;h3 id="example-dataset-all-the-worlds-letters1"&gt;Example dataset: All the world’s letters&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The overall process of model inversion used here is the following. With no, or scarcely any, insider knowledge about a model, – but given opportunities to repeatedly query it –, I want to learn how to reconstruct unknown inputs based on just model outputs . Independently of original model training, this, too, is a training process; however, in general it will not involve the original data, as those won’t be publicly available. Still, for best success, the attacker model is trained with data as similar as possible to the original training data assumed. Thinking of images, for example, and presupposing the popular view of successive layers representing successively coarse-grained features, we want that the surrogate data to share as many representation spaces with the real data as possible – up to the very highest layers before final classification, ideally.&lt;/p&gt;
&lt;p&gt;If we wanted to use classical MNIST as an example, one thing we could do is to only use some of the digits for training the “real” model; and the rest, for training the adversary. Let’s try something different though, something that might make the undertaking harder as well as easier at the same time. Harder, because the dataset features exemplars more complex than MNIST digits; easier because of the same reason: More could possibly be learned, by the adversary, from a complex task.&lt;/p&gt;
&lt;p&gt;Originally designed to develop a machine model of concept learning and generalization &lt;span class="citation"&gt;(Lake, Salakhutdinov, and Tenenbaum 2015)&lt;/span&gt;, the &lt;a href="https://github.com/brendenlake/omniglot/"&gt;OmniGlot&lt;/a&gt; dataset incorporates characters from fifty alphabets, split into two disjoint groups of thirty and twenty alphabets each. We’ll use the group of twenty to train our target model. Here is a sample:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-15-model-inversion-attacks/images/training_set_images.png" alt="Sample from the twenty-alphabet set used to train the target model (originally: 'evaluation set')" width="398" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-1)Sample from the twenty-alphabet set used to train the target model (originally: ‘evaluation set’)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The group of thirty we don’t use; instead, we’ll employ two small five-alphabet collections to train the adversary and to test reconstruction, respectively. (These small subsets of the original “big” thirty-alphabet set are again disjoint.)&lt;/p&gt;
&lt;p&gt;Here first is a sample from the set used to train the adversary.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-15-model-inversion-attacks/images/attacker_ds.png" alt="Sample from the five-alphabet set used to train the adversary (originally: 'background small 1')" width="404" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-2)Sample from the five-alphabet set used to train the adversary (originally: ‘background small 1’)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The other small subset will be used to test the adversary’s spying capabilities after training. Let’s peek at this one, too:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-15-model-inversion-attacks/images/actual_test_images.png" alt="Sample from the five-alphabet set used to test the adversary after training(originally: 'background small 2')" width="386" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-3)Sample from the five-alphabet set used to test the adversary after training(originally: ‘background small 2’)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Conveniently, we can use &lt;a href="https://github.com/rstudio/tfds"&gt;tfds&lt;/a&gt;, the R wrapper to TensorFlow Datasets, to load those subsets:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(reticulate)
library(keras)
library(tfdatasets)
library(tfautograph)
library(tfds)

library(purrr)

# we&amp;#39;ll use this to train the target model
# n = 13180
omni_train &amp;lt;- tfds$load(&amp;quot;omniglot&amp;quot;, split = &amp;quot;test&amp;quot;)

# this is used to train the adversary
# n = 2720
omni_spy &amp;lt;- tfds$load(&amp;quot;omniglot&amp;quot;, split = &amp;quot;small1&amp;quot;)

# this we&amp;#39;ll use for testing
# n = 3120
omni_test &amp;lt;- tfds$load(&amp;quot;omniglot&amp;quot;, split = &amp;quot;small2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now first, we train the target model.&lt;/p&gt;
&lt;h3 id="train-target-model"&gt;Train target model&lt;/h3&gt;
&lt;p&gt;The dataset originally has four columns: the image, of size 105 x 105; an alphabet id and a within-dataset character id; and a label. For our use case, we’re not really interested in the task the target model was/is used for; we just want to get at the data. Basically, whatever task we choose, it is not much more than a dummy task. So, let’s just say we train the target to classify characters &lt;em&gt;by alphabet&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We thus throw out all unneeded features, keeping just the alphabet id and the image itself:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# normalize and work with a single channel (images are black-and-white anyway)
preprocess_image &amp;lt;- function(image) {
  image %&amp;gt;%
    tf$cast(dtype = tf$float32) %&amp;gt;%
    tf$truediv(y = 255) %&amp;gt;%
    tf$image$rgb_to_grayscale()
}

# use the first 11000 images for training
train_ds &amp;lt;- omni_train %&amp;gt;% 
  dataset_take(11000) %&amp;gt;%
  dataset_map(function(record) {
    record$image &amp;lt;- preprocess_image(record$image)
    list(record$image, record$alphabet)}) %&amp;gt;%
  dataset_shuffle(1000) %&amp;gt;% 
  dataset_batch(32)

# use the remaining 2180 records for validation
val_ds &amp;lt;- omni_train %&amp;gt;% 
  dataset_skip(11000) %&amp;gt;%
  dataset_map(function(record) {
    record$image &amp;lt;- preprocess_image(record$image)
    list(record$image, record$alphabet)}) %&amp;gt;%
  dataset_batch(32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model consists of two parts. The first is imagined to run in a distributed fashion; for example, on mobile devices (stage one). These devices then send model outputs to a central server, where final results are computed (stage two). Sure, you will be thinking, this is a convenient setup for our scenario: If we intercept stage one results, we – most probably – gain access to richer information than what is contained in a model’s final output layer. — That is correct, but the scenario is less contrived than one might assume. Just like federated learning &lt;span class="citation"&gt;(McMahan et al. 2016)&lt;/span&gt;, it fulfills important desiderata: Actual training data never leaves the devices, thus staying (in theory!) private; at the same time, ingoing traffic to the server is significantly reduced.&lt;/p&gt;
&lt;p&gt;In our example setup, the on-device model is a convnet, while the server model is a simple feedforward network.&lt;/p&gt;
&lt;p&gt;We link both together as a &lt;em&gt;TargetModel&lt;/em&gt; that when called normally, will run both steps in succession. However, we’ll be able to call &lt;code&gt;target_model$mobile_step()&lt;/code&gt; separately, thereby intercepting intermediate results.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;on_device_model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_conv_2d(filters = 32, kernel_size = c(7, 7),
                input_shape = c(105, 105, 1), activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_max_pooling_2d(pool_size = c(3, 3), strides = 3) %&amp;gt;%
  layer_dropout(0.2) %&amp;gt;%
  layer_conv_2d(filters = 32, kernel_size = c(7, 7), activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_max_pooling_2d(pool_size = c(3, 3), strides = 2) %&amp;gt;%
  layer_dropout(0.2) %&amp;gt;%
  layer_conv_2d(filters = 32, kernel_size = c(5, 5), activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_max_pooling_2d(pool_size = c(2, 2), strides = 2) %&amp;gt;%
  layer_dropout(0.2) %&amp;gt;%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_max_pooling_2d(pool_size = c(2, 2), strides = 2) %&amp;gt;%
  layer_dropout(0.2) 

server_model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_dense(units = 256, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_flatten() %&amp;gt;%
  layer_dropout(0.2) %&amp;gt;% 
  # we have just 20 different ids, but they are not in lexicographic order
  layer_dense(units = 50, activation = &amp;quot;softmax&amp;quot;)

target_model &amp;lt;- function() {
  keras_model_custom(name = &amp;quot;TargetModel&amp;quot;, function(self) {
    
    self$on_device_model &amp;lt;-on_device_model
    self$server_model &amp;lt;- server_model
    self$mobile_step &amp;lt;- function(inputs) 
      self$on_device_model(inputs)
    self$server_step &amp;lt;- function(inputs)
      self$server_model(inputs)

    function(inputs, mask = NULL) {
      inputs %&amp;gt;% 
        self$mobile_step() %&amp;gt;%
        self$server_step()
    }
  })
  
}

model &amp;lt;- target_model()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The overall model is a Keras custom model, so we train it &lt;a href="https://tensorflow.rstudio.com/tutorials/advanced/"&gt;TensorFlow 2.x - style&lt;/a&gt;. After ten epochs, training and validation accuracy are at ~0.84 and ~0.73, respectively – not bad at all for a 20-class discrimination task.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;loss &amp;lt;- loss_sparse_categorical_crossentropy
optimizer &amp;lt;- optimizer_adam()

train_loss &amp;lt;- tf$keras$metrics$Mean(name=&amp;#39;train_loss&amp;#39;)
train_accuracy &amp;lt;-  tf$keras$metrics$SparseCategoricalAccuracy(name=&amp;#39;train_accuracy&amp;#39;)

val_loss &amp;lt;- tf$keras$metrics$Mean(name=&amp;#39;val_loss&amp;#39;)
val_accuracy &amp;lt;-  tf$keras$metrics$SparseCategoricalAccuracy(name=&amp;#39;val_accuracy&amp;#39;)

train_step &amp;lt;- function(images, labels) {
  with (tf$GradientTape() %as% tape, {
    predictions &amp;lt;- model(images)
    l &amp;lt;- loss(labels, predictions)
  })
  gradients &amp;lt;- tape$gradient(l, model$trainable_variables)
  optimizer$apply_gradients(purrr::transpose(list(
    gradients, model$trainable_variables
  )))
  train_loss(l)
  train_accuracy(labels, predictions)
}

val_step &amp;lt;- function(images, labels) {
  predictions &amp;lt;- model(images)
  l &amp;lt;- loss(labels, predictions)
  val_loss(l)
  val_accuracy(labels, predictions)
}


training_loop &amp;lt;- tf_function(autograph(function(train_ds, val_ds) {
  for (b1 in train_ds) {
    train_step(b1[[1]], b1[[2]])
  }
  for (b2 in val_ds) {
    val_step(b2[[1]], b2[[2]])
  }
  
  tf$print(&amp;quot;Train accuracy&amp;quot;, train_accuracy$result(),
           &amp;quot;    Validation Accuracy&amp;quot;, val_accuracy$result())
  
  train_loss$reset_states()
  train_accuracy$reset_states()
  val_loss$reset_states()
  val_accuracy$reset_states()
}))


for (epoch in 1:10) {
  cat(&amp;quot;Epoch: &amp;quot;, epoch, &amp;quot; -----------\n&amp;quot;)
  training_loop(train_ds, val_ds)  
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch:  1  -----------
Train accuracy 0.195090905     Validation Accuracy 0.376605511
Epoch:  2  -----------
Train accuracy 0.472272724     Validation Accuracy 0.5243119
...
...
Epoch:  9  -----------
Train accuracy 0.821454525     Validation Accuracy 0.720183492
Epoch:  10  -----------
Train accuracy 0.840454519     Validation Accuracy 0.726605475&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we train the adversary.&lt;/p&gt;
&lt;h3 id="train-adversary"&gt;Train adversary&lt;/h3&gt;
&lt;p&gt;The adversary’s general strategy will be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Feed its small, surrogate dataset to the on-device model.&lt;/strong&gt; The output received can be regarded as a (highly) &lt;em&gt;compressed&lt;/em&gt; version of the original images.&lt;/li&gt;
&lt;li&gt;P&lt;strong&gt;ass that “compressed” version as input to its own model,&lt;/strong&gt; which tries to reconstruct the original images from the sparse code.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compare original images (those from the surrogate dataset) to the reconstruction pixel-wise.&lt;/strong&gt; The goal is to minimize the mean (squared, say) error.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Doesn’t this sound a lot like the decoding side of an autoencoder? No wonder the attacker model is a deconvolutional network. Its input – equivalently, the on-device model’s output – is of size &lt;code&gt;batch_size x 1 x 1 x 32&lt;/code&gt;. That is, the information is encoded in 32 channels, but the spatial resolution is 1. Just like in an autoencoder operating on images, we need to &lt;em&gt;upsample&lt;/em&gt; until we arrive at the original resolution of 105 x 105.&lt;/p&gt;
&lt;p&gt;This is exactly what’s happening in the attacker model:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;attack_model &amp;lt;- function() {
  
  keras_model_custom(name = &amp;quot;AttackModel&amp;quot;, function(self) {
    
    self$conv1 &amp;lt;-layer_conv_2d_transpose(filters = 32, kernel_size = 9,
                                         padding = &amp;quot;valid&amp;quot;,
                                         strides = 1, activation = &amp;quot;relu&amp;quot;)
    self$conv2 &amp;lt;- layer_conv_2d_transpose(filters = 32, kernel_size = 7,
                                          padding = &amp;quot;valid&amp;quot;,
                                          strides = 2, activation = &amp;quot;relu&amp;quot;) 
    self$conv3 &amp;lt;- layer_conv_2d_transpose(filters = 1, kernel_size = 7,
                                          padding = &amp;quot;valid&amp;quot;,
                                          strides = 2, activation = &amp;quot;relu&amp;quot;)  
    self$conv4 &amp;lt;- layer_conv_2d_transpose(filters = 1, kernel_size = 5,
                                          padding = &amp;quot;valid&amp;quot;,
                                          strides = 2, activation = &amp;quot;relu&amp;quot;)
    
    function(inputs, mask = NULL) {
      inputs %&amp;gt;% 
        # bs * 9 * 9 * 32
        # output = strides * (input - 1) + kernel_size - 2 * padding
        self$conv1() %&amp;gt;%
        # bs * 23 * 23 * 32
        self$conv2() %&amp;gt;%
        # bs * 51 * 51 * 1
        self$conv3() %&amp;gt;%
        # bs * 105 * 105 * 1
        self$conv4()
    }
  })
  
}

attacker = attack_model()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To train the adversary, we use one of the small (five-alphabet) subsets. To reiterate what was said above, there is no overlap with the data used to train the target model.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;attacker_ds &amp;lt;- omni_spy %&amp;gt;% 
dataset_map(function(record) {
    record$image &amp;lt;- preprocess_image(record$image)
    list(record$image, record$alphabet)}) %&amp;gt;%
  dataset_batch(32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, then, is the attacker training loop, striving to refine the decoding process over a hundred – short – epochs:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;attacker_criterion &amp;lt;- loss_mean_squared_error
attacker_optimizer &amp;lt;- optimizer_adam()
attacker_loss &amp;lt;- tf$keras$metrics$Mean(name=&amp;#39;attacker_loss&amp;#39;)
attacker_mse &amp;lt;-  tf$keras$metrics$MeanSquaredError(name=&amp;#39;attacker_mse&amp;#39;)

attacker_step &amp;lt;- function(images) {
  
  attack_input &amp;lt;- model$mobile_step(images)
  
  with (tf$GradientTape() %as% tape, {
    generated &amp;lt;- attacker(attack_input)
    l &amp;lt;- attacker_criterion(images, generated)
  })
  gradients &amp;lt;- tape$gradient(l, attacker$trainable_variables)
  attacker_optimizer$apply_gradients(purrr::transpose(list(
    gradients, attacker$trainable_variables
  )))
  attacker_loss(l)
  attacker_mse(images, generated)
}


attacker_training_loop &amp;lt;- tf_function(autograph(function(attacker_ds) {
  for (b in attacker_ds) {
    attacker_step(b[[1]])
  }
  
  tf$print(&amp;quot;mse: &amp;quot;, attacker_mse$result())
  
  attacker_loss$reset_states()
  attacker_mse$reset_states()
}))

for (epoch in 1:100) {
  cat(&amp;quot;Epoch: &amp;quot;, epoch, &amp;quot; -----------\n&amp;quot;)
  attacker_training_loop(attacker_ds)  
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch:  1  -----------
  mse:  0.530902684
Epoch:  2  -----------
  mse:  0.201351956
...
...
Epoch:  99  -----------
  mse:  0.0413453057
Epoch:  100  -----------
  mse:  0.0413028933&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The question now is, – does it work? Has the attacker really learned to infer actual data from (stage one) model output?&lt;/p&gt;
&lt;h3 id="test-adversary"&gt;Test adversary&lt;/h3&gt;
&lt;p&gt;To test the adversary, we use the third dataset we downloaded, containing images from five yet-unseen alphabets. For display, we select just the first sixteen records – a completely arbitrary decision, of course.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;test_ds &amp;lt;- omni_test %&amp;gt;% 
  dataset_map(function(record) {
    record$image &amp;lt;- preprocess_image(record$image)
    list(record$image, record$alphabet)}) %&amp;gt;%
  dataset_take(16) %&amp;gt;%
  dataset_batch(16)

batch &amp;lt;- as_iterator(test_ds) %&amp;gt;% iterator_get_next()
images &amp;lt;- batch[[1]]

attack_input &amp;lt;- model$mobile_step(images)
generated &amp;lt;- attacker(attack_input) %&amp;gt;% as.array()

generated[generated &amp;gt; 1] &amp;lt;- 1
generated &amp;lt;- generated[ , , , 1]
generated %&amp;gt;%
  purrr::array_tree(1) %&amp;gt;%
  purrr::map(as.raster) %&amp;gt;%
  purrr::iwalk(~{plot(.x)})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just like during the training process, the adversary queries the target model (stage one), obtains the compressed representation, and attempts to reconstruct the original image. (Of course, in the real world, the setup would be different in that the attacker would &lt;em&gt;not&lt;/em&gt; be able to simply inspect the images, as is the case here. There would thus have to be some way to intercept, and make sense of, network traffic.)&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;attack_input &amp;lt;- model$mobile_step(images)
generated &amp;lt;- attacker(attack_input) %&amp;gt;% as.array()

generated[generated &amp;gt; 1] &amp;lt;- 1
generated &amp;lt;- generated[ , , , 1]
generated %&amp;gt;%
  purrr::array_tree(1) %&amp;gt;%
  purrr::map(as.raster) %&amp;gt;%
  purrr::iwalk(~{plot(.x)})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To allow for easier comparison (and increase suspense …!), here again are the actual images, which we displayed already when introducing the dataset:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-15-model-inversion-attacks/images/actual_test_images.png" alt="First images from the test set, the way they really look." width="386" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-13)First images from the test set, the way they really look.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;And here is the reconstruction:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-15-model-inversion-attacks/images/recon_noeps_dropout.png" alt="First images from the test set, as reconstructed by the adversary." width="386" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-14)First images from the test set, as reconstructed by the adversary.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Of course, it is hard to say how revealing these “guesses” are. There definitely seems to be a connection to character complexity; overall, it seems like the Greek and Roman letters, which are the least complex, are also the ones most easily reconstructed. Still, in the end, how much privacy is lost will very much depend on contextual factors.&lt;/p&gt;
&lt;p&gt;First and foremost, do the exemplars in the dataset represent &lt;em&gt;individuals&lt;/em&gt; or &lt;em&gt;classes&lt;/em&gt; of individuals? If – as in reality – the character &lt;code&gt;X&lt;/code&gt; represents a class, it might not be so grave if we were able to reconstruct “some X” here: There are many &lt;code&gt;X&lt;/code&gt;s in the dataset, all pretty similar to each other; we’re unlikely to exactly to have reconstructed one special, individual &lt;code&gt;X&lt;/code&gt;. If, however, this was a dataset of individual people, with all &lt;code&gt;X&lt;/code&gt;s being photographs of Alex, then in reconstructing an &lt;code&gt;X&lt;/code&gt; we have effectively reconstructed Alex.&lt;/p&gt;
&lt;p&gt;Second, in less obvious scenarios, evaluating the degree of privacy breach will likely surpass computation of quantitative metrics, and involve the judgment of domain experts.&lt;/p&gt;
&lt;p&gt;Speaking of quantitative metrics though – our example seems like a perfect use case to experiment with &lt;em&gt;differential privacy.&lt;/em&gt; Differential privacy is measured by &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt; (lower is better), the main idea being that answers to queries to a system should depend as little as possible on the presence or absence of a single (&lt;em&gt;any&lt;/em&gt; single) datapoint.&lt;/p&gt;
&lt;p&gt;So, we will repeat the above experiment, using TensorFlow Privacy (TFP) to add noise, as well as clip gradients, during optimization of the target model. We’ll try three different conditions, resulting in three different values for &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt;s, and for each condition, inspect the images reconstructed by the adversary.&lt;/p&gt;
&lt;h2 id="part-2-differential-privacy-to-the-rescue"&gt;Part 2: Differential privacy to the rescue&lt;/h2&gt;
&lt;p&gt;Unfortunately, the setup for this part of the experiment requires a little workaround. Making use of the flexibility afforded by TensorFlow 2.x, our target model has been a custom model, joining two distinct stages (“mobile” and “server”) that could be called independently.&lt;/p&gt;
&lt;p&gt;TFP, however, does still not work with TensorFlow 2.x, meaning we have to use old-style, non-eager model definitions and training. Luckily, the workaround will be easy.&lt;/p&gt;
&lt;p&gt;First, load (and possibly, install) libraries, taking care to disable TensorFlow V2 behavior.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
library(tensorflow)
# still necessary when working with TensorFlow Privacy, as of this writing
tf$compat$v1$disable_v2_behavior()

# if you don&amp;#39;t have it installed:
# reticulate::py_install(&amp;quot;tensorflow_privacy&amp;quot;)
tfp &amp;lt;- import(&amp;quot;tensorflow_privacy&amp;quot;)

library(tfdatasets)
library(tfds)

library(purrr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The training set is loaded, preprocessed and batched (nearly) as before.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;omni_train &amp;lt;- tfds$load(&amp;quot;omniglot&amp;quot;, split = &amp;quot;test&amp;quot;)

batch_size &amp;lt;- 32

train_ds &amp;lt;- omni_train %&amp;gt;%
  dataset_take(11000) %&amp;gt;%
  dataset_map(function(record) {
    record$image &amp;lt;- preprocess_image(record$image)
    list(record$image, record$alphabet)}) %&amp;gt;%
  dataset_shuffle(1000) %&amp;gt;%
  # need dataset_repeat() when not eager
  dataset_repeat() %&amp;gt;%
  dataset_batch(batch_size)&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="train-target-model-with-tensorflow-privacy"&gt;Train target model – with TensorFlow Privacy&lt;/h3&gt;
&lt;p&gt;To train the target, we put the layers from both stages – “mobile” and “server” – into one sequential model. Note how we remove the dropout. This is because noise will be added during optimization anyway.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;complete_model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_conv_2d(filters = 32, kernel_size = c(7, 7),
                input_shape = c(105, 105, 1),
                activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_max_pooling_2d(pool_size = c(3, 3), strides = 3) %&amp;gt;%
  #layer_dropout(0.2) %&amp;gt;%
  layer_conv_2d(filters = 32, kernel_size = c(7, 7), activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_max_pooling_2d(pool_size = c(3, 3), strides = 2) %&amp;gt;%
  #layer_dropout(0.2) %&amp;gt;%
  layer_conv_2d(filters = 32, kernel_size = c(5, 5), activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_max_pooling_2d(pool_size = c(2, 2), strides = 2) %&amp;gt;%
  #layer_dropout(0.2) %&amp;gt;%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_max_pooling_2d(pool_size = c(2, 2), strides = 2, name = &amp;quot;mobile_output&amp;quot;) %&amp;gt;%
  #layer_dropout(0.2) %&amp;gt;%
  layer_dense(units = 256, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_flatten() %&amp;gt;%
  #layer_dropout(0.2) %&amp;gt;%
  layer_dense(units = 50, activation = &amp;quot;softmax&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using TFP mainly means using a TFP optimizer, one that clips gradients according to some defined magnitude and adds noise of defined size. &lt;code&gt;noise_multiplier&lt;/code&gt; is the parameter we are going to vary to arrive at different &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt;s:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;l2_norm_clip &amp;lt;- 1

# ratio of the standard deviation to the clipping norm
# we run training for each of the three values
noise_multiplier &amp;lt;- 0.7
noise_multiplier &amp;lt;- 0.5
noise_multiplier &amp;lt;- 0.3

# same as batch size
num_microbatches &amp;lt;- k_cast(batch_size, &amp;quot;int32&amp;quot;)
learning_rate &amp;lt;- 0.005

optimizer &amp;lt;- tfp$DPAdamGaussianOptimizer(
  l2_norm_clip = l2_norm_clip,
  noise_multiplier = noise_multiplier,
  num_microbatches = num_microbatches,
  learning_rate = learning_rate
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In training the model, the second important change for TFP we need to make is to have loss and gradients computed on the individual level.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# need to add noise to every individual contribution
loss &amp;lt;- tf$keras$losses$SparseCategoricalCrossentropy(reduction =   tf$keras$losses$Reduction$NONE)

complete_model %&amp;gt;% compile(loss = loss, optimizer = optimizer, metrics = &amp;quot;sparse_categorical_accuracy&amp;quot;)

num_epochs &amp;lt;- 20

n_train &amp;lt;- 13180

history &amp;lt;- complete_model %&amp;gt;% fit(
  train_ds,
  # need steps_per_epoch when not in eager mode
  steps_per_epoch = n_train/batch_size,
  epochs = num_epochs)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To test three different &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt;s, we run this thrice, each time with a different &lt;code&gt;noise_multiplier&lt;/code&gt;. Each time we arrive at a different final accuracy.&lt;/p&gt;
&lt;p&gt;Here is a synopsis, where &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt; was computed like so:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;compute_priv &amp;lt;- tfp$privacy$analysis$compute_dp_sgd_privacy

compute_priv$compute_dp_sgd_privacy(
  # number of records in training set
  n_train,
  batch_size,
  # noise_multiplier
  0.7, # or 0.5, or 0.3
  # number of epochs
  20,
  # delta - should not exceed 1/number of examples in training set
  1e-5)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th&gt;noise multiplier&lt;/th&gt;
&lt;th&gt;epsilon&lt;/th&gt;
&lt;th&gt;final acc. (training set)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;0.7&lt;/td&gt;
&lt;td&gt;4.0&lt;/td&gt;
&lt;td&gt;0.37&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;12.5&lt;/td&gt;
&lt;td&gt;0.45&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;84.7&lt;/td&gt;
&lt;td&gt;0.56&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now, as the adversary won’t call the complete model, we need to “cut off” the second-stage layers. This leaves us with a model that executes stage-one logic only. We save its weights, so we can later call it from the adversary:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;intercepted &amp;lt;- keras_model(
  complete_model$input,
  complete_model$get_layer(&amp;quot;mobile_output&amp;quot;)$output
)

intercepted %&amp;gt;% save_model_hdf5(&amp;quot;./intercepted.hdf5&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="train-adversary-against-differentially-private-target"&gt;Train adversary (against differentially private target)&lt;/h3&gt;
&lt;p&gt;In training the adversary, we can keep most of the original code – meaning, we’re back to TF-2 style. Even the definition of the target model is the same as before:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;on_device_model &amp;lt;- keras_model_sequential() %&amp;gt;%
  [...]

server_model &amp;lt;- keras_model_sequential() %&amp;gt;%
  [...]

target_model &amp;lt;- function() {
  keras_model_custom(name = &amp;quot;TargetModel&amp;quot;, function(self) {
    
    self$on_device_model &amp;lt;-on_device_model
    self$server_model &amp;lt;- server_model
    self$mobile_step &amp;lt;- function(inputs) 
      self$on_device_model(inputs)
    self$server_step &amp;lt;- function(inputs)
      self$server_model(inputs)
    
    function(inputs, mask = NULL) {
      inputs %&amp;gt;% 
        self$mobile_step() %&amp;gt;%
        self$server_step()
    }
  })
}

intercepted &amp;lt;- target_model()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But now, we load the trained target’s weights into the freshly defined model’s “mobile stage”:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;intercepted$on_device_model$load_weights(&amp;quot;intercepted.hdf5&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now, we’re back to the old training routine. Testing setup is the same as before, as well.&lt;/p&gt;
&lt;p&gt;So how well does the adversary perform with differential privacy added to the picture?&lt;/p&gt;
&lt;h3 id="test-adversary-against-differentially-private-target"&gt;Test adversary (against differentially private target)&lt;/h3&gt;
&lt;p&gt;Here, ordered by decreasing &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt;, are the reconstructions. Again, we refrain from judging the results, for the same reasons as before: In real-world applications, whether privacy is preserved “well enough” will depend on the context.&lt;/p&gt;
&lt;p&gt;Here, first, are reconstructions from the run where the least noise was added.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-15-model-inversion-attacks/images/intercepted_eps_84.7.png" alt="Reconstruction attempts from a setup where the target model was trained with an epsilon of 84.7." width="384" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-24)Reconstruction attempts from a setup where the target model was trained with an epsilon of 84.7.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;On to the next level of privacy protection:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-15-model-inversion-attacks/images/intercepted_eps_12.5.png" alt="Reconstruction attempts from a setup where the target model was trained with an epsilon of 12.5." width="385" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-25)Reconstruction attempts from a setup where the target model was trained with an epsilon of 12.5.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;And the highest-&lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt; one:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-05-15-model-inversion-attacks/images/intercepted_eps_4.0.png" alt="Reconstruction attempts from a setup where the target model was trained with an epsilon of 4.0." width="386" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-26)Reconstruction attempts from a setup where the target model was trained with an epsilon of 4.0.
&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Throughout this post, we’ve refrained from “over-commenting” on results, and focused on the why-and-how instead. This is because in an artificial setup, chosen to facilitate exposition of concepts and methods, there really is no objective frame of reference. What is a good reconstruction? What is a good &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt;? What constitutes a data breach? No-one knows.&lt;/p&gt;
&lt;p&gt;In the real world, there is a context to everything – there are people involved, the people whose data we’re talking about. There are organizations, regulations, laws. There are abstract principles, and there are implementations; different implementations of the same “idea” can differ.&lt;/p&gt;
&lt;p&gt;As in machine learning overall, research papers on privacy-, ethics- or otherwise society-related topics are full of LaTeX formulae. Amid the math, let’s not forget the people.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-Dwork2006"&gt;
&lt;p&gt;Dwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. “Calibrating Noise to Sensitivity in Private Data Analysis.” In &lt;em&gt;Proceedings of the Third Conference on Theory of Cryptography&lt;/em&gt;, 265–84. TCC’06. Berlin, Heidelberg: Springer-Verlag. &lt;a href="https://doi.org/10.1007/11681878_14"&gt;https://doi.org/10.1007/11681878_14&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Fred"&gt;
&lt;p&gt;Fredrikson, Matthew, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas Ristenpart. 2014. “Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing.” In &lt;em&gt;Proceedings of the 23rd Usenix Conference on Security Symposium&lt;/em&gt;, 17–32. SEC’14. USA: USENIX Association.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Lake1332"&gt;
&lt;p&gt;Lake, Brenden M., Ruslan Salakhutdinov, and Joshua B. Tenenbaum. 2015. “Human-Level Concept Learning Through Probabilistic Program Induction.” &lt;em&gt;Science&lt;/em&gt; 350 (6266): 1332–8. &lt;a href="https://doi.org/10.1126/science.aab3050"&gt;https://doi.org/10.1126/science.aab3050&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-McMahanMRA16"&gt;
&lt;p&gt;McMahan, H. Brendan, Eider Moore, Daniel Ramage, and Blaise Agüera y Arcas. 2016. “Federated Learning of Deep Networks Using Model Averaging.” &lt;em&gt;CoRR&lt;/em&gt; abs/1602.05629. &lt;a href="http://arxiv.org/abs/1602.05629"&gt;http://arxiv.org/abs/1602.05629&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-7536387"&gt;
&lt;p&gt;Wu, X., M. Fredrikson, S. Jha, and J. F. Naughton. 2016. “A Methodology for Formalizing Model-Inversion Attacks.” In &lt;em&gt;2016 Ieee 29th Computer Security Foundations Symposium (Csf)&lt;/em&gt;, 355–70.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Don’t take &lt;em&gt;all&lt;/em&gt; literally please; it’s just a nice phrase.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">bd63aed9dff1f6f4ec1897da9c5c7915</distill:md5>
      <category>R</category>
      <category>Privacy &amp; Security</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks</guid>
      <pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks/images/results.png" medium="image" type="image/png" width="600" height="394"/>
    </item>
    <item>
      <title>Towards privacy: Encrypted deep learning with Syft and Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft</link>
      <description>


&lt;p&gt;The word &lt;em&gt;privacy&lt;/em&gt;, in the context of deep learning (or machine learning, or “AI”), and especially when combined with things like &lt;em&gt;security&lt;/em&gt;, sounds like it could be part of a catch phrase: &lt;em&gt;privacy, safety, security&lt;/em&gt; – like &lt;em&gt;liberté, fraternité, égalité&lt;/em&gt;. In fact, there should probably be a mantra like that. But that’s another topic, and like with the other catch phrase just cited, not everyone interprets these terms in the same way.&lt;/p&gt;
&lt;p&gt;So let’s think about privacy, narrowed down to its role in training or using deep learning models, in a more technical way. Since privacy – or rather, its violations – may appear in various ways, different violations will demand different countermeasures. Of course, in the end, we’d like to see them all integrated – but re privacy-related technologies, the field is really just starting out on a journey. The most important thing we can do, then, is to learn about the concepts, investigate the landscape of implementations under development, and – perhaps – decide to join the effort.&lt;/p&gt;
&lt;p&gt;This post tries to do a tiny little bit of all of those.&lt;/p&gt;
&lt;h2 id="aspects-of-privacy-in-deep-learning"&gt;Aspects of privacy in deep learning&lt;/h2&gt;
&lt;p&gt;Say you work at a hospital, and would be interested in training a deep learning model to help diagnose some disease from brain scans. Where you work, you don’t have many patients with this disease; moreover, they tend to mostly be affected by the same subtypes: Your training set, were you to create one, would not reflect the overall distribution very well. It would, thus, make sense to cooperate with other hospitals; but that isn’t so easy, as the data collected is protected by privacy regulations. So, the first requirement is: The data has to stay where it is; e.g., it may not be sent to a central server.&lt;/p&gt;
&lt;h4 id="federated-learning"&gt;Federated learning&lt;/h4&gt;
&lt;p&gt;This first &lt;em&gt;sine qua non&lt;/em&gt; is addressed by &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2020-04-08-tf-federated-intro/"&gt;federated learning&lt;/a&gt; &lt;span class="citation"&gt;(McMahan et al. 2016)&lt;/span&gt;. Federated learning is not “just” desirable for privacy reasons. On the contrary, in many use cases, it may be the only viable way (like with smartphones or sensors, which collect gigantic amounts of data). In federated learning, each participant receives a copy of the model, trains on their own data, and sends back the gradients obtained to the central server, where gradients are averaged and applied to the model.&lt;/p&gt;
&lt;p&gt;This is good insofar as the data never leaves the individual devices; however, a lot of information can still be extracted from plain-text gradients. Imagine a smartphone app that provides trainable auto-completion for text messages. Even if gradient updates from many iterations are averaged, their distributions will greatly vary between individuals. Some form of encryption is needed. But then how is the server going to make sense of the encrypted gradients?&lt;/p&gt;
&lt;p&gt;One way to accomplish this relies on &lt;em&gt;secure multi-party computation&lt;/em&gt; (SMPC).&lt;/p&gt;
&lt;h4 id="secure-multi-party-computation"&gt;Secure multi-party computation&lt;/h4&gt;
&lt;p&gt;In SMPC, we need a system of several agents who collaborate to provide a result no single agent could provide alone: “normal” computations (like addition, multiplication …) on “secret” (encrypted) data. The assumption is that these agents are “honest but curious” – honest, because they won’t tamper with their share of data; curious in the sense that if they &lt;em&gt;were&lt;/em&gt; (curious, that is), they wouldn’t be able to inspect the data because it’s encrypted.&lt;/p&gt;
&lt;p&gt;The principle behind this is &lt;em&gt;secret sharing&lt;/em&gt;. A single piece of data – a salary, say – is “split up” into meaningless (hence, encrypted) parts which, when put together again, yield the original data. Here is an example.&lt;/p&gt;
&lt;p&gt;Say the parties involved are Julia, Greg, and me. The below function encrypts a single value, assigning to each of us their “meaningless” share:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# a big prime number
# all computations are performed in a finite field, for example, the integers modulo that prime
Q &amp;lt;- 78090573363827
 
encrypt &amp;lt;- function(x) {
  # all but the very last share are random 
  julias &amp;lt;- runif(1, min = -Q, max = Q)
  gregs &amp;lt;- runif(1, min = -Q, max = Q)
  mine &amp;lt;- (x - julias - gregs) %% Q
  list (julias, gregs, mine)
}

# some top secret value no-one may get to see
value &amp;lt;- 77777

encrypted &amp;lt;- encrypt(value)
encrypted&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
[1] 7467283737857

[[2]]
[1] 36307804406429

[[3]]
[1] 34315485297318&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the three of us put our shares together, getting back the plain value is straightforward:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;decrypt &amp;lt;- function(shares) {
  Reduce(sum, shares) %% Q  
}

decrypt(encrypted)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;77777&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As an example of how to compute on encrypted data, here’s addition. (Other operations will be a lot less straightforward.) To add two numbers, just have everyone add their respective shares:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;add &amp;lt;- function(x, y) {
  list(
    # julia
    (x[[1]] + y[[1]]) %% Q,
    # greg
    (x[[2]] + y[[2]]) %% Q,
    # me
    (x[[3]] + y[[3]]) %% Q
  )
}
  
x &amp;lt;- encrypt(11)
y &amp;lt;- encrypt(122)

decrypt(add(x, y))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;133&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Back to the setting of deep learning and the current task to be solved: Have the server apply gradient updates without ever seeing them. With secret sharing, it would work like this:&lt;/p&gt;
&lt;p&gt;Julia, Greg and me each want to train on our own private data. Together, we will be responsible for gradient averaging, that is, we’ll form a &lt;em&gt;cluster&lt;/em&gt; of &lt;em&gt;workers&lt;/em&gt; united in that task. Now, the model owner &lt;em&gt;secret shares&lt;/em&gt; the model, and we start training, each on their own data. After some number of iterations, we use secure averaging to combine our respective gradients. Then, all the server gets to see is the mean gradient, and there is no way to determine our respective contributions.&lt;/p&gt;
&lt;h4 id="beyond-private-gradients"&gt;Beyond private gradients&lt;/h4&gt;
&lt;p&gt;Amazingly, it is even possible to &lt;em&gt;train&lt;/em&gt; on encrypted data – amongst others, using that same technique of secret sharing. Of course, this has to negatively affect training speed. But it’s good to know that if one’s use case were to demand it, it would be feasible. (One possible use case is when training on one party’s data alone doesn’t make any sense, but data is sensitive, so others won’t let you access their data unless encrypted.)&lt;/p&gt;
&lt;p&gt;So with encryption available on an all-you-need basis, are we completely safe, privacy-wise? The answer is no. The model can still leak information. For example, in some cases it is possible to perform &lt;em&gt;model inversion&lt;/em&gt; [@abs-1805-04049], that is, with just black-box access to a model, train an &lt;em&gt;attack model&lt;/em&gt; that allows reconstructing some of the original training data. Needless to say, this kind of leakage has to be avoided. &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-12-20-differential-privacy/"&gt;Differential privacy&lt;/a&gt; &lt;span class="citation"&gt;(Dwork et al. 2006)&lt;/span&gt;, &lt;span class="citation"&gt;(Dwork 2006)&lt;/span&gt; demands that results obtained from querying a model be independent from the presence or absence, in the dataset employed for training, of a single individual. In general, this is ensured by adding noise to the answer to every query. In training deep learning models, we add noise to the gradients, as well as clip them according to some chosen norm.&lt;/p&gt;
&lt;p&gt;At some point, then, we will want all of those in combination: federated learning, encryption, and differential privacy.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Syft&lt;/em&gt; is a very promising, very actively developed framework that aims for providing all of them. Instead of “aims for”, I should perhaps have written “provides” – it depends. We need some more context.&lt;/p&gt;
&lt;h2 id="introducing-syft"&gt;Introducing Syft&lt;/h2&gt;
&lt;p&gt;Syft – also known as &lt;a href="https://github.com/OpenMined/PySyft"&gt;PySyft&lt;/a&gt;, since as of today, its most mature implementation is written in and for Python – is maintained by &lt;a href="https://www.openmined.org/"&gt;OpenMined&lt;/a&gt;, an open source community dedicated to enabling privacy-preserving AI. It’s worth it reproducing their mission statement here:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Industry standard tools for artificial intelligence have been designed with several assumptions: data is centralized into a single compute cluster, the cluster exists in a secure cloud, and the resulting models will be owned by a central authority. We envision a world in which we are not restricted to this scenario - a world in which AI tools treat privacy, security, and multi-owner governance as first class citizens. […] The mission of the OpenMined community is to create an accessible ecosystem of tools for private, secure, multi-owner governed AI.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While far from being the only one, PySyft is their most maturely developed framework. Its role is to provide secure federated learning, including encryption and differential privacy. For deep learning, it relies on existing frameworks.&lt;/p&gt;
&lt;p&gt;PyTorch integration seems the most mature, as of today; with PyTorch, encrypted and differentially private training are already available. Integration with TensorFlow is a bit more involved; it does not yet include TensorFlow Federated and TensorFlow Privacy. For encryption, it relies on &lt;a href="https://github.com/tf-encrypted/tf-encrypted"&gt;TensorFlow Encrypted&lt;/a&gt; (TFE), which as of this writing is not an official TensorFlow subproject.&lt;/p&gt;
&lt;p&gt;However, even now it is already possible to &lt;em&gt;secret share&lt;/em&gt; Keras models and administer private predictions. Let’s see how.&lt;/p&gt;
&lt;h2 id="private-predictions-with-syft-tensorflow-encrypted-and-keras"&gt;Private predictions with Syft, TensorFlow Encrypted and Keras&lt;/h2&gt;
&lt;p&gt;Our introductory example will show how to use an externally-provided model to classify private data – without the model owner ever seeing that data, &lt;em&gt;and&lt;/em&gt; without the user ever getting hold of (e.g., downloading) the model. (Think about the model owner wanting to keep the fruits of their labour hidden, as well.)&lt;/p&gt;
&lt;p&gt;Put differently: The model is encrypted, and the data is, too. As you might imagine, this involves a cluster of agents, together performing secure multi-party computation.&lt;/p&gt;
&lt;p&gt;This use case presupposing an already trained model, we start by quickly creating one. There is nothing special going on here.&lt;/p&gt;
&lt;h4 id="prelude-train-a-simple-model-on-mnist"&gt;Prelude: Train a simple model on MNIST&lt;/h4&gt;
&lt;pre class="r"&gt;&lt;code&gt;# create_model.R

library(tensorflow)
library(keras)

mnist &amp;lt;- dataset_mnist()
mnist$train$x &amp;lt;- mnist$train$x/255
mnist$test$x &amp;lt;- mnist$test$x/255

dim(mnist$train$x) &amp;lt;- c(dim(mnist$train$x), 1)
dim(mnist$test$x) &amp;lt;- c(dim(mnist$test$x), 1)

input_shape &amp;lt;- c(28, 28, 1)

model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_conv_2d(filters = 16, kernel_size = c(3, 3), input_shape = input_shape) %&amp;gt;%
  layer_average_pooling_2d(pool_size = c(2, 2)) %&amp;gt;%
  layer_activation(&amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3)) %&amp;gt;%
  layer_average_pooling_2d(pool_size = c(2, 2)) %&amp;gt;%
  layer_activation(&amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3)) %&amp;gt;%
  layer_average_pooling_2d(pool_size = c(2, 2)) %&amp;gt;%
  layer_activation(&amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_flatten() %&amp;gt;%
  layer_dense(units = 10, activation = &amp;quot;linear&amp;quot;)
  

model %&amp;gt;% compile(
  loss = &amp;quot;sparse_categorical_crossentropy&amp;quot;,
  optimizer = &amp;quot;adam&amp;quot;,
  metrics = &amp;quot;accuracy&amp;quot;
)

model %&amp;gt;% fit(
    x = mnist$train$x,
    y = mnist$train$y,
    epochs = 1,
    validation_split = 0.3,
    verbose = 2
)

model$save(filepath = &amp;quot;model.hdf5&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="set-up-cluster-and-serve-model"&gt;Set up cluster and serve model&lt;/h4&gt;
&lt;p&gt;The easiest way to get all required packages is to install the ensemble OpenMined put together for their &lt;a href="https://www.udacity.com/course/secure-and-private-ai--ud185"&gt;Udacity Course&lt;/a&gt; that introduces federated learning and differential privacy with PySyft. This will install TensorFlow 1.15 and TensorFlow Encrypted, amongst others.&lt;/p&gt;
&lt;p&gt;The following lines of code should all be put together in a single file. I found it practical to “source” this script from an R process running in a console tab.&lt;/p&gt;
&lt;p&gt;To begin, we again define the model, two things being different now. First, for technical reasons, we need to pass in &lt;code&gt;batch_input_shape&lt;/code&gt; instead of &lt;code&gt;input_shape&lt;/code&gt;. Second, the final layer is “missing” the softmax activation. This is not an oversight – SMPC &lt;code&gt;softmax&lt;/code&gt; has not been implemented yet. (Depending on when you read this, that statement may no longer be true.) Were we training this model in &lt;em&gt;secret sharing&lt;/em&gt; mode, this would of course be a problem; for classification though, all we care about is the maximum score.&lt;/p&gt;
&lt;p&gt;After model definition, we load the actual weights from the model we trained in the previous step. Then, the action begins. We create an ensemble of TFE workers that together run a distributed TensorFlow cluster. The model is &lt;em&gt;secret&lt;/em&gt; &lt;em&gt;shared&lt;/em&gt; with the workers, that is, model weights are split up into shares that, each inspected alone, are unusable. Finally, the model is &lt;em&gt;served&lt;/em&gt;, i.e., made available to clients requesting predictions.&lt;/p&gt;
&lt;p&gt;How can a Keras model be &lt;em&gt;shared&lt;/em&gt; and &lt;em&gt;served&lt;/em&gt;? These are not methods provided by Keras itself. The magic comes from Syft &lt;em&gt;hooking&lt;/em&gt; into Keras, extending the &lt;code&gt;model&lt;/code&gt; object: cf. &lt;code&gt;hook &amp;lt;- sy$KerasHook(tf$keras)&lt;/code&gt; right after we import Syft.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# serve.R
# you could start R on the console and &amp;quot;source&amp;quot; this file

# do this just once
reticulate::py_install(&amp;quot;syft[udacity]&amp;quot;)

library(tensorflow)
library(keras)

sy &amp;lt;- reticulate::import((&amp;quot;syft&amp;quot;))
hook &amp;lt;- sy$KerasHook(tf$keras)

batch_input_shape &amp;lt;- c(1, 28, 28, 1)

model &amp;lt;- keras_model_sequential() %&amp;gt;%
 layer_conv_2d(filters = 16, kernel_size = c(3, 3), batch_input_shape = batch_input_shape) %&amp;gt;%
 layer_average_pooling_2d(pool_size = c(2, 2)) %&amp;gt;%
 layer_activation(&amp;quot;relu&amp;quot;) %&amp;gt;%
 layer_conv_2d(filters = 32, kernel_size = c(3, 3)) %&amp;gt;%
 layer_average_pooling_2d(pool_size = c(2, 2)) %&amp;gt;%
 layer_activation(&amp;quot;relu&amp;quot;) %&amp;gt;%
 layer_conv_2d(filters = 64, kernel_size = c(3, 3)) %&amp;gt;%
 layer_average_pooling_2d(pool_size = c(2, 2)) %&amp;gt;%
 layer_activation(&amp;quot;relu&amp;quot;) %&amp;gt;%
 layer_flatten() %&amp;gt;%
 layer_dense(units = 10) 
 
pre_trained_weights &amp;lt;- &amp;quot;model.hdf5&amp;quot;
model$load_weights(pre_trained_weights)

# create and start TFE cluster
AUTO &amp;lt;- TRUE
julia &amp;lt;- sy$TFEWorker(host = &amp;#39;localhost:4000&amp;#39;, auto_managed = AUTO)
greg &amp;lt;- sy$TFEWorker(host = &amp;#39;localhost:4001&amp;#39;, auto_managed = AUTO)
me &amp;lt;- sy$TFEWorker(host = &amp;#39;localhost:4002&amp;#39;, auto_managed = AUTO)
cluster &amp;lt;- sy$TFECluster(julia, greg, me)
cluster$start()

# split up model weights into shares 
model$share(cluster)

# serve model (limiting number of requests)
model$serve(num_requests = 3L)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the desired number of requests have been served, we can go to this R process, stop model sharing, and shut down the cluster:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# stop model sharing
model$stop()

# stop cluster
cluster$stop()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, on to the client(s).&lt;/p&gt;
&lt;h4 id="request-predictions-on-private-data"&gt;Request predictions on private data&lt;/h4&gt;
&lt;p&gt;In our example, we have one client. The client is a TFE worker, just like the agents that make up the cluster.&lt;/p&gt;
&lt;p&gt;We define the cluster here, client-side, as well; create the client; and connect the client to the model. This will set up a queueing server that takes care of &lt;em&gt;secret sharing&lt;/em&gt; all input data before submitting them for prediction.&lt;/p&gt;
&lt;p&gt;Finally, we have the client asking for classification of the first three MNIST images.&lt;/p&gt;
&lt;p&gt;With the server running in some different R process, we can conveniently run this in RStudio:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# client.R

library(tensorflow)
library(keras)

sy &amp;lt;- reticulate::import((&amp;quot;syft&amp;quot;))
hook &amp;lt;- sy$KerasHook(tf$keras)

mnist &amp;lt;- dataset_mnist()
mnist$train$x &amp;lt;- mnist$train$x/255
mnist$test$x &amp;lt;- mnist$test$x/255

dim(mnist$train$x) &amp;lt;- c(dim(mnist$train$x), 1)
dim(mnist$test$x) &amp;lt;- c(dim(mnist$test$x), 1)

batch_input_shape &amp;lt;- c(1, 28, 28, 1)
batch_output_shape &amp;lt;- c(1, 10)

# define the same TFE cluster
AUTO &amp;lt;- TRUE
julia &amp;lt;- sy$TFEWorker(host = &amp;#39;localhost:4000&amp;#39;, auto_managed = AUTO)
greg &amp;lt;- sy$TFEWorker(host = &amp;#39;localhost:4001&amp;#39;, auto_managed = AUTO)
me &amp;lt;- sy$TFEWorker(host = &amp;#39;localhost:4002&amp;#39;, auto_managed = AUTO)
cluster &amp;lt;- sy$TFECluster(julia, greg, me)

# create the client
client &amp;lt;- sy$TFEWorker()

# create a queueing server on the client that secret shares the data 
# before submitting a prediction request
client$connect_to_model(batch_input_shape, batch_output_shape, cluster)

num_tests &amp;lt;- 3
images &amp;lt;- mnist$test$x[1: num_tests, , , , drop = FALSE]
expected_labels &amp;lt;- mnist$test$y[1: num_tests]

for (i in 1:num_tests) {
  res &amp;lt;- client$query_model(images[i, , , , drop = FALSE])
  predicted_label &amp;lt;- which.max(res) - 1
  cat(&amp;quot;Actual: &amp;quot;, expected_labels[i], &amp;quot;, predicted: &amp;quot;, predicted_label)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Actual:  7 , predicted:  7 
Actual:  2 , predicted:  2 
Actual:  1 , predicted:  1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There we go. Both model and data did remain secret, yet we were able to classify our data.&lt;/p&gt;
&lt;p&gt;Let’s wrap up.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Our example use case has not been too ambitious – we started with a trained model, thus leaving aside federated learning. Keeping the setup simple, we were able to focus on underlying principles: &lt;em&gt;Secret sharing&lt;/em&gt; as a means of encryption, and setting up a Syft/TFE cluster of workers that together, provide the infrastructure for encrypting model weights as well as client data.&lt;/p&gt;
&lt;p&gt;In case you’ve read our previous post on &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-12-20-differential-privacy/"&gt;TensorFlow Federated&lt;/a&gt; – that, too, a framework under development – you may have gotten an impression similar to the one I got: Setting up Syft was a lot more straightforward, concepts were easy to grasp, and surprisingly little code was required. As we may gather from a &lt;a href="https://blog.openmined.org/introducing-pysyft-tensorflow/"&gt;recent blog post&lt;/a&gt;, integration of Syft with TensorFlow Federated and TensorFlow Privacy are on the roadmap. I am looking forward &lt;em&gt;a lot&lt;/em&gt; for this to happen.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;h2 id="section"&gt;&lt;/h2&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-dwork2006differential"&gt;
&lt;p&gt;Dwork, Cynthia. 2006. “Differential Privacy.” In &lt;em&gt;33rd International Colloquium on Automata, Languages and Programming, Part Ii (Icalp 2006)&lt;/em&gt;, 33rd International Colloquium on Automata, Languages and Programming, part II (ICALP 2006), 4052:1–12. Lecture Notes in Computer Science. Springer Verlag. &lt;a href="https://www.microsoft.com/en-us/research/publication/differential-privacy/"&gt;https://www.microsoft.com/en-us/research/publication/differential-privacy/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Dwork2006"&gt;
&lt;p&gt;Dwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. “Calibrating Noise to Sensitivity in Private Data Analysis.” In &lt;em&gt;Proceedings of the Third Conference on Theory of Cryptography&lt;/em&gt;, 265–84. TCC’06. Berlin, Heidelberg: Springer-Verlag. &lt;a href="https://doi.org/10.1007/11681878_14"&gt;https://doi.org/10.1007/11681878_14&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-McMahanMRA16"&gt;
&lt;p&gt;McMahan, H. Brendan, Eider Moore, Daniel Ramage, and Blaise Agüera y Arcas. 2016. “Federated Learning of Deep Networks Using Model Averaging.” &lt;em&gt;CoRR&lt;/em&gt; abs/1602.05629. &lt;a href="http://arxiv.org/abs/1602.05629"&gt;http://arxiv.org/abs/1602.05629&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">dbe1974c9c6e217e2558943db1540a10</distill:md5>
      <category>R</category>
      <category>Privacy &amp; Security</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft</guid>
      <pubDate>Wed, 29 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft/images/thumb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>A first look at federated learning with TensorFlow</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-08-tf-federated-intro</link>
      <description>


&lt;p&gt;Here, stereotypically, is the process of applied deep learning: Gather/get data; iteratively train and evaluate; deploy. Repeat (or have it all automated as a continuous workflow). We often discuss training and evaluation; deployment matters to varying degrees, depending on the circumstances. But the data often is just assumed to be there: All together, in one place (on your laptop; on a central server; in some cluster in the cloud.) In real life though, data could be all over the world: on smartphones for example, or on IoT devices. There are a lot of reasons why we don’t want to ship all that data to some central location: Privacy, of course (why should some third party get to know about what you texted your friend?); but also, sheer mass (and this latter aspect is bound to become more influential all the time).&lt;/p&gt;
&lt;p&gt;A solution is that data on client devices stays on client devices, yet participates in training a global model. How? In so-called &lt;em&gt;federated learning&lt;/em&gt;&lt;span class="citation"&gt;(McMahan et al. 2016)&lt;/span&gt;, there is a central coordinator (“server”), as well as a potentially huge number of clients (e.g., phones) who participate in learning on an “as-fits” basis: e.g., if plugged in and on a high-speed connection. Whenever they’re ready to train, clients are passed the current model weights, and perform some number of training iterations on their own data. They then send back gradient information to the server (more on that soon), whose job is to update the weights accordingly. Federated learning is not the only conceivable protocol to jointly train a deep learning model while keeping the data private: A fully decentralized alternative could be &lt;em&gt;gossip learning&lt;/em&gt; &lt;span class="citation"&gt;(Blot et al. 2016)&lt;/span&gt;, following the &lt;a href="https://en.wikipedia.org/wiki/Gossip_protocol"&gt;gossip protocol&lt;/a&gt; . As of today, however, I am not aware of existing implementations in any of the major deep learning frameworks.&lt;/p&gt;
&lt;p&gt;In fact, even TensorFlow Federated (TFF), the library used in this post, was officially introduced just about a year ago. Meaning, all this is pretty new technology, somewhere inbetween proof-of-concept state and production readiness. So, let’s set expectations as to what you might get out of this post.&lt;/p&gt;
&lt;h3 id="what-to-expect-from-this-post"&gt;What to expect from this post&lt;/h3&gt;
&lt;p&gt;We start with quick glance at federated learning in the context of &lt;em&gt;privacy&lt;/em&gt; overall. Subsequently, we introduce, by example, some of TFF’s basic building blocks. Finally, we show a complete image classification example using Keras – from R.&lt;/p&gt;
&lt;p&gt;While this sounds like “business as usual”, it’s not – or not quite. With no R package existing, as of this writing, that would wrap TFF, we’re accessing its functionality using &lt;code&gt;$&lt;/code&gt;-syntax – not in itself a big problem. But there’s something else.&lt;/p&gt;
&lt;p&gt;TFF, while providing a Python API, itself is not written in Python. Instead, it is an internal language designed specifically for serializability and distributed computation. One of the consequences is that TensorFlow (that is: TF as opposed to TFF) code has to be wrapped in calls to &lt;code&gt;tf.function&lt;/code&gt;, triggering static-graph construction. However, as I write this, the TFF documentation &lt;a href="https://github.com/tensorflow/federated/blob/master/docs/federated_learning.md"&gt;cautions&lt;/a&gt;: “Currently, TensorFlow does not fully support serializing and deserializing eager-mode TensorFlow.” Now when we call TFF from R, we add another layer of complexity, and are more likely to run into corner cases.&lt;/p&gt;
&lt;p&gt;Therefore, at the current stage, when using TFF from R it’s advisable to play around with high-level functionality – using Keras models – instead of, e.g., translating to R the low-level functionality shown in the &lt;a href="https://github.com/tensorflow/federated/blob/master/docs/tutorials/custom_federated_algorithms_2.ipynb"&gt;second TFF Core tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One final remark before we get started: As of this writing, there is no documentation on how to actually run federated training on “real clients”. There is, however, a &lt;a href="https://github.com/tensorflow/federated/blob/master/docs/tutorials/high_performance_simulation_with_kubernetes.ipynb"&gt;document&lt;/a&gt; that describes how to run TFF on Google Kubernetes Engine, and deployment-related documentation is visibly and steadily growing.)&lt;/p&gt;
&lt;p&gt;That said, now how does federated learning relate to privacy, and how does it look in TFF?&lt;/p&gt;
&lt;h3 id="federated-learning-in-context"&gt;Federated learning in context&lt;/h3&gt;
&lt;p&gt;In federated learning, client data never leaves the device. So in an immediate sense, computations are private. However, gradient updates are sent to a central server, and this is where privacy guarantees may be violated. In some cases, it may be easy to reconstruct the actual data from the gradients – in an NLP task, for example, when the vocabulary is known on the server, and gradient updates are sent for small pieces of text.&lt;/p&gt;
&lt;p&gt;This may sound like a special case, but general methods have been demonstrated that work regardless of circumstances. For example, Zhu et al. &lt;span class="citation"&gt;(Zhu, Liu, and Han 2019)&lt;/span&gt; use a “generative” approach, with the server starting from randomly generated fake data (resulting in fake gradients) and then, iteratively updating that data to obtain gradients more and more like the real ones – at which point the real data has been reconstructed.&lt;/p&gt;
&lt;p&gt;Comparable attacks would not be feasible were gradients not sent in clear text. However, the server needs to actually use them to update the model – so it must be able to “see” them, right? As hopeless as this sounds, there are ways out of the dilemma. For example, &lt;a href="https://en.wikipedia.org/wiki/Homomorphic_encryption"&gt;homomorphic encryption&lt;/a&gt;, a technique that enables computation on encrypted data. Or &lt;a href="https://en.wikipedia.org/wiki/Secure_multi-party_computation"&gt;secure multi-party aggregation&lt;/a&gt;, often achieved through &lt;a href="https://en.wikipedia.org/wiki/Secret_sharing"&gt;secret sharing&lt;/a&gt;, where individual pieces of data (e.g.: individual salaries) are split up into “shares”, exchanged and combined with random data in various ways, until finally the desired global result (e.g.: mean salary) is computed. (These are extremely fascinating topics that unfortunately, by far surpass the scope of this post.)&lt;/p&gt;
&lt;p&gt;Now, with the server prevented from actually “seeing” the gradients, a problem still remains. The model – especially a high-capacity one, with many parameters – could still memorize individual training data. Here is where &lt;em&gt;differential privacy&lt;/em&gt; comes into play. In differential privacy, noise is added to the gradients to decouple them from actual training examples. (&lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-12-20-differential-privacy/"&gt;This post&lt;/a&gt; gives an introduction to differential privacy with TensorFlow, from R.)&lt;/p&gt;
&lt;p&gt;As of this writing, TFF’s federal averaging mechanism &lt;span class="citation"&gt;(McMahan et al. 2016)&lt;/span&gt; does not yet include these additional privacy-preserving techniques. But research papers exist that outline algorithms for integrating both secure aggregation &lt;span class="citation"&gt;(Bonawitz et al. 2016)&lt;/span&gt; and differential privacy &lt;span class="citation"&gt;(McMahan et al. 2017)&lt;/span&gt; .&lt;/p&gt;
&lt;h3 id="client-side-and-server-side-computations"&gt;Client-side and server-side computations&lt;/h3&gt;
&lt;p&gt;Like we said above, at this point it is advisable to mainly stick with high-level computations using TFF from R. (Presumably that is what we’d be interested in in many cases, anyway.) But it’s instructive to look at a few building blocks from a high-level, functional point of view.&lt;/p&gt;
&lt;p&gt;In federated learning, model training happens on the clients. Clients each compute their local gradients, as well as local metrics. The server, on the other hand, calculates global gradient updates, as well as global metrics.&lt;/p&gt;
&lt;p&gt;Let’s say the metric is accuracy. Then clients and server both compute averages: local averages and a global average, respectively. All the server will need to know to determine the global averages are the local ones and the respective sample sizes.&lt;/p&gt;
&lt;p&gt;Let’s see how TFF would calculate a simple average.&lt;/p&gt;
&lt;p&gt;The code in this post was run with the current TensorFlow release 2.1 and TFF version 0.13.1. We use &lt;code&gt;reticulate&lt;/code&gt; to install and import TFF.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tensorflow)
library(reticulate)
library(tfdatasets)

py_install(&amp;quot;tensorflow-federated&amp;quot;)

tff &amp;lt;- import(&amp;quot;tensorflow_federated&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, we need every client to be able to compute their own local averages.&lt;/p&gt;
&lt;p&gt;Here is a function that &lt;em&gt;reduces&lt;/em&gt; a list of values to their sum and count, both at the same time, and then returns their quotient.&lt;/p&gt;
&lt;p&gt;The function contains only TensorFlow operations, not computations described in R directly; if there were any, they would have to be wrapped in calls to &lt;code&gt;tf_function&lt;/code&gt;, calling for construction of a static graph. (The same would apply to raw (non-TF) Python code.)&lt;/p&gt;
&lt;p&gt;Now, this function will still have to be wrapped (we’re getting to that in an instant), as TFF expects functions that make use of TF operations to be &lt;em&gt;decorated&lt;/em&gt; by calls to &lt;code&gt;tff$tf_computation&lt;/code&gt;. Before we do that, one comment on the use of &lt;code&gt;dataset_reduce&lt;/code&gt;: Inside &lt;code&gt;tff$tf_computation&lt;/code&gt;, the data that is passed in behaves like a &lt;code&gt;dataset&lt;/code&gt;, so we can perform &lt;code&gt;tfdatasets&lt;/code&gt; operations like &lt;code&gt;dataset_map&lt;/code&gt;, &lt;code&gt;dataset_filter&lt;/code&gt; etc. on it.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;get_local_temperature_average &amp;lt;- function(local_temperatures) {
  sum_and_count &amp;lt;- local_temperatures %&amp;gt;% 
    dataset_reduce(tuple(0, 0), function(x, y) tuple(x[[1]] + y, x[[2]] + 1))
  sum_and_count[[1]] / tf$cast(sum_and_count[[2]], tf$float32)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next is the call to &lt;code&gt;tff$tf_computation&lt;/code&gt; we already alluded to, wrapping &lt;code&gt;get_local_temperature_average&lt;/code&gt;. We also need to indicate the argument’s TFF-level type. (In the context of this post, TFF datatypes are definitely out-of-scope, but the TFF documentation has lots of detailed information in that regard. All we need to know right now is that we will be able to pass the data as a &lt;code&gt;list&lt;/code&gt;.)&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;get_local_temperature_average &amp;lt;- tff$tf_computation(get_local_temperature_average, tff$SequenceType(tf$float32))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s test this function:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;get_local_temperature_average(list(1, 2, 3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So that’s a local average, but we originally set out to compute a global one. Time to move on to server side (code-wise).&lt;/p&gt;
&lt;p&gt;Non-local computations are called &lt;em&gt;federated&lt;/em&gt; (not too surprisingly). Individual operations start with &lt;code&gt;federated_&lt;/code&gt;; and these have to be wrapped in &lt;code&gt;tff$federated_computation&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;get_global_temperature_average &amp;lt;- function(sensor_readings) {
  tff$federated_mean(tff$federated_map(get_local_temperature_average, sensor_readings))
}

get_global_temperature_average &amp;lt;- tff$federated_computation(
  get_global_temperature_average, tff$FederatedType(tff$SequenceType(tf$float32), tff$CLIENTS))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Calling this on a list of lists – each sub-list presumedly representing client data – will display the global (non-weighted) average:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;get_global_temperature_average(list(list(1, 1, 1), list(13)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we’ve gotten a bit of a feeling for “low-level TFF”, let’s train a Keras model the federated way.&lt;/p&gt;
&lt;h3 id="federated-keras"&gt;Federated Keras&lt;/h3&gt;
&lt;p&gt;The setup for this example looks a bit more Pythonian&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; than usual. We need the &lt;code&gt;collections&lt;/code&gt; module from Python to make use of &lt;code&gt;OrderedDict&lt;/code&gt;s, and we want them to be passed to Python without intermediate conversion to R – that’s why we import the module with &lt;code&gt;convert&lt;/code&gt; set to &lt;code&gt;FALSE&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tensorflow)
library(keras)
library(tfds)
library(reticulate)
library(tfdatasets)
library(dplyr)

tff &amp;lt;- import(&amp;quot;tensorflow_federated&amp;quot;)
collections &amp;lt;- import(&amp;quot;collections&amp;quot;, convert = FALSE)
np &amp;lt;- import(&amp;quot;numpy&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this example, we use &lt;a href="https://github.com/rois-codh/kmnist"&gt;Kuzushiji-MNIST&lt;/a&gt; &lt;span class="citation"&gt;(Clanuwat et al. 2018)&lt;/span&gt;, which may conveniently be obtained through &lt;a href="https://github.com/rstudio/tfds"&gt;tfds&lt;/a&gt;, the R wrapper for &lt;a href="https://www.tensorflow.org/datasets"&gt;TensorFlow Datasets&lt;/a&gt;.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-04-08-tf-federated-intro/images/kmnist_examples.png" alt="" /&gt;
&lt;p class="caption"&gt;The 10 classes of Kuzushiji-MNIST, with the first column showing each character's modern hiragana counterpart. From: &lt;a href="https://github.com/rois-codh/kmnist" class="uri"&gt;https://github.com/rois-codh/kmnist&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;TensorFlow datasets come as – well – &lt;code&gt;dataset&lt;/code&gt;s, which normally would be just fine; here however, we want to simulate different clients each with their own data. The following code splits up the dataset into ten arbitrary – sequential, for convenience – ranges and, for each range (that is: client), creates a list of &lt;code&gt;OrderedDict&lt;/code&gt;s that have the images as their &lt;code&gt;x&lt;/code&gt;, and the labels as their &lt;code&gt;y&lt;/code&gt; component:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;n_train &amp;lt;- 60000
n_test &amp;lt;- 10000

s &amp;lt;- seq(0, 90, by = 10)
train_ranges &amp;lt;- paste0(&amp;quot;train[&amp;quot;, s, &amp;quot;%:&amp;quot;, s + 10, &amp;quot;%]&amp;quot;) %&amp;gt;% as.list()
train_splits &amp;lt;- purrr::map(train_ranges, function(r) tfds_load(&amp;quot;kmnist&amp;quot;, split = r))

test_ranges &amp;lt;- paste0(&amp;quot;test[&amp;quot;, s, &amp;quot;%:&amp;quot;, s + 10, &amp;quot;%]&amp;quot;) %&amp;gt;% as.list()
test_splits &amp;lt;- purrr::map(test_ranges, function(r) tfds_load(&amp;quot;kmnist&amp;quot;, split = r))

batch_size &amp;lt;- 100

create_client_dataset &amp;lt;- function(source, n_total, batch_size) {
  iter &amp;lt;- as_iterator(source %&amp;gt;% dataset_batch(batch_size))
  output_sequence &amp;lt;- vector(mode = &amp;quot;list&amp;quot;, length = n_total/10/batch_size)
  i &amp;lt;- 1
  while (TRUE) {
    item &amp;lt;- iter_next(iter)
    if (is.null(item)) break
    x &amp;lt;- tf$reshape(tf$cast(item$image, tf$float32), list(100L,784L))/255
    y &amp;lt;- item$label
    output_sequence[[i]] &amp;lt;-
      collections$OrderedDict(&amp;quot;x&amp;quot; = np_array(x$numpy(), np$float32), &amp;quot;y&amp;quot; = y$numpy())
     i &amp;lt;- i + 1
  }
  output_sequence
}

federated_train_data &amp;lt;- purrr::map(
  train_splits, function(split) create_client_dataset(split, n_train, batch_size))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a quick check, the following are the labels for the first batch of images for client 5:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;federated_train_data[[5]][[1]][[&amp;#39;y&amp;#39;]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; [0. 9. 8. 3. 1. 6. 2. 8. 8. 2. 5. 7. 1. 6. 1. 0. 3. 8. 5. 0. 5. 6. 6. 5.
 2. 9. 5. 0. 3. 1. 0. 0. 6. 3. 6. 8. 2. 8. 9. 8. 5. 2. 9. 0. 2. 8. 7. 9.
 2. 5. 1. 7. 1. 9. 1. 6. 0. 8. 6. 0. 5. 1. 3. 5. 4. 5. 3. 1. 3. 5. 3. 1.
 0. 2. 7. 9. 6. 2. 8. 8. 4. 9. 4. 2. 9. 5. 7. 6. 5. 2. 0. 3. 4. 7. 8. 1.
 8. 2. 7. 9.]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model is a simple, one-layer sequential Keras model. For TFF to have full control over graph construction, it has to be defined inside a function. The blueprint for creation is passed to &lt;code&gt;tff$learning$from_keras_model&lt;/code&gt;, together with a “dummy” batch that exemplifies how the training data will look:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;sample_batch = federated_train_data[[5]][[1]]

create_keras_model &amp;lt;- function() {
  keras_model_sequential() %&amp;gt;%
    layer_dense(input_shape = 784,
                units = 10,
                kernel_initializer = &amp;quot;zeros&amp;quot;,
                activation = &amp;quot;softmax&amp;quot;) 
}

model_fn &amp;lt;- function() {
  keras_model &amp;lt;- create_keras_model()
  tff$learning$from_keras_model(
    keras_model,
    dummy_batch = sample_batch,
    loss = tf$keras$losses$SparseCategoricalCrossentropy(),
    metrics = list(tf$keras$metrics$SparseCategoricalAccuracy()))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Training is a stateful process that keeps updating model weights (and if applicable, optimizer states). It is created via &lt;code&gt;tff$learning$build_federated_averaging_process&lt;/code&gt; …&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;iterative_process &amp;lt;- tff$learning$build_federated_averaging_process(
  model_fn,
  client_optimizer_fn = function() tf$keras$optimizers$SGD(learning_rate = 0.02),
  server_optimizer_fn = function() tf$keras$optimizers$SGD(learning_rate = 1.0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;… and on initialization, produces a starting state:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;state &amp;lt;- iterative_process$initialize()
state&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;model=&amp;lt;trainable=&amp;lt;[[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]],[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]&amp;gt;,non_trainable=&amp;lt;&amp;gt;&amp;gt;,optimizer_state=&amp;lt;0&amp;gt;,delta_aggregate_state=&amp;lt;&amp;gt;,model_broadcast_state=&amp;lt;&amp;gt;&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus before training, all the state does is reflect our zero-initialized model weights.&lt;/p&gt;
&lt;p&gt;Now, state transitions are accomplished via calls to &lt;code&gt;next()&lt;/code&gt;. After one round of training, the state then comprises the “state proper” (weights, optimizer parameters …) as well as the current training metrics:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;state_and_metrics &amp;lt;- iterative_process$`next`(state, federated_train_data)

state &amp;lt;- state_and_metrics[0]
state&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;model=&amp;lt;trainable=&amp;lt;[[ 9.9695253e-06 -8.5083229e-05 -8.9266898e-05 ... -7.7834651e-05
  -9.4819807e-05  3.4227365e-04]
 [-5.4778640e-05 -1.5390900e-04 -1.7912561e-04 ... -1.4122366e-04
  -2.4614178e-04  7.7663612e-04]
 [-1.9177950e-04 -9.0706220e-05 -2.9841764e-04 ... -2.2249141e-04
  -4.1685964e-04  1.1348884e-03]
 ...
 [-1.3832574e-03 -5.3664664e-04 -3.6622395e-04 ... -9.0854493e-04
   4.9618416e-04  2.6899918e-03]
 [-7.7253254e-04 -2.4583895e-04 -8.3220737e-05 ... -4.5274393e-04
   2.6396243e-04  1.7454443e-03]
 [-2.4157032e-04 -1.3836231e-05  5.0371520e-05 ... -1.0652864e-04
   1.5947431e-04  4.5250656e-04]],[-0.01264258  0.00974309  0.00814162  0.00846065 -0.0162328   0.01627758
 -0.00445857 -0.01607843  0.00563046  0.00115899]&amp;gt;,non_trainable=&amp;lt;&amp;gt;&amp;gt;,optimizer_state=&amp;lt;1&amp;gt;,delta_aggregate_state=&amp;lt;&amp;gt;,model_broadcast_state=&amp;lt;&amp;gt;&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;metrics &amp;lt;- state_and_metrics[1]
metrics&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;sparse_categorical_accuracy=0.5710999965667725,loss=1.8662642240524292,keras_training_time_client_sum_sec=0.0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s train for a few more epochs, keeping track of accuracy:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;num_rounds &amp;lt;- 20

for (round_num in (2:num_rounds)) {
  state_and_metrics &amp;lt;- iterative_process$`next`(state, federated_train_data)
  state &amp;lt;- state_and_metrics[0]
  metrics &amp;lt;- state_and_metrics[1]
  cat(&amp;quot;round: &amp;quot;, round_num, &amp;quot;  accuracy: &amp;quot;, round(metrics$sparse_categorical_accuracy, 4), &amp;quot;\n&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;round:  2    accuracy:  0.6949 
round:  3    accuracy:  0.7132 
round:  4    accuracy:  0.7231 
round:  5    accuracy:  0.7319 
round:  6    accuracy:  0.7404 
round:  7    accuracy:  0.7484 
round:  8    accuracy:  0.7557 
round:  9    accuracy:  0.7617 
round:  10   accuracy:  0.7661 
round:  11   accuracy:  0.7695 
round:  12   accuracy:  0.7728 
round:  13   accuracy:  0.7764 
round:  14   accuracy:  0.7788 
round:  15   accuracy:  0.7814 
round:  16   accuracy:  0.7836 
round:  17   accuracy:  0.7855 
round:  18   accuracy:  0.7872 
round:  19   accuracy:  0.7885 
round:  20   accuracy:  0.7902 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Training accuracy is increasing continuously. These values represent averages of &lt;em&gt;local&lt;/em&gt; accuracy measurements, so in the real world, they might well be overly optimistic (with each client overfitting on their respective data). So supplementing federated training, a federated evaluation process would need to be built in order to get a realistic view on performance. This is a topic to come back to when more related TFF documentation is available.&lt;/p&gt;
&lt;h3 id="conclusion"&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;We hope you’ve enjoyed this first introduction to TFF using R. Certainly at this time, it is too early for use in production; and for application in research (e.g., adversarial attacks on federated learning) familiarity with “lowish”-level implementation code is required – regardless whether you use R or Python.&lt;/p&gt;
&lt;p&gt;However, judging from activity on GitHub, TFF is under very active development right now (including new documentation being added!), so we’re looking forward to what’s to come. In the meantime, it’s never too early to start learning the concepts…&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-BlotPCT16"&gt;
&lt;p&gt;Blot, Michael, David Picard, Matthieu Cord, and Nicolas Thome. 2016. “Gossip Training for Deep Learning.” &lt;em&gt;CoRR&lt;/em&gt; abs/1611.09726. &lt;a href="http://arxiv.org/abs/1611.09726"&gt;http://arxiv.org/abs/1611.09726&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-BonawitzIKMMPRS16"&gt;
&lt;p&gt;Bonawitz, Keith, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. 2016. “Practical Secure Aggregation for Federated Learning on User-Held Data.” &lt;em&gt;CoRR&lt;/em&gt; abs/1611.04482. &lt;a href="http://arxiv.org/abs/1611.04482"&gt;http://arxiv.org/abs/1611.04482&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-clanuwat2018deep"&gt;
&lt;p&gt;Clanuwat, Tarin, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. 2018. “Deep Learning for Classical Japanese Literature.” December 3, 2018. &lt;a href="http://arxiv.org/abs/cs.CV/1812.01718"&gt;http://arxiv.org/abs/cs.CV/1812.01718&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-McMahanMRA16"&gt;
&lt;p&gt;McMahan, H. Brendan, Eider Moore, Daniel Ramage, and Blaise Agüera y Arcas. 2016. “Federated Learning of Deep Networks Using Model Averaging.” &lt;em&gt;CoRR&lt;/em&gt; abs/1602.05629. &lt;a href="http://arxiv.org/abs/1602.05629"&gt;http://arxiv.org/abs/1602.05629&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-abs-1710-06963"&gt;
&lt;p&gt;McMahan, H. Brendan, Daniel Ramage, Kunal Talwar, and Li Zhang. 2017. “Learning Differentially Private Language Models Without Losing Accuracy.” &lt;em&gt;CoRR&lt;/em&gt; abs/1710.06963. &lt;a href="http://arxiv.org/abs/1710.06963"&gt;http://arxiv.org/abs/1710.06963&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-abs-1906-08935"&gt;
&lt;p&gt;Zhu, Ligeng, Zhijian Liu, and Song Han. 2019. “Deep Leakage from Gradients.” &lt;em&gt;CoRR&lt;/em&gt; abs/1906.08935. &lt;a href="http://arxiv.org/abs/1906.08935"&gt;http://arxiv.org/abs/1906.08935&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;not &lt;em&gt;Pythonic&lt;/em&gt; :-)&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">1b4496519c6088e730e857befa4f0353</distill:md5>
      <category>Privacy &amp; Security</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-08-tf-federated-intro</guid>
      <pubDate>Wed, 08 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-08-tf-federated-intro/images/federated_learning.png" medium="image" type="image/png" width="1122" height="570"/>
    </item>
    <item>
      <title>NumPy-style broadcasting for R TensorFlow users</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-01-24-numpy-broadcasting</link>
      <description>


&lt;p&gt;We develop, train, and deploy TensorFlow models from R. But that doesn’t mean we don’t make use of documentation, blog posts, and examples written in Python. We look up specific functionality in the &lt;a href="https://www.tensorflow.org/api_docs/python/tf"&gt;official TensorFlow API docs&lt;/a&gt;; we get inspiration from other people’s code.&lt;/p&gt;
&lt;p&gt;Depending on how comfortable you are with Python, there’s a problem. For example: You’re supposed to know how &lt;em&gt;broadcasting&lt;/em&gt; works. And perhaps, you’d say you’re vaguely familiar with it: So when arrays have different shapes, some elements get duplicated until their shapes match and … and isn’t R vectorized anyway?&lt;/p&gt;
&lt;p&gt;While such a global notion may work in general, like when skimming a blog post, it’s not enough to understand, say, examples in the TensorFlow API docs. In this post, we’ll try to arrive at a more exact understanding, and check it on concrete examples.&lt;/p&gt;
&lt;p&gt;Speaking of examples, here are two motivating ones.&lt;/p&gt;
&lt;h2 id="broadcasting-in-action"&gt;Broadcasting in action&lt;/h2&gt;
&lt;p&gt;The first uses TensorFlow’s &lt;code&gt;matmul&lt;/code&gt; to multiply two tensors. Would you like to guess the result – not the numbers, but how it comes about in general? Does this even run without error – shouldn’t matrices be two-dimensional (&lt;em&gt;rank&lt;/em&gt;-2 tensors, in TensorFlow speak)?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;a &amp;lt;- tf$constant(keras::array_reshape(1:12, dim = c(2, 2, 3)))
a 
# tf.Tensor(
# [[[ 1.  2.  3.]
#   [ 4.  5.  6.]]
# 
#  [[ 7.  8.  9.]
#   [10. 11. 12.]]], shape=(2, 2, 3), dtype=float64)

b &amp;lt;- tf$constant(keras::array_reshape(101:106, dim = c(1, 3, 2)))
b  
# tf.Tensor(
# [[[101. 102.]
#   [103. 104.]
#   [105. 106.]]], shape=(1, 3, 2), dtype=float64)

c &amp;lt;- tf$matmul(a, b)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Second, here is a “real example” from a TensorFlow Probability (TFP) &lt;a href="https://github.com/tensorflow/probability/issues/716"&gt;github issue&lt;/a&gt;. (Translated to R, but keeping the semantics). In TFP, we can have &lt;em&gt;batches&lt;/em&gt; of distributions. That, per se, is not surprising. But look at this:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tfprobability)
d &amp;lt;- tfd_normal(loc = c(0, 1), scale = matrix(1.5:4.5, ncol = 2, byrow = TRUE))
d
# tfp.distributions.Normal(&amp;quot;Normal&amp;quot;, batch_shape=[2, 2], event_shape=[], dtype=float64)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We create a batch of four normal distributions: each with a different &lt;em&gt;scale&lt;/em&gt; (1.5, 2.5, 3.5, 4.5). But wait: there are only two &lt;em&gt;location&lt;/em&gt; parameters given. So what are their &lt;em&gt;scales&lt;/em&gt;, respectively? Thankfully, TFP developers Brian Patton and Chris Suter explained how it works: TFP actually does broadcasting – with distributions – just like with tensors!&lt;/p&gt;
&lt;p&gt;We get back to both examples at the end of this post. Our main focus will be to explain broadcasting as done in NumPy, as NumPy-style broadcasting is what numerous other frameworks have adopted (e.g., TensorFlow).&lt;/p&gt;
&lt;p&gt;Before though, let’s quickly review a few basics about NumPy arrays: How to index or &lt;em&gt;slice&lt;/em&gt; them (indexing normally referring to single-element extraction, while slicing would yield – well – slices containing several elements); how to parse their shapes; some terminology and related background. Though not complicated per se, these are the kinds of things that can be confusing to infrequent Python users; yet they’re often a prerequisite to successfully making use of Python documentation.&lt;/p&gt;
&lt;p&gt;Stated upfront, we’ll really restrict ourselves to the basics here; for example, we won’t touch &lt;a href="https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing"&gt;advanced indexing&lt;/a&gt; which – just like lots more –, can be looked up in detail in the &lt;a href="https://docs.scipy.org/doc/numpy/reference/index.html"&gt;NumPy documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="few-facts-about-numpy"&gt;Few facts about NumPy&lt;/h2&gt;
&lt;h3 id="basic-slicing"&gt;Basic slicing&lt;/h3&gt;
&lt;p&gt;For simplicity, we’ll use the terms indexing and slicing more or less synonymously from now on. The basic device here is a &lt;em&gt;slice&lt;/em&gt;, namely, a &lt;code&gt;start:stop&lt;/code&gt; &lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; structure indicating, for a single dimension, which range of elements to include in the selection.&lt;/p&gt;
&lt;p&gt;In contrast to R, Python indexing is zero-based, and the end index is exclusive:&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;import numpy as np
x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

x[1:7] 
# array([1, 2, 3, 4, 5, 6])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Minus&lt;/em&gt;, to R users, is a false friend; it means we start counting from the end (the last element being -1):&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;x[-2:10] 
# array([8, 9])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Leaving out &lt;code&gt;start&lt;/code&gt; (&lt;code&gt;stop&lt;/code&gt;, resp.) selects all elements from the start (till the end). This may feel so convenient that Python users might miss it in R:&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;x[5:] 
# array([5, 6, 7, 8, 9])

x[:7]
# array([0, 1, 2, 3, 4, 5, 6])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just to make a point about the syntax, we could leave out &lt;em&gt;both&lt;/em&gt; the &lt;code&gt;start&lt;/code&gt; and the &lt;code&gt;stop&lt;/code&gt; indices, in this one-dimensional case effectively resulting in a no-op:&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;x[:] 
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Going on to two dimensions – without commenting on array creation just yet –, we can immediately apply the “semicolon trick” here too. This will select the second row with all its columns:&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;x = np.array([[1, 2], [3, 4], [5, 6]])
x
# array([[1, 2],
#        [3, 4],
#        [5, 6]])

x[1, :] 
# array([3, 4])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While this, arguably, makes for the easiest way to achieve that result and thus, would be the way you’d write it yourself, it’s good to know that these are two alternative ways that do the same:&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;x[1] 
# array([3, 4])

x[1, ] 
# array([3, 4])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While the second one sure looks a bit like R, the mechanism is different. Technically, these &lt;code&gt;start:stop&lt;/code&gt; things are parts of a Python &lt;em&gt;tuple&lt;/em&gt; – that list-like, but immutable data structure that can be written with or without parentheses, e.g., &lt;code&gt;1,2&lt;/code&gt; or &lt;code&gt;(1,2&lt;/code&gt;) –, and whenever we have more dimensions in the array than elements in the tuple NumPy will assume we meant &lt;code&gt;:&lt;/code&gt; for that dimension: Just select everything.&lt;/p&gt;
&lt;p&gt;We can see that moving on to three dimensions. Here is a 2 x 3 x 1-dimensional array:&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;x = np.array([[[1],[2],[3]], [[4],[5],[6]]])
x
# array([[[1],
#         [2],
#         [3]],
# 
#        [[4],
#         [5],
#         [6]]])

x.shape
# (2, 3, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In R, this would throw an error, while in Python it works:&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;x[0,]
#array([[1],
#       [2],
#       [3]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In such a case, for enhanced readability we could instead use the so-called &lt;code&gt;Ellipsis&lt;/code&gt;, explicitly asking Python to “use up all dimensions required to make this work”:&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;x[0, ...]
#array([[1],
#       [2],
#       [3]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We stop here with our selection of essential (yet confusing, possibly, to infrequent Python users) Numpy indexing features; re. “possibly confusing” though, here are a few remarks about array creation.&lt;/p&gt;
&lt;h3 id="syntax-for-array-creation"&gt;Syntax for array creation&lt;/h3&gt;
&lt;p&gt;Creating a more-dimensional NumPy array is not that hard – depending on how you do it. The trick is to use &lt;code&gt;reshape&lt;/code&gt; to tell NumPy exactly what shape you want. For example, to create an array of all zeros, of dimensions 3 x 4 x 2:&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;np.zeros(24).reshape(4, 3, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But we also want to understand what others might write. And then, you might see things like these:&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;c1 = np.array([[[0, 0, 0]]])
c2 = np.array([[[0], [0], [0]]]) 
c3 = np.array([[[0]], [[0]], [[0]]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These are all 3-dimensional, and all have three elements, so their shapes must be 1 x 1 x 3, 1 x 3 x 1, and 3 x 1 x 1, in some order. Of course, &lt;code&gt;shape&lt;/code&gt; is there to tell us:&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;c1.shape # (1, 1, 3)
c2.shape # (1, 3, 1)
c3.shape # (3, 1, 1) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;but we’d like to be able to “parse” internally without executing the code. One way to think about it would be processing the brackets like a state machine, every opening bracket moving one axis to the right and every closing bracket moving back left by one axis. Let us know if you can think of other – possibly more helpful – mnemonics!&lt;/p&gt;
&lt;p&gt;In the very last sentence, we on purpose used “left” and “right” referring to the array axes; “out there” though, you’ll also hear “outmost” and “innermost”. Which, then, is which?&lt;/p&gt;
&lt;h3 id="a-bit-of-terminology"&gt;A bit of terminology&lt;/h3&gt;
&lt;p&gt;In common Python (TensorFlow, for example) usage, when talking of an array shape like &lt;code&gt;(2, 6, 7)&lt;/code&gt;, &lt;em&gt;outmost&lt;/em&gt; is &lt;em&gt;left&lt;/em&gt; and &lt;em&gt;innermost&lt;/em&gt; is &lt;em&gt;right&lt;/em&gt;. Why? Let’s take a simpler, two-dimensional example of shape &lt;code&gt;(2, 3)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;a = np.array([[1, 2, 3], [4, 5, 6]])
a
# array([[1, 2, 3],
#        [4, 5, 6]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Computer memory is conceptually one-dimensional, a sequence of locations; so when we create arrays in a high-level programming language, their contents are effectively “flattened” into a vector. That flattening could occur “by row” (&lt;em&gt;row-major&lt;/em&gt;, &lt;em&gt;C-style&lt;/em&gt;, the default in NumPy), resulting in the above array ending up like this&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1 2 3 4 5 6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or “by column” (&lt;em&gt;column-major&lt;/em&gt;, &lt;em&gt;Fortran-style&lt;/em&gt;, the ordering used in R), yielding&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1 4 2 5 3 6
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;for the above example.&lt;/p&gt;
&lt;p&gt;Now if we see “outmost” as the axis whose index varies the least often, and “innermost” as the one that changes most quickly, in row-major ordering the left axis is “outer”, and the right one is “inner”.&lt;/p&gt;
&lt;p&gt;Just as a (cool!) aside, NumPy arrays have an attribute called &lt;code&gt;strides&lt;/code&gt; that stores how many bytes have to be traversed, for each axis, to arrive at its next element. For our above example:&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;c1 = np.array([[[0, 0, 0]]])
c1.shape   # (1, 1, 3)
c1.strides # (24, 24, 8)

c2 = np.array([[[0], [0], [0]]]) 
c2.shape   # (1, 3, 1)
c2.strides # (24, 8, 8)

c3 = np.array([[[0]], [[0]], [[0]]])
c3.shape   # (3, 1, 1) 
c3.strides # (8, 8, 8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For array &lt;code&gt;c3&lt;/code&gt;, every element is on its own on the outmost level; so for axis 0, to jump from one element to the next, it’s just 8 bytes. For &lt;code&gt;c2&lt;/code&gt; and &lt;code&gt;c1&lt;/code&gt; though, everything is “squished” in the first element of axis 0 (there is just a single element there). So if we wanted to jump to another, nonexisting-as-yet, outmost item, it’d take us 3 * 8 = 24 bytes.&lt;/p&gt;
&lt;p&gt;At this point, we’re ready to talk about broadcasting. We first stay with NumPy and then, examine some TensorFlow examples.&lt;/p&gt;
&lt;h2 id="numpy-broadcasting"&gt;NumPy Broadcasting&lt;/h2&gt;
&lt;p&gt;What happens if we add a scalar to an array? This won’t be surprising for R users:&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;a = np.array([1,2,3])
b = 1
a + b&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([2, 3, 4])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Technically, this is already broadcasting in action; &lt;code&gt;b&lt;/code&gt; is virtually (not physically!) expanded to shape &lt;code&gt;(3,)&lt;/code&gt; in order to match the shape of &lt;code&gt;a&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;How about two arrays, one of shape &lt;code&gt;(2, 3)&lt;/code&gt; – two rows, three columns –, the other one-dimensional, of shape &lt;code&gt;(3,)&lt;/code&gt;?&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;a = np.array([1,2,3])
b = np.array([[1,2,3], [4,5,6]])
a + b&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[2, 4, 6],
       [5, 7, 9]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The one-dimensional array gets added to both rows. If &lt;code&gt;a&lt;/code&gt; were length-two instead, would it get added to every column?&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;a = np.array([1,2,3])
b = np.array([[1,2,3], [4,5,6]])
a + b&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;ValueError: operands could not be broadcast together with shapes (2,) (2,3) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So now it is time for the broadcasting rule. For broadcasting (virtual expansion) to happen, the following is required.&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;We align array shapes, starting from the right.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;   # array 1, shape:     8  1  6  1
   # array 2, shape:        7  1  5&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="2" style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;Starting to look from the right, the sizes along aligned axes either have to &lt;em&gt;match exactly&lt;/em&gt;, or one of them has to be &lt;code&gt;1&lt;/code&gt;: In which case the latter is broadcast to the one not equal to &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If on the left, one of the arrays has an additional axis (or more than one), the other is virtually expanded to have a &lt;code&gt;1&lt;/code&gt; in that place, in which case broadcasting will happen as stated in (2).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Stated like this, it probably sounds incredibly simple. Maybe it is, and it only seems complicated because it presupposes correct parsing of array shapes (which as shown above, can be confusing)?&lt;/p&gt;
&lt;p&gt;Here again is a quick example to test our understanding:&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;a = np.zeros([2, 3]) # shape (2, 3)
b = np.zeros([2])    # shape (2,)
c = np.zeros([3])    # shape (3,)

a + b # error

a + c
# array([[0., 0., 0.],
#        [0., 0., 0.]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All in accord with the rules. Maybe there’s something else that makes it confusing? From linear algebra, we are used to thinking in terms of column vectors (often seen as the default) and row vectors (accordingly, seen as their transposes). What now is&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;np.array([0, 0])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;, of shape – as we’ve seen a few times by now – &lt;code&gt;(2,)&lt;/code&gt;? Really it’s neither, it’s just some one-dimensional array structure. We can create row vectors and column vectors though, in the sense of 1 x n and n x 1 matrices, by explicitly adding a second axis. Any of these would create a column vector:&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;# start with the above &amp;quot;non-vector&amp;quot;
c = np.array([0, 0])
c.shape
# (2,)

# way 1: reshape
c.reshape(2, 1).shape
# (2, 1)

# np.newaxis inserts new axis
c[ :, np.newaxis].shape
# (2, 1)

# None does the same
c[ :, None].shape
# (2, 1)

# or construct directly as (2, 1), paying attention to the parentheses...
c = np.array([[0], [0]])
c.shape
# (2, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And analogously for row vectors. Now these “more explicit”, to a human reader, shapes should make it easier to assess where broadcasting will work, and where it won’t.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;c = np.array([[0], [0]])
c.shape
# (2, 1)

a = np.zeros([2, 3])
a.shape
# (2, 3)
a + c
# array([[0., 0., 0.],
#       [0., 0., 0.]])

a = np.zeros([3, 2])
a.shape
# (3, 2)
a + c
# ValueError: operands could not be broadcast together with shapes (3,2) (2,1) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we jump to TensorFlow, let’s see a simple practical application: computing an outer product.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;a = np.array([0.0, 10.0, 20.0, 30.0])
a.shape
# (4,)

b = np.array([1.0, 2.0, 3.0])
b.shape
# (3,)

a[:, np.newaxis] * b
# array([[ 0.,  0.,  0.],
#        [10., 20., 30.],
#        [20., 40., 60.],
#        [30., 60., 90.]])&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="tensorflow"&gt;TensorFlow&lt;/h2&gt;
&lt;p&gt;If by now, you’re feeling less than enthusiastic about hearing a detailed exposition of how TensorFlow broadcasting differs from NumPy’s, there is good news: Basically, the rules are the same. However, when matrix operations work on batches – as in the case of &lt;code&gt;matmul&lt;/code&gt; and friends – , things may still get complicated; the best advice here probably is to carefully read the documentation (and as always, try things out).&lt;/p&gt;
&lt;p&gt;Before revisiting our introductory &lt;code&gt;matmul&lt;/code&gt; example, we quickly check that really, things work just like in NumPy. Thanks to the &lt;code&gt;tensorflow&lt;/code&gt; R package, there is no reason to do this in Python; so at this point, we switch to R – attention, it’s 1-based indexing from here.&lt;/p&gt;
&lt;p&gt;First check – &lt;code&gt;(4, 1)&lt;/code&gt; added to &lt;code&gt;(4,)&lt;/code&gt; should yield &lt;code&gt;(4, 4)&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;a &amp;lt;- tf$ones(shape = c(4L, 1L))
a
# tf.Tensor(
# [[1.]
#  [1.]
#  [1.]
#  [1.]], shape=(4, 1), dtype=float32)

b &amp;lt;- tf$constant(c(1, 2, 3, 4))
b
# tf.Tensor([1. 2. 3. 4.], shape=(4,), dtype=float32)

a + b
# tf.Tensor(
# [[2. 3. 4. 5.]
# [2. 3. 4. 5.]
# [2. 3. 4. 5.]
# [2. 3. 4. 5.]], shape=(4, 4), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And second, when we add tensors with shapes &lt;code&gt;(3, 3)&lt;/code&gt; and &lt;code&gt;(3,)&lt;/code&gt;, the 1-d tensor should get added to every row (not every column):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;a &amp;lt;- tf$constant(matrix(1:9, ncol = 3, byrow = TRUE), dtype = tf$float32)
a
# tf.Tensor(
# [[1. 2. 3.]
#  [4. 5. 6.]
#  [7. 8. 9.]], shape=(3, 3), dtype=float32)

b &amp;lt;- tf$constant(c(100, 200, 300))
b
# tf.Tensor([100. 200. 300.], shape=(3,), dtype=float32)

a + b
# tf.Tensor(
# [[101. 202. 303.]
#  [104. 205. 306.]
#  [107. 208. 309.]], shape=(3, 3), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now back to the initial &lt;code&gt;matmul&lt;/code&gt; example.&lt;/p&gt;
&lt;h2 id="back-to-the-puzzles"&gt;Back to the puzzles&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://www.tensorflow.org/api_docs/python/tf/linalg/matmul"&gt;documentation for matmul says&lt;/a&gt;,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The inputs must, following any transpositions, be tensors of rank &amp;gt;= 2 where the inner 2 dimensions specify valid matrix multiplication dimensions, and any further outer dimensions specify matching batch size.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So here (see code just below), the inner two dimensions look good – &lt;code&gt;(2, 3)&lt;/code&gt; and &lt;code&gt;(3, 2)&lt;/code&gt; – while the one (one and only, in this case) batch dimension shows mismatching values &lt;code&gt;2&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;, respectively. A case for broadcasting thus: Both “batches” of &lt;code&gt;a&lt;/code&gt; get matrix-multiplied with &lt;code&gt;b&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;a &amp;lt;- tf$constant(keras::array_reshape(1:12, dim = c(2, 2, 3)))
a 
# tf.Tensor(
# [[[ 1.  2.  3.]
#   [ 4.  5.  6.]]
# 
#  [[ 7.  8.  9.]
#   [10. 11. 12.]]], shape=(2, 2, 3), dtype=float64)

b &amp;lt;- tf$constant(keras::array_reshape(101:106, dim = c(1, 3, 2)))
b  
# tf.Tensor(
# [[[101. 102.]
#   [103. 104.]
#   [105. 106.]]], shape=(1, 3, 2), dtype=float64)

c &amp;lt;- tf$matmul(a, b)
c
# tf.Tensor(
# [[[ 622.  628.]
#   [1549. 1564.]]
# 
#  [[2476. 2500.]
#   [3403. 3436.]]], shape=(2, 2, 2), dtype=float64) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s quickly check this really is what happens, by multiplying both batches separately:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;tf$matmul(a[1, , ], b)
# tf.Tensor(
# [[[ 622.  628.]
#   [1549. 1564.]]], shape=(1, 2, 2), dtype=float64)

tf$matmul(a[2, , ], b)
# tf.Tensor(
# [[[2476. 2500.]
#   [3403. 3436.]]], shape=(1, 2, 2), dtype=float64)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Is it too weird to be wondering if broadcasting would also happen for matrix dimensions? E.g., could we try &lt;code&gt;matmul&lt;/code&gt;ing tensors of shapes &lt;code&gt;(2, 4, 1)&lt;/code&gt; and &lt;code&gt;(2, 3, 1)&lt;/code&gt;, where the &lt;code&gt;4 x 1&lt;/code&gt; matrix would be broadcast to &lt;code&gt;4 x 3&lt;/code&gt;? – A quick test shows that no.&lt;/p&gt;
&lt;p&gt;To see how really, when dealing with TensorFlow operations, it pays off overcoming one’s initial reluctance and actually consult the documentation, let’s try another one.&lt;/p&gt;
&lt;p&gt;In the documentation for &lt;a href="https://www.tensorflow.org/api_docs/python/tf/linalg/matvec"&gt;matvec&lt;/a&gt;, we are told:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Multiplies matrix a by vector b, producing a * b. The matrix a must, following any transpositions, be a tensor of rank &amp;gt;= 2, with shape(a)[-1] == shape(b)[-1], and shape(a)[:-2] able to broadcast with shape(b)[:-1].&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In our understanding, given input tensors of shapes &lt;code&gt;(2, 2, 3)&lt;/code&gt; and &lt;code&gt;(2, 3)&lt;/code&gt;, &lt;code&gt;matvec&lt;/code&gt; should perform two matrix-vector multiplications: once for each batch, as indexed by each input’s leftmost dimension. Let’s check this – so far, there is no broadcasting involved:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# two matrices
a &amp;lt;- tf$constant(keras::array_reshape(1:12, dim = c(2, 2, 3)))
a
# tf.Tensor(
# [[[ 1.  2.  3.]
#   [ 4.  5.  6.]]
# 
#  [[ 7.  8.  9.]
#   [10. 11. 12.]]], shape=(2, 2, 3), dtype=float64)

b = tf$constant(keras::array_reshape(101:106, dim = c(2, 3)))
b
# tf.Tensor(
# [[101. 102. 103.]
#  [104. 105. 106.]], shape=(2, 3), dtype=float64)

c &amp;lt;- tf$linalg$matvec(a, b)
c
# tf.Tensor(
# [[ 614. 1532.]
#  [2522. 3467.]], shape=(2, 2), dtype=float64)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Doublechecking, we manually multiply the corresponding matrices and vectors, and get:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;tf$linalg$matvec(a[1,  , ], b[1, ])
# tf.Tensor([ 614. 1532.], shape=(2,), dtype=float64)

tf$linalg$matvec(a[2,  , ], b[2, ])
# tf.Tensor([2522. 3467.], shape=(2,), dtype=float64)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The same. Now, will we see broadcasting if &lt;code&gt;b&lt;/code&gt; has just a single batch?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;b = tf$constant(keras::array_reshape(101:103, dim = c(1, 3)))
b
# tf.Tensor([[101. 102. 103.]], shape=(1, 3), dtype=float64)

c &amp;lt;- tf$linalg$matvec(a, b)
c
# tf.Tensor(
# [[ 614. 1532.]
#  [2450. 3368.]], shape=(2, 2), dtype=float64)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Multiplying every batch of &lt;code&gt;a&lt;/code&gt; with &lt;code&gt;b&lt;/code&gt;, for comparison:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;tf$linalg$matvec(a[1,  , ], b)
# tf.Tensor([ 614. 1532.], shape=(2,), dtype=float64)

tf$linalg$matvec(a[2,  , ], b)
# tf.Tensor([[2450. 3368.]], shape=(1, 2), dtype=float64)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It worked!&lt;/p&gt;
&lt;p&gt;Now, on to the other motivating example, using &lt;em&gt;tfprobability&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id="broadcasting-everywhere"&gt;Broadcasting everywhere&lt;/h3&gt;
&lt;p&gt;Here again is the setup:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tfprobability)
d &amp;lt;- tfd_normal(loc = c(0, 1), scale = matrix(1.5:4.5, ncol = 2, byrow = TRUE))
d
# tfp.distributions.Normal(&amp;quot;Normal&amp;quot;, batch_shape=[2, 2], event_shape=[], dtype=float64)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is going on? Let’s inspect &lt;em&gt;location&lt;/em&gt; and &lt;em&gt;scale&lt;/em&gt; separately:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;d$loc
# tf.Tensor([0. 1.], shape=(2,), dtype=float64)

d$scale
# tf.Tensor(
# [[1.5 2.5]
#  [3.5 4.5]], shape=(2, 2), dtype=float64)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just focusing on these tensors and their shapes, and having been told that there’s broadcasting going on, we can reason like this: Aligning both shapes on the right and extending &lt;code&gt;loc&lt;/code&gt;’s shape by &lt;code&gt;1&lt;/code&gt; (on the left), we have &lt;code&gt;(1, 2)&lt;/code&gt; which may be broadcast with &lt;code&gt;(2,2)&lt;/code&gt; - in matrix-speak, &lt;code&gt;loc&lt;/code&gt; is treated as a row and duplicated.&lt;/p&gt;
&lt;p&gt;Meaning: We have two distributions with mean &lt;span class="math inline"&gt;\(0\)&lt;/span&gt; (one of scale &lt;span class="math inline"&gt;\(1.5\)&lt;/span&gt;, the other of scale &lt;span class="math inline"&gt;\(3.5\)&lt;/span&gt;), and also two with mean &lt;span class="math inline"&gt;\(1\)&lt;/span&gt; (corresponding scales being &lt;span class="math inline"&gt;\(2.5\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(4.5\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Here’s a more direct way to see this:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;d$mean()
# tf.Tensor(
# [[0. 1.]
#  [0. 1.]], shape=(2, 2), dtype=float64)

d$stddev()
# tf.Tensor(
# [[1.5 2.5]
#  [3.5 4.5]], shape=(2, 2), dtype=float64)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Puzzle solved!&lt;/p&gt;
&lt;p&gt;Summing up, broadcasting is simple “in theory” (its rules are), but may need some practicing to get it right. Especially in conjunction with the fact that functions / operators do have their own views on which parts of its inputs should broadcast, and which shouldn’t. Really, there is no way around looking up the actual behaviors in the documentation.&lt;/p&gt;
&lt;p&gt;Hopefully though, you’ve found this post to be a good start into the topic. Maybe, like the author, you feel like you might see broadcasting going on anywhere in the world now. Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;or &lt;code&gt;start:stop:step&lt;/code&gt;, if applicable&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">ceab17939e2812028b40d58ace1295a6</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Concepts</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-01-24-numpy-broadcasting</guid>
      <pubDate>Fri, 24 Jan 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-01-24-numpy-broadcasting/images/thumb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>First experiments with TensorFlow mixed-precision training</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-01-13-mixed-precision-training</link>
      <description>


&lt;p&gt;Starting from its - very - recent 2.1 release, TensorFlow supports what is called &lt;em&gt;mixed-precision training&lt;/em&gt; (in the following: MPT) for Keras. In this post, we experiment with MPT and provide some background. Stated upfront: On a Tesla V100 GPU, our CNN-based experiment did not reveal substantial reductions in execution time. In a case like this, it is hard to decide whether to actually write a post or not. You could argue that just like in science, &lt;em&gt;null&lt;/em&gt; results are results. Or, more practically: They open up a discussion that may lead to bug discovery, clarification of usage instructions, and further experimentation, among others.&lt;/p&gt;
&lt;p&gt;In addition, the topic itself is interesting enough to deserve some background explanations – even if the results are not quite there &lt;em&gt;yet&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;So to start, let’s hear some context on MPT.&lt;/p&gt;
&lt;h2 id="this-is-not-just-about-saving-memory"&gt;This is not just about saving memory&lt;/h2&gt;
&lt;p&gt;One way to describe MPT in TensorFlow could go like this: MPT lets you train models where the &lt;em&gt;weights&lt;/em&gt; are of type &lt;code&gt;float32&lt;/code&gt; or &lt;code&gt;float64&lt;/code&gt;, as usual (for reasons of numeric stability), but the &lt;em&gt;data&lt;/em&gt; – the tensors pushed between operations – have lower precision, namely, 16bit (&lt;code&gt;float16&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;This sentence would probably do fine as a &lt;em&gt;TLDR;&lt;/em&gt; &lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; for the new-ish &lt;a href="https://www.tensorflow.org/guide/keras/mixed_precision"&gt;MPT documentation page&lt;/a&gt;, also available for R on the &lt;a href="https://tensorflow.rstudio.com/guide/keras/mixed_precision/"&gt;TensorFlow for R website&lt;/a&gt;. And based on this sentence, you might be lead to think &lt;em&gt;“oh sure, so this is about saving memory”&lt;/em&gt;. Less memory usage would then imply you could run larger batch sizes without getting out-of-memory errors.&lt;/p&gt;
&lt;p&gt;This is of course correct, and you’ll see it happening in the experimentation results. But it’s only part of the story. The other part is related to GPU architecture and parallel (not just parallel on-GPU, as we’ll see) computing.&lt;/p&gt;
&lt;h3 id="avx-co."&gt;AVX &amp;amp; co.&lt;/h3&gt;
&lt;p&gt;GPUs are all about parallelization. But for CPUs as well, the last ten years have seen important developments in architecture and instruction sets. &lt;a href="https://en.wikipedia.org/wiki/SIMD"&gt;SIMD (Single Instruction Multiple Data)&lt;/a&gt; operations perform one instruction over a bunch of data at once. For example, two 128-bit operands could hold two 64-bit integers each, and these could be added pairwise. Conceptually, this reminds of vector addition in R (it’s just an analogue though!):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# picture these as 64-bit integers
c(1, 2) + c(3, 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or, those operands could contain four 32-bit integers each, in which case we could symbolically write&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# picture these as 32-bit integers
c(1, 2, 3, 4) + c(5, 6, 7, 8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With 16-bit integers, we could again double the number of elements operated upon:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# picture these as 16-bit integers
c(1, 2, 3, 4, 5, 6, 7, 8) + c(9, 10, 11, 12, 13, 14, 15, 16)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Over the last decade, the major SIMD-related X-86 assembly language extensions have been AVX (&lt;em&gt;Advanced Vector Extensions&lt;/em&gt;), AVX2, AVX-512, and FMA (more on FMA soon). Do any of these ring a bell?&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Your CPU supports instructions that this TensorFlow binary was not compiled to use:
AVX2 FMA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a line you are likely to see if you are using a pre-built TensorFlow binary, as opposed to compiling from source. (Later, when reporting experimentation results, we will also indicate on-CPU execution times, to provide some context for the GPU execution times we’re interested in – and just for fun, we’ll also do a – &lt;em&gt;very&lt;/em&gt; superficial – comparison between a TensorFlow binary installed from PyPi and one that was compiled manually.)&lt;/p&gt;
&lt;p&gt;While all those AVXes are (basically) about an extension of vector processing to larger and larger data types, FMA is different, and it’s an interesting thing to know about in itself – for anyone doing signal processing or using neural networks.&lt;/p&gt;
&lt;h3 id="fused-multiply-add-fma"&gt;Fused Multiply-Add (FMA)&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation#Fused_multiply%E2%80%93add"&gt;Fused Multiply-Add&lt;/a&gt; is a type of &lt;em&gt;multiply-accumulate&lt;/em&gt; operation. In &lt;em&gt;multiply-accumulate&lt;/em&gt;, operands are multiplied and then added to accumulator keeping track of the running sum. If “fused”, the whole multiply-then-add operation is performed with a single rounding at the end (as opposed to rounding once after the multiplication, and then again after the addition). Usually, this results in higher accuracy.&lt;/p&gt;
&lt;p&gt;For CPUs, FMA was introduced concurrently with AVX2. FMA can be performed on scalars or on vectors, “packed” in the way described in the previous paragraph.&lt;/p&gt;
&lt;p&gt;Why did we say this was so interesting to data scientists? Well, a lot of operations – dot products, matrix multiplications, convolutions – involve multiplications followed by additions. “Matrix multiplication” here actually has us leave the realm of CPUs and jump to GPUs instead, because what MPT does is make use of the new-ish NVidia &lt;em&gt;Tensor Cores&lt;/em&gt; that extend FMA from scalars/vectors to matrices.&lt;/p&gt;
&lt;h3 id="tensor-cores"&gt;Tensor Cores&lt;/h3&gt;
&lt;p&gt;As &lt;a href="https://tensorflow.rstudio.com/guide/keras/mixed_precision/"&gt;documented&lt;/a&gt;, MPT requires GPUs with &lt;a href="https://en.wikipedia.org/wiki/CUDA"&gt;compute capability&lt;/a&gt; &amp;gt;= 7.0. The respective GPUs, in addition to the usual &lt;em&gt;Cuda Cores&lt;/em&gt;, have so called “Tensor Cores” that perform FMA on matrices:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-01-13-mixed-precision-training/images/fma.png" alt="Source: https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf." width="354" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-4)Source: &lt;a href="https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf" class="uri"&gt;https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The operation takes place on 4x4 matrices; multiplications happen on 16-bit operands while the final result could be 16-bit or 32-bit.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-01-13-mixed-precision-training/images/fma2.png" alt="Source: https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf" width="324" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-5)Source: &lt;a href="https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf" class="uri"&gt;https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We can see how this is immediately relevant to the operations involved in deep learning; the details, however, are &lt;a href="https://www.anandtech.com/show/12673/titan-v-deep-learning-deep-dive/3"&gt;not necessarily clear&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Leaving those internals to the experts, we now proceed to the actual experiment.&lt;/p&gt;
&lt;h2 id="experiments"&gt;Experiments&lt;/h2&gt;
&lt;h3 id="dataset"&gt;Dataset&lt;/h3&gt;
&lt;p&gt;With their 28x28px / 32x32px sized images, neither MNIST nor CIFAR seemed particularly suited to challenge the GPU. Instead, we chose &lt;a href="https://github.com/fastai/imagenette"&gt;&lt;em&gt;Imagenette&lt;/em&gt;&lt;/a&gt;, the “little ImageNet” created by the &lt;em&gt;fast.ai&lt;/em&gt; folks, consisting of 10 classes: &lt;em&gt;tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball,&lt;/em&gt; and &lt;em&gt;parachute&lt;/em&gt;. &lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; Here are a few examples, taken from the 320px version:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-01-13-mixed-precision-training/images/results.jpg" alt="Examples of the 10 classes of Imagenette."  /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-6)Examples of the 10 classes of Imagenette.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;These images have been resized - keeping the aspect ratio - such that the larger dimension has length 320px. As part of preprocessing, we’ll further resize to 256x256px, to work with a nice power of 2. &lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The dataset may conveniently be obtained via using &lt;a href=""&gt;tfds&lt;/a&gt;, the R interface to TensorFlow Datasets.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
# needs version 2.1
library(tensorflow)
library(tfdatasets)
# available from github: devtools::install_github(&amp;quot;rstudio/tfds&amp;quot;)
library(tfds)

# to use TensorFlow Datasets, we need the Python backend
# normally, just use tfds::install_tfds for this
# as of this writing though, we need a nightly build of TensorFlow Datasets
# envname should refer to whatever environment you run TensorFlow in
reticulate::py_install(&amp;quot;tfds-nightly&amp;quot;, envname = &amp;quot;r-reticulate&amp;quot;) 

# on first execution, this downloads the dataset
imagenette &amp;lt;- tfds_load(&amp;quot;imagenette/320px&amp;quot;)

# extract train and test parts
train &amp;lt;- imagenette$train
test &amp;lt;- imagenette$validation

# batch size for the initial run
batch_size &amp;lt;- 32
# 12895 is the number of items in the training set
buffer_size &amp;lt;- 12895/batch_size

# training dataset is resized, scaled to between 0 and 1,
# cached, shuffled, and divided into batches
train_dataset &amp;lt;- train %&amp;gt;%
  dataset_map(function(record) {
    record$image &amp;lt;- record$image %&amp;gt;%
      tf$image$resize(size = c(256L, 256L)) %&amp;gt;%
      tf$truediv(255)
    record
  }) %&amp;gt;%
  dataset_cache() %&amp;gt;%
  dataset_shuffle(buffer_size) %&amp;gt;%
  dataset_batch(batch_size) %&amp;gt;%
  dataset_map(unname)

# test dataset is resized, scaled to between 0 and 1, and divided into batches
test_dataset &amp;lt;- test %&amp;gt;% 
  dataset_map(function(record) {
    record$image &amp;lt;- record$image %&amp;gt;% 
      tf$image$resize(size = c(256L, 256L)) %&amp;gt;%
      tf$truediv(255)
    record}) %&amp;gt;%
  dataset_batch(batch_size) %&amp;gt;% 
  dataset_map(unname)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above code, we cache the dataset after the resize and scale operations, as we want to minimize preprocessing time spent on the CPU.&lt;/p&gt;
&lt;h3 id="configuring-mpt"&gt;Configuring MPT&lt;/h3&gt;
&lt;p&gt;Our experiment uses Keras &lt;code&gt;fit&lt;/code&gt; – as opposed to a custom training loop –, and given these preconditions, running MPT is mostly a matter of adding three lines of code. (There is a small change to the model, as we’ll see in a moment.)&lt;a href="#fn4" class="footnote-ref" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We tell Keras to use the &lt;em&gt;mixed_float16&lt;/em&gt; &lt;code&gt;Policy&lt;/code&gt;, and verify that the tensors have type &lt;code&gt;float16&lt;/code&gt; while the &lt;code&gt;Variables&lt;/code&gt; (weights) still are of type &lt;code&gt;float32&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# if you read this at a later time and get an error here,
# check out whether the location in the codebase has changed
mixed_precision &amp;lt;- tf$keras$mixed_precision$experimental

policy &amp;lt;- mixed_precision$Policy(&amp;#39;mixed_float16&amp;#39;)
mixed_precision$set_policy(policy)

# float16
policy$compute_dtype
# float32
policy$variable_dtype&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model is a straightforward convnet, with numbers of filters being multiples of 8, as specified in the &lt;a href=""&gt;documentation&lt;/a&gt;. There is one thing to note though: For reasons of numerical stability, the actual output tensor of the model should be of type &lt;code&gt;float32&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;% 
  layer_conv_2d(filters = 32, kernel_size = 5, strides = 2, padding = &amp;quot;same&amp;quot;, input_shape = c(256, 256, 3), activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_conv_2d(filters = 64, kernel_size = 7, strides = 2, padding = &amp;quot;same&amp;quot;, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_conv_2d(filters = 128, kernel_size = 11, strides = 2, padding = &amp;quot;same&amp;quot;, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_global_average_pooling_2d() %&amp;gt;%
  # separate logits from activations so actual outputs can be float32
  layer_dense(units = 10) %&amp;gt;%
  layer_activation(&amp;quot;softmax&amp;quot;, dtype = &amp;quot;float32&amp;quot;)

model %&amp;gt;% compile(
  loss = &amp;quot;sparse_categorical_crossentropy&amp;quot;,
  optimizer = &amp;quot;adam&amp;quot;,
  metrics = &amp;quot;accuracy&amp;quot;)

model %&amp;gt;% 
  fit(train_dataset, validation_data = test_dataset, epochs = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="results"&gt;Results&lt;/h2&gt;
&lt;p&gt;The main experiment was done on a Tesla V100 with 16G of memory. Just for curiosity, we ran that same model under four other conditions, none of which fulfill the prerequisite of having a &lt;em&gt;compute capability&lt;/em&gt; equal to at least 7.0. We’ll quickly mention those after the main results.&lt;/p&gt;
&lt;p&gt;With the above model, final accuracy (final as in: after 20 epochs) fluctuated about 0.78: &lt;a href="#fn5" class="footnote-ref" id="fnref5"&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Epoch 16/20
403/403 [==============================] - 12s 29ms/step - loss: 0.3365 -
accuracy: 0.8982 - val_loss: 0.7325 - val_accuracy: 0.8060
Epoch 17/20
403/403 [==============================] - 12s 29ms/step - loss: 0.3051 -
accuracy: 0.9084 - val_loss: 0.6683 - val_accuracy: 0.7820
Epoch 18/20
403/403 [==============================] - 11s 28ms/step - loss: 0.2693 -
accuracy: 0.9208 - val_loss: 0.8588 - val_accuracy: 0.7840
Epoch 19/20
403/403 [==============================] - 11s 28ms/step - loss: 0.2274 -
accuracy: 0.9358 - val_loss: 0.8692 - val_accuracy: 0.7700
Epoch 20/20
403/403 [==============================] - 11s 28ms/step - loss: 0.2082 -
accuracy: 0.9410 - val_loss: 0.8473 - val_accuracy: 0.7460&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The numbers reported below are milliseconds per step, &lt;em&gt;step&lt;/em&gt; being a pass over a single batch. Thus in general, doubling the batch size we would expect execution time to double as well.&lt;/p&gt;
&lt;p&gt;Here are execution times, taken from epoch 20, for five different batch sizes, comparing MPT with a default &lt;code&gt;Policy&lt;/code&gt; that uses &lt;code&gt;float32&lt;/code&gt; throughout. (We should add that apart from the very first epoch, execution times per step fluctuated by at most one millisecond in every condition.)&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th&gt;Batch size&lt;/th&gt;
&lt;th&gt;ms/step, MPT&lt;/th&gt;
&lt;th&gt;ms/step, f32&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;td&gt;28&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;52&lt;/td&gt;
&lt;td&gt;56&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;td&gt;97&lt;/td&gt;
&lt;td&gt;106&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;188&lt;/td&gt;
&lt;td&gt;206&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;377&lt;/td&gt;
&lt;td&gt;415&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Consistently, MPT was faster, indicating that the intended code path was used. But the speedup is not that big.&lt;/p&gt;
&lt;p&gt;We also watched GPU utilization during the runs. These ranged from around 72% for &lt;code&gt;batch_size&lt;/code&gt; 32 over ~ 78% for &lt;code&gt;batch_size&lt;/code&gt; 128 to hightly fluctuating values, repeatedly reaching 100%, for &lt;code&gt;batch_size&lt;/code&gt; 512.&lt;/p&gt;
&lt;p&gt;As alluded to above, just to anchor these values we ran the same model in four other conditions, where no speedup was to be expected. Even though these execution times are not strictly part of the experiments, we report them, in case the reader is as curious about some context as we were.&lt;/p&gt;
&lt;p&gt;Firstly, here is the equivalent table for a Titan XP with 12G of memory and &lt;em&gt;compute capability&lt;/em&gt; 6.1.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th&gt;Batch size&lt;/th&gt;
&lt;th&gt;ms/step, MPT&lt;/th&gt;
&lt;th&gt;ms/step, f32&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;td&gt;44&lt;/td&gt;
&lt;td&gt;38&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;70&lt;/td&gt;
&lt;td&gt;70&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;td&gt;142&lt;/td&gt;
&lt;td&gt;136&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;270&lt;/td&gt;
&lt;td&gt;270&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;518&lt;/td&gt;
&lt;td&gt;539&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As expected, there is no consistent superiority of MPT; as an aside, looking at the values overall (especially as compared to CPU execution times to come!) you might conclude that luckily, one doesn’t always need the latest and greatest GPU to train neural networks!&lt;/p&gt;
&lt;p&gt;Next, we take one further step down the hardware ladder. Here are execution times from a Quadro M2200 (4G, &lt;em&gt;compute capability&lt;/em&gt; 5.2). (The three runs that don’t have a number crashed with &lt;em&gt;out of memory&lt;/em&gt;.)&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th&gt;Batch size&lt;/th&gt;
&lt;th&gt;ms/step, MPT&lt;/th&gt;
&lt;th&gt;ms/step, f32&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;td&gt;186&lt;/td&gt;
&lt;td&gt;197&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;352&lt;/td&gt;
&lt;td&gt;375&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;td&gt;687&lt;/td&gt;
&lt;td&gt;746&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;1000&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This time, we actually see how the pure memory-usage aspect plays a role: With MPT, we can run batches of size 256; without, we get an out-of-memory error.&lt;/p&gt;
&lt;p&gt;Now, we also compared with runtime on CPU (Intel Core I7, clock speed 2.9Ghz). To be honest, we stopped after a single epoch though. With a &lt;code&gt;batch_size&lt;/code&gt; of 32 and running a standard pre-built installation of TensorFlow, a single step now took 321 - not milliseconds, but seconds. Just for fun, we compared to a manually built TensorFlow that can make use of &lt;em&gt;AVX2&lt;/em&gt; and &lt;em&gt;FMA&lt;/em&gt; instructions (this topic might in fact deserve a dedicated experiment): Execution time per step was reduced to 304 seconds/step.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Summing up, our experiment did not show important reductions in execution times – for reasons as yet unclear. We’d be happy to encourage a discussion in the comments!&lt;/p&gt;
&lt;p&gt;Experimental results notwithstanding, we hope you’ve enjoyed getting some background information on a not-too-frequently discussed topic. Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Evidently, &lt;em&gt;TLDR;&lt;/em&gt; has been inserted &lt;em&gt;inside&lt;/em&gt; some chunk of text here in order to confuse any future GPT-2s.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;We do hope usage is allowed even in case we can’t produce the required “corny inauthentic French accent”.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;As per the &lt;a href="https://tensorflow.rstudio.com/guide/keras/mixed_precision/"&gt;documentation&lt;/a&gt;, the number of filters in a convolutional layer should be a multiple of 8; however, taking this additional measure couldn’t possibly hurt.&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;With custom training loops, losses should be scaled (multiplied by a large number) before being passed into the gradient calculation, to avoid numerical underflow/overflow. For detailed instructions, see the &lt;a href="https://tensorflow.rstudio.com/guide/keras/mixed_precision/"&gt;documentation&lt;/a&gt;.&lt;a href="#fnref4" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn5"&gt;&lt;p&gt;Example output from run with &lt;code&gt;batch_size&lt;/code&gt; 32 and MPT.&lt;a href="#fnref5" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">b5bd0cbbe49ea6c289c8d7bb9e765aa3</distill:md5>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-01-13-mixed-precision-training</guid>
      <pubDate>Mon, 13 Jan 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-01-13-mixed-precision-training/images/tc.png" medium="image" type="image/png" width="589" height="399"/>
    </item>
    <item>
      <title>Differential Privacy with TensorFlow</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-12-20-differential-privacy</link>
      <description>


&lt;p&gt;What could be treacherous about summary statistics?&lt;/p&gt;
&lt;p&gt;The famous &lt;em&gt;cat overweight&lt;/em&gt; study (X. et al., 2019) showed that as of May 1st, 2019, 32 of 101 domestic cats held in Y., a cozy Bavarian village, were overweight. Even though I’d be curious to know if my aunt G.’s cat (a happy resident of that village) has been fed too many treats and has accumulated some excess pounds, the study results don’t tell.&lt;/p&gt;
&lt;p&gt;Then, six months later, out comes a new study, ambitious to earn scientific fame. The authors report that of 100 cats living in Y., 50 are striped, 31 are black, and the rest are white; the 31 black ones are all overweight. Now, I happen to know that, with one exception, no new cats joined the community, and no cats left. &lt;em&gt;But&lt;/em&gt;, my aunt moved away to a retirement home, chosen of course for the possibility to bring one’s cat.&lt;/p&gt;
&lt;p&gt;What have I just learned? My aunt’s cat is overweight. (Or was, at least, before they moved to the retirement home.)&lt;/p&gt;
&lt;p&gt;Even though none of the studies reported anything but summary statistics, I was able to infer individual-level facts by connecting both studies and adding in another piece of information I had access to.&lt;/p&gt;
&lt;p&gt;In reality, mechanisms like the above – technically called &lt;em&gt;linkage&lt;/em&gt; – have been shown to lead to privacy breaches many times, thus defeating the purpose of &lt;em&gt;database anonymization&lt;/em&gt; seen as a panacea in many organizations. A more promising alternative is offered by the concept of &lt;em&gt;differential privacy&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id="differential-privacy"&gt;Differential Privacy&lt;/h2&gt;
&lt;p&gt;In differential privacy (DP)&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;span class="citation"&gt;(Dwork et al. 2006)&lt;/span&gt;, privacy is not a property of what’s in the database; it’s a property of how query results are delivered.&lt;/p&gt;
&lt;p&gt;Intuitively paraphrasing results from a domain where results are communicated as theorems and proofs &lt;span class="citation"&gt;(Dwork 2006)&lt;/span&gt;&lt;span class="citation"&gt;(Dwork and Roth 2014)&lt;/span&gt;, the only achievable (in a lossy but quantifiable way) objective is that from queries to a database, nothing more should be learned about an individual in that database than if they hadn’t been in there at all.&lt;span class="citation"&gt;(Wood et al. 2018)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What this statement does is caution against overly high expectations: Even if query results are reported in a DP way (we’ll see how that goes in a second), they enable some probabilistic inferences about individuals in the respective population. (Otherwise, why conduct studies at all.)&lt;/p&gt;
&lt;p&gt;So how is DP being achieved? The main ingredient is &lt;em&gt;noise&lt;/em&gt; added to the results of a query. In the above cat example, instead of exact numbers we’d report approximate ones: “Of ~ 100 cats living in Y, about 30 are overweight…”. If this is done for both of the above studies, no inference will be possible about aunt G.’s cat.&lt;/p&gt;
&lt;p&gt;Even with random noise added to query results though, answers to &lt;em&gt;repeated&lt;/em&gt; queries will leak information. So in reality, there is a &lt;em&gt;privacy budget&lt;/em&gt; that can be tracked, and may be used up in the course of consecutive queries.&lt;/p&gt;
&lt;p&gt;This is reflected in the formal definition of DP. The idea is that queries to two databases differing in at most one element should give basically the same result. Put formally &lt;span class="citation"&gt;(Dwork 2006)&lt;/span&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A randomized function &lt;span class="math inline"&gt;\(\mathcal{K}\)&lt;/span&gt; gives &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt; -differential privacy if for all data sets D1 and D2 differing on at most one element, and all &lt;span class="math inline"&gt;\(S \subseteq Range(K)\)&lt;/span&gt;,&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class="math inline"&gt;\(Pr[\mathcal{K}(D1)\in S] \leq exp(\epsilon) × Pr[K(D2) \in S]\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt; -differential privacy is additive: If one query is &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt;-DP at a value of 0.01, and another one at 0.03, together they will be 0.04 &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt;-differentially private.&lt;/p&gt;
&lt;p&gt;If &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt;-DP is to be achieved via adding noise, how exactly should this be done? Here, several mechanisms exist; the basic, intuitively plausible principle though is that the amount of noise should be calibrated to the target function’s &lt;em&gt;sensitivity&lt;/em&gt;, defined as the maximum &lt;span class="math inline"&gt;\(\ell 1\)&lt;/span&gt; norm of the difference of function values computed on all pairs of datasets differing in a single example &lt;span class="citation"&gt;(Dwork 2006)&lt;/span&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class="math inline"&gt;\(\Delta f = \max_{D1,D2} {\| f(D1)−f(D2) \|}_1\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So far, we’ve been talking about databases and datasets. How does this apply to machine and/or deep learning?&lt;/p&gt;
&lt;h2 id="tensorflow-privacy"&gt;TensorFlow Privacy&lt;/h2&gt;
&lt;p&gt;Applying DP to deep learning, we want a model’s parameters to wind up “essentially the same” whether trained on a dataset including that cute little kitty or not. TensorFlow (TF) Privacy &lt;span class="citation"&gt;(Abadi et al. 2016)&lt;/span&gt;, a library built on top of TF, makes it easy on users to add privacy guarantees to their models – easy, that is, from a technical point of view. (As with life overall, the hard decisions on &lt;em&gt;how much&lt;/em&gt; of an asset we should be reaching for, and how to trade off one asset (here: privacy) with another (here: model performance), remain to be taken by each of us ourselves.)&lt;/p&gt;
&lt;p&gt;Concretely, about all we have to do is exchange the optimizer we were using against one provided by TF Privacy.&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; TF Privacy optimizers wrap the original TF ones, adding two actions:&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;To honor the principle that each individual training example should have just moderate influence on optimization, gradients are &lt;em&gt;clipped&lt;/em&gt; (to a degree specifiable by the user). In contrast to the familiar gradient clipping sometimes used to prevent exploding gradients, what is clipped here is gradient contribution &lt;em&gt;per user&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Before updating the parameters, noise is added to the gradients, thus implementing the main idea of &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt;-DP algorithms.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In addition to &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt;-DP optimization, TF Privacy provides &lt;em&gt;privacy accounting&lt;/em&gt;. We’ll see all this applied after an introduction to our example dataset.&lt;/p&gt;
&lt;h2 id="dataset"&gt;Dataset&lt;/h2&gt;
&lt;p&gt;The dataset we’ll be working with&lt;span class="citation"&gt;(Reiss et al. 2019)&lt;/span&gt;, downloadable from the &lt;a href="https://archive.ics.uci.edu/ml/datasets/PPG-DaLiA"&gt;UCI Machine Learning Repository&lt;/a&gt;, is dedicated to heart rate estimation via &lt;a href="https://en.wikipedia.org/wiki/Photoplethysmogram"&gt;photoplethysmography&lt;/a&gt;. Photoplethysmography (PPG) is an optical method of measuring blood volume changes in the microvascular bed of tissue, which are indicative of cardiovascular activity. More precisely,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The PPG waveform comprises a pulsatile (‘AC’) physiological waveform attributed to cardiac synchronous changes in the blood volume with each heart beat, and is superimposed on a slowly varying (‘DC’) baseline with various lower frequency components attributed to respiration, sympathetic nervous system activity and thermoregulation. &lt;span class="citation"&gt;(Allen 2007)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this dataset, heart rate determined from EKG provides the ground truth; predictors were obtained from two commercial devices, comprising PPG, electrodermal activity, body temperature as well as accelerometer data. Additionally, a wealth of contextual data is available, ranging from age, height, and weight to fitness level and type of activity performed.&lt;/p&gt;
&lt;p&gt;With this data, it’s easy to imagine a bunch of interesting data-analysis questions; however here our focus is on differential privacy, so we’ll keep the setup simple. We will try to predict heart rate given the physiological measurements from one of the two devices, Empatica E4. Also, we’ll zoom in on a single subject, &lt;em&gt;S1&lt;/em&gt;, who will provide us with 4603 instances of two-second heart rate values.&lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As usual, we start with the required libraries; unusually though, as of this writing we need to disable version 2 behavior in TensorFlow, as TensorFlow Privacy does not yet fully work with TF 2. (Hopefully, for many future readers, this won’t be the case anymore.) Note how TF Privacy – a Python library – is imported via &lt;code&gt;reticulate&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tensorflow)
tf$compat$v1$disable_v2_behavior()

library(keras)
library(tfdatasets)
library(tfautograph)

library(purrr)

library(reticulate)
# if you haven&amp;#39;t yet, install TF Privacy, e.g. using reticulate:
# py_install(&amp;quot;tensorflow_privacy&amp;quot;)
priv &amp;lt;- import(&amp;quot;tensorflow_privacy&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the downloaded archive, we just need &lt;code&gt;S1.pkl&lt;/code&gt;, saved in a &lt;a href="https://docs.python.org/3.8/library/pickle.html"&gt;native Python serialization format&lt;/a&gt;, yet nicely loadable using &lt;code&gt;reticulate&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;s1 &amp;lt;- py_load_object(&amp;quot;PPG_FieldStudy/S1/S1.pkl&amp;quot;, encoding = &amp;quot;latin1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;s1&lt;/code&gt; points to an R list comprising elements of different length – the various physical/physiological signals have been sampled with different frequencies:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;### predictors ###

# accelerometer data - sampling freq. 32 Hz
# also note that these are 3 &amp;quot;columns&amp;quot;, for each of x, y, and z axes
s1$signal$wrist$ACC %&amp;gt;% nrow() # 294784
# PPG data - sampling freq. 64 Hz
s1$signal$wrist$BVP %&amp;gt;% nrow() # 589568
# electrodermal activity data - sampling freq. 4 Hz
s1$signal$wrist$EDA %&amp;gt;% nrow() # 36848
# body temperature data - sampling freq. 4 Hz
s1$signal$wrist$TEMP %&amp;gt;% nrow() # 36848

### target ###

# EKG data - provided in already averaged form, at frequency 0.5 Hz
s1$label %&amp;gt;% nrow() # 4603&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In light of the different sampling frequencies, our &lt;code&gt;tfdatasets&lt;/code&gt; pipeline will have do some moving averaging, paralleling that applied to construct the ground truth data.&lt;/p&gt;
&lt;h2 id="preprocessing-pipeline"&gt;Preprocessing pipeline&lt;/h2&gt;
&lt;p&gt;As every “column”&lt;a href="#fn4" class="footnote-ref" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; is of different length and resolution, we build up the final &lt;em&gt;dataset&lt;/em&gt; piece-by-piece. The following function serves two purposes:&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;compute running averages over differently sized windows, thus downsampling to 0.5Hz for every modality&lt;/li&gt;
&lt;li&gt;transform the data to the &lt;code&gt;(num_timesteps, num_features)&lt;/code&gt; format that will be required by the 1d-convnet we’re going to use soon&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class="r"&gt;&lt;code&gt;average_and_make_sequences &amp;lt;-
  function(data, window_size_avg, num_timesteps) {
    data %&amp;gt;% k_cast(&amp;quot;float32&amp;quot;) %&amp;gt;%
      # create an initial tf.data dataset to work with
      tensor_slices_dataset() %&amp;gt;%
      # use dataset_window to compute the running average of size window_size_avg
      dataset_window(window_size_avg) %&amp;gt;%
      dataset_flat_map(function (x)
        x$batch(as.integer(window_size_avg), drop_remainder = TRUE)) %&amp;gt;%
      dataset_map(function(x)
        tf$reduce_mean(x, axis = 0L)) %&amp;gt;%
      # use dataset_window to create a &amp;quot;timesteps&amp;quot; dimension with length num_timesteps)
      dataset_window(num_timesteps, shift = 1) %&amp;gt;%
      dataset_flat_map(function(x)
        x$batch(as.integer(num_timesteps), drop_remainder = TRUE))
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll call this function for every column separately. Not all columns are exactly the same length (in terms of time), thus it’s safest to cut off individual observations that surpass a common length (dictated by the target variable):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;label &amp;lt;- s1$label %&amp;gt;% matrix() # 4603 observations, each spanning 2 secs
n_total &amp;lt;- 4603 # keep track of this

# keep matching numbers of observations of predictors
acc &amp;lt;- s1$signal$wrist$ACC[1:(n_total * 64), ] # 32 Hz, 3 columns
bvp &amp;lt;- s1$signal$wrist$BVP[1:(n_total * 128)] %&amp;gt;% matrix() # 64 Hz
eda &amp;lt;- s1$signal$wrist$EDA[1:(n_total * 8)] %&amp;gt;% matrix() # 4 Hz
temp &amp;lt;- s1$signal$wrist$TEMP[1:(n_total * 8)] %&amp;gt;% matrix() # 4 Hz&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some more housekeeping. Both training and the test set need to have a &lt;code&gt;timesteps&lt;/code&gt; dimension, as usual with architectures that work on sequential data (1-d convnets and RNNs). To make sure there is no overlap between respective &lt;code&gt;timesteps&lt;/code&gt;, we split the data “up front” and assemble both sets separately. We’ll use the first 4000 observations for training.&lt;/p&gt;
&lt;p&gt;Housekeeping-wise, we also keep track of actual training and test set cardinalities. The target variable will be matched to the last of any twelve timesteps, so we end up throwing away the first eleven ground truth measurements for each of the training and test datasets. (We don’t have complete sequences building up to them.)&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# number of timesteps used in the second dimension
num_timesteps &amp;lt;- 12

# number of observations to be used for the training set
# a round number for easier checking!
train_max &amp;lt;- 4000

# also keep track of actual number of training and test observations
n_train &amp;lt;- train_max - num_timesteps + 1
n_test &amp;lt;- n_total - train_max - num_timesteps + 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, then, are the basic building blocks that will go into the final training and test datasets.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;acc_train &amp;lt;-
  average_and_make_sequences(acc[1:(train_max * 64), ], 64, num_timesteps)
bvp_train &amp;lt;-
  average_and_make_sequences(bvp[1:(train_max * 128), , drop = FALSE], 128, num_timesteps)
eda_train &amp;lt;-
  average_and_make_sequences(eda[1:(train_max * 8), , drop = FALSE], 8, num_timesteps)
temp_train &amp;lt;-
  average_and_make_sequences(temp[1:(train_max * 8), , drop = FALSE], 8, num_timesteps)


acc_test &amp;lt;-
  average_and_make_sequences(acc[(train_max * 64 + 1):nrow(acc), ], 64, num_timesteps)
bvp_test &amp;lt;-
  average_and_make_sequences(bvp[(train_max * 128 + 1):nrow(bvp), , drop = FALSE], 128, num_timesteps)
eda_test &amp;lt;-
  average_and_make_sequences(eda[(train_max * 8 + 1):nrow(eda), , drop = FALSE], 8, num_timesteps)
temp_test &amp;lt;-
  average_and_make_sequences(temp[(train_max * 8 + 1):nrow(temp), , drop = FALSE], 8, num_timesteps)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now put all predictors together:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# all predictors
x_train &amp;lt;- zip_datasets(acc_train, bvp_train, eda_train, temp_train) %&amp;gt;%
  dataset_map(function(...)
    tf$concat(list(...), axis = 1L))

x_test &amp;lt;- zip_datasets(acc_test, bvp_test, eda_test, temp_test) %&amp;gt;%
  dataset_map(function(...)
    tf$concat(list(...), axis = 1L))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the ground truth side, as alluded to before, we leave out the first eleven values in each case:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;y_train &amp;lt;- tensor_slices_dataset(label[num_timesteps:train_max] %&amp;gt;% k_cast(&amp;quot;float32&amp;quot;))

y_test &amp;lt;- tensor_slices_dataset(label[(train_max + num_timesteps):nrow(label)] %&amp;gt;% k_cast(&amp;quot;float32&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Zip predictors and targets together, configure shuffling/batching, and the datasets are complete:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ds_train &amp;lt;- zip_datasets(x_train, y_train)
ds_test &amp;lt;- zip_datasets(x_test, y_test)

batch_size &amp;lt;- 32

ds_train &amp;lt;- ds_train %&amp;gt;% 
  dataset_shuffle(n_train) %&amp;gt;%
  # dataset_repeat is needed because of pre-TF 2 style
  # hopefully at a later time, the code can run eagerly and this is no longer needed
  dataset_repeat() %&amp;gt;%
  dataset_batch(batch_size, drop_remainder = TRUE)

ds_test &amp;lt;- ds_test %&amp;gt;%
  # see above reg. dataset_repeat
  dataset_repeat() %&amp;gt;%
  dataset_batch(batch_size)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With data manipulations as complicated as the above, it’s always worthwhile checking some pipeline outputs. We can do that using the usual &lt;code&gt;reticulate::as_iterator&lt;/code&gt; magic, provided that for this test run, we &lt;em&gt;don’t&lt;/em&gt; disable V2 behavior. (Just restart the R session between a “pipeline checking” and the later modeling runs.)&lt;/p&gt;
&lt;p&gt;Here, in any case, would be the relevant code:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# this piece needs TF 2 behavior enabled
# run after restarting R and commenting the tf$compat$v1$disable_v2_behavior() line
# then to fit the DP model, undo comment, restart R and rerun
iter &amp;lt;- as_iterator(ds_test) # or any other dataset you want to check
while (TRUE) {
 item &amp;lt;- iter_next(iter)
 if (is.null(item)) break
 print(item)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With that we’re ready to create the model.&lt;/p&gt;
&lt;h2 id="model"&gt;Model&lt;/h2&gt;
&lt;p&gt;The model will be a rather simple convnet. The main difference between standard and DP training lies in the optimization procedure; thus, it’s straightforward to first establish a non-DP baseline. Later, when switching to DP, we’ll be able to reuse almost everything.&lt;/p&gt;
&lt;p&gt;Here, then, is the model definition valid for both cases:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_conv_1d(
      filters = 32,
      kernel_size = 3,
      activation = &amp;quot;relu&amp;quot;
    ) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_conv_1d(
      filters = 64,
      kernel_size = 5,
      activation = &amp;quot;relu&amp;quot;
    ) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_conv_1d(
      filters = 128,
      kernel_size = 5,
      activation = &amp;quot;relu&amp;quot;
    ) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_global_average_pooling_1d() %&amp;gt;%
  layer_dense(units = 128, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dense(units = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We train the model with mean squared error loss.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;optimizer &amp;lt;- optimizer_adam()
model %&amp;gt;% compile(loss = &amp;quot;mse&amp;quot;, optimizer = optimizer, metrics = metric_mean_absolute_error)

num_epochs &amp;lt;- 20
history &amp;lt;- model %&amp;gt;% fit(
  ds_train, 
  steps_per_epoch = n_train/batch_size,
  validation_data = ds_test,
  epochs = num_epochs,
  validation_steps = n_test/batch_size)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="baseline-results"&gt;Baseline results&lt;/h2&gt;
&lt;p&gt;After 20 epochs, mean absolute error is around 6 bpm:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-12-20-differential-privacy/images/baseline.png" alt="Training history without differential privacy." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-14)Training history without differential privacy.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Just to put this in context, the MAE reported for subject S1 in the paper&lt;span class="citation"&gt;(Reiss et al. 2019)&lt;/span&gt; – based on a higher-capacity network, extensive hyperparameter tuning, and naturally, training on the complete dataset – amounts to 8.45 bpm on average; so our setup seems to be sound.&lt;/p&gt;
&lt;p&gt;Now we’ll make this differentially private.&lt;/p&gt;
&lt;h2 id="dp-training"&gt;DP training&lt;/h2&gt;
&lt;p&gt;Instead of the plain &lt;code&gt;Adam&lt;/code&gt; optimizer, we use the corresponding TF Privacy wrapper, &lt;code&gt;DPAdamGaussianOptimizer&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We need to tell it how aggressive gradient clipping should be (&lt;code&gt;l2_norm_clip&lt;/code&gt;) and how much noise to add (&lt;code&gt;noise_multiplier&lt;/code&gt;). Furthermore, we define the learning rate (there is no default), going for 10 times the default &lt;code&gt;0.001&lt;/code&gt; based on initial experiments.&lt;/p&gt;
&lt;p&gt;There is an additional parameter, &lt;code&gt;num_microbatches&lt;/code&gt;, that could be used to speed up training &lt;span class="citation"&gt;(McMahan and Andrew 2018)&lt;/span&gt;, but, as training duration is not an issue here, we just set it equal to &lt;code&gt;batch_size&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The values for &lt;code&gt;l2_norm_clip&lt;/code&gt; and &lt;code&gt;noise_multiplier&lt;/code&gt; chosen here follow those used in the &lt;a href="https://github.com/tensorflow/privacy/tree/master/tutorials"&gt;tutorials in the TF Privacy repo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Nicely, TF Privacy comes with a script that allows one to compute the attained &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt; beforehand, based on number of training examples, &lt;code&gt;batch_size&lt;/code&gt;, &lt;code&gt;noise_multiplier&lt;/code&gt; and number of training epochs.&lt;a href="#fn5" class="footnote-ref" id="fnref5"&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Calling that &lt;a href="https://github.com/tensorflow/privacy/blob/master/tensorflow_privacy/privacy/analysis/compute_dp_sgd_privacy.py"&gt;script&lt;/a&gt;, and assuming we train for 20 epochs here as well,&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;python compute_dp_sgd_privacy.py --N=3989 --batch_size=32 --noise_multiplier=1.1 --epochs=20&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;this is what we get back:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;DP-SGD with sampling rate = 0.802% and noise_multiplier = 1.1 iterated over
2494 steps satisfies differential privacy with eps = 2.73 and delta = 1e-06.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How good is a value of 2.73? Citing the &lt;a href="https://github.com/tensorflow/privacy/tree/master/tutorials"&gt;TF Privacy authors&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt; gives a ceiling on how much the probability of a particular output can increase by including (or removing) a single training example. We usually want it to be a small constant (less than 10, or, for more stringent privacy guarantees, less than 1). However, this is only an upper bound, and a large value of epsilon may still mean good practical privacy.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Obviously, choice of &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt; is a (challenging) topic unto itself, and not something we can elaborate on in a post dedicated to the technical aspects of DP with TensorFlow.&lt;/p&gt;
&lt;p&gt;How would &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt; change if we trained for 50 epochs instead? (This is actually what we’ll do, seeing that training results on the test set tend to jump around quite a bit.)&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;python compute_dp_sgd_privacy.py --N=3989 --batch_size=32 --noise_multiplier=1.1 --epochs=60&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;DP-SGD with sampling rate = 0.802% and noise_multiplier = 1.1 iterated over
6233 steps satisfies differential privacy with eps = 4.25 and delta = 1e-06.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Having talked about its parameters, now let’s define the DP optimizer:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;l2_norm_clip &amp;lt;- 1
noise_multiplier &amp;lt;- 1.1
num_microbatches &amp;lt;- k_cast(batch_size, &amp;quot;int32&amp;quot;)
learning_rate &amp;lt;- 0.01

optimizer &amp;lt;- priv$DPAdamGaussianOptimizer(
  l2_norm_clip = l2_norm_clip,
  noise_multiplier = noise_multiplier,
  num_microbatches = num_microbatches,
  learning_rate = learning_rate
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is one other change to make for DP. As gradients are clipped on a per-sample basis, the optimizer needs to work with per-sample losses as well:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;loss &amp;lt;- tf$keras$losses$MeanSquaredError(reduction =  tf$keras$losses$Reduction$NONE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Everything else stays the same. Training history (like we said above, lasting for 50 epochs now) looks a lot more turbulent, with MAEs on the test set fluctuating between 8 and 20 over the last 10 training epochs:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-12-20-differential-privacy/images/dp.png" alt="Training history with differential privacy." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-19)Training history with differential privacy.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In addition to the above-mentioned command line script, we can also compute &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt; as part of the training code. Let’s double check:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# probability of an individual training point being included in a minibatch
sampling_probability &amp;lt;- batch_size / n_train

# number of steps the optimizer takes over the training data
steps &amp;lt;- num_epochs * n_train / batch_size

# required for reasons related to how TF Privacy computes privacy
# this actually is Renyi Differential Privacy: https://arxiv.org/abs/1702.07476
# we don&amp;#39;t go into details here and use same values as the command line script
orders &amp;lt;- c((1 + (1:99)/10), 12:63)

rdp &amp;lt;- priv$privacy$analysis$rdp_accountant$compute_rdp(
  q = sampling_probability,
  noise_multiplier = noise_multiplier,
  steps = steps,
  orders = orders)

priv$privacy$analysis$rdp_accountant$get_privacy_spent(
  orders, rdp, target_delta = 1e-6)[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 4.249645&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, we do get the same result.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This post showed how to convert a normal deep learning procedure into an &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt;-differentially private one. Necessarily, a blog post has to leave open questions. In the present case, some possible questions could be answered by straightforward experimentation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How well do other optimizers work in this setting?&lt;/li&gt;
&lt;li&gt;How does the learning rate affect privacy and performance?&lt;/li&gt;
&lt;li&gt;What happens if we train for a lot longer?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Others sound more like they could lead to a research project:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When model performance – and thus, model parameters – fluctuate that much, how do we decide on when to stop training? Is stopping at high model performance &lt;em&gt;cheating&lt;/em&gt;? Is model averaging a sound solution?&lt;/li&gt;
&lt;li&gt;How good really &lt;em&gt;is&lt;/em&gt; any one &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt;?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, yet others transcend the realms of experimentation as well as mathematics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How do we trade off &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt;-DP against model performance – for different applications, with different types of data, in different societal contexts?&lt;/li&gt;
&lt;li&gt;Assuming we “have” &lt;span class="math inline"&gt;\(\epsilon\)&lt;/span&gt;-DP, what might we still be missing?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With questions like these – and more, probably – to ponder: Thanks for reading and a happy new year!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-abaditfp"&gt;
&lt;p&gt;Abadi, Martin, Andy Chu, Ian Goodfellow, Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. 2016. “Deep Learning with Differential Privacy.” In &lt;em&gt;23rd Acm Conference on Computer and Communications Security (Acm Ccs)&lt;/em&gt;, 308–18. &lt;a href="https://arxiv.org/abs/1607.00133"&gt;https://arxiv.org/abs/1607.00133&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Allen_2007"&gt;
&lt;p&gt;Allen, John. 2007. “Photoplethysmography and Its Application in Clinical Physiological Measurement.” &lt;em&gt;Physiological Measurement&lt;/em&gt; 28 (3): R1–R39. &lt;a href="https://doi.org/10.1088/0967-3334/28/3/r01"&gt;https://doi.org/10.1088/0967-3334/28/3/r01&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-dwork2006differential"&gt;
&lt;p&gt;Dwork, Cynthia. 2006. “Differential Privacy.” In &lt;em&gt;33rd International Colloquium on Automata, Languages and Programming, Part Ii (Icalp 2006)&lt;/em&gt;, 33rd International Colloquium on Automata, Languages and Programming, part II (ICALP 2006), 4052:1–12. Lecture Notes in Computer Science. Springer Verlag. &lt;a href="https://www.microsoft.com/en-us/research/publication/differential-privacy/"&gt;https://www.microsoft.com/en-us/research/publication/differential-privacy/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Dwork2006"&gt;
&lt;p&gt;Dwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. “Calibrating Noise to Sensitivity in Private Data Analysis.” In &lt;em&gt;Proceedings of the Third Conference on Theory of Cryptography&lt;/em&gt;, 265–84. TCC’06. Berlin, Heidelberg: Springer-Verlag. &lt;a href="https://doi.org/10.1007/11681878_14"&gt;https://doi.org/10.1007/11681878_14&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Dwork"&gt;
&lt;p&gt;Dwork, Cynthia, and Aaron Roth. 2014. “The Algorithmic Foundations of Differential Privacy.” &lt;em&gt;Found. Trends Theor. Comput. Sci.&lt;/em&gt; 9 (3&amp;amp;#8211;4): 211–407. &lt;a href="https://doi.org/10.1561/0400000042"&gt;https://doi.org/10.1561/0400000042&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-abs181206210"&gt;
&lt;p&gt;McMahan, H. Brendan, and Galen Andrew. 2018. “A General Approach to Adding Differential Privacy to Iterative Training Procedures.” &lt;em&gt;CoRR&lt;/em&gt; abs/1812.06210. &lt;a href="http://arxiv.org/abs/1812.06210"&gt;http://arxiv.org/abs/1812.06210&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-ReissISL19"&gt;
&lt;p&gt;Reiss, Attila, Ina Indlekofer, Philip Schmidt, and Kristof Van Laerhoven. 2019. “Deep Ppg: Large-Scale Heart Rate Estimation with Convolutional Neural Networks.” &lt;em&gt;Sensors&lt;/em&gt; 19 (14): 3079. &lt;a href="https://doi.org/10.3390/s19143079"&gt;https://doi.org/10.3390/s19143079&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-primer"&gt;
&lt;p&gt;Wood, Alexandra, Micah Altman, Aaron Bembenek, Mark Bun, Marco Gaboardi, James Honaker, Kobbi Nissim, David O’Brien, Thomas Steinke, and Salil Vadhan. 2018. “Differential Privacy: A Primer for a Non-Technical Audience.” &lt;em&gt;SSRN Electronic Journal&lt;/em&gt;, January. &lt;a href="https://doi.org/10.2139/ssrn.3338027"&gt;https://doi.org/10.2139/ssrn.3338027&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;We’ll be using DP as an acronym for both the noun phrase “differential privacy” and the adjective phrase “differentially private”.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;This is not &lt;em&gt;exactly&lt;/em&gt; everything, as we’ll see when we get to the code, but “just about”.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;Relative files per subject are 1.4G in size.&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;For convenience, we take the liberty to talk as if we had the usual rectangular data here.&lt;a href="#fnref4" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn5"&gt;&lt;p&gt;There is an additional parameter, &lt;span class="math inline"&gt;\(\delta\)&lt;/span&gt;, that allows for bounding the risk that the privacy guarantee does not hold. The &lt;a href="https://github.com/tensorflow/privacy/tree/master/tutorials"&gt;recommendation&lt;/a&gt; is to set this to at most the inverse of the number of training examples, which in out case would mean &amp;lt;= ~ &lt;code&gt;1e-04&lt;/code&gt;; the default setting is &lt;code&gt;1e-06&lt;/code&gt; so we should be fine here.&lt;a href="#fnref5" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">6d947cd8e6a99cd81575dc99eb772e77</distill:md5>
      <category>Privacy &amp; Security</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-12-20-differential-privacy</guid>
      <pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-12-20-differential-privacy/images/cat.png" medium="image" type="image/png" width="400" height="251"/>
    </item>
    <item>
      <title>tfhub: R interface to TensorFlow Hub</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Falbel</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-12-18-tfhub-0.7.0</link>
      <description>


&lt;p&gt;We are pleased to announce that the first version of &lt;a href="https://github.com/rstudio/tfhub"&gt;tfhub&lt;/a&gt; is now on CRAN. tfhub is an R interface to TensorFlow Hub - a library for the publication, discovery, and consumption of reusable parts of machine learning models. A module is a self-contained piece of a TensorFlow graph, along with its weights and assets, that can be reused across different tasks in a process known as transfer learning.&lt;/p&gt;
&lt;p&gt;The CRAN version of tfhub can be installed with:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;install.packages(&amp;quot;tfhub&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After installing the R package you need to install the TensorFlow Hub python package. You can do it by running:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;tfhub::install_tfhub()&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="getting-started"&gt;Getting started&lt;/h2&gt;
&lt;p&gt;The essential function of tfhub is &lt;code&gt;layer_hub&lt;/code&gt; which works just like a &lt;a href="https://github.com/rstudio/keras"&gt;keras&lt;/a&gt; layer but allows you to load a complete pre-trained deep learning model.&lt;/p&gt;
&lt;p&gt;For example you can:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tfhub)
layer_mobilenet &amp;lt;- layer_hub(
  handle = &amp;quot;https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will download the MobileNet model pre-trained on the ImageNet dataset. tfhub models are cached locally and don’t need to be downloaded the next time you use the same model.&lt;/p&gt;
&lt;p&gt;You can now use &lt;code&gt;layer_mobilenet&lt;/code&gt; as a usual Keras layer. For example you can define a model:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
input &amp;lt;- layer_input(shape = c(224, 224, 3))
output &amp;lt;- layer_mobilenet(input)
model &amp;lt;- keras_model(input, output)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model: &amp;quot;model&amp;quot;
____________________________________________________________________
Layer (type)                  Output Shape               Param #    
====================================================================
input_2 (InputLayer)          [(None, 224, 224, 3)]      0          
____________________________________________________________________
keras_layer_1 (KerasLayer)    (None, 1001)               3540265    
====================================================================
Total params: 3,540,265
Trainable params: 0
Non-trainable params: 3,540,265
____________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This model can now be used to predict Imagenet labels for an image. For example, let’s see the results for the famous Grace Hopper’s photo:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-12-18-tfhub-0.7.0/images/grace-hopper.jpg" style="width:30.0%" alt="" /&gt;
&lt;p class="caption"&gt;Grace Hopper&lt;/p&gt;
&lt;/div&gt;
&lt;pre class="r"&gt;&lt;code&gt;img &amp;lt;- image_load(&amp;quot;images/grace-hopper.jpg&amp;quot;, target_size = c(224,224)) %&amp;gt;% 
  image_to_array()
img &amp;lt;- img/255
dim(img) &amp;lt;- c(1, dim(img))
pred &amp;lt;- predict(model, img)
imagenet_decode_predictions(pred[,-1,drop=FALSE])[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  class_name class_description    score
1  n03763968  military_uniform 9.760404
2  n02817516          bearskin 5.922512
3  n04350905              suit 5.729345
4  n03787032       mortarboard 5.400651
5  n03929855       pickelhaube 5.008665&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;TensorFlow Hub also offers many other pre-trained image, text and video models. All possible models can be found on the TensorFlow hub &lt;a href="https://tfhub.dev"&gt;website&lt;/a&gt;.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-12-18-tfhub-0.7.0/images/tfhub.png" alt="" /&gt;
&lt;p class="caption"&gt;TensorFlow Hub&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;You can find more examples of &lt;code&gt;layer_hub&lt;/code&gt; usage in the following articles on the TensorFlow for R website:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_text_classification_with_tfhub/"&gt;Transfer Learning with tfhub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tensorflow.rstudio.com/guide/tfhub/hub-with-keras/"&gt;Using tfhub with Keras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tensorflow.rstudio.com/guide/tfhub/intro/"&gt;tfhub Basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tensorflow.rstudio.com/guide/tfhub/examples/text_classification/"&gt;Text classification example&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="usage-with-recipes-and-the-feature-spec-api"&gt;Usage with Recipes and the Feature Spec API&lt;/h2&gt;
&lt;p&gt;tfhub also offers &lt;a href="https://github.com/tidymodels/recipes"&gt;recipes&lt;/a&gt; steps to make it easier to use pre-trained deep learning models in your machine learning workflow.&lt;/p&gt;
&lt;p&gt;For example, you can define a recipe that uses a pre-trained text embedding model with:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;rec &amp;lt;- recipe(obscene ~ comment_text, data = train) %&amp;gt;%
  step_pretrained_text_embedding(
    comment_text,
    handle = &amp;quot;https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim-with-oov/1&amp;quot;
  ) %&amp;gt;%
  step_bin2factor(obscene)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see a complete running example &lt;a href="https://tensorflow.rstudio.com/guide/tfhub/examples/recipes/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can also use tfhub with the new &lt;a href="https://tensorflow.rstudio.com/guide/tfdatasets/feature_spec/"&gt;Feature Spec API&lt;/a&gt; implemented in tfdatasets. You can see a complete example &lt;a href="https://tensorflow.rstudio.com/guide/tfhub/examples/feature_column/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We hope our readers have fun experimenting with Hub models and/or can put them to good use. If you run into any problems, let us know by creating an issue in the tfhub repository&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">4d8a05b040d0af75c6540549d2fc9dc5</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-12-18-tfhub-0.7.0</guid>
      <pubDate>Wed, 18 Dec 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-12-18-tfhub-0.7.0/images/tfhub.png" medium="image" type="image/png" width="1365" height="909"/>
    </item>
    <item>
      <title>Gaussian Process Regression with tfprobability</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-12-10-variational-gaussian-process</link>
      <description>


&lt;p&gt;How do you motivate, or come up with a story around Gaussian Process Regression on a blog primarily dedicated to deep learning?&lt;/p&gt;
&lt;p&gt;Easy. As demonstrated by seemingly unavoidable, reliably recurring Twitter “wars” surrounding AI, nothing attracts attention like controversy and antagonism. So, let’s go back twenty years and find citations of people saying, “here come Gaussian Processes, we don’t need to bother with those finicky, hard to tune neural networks anymore!”. And today, here we are; everyone knows &lt;em&gt;something&lt;/em&gt; about deep learning but who’s heard of Gaussian Processes?&lt;/p&gt;
&lt;p&gt;While similar tales tell a lot about history of science and development of opinions, we prefer a different angle here. In the preface to their 2006 book on &lt;em&gt;Gaussian Processes for Machine Learning&lt;/em&gt; &lt;span class="citation"&gt;(Rasmussen and Williams 2005)&lt;/span&gt;, Rasmussen and Williams say, referring to the “two cultures” – the disciplines of statistics and machine learning, respectively:&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Gaussian process models in some sense bring together work in the two communities.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this post, that “in some sense” gets very concrete. We’ll see a Keras network, defined and trained the usual way, that has a Gaussian Process layer for its main constituent. The task will be “simple” multivariate regression.&lt;/p&gt;
&lt;p&gt;As an aside, this “bringing together communities” – or ways of thinking, or solution strategies – makes for a good overall characterization of TensorFlow Probability as well.&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="gaussian-processes"&gt;Gaussian Processes&lt;/h2&gt;
&lt;p&gt;A Gaussian Process is a &lt;a href="https://math.stackexchange.com/a/2297480"&gt;distribution over functions, where the function values you sample are jointly Gaussian&lt;/a&gt; - roughly speaking, a generalization to infinity of the multivariate Gaussian. Besides the reference book we already mentioned &lt;span class="citation"&gt;(Rasmussen and Williams 2005)&lt;/span&gt;, there are a number of nice introductions on the net: see e.g. &lt;a href="https://distill.pub/2019/visual-exploration-gaussian-processes/"&gt;https://distill.pub/2019/visual-exploration-gaussian-processes/&lt;/a&gt; or &lt;a href="https://peterroelants.github.io/posts/gaussian-process-tutorial/"&gt;https://peterroelants.github.io/posts/gaussian-process-tutorial/&lt;/a&gt;. And like on everything cool, there is a chapter on Gaussian Processes in the late David MacKay’s &lt;span class="citation"&gt;(MacKay 2002)&lt;/span&gt; &lt;a href="http://www.inference.org.uk/itprnn/book.pdf"&gt;book&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this post, we’ll use TensorFlow Probability’s &lt;em&gt;Variational Gaussian Process&lt;/em&gt; (VGP) layer, designed to efficiently work with “big data”. As Gaussian Process Regression (GPR, from now on) involves the inversion of a – possibly big – covariance matrix, attempts have been made to design approximate versions, often based on variational principles. The TFP implementation is based on papers by Titsias (2009) &lt;span class="citation"&gt;(Titsias 2009)&lt;/span&gt; and Hensman et al. (2013) &lt;span class="citation"&gt;(Hensman, Fusi, and Lawrence 2013)&lt;/span&gt;. Instead of &lt;span class="math inline"&gt;\(p(\mathbf{y}|\mathbf{X})\)&lt;/span&gt;, the actual probability of the target data given the actual input, we work with a variational distribution &lt;span class="math inline"&gt;\(q(\mathbf{u})\)&lt;/span&gt; that acts as a lower bound.&lt;/p&gt;
&lt;p&gt;Here &lt;span class="math inline"&gt;\(\mathbf{u}\)&lt;/span&gt; are the function values at a set of so-called &lt;em&gt;inducing index points&lt;/em&gt; specified by the user, chosen to well cover the range of the actual data. This algorithm is a lot faster than “normal” GPR, as only the covariance matrix of &lt;span class="math inline"&gt;\(\mathbf{u}\)&lt;/span&gt; has to be inverted. As we’ll see below, at least in this example (as well as in others not described here) it seems to be pretty robust as to the number of &lt;em&gt;inducing points&lt;/em&gt; passed.&lt;/p&gt;
&lt;p&gt;Let’s start.&lt;/p&gt;
&lt;h2 id="the-dataset"&gt;The dataset&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://archive.ics.uci.edu/ml/datasets/concrete+compressive+strength"&gt;Concrete Compressive Strength Data Set&lt;/a&gt; is part of the UCI Machine Learning Repository. Its web page says:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Concrete is the most important material in civil engineering. The concrete compressive strength is a highly nonlinear function of age and ingredients.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;Highly nonlinear function&lt;/em&gt; - doesn’t that sound intriguing? In any case, it should constitute an interesting test case for GPR.&lt;/p&gt;
&lt;p&gt;Here is a first look.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tidyverse)
library(GGally)
library(visreg)
library(readxl)
library(rsample)
library(reticulate)
library(tfdatasets)
library(keras)
library(tfprobability)

concrete &amp;lt;- read_xls(
  &amp;quot;Concrete_Data.xls&amp;quot;,
  col_names = c(
    &amp;quot;cement&amp;quot;,
    &amp;quot;blast_furnace_slag&amp;quot;,
    &amp;quot;fly_ash&amp;quot;,
    &amp;quot;water&amp;quot;,
    &amp;quot;superplasticizer&amp;quot;,
    &amp;quot;coarse_aggregate&amp;quot;,
    &amp;quot;fine_aggregate&amp;quot;,
    &amp;quot;age&amp;quot;,
    &amp;quot;strength&amp;quot;
  ),
  skip = 1
)

concrete %&amp;gt;% glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Observations: 1,030
Variables: 9
$ cement             &amp;lt;dbl&amp;gt; 540.0, 540.0, 332.5, 332.5, 198.6, 266.0, 380.0, 380.0, …
$ blast_furnace_slag &amp;lt;dbl&amp;gt; 0.0, 0.0, 142.5, 142.5, 132.4, 114.0, 95.0, 95.0, 114.0,…
$ fly_ash            &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
$ water              &amp;lt;dbl&amp;gt; 162, 162, 228, 228, 192, 228, 228, 228, 228, 228, 192, 1…
$ superplasticizer   &amp;lt;dbl&amp;gt; 2.5, 2.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0…
$ coarse_aggregate   &amp;lt;dbl&amp;gt; 1040.0, 1055.0, 932.0, 932.0, 978.4, 932.0, 932.0, 932.0…
$ fine_aggregate     &amp;lt;dbl&amp;gt; 676.0, 676.0, 594.0, 594.0, 825.5, 670.0, 594.0, 594.0, …
$ age                &amp;lt;dbl&amp;gt; 28, 28, 270, 365, 360, 90, 365, 28, 28, 28, 90, 28, 270,…
$ strength           &amp;lt;dbl&amp;gt; 79.986111, 61.887366, 40.269535, 41.052780, 44.296075, 4…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is not that big – just a little more than 1000 rows –, but still, we will have room to experiment with different numbers of &lt;em&gt;inducing points&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We have eight predictors, all numeric. With the exception of &lt;code&gt;age&lt;/code&gt; (in &lt;em&gt;days&lt;/em&gt;), these represent masses (in &lt;em&gt;kg&lt;/em&gt;) in one cubic metre of concrete. The target variable, &lt;code&gt;strength&lt;/code&gt;, is measured in megapascals.&lt;/p&gt;
&lt;p&gt;Let’s get a quick overview of mutual relationships.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggpairs(concrete)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-12-10-variational-gaussian-process/images/pairs.png" width="700" /&gt;&lt;/p&gt;
&lt;p&gt;Checking for a possible interaction (one that a layperson could easily think of), does cement concentration act differently on concrete strength depending on how much water there is in the mixture?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;cement_ &amp;lt;- cut(concrete$cement, 3, labels = c(&amp;quot;low&amp;quot;, &amp;quot;medium&amp;quot;, &amp;quot;high&amp;quot;))
fit &amp;lt;- lm(strength ~ (.) ^ 2, data = cbind(concrete[, 2:9], cement_))
summary(fit)

visreg(fit, &amp;quot;cement_&amp;quot;, &amp;quot;water&amp;quot;, gg = TRUE) + theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-12-10-variational-gaussian-process/images/visreg.png" width="700" /&gt;&lt;/p&gt;
&lt;p&gt;To anchor our future perception of how well VGP does for this example, we fit a simple linear model, as well as one involving two-way interactions.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# scale predictors here already, so data are the same for all models
concrete[, 1:8] &amp;lt;- scale(concrete[, 1:8])

# train-test split 
set.seed(777)
split &amp;lt;- initial_split(concrete, prop = 0.8)
train &amp;lt;- training(split)
test &amp;lt;- testing(split)

# simple linear model with no interactions
fit1 &amp;lt;- lm(strength ~ ., data = train)
fit1 %&amp;gt;% summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Call:
lm(formula = strength ~ ., data = train)

Residuals:
    Min      1Q  Median      3Q     Max 
-30.594  -6.075   0.612   6.694  33.032 

Coefficients:
                   Estimate Std. Error t value Pr(&amp;gt;|t|)    
(Intercept)         35.6773     0.3596  99.204  &amp;lt; 2e-16 ***
cement              13.0352     0.9702  13.435  &amp;lt; 2e-16 ***
blast_furnace_slag   9.1532     0.9582   9.552  &amp;lt; 2e-16 ***
fly_ash              5.9592     0.8878   6.712 3.58e-11 ***
water               -2.5681     0.9503  -2.702  0.00703 ** 
superplasticizer     1.9660     0.6138   3.203  0.00141 ** 
coarse_aggregate     1.4780     0.8126   1.819  0.06929 .  
fine_aggregate       2.2213     0.9470   2.346  0.01923 *  
age                  7.7032     0.3901  19.748  &amp;lt; 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 10.32 on 816 degrees of freedom
Multiple R-squared:  0.627, Adjusted R-squared:  0.6234 
F-statistic: 171.5 on 8 and 816 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# two-way interactions
fit2 &amp;lt;- lm(strength ~ (.) ^ 2, data = train)
fit2 %&amp;gt;% summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Call:
lm(formula = strength ~ (.)^2, data = train)

Residuals:
     Min       1Q   Median       3Q      Max 
-24.4000  -5.6093  -0.0233   5.7754  27.8489 

Coefficients:
                                    Estimate Std. Error t value Pr(&amp;gt;|t|)    
(Intercept)                          40.7908     0.8385  48.647  &amp;lt; 2e-16 ***
cement                               13.2352     1.0036  13.188  &amp;lt; 2e-16 ***
blast_furnace_slag                    9.5418     1.0591   9.009  &amp;lt; 2e-16 ***
fly_ash                               6.0550     0.9557   6.336 3.98e-10 ***
water                                -2.0091     0.9771  -2.056 0.040090 *  
superplasticizer                      3.8336     0.8190   4.681 3.37e-06 ***
coarse_aggregate                      0.3019     0.8068   0.374 0.708333    
fine_aggregate                        1.9617     0.9872   1.987 0.047256 *  
age                                  14.3906     0.5557  25.896  &amp;lt; 2e-16 ***
cement:blast_furnace_slag             0.9863     0.5818   1.695 0.090402 .  
cement:fly_ash                        1.6434     0.6088   2.700 0.007093 ** 
cement:water                         -4.2152     0.9532  -4.422 1.11e-05 ***
cement:superplasticizer              -2.1874     1.3094  -1.670 0.095218 .  
cement:coarse_aggregate               0.2472     0.5967   0.414 0.678788    
cement:fine_aggregate                 0.7944     0.5588   1.422 0.155560    
cement:age                            4.6034     1.3811   3.333 0.000899 ***
blast_furnace_slag:fly_ash            2.1216     0.7229   2.935 0.003434 ** 
blast_furnace_slag:water             -2.6362     1.0611  -2.484 0.013184 *  
blast_furnace_slag:superplasticizer  -0.6838     1.2812  -0.534 0.593676    
blast_furnace_slag:coarse_aggregate  -1.0592     0.6416  -1.651 0.099154 .  
blast_furnace_slag:fine_aggregate     2.0579     0.5538   3.716 0.000217 ***
blast_furnace_slag:age                4.7563     1.1148   4.266 2.23e-05 ***
fly_ash:water                        -2.7131     0.9858  -2.752 0.006054 ** 
fly_ash:superplasticizer             -2.6528     1.2553  -2.113 0.034891 *  
fly_ash:coarse_aggregate              0.3323     0.7004   0.474 0.635305    
fly_ash:fine_aggregate                2.6764     0.7817   3.424 0.000649 ***
fly_ash:age                           7.5851     1.3570   5.589 3.14e-08 ***
water:superplasticizer                1.3686     0.8704   1.572 0.116289    
water:coarse_aggregate               -1.3399     0.5203  -2.575 0.010194 *  
water:fine_aggregate                 -0.7061     0.5184  -1.362 0.173533    
water:age                             0.3207     1.2991   0.247 0.805068    
superplasticizer:coarse_aggregate     1.4526     0.9310   1.560 0.119125    
superplasticizer:fine_aggregate       0.1022     1.1342   0.090 0.928239    
superplasticizer:age                  1.9107     0.9491   2.013 0.044444 *  
coarse_aggregate:fine_aggregate       1.3014     0.4750   2.740 0.006286 ** 
coarse_aggregate:age                  0.7557     0.9342   0.809 0.418815    
fine_aggregate:age                    3.4524     1.2165   2.838 0.004657 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 8.327 on 788 degrees of freedom
Multiple R-squared:  0.7656,    Adjusted R-squared:  0.7549 
F-statistic: 71.48 on 36 and 788 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We also store the predictions on the test set, for later comparison.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;linreg_preds1 &amp;lt;- fit1 %&amp;gt;% predict(test[, 1:8])
linreg_preds2 &amp;lt;- fit2 %&amp;gt;% predict(test[, 1:8])

compare &amp;lt;-
  data.frame(
    y_true = test$strength,
    linreg_preds1 = linreg_preds1,
    linreg_preds2 = linreg_preds2
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With no further preprocessing required, the &lt;a href="https://tensorflow.rstudio.com/guide/tfdatasets/introduction/"&gt;tfdatasets&lt;/a&gt; input pipeline ends up nice and short:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;create_dataset &amp;lt;- function(df, batch_size, shuffle = TRUE) {
  
  df &amp;lt;- as.matrix(df)
  ds &amp;lt;-
    tensor_slices_dataset(list(df[, 1:8], df[, 9, drop = FALSE]))
  if (shuffle)
    ds &amp;lt;- ds %&amp;gt;% dataset_shuffle(buffer_size = nrow(df))
  ds %&amp;gt;%
    dataset_batch(batch_size = batch_size)
  
}

# just one possible choice for batch size ...
batch_size &amp;lt;- 64
train_ds &amp;lt;- create_dataset(train, batch_size = batch_size)
test_ds &amp;lt;- create_dataset(test, batch_size = nrow(test), shuffle = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And on to model creation.&lt;/p&gt;
&lt;h2 id="the-model"&gt;The model&lt;/h2&gt;
&lt;p&gt;Model definition is short as well, although there are a few things to expand on. Don’t execute this yet:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_dense(units = 8,
              input_shape = 8,
              use_bias = FALSE) %&amp;gt;%
  layer_variational_gaussian_process(
    # number of inducing points
    num_inducing_points = num_inducing_points,
    # kernel to be used by the wrapped Gaussian Process distribution
    kernel_provider = RBFKernelFn(),
    # output shape 
    event_shape = 1, 
    # initial values for the inducing points
    inducing_index_points_initializer = initializer_constant(as.matrix(sampled_points)),
    unconstrained_observation_noise_variance_initializer =
      initializer_constant(array(0.1))
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Two arguments to &lt;code&gt;layer_variational_gaussian_process()&lt;/code&gt; need some preparation before we can actually run this. First, as the documentation tells us, &lt;code&gt;kernel_provider&lt;/code&gt; should be&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;a layer instance equipped with an @property, which yields a &lt;code&gt;PositiveSemidefiniteKernel&lt;/code&gt; instance".&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In other words, the VGP layer wraps another Keras layer that, &lt;em&gt;itself&lt;/em&gt;, wraps or bundles together the TensorFlow &lt;code&gt;Variables&lt;/code&gt; containing the kernel parameters.&lt;/p&gt;
&lt;p&gt;We can make use of &lt;code&gt;reticulate&lt;/code&gt;’s new &lt;code&gt;PyClass&lt;/code&gt; constructor to fulfill the above requirements. Using &lt;code&gt;PyClass&lt;/code&gt;, we can directly inherit from a Python object, adding and/or overriding methods or fields as we like - and yes, even create a Python &lt;a href="https://docs.python.org/3/library/functions.html?#property"&gt;property&lt;/a&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;bt &amp;lt;- import(&amp;quot;builtins&amp;quot;)
RBFKernelFn &amp;lt;- reticulate::PyClass(
  &amp;quot;KernelFn&amp;quot;,
  inherit = tensorflow::tf$keras$layers$Layer,
  list(
    `__init__` = function(self, ...) {
      kwargs &amp;lt;- list(...)
      super()$`__init__`(kwargs)
      dtype &amp;lt;- kwargs[[&amp;quot;dtype&amp;quot;]]
      self$`_amplitude` = self$add_variable(initializer = initializer_zeros(),
                                            dtype = dtype,
                                            name = &amp;#39;amplitude&amp;#39;)
      self$`_length_scale` = self$add_variable(initializer = initializer_zeros(),
                                               dtype = dtype,
                                               name = &amp;#39;length_scale&amp;#39;)
      NULL
    },
    
    call = function(self, x, ...) {
      x
    },
    
    kernel = bt$property(
      reticulate::py_func(
        function(self)
          tfp$math$psd_kernels$ExponentiatedQuadratic(
            amplitude = tf$nn$softplus(array(0.1) * self$`_amplitude`),
            length_scale = tf$nn$softplus(array(2) * self$`_length_scale`)
          )
      )
    )
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Gaussian Process kernel used is one of several available in &lt;code&gt;tfp.math.psd_kernels&lt;/code&gt; (&lt;code&gt;psd&lt;/code&gt; standing for positive semidefinite), and probably the one that comes to mind first when thinking of GPR: the &lt;em&gt;squared exponential&lt;/em&gt;, or &lt;em&gt;exponentiated quadratic&lt;/em&gt;.&lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; The version used in TFP, with hyperparameters &lt;em&gt;amplitude&lt;/em&gt; &lt;span class="math inline"&gt;\(a\)&lt;/span&gt; and &lt;em&gt;length scale&lt;/em&gt; &lt;span class="math inline"&gt;\(\lambda\)&lt;/span&gt;, is&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[k(x,x&amp;#39;) = 2 \ a \ exp (\frac{- 0.5 (x−x&amp;#39;)^2}{\lambda^2}) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here the interesting parameter is the length scale &lt;span class="math inline"&gt;\(\lambda\)&lt;/span&gt;. When we have several features, their length scales – as induced by the learning algorithm – reflect their importance: If, for some feature, &lt;span class="math inline"&gt;\(\lambda\)&lt;/span&gt; is large, the respective squared deviations from the mean don’t matter that much. The inverse length scale can thus be used for &lt;em&gt;automatic relevance determination&lt;/em&gt; &lt;span class="citation"&gt;(Neal 1996)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The second thing to take care of is choosing the initial index points. From experiments, the exact choices don’t matter that much, as long as the data are sensibly covered. For instance, an alternative way we tried was to construct an empirical distribution (&lt;a href="https://rstudio.github.io/tfprobability/reference/tfd_empirical.html"&gt;tfd_empirical&lt;/a&gt;) from the data, and then sample from it. Here instead, we just use an – unnecessary, admittedly, given the availability of &lt;code&gt;sample&lt;/code&gt; in R – fancy way to select random observations from the training data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;num_inducing_points &amp;lt;- 50

sample_dist &amp;lt;- tfd_uniform(low = 1, high = nrow(train) + 1)
sample_ids &amp;lt;- sample_dist %&amp;gt;%
  tfd_sample(num_inducing_points) %&amp;gt;%
  tf$cast(tf$int32) %&amp;gt;%
  as.numeric()
sampled_points &amp;lt;- train[sample_ids, 1:8]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One interesting point to note before we start training: Computation of the posterior predictive parameters involves a Cholesky decomposition, which could fail if, due to numerical issues, the covariance matrix is no longer positive definite. A sufficient action to take in our case is to do all computations using &lt;code&gt;tf$float64&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;k_set_floatx(&amp;quot;float64&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we define (for real, this time) and run the model.&lt;a href="#fn4" class="footnote-ref" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_dense(units = 8,
              input_shape = 8,
              use_bias = FALSE) %&amp;gt;%
  layer_variational_gaussian_process(
    num_inducing_points = num_inducing_points,
    kernel_provider = RBFKernelFn(),
    event_shape = 1,
    inducing_index_points_initializer = initializer_constant(as.matrix(sampled_points)),
    unconstrained_observation_noise_variance_initializer =
      initializer_constant(array(0.1))
  )

# KL weight sums to one for one epoch
kl_weight &amp;lt;- batch_size / nrow(train)

# loss that implements the VGP algorithm
loss &amp;lt;- function(y, rv_y)
  rv_y$variational_loss(y, kl_weight = kl_weight)

model %&amp;gt;% compile(optimizer = optimizer_adam(lr = 0.008),
                  loss = loss,
                  metrics = &amp;quot;mse&amp;quot;)

history &amp;lt;- model %&amp;gt;% fit(train_ds,
                         epochs = 100,
                         validation_data = test_ds)

plot(history)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-12-10-variational-gaussian-process/images/history.png" width="700" /&gt;&lt;/p&gt;
&lt;p&gt;Interestingly, higher numbers of &lt;em&gt;inducing points&lt;/em&gt; (we tried 100 and 200) did not have much impact on regression performance.&lt;a href="#fn5" class="footnote-ref" id="fnref5"&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; Nor does the exact choice of multiplication constants (&lt;code&gt;0.1&lt;/code&gt; and &lt;code&gt;2&lt;/code&gt;) applied to the trained kernel &lt;code&gt;Variables&lt;/code&gt; (&lt;code&gt;_amplitude&lt;/code&gt; and &lt;code&gt;_length_scale&lt;/code&gt;)&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;tfp$math$psd_kernels$ExponentiatedQuadratic(
  amplitude = tf$nn$softplus(array(0.1) * self$`_amplitude`),
  length_scale = tf$nn$softplus(array(2) * self$`_length_scale`)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;make much of a difference to the end result.&lt;a href="#fn6" class="footnote-ref" id="fnref6"&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="predictions"&gt;Predictions&lt;/h2&gt;
&lt;p&gt;We generate predictions on the test set and add them to the &lt;code&gt;data.frame&lt;/code&gt; containing the linear models’ predictions. As with other probabilistic output layers, “the predictions” are in fact distributions; to obtain actual tensors we sample from them. Here, we average over 10 samples:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;yhats &amp;lt;- model(tf$convert_to_tensor(as.matrix(test[, 1:8])))

yhat_samples &amp;lt;-  yhats %&amp;gt;%
  tfd_sample(10) %&amp;gt;%
  tf$squeeze() %&amp;gt;%
  tf$transpose()

sample_means &amp;lt;- yhat_samples %&amp;gt;% apply(1, mean)

compare &amp;lt;- compare %&amp;gt;%
  cbind(vgp_preds = sample_means)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We plot the average VGP predictions against the ground truth, together with the predictions from the simple linear model (cyan) and the model including two-way interactions (violet):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggplot(compare, aes(x = y_true)) +
  geom_abline(slope = 1, intercept = 0) +
  geom_point(aes(y = vgp_preds, color = &amp;quot;VGP&amp;quot;)) +
  geom_point(aes(y = linreg_preds1, color = &amp;quot;simple lm&amp;quot;), alpha = 0.4) +
  geom_point(aes(y = linreg_preds2, color = &amp;quot;lm w/ interactions&amp;quot;), alpha = 0.4) +
  scale_colour_manual(&amp;quot;&amp;quot;, 
                      values = c(&amp;quot;VGP&amp;quot; = &amp;quot;black&amp;quot;, &amp;quot;simple lm&amp;quot; = &amp;quot;cyan&amp;quot;, &amp;quot;lm w/ interactions&amp;quot; = &amp;quot;violet&amp;quot;)) +
  coord_cartesian(xlim = c(min(compare$y_true), max(compare$y_true)), ylim = c(min(compare$y_true), max(compare$y_true))) +
  ylab(&amp;quot;predictions&amp;quot;) +
  theme(aspect.ratio = 1) &lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-12-10-variational-gaussian-process/images/preds.png" alt="Predictions vs. ground truth for linear regression (no interactions; cyan), linear regression with 2-way interactions (violet), and VGP (black)." width="448" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-19)Predictions vs. ground truth for linear regression (no interactions; cyan), linear regression with 2-way interactions (violet), and VGP (black).
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Additionally, comparing MSEs for the three sets of predictions, we see&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;mse &amp;lt;- function(y_true, y_pred) {
  sum((y_true - y_pred) ^ 2) / length(y_true)
}

lm_mse1 &amp;lt;- mse(compare$y_true, compare$linreg_preds1) # 117.3111
lm_mse2 &amp;lt;- mse(compare$y_true, compare$linreg_preds2) # 80.79726
vgp_mse &amp;lt;- mse(compare$y_true, compare$vgp_preds)     # 58.49689&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, the VGP does in fact outperform both baselines. Something else we might be interested in: How do its predictions vary? Not as much as we might want, were we to construct uncertainty estimates from them alone. Here we plot the 10 samples we drew before:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;samples_df &amp;lt;-
  data.frame(cbind(compare$y_true, as.matrix(yhat_samples))) %&amp;gt;%
  gather(key = run, value = prediction, -X1) %&amp;gt;% 
  rename(y_true = &amp;quot;X1&amp;quot;)

ggplot(samples_df, aes(y_true, prediction)) +
  geom_point(aes(color = run),
             alpha = 0.2,
             size = 2) +
  geom_abline(slope = 1, intercept = 0) +
  theme(legend.position = &amp;quot;none&amp;quot;) +
  ylab(&amp;quot;repeated predictions&amp;quot;) +
  theme(aspect.ratio = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-12-10-variational-gaussian-process/images/preds2.png" alt="Predictions from 10 consecutive samples from the VGP distribution." width="700" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-22)Predictions from 10 consecutive samples from the VGP distribution.
&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="discussion-feature-relevance"&gt;Discussion: Feature Relevance&lt;/h2&gt;
&lt;p&gt;As mentioned above, the inverse length scale can be used as an indicator of feature importance. When using the &lt;code&gt;ExponentiatedQuadratic&lt;/code&gt; kernel alone, there will only be a single length scale; in our example, the initial &lt;code&gt;dense&lt;/code&gt; layer takes of scaling (and additionally, recombining) the features.&lt;/p&gt;
&lt;p&gt;Alternatively, we could wrap the &lt;code&gt;ExponentiatedQuadratic&lt;/code&gt; in a &lt;code&gt;FeatureScaled&lt;/code&gt; kernel. &lt;code&gt;FeatureScaled&lt;/code&gt; has an additional &lt;code&gt;scale_diag&lt;/code&gt; parameter related to exactly that: feature scaling. Experiments with &lt;code&gt;FeatureScaled&lt;/code&gt; (and initial &lt;code&gt;dense&lt;/code&gt; layer removed, to be “fair”) showed slightly worse performance, and the learned &lt;code&gt;scale_diag&lt;/code&gt; values varied quite a bit from run to run. For that reason, we chose to present the other approach; however, we include the code for a wrapping &lt;code&gt;FeatureScaled&lt;/code&gt; in case readers would like to experiment with this:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ScaledRBFKernelFn &amp;lt;- reticulate::PyClass(
  &amp;quot;KernelFn&amp;quot;,
  inherit = tensorflow::tf$keras$layers$Layer,
  list(
    `__init__` = function(self, ...) {
      kwargs &amp;lt;- list(...)
      super()$`__init__`(kwargs)
      dtype &amp;lt;- kwargs[[&amp;quot;dtype&amp;quot;]]
      self$`_amplitude` = self$add_variable(initializer = initializer_zeros(),
                                            dtype = dtype,
                                            name = &amp;#39;amplitude&amp;#39;)
      self$`_length_scale` = self$add_variable(initializer = initializer_zeros(),
                                               dtype = dtype,
                                               name = &amp;#39;length_scale&amp;#39;)
      self$`_scale_diag` = self$add_variable(
        initializer = initializer_ones(),
        dtype = dtype,
        shape = 8L,
        name = &amp;#39;scale_diag&amp;#39;
      )
      NULL
    },
    
    call = function(self, x, ...) {
      x
    },
    
    kernel = bt$property(
      reticulate::py_func(
        function(self)
          tfp$math$psd_kernels$FeatureScaled(
            kernel = tfp$math$psd_kernels$ExponentiatedQuadratic(
              amplitude = tf$nn$softplus(array(1) * self$`_amplitude`),
              length_scale = tf$nn$softplus(array(2) * self$`_length_scale`)
            ),
            scale_diag = tf$nn$softplus(array(1) * self$`_scale_diag`)
          )
      )
    )
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, if all you cared about was prediction performance, you could use &lt;code&gt;FeatureScaled&lt;/code&gt; and keep the initial &lt;code&gt;dense&lt;/code&gt; layer all the same. But in that case, you’d probably use a neural network – not a Gaussian Process – anyway …&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-breiman2001"&gt;
&lt;p&gt;Breiman, Leo. 2001. “Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).” &lt;em&gt;Statist. Sci.&lt;/em&gt; 16 (3): 199–231. &lt;a href="https://doi.org/10.1214/ss/1009213726"&gt;https://doi.org/10.1214/ss/1009213726&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-HensmanFL13"&gt;
&lt;p&gt;Hensman, James, Nicolo Fusi, and Neil D. Lawrence. 2013. “Gaussian Processes for Big Data.” &lt;em&gt;CoRR&lt;/em&gt; abs/1309.6835. &lt;a href="http://arxiv.org/abs/1309.6835"&gt;http://arxiv.org/abs/1309.6835&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-MacKay"&gt;
&lt;p&gt;MacKay, David J. C. 2002. &lt;em&gt;Information Theory, Inference &amp;amp; Learning Algorithms&lt;/em&gt;. New York, NY, USA: Cambridge University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Neal"&gt;
&lt;p&gt;Neal, Radford M. 1996. &lt;em&gt;Bayesian Learning for Neural Networks&lt;/em&gt;. Berlin, Heidelberg: Springer-Verlag.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Rasmussen"&gt;
&lt;p&gt;Rasmussen, Carl Edward, and Christopher K. I. Williams. 2005. &lt;em&gt;Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)&lt;/em&gt;. The MIT Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-titsias09a"&gt;
&lt;p&gt;Titsias, Michalis. 2009. “Variational Learning of Inducing Variables in Sparse Gaussian Processes.” In &lt;em&gt;Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics&lt;/em&gt;, edited by David van Dyk and Max Welling, 5:567–74. Proceedings of Machine Learning Research. Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA: PMLR. &lt;a href="http://proceedings.mlr.press/v5/titsias09a.html"&gt;http://proceedings.mlr.press/v5/titsias09a.html&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;In the book, it’s “two communities”, not “two cultures”; we choose L. Breiman’s &lt;span class="citation"&gt;(Breiman 2001)&lt;/span&gt; more punchy expression just … because.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;So far, we have seen uses of TensorFlow Probability for such different applications as &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-11-13-variational-convnet/"&gt;adding uncertainty estimates to neural networks&lt;/a&gt;, &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-05-24-varying-slopes/"&gt;Bayesian model estimation using Hamiltonian Monte Carlo&lt;/a&gt;, or &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-06-25-dynamic_linear_models_tfprobability/"&gt;linear-Gaussian state space models&lt;/a&gt;.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;See David Duvenaud’s &lt;a href="https://www.cs.toronto.edu/~duvenaud/cookbook/" class="uri"&gt;https://www.cs.toronto.edu/~duvenaud/cookbook/&lt;/a&gt; for an excellent synopsis of kernels and kernel composition.&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;In case you’re wondering about the &lt;code&gt;dense&lt;/code&gt; layer up front: This will be discussed in the last section on feature relevance.&lt;a href="#fnref4" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn5"&gt;&lt;p&gt;Performance being used in the sense of “lower regression error”, not running speed. As to the latter, there definitely is a (negative) relationship between number of inducing points and training speed.&lt;a href="#fnref5" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn6"&gt;&lt;p&gt;This may sound like a matter of course; it isn’t necessarily, as shown by prior experiments with variational layers e.g. in &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/"&gt;Adding uncertainty estimates to Keras models with tfprobability&lt;/a&gt;.&lt;a href="#fnref6" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">c277189acee7af465cb35f1e8d3bc6bf</distill:md5>
      <category>Probabilistic ML/DL</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-12-10-variational-gaussian-process</guid>
      <pubDate>Tue, 10 Dec 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-12-10-variational-gaussian-process/images/kernel_cookbook.png" medium="image" type="image/png" width="818" height="352"/>
    </item>
    <item>
      <title>Getting started with Keras from R - the 2020 edition</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-11-27-gettingstarted-2020</link>
      <description>


&lt;p&gt;If you’ve been thinking about diving into deep learning for a while – using R, preferentially –, now is a good time. For TensorFlow / Keras, one of the predominant deep learning frameworks on the market, last year was a year of substantial changes; for users, this sometimes would mean ambiguity and confusion about the “right” (or: recommended) way to do things. By now, &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-10-08-tf2-whatchanges/"&gt;TensorFlow 2.0&lt;/a&gt; has been the current stable release for about two months; the mists have cleared away, and patterns have emerged, enabling leaner, more modular code that accomplishes a lot in just a few lines.&lt;/p&gt;
&lt;p&gt;To give the new features the space they deserve, and assemble central contributions from related packages all in one place, we have significantly remodeled the &lt;a href="https://tensorflow.rstudio.com/"&gt;TensorFlow for R website&lt;/a&gt;. So this post really has two objectives.&lt;/p&gt;
&lt;p&gt;First, it would like to do exactly what is suggested by the title: Point new users to resources that make for an effective start into the subject.&lt;/p&gt;
&lt;p&gt;Second, it could be read as a “best of new website content”. Thus, as an existing user, you might still be interested in giving it a quick skim, checking for pointers to new features that appear in familiar contexts. To make this easier, we’ll add side notes to highlight new features.&lt;/p&gt;
&lt;p&gt;Overall, the structure of what follows is this. We start from the core question: &lt;em&gt;How do you build a model?&lt;/em&gt;, then frame it from both sides; i.e.: &lt;em&gt;What comes before?&lt;/em&gt; (data loading / preprocessing) and &lt;em&gt;What comes after?&lt;/em&gt; (model saving / deployment).&lt;/p&gt;
&lt;p&gt;After that, we quickly go into creating models for different types of data: images, text, tabular.&lt;/p&gt;
&lt;p&gt;Then, we touch on where to find background information, such as: How do I add a custom callback? How do I create a custom layer? How can I define my own training loop?&lt;/p&gt;
&lt;p&gt;Finally, we round up with something that looks like a tiny technical addition but has far greater impact: integrating modules from TensorFlow (TF) Hub.&lt;/p&gt;
&lt;h2 id="getting-started"&gt;Getting started&lt;/h2&gt;
&lt;h3 id="how-to-build-a-model"&gt;How to build a model?&lt;/h3&gt;
&lt;p&gt;If linear regression is the Hello World of machine learning, non-linear regression has to be the Hello World of neural networks. The &lt;a href="https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_regression/"&gt;Basic Regression tutorial&lt;/a&gt; shows how to train a dense network on the Boston Housing dataset. This example uses the Keras &lt;a href="https://tensorflow.rstudio.com/guide/keras/functional_api/"&gt;Functional API&lt;/a&gt;, one of the two “classical” model-building approaches – the one that tends to be used when some sort of flexibility is required. In this case, the desire for flexibility comes from the use of &lt;a href="https://tensorflow.rstudio.com/guide/tfdatasets/feature_columns/"&gt;feature columns&lt;/a&gt; - a nice new addition to TensorFlow that allows for convenient integration of e.g. feature normalization (more about this in the next section).&lt;/p&gt;
&lt;aside&gt;
The &lt;a href="https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_regression/"&gt;regression tutorial&lt;/a&gt; now uses feature columns for convenient data preprocessing.
&lt;/aside&gt;
&lt;p&gt;This introduction to regression is complemented by a &lt;a href="https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_classification/"&gt;tutorial on multi-class classification&lt;/a&gt; using “Fashion MNIST”. It is equally suited for a first encounter with Keras.&lt;/p&gt;
&lt;p&gt;A third tutorial in this section is dedicated to &lt;a href="https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_text_classification/"&gt;text classification&lt;/a&gt;. Here too, there is a hidden gem in the current version that makes text preprocessing a lot easier: &lt;code&gt;layer_text_vectorization&lt;/code&gt;, one of the brand new &lt;a href="https://github.com/keras-team/governance/blob/master/rfcs/20190502-preprocessing-layers.md"&gt;Keras preprocessing layers&lt;/a&gt;.&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; If you’ve used Keras for NLP before: No more messing with &lt;code&gt;text_tokenizer&lt;/code&gt;!&lt;/p&gt;
&lt;aside&gt;
Check out the new text vectorization layer in the &lt;a href="https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_text_classification/"&gt;text classification tutorial&lt;/a&gt;.
&lt;/aside&gt;
&lt;p&gt;These tutorials are nice introductions explaining code as well as concepts. What if you’re familiar with the basic procedure and just need a quick reminder (or: something to quickly copy-paste from)? The ideal document to consult for those purposes is the &lt;a href="https://tensorflow.rstudio.com/tutorials/beginners/"&gt;Overview&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now – knowledge how to build models is fine, but as in data science overall, there is no modeling without data.&lt;/p&gt;
&lt;h3 id="data-ingestion-and-preprocessing"&gt;Data ingestion and preprocessing&lt;/h3&gt;
&lt;p&gt;Two detailed, end-to-end tutorials show how to load &lt;a href="https://tensorflow.rstudio.com/tutorials/beginners/load/load_csv/"&gt;csv data&lt;/a&gt; and &lt;a href="https://tensorflow.rstudio.com/tutorials/beginners/load/load_image/"&gt;images&lt;/a&gt;, respectively.&lt;/p&gt;
&lt;p&gt;In current Keras, two mechanisms are central to data preparation. One is the use of &lt;a href="https://tensorflow.rstudio.com/guide/tfdatasets/introduction/"&gt;tfdatasets pipelines&lt;/a&gt;. &lt;code&gt;tfdatasets&lt;/code&gt; lets you load data in a streaming fashion (batch-by-batch), optionally applying transformations as you go. The other handy device here is &lt;a href="https://tensorflow.rstudio.com/guide/tfdatasets/feature_spec/"&gt;feature specs&lt;/a&gt; and&lt;a href="https://tensorflow.rstudio.com/guide/tfdatasets/feature_columns/"&gt;feature columns&lt;/a&gt;. Together with a matching Keras layer, these allow for transforming the input data without having to think about what the new format will mean to Keras.&lt;/p&gt;
&lt;p&gt;While there are other types of data not discussed in the docs, the principles – pre-processing pipelines and feature extraction – generalize.&lt;/p&gt;
&lt;h3 id="model-saving"&gt;Model saving&lt;/h3&gt;
&lt;p&gt;The best-performing model is of little use if ephemeral. Straightforward ways of saving Keras models are explained in a dedicated &lt;a href="https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_save_and_restore/"&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;aside&gt;
Advanced users: Additional options exist; see the &lt;a href="https://tensorflow.rstudio.com/guide/saving/checkpoints/"&gt;tutorial on checkpoints&lt;/a&gt;.
&lt;/aside&gt;
&lt;p&gt;And unless one’s just tinkering around, the question will often be: How can I deploy my model? There is a complete new section on &lt;a href="https://tensorflow.rstudio.com/deploy/"&gt;deployment&lt;/a&gt;, featuring options like &lt;code&gt;plumber&lt;/code&gt;, Shiny, TensorFlow Serving and RStudio Connect.&lt;/p&gt;
&lt;aside&gt;
Check out the new section on &lt;a href="https://tensorflow.rstudio.com/deploy/"&gt;deployment options&lt;/a&gt;.
&lt;/aside&gt;
&lt;p&gt;After this workflow-oriented run-through, let’s see about different types of data you might want to model.&lt;/p&gt;
&lt;h2 id="neural-networks-for-different-kinds-of-data"&gt;Neural networks for different kinds of data&lt;/h2&gt;
&lt;p&gt;No introduction to deep learning is complete without image classification. The “Fashion MNIST” classification tutorial mentioned in the beginning is a good introduction, but it uses a fully connected neural network to make it easy to remain focused on the overall approach. Standard models for image recognition, however, are commonly based on a convolutional architecture. &lt;a href="https://tensorflow.rstudio.com/tutorials/advanced/images/cnn/"&gt;Here&lt;/a&gt; is a nice introductory tutorial.&lt;/p&gt;
&lt;p&gt;For text data, the concept of &lt;em&gt;embeddings&lt;/em&gt; – distributed representations endowed with a measure of similarity – is central. As in the aforementioned text classification tutorial, embeddings can be learned using the respective Keras layer (&lt;code&gt;layer_embedding&lt;/code&gt;); in fact, the more idiosyncratic the dataset, the more recommendable this approach. Often though, it makes a lot of sense to use &lt;em&gt;pre-trained embeddings&lt;/em&gt;, obtained from large language models trained on enormous amounts of data. With TensorFlow Hub, discussed in more detail in the last section, pre-trained embeddings can be made use of simply by integrating an adequate &lt;em&gt;hub layer&lt;/em&gt;, as shown in &lt;a href="https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_text_classification_with_tfhub/"&gt;one of the Hub tutorials&lt;/a&gt;.&lt;/p&gt;
&lt;aside&gt;
Models from TF Hub can now conveniently be integrated into a model as Keras layers.
&lt;/aside&gt;
&lt;p&gt;As opposed to images and text, “normal”, a.k.a. &lt;em&gt;tabular&lt;/em&gt;, a.k.a. &lt;em&gt;structured&lt;/em&gt; data often seems like less of a candidate for deep learning. Historically, the mix of data types – numeric, binary, categorical –, together with different handling in the network (“leave alone” or embed) used to require a fair amount of manual fiddling. In contrast, the &lt;a href="https://tensorflow.rstudio.com/tutorials/advanced/structured/classify/"&gt;Structured data tutorial&lt;/a&gt; shows the, quote-unquote, modern way, again using feature columns and feature specs. The consequence: If you’re not sure that in the area of tabular data, deep learning will lead to improved performance – if it’s as easy as that, why not give it a try?&lt;/p&gt;
&lt;aside&gt;
If you’re working with structured data, definitely check out the &lt;a href="https://tensorflow.rstudio.com/guide/tfdatasets/feature_spec/"&gt;feature spec way to do it&lt;/a&gt;.
&lt;/aside&gt;
&lt;p&gt;Before rounding up with a special on TensorFlow Hub, let’s quickly see where to get more information on immediate and background-level technical questions.&lt;/p&gt;
&lt;h2 id="guides-topic-related-and-background-information"&gt;Guides: topic-related and background information&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://tensorflow.rstudio.com/guide/"&gt;Guide section&lt;/a&gt; has lots of additional information, covering specific questions that will come up when coding Keras models&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How can I define a &lt;a href="https://tensorflow.rstudio.com/guide/keras/custom_layers/"&gt;custom layer&lt;/a&gt;?&lt;/li&gt;
&lt;li&gt;A &lt;a href="https://tensorflow.rstudio.com/guide/keras/custom_models/"&gt;custom model&lt;/a&gt;?&lt;/li&gt;
&lt;li&gt;What are &lt;a href="https://tensorflow.rstudio.com/guide/keras/training_callbacks/"&gt;training callbacks&lt;/a&gt;?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;as well as background knowledge and terminology: What are &lt;a href="https://tensorflow.rstudio.com/guide/tensorflow/tensors/"&gt;tensors&lt;/a&gt;, &lt;a href="https://tensorflow.rstudio.com/guide/tensorflow/variables/"&gt;&lt;code&gt;Variables&lt;/code&gt;&lt;/a&gt;, how does &lt;a href="https://tensorflow.rstudio.com/tutorials/advanced/customization/autodiff/"&gt;automatic differentiation&lt;/a&gt; work in TensorFlow?&lt;/p&gt;
&lt;p&gt;Like for the basics, above we pointed out a document called “Quickstart”, for advanced topics here too is a &lt;a href="https://tensorflow.rstudio.com/tutorials/advanced/"&gt;Quickstart&lt;/a&gt; that in one end-to-end example, shows how to define and train a custom model. One especially nice aspect is the use of &lt;a href="https://github.com/t-kalinowski/tfautograph"&gt;tfautograph&lt;/a&gt;, a package developed by T. Kalinowski that – among others – allows for concisely iterating over a dataset in a &lt;code&gt;for&lt;/code&gt; loop.&lt;/p&gt;
&lt;aside&gt;
Power users: Check out the custom training &lt;a href="https://tensorflow.rstudio.com/tutorials/advanced/"&gt;Quickstart&lt;/a&gt; featuring custom models, &lt;code&gt;GradientTape&lt;/code&gt;s and &lt;code&gt;tfautograph&lt;/code&gt;.
&lt;/aside&gt;
&lt;p&gt;Finally, let’s talk about TF Hub.&lt;/p&gt;
&lt;h2 id="a-special-highlight-hub-layers"&gt;A special highlight: Hub layers&lt;/h2&gt;
&lt;p&gt;One of the most interesting aspects of contemporary neural network architectures is the use of transfer learning. Not everyone has the data, or computing facilities, to train big networks on big data from scratch. Through transfer learning, existing pre-trained models can be used for similar (but not identical) applications and in similar (but not identical) domains.&lt;/p&gt;
&lt;p&gt;Depending on one’s requirements, building on an existing model could be more or less cumbersome. Some time ago, TensorFlow Hub was created as a mechanism to publicly share models, or &lt;em&gt;modules&lt;/em&gt;, that is, reusable building blocks that could be made use of by others. Until recently, there was no convenient way to incorporate these modules, though.&lt;/p&gt;
&lt;p&gt;Starting from TensorFlow 2.0, Hub modules can now seemlessly be integrated in Keras models, using &lt;code&gt;layer_hub&lt;/code&gt;. This is demonstrated in two tutorials, for &lt;a href="https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_text_classification_with_tfhub/"&gt;text&lt;/a&gt; and &lt;a href="https://tensorflow.rstudio.com/tutorials/advanced/images/transfer-learning-hub/"&gt;images&lt;/a&gt;, respectively. But really, these two documents are just starting points: Starting points into a journey of experimentation, with other modules, combination of modules, areas of applications…&lt;/p&gt;
&lt;aside&gt;
Don’t miss out on the new TensorFlow Hub layer available in Keras… potentially, an extremely powerful way to enhance your models.
&lt;/aside&gt;
&lt;p&gt;In sum, we hope you have fun with the “new” (TF 2.0) Keras and find the documentation useful. Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;In fact, it is so new that as of this writing, you will have to install the nightly build of TensorFlow – as well as &lt;code&gt;tensorflow&lt;/code&gt; from github – to use it.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">bd27a7916bb64b881fb8a8813910618d</distill:md5>
      <category>Packages/Releases</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-11-27-gettingstarted-2020</guid>
      <pubDate>Wed, 27 Nov 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-11-27-gettingstarted-2020/images/website.png" medium="image" type="image/png" width="1591" height="725"/>
    </item>
    <item>
      <title>Variational convnets with tfprobability</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-11-13-variational-convnet</link>
      <description>


&lt;p&gt;A bit more than a year ago, in his beautiful &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-07-17-activity-detection/"&gt;guest post&lt;/a&gt;, Nick Strayer showed how to classify a set of everyday activities using smartphone-recorded gyroscope and accelerometer data. Accuracy was very good, but Nick went on to inspect classification results more closely. Were there activities more prone to misclassification than others? And how about those erroneous results: Did the network report them with equal, or less confidence than those that were correct?&lt;/p&gt;
&lt;p&gt;Technically, when we speak of &lt;em&gt;confidence&lt;/em&gt; in that manner, we’re referring to the &lt;em&gt;score&lt;/em&gt; obtained for the “winning” class after softmax activation.&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; If that winning score is 0.9, we might say “the network is sure that’s a gentoo penguin”; if it’s 0.2, we’d instead conclude “to the network, neither option seemed fitting, but cheetah looked best”.&lt;/p&gt;
&lt;p&gt;This use of “confidence” is convincing, but it has nothing to do with confidence – or credibility, or prediction, what have you – intervals. What we’d really like to be able to do is put distributions over the network’s weights and make it &lt;em&gt;Bayesian&lt;/em&gt;. Using &lt;em&gt;tfprobability&lt;/em&gt;’s variational Keras-compatible layers, this is something we actually can do.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/"&gt;Adding uncertainty estimates to Keras models with tfprobability&lt;/a&gt; shows how to use a variational dense layer to obtain estimates of epistemic uncertainty. In this post, we modify the convnet used in Nick’s post to be variational throughout. Before we start, let’s quickly summarize the task.&lt;/p&gt;
&lt;h2 id="the-task"&gt;The task&lt;/h2&gt;
&lt;p&gt;To create the &lt;a href="http://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions"&gt;Smartphone-Based Recognition of Human Activities and Postural Transitions Data Set&lt;/a&gt; &lt;span class="citation"&gt;(Reyes-Ortiz et al. 2016)&lt;/span&gt;, the researchers had subjects walk, sit, stand, and transition from one of those activities to another. Meanwhile, two types of smartphone sensors were used to record motion data: &lt;em&gt;Accelerometers&lt;/em&gt; measure linear acceleration in three&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; dimensions, while &lt;em&gt;gyroscopes&lt;/em&gt; are used to track angular velocity around the coordinate axes. Here are the respective raw sensor data for six types of activities from Nick’s original post:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-11-13-variational-convnet/images/raw-data.png" alt="Source: https://blogs.rstudio.com/tensorflow/posts/2018-07-17-activity-detection/" width="1152" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-1)Source: &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-07-17-activity-detection/" class="uri"&gt;https://blogs.rstudio.com/tensorflow/posts/2018-07-17-activity-detection/&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Just like Nick, we’re going to zoom in on those six types of activity, and try to infer them from the sensor data. Some data wrangling is needed to get the dataset into a form we can work with; here we’ll build on Nick’s post, and effectively start from the data nicely pre-processed and split up into training and test sets:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;trainData %&amp;gt;% glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Observations: 289
Variables: 6
$ experiment    &amp;lt;int&amp;gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 17, 18, 19, 2…
$ userId        &amp;lt;int&amp;gt; 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 7, 7, 9, 9, 10, 10, 11…
$ activity      &amp;lt;int&amp;gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7…
$ data          &amp;lt;list&amp;gt; [&amp;lt;data.frame[160 x 6]&amp;gt;, &amp;lt;data.frame[206 x 6]&amp;gt;, &amp;lt;dat…
$ activityName  &amp;lt;fct&amp;gt; STAND_TO_SIT, STAND_TO_SIT, STAND_TO_SIT, STAND_TO_S…
$ observationId &amp;lt;int&amp;gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 17, 18, 19, 2…&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;testData %&amp;gt;% glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Observations: 69
Variables: 6
$ experiment    &amp;lt;int&amp;gt; 11, 12, 15, 16, 32, 33, 42, 43, 52, 53, 56, 57, 11, …
$ userId        &amp;lt;int&amp;gt; 6, 6, 8, 8, 16, 16, 21, 21, 26, 26, 28, 28, 6, 6, 8,…
$ activity      &amp;lt;int&amp;gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8…
$ data          &amp;lt;list&amp;gt; [&amp;lt;data.frame[185 x 6]&amp;gt;, &amp;lt;data.frame[151 x 6]&amp;gt;, &amp;lt;dat…
$ activityName  &amp;lt;fct&amp;gt; STAND_TO_SIT, STAND_TO_SIT, STAND_TO_SIT, STAND_TO_S…
$ observationId &amp;lt;int&amp;gt; 11, 12, 15, 16, 31, 32, 41, 42, 51, 52, 55, 56, 71, …&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code required to arrive at this stage (copied from Nick’s post) may be found in the appendix at the bottom of this page.&lt;/p&gt;
&lt;h2 id="training-pipeline"&gt;Training pipeline&lt;/h2&gt;
&lt;p&gt;The dataset in question is small enough to fit in memory – but yours might not be, so it can’t hurt to see some streaming in action. Besides, it’s probably safe to say that with TensorFlow 2.0, &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/introduction.html"&gt;tfdatasets&lt;/a&gt; pipelines are &lt;em&gt;the&lt;/em&gt; way to feed data to a model.&lt;/p&gt;
&lt;p&gt;Once the code listed in the appendix has run, the sensor data is to be found in &lt;code&gt;trainData$data&lt;/code&gt;, a list column containing &lt;code&gt;data.frame&lt;/code&gt;s where each row corresponds to a point in time and each column holds one of the measurements. However, not all time series (recordings) are of the same length; we thus follow the original post to pad all series to length &lt;code&gt;pad_size&lt;/code&gt; (= 338). The expected shape of training batches will then be &lt;code&gt;(batch_size, pad_size, 6)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We initially create our training dataset:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_x &amp;lt;- train_data$data %&amp;gt;% 
  map(as.matrix) %&amp;gt;%
  pad_sequences(maxlen = pad_size, dtype = &amp;quot;float32&amp;quot;) %&amp;gt;%
  tensor_slices_dataset() 

train_y &amp;lt;- train_data$activity %&amp;gt;% 
  one_hot_classes() %&amp;gt;% 
  tensor_slices_dataset()

train_dataset &amp;lt;- zip_datasets(train_x, train_y)
train_dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;ZipDataset shapes: ((338, 6), (6,)), types: (tf.float64, tf.float64)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then shuffle and batch it:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;n_train &amp;lt;- nrow(train_data)
# the highest possible batch size for this dataset
# chosen because it yielded the best performance
# alternatively, experiment with e.g. different learning rates, ...
batch_size &amp;lt;- n_train

train_dataset &amp;lt;- train_dataset %&amp;gt;% 
  dataset_shuffle(n_train) %&amp;gt;%
  dataset_batch(batch_size)
train_dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;BatchDataset shapes: ((None, 338, 6), (None, 6)), types: (tf.float64, tf.float64)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Same for the test data.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;test_x &amp;lt;- test_data$data %&amp;gt;% 
  map(as.matrix) %&amp;gt;%
  pad_sequences(maxlen = pad_size, dtype = &amp;quot;float32&amp;quot;) %&amp;gt;%
  tensor_slices_dataset() 

test_y &amp;lt;- test_data$activity %&amp;gt;% 
  one_hot_classes() %&amp;gt;% 
  tensor_slices_dataset()

n_test &amp;lt;- nrow(test_data)
test_dataset &amp;lt;- zip_datasets(test_x, test_y) %&amp;gt;%
  dataset_batch(n_test)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using &lt;code&gt;tfdatasets&lt;/code&gt; does not mean we cannot run a quick sanity check on our data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;first &amp;lt;- test_dataset %&amp;gt;% 
  reticulate::as_iterator() %&amp;gt;% 
  # get first batch (= whole test set, in our case)
  reticulate::iter_next() %&amp;gt;%
  # predictors only
  .[[1]] %&amp;gt;% 
  # first item in batch
  .[1,,]
first&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor(
[[ 0.          0.          0.          0.          0.          0.        ]
 [ 0.          0.          0.          0.          0.          0.        ]
 [ 0.          0.          0.          0.          0.          0.        ]
 ...
 [ 1.00416672  0.2375      0.12916666 -0.40225476 -0.20463985 -0.14782938]
 [ 1.04166663  0.26944447  0.12777779 -0.26755899 -0.02779437 -0.1441642 ]
 [ 1.0250001   0.27083334  0.15277778 -0.19639318  0.35094208 -0.16249016]],
 shape=(338, 6), dtype=float64)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s build the network.&lt;/p&gt;
&lt;h2 id="a-variational-convnet"&gt;A variational convnet&lt;/h2&gt;
&lt;p&gt;We build on the straightforward convolutional architecture from Nick’s post, just making minor modifications to kernel sizes and numbers of filters. We also throw out all dropout layers; no additional regularization is needed on top of the priors applied to the weights.&lt;/p&gt;
&lt;p&gt;Note the following about the “Bayesified” network.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Each layer is variational in nature, the convolutional ones (&lt;a href="https://rstudio.github.io/tfprobability/reference/layer_conv_1d_flipout.html"&gt;layer_conv_1d_flipout&lt;/a&gt;) as well as the dense layers (&lt;a href="https://rstudio.github.io/tfprobability/reference/layer_dense_flipout.html"&gt;layer_dense_flipout&lt;/a&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;With variational layers, we can specify the prior weight distribution as well as the form of the posterior; here the defaults are used, resulting in a standard normal prior and a default mean-field posterior.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Likewise, the user may influence the divergence function used to assess the mismatch between prior and posterior; in this case, we actually take some action: We scale the (default) KL divergence by the number of samples in the training set.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;One last thing to note is the output layer. It is a distribution layer, that is, a layer wrapping a distribution – where wrapping means: Training the network is business as usual, but predictions are &lt;em&gt;distributions&lt;/em&gt;, one for each data point.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tfprobability)

num_classes &amp;lt;- 6

# scale the KL divergence by number of training examples
n &amp;lt;- n_train %&amp;gt;% tf$cast(tf$float32)
kl_div &amp;lt;- function(q, p, unused)
  tfd_kl_divergence(q, p) / n

model &amp;lt;- keras_model_sequential()
model %&amp;gt;% 
  layer_conv_1d_flipout(
    filters = 12,
    kernel_size = 3, 
    activation = &amp;quot;relu&amp;quot;,
    kernel_divergence_fn = kl_div
  ) %&amp;gt;%
  layer_conv_1d_flipout(
    filters = 24,
    kernel_size = 5, 
    activation = &amp;quot;relu&amp;quot;,
    kernel_divergence_fn = kl_div
  ) %&amp;gt;%
  layer_conv_1d_flipout(
    filters = 48,
    kernel_size = 7, 
    activation = &amp;quot;relu&amp;quot;,
    kernel_divergence_fn = kl_div
  ) %&amp;gt;%
  layer_global_average_pooling_1d() %&amp;gt;% 
  layer_dense_flipout(
    units = 48,
    activation = &amp;quot;relu&amp;quot;,
    kernel_divergence_fn = kl_div
  ) %&amp;gt;% 
  layer_dense_flipout(
    num_classes, 
    kernel_divergence_fn = kl_div,
    name = &amp;quot;dense_output&amp;quot;
  ) %&amp;gt;%
  layer_one_hot_categorical(event_size = num_classes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We tell the network to minimize the negative log likelihood.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;nll &amp;lt;- function(y, model) - (model %&amp;gt;% tfd_log_prob(y))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will become part of the loss. The way we set up this example, this is not its most substantial part though. Here, what dominates the loss is the sum of the KL divergences, added (automatically) to &lt;code&gt;model$losses&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In a setup like this, it’s interesting to monitor both parts of the loss separately. We can do this by means of two metrics:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# the KL part of the loss
kl_part &amp;lt;-  function(y_true, y_pred) {
    kl &amp;lt;- tf$reduce_sum(model$losses)
    kl
}

# the NLL part
nll_part &amp;lt;- function(y_true, y_pred) {
    cat_dist &amp;lt;- tfd_one_hot_categorical(logits = y_pred)
    nll &amp;lt;- - (cat_dist %&amp;gt;% tfd_log_prob(y_true) %&amp;gt;% tf$reduce_mean())
    nll
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We train somewhat longer than Nick did in the original post, allowing for early stopping though.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% compile(
  optimizer = &amp;quot;rmsprop&amp;quot;,
  loss = nll,
  metrics = c(&amp;quot;accuracy&amp;quot;, 
              custom_metric(&amp;quot;kl_part&amp;quot;, kl_part),
              custom_metric(&amp;quot;nll_part&amp;quot;, nll_part)),
  experimental_run_tf_function = FALSE
)

train_history &amp;lt;- model %&amp;gt;% fit(
  train_dataset,
  epochs = 1000,
  validation_data = test_dataset,
  callbacks = list(
    callback_early_stopping(patience = 10)
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While the overall loss declines linearly (and probably would for many more epochs), this is not the case for classification accuracy or the NLL part of the loss:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-11-13-variational-convnet/images/history.png" width="600" /&gt;&lt;/p&gt;
&lt;p&gt;Final accuracy is not as high as in the non-variational setup, though still not bad for a six-class problem. We see that without any additional regularization, there is very little overfitting to the training data.&lt;/p&gt;
&lt;p&gt;Now how do we obtain predictions from this model?&lt;/p&gt;
&lt;h2 id="probabilistic-predictions"&gt;Probabilistic predictions&lt;/h2&gt;
&lt;p&gt;Though we won’t go into this here, it’s good to know that we access more than just the output distributions; through their &lt;code&gt;kernel_posterior&lt;/code&gt; attribute, we can access the hidden layers’ posterior weight distributions as well.&lt;/p&gt;
&lt;p&gt;Given the small size of the test set, we compute all predictions at once. The predictions are now categorical distributions, one for each sample in the batch:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;test_data_all &amp;lt;- dataset_collect(test_dataset) %&amp;gt;% { .[[1]][[1]]}

one_shot_preds &amp;lt;- model(test_data_all) 

one_shot_preds&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tfp.distributions.OneHotCategorical(
 &amp;quot;sequential_one_hot_categorical_OneHotCategorical_OneHotCategorical&amp;quot;,
 batch_shape=[69], event_shape=[6], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We prefixed those predictions with &lt;code&gt;one_shot&lt;/code&gt; to indicate their noisy nature: These are predictions obtained on a single pass through the network, all layer weights being sampled from their respective posteriors.&lt;/p&gt;
&lt;p&gt;From the predicted distributions, we calculate mean and standard deviation &lt;em&gt;per (test) sample&lt;/em&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;one_shot_means &amp;lt;- tfd_mean(one_shot_preds) %&amp;gt;% 
  as.matrix() %&amp;gt;%
  as_tibble() %&amp;gt;% 
  mutate(obs = 1:n()) %&amp;gt;% 
  gather(class, mean, -obs) 

one_shot_sds &amp;lt;- tfd_stddev(one_shot_preds) %&amp;gt;% 
  as.matrix() %&amp;gt;%
  as_tibble() %&amp;gt;% 
  mutate(obs = 1:n()) %&amp;gt;% 
  gather(class, sd, -obs) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The standard deviations thus obtained could be said to reflect the overall &lt;em&gt;predictive uncertainty&lt;/em&gt;. We can estimate another kind of uncertainty, called &lt;em&gt;epistemic&lt;/em&gt;,&lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; by making a number of passes through the network and then, calculating – again, per test sample – the standard deviations of the predicted means.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;mc_preds &amp;lt;- purrr::map(1:100, function(x) {
  preds &amp;lt;- model(test_data_all)
  tfd_mean(preds) %&amp;gt;% as.matrix()
})

mc_sds &amp;lt;- abind::abind(mc_preds, along = 3) %&amp;gt;% 
  apply(c(1,2), sd) %&amp;gt;% 
  as_tibble() %&amp;gt;%
  mutate(obs = 1:n()) %&amp;gt;% 
  gather(class, mc_sd, -obs) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Putting it all together, we have&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pred_data &amp;lt;- one_shot_means %&amp;gt;%
  inner_join(one_shot_sds, by = c(&amp;quot;obs&amp;quot;, &amp;quot;class&amp;quot;)) %&amp;gt;% 
  inner_join(mc_sds, by = c(&amp;quot;obs&amp;quot;, &amp;quot;class&amp;quot;)) %&amp;gt;% 
  right_join(one_hot_to_label, by = &amp;quot;class&amp;quot;) %&amp;gt;% 
  arrange(obs)

pred_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 414 x 6
     obs class       mean      sd    mc_sd label       
   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;       
 1     1 V1    0.945      0.227   0.0743   STAND_TO_SIT
 2     1 V2    0.0534     0.225   0.0675   SIT_TO_STAND
 3     1 V3    0.00114    0.0338  0.0346   SIT_TO_LIE  
 4     1 V4    0.00000238 0.00154 0.000336 LIE_TO_SIT  
 5     1 V5    0.0000132  0.00363 0.00164  STAND_TO_LIE
 6     1 V6    0.0000305  0.00553 0.00398  LIE_TO_STAND
 7     2 V1    0.993      0.0813  0.149    STAND_TO_SIT
 8     2 V2    0.00153    0.0390  0.102    SIT_TO_STAND
 9     2 V3    0.00476    0.0688  0.108    SIT_TO_LIE  
10     2 V4    0.00000172 0.00131 0.000613 LIE_TO_SIT  
# … with 404 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Comparing predictions to the ground truth:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;eval_table &amp;lt;- pred_data %&amp;gt;% 
  group_by(obs) %&amp;gt;% 
  summarise(
    maxprob = max(mean),
    maxprob_sd = sd[mean == maxprob],
    maxprob_mc_sd = mc_sd[mean == maxprob],
    predicted = label[mean == maxprob]
  ) %&amp;gt;% 
  mutate(
    truth = test_data$activityName,
    correct = truth == predicted
  ) 

eval_table %&amp;gt;% print(n = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 69 x 7
     obs maxprob maxprob_sd maxprob_mc_sd predicted    truth        correct
   &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;        &amp;lt;fct&amp;gt;        &amp;lt;lgl&amp;gt;  
 1     1   0.945     0.227         0.0743 STAND_TO_SIT STAND_TO_SIT TRUE   
 2     2   0.993     0.0813        0.149  STAND_TO_SIT STAND_TO_SIT TRUE   
 3     3   0.733     0.443         0.131  STAND_TO_SIT STAND_TO_SIT TRUE   
 4     4   0.796     0.403         0.138  STAND_TO_SIT STAND_TO_SIT TRUE   
 5     5   0.843     0.364         0.358  SIT_TO_STAND STAND_TO_SIT FALSE  
 6     6   0.816     0.387         0.176  SIT_TO_STAND STAND_TO_SIT FALSE  
 7     7   0.600     0.490         0.370  STAND_TO_SIT STAND_TO_SIT TRUE   
 8     8   0.941     0.236         0.0851 STAND_TO_SIT STAND_TO_SIT TRUE   
 9     9   0.853     0.355         0.274  SIT_TO_STAND STAND_TO_SIT FALSE  
10    10   0.961     0.195         0.195  STAND_TO_SIT STAND_TO_SIT TRUE   
11    11   0.918     0.275         0.168  STAND_TO_SIT STAND_TO_SIT TRUE   
12    12   0.957     0.203         0.150  STAND_TO_SIT STAND_TO_SIT TRUE   
13    13   0.987     0.114         0.188  SIT_TO_STAND SIT_TO_STAND TRUE   
14    14   0.974     0.160         0.248  SIT_TO_STAND SIT_TO_STAND TRUE   
15    15   0.996     0.0657        0.0534 SIT_TO_STAND SIT_TO_STAND TRUE   
16    16   0.886     0.318         0.0868 SIT_TO_STAND SIT_TO_STAND TRUE   
17    17   0.773     0.419         0.173  SIT_TO_STAND SIT_TO_STAND TRUE   
18    18   0.998     0.0444        0.222  SIT_TO_STAND SIT_TO_STAND TRUE   
19    19   0.885     0.319         0.161  SIT_TO_STAND SIT_TO_STAND TRUE   
20    20   0.930     0.255         0.271  SIT_TO_STAND SIT_TO_STAND TRUE   
# … with 49 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Are standard deviations higher for misclassifications?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;eval_table %&amp;gt;% 
  group_by(truth, predicted) %&amp;gt;% 
  summarise(avg_mean = mean(maxprob),
            avg_sd = mean(maxprob_sd),
            avg_mc_sd = mean(maxprob_mc_sd)) %&amp;gt;% 
  mutate(correct = truth == predicted) %&amp;gt;%
  arrange(avg_mc_sd) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 2 x 5
  correct count avg_mean avg_sd avg_mc_sd
  &amp;lt;lgl&amp;gt;   &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
1 FALSE      19    0.775  0.380     0.237
2 TRUE       50    0.879  0.264     0.183&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;They are; though perhaps not to the extent we might desire.&lt;/p&gt;
&lt;p&gt;With just six classes, we can also inspect standard deviations on the individual prediction-target pairings level.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;eval_table %&amp;gt;% 
  group_by(truth, predicted) %&amp;gt;% 
  summarise(cnt = n(),
            avg_mean = mean(maxprob),
            avg_sd = mean(maxprob_sd),
            avg_mc_sd = mean(maxprob_mc_sd)) %&amp;gt;% 
  mutate(correct = truth == predicted) %&amp;gt;%
  arrange(desc(cnt), avg_mc_sd) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 14 x 7
# Groups:   truth [6]
   truth        predicted      cnt avg_mean avg_sd avg_mc_sd correct
   &amp;lt;fct&amp;gt;        &amp;lt;fct&amp;gt;        &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;lgl&amp;gt;  
 1 SIT_TO_STAND SIT_TO_STAND    12    0.935  0.205    0.184  TRUE   
 2 STAND_TO_SIT STAND_TO_SIT     9    0.871  0.284    0.162  TRUE   
 3 LIE_TO_SIT   LIE_TO_SIT       9    0.765  0.377    0.216  TRUE   
 4 SIT_TO_LIE   SIT_TO_LIE       8    0.908  0.254    0.187  TRUE   
 5 STAND_TO_LIE STAND_TO_LIE     7    0.956  0.144    0.132  TRUE   
 6 LIE_TO_STAND LIE_TO_STAND     5    0.809  0.353    0.227  TRUE   
 7 SIT_TO_LIE   STAND_TO_LIE     4    0.685  0.436    0.233  FALSE  
 8 LIE_TO_STAND SIT_TO_STAND     4    0.909  0.271    0.282  FALSE  
 9 STAND_TO_LIE SIT_TO_LIE       3    0.852  0.337    0.238  FALSE  
10 STAND_TO_SIT SIT_TO_STAND     3    0.837  0.368    0.269  FALSE  
11 LIE_TO_STAND LIE_TO_SIT       2    0.689  0.454    0.233  FALSE  
12 LIE_TO_SIT   STAND_TO_SIT     1    0.548  0.498    0.0805 FALSE  
13 SIT_TO_STAND LIE_TO_STAND     1    0.530  0.499    0.134  FALSE  
14 LIE_TO_SIT   LIE_TO_STAND     1    0.824  0.381    0.231  FALSE  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, we see higher standard deviations for wrong predictions, but not to a high degree.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We’ve shown how to build, train, and obtain predictions from a fully variational convnet. Evidently, there is room for experimentation: Alternative layer implementations exist&lt;a href="#fn4" class="footnote-ref" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;; a different prior could be specified; the divergence could be calculated differently; and the usual neural network hyperparameter tuning options apply.&lt;/p&gt;
&lt;p&gt;Then, there’s the question of consequences (or: decision making). What is going to happen in high-uncertainty cases, what even is a high-uncertainty case? Naturally, questions like these are out-of-scope for this post, yet of essential importance in real-world applications. Thanks for reading!&lt;/p&gt;
&lt;h3 id="appendix"&gt;Appendix&lt;/h3&gt;
&lt;p&gt;To be executed before running this post’s code. Copied from &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-07-17-activity-detection/"&gt;Classifying physical activity from smartphone data&lt;/a&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)     
library(tidyverse) 

activity_labels &amp;lt;- read.table(&amp;quot;data/activity_labels.txt&amp;quot;, 
                             col.names = c(&amp;quot;number&amp;quot;, &amp;quot;label&amp;quot;)) 

one_hot_to_label &amp;lt;- activity_labels %&amp;gt;% 
  mutate(number = number - 7) %&amp;gt;% 
  filter(number &amp;gt;= 0) %&amp;gt;% 
  mutate(class = paste0(&amp;quot;V&amp;quot;,number + 1)) %&amp;gt;% 
  select(-number)

labels &amp;lt;- read.table(
  &amp;quot;data/RawData/labels.txt&amp;quot;,
  col.names = c(&amp;quot;experiment&amp;quot;, &amp;quot;userId&amp;quot;, &amp;quot;activity&amp;quot;, &amp;quot;startPos&amp;quot;, &amp;quot;endPos&amp;quot;)
)

dataFiles &amp;lt;- list.files(&amp;quot;data/RawData&amp;quot;)
dataFiles %&amp;gt;% head()

fileInfo &amp;lt;- data_frame(
  filePath = dataFiles
) %&amp;gt;%
  filter(filePath != &amp;quot;labels.txt&amp;quot;) %&amp;gt;%
  separate(filePath, sep = &amp;#39;_&amp;#39;,
           into = c(&amp;quot;type&amp;quot;, &amp;quot;experiment&amp;quot;, &amp;quot;userId&amp;quot;),
           remove = FALSE) %&amp;gt;%
  mutate(
    experiment = str_remove(experiment, &amp;quot;exp&amp;quot;),
    userId = str_remove_all(userId, &amp;quot;user|\\.txt&amp;quot;)
  ) %&amp;gt;%
  spread(type, filePath)

# Read contents of single file to a dataframe with accelerometer and gyro data.
readInData &amp;lt;- function(experiment, userId){
  genFilePath = function(type) {
    paste0(&amp;quot;data/RawData/&amp;quot;, type, &amp;quot;_exp&amp;quot;,experiment, &amp;quot;_user&amp;quot;, userId, &amp;quot;.txt&amp;quot;)
  }
  bind_cols(
    read.table(genFilePath(&amp;quot;acc&amp;quot;), col.names = c(&amp;quot;a_x&amp;quot;, &amp;quot;a_y&amp;quot;, &amp;quot;a_z&amp;quot;)),
    read.table(genFilePath(&amp;quot;gyro&amp;quot;), col.names = c(&amp;quot;g_x&amp;quot;, &amp;quot;g_y&amp;quot;, &amp;quot;g_z&amp;quot;))
  )
}

# Function to read a given file and get the observations contained along
# with their classes.
loadFileData &amp;lt;- function(curExperiment, curUserId) {

  # load sensor data from file into dataframe
  allData &amp;lt;- readInData(curExperiment, curUserId)
  extractObservation &amp;lt;- function(startPos, endPos){
    allData[startPos:endPos,]
  }

  # get observation locations in this file from labels dataframe
  dataLabels &amp;lt;- labels %&amp;gt;%
    filter(userId == as.integer(curUserId),
           experiment == as.integer(curExperiment))

  # extract observations as dataframes and save as a column in dataframe.
  dataLabels %&amp;gt;%
    mutate(
      data = map2(startPos, endPos, extractObservation)
    ) %&amp;gt;%
    select(-startPos, -endPos)
}

# scan through all experiment and userId combos and gather data into a dataframe.
allObservations &amp;lt;- map2_df(fileInfo$experiment, fileInfo$userId, loadFileData) %&amp;gt;%
  right_join(activityLabels, by = c(&amp;quot;activity&amp;quot; = &amp;quot;number&amp;quot;)) %&amp;gt;%
  rename(activityName = label)

write_rds(allObservations, &amp;quot;allObservations.rds&amp;quot;)

allObservations &amp;lt;- readRDS(&amp;quot;allObservations.rds&amp;quot;)

desiredActivities &amp;lt;- c(
  &amp;quot;STAND_TO_SIT&amp;quot;, &amp;quot;SIT_TO_STAND&amp;quot;, &amp;quot;SIT_TO_LIE&amp;quot;, 
  &amp;quot;LIE_TO_SIT&amp;quot;, &amp;quot;STAND_TO_LIE&amp;quot;, &amp;quot;LIE_TO_STAND&amp;quot;  
)

filteredObservations &amp;lt;- allObservations %&amp;gt;% 
  filter(activityName %in% desiredActivities) %&amp;gt;% 
  mutate(observationId = 1:n())

# get all users
userIds &amp;lt;- allObservations$userId %&amp;gt;% unique()

# randomly choose 24 (80% of 30 individuals) for training
set.seed(42) # seed for reproducibility
trainIds &amp;lt;- sample(userIds, size = 24)

# set the rest of the users to the testing set
testIds &amp;lt;- setdiff(userIds,trainIds)

# filter data. 
# note S.K.: renamed to train_data for consistency with 
# variable naming used in this post
train_data &amp;lt;- filteredObservations %&amp;gt;% 
  filter(userId %in% trainIds)

# note S.K.: renamed to test_data for consistency with 
# variable naming used in this post
test_data &amp;lt;- filteredObservations %&amp;gt;% 
  filter(userId %in% testIds)

# note S.K.: renamed to pad_size for consistency with 
# variable naming used in this post
pad_size &amp;lt;- trainData$data %&amp;gt;% 
  map_int(nrow) %&amp;gt;% 
  quantile(p = 0.98) %&amp;gt;% 
  ceiling()

# note S.K.: renamed to one_hot_classes for consistency with 
# variable naming used in this post
one_hot_classes &amp;lt;- . %&amp;gt;% 
  {. - 7} %&amp;gt;%        # bring integers down to 0-6 from 7-12
  to_categorical()   # One-hot encode&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-Reyes-Ortiz"&gt;
&lt;p&gt;Reyes-Ortiz, Jorge-L., Luca Oneto, Albert Samà, Xavier Parra, and Davide Anguita. 2016. “Transition-Aware Human Activity Recognition Using Smartphones.” &lt;em&gt;Neurocomput.&lt;/em&gt; 171 (C): 754–67. &lt;a href="https://doi.org/10.1016/j.neucom.2015.07.085"&gt;https://doi.org/10.1016/j.neucom.2015.07.085&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;see &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-10-11-activations-intro/"&gt;Winner takes all: A look at activations and cost functions&lt;/a&gt;&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;or two, depending on the application&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;see &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/"&gt;Adding uncertainty estimates to Keras models with tfprobability&lt;/a&gt;&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;e.g., &lt;a href="https://rstudio.github.io/tfprobability/reference/layer_conv_1d_reparameterization.html"&gt;layer_conv_1d_reparameterization&lt;/a&gt;, &lt;a href="https://rstudio.github.io/tfprobability/reference/layer_dense_local_reparameterization.html"&gt;layer_dense_local_reparameterization&lt;/a&gt;&lt;a href="#fnref4" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">9710ffff530835d854913969e1b7359e</distill:md5>
      <category>Probabilistic ML/DL</category>
      <category>Time Series</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-11-13-variational-convnet</guid>
      <pubDate>Wed, 13 Nov 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-11-13-variational-convnet/images/bbb.png" medium="image" type="image/png" width="796" height="378"/>
    </item>
    <item>
      <title>tfprobability 0.8 on CRAN: Now how can you use it?</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-11-07-tfp-cran</link>
      <description>


&lt;p&gt;About a week ago, &lt;code&gt;tfprobability&lt;/code&gt; 0.8 was accepted on CRAN. While we’ve been using this package quite frequently already on this blog, in this post we’d like to (re-)introduce it on a high level, especially addressing new users.&lt;/p&gt;
&lt;h2 id="tfprobability-what-is-it"&gt;tfprobability, what is it?&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;tfprobability&lt;/code&gt; is an R wrapper for &lt;a href="https://www.tensorflow.org/probability/"&gt;TensorFlow Probability&lt;/a&gt;, a Python library built on top of the &lt;a href="https://www.tensorflow.org/"&gt;TensorFlow&lt;/a&gt; framework. So now the question is, what is TensorFlow Probability?&lt;/p&gt;
&lt;p&gt;If – let’s call it “probabilistic programming” – is not something you do every day, an enumeration of features, or even a hierarchical listing of modules as given on the &lt;a href="https://www.tensorflow.org/probability/overview"&gt;TensorFlow Probability website&lt;/a&gt; might leave you a bit helpless, informative though it may be.&lt;/p&gt;
&lt;p&gt;Let’s start from use cases instead. We’ll look at three high-level example scenarios, before rounding up with a quick tour of more basic building blocks provided by TFP. (Short note aside: We’ll use &lt;em&gt;TFP&lt;/em&gt; as an acronym for the Python library as well as the R package, unless we’re referring to the R wrapper specifically, in which case we’ll say &lt;code&gt;tfprobability&lt;/code&gt;).&lt;/p&gt;
&lt;h2 id="use-case-1-extending-deep-learning"&gt;Use case 1: Extending deep learning&lt;/h2&gt;
&lt;p&gt;We start with the type of use case that might be the most interesting to the majority of our readers: extending deep learning.&lt;/p&gt;
&lt;h3 id="distribution-layers"&gt;Distribution layers&lt;/h3&gt;
&lt;p&gt;In deep learning, usually output layers are deterministic. True, in classification we are used to talking about “class probabilities”. Take the multi-class case: We may attribute to the network the conclusion, “with 80% probability this is a Bernese mountain dog” – but we can only do this because the last layer’s output has been squished, by a softmax activation, to values between &lt;span class="math inline"&gt;\(0\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(1\)&lt;/span&gt;. Nonetheless, the actual output is a tensor (a number).&lt;/p&gt;
&lt;p&gt;TFP, however, augments TensorFlow by means of &lt;em&gt;distribution layers&lt;/em&gt;: a hybrid species that can be used just like a normal TensorFlow/Keras layer but that, internally, contains the defining characteristics of some probability distribution.&lt;/p&gt;
&lt;p&gt;Concretely, for multi-class classification, we could use a categorical layer (&lt;a href="https://rstudio.github.io/tfprobability/reference/layer_one_hot_categorical.html"&gt;layer_one_hot_categorical&lt;/a&gt;), replacing something like&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;layer_dense(
  num_classes, 
  activation = &amp;quot;softmax&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;by&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;layer_one_hot_categorical(event_size = num_classes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model, thus modified, now outputs a distribution, not a tensor. However, it can still be trained passing “normal” target tensors. This is what was meant by &lt;em&gt;use like a normal layer&lt;/em&gt;, above: TFP will take the distribution, obtain a tensor from it &lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, and compare that to the target. The other side of the layer’s personality is seen when generating predictions: Calling the model on fresh data will result in a bunch of distributions, one for every data point. You then call &lt;a href="https://rstudio.github.io/tfprobability/reference/tfd_mean.html"&gt;tfd_mean&lt;/a&gt; to elicit actual predictions:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pred_dists &amp;lt;- model(x_test)
pred_means &amp;lt;- pred_dists %&amp;gt;% tfd_mean()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You may be wondering, what good is this? One way or the other, we’ll decide on picking the class with the highest probability, right?&lt;/p&gt;
&lt;p&gt;Right, but there are a number of interesting things you can do with these layers. We’ll quickly introduce three well-known ones here, but wouldn’t be surprised if we saw a lot more emerging in the near future.&lt;/p&gt;
&lt;h3 id="variational-autoencoders-the-elegant-way"&gt;Variational autoencoders, the elegant way&lt;/h3&gt;
&lt;p&gt;Variational autoencoders are a prime example of something that got way easier to code when TF-2 style custom models and custom training appeared (TF-2 &lt;em&gt;style&lt;/em&gt;, not TF-2, as these techniques actually became available more than a year before TF 2 was finally released). &lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Evolutionarily, the next step was to use TFP distributions &lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;, but back in the time some fiddling was required, as distributions could not yet alias as layers.&lt;/p&gt;
&lt;p&gt;Due to those hybrids though, we now can do something like this &lt;a href="#fn4" class="footnote-ref" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# encoder
encoder_model &amp;lt;- keras_model_sequential() %&amp;gt;%
  [...] %&amp;gt;%
  layer_multivariate_normal_tri_l(event_size = encoded_size) %&amp;gt;%
  layer_kl_divergence_add_loss([...])

# decoder
decoder_model &amp;lt;- keras_model_sequential() %&amp;gt;%
  [...] %&amp;gt;%
 layer_independent_bernoulli([...])

# complete VAE
vae_model &amp;lt;- keras_model(inputs = encoder_model$inputs,
                         outputs = decoder_model(encoder_model$outputs[1]))

# loss function
vae_loss &amp;lt;- function (x, rv_x) - (rv_x %&amp;gt;% tfd_log_prob(x))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both the encoder and the decoder are “just” sequential models, joined through the functional API(&lt;code&gt;keras_model&lt;/code&gt;). The loss is just the negative log-probability of the data given the model. So where is the other part of the (negative) ELBO, the KL divergence? It is implicit in the encoder’s output layer, &lt;a href="https://rstudio.github.io/tfprobability/reference/layer_kl_divergence_add_loss.html"&gt;layer_kl_divergence_add_loss&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Our two other examples involve quantifying uncertainty.&lt;/p&gt;
&lt;h3 id="learning-the-spread-in-the-data"&gt;Learning the spread in the data&lt;/h3&gt;
&lt;p&gt;If a model’s last layer wraps a distribution parameterized by location and scale, like the normal distribution, we can train the network to learn not just the mean, but also the spread in the data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_dense(units = 8, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dense(units = 2, activation = &amp;quot;linear&amp;quot;) %&amp;gt;%
  layer_distribution_lambda(function(x)
    tfd_normal(# use unit 1 of previous layer
               loc = x[, 1, drop = FALSE],
               # use unit 2 of previous layer
               scale = 1e-3 + tf$math$softplus(x[, 2, drop = FALSE])
               )
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In essence, this means the network’s predictions will reflect any existing heteroscedasticity in the data. Here is an example: Given simulated training data of shape&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-11-07-tfp-cran/images/simdata.png" alt="Simulated data (from https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/)" width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-6)Simulated data (from &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/" class="uri"&gt;https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/&lt;/a&gt;)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;the network’s predictions show the same spread:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-11-07-tfp-cran/images/g_aleatoric_relu_8.png" alt="Aleatoric uncertainty on simulated data (from https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/) " width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-7)Aleatoric uncertainty on simulated data (from &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/" class="uri"&gt;https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/&lt;/a&gt;)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Please see &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/"&gt;Adding uncertainty estimates to Keras models with tfprobability&lt;/a&gt; for a detailed explanation.&lt;/p&gt;
&lt;p&gt;Using a normal distribution layer as the output, we can capture irreducible variability in the data, also known as &lt;em&gt;aleatoric uncertainty&lt;/em&gt;. A different type of probabilistic layer allows to model what is called &lt;em&gt;epistemic uncertainty&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id="putting-distributions-over-network-weights"&gt;Putting distributions over network weights&lt;/h3&gt;
&lt;p&gt;Using &lt;em&gt;variational layers&lt;/em&gt;, we can make neural networks probabilistic. A simple example could look like so:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_dense_variational(
    units = 1,
    make_posterior_fn = posterior_mean_field,
    make_prior_fn = prior_trainable,
    kl_weight = 1 / n
  ) %&amp;gt;%
  layer_distribution_lambda(function(x)
    tfd_normal(loc = x, scale = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This defines a network with a single dense layer, containing a single neuron, that has a prior distribution put over its weights. The network will be trained to minimize the KL divergence between that prior and an approximate posterior weight distribution, as well as maximize the probability of the data under the posterior weights. (For details, please again see the aforementioned post.)&lt;/p&gt;
&lt;p&gt;As a consequence of this setup, each test run will now yield different predictions. For the above simulated data, we might get an ensemble of predictions, like so:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-11-07-tfp-cran/images/g_epistemic_linear_kl150.png" alt="Epistemic uncertainty on simulated data (from https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/) " width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-9)Epistemic uncertainty on simulated data (from &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/" class="uri"&gt;https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/&lt;/a&gt;)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Variational layers for non-dense layers exist, and we’ll see an example next week. Now let’s move on to the next type of use cases, from big data to small data, in a way.&lt;/p&gt;
&lt;h2 id="use-case-2-fitting-bayesian-models-with-monte-carlo-methods"&gt;Use case 2: Fitting Bayesian models with Monte Carlo methods&lt;/h2&gt;
&lt;p&gt;In sciences where data aren’t abound, Markov Chain Monte Carlo (MCMC) methods are common. We’ve shown some examples how to this with TFP (&lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-05-06-tadpoles-on-tensorflow/"&gt;Tadpoles on TensorFlow: Hierarchical partial pooling with tfprobability&lt;/a&gt;, &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-05-24-varying-slopes/"&gt;Hierarchical partial pooling, continued: Varying slopes models with TensorFlow Probability&lt;/a&gt;, &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-07-31-censored-data/"&gt;Modeling censored data with tfprobability&lt;/a&gt;, best read in this order), as well as tried to explain, in an accessible way, some of the background (&lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-10-03-intro-to-hmc/"&gt;On leapfrogs, crashing satellites, and going nuts: A very first conceptual introduction to Hamiltonian Monte Carlo&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;MCMC software may roughly be divided into two flavors: “low-level” and “high-level”. Low-level software, like &lt;a href="https://mc-stan.org/"&gt;Stan&lt;/a&gt; – or TFP, for that matter – requires you to write code in either some programming language, or in a DSL that is pretty close in syntax and semantics to an existing programming language. High-level tools, on the other hand, are DSLs that resemble the way you’d express a model using mathematical notation. (Put differently, the former read like C or Python; the latter read like LaTeX.)&lt;/p&gt;
&lt;p&gt;In general, low-level software tends to offer more flexibility, while high-level interfaces may be more convenient to use and easier to learn. To start with MCMC in TFP, we recommend checking out the first of the posts listed above, &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-05-06-tadpoles-on-tensorflow/"&gt;Tadpoles on TensorFlow: Hierarchical partial pooling with tfprobability&lt;/a&gt;. If you prefer a higher-level interface, you might be interested in &lt;a href="https://greta-stats.org/"&gt;greta&lt;/a&gt;, which is built on TFP.&lt;/p&gt;
&lt;p&gt;Our last use case is of a Bayesian nature as well.&lt;/p&gt;
&lt;h2 id="use-case-3-state-space-models"&gt;Use case 3: State space models&lt;/h2&gt;
&lt;p&gt;State space models are a perhaps lesser used, but highly conceptually attractive way of performing inference and prediction on signals evolving in time. &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-06-25-dynamic_linear_models_tfprobability/"&gt;Dynamic linear models with tfprobability&lt;/a&gt; is an introduction to dynamic linear models with TFP, showcasing two of their (many) great strengths: ease of performing dynamic regression and additive (de)composition.&lt;/p&gt;
&lt;p&gt;In dynamic regression, coefficients are allowed to vary over time. Here is an example from the above-mentioned post showing, for both a single predictor and the regression intercept, the filtered (in the sense of Kálmán filtering) estimates over time:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-11-07-tfp-cran/images/capm_filtered.png" alt="Filtering estimates from the Kálmán filter (from: https://blogs.rstudio.com/tensorflow/posts/2019-06-25-dynamic_linear_models_tfprobability/)." width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-10)Filtering estimates from the Kálmán filter (from: &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-06-25-dynamic_linear_models_tfprobability/" class="uri"&gt;https://blogs.rstudio.com/tensorflow/posts/2019-06-25-dynamic_linear_models_tfprobability/&lt;/a&gt;).
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;And here is the ubiquitous &lt;em&gt;AirPassengers&lt;/em&gt; dataset, decomposed into a trend and a seasonal component:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-11-07-tfp-cran/images/airp_components.png" alt="AirPassengers, decomposition into a linear trend and a seasonal component (from: https://blogs.rstudio.com/tensorflow/posts/2019-06-25-dynamic_linear_models_tfprobability/)." width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-11)AirPassengers, decomposition into a linear trend and a seasonal component (from: &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-06-25-dynamic_linear_models_tfprobability/" class="uri"&gt;https://blogs.rstudio.com/tensorflow/posts/2019-06-25-dynamic_linear_models_tfprobability/&lt;/a&gt;).
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;If this interests you, you might want to take a look at &lt;a href="https://rstudio.github.io/tfprobability/reference/index.html#section-structural-time-series-models"&gt;available state space models&lt;/a&gt;. Given how rapidly TFP is evolving, plus the model-inherent composability, we expect the number of options in this area to grow quite a bit.&lt;/p&gt;
&lt;p&gt;That is it for our tour of use cases. To wrap up, let’s talk about the basic building blocks of TFP.&lt;/p&gt;
&lt;h2 id="the-basics-distributions-and-bijectors"&gt;The basics: Distributions and bijectors&lt;/h2&gt;
&lt;p&gt;No probabilistic framework without probability distributions – that’s for sure. In release 0.8, TFP has &lt;a href="https://rstudio.github.io/tfprobability/reference/index.html#section-distributions"&gt;about 80 distributions&lt;/a&gt;. But what are bijectors?&lt;/p&gt;
&lt;p&gt;Bijectors are invertible, differentiable maps. &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-04-05-bijectors-flows/"&gt;Getting into the flow: Bijectors in TensorFlow Probability&lt;/a&gt; introduces the main ideas and shows how to chain such bijective transformations into a &lt;em&gt;flow&lt;/em&gt;. Bijectors are being used by TFP internally all the time, and as a user too you’ll likely encounter situations where you need them.&lt;/p&gt;
&lt;p&gt;One example is doing MCMC (for example, Hamiltonian Monte Carlo) with TFP. In your model, you might have a prior on a standard deviation. Standard deviations are positive, so you’d like to specify, for example, an exponential distribution for it, resulting in exclusively positive values. But Hamiltonian Monte Carlo has to run in unconstrained space to work. This is where a bijector comes in, mapping between the two spaces used for model specification and sampling.&lt;/p&gt;
&lt;p&gt;For bijectors too, there are &lt;a href="https://rstudio.github.io/tfprobability/reference/index.html#section-bijectors"&gt;many of them&lt;/a&gt; - about 40, ranging from straightforward affine maps to more complex operations on Cholesky factors or the discrete cosine transform.&lt;/p&gt;
&lt;p&gt;To see both building blocks in action, let’s end with a “Hello World” of TFP, or two, rather. Here first is the direct way to obtain samples from a standard normal distribution.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tfprobability)

# create a normal distribution
d &amp;lt;- tfd_normal(loc = 0, scale = 1)
# sample from it
d %&amp;gt;% tfd_sample(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor([-1.0863057  -0.61655647  1.8151687 ], shape=(3,), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In case you thought that was too easy, here’s how to do the same using a bijector instead of a distribution.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# generate 3 values uniformly distributed between 0 and 1
u &amp;lt;- runif(3)

# a bijector that in the inverse direction, maps values between 0 and 1
# to a normal distribution
b &amp;lt;- tfb_normal_cdf()

# call bijector&amp;#39;s inverse transform op
b %&amp;gt;% tfb_inverse(u) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor([ 0.96157753  1.0103974  -1.4986734 ], shape=(3,), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this we conclude our introduction. If you run into problems using &lt;em&gt;tfprobability&lt;/em&gt;, or have questions about it, please open an issue in the &lt;a href="https://github.com/rstudio/tfprobability"&gt;github repo&lt;/a&gt;. Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;By default, just sampling from the distribution – but this is something the user can influence if desired, making use of the &lt;code&gt;convert_to_tensor_fn&lt;/code&gt; argument.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;See &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae/"&gt;Representation learning with MMD-VAE&lt;/a&gt; for an example.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;Done so, for example, in &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-01-08-getting-started-with-tf-probability/"&gt;Getting started with TensorFlow Probability from R&lt;/a&gt;.&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;For a complete running example, see the &lt;a href="https://rstudio.github.io/tfprobability/"&gt;tfprobability README&lt;/a&gt;.&lt;a href="#fnref4" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">832773038fec3abda000e4fa72b90c73</distill:md5>
      <category>Probabilistic ML/DL</category>
      <category>Packages/Releases</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-11-07-tfp-cran</guid>
      <pubDate>Thu, 07 Nov 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-11-07-tfp-cran/images/tfprobability.png" medium="image" type="image/png" width="518" height="600"/>
    </item>
    <item>
      <title>TensorFlow 2.0 is here - what changes for R users?</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-10-08-tf2-whatchanges</link>
      <description>


&lt;p&gt;The wait is over – TensorFlow 2.0 (TF 2) is now officially here! What does this mean for us, users of R packages &lt;code&gt;keras&lt;/code&gt; and/or &lt;code&gt;tensorflow&lt;/code&gt;, which, as we know, &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-08-29-using-tf-from-r/"&gt;rely on the Python TensorFlow backend&lt;/a&gt;?&lt;/p&gt;
&lt;p&gt;Before we go into details and explanations, here is an &lt;em&gt;all-clear&lt;/em&gt;, for the concerned user who fears their &lt;code&gt;keras&lt;/code&gt; code might become obsolete (it won’t).&lt;/p&gt;
&lt;h2 id="dont-panic"&gt;Don’t panic&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;If you are using &lt;code&gt;keras&lt;/code&gt; in standard ways, such as those depicted in most code examples and tutorials seen on the web, and things have been working fine for you in recent &lt;code&gt;keras&lt;/code&gt; releases (&amp;gt;= 2.2.4.1), don’t worry. Most everything should work without major changes.&lt;/li&gt;
&lt;li&gt;If you are using an older release of &lt;code&gt;keras&lt;/code&gt; (&amp;lt; 2.2.4.1), syntactically things should work fine as well, but you will want to check for changes in behavior/performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And now for some news and background. This post aims to do three things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explain the above &lt;em&gt;all-clear&lt;/em&gt; statement. Is it really that simple – what exactly is going on?&lt;/li&gt;
&lt;li&gt;Characterize the changes brought about by TF 2, from the &lt;em&gt;point of view of the R user&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;And, perhaps most interestingly: Take a look at what is going on, in the &lt;code&gt;r-tensorflow&lt;/code&gt; ecosystem, around new functionality related to the advent of TF 2.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="some-background"&gt;Some background&lt;/h2&gt;
&lt;p&gt;So if all still works fine (assuming standard usage), why so much ado about TF 2 in Python land?&lt;/p&gt;
&lt;p&gt;The difference is that on the R side, for the vast majority of users, the framework you used to do deep learning was &lt;code&gt;keras&lt;/code&gt;. &lt;code&gt;tensorflow&lt;/code&gt; was needed just occasionally, or not at all.&lt;/p&gt;
&lt;p&gt;Between &lt;code&gt;keras&lt;/code&gt; and &lt;code&gt;tensorflow&lt;/code&gt;, there was a clear separation of responsibilities: &lt;code&gt;keras&lt;/code&gt; was the frontend, depending on TensorFlow as a low-level backend, just like the &lt;a href="http://keras.io"&gt;original Python Keras&lt;/a&gt; it was wrapping did. &lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. In some cases, this lead to people using the words &lt;code&gt;keras&lt;/code&gt; and &lt;code&gt;tensorflow&lt;/code&gt; almost synonymously: Maybe they said &lt;code&gt;tensorflow&lt;/code&gt;, but the code they wrote was &lt;code&gt;keras&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Things were different in Python land. There was original Python Keras, but TensorFlow had its own &lt;code&gt;layers&lt;/code&gt; API, and there were a number of third-party high-level APIs built on TensorFlow. Keras, in contrast, was a separate library that just happened to rely on TensorFlow.&lt;/p&gt;
&lt;p&gt;So in Python land, now we have a big change: &lt;em&gt;With TF 2, Keras (as incorporated in the TensorFlow codebase) is now the official high-level API for TensorFlow&lt;/em&gt;. To bring this across has been a major point of Google’s TF 2 information campaign since the early stages.&lt;/p&gt;
&lt;p&gt;As R users, who have been focusing on &lt;code&gt;keras&lt;/code&gt; all the time, we are essentially less affected. Like we said above, syntactically most everything stays the way it was. So why differentiate between different &lt;code&gt;keras&lt;/code&gt; versions?&lt;/p&gt;
&lt;p&gt;When &lt;code&gt;keras&lt;/code&gt; was written, there was original Python Keras, and that was the library we were binding to. However, Google started to incorporate original Keras code into their TensorFlow codebase as a fork, to continue development independently. For a while there were two “Kerases”: Original Keras and &lt;code&gt;tf.keras&lt;/code&gt;. Our R &lt;code&gt;keras&lt;/code&gt; offered to switch between implementations &lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, the default being original Keras.&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;keras&lt;/code&gt; release 2.2.4.1, anticipating discontinuation of original Keras and wanting to get ready for TF 2, we switched to using &lt;code&gt;tf.keras&lt;/code&gt; as the default. While in the beginning, the &lt;code&gt;tf.keras&lt;/code&gt; fork and original Keras developed more or less in sync, the latest developments for TF 2 brought with them bigger changes in the &lt;code&gt;tf.keras&lt;/code&gt; codebase, especially as regards optimizers. This is why, if you are using a &lt;code&gt;keras&lt;/code&gt; version &amp;lt; 2.2.4.1, upgrading to TF 2 you will want to check for changes in behavior and/or performance. &lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;That’s it for some background. In sum, we’re happy most existing code will run just fine. But for us R users, &lt;em&gt;something&lt;/em&gt; must be changing as well, right?&lt;/p&gt;
&lt;h2 id="tf-2-in-a-nutshell-from-an-r-perspective"&gt;TF 2 in a nutshell, from an R perspective&lt;/h2&gt;
&lt;p&gt;In fact, the most evident-on-user-level change is something we wrote several posts about, more than a year ago &lt;a href="#fn4" class="footnote-ref" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. By then, &lt;em&gt;eager execution&lt;/em&gt; was a brand-new option that had to be turned on explicitly; TF 2 now makes it the default. Along with it came &lt;em&gt;custom models&lt;/em&gt; (a.k.a. subclassed models, in Python land) and &lt;em&gt;custom training&lt;/em&gt;, making use of &lt;code&gt;tf$GradientTape&lt;/code&gt;. Let’s talk about what those termini refer to, and how they are relevant to R users.&lt;/p&gt;
&lt;h3 id="eager-execution"&gt;Eager Execution&lt;/h3&gt;
&lt;p&gt;In TF 1, it was all about the &lt;em&gt;graph&lt;/em&gt; you built when defining your model. The graph, that was – and is – an &lt;em&gt;Abstract Syntax Tree&lt;/em&gt; (AST), with operations as nodes and &lt;em&gt;tensors “flowing”&lt;/em&gt; along the edges. Defining a graph and running it (on actual data) were different steps.&lt;/p&gt;
&lt;p&gt;In contrast, with eager execution, operations are run directly when defined.&lt;/p&gt;
&lt;p&gt;While this is a more-than-substantial change that must have required lots of resources to implement, if you use &lt;code&gt;keras&lt;/code&gt; you won’t notice. Just as previously, the typical &lt;code&gt;keras&lt;/code&gt; workflow of &lt;code&gt;create model&lt;/code&gt; -&amp;gt; &lt;code&gt;compile model&lt;/code&gt; -&amp;gt; &lt;code&gt;train model&lt;/code&gt; never made you think about there being two distinct phases (define and run), now again you don’t have to do anything. Even though the overall execution mode is eager, Keras models are trained in graph mode, to maximize performance. We will talk about how this is done in part 3 when introducing the &lt;code&gt;tfautograph&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;If &lt;code&gt;keras&lt;/code&gt; runs in graph mode, how can you even see that eager execution is “on”? Well, in TF 1, when you ran a TensorFlow operation on a tensor &lt;a href="#fn5" class="footnote-ref" id="fnref5"&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;, like so&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tensorflow)
tf$math$cumprod(1:5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;this is what you saw:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Tensor(&amp;quot;Cumprod:0&amp;quot;, shape=(5,), dtype=int32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To extract the actual values, you had to create a TensorFlow &lt;em&gt;Session&lt;/em&gt; and &lt;code&gt;run&lt;/code&gt; the tensor, or alternatively, use &lt;code&gt;keras::k_eval&lt;/code&gt; that did this under the hood:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
tf$math$cumprod(1:5) %&amp;gt;% k_eval()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1]   1   2   6  24 120&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With TF 2’s execution mode defaulting to &lt;em&gt;eager&lt;/em&gt;, we now automatically see the values contained in the tensor: &lt;a href="#fn6" class="footnote-ref" id="fnref6"&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;tf$math$cumprod(1:5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor([  1   2   6  24 120], shape=(5,), dtype=int32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So that’s eager execution. In our last year’s &lt;em&gt;Eager&lt;/em&gt;-category blog posts, it was always accompanied by &lt;a href="https://tensorflow.rstudio.com/keras/articles/custom_models.html"&gt;custom models&lt;/a&gt;, so let’s turn there next.&lt;/p&gt;
&lt;h3 id="custom-models"&gt;Custom models&lt;/h3&gt;
&lt;p&gt;As a &lt;code&gt;keras&lt;/code&gt; user, probably you’re familiar with the &lt;em&gt;sequential&lt;/em&gt; and &lt;em&gt;functional&lt;/em&gt; styles of building a model. Custom models allow for even greater flexibility than functional-style ones. Check out the &lt;a href="https://tensorflow.rstudio.com/keras/articles/custom_models.html"&gt;documentation&lt;/a&gt; for how to create one.&lt;/p&gt;
&lt;p&gt;Last year’s series on eager execution has plenty of examples using custom models, featuring not just their flexibility, but another important aspect as well: the way they allow for modular, easily-intelligible code. &lt;a href="#fn7" class="footnote-ref" id="fnref7"&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Encoder-decoder scenarios are a natural match. If you have seen, or written, “old-style” code for a Generative Adversarial Network (GAN), imagine something like this instead:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# define the generator (simplified)
generator &amp;lt;-
  function(name = NULL) {
    keras_model_custom(name = name, function(self) {
      
      # define layers for the generator 
      self$fc1 &amp;lt;- layer_dense(units = 7 * 7 * 64, use_bias = FALSE)
      self$batchnorm1 &amp;lt;- layer_batch_normalization()
      # more layers ...
      
      # define what should happen in the forward pass
      function(inputs, mask = NULL, training = TRUE) {
        self$fc1(inputs) %&amp;gt;%
          self$batchnorm1(training = training) %&amp;gt;%
          # call remaining layers ...
      }
    })
  }

# define the discriminator
discriminator &amp;lt;-
  function(name = NULL) {
    keras_model_custom(name = name, function(self) {
      
      self$conv1 &amp;lt;- layer_conv_2d(filters = 64, #...)
      self$leaky_relu1 &amp;lt;- layer_activation_leaky_relu()
      # more layers ...
    
      function(inputs, mask = NULL, training = TRUE) {
        inputs %&amp;gt;% self$conv1() %&amp;gt;%
          self$leaky_relu1() %&amp;gt;%
          # call remaining layers ...
      }
    })
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Coded like this, picture the generator and the discriminator as agents, ready to engage in what is actually the opposite of a zero-sum game.&lt;/p&gt;
&lt;p&gt;The game, then, can be nicely coded using &lt;em&gt;custom training&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id="custom-training"&gt;Custom training&lt;/h3&gt;
&lt;p&gt;Custom training, as opposed to using &lt;code&gt;keras&lt;/code&gt; &lt;code&gt;fit&lt;/code&gt;, allows to interleave the training of several models. Models are &lt;em&gt;called&lt;/em&gt; on data, and all calls have to happen inside the context of a &lt;code&gt;GradientTape&lt;/code&gt;. In eager mode, &lt;code&gt;GradientTape&lt;/code&gt;s are used to keep track of operations such that during backprop, their gradients can be calculated.&lt;/p&gt;
&lt;p&gt;The following code example shows how using &lt;code&gt;GradientTape&lt;/code&gt;-style training, we can &lt;em&gt;see&lt;/em&gt; our actors play against each other:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# zooming in on a single batch of a single epoch
with(tf$GradientTape() %as% gen_tape, { with(tf$GradientTape() %as% disc_tape, {
  
  # first, it&amp;#39;s the generator&amp;#39;s call (yep pun intended)
  generated_images &amp;lt;- generator(noise)
  # now the discriminator gives its verdict on the real images 
  disc_real_output &amp;lt;- discriminator(batch, training = TRUE)
  # as well as the fake ones
  disc_generated_output &amp;lt;- discriminator(generated_images, training = TRUE)
  
  # depending on the discriminator&amp;#39;s verdict we just got,
  # what&amp;#39;s the generator&amp;#39;s loss?
  gen_loss &amp;lt;- generator_loss(disc_generated_output)
  # and what&amp;#39;s the loss for the discriminator?
  disc_loss &amp;lt;- discriminator_loss(disc_real_output, disc_generated_output)
}) })

# now outside the tape&amp;#39;s context compute the respective gradients
gradients_of_generator &amp;lt;- gen_tape$gradient(gen_loss, generator$variables)
gradients_of_discriminator &amp;lt;- disc_tape$gradient(disc_loss, discriminator$variables)
 
# and apply them!
generator_optimizer$apply_gradients(
  purrr::transpose(list(gradients_of_generator, generator$variables)))
discriminator_optimizer$apply_gradients(
  purrr::transpose(list(gradients_of_discriminator, discriminator$variables)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, compare this with pre-TF 2 GAN training – it makes for a &lt;em&gt;lot&lt;/em&gt; more readable code.&lt;/p&gt;
&lt;p&gt;As an aside, last year’s post series may have created the impression that with eager execution, you &lt;em&gt;have&lt;/em&gt; to use custom (&lt;code&gt;GradientTape&lt;/code&gt;) training instead of Keras-style &lt;code&gt;fit&lt;/code&gt;. In fact, that was the case at the time those posts were written. Today, Keras-style code works just fine with eager execution.&lt;/p&gt;
&lt;p&gt;So now with TF 2, we are in an optimal position. We &lt;em&gt;can&lt;/em&gt; use custom training when we want to, but we don’t have to if declarative &lt;code&gt;fit&lt;/code&gt; is all we need.&lt;/p&gt;
&lt;p&gt;That’s it for a flashlight on what TF 2 means to R users. We now take a look around in the &lt;code&gt;r-tensorflow&lt;/code&gt; ecosystem to see new developments – recent-past, present and future – in areas like data loading, preprocessing, and more.&lt;/p&gt;
&lt;h2 id="new-developments-in-the-r-tensorflow-ecosystem"&gt;New developments in the &lt;code&gt;r-tensorflow&lt;/code&gt; ecosystem&lt;/h2&gt;
&lt;p&gt;These are what we’ll cover:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;tfdatasets&lt;/code&gt;: Over the recent past, &lt;code&gt;tfdatasets&lt;/code&gt; pipelines have become the preferred way for data loading and preprocessing.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;feature columns&lt;/em&gt; and &lt;em&gt;feature specs&lt;/em&gt;: Specify your features &lt;code&gt;recipes&lt;/code&gt;-style and have &lt;code&gt;keras&lt;/code&gt; generate the adequate layers for them.&lt;/li&gt;
&lt;li&gt;Keras preprocessing layers: Keras preprocessing pipelines integrating functionality such as data augmentation (currently in planning).&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tfhub&lt;/code&gt;: Use pretrained models as &lt;code&gt;keras&lt;/code&gt; layers, and/or as feature columns in a &lt;code&gt;keras&lt;/code&gt; model.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tf_function&lt;/code&gt; and &lt;code&gt;tfautograph&lt;/code&gt;: Speed up training by running parts of your code in graph mode.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="tfdatasets-input-pipelines"&gt;&lt;em&gt;tfdatasets&lt;/em&gt; input pipelines&lt;/h3&gt;
&lt;p&gt;For 2 years now, the &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/introduction.html"&gt;tfdatasets&lt;/a&gt; package has been available to load data for training Keras models in a streaming way.&lt;/p&gt;
&lt;p&gt;Logically, there are three steps involved:&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;First, data has to be loaded from some place. This could be a csv file, a directory containing images, or other sources. In this recent example from &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-08-23-unet/"&gt;Image segmentation with U-Net&lt;/a&gt;, information about file names was first stored into an R &lt;code&gt;tibble&lt;/code&gt;, and then &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/reference/tensor_slices_dataset.html"&gt;tensor_slices_dataset&lt;/a&gt; was used to create a &lt;code&gt;dataset&lt;/code&gt; from it:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class="r"&gt;&lt;code&gt;data &amp;lt;- tibble(
  img = list.files(here::here(&amp;quot;data-raw/train&amp;quot;), full.names = TRUE),
  mask = list.files(here::here(&amp;quot;data-raw/train_masks&amp;quot;), full.names = TRUE)
)

data &amp;lt;- initial_split(data, prop = 0.8)

dataset &amp;lt;- training(data) %&amp;gt;%  
  tensor_slices_dataset() &lt;/code&gt;&lt;/pre&gt;
&lt;ol start="2" style="list-style-type: decimal"&gt;
&lt;li&gt;Once we have a &lt;code&gt;dataset&lt;/code&gt;, we perform any required transformations, &lt;em&gt;mapping&lt;/em&gt; over the batch dimension. Continuing with the example from the U-Net post, here we use functions from the &lt;a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/image/"&gt;tf.image&lt;/a&gt; module to (1) load images according to their file type, (2) scale them to values between 0 and 1 (converting to &lt;code&gt;float32&lt;/code&gt; at the same time), and (3) resize them to the desired format:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class="r"&gt;&lt;code&gt;dataset &amp;lt;- dataset %&amp;gt;%
  dataset_map(~.x %&amp;gt;% list_modify(
    img = tf$image$decode_jpeg(tf$io$read_file(.x$img)),
    mask = tf$image$decode_gif(tf$io$read_file(.x$mask))[1,,,][,,1,drop=FALSE]
  )) %&amp;gt;% 
  dataset_map(~.x %&amp;gt;% list_modify(
    img = tf$image$convert_image_dtype(.x$img, dtype = tf$float32),
    mask = tf$image$convert_image_dtype(.x$mask, dtype = tf$float32)
  )) %&amp;gt;% 
  dataset_map(~.x %&amp;gt;% list_modify(
    img = tf$image$resize(.x$img, size = shape(128, 128)),
    mask = tf$image$resize(.x$mask, size = shape(128, 128))
  ))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note how once you know what these functions do, they free you of a lot of thinking (remember how in the “old” Keras approach to image preprocessing, you were doing things like dividing pixel values by 255 “by hand”?)&lt;/p&gt;
&lt;ol start="3" style="list-style-type: decimal"&gt;
&lt;li&gt;After transformation, a third conceptual step relates to item arrangement. You will often want to &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/reference/tensor_slices_dataset.html"&gt;shuffle&lt;/a&gt;, and you certainly will want to &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/reference/dataset_batch.html"&gt;batch&lt;/a&gt; the data:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class="r"&gt;&lt;code&gt; if (train) {
    dataset &amp;lt;- dataset %&amp;gt;% 
      dataset_shuffle(buffer_size = batch_size*128)
  }

dataset &amp;lt;- dataset %&amp;gt;%  dataset_batch(batch_size)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Summing up, using &lt;code&gt;tfdatasets&lt;/code&gt; you build a pipeline, from loading over transformations to batching, that can then be fed directly to a Keras model. From preprocessing, let’s go a step further and look at a new, extremely convenient way to do feature engineering.&lt;/p&gt;
&lt;h3 id="feature-columns-and-feature-specs"&gt;Feature columns and feature specs&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/feature_columns.html"&gt;Feature columns&lt;/a&gt; as such are a Python-TensorFlow feature, while &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/feature_spec.html"&gt;feature specs&lt;/a&gt; are an R-only idiom modeled after the popular &lt;a href="https://cran.r-project.org/web/packages/recipes/index.html"&gt;recipes&lt;/a&gt; package.&lt;/p&gt;
&lt;p&gt;It all starts off with creating a feature spec object, using formula syntax to indicate what’s predictor and what’s target:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tfdatasets)
hearts_dataset &amp;lt;- tensor_slices_dataset(hearts)
spec &amp;lt;- feature_spec(hearts_dataset, target ~ .)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That specification is then refined by successive information about how we want to make use of the raw predictors. This is where feature columns come into play. Different column types exist, of which you can see a few in the following code snippet:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;spec &amp;lt;- feature_spec(hearts, target ~ .) %&amp;gt;% 
  step_numeric_column(
    all_numeric(), -cp, -restecg, -exang, -sex, -fbs,
    normalizer_fn = scaler_standard()
  ) %&amp;gt;% 
  step_categorical_column_with_vocabulary_list(thal) %&amp;gt;% 
  step_bucketized_column(age, boundaries = c(18, 25, 30, 35, 40, 45, 50, 55, 60, 65)) %&amp;gt;% 
  step_indicator_column(thal) %&amp;gt;% 
  step_embedding_column(thal, dimension = 2) %&amp;gt;% 
  step_crossed_column(c(thal, bucketized_age), hash_bucket_size = 10) %&amp;gt;%
  step_indicator_column(crossed_thal_bucketized_age)

spec %&amp;gt;% fit()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What happened here is that we told TensorFlow, please take all numeric columns (besides a few ones listed exprès) and scale them; take column &lt;code&gt;thal&lt;/code&gt;, treat it as categorical and create an embedding for it; discretize &lt;code&gt;age&lt;/code&gt; according to the given ranges; and finally, create a &lt;em&gt;crossed column&lt;/em&gt; to capture interaction between &lt;code&gt;thal&lt;/code&gt; and that discretized age-range column. &lt;a href="#fn8" class="footnote-ref" id="fnref8"&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is nice, but when creating the model, we’ll still have to define all those layers, right? (Which would be pretty cumbersome, having to figure out all the right dimensions…) Luckily, we don’t have to. In sync with &lt;code&gt;tfdatasets&lt;/code&gt;, &lt;code&gt;keras&lt;/code&gt; now provides &lt;a href="https://tensorflow.rstudio.com/keras/reference/layer_dense_features.html"&gt;layer_dense_features&lt;/a&gt; to create a layer tailor-made to accommodate the specification.&lt;/p&gt;
&lt;p&gt;And we don’t need to create separate input layers either, due to &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/reference/layer_input_from_dataset.html"&gt;layer_input_from_dataset&lt;/a&gt;. Here we see both in action:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;input &amp;lt;- layer_input_from_dataset(hearts %&amp;gt;% select(-target))

output &amp;lt;- input %&amp;gt;% 
  layer_dense_features(feature_columns = dense_features(spec)) %&amp;gt;% 
  layer_dense(units = 1, activation = &amp;quot;sigmoid&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From then on, it’s just normal &lt;code&gt;keras&lt;/code&gt; &lt;code&gt;compile&lt;/code&gt; and &lt;code&gt;fit&lt;/code&gt;. See the &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/feature_columns.html"&gt;vignette&lt;/a&gt; for the complete example. There also is a &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-07-09-feature-columns/"&gt;post on feature columns&lt;/a&gt; explaining more of how this works, and illustrating the time-and-nerve-saving effect by comparing with the pre-feature-spec way of working with heterogeneous datasets.&lt;/p&gt;
&lt;p&gt;As a last item on the topics of preprocessing and feature engineering, let’s look at a promising thing to come in what we hope is the near future.&lt;/p&gt;
&lt;h3 id="keras-preprocessing-layers"&gt;Keras preprocessing layers&lt;/h3&gt;
&lt;p&gt;Reading what we wrote above about using &lt;code&gt;tfdatasets&lt;/code&gt; for building a input pipeline, and seeing how we gave an image loading example, you may have been wondering: What about data augmentation functionality available, historically, through &lt;code&gt;keras&lt;/code&gt;? Like &lt;code&gt;image_data_generator&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;This functionality does not seem to fit. But a nice-looking solution is in preparation. In the Keras community, the recent &lt;a href="https://github.com/keras-team/governance/blob/master/rfcs/20190729-keras-preprocessing-redesign.md"&gt;RFC on preprocessing layers for Keras&lt;/a&gt; addresses this topic. The RFC is still under discussion, but as soon as it gets implemented in Python we’ll follow up on the R side.&lt;/p&gt;
&lt;p&gt;The idea is to provide (chainable) preprocessing layers to be used for data transformation and/or augmentation in areas such as image classification, image segmentation, object detection, text processing, and more. &lt;a href="#fn9" class="footnote-ref" id="fnref9"&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt; The envisioned, in the RFC, pipeline of preprocessing layers should return a &lt;code&gt;dataset&lt;/code&gt;, for compatibility with &lt;code&gt;tf.data&lt;/code&gt; (our &lt;code&gt;tfdatasets&lt;/code&gt;). We’re definitely looking forward to having available this sort of workflow!&lt;/p&gt;
&lt;p&gt;Let’s move on to the next topic, the common denominator being convenience. But now convenience means not having to build billion-parameter models yourself!&lt;/p&gt;
&lt;h3 id="tensorflow-hub-and-the-tfhub-package"&gt;Tensorflow Hub and the &lt;code&gt;tfhub&lt;/code&gt; package&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://www.tensorflow.org/hub"&gt;Tensorflow Hub&lt;/a&gt; is a library for publishing and using pretrained models. Existing models can be browsed on &lt;a href="https://tfhub.dev/"&gt;tfhub.dev&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As of this writing, the original Python library is still under development, so complete stability is not guaranteed. That notwithstanding, the &lt;a href="https://github.com/rstudio/tfhub"&gt;tfhub&lt;/a&gt; R package already allows for some instructive experimentation.&lt;/p&gt;
&lt;p&gt;The traditional Keras idea of using pretrained models typically involved either (1) applying a model like &lt;em&gt;MobileNet&lt;/em&gt; as a whole, including its output layer, or (2) chaining a “custom head” to its penultimate layer &lt;a href="#fn10" class="footnote-ref" id="fnref10"&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt;. In contrast, the TF Hub idea is to use a pretrained model as a &lt;em&gt;module&lt;/em&gt; in a larger setting.&lt;/p&gt;
&lt;p&gt;There are two main ways to accomplish this, namely, integrating a module as a &lt;code&gt;keras&lt;/code&gt; layer and using it as a feature column. The &lt;a href="https://github.com/rstudio/tfhub"&gt;tfhub README&lt;/a&gt; shows the first option:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tfhub)
library(keras)

input &amp;lt;- layer_input(shape = c(32, 32, 3))

output &amp;lt;- input %&amp;gt;%
  # we are using a pre-trained MobileNet model!
  layer_hub(handle = &amp;quot;https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2&amp;quot;) %&amp;gt;%
  layer_dense(units = 10, activation = &amp;quot;softmax&amp;quot;)

model &amp;lt;- keras_model(input, output)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While the &lt;a href="https://github.com/rstudio/tfhub/blob/master/vignettes/examples/feature_column.R"&gt;tfhub feature columns vignette&lt;/a&gt; illustrates the second one:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;spec &amp;lt;- dataset_train %&amp;gt;%
  feature_spec(AdoptionSpeed ~ .) %&amp;gt;%
  step_text_embedding_column(
    Description,
    module_spec = &amp;quot;https://tfhub.dev/google/universal-sentence-encoder/2&amp;quot;
    ) %&amp;gt;%
  step_image_embedding_column(
    img,
    module_spec = &amp;quot;https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/3&amp;quot;
  ) %&amp;gt;%
  step_numeric_column(Age, Fee, Quantity, normalizer_fn = scaler_standard()) %&amp;gt;%
  step_categorical_column_with_vocabulary_list(
    has_type(&amp;quot;string&amp;quot;), -Description, -RescuerID, -img_path, -PetID, -Name
  ) %&amp;gt;%
  step_embedding_column(Breed1:Health, State)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both usage modes illustrate the high potential of working with Hub modules. Just be cautioned that, as of today, not every model published will work with TF 2.&lt;/p&gt;
&lt;h3 id="tf_function-tf-autograph-and-the-r-package-tfautograph"&gt;&lt;code&gt;tf_function&lt;/code&gt;, TF autograph and the R package &lt;code&gt;tfautograph&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;As explained above, the default execution mode in TF 2 is eager. For performance reasons however, in many cases it will be desirable to compile parts of your code into a graph. Calls to Keras layers, for example, are run in graph mode.&lt;/p&gt;
&lt;p&gt;To compile a function into a graph, wrap it in a call to &lt;code&gt;tf_function&lt;/code&gt;, as done e.g. in the post &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-07-31-censored-data/"&gt;Modeling censored data with tfprobability&lt;/a&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;run_mcmc &amp;lt;- function(kernel) {
  kernel %&amp;gt;% mcmc_sample_chain(
    num_results = n_steps,
    num_burnin_steps = n_burnin,
    current_state = tf$ones_like(initial_betas),
    trace_fn = trace_fn
  )
}

# important for performance: run HMC in graph mode
run_mcmc &amp;lt;- tf_function(run_mcmc)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the Python side, the &lt;code&gt;tf.autograph&lt;/code&gt; module automatically translates Python control flow statements into appropriate graph operations.&lt;/p&gt;
&lt;p&gt;Independently of &lt;code&gt;tf.autograph&lt;/code&gt;, the R package &lt;a href="https://t-kalinowski.github.io/tfautograph/index.html"&gt;tfautograph&lt;/a&gt;, developed by Tomasz Kalinowski, implements control flow conversion directly from R to TensorFlow. This lets you use R’s &lt;code&gt;if&lt;/code&gt;, &lt;code&gt;while&lt;/code&gt;, &lt;code&gt;for&lt;/code&gt;, &lt;code&gt;break&lt;/code&gt;, and &lt;code&gt;next&lt;/code&gt; when writing custom training flows. Check out the package’s extensive documentation for instructive examples!&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;With that, we end our introduction of TF 2 and the new developments that surround it.&lt;/p&gt;
&lt;p&gt;If you have been using &lt;code&gt;keras&lt;/code&gt; in traditional ways, how much changes &lt;em&gt;for you&lt;/em&gt; is mainly up to you: Most everything will still work, but new options exist to write more performant, more modular, more elegant code. In particular, check out &lt;code&gt;tfdatasets&lt;/code&gt; pipelines for efficient data loading.&lt;/p&gt;
&lt;p&gt;If you’re an advanced user requiring non-standard setup, have a look into custom training and custom models, and consult the &lt;code&gt;tfautograph&lt;/code&gt; documentation to see how the package can help.&lt;/p&gt;
&lt;p&gt;In any case, stay tuned for upcoming posts showing some of the above-mentioned functionality in action. Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Original Python Keras, and thus, R &lt;code&gt;keras&lt;/code&gt;, supported additional backends: Theano and CNTK. But the default backend in R &lt;code&gt;keras&lt;/code&gt; always was TensorFlow.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;Note the terminology: in R &lt;code&gt;keras&lt;/code&gt;, &lt;em&gt;implementation&lt;/em&gt; referred to the Python library (Keras or TensorFlow, with its module &lt;code&gt;tf.keras&lt;/code&gt;) bound to, while &lt;em&gt;backend&lt;/em&gt; referred to the framework providing low-level operations, which could be one of Theano, TensorFlow and CNTK.)&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;E.g., parameters like &lt;em&gt;learning_rate&lt;/em&gt; may have to be adapted.&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;See &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-10-02-eager-wrapup/"&gt;More flexible models with TensorFlow eager execution and Keras&lt;/a&gt; for an overview and annotated links.&lt;a href="#fnref4" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn5"&gt;&lt;p&gt;Here the nominal input is an R vector that gets converted to a Python list by &lt;code&gt;reticulate&lt;/code&gt;, and to a tensor by TensorFlow.&lt;a href="#fnref5" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn6"&gt;&lt;p&gt;This is still a tensor though. To continue working with its values in R, we need to convert it to R using &lt;code&gt;as.numeric&lt;/code&gt;, &lt;code&gt;as.matrix&lt;/code&gt;, &lt;code&gt;as.array&lt;/code&gt; etc.&lt;a href="#fnref6" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn7"&gt;&lt;p&gt;For example, see &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-08-26-eager-dcgan/"&gt;Generating images with Keras and TensorFlow eager execution&lt;/a&gt; on GANs, &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-09-10-eager-style-transfer/"&gt;Neural style transfer with eager execution and Keras&lt;/a&gt; on neural style transfer, or &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae/"&gt;Representation learning with MMD-VAE&lt;/a&gt; on Variational Autoencoders.&lt;a href="#fnref7" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn8"&gt;&lt;p&gt;&lt;code&gt;step_indicator_column&lt;/code&gt; is there (twice) for technical reasons. Our &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-07-09-feature-columns/"&gt;post on feature columns&lt;/a&gt; explains.&lt;a href="#fnref8" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn9"&gt;&lt;p&gt;As readers working in e.g. image segmentation will know, data augmentation is not as easy as just using &lt;code&gt;image_data_generator&lt;/code&gt; on the input images, as analogous distortions have to be applied to the masks.&lt;a href="#fnref9" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn10"&gt;&lt;p&gt;or block of layers&lt;a href="#fnref10" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">c184304f260d68951aaedf2326fc05d7</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-10-08-tf2-whatchanges</guid>
      <pubDate>Tue, 08 Oct 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-10-08-tf2-whatchanges/images/thumb.png" medium="image" type="image/png" width="400" height="400"/>
    </item>
    <item>
      <title>BERT from R</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Turgut Abdullayev</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-09-30-bert-r</link>
      <description>


&lt;p&gt;&lt;em&gt;Today, we’re happy to feature a guest post written by Turgut Abdullayev, showing how to use BERT from R. Turgut is a data scientist at AccessBank Azerbaijan. Currently, he is pursuing a Ph.D. in economics at Baku State University, Azerbaijan.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-08-29-using-tf-from-r/"&gt;In the previous post, Sigrid Keydana&lt;/a&gt; explained the logic behind the &lt;a href="https://rstudio.github.io/reticulate/"&gt;reticulate package&lt;/a&gt; and how it enables interoperability between Python and R. So, this time we will build a classification model with &lt;a href="https://github.com/google-research/bert"&gt;BERT&lt;/a&gt;, taking into account one of the powerful capabilities of the reticulate package – calling Python from R via importing Python modules.&lt;/p&gt;
&lt;p&gt;Before we start, make sure that the Python version used is 3, as Python 2 can introduce lots of difficulties while working with BERT, such as Unicode issues related to the input text.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: The R implementation presupposes TF Keras while by default, keras-bert does not use it. So, adding that environment variable makes it work.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class="r"&gt;&lt;code&gt;Sys.setenv(TF_KERAS=1) 
# make sure we use python 3
reticulate::use_python(&amp;#39;C:/Users/turgut.abdullayev/AppData/Local/Continuum/anaconda3/python.exe&amp;#39;,
                       required=T)
# to see python version
reticulate::py_config()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;python:         C:/Users/turgut.abdullayev/AppData/Local/Continuum/anaconda3/python.exe
libpython:      C:/Users/turgut.abdullayev/AppData/Local/Continuum/anaconda3/python37.dll
pythonhome:     C:\Users\TURGUT~1.ABD\AppData\Local\CONTIN~1\ANACON~1
version:        3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)]
Architecture:   64bit
numpy:          C:\Users\TURGUT~1.ABD\AppData\Local\CONTIN~1\ANACON~1\lib\site-packages\numpy
numpy_version:  1.16.4

NOTE: Python version was forced by use_python function&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Luckily for us, a convenient way of importing BERT with Keras was created by Zhao HG. It is called &lt;a href="https://github.com/CyberZHG/keras-bert"&gt;Keras-bert&lt;/a&gt;. For us, this means that importing that same python library with &lt;code&gt;reticulate&lt;/code&gt; will allow us to build a popular state-of-the-art model within R.&lt;/p&gt;
&lt;p&gt;There are several methods to install keras-bert in Python.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;in Jupyter Notebook, run:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;!pip install keras-bert&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;in Terminal (Linux, Mac OS), run:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;python3 -m pip install keras-bert&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;in Anaconda prompt (Windows), run:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;conda install keras-bert&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After this procedure, you can check whether keras-bert is installed or not.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;reticulate::py_module_available(&amp;#39;keras_bert&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;[1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, the TensorFlow version used should be 1.14/1.15. You can check it in the following form:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;tensorflow::tf_version()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;[1] ‘1.14’&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In a nutshell:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pip install keras-bert
tensorflow::install_tensorflow(version = &amp;quot;1.15&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="what-is-bert"&gt;What is BERT?&lt;/h2&gt;
&lt;p&gt;BERT&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; is a pre-trained deep learning model introduced by Google AI Research which has been trained on Wikipedia and BooksCorpus. It has a unique way to understand the structure of a given text. Instead of reading the text from left to right or from right to left, BERT, using an attention mechanism which is called Transformer encoder&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, reads the entire word sequences at once. So, it allows to understanding a word based on its surroundings. There are different kind of pre-trained BERT models but the main difference between them is trained parameters. In our case, &lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip"&gt;BERT&lt;/a&gt; with 12 encoder layers (Transformer Blocks), 768-hidden hidden units, 12-heads&lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;, and 110M parameters will be used to create a text classification model.&lt;/p&gt;
&lt;h2 id="model-structure"&gt;Model structure&lt;/h2&gt;
&lt;p&gt;Loading a pre-trained BERT model is straightforward. The &lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip"&gt;downloaded zip file&lt;/a&gt; contains:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;bert_model.ckpt&lt;/em&gt;, which is for loading the weights from the TensorFlow checkpoint&lt;/li&gt;
&lt;li&gt;&lt;em&gt;bert_config.json&lt;/em&gt;, which is a configuration file&lt;/li&gt;
&lt;li&gt;&lt;em&gt;vocab.txt&lt;/em&gt;, which is for text tokenization&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;pretrained_path = &amp;#39;/Users/turgutabdullayev/Downloads/uncased_L-12_H-768_A-12&amp;#39;
config_path = file.path(pretrained_path, &amp;#39;bert_config.json&amp;#39;)
checkpoint_path = file.path(pretrained_path, &amp;#39;bert_model.ckpt&amp;#39;)
vocab_path = file.path(pretrained_path, &amp;#39;vocab.txt&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="import-keras-bert-module-via-reticulate"&gt;Import Keras-Bert module via reticulate&lt;/h2&gt;
&lt;p&gt;Let’s load keras-bert via &lt;code&gt;reticulate&lt;/code&gt; and prepare a tokenizer object. The BERT tokenizer will help us to turn words into indices.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(reticulate)
k_bert = import(&amp;#39;keras_bert&amp;#39;)
token_dict = k_bert$load_vocabulary(vocab_path)
tokenizer = k_bert$Tokenizer(token_dict)&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="how-does-the-tokenizer-work"&gt;How does the tokenizer work?&lt;/h3&gt;
&lt;p&gt;BERT uses a WordPiece tokenization strategy. If a word is Out-of-vocabulary (OOV), then BERT will break it down into subwords. (eating =&amp;gt; eat, ##ing).&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-09-30-bert-r/images/emb.png" alt="[BERT input representation. The input embeddings are the sum of the token embeddings, the segmentation embeddings, and the position embeddings](https://arxiv.org/pdf/1810.04805.pdf)" width="549" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-13)&lt;a href="https://arxiv.org/pdf/1810.04805.pdf"&gt;BERT input representation. The input embeddings are the sum of the token embeddings, the segmentation embeddings, and the position embeddings&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="embedding-layers-in-bert"&gt;Embedding Layers in BERT&lt;/h2&gt;
&lt;p&gt;There are 3 types of embedding layers in BERT:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Token Embeddings&lt;/strong&gt; help to transform words into vector representations. In our model dimension size is 768.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Segment Embeddings&lt;/strong&gt; help to understand the semantic similarity of different pieces of the text.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Position Embeddings&lt;/strong&gt; mean that identical words at different positions will not have the same output representation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="define-model-parameters-and-column-names"&gt;Define model parameters and column names&lt;/h2&gt;
&lt;p&gt;As usual with keras, the batch size, number of epochs and the learning rate should be defined for training BERT. Additionally, the &lt;em&gt;sequence length&lt;/em&gt; is needed.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;seq_length = 50L
bch_size = 70
epochs = 1
learning_rate = 1e-4

DATA_COLUMN = &amp;#39;comment_text&amp;#39;
LABEL_COLUMN = &amp;#39;target&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: the max input length is 512, and the model is extremely compute intensive even on GPU.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="load-bert-model-into-r"&gt;Load BERT model into R&lt;/h2&gt;
&lt;p&gt;We can load the BERT model and automatically pad sequences with &lt;code&gt;seq_len&lt;/code&gt; function. Keras-bert&lt;a href="#fn4" class="footnote-ref" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; makes the loading process very easy and comfortable.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model = k_bert$load_trained_model_from_checkpoint(
  config_path,
  checkpoint_path,
  training=T,
  trainable=T,
  seq_len=seq_length)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="data-structure-reading-preparation"&gt;Data structure, reading, preparation&lt;/h2&gt;
&lt;p&gt;The dataset for this post is taken from the &lt;a href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification"&gt;Kaggle Jigsaw Unintended Bias in Toxicity Classification competition&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In order to prepare the dataset, we write a preprocessing function which will read and tokenize data simultaneously. Then, we feed the outputs of the function as input for BERT model.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# tokenize text
tokenize_fun = function(dataset) {
  c(indices, target, segments) %&amp;lt;-% list(list(),list(),list())
  for ( i in 1:nrow(dataset)) {
    c(indices_tok, segments_tok) %&amp;lt;-% tokenizer$encode(dataset[[DATA_COLUMN]][i], 
                                                       max_len=seq_length)
    indices = indices %&amp;gt;% append(list(as.matrix(indices_tok)))
    target = target %&amp;gt;% append(dataset[[LABEL_COLUMN]][i])
    segments = segments %&amp;gt;% append(list(as.matrix(segments_tok)))
  }
  return(list(indices,segments, target))
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# read data
dt_data = function(dir, rows_to_read){
  data = data.table::fread(dir, nrows=rows_to_read)
  c(x_train, x_segment, y_train) %&amp;lt;-% tokenize_fun(data)
  return(list(x_train, x_segment, y_train))
}&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="load-dataset"&gt;Load dataset&lt;/h2&gt;
&lt;p&gt;The way we have written the preprocess function, at first, it will read data, then add zeros and encode words into indices. Hence, we will have 3 output files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;x_train&lt;/strong&gt; is input matrix for BERT&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;x_segment&lt;/strong&gt; contains zeros for segment embeddings&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;y_train&lt;/strong&gt; is the output target which we should predict&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;c(x_train,x_segment, y_train) %&amp;lt;-% 
dt_data(&amp;#39;~/Downloads/jigsaw-unintended-bias-in-toxicity-classification/train.csv&amp;#39;,2000)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="matrix-format-for-keras-bert"&gt;Matrix format for Keras-Bert&lt;/h2&gt;
&lt;p&gt;The input data are in list format. They need to be extracted and transposed. Then, the train and segment matrices should be placed into the list.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train = do.call(cbind,x_train) %&amp;gt;% t()
segments = do.call(cbind,x_segment) %&amp;gt;% t()
targets = do.call(cbind,y_train) %&amp;gt;% t()

concat = c(list(train ),list(segments))&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="calculate-decay-and-warmup-steps"&gt;Calculate decay and warmup steps&lt;/h2&gt;
&lt;p&gt;Using the Adam optimizer with warmup helps to lower the learning rate at the beginning of the training process. After certain training steps, the learning rate will gradually be increased, because learning new data without warmup can negatively affect a BERT model.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;c(decay_steps, warmup_steps) %&amp;lt;-% k_bert$calc_train_steps(
  targets %&amp;gt;% length(),
  batch_size=bch_size,
  epochs=epochs
)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="determine-inputs-and-outputs-then-concatenate-them"&gt;Determine inputs and outputs, then concatenate them&lt;/h2&gt;
&lt;p&gt;In order to build a binary classification model, the output of the BERT model should contain 1 unit. Therefore, first of all, we should get input and output layers. Then, adding an additional dense layer to the output can perfectly meet our needs.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)

input_1 = get_layer(model,name = &amp;#39;Input-Token&amp;#39;)$input
input_2 = get_layer(model,name = &amp;#39;Input-Segment&amp;#39;)$input
inputs = list(input_1,input_2)

dense = get_layer(model,name = &amp;#39;NSP-Dense&amp;#39;)$output

outputs = dense %&amp;gt;% layer_dense(units=1L, activation=&amp;#39;sigmoid&amp;#39;,
                         kernel_initializer=initializer_truncated_normal(stddev = 0.02),
                         name = &amp;#39;output&amp;#39;)

model = keras_model(inputs = inputs,outputs = outputs)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is how the model architecture looks like after adding a dense layer and padding input sequences.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;Model
__________________________________________________________________________________________
Layer (type)                 Output Shape        Param #    Connected to                  
==========================================================================================
Input-Token (InputLayer)     (None, 50)          0                                        
__________________________________________________________________________________________
Input-Segment (InputLayer)   (None, 50)          0                                        
__________________________________________________________________________________________
Embedding-Token (TokenEmbedd [(None, 50, 768), ( 23440896   Input-Token[0][0]             
__________________________________________________________________________________________
Embedding-Segment (Embedding (None, 50, 768)     1536       Input-Segment[0][0]           
__________________________________________________________________________________________
Embedding-Token-Segment (Add (None, 50, 768)     0          Embedding-Token[0][0]         
                                                            Embedding-Segment[0][0]       
__________________________________________________________________________________________
Embedding-Position (Position (None, 50, 768)     38400      Embedding-Token-Segment[0][0] 
__________________________________________________________________________________________
Embedding-Dropout (Dropout)  (None, 50, 768)     0          Embedding-Position[0][0]      
__________________________________________________________________________________________
Embedding-Norm (LayerNormali (None, 50, 768)     1536       Embedding-Dropout[0][0]       
__________________________________________________________________________________________
Encoder-1-MultiHeadSelfAtten (None, 50, 768)     2362368    Embedding-Norm[0][0]          
__________________________________________________________________________________________
Encoder-1-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-1-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-1-MultiHeadSelfAtten (None, 50, 768)     0          Embedding-Norm[0][0]          
                                                            Encoder-1-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-1-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-1-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-1-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-1-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-1-FeedForward-Dropou (None, 50, 768)     0          Encoder-1-FeedForward[0][0]   
__________________________________________________________________________________________
Encoder-1-FeedForward-Add (A (None, 50, 768)     0          Encoder-1-MultiHeadSelfAttenti
                                                            Encoder-1-FeedForward-Dropout[
__________________________________________________________________________________________
Encoder-1-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-1-FeedForward-Add[0][0
__________________________________________________________________________________________
Encoder-2-MultiHeadSelfAtten (None, 50, 768)     2362368    Encoder-1-FeedForward-Norm[0][
__________________________________________________________________________________________
Encoder-2-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-2-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-2-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-1-FeedForward-Norm[0][
                                                            Encoder-2-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-2-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-2-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-2-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-2-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-2-FeedForward-Dropou (None, 50, 768)     0          Encoder-2-FeedForward[0][0]   
__________________________________________________________________________________________
Encoder-2-FeedForward-Add (A (None, 50, 768)     0          Encoder-2-MultiHeadSelfAttenti
                                                            Encoder-2-FeedForward-Dropout[
__________________________________________________________________________________________
Encoder-2-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-2-FeedForward-Add[0][0
__________________________________________________________________________________________
Encoder-3-MultiHeadSelfAtten (None, 50, 768)     2362368    Encoder-2-FeedForward-Norm[0][
__________________________________________________________________________________________
Encoder-3-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-3-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-3-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-2-FeedForward-Norm[0][
                                                            Encoder-3-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-3-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-3-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-3-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-3-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-3-FeedForward-Dropou (None, 50, 768)     0          Encoder-3-FeedForward[0][0]   
__________________________________________________________________________________________
Encoder-3-FeedForward-Add (A (None, 50, 768)     0          Encoder-3-MultiHeadSelfAttenti
                                                            Encoder-3-FeedForward-Dropout[
__________________________________________________________________________________________
Encoder-3-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-3-FeedForward-Add[0][0
__________________________________________________________________________________________
Encoder-4-MultiHeadSelfAtten (None, 50, 768)     2362368    Encoder-3-FeedForward-Norm[0][
__________________________________________________________________________________________
Encoder-4-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-4-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-4-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-3-FeedForward-Norm[0][
                                                            Encoder-4-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-4-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-4-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-4-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-4-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-4-FeedForward-Dropou (None, 50, 768)     0          Encoder-4-FeedForward[0][0]   
__________________________________________________________________________________________
Encoder-4-FeedForward-Add (A (None, 50, 768)     0          Encoder-4-MultiHeadSelfAttenti
                                                            Encoder-4-FeedForward-Dropout[
__________________________________________________________________________________________
Encoder-4-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-4-FeedForward-Add[0][0
__________________________________________________________________________________________
Encoder-5-MultiHeadSelfAtten (None, 50, 768)     2362368    Encoder-4-FeedForward-Norm[0][
__________________________________________________________________________________________
Encoder-5-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-5-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-5-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-4-FeedForward-Norm[0][
                                                            Encoder-5-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-5-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-5-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-5-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-5-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-5-FeedForward-Dropou (None, 50, 768)     0          Encoder-5-FeedForward[0][0]   
__________________________________________________________________________________________
Encoder-5-FeedForward-Add (A (None, 50, 768)     0          Encoder-5-MultiHeadSelfAttenti
                                                            Encoder-5-FeedForward-Dropout[
__________________________________________________________________________________________
Encoder-5-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-5-FeedForward-Add[0][0
__________________________________________________________________________________________
Encoder-6-MultiHeadSelfAtten (None, 50, 768)     2362368    Encoder-5-FeedForward-Norm[0][
__________________________________________________________________________________________
Encoder-6-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-6-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-6-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-5-FeedForward-Norm[0][
                                                            Encoder-6-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-6-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-6-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-6-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-6-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-6-FeedForward-Dropou (None, 50, 768)     0          Encoder-6-FeedForward[0][0]   
__________________________________________________________________________________________
Encoder-6-FeedForward-Add (A (None, 50, 768)     0          Encoder-6-MultiHeadSelfAttenti
                                                            Encoder-6-FeedForward-Dropout[
__________________________________________________________________________________________
Encoder-6-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-6-FeedForward-Add[0][0
__________________________________________________________________________________________
Encoder-7-MultiHeadSelfAtten (None, 50, 768)     2362368    Encoder-6-FeedForward-Norm[0][
__________________________________________________________________________________________
Encoder-7-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-7-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-7-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-6-FeedForward-Norm[0][
                                                            Encoder-7-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-7-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-7-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-7-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-7-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-7-FeedForward-Dropou (None, 50, 768)     0          Encoder-7-FeedForward[0][0]   
__________________________________________________________________________________________
Encoder-7-FeedForward-Add (A (None, 50, 768)     0          Encoder-7-MultiHeadSelfAttenti
                                                            Encoder-7-FeedForward-Dropout[
__________________________________________________________________________________________
Encoder-7-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-7-FeedForward-Add[0][0
__________________________________________________________________________________________
Encoder-8-MultiHeadSelfAtten (None, 50, 768)     2362368    Encoder-7-FeedForward-Norm[0][
__________________________________________________________________________________________
Encoder-8-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-8-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-8-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-7-FeedForward-Norm[0][
                                                            Encoder-8-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-8-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-8-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-8-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-8-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-8-FeedForward-Dropou (None, 50, 768)     0          Encoder-8-FeedForward[0][0]   
__________________________________________________________________________________________
Encoder-8-FeedForward-Add (A (None, 50, 768)     0          Encoder-8-MultiHeadSelfAttenti
                                                            Encoder-8-FeedForward-Dropout[
__________________________________________________________________________________________
Encoder-8-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-8-FeedForward-Add[0][0
__________________________________________________________________________________________
Encoder-9-MultiHeadSelfAtten (None, 50, 768)     2362368    Encoder-8-FeedForward-Norm[0][
__________________________________________________________________________________________
Encoder-9-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-9-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-9-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-8-FeedForward-Norm[0][
                                                            Encoder-9-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-9-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-9-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-9-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-9-MultiHeadSelfAttenti
__________________________________________________________________________________________
Encoder-9-FeedForward-Dropou (None, 50, 768)     0          Encoder-9-FeedForward[0][0]   
__________________________________________________________________________________________
Encoder-9-FeedForward-Add (A (None, 50, 768)     0          Encoder-9-MultiHeadSelfAttenti
                                                            Encoder-9-FeedForward-Dropout[
__________________________________________________________________________________________
Encoder-9-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-9-FeedForward-Add[0][0
__________________________________________________________________________________________
Encoder-10-MultiHeadSelfAtte (None, 50, 768)     2362368    Encoder-9-FeedForward-Norm[0][
__________________________________________________________________________________________
Encoder-10-MultiHeadSelfAtte (None, 50, 768)     0          Encoder-10-MultiHeadSelfAttent
__________________________________________________________________________________________
Encoder-10-MultiHeadSelfAtte (None, 50, 768)     0          Encoder-9-FeedForward-Norm[0][
                                                            Encoder-10-MultiHeadSelfAttent
__________________________________________________________________________________________
Encoder-10-MultiHeadSelfAtte (None, 50, 768)     1536       Encoder-10-MultiHeadSelfAttent
__________________________________________________________________________________________
Encoder-10-FeedForward (Feed (None, 50, 768)     4722432    Encoder-10-MultiHeadSelfAttent
__________________________________________________________________________________________
Encoder-10-FeedForward-Dropo (None, 50, 768)     0          Encoder-10-FeedForward[0][0]  
__________________________________________________________________________________________
Encoder-10-FeedForward-Add ( (None, 50, 768)     0          Encoder-10-MultiHeadSelfAttent
                                                            Encoder-10-FeedForward-Dropout
__________________________________________________________________________________________
Encoder-10-FeedForward-Norm  (None, 50, 768)     1536       Encoder-10-FeedForward-Add[0][
__________________________________________________________________________________________
Encoder-11-MultiHeadSelfAtte (None, 50, 768)     2362368    Encoder-10-FeedForward-Norm[0]
__________________________________________________________________________________________
Encoder-11-MultiHeadSelfAtte (None, 50, 768)     0          Encoder-11-MultiHeadSelfAttent
__________________________________________________________________________________________
Encoder-11-MultiHeadSelfAtte (None, 50, 768)     0          Encoder-10-FeedForward-Norm[0]
                                                            Encoder-11-MultiHeadSelfAttent
__________________________________________________________________________________________
Encoder-11-MultiHeadSelfAtte (None, 50, 768)     1536       Encoder-11-MultiHeadSelfAttent
__________________________________________________________________________________________
Encoder-11-FeedForward (Feed (None, 50, 768)     4722432    Encoder-11-MultiHeadSelfAttent
__________________________________________________________________________________________
Encoder-11-FeedForward-Dropo (None, 50, 768)     0          Encoder-11-FeedForward[0][0]  
__________________________________________________________________________________________
Encoder-11-FeedForward-Add ( (None, 50, 768)     0          Encoder-11-MultiHeadSelfAttent
                                                            Encoder-11-FeedForward-Dropout
__________________________________________________________________________________________
Encoder-11-FeedForward-Norm  (None, 50, 768)     1536       Encoder-11-FeedForward-Add[0][
__________________________________________________________________________________________
Encoder-12-MultiHeadSelfAtte (None, 50, 768)     2362368    Encoder-11-FeedForward-Norm[0]
__________________________________________________________________________________________
Encoder-12-MultiHeadSelfAtte (None, 50, 768)     0          Encoder-12-MultiHeadSelfAttent
__________________________________________________________________________________________
Encoder-12-MultiHeadSelfAtte (None, 50, 768)     0          Encoder-11-FeedForward-Norm[0]
                                                            Encoder-12-MultiHeadSelfAttent
__________________________________________________________________________________________
Encoder-12-MultiHeadSelfAtte (None, 50, 768)     1536       Encoder-12-MultiHeadSelfAttent
__________________________________________________________________________________________
Encoder-12-FeedForward (Feed (None, 50, 768)     4722432    Encoder-12-MultiHeadSelfAttent
__________________________________________________________________________________________
Encoder-12-FeedForward-Dropo (None, 50, 768)     0          Encoder-12-FeedForward[0][0]  
__________________________________________________________________________________________
Encoder-12-FeedForward-Add ( (None, 50, 768)     0          Encoder-12-MultiHeadSelfAttent
                                                            Encoder-12-FeedForward-Dropout
__________________________________________________________________________________________
Encoder-12-FeedForward-Norm  (None, 50, 768)     1536       Encoder-12-FeedForward-Add[0][
__________________________________________________________________________________________
Extract (Extract)            (None, 768)         0          Encoder-12-FeedForward-Norm[0]
__________________________________________________________________________________________
NSP-Dense (Dense)            (None, 768)         590592     Extract[0][0]                 
__________________________________________________________________________________________
output (Dense)               (None, 1)           769        NSP-Dense[0][0]               
==========================================================================================
Total params: 109,128,193
Trainable params: 109,128,193
Non-trainable params: 0
__________________________________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="compile-model-and-begin-training"&gt;Compile model and begin training&lt;/h2&gt;
&lt;p&gt;Aus usual with Keras, before training a model, we need to compile the model. And using &lt;code&gt;fit()&lt;/code&gt;, we feed it the R arrays.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% compile(
  k_bert$AdamWarmup(decay_steps=decay_steps, 
                    warmup_steps=warmup_steps, lr=learning_rate),
  loss = &amp;#39;binary_crossentropy&amp;#39;,
  metrics = &amp;#39;accuracy&amp;#39;
)

model %&amp;gt;% fit(
  concat,
  targets,
  epochs=epochs,
  batch_size=bch_size, validation_split=0.2)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we’ve shown how we can use Keras to conveniently load, configure, and train a BERT model.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/1810.04805.pdf"&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/a&gt;&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/1706.03762.pdf"&gt;Attention Is All You Need&lt;/a&gt;&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;Attention — focuses on salient parts of input by taking a weighted average of them. 768 hidden units divided by 12 chunks and each chunk will have 64 output dimensions, afterward, the result from each chunk will be concatenated and forwarded to the next layer&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;&lt;a href="https://github.com/CyberZHG/keras-bert"&gt;Implementation of the BERT. Official pre-trained models could be loaded for feature extraction and prediction&lt;/a&gt;&lt;a href="#fnref4" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">42a74f5f925728c0f4877ebaa9e144b7</distill:md5>
      <category>Natural Language Processing</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-09-30-bert-r</guid>
      <pubDate>Mon, 30 Sep 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-09-30-bert-r/images/bert.png" medium="image" type="image/png" width="437" height="367"/>
    </item>
    <item>
      <title>So, how come we can use TensorFlow from R?</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-08-29-using-tf-from-r</link>
      <description>


&lt;p&gt;Which computer language is most closely associated with &lt;em&gt;TensorFlow&lt;/em&gt;? While on the &lt;em&gt;TensorFlow for R blog&lt;/em&gt;, we would of course like the answer to be &lt;em&gt;R&lt;/em&gt;, chances are it is Python (though TensorFlow has official &lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; bindings for C++, Swift, Javascript, Java, and Go as well).&lt;/p&gt;
&lt;p&gt;So why is it you can define a Keras model as&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_dense(units = 32, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dense(units = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(nice with &lt;code&gt;%&amp;gt;%&lt;/code&gt;s and all!) – then train and evaluate it, get predictions and plot them, all that without ever leaving R?&lt;/p&gt;
&lt;p&gt;The short answer is, you have &lt;code&gt;keras&lt;/code&gt;, &lt;code&gt;tensorflow&lt;/code&gt; and &lt;code&gt;reticulate&lt;/code&gt; installed. &lt;code&gt;reticulate&lt;/code&gt; &lt;em&gt;embeds&lt;/em&gt; a Python session &lt;em&gt;within&lt;/em&gt; the R process. A single process means a single address space: The same objects exist, and can be operated upon, regardless of whether they’re seen by R or by Python. On that basis, &lt;code&gt;tensorflow&lt;/code&gt; and &lt;code&gt;keras&lt;/code&gt; then wrap the respective Python libraries &lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; and let you write R code that, in fact, looks like R.&lt;/p&gt;
&lt;p&gt;This post first elaborates a bit on the short answer. We then go deeper into what happens in the background.&lt;/p&gt;
&lt;p&gt;One note on terminology before we jump in: On the R side, we’re making a clear distinction between the packages &lt;code&gt;keras&lt;/code&gt; and &lt;code&gt;tensorflow&lt;/code&gt;. For Python we are going to use &lt;em&gt;TensorFlow&lt;/em&gt; and &lt;em&gt;Keras&lt;/em&gt; interchangeably. Historically, these have been different, and TensorFlow was commonly thought of as one possible backend to run Keras on, besides the pioneering, now discontinued Theano, and CNTK. Standalone Keras does still &lt;a href="https://github.com/keras-team/keras"&gt;exist&lt;/a&gt;, but recent work has been, and is being, done in &lt;a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/keras"&gt;tf.keras&lt;/a&gt;. Of course, this makes Python &lt;code&gt;Keras&lt;/code&gt; a subset of Python &lt;code&gt;TensorFlow&lt;/code&gt;, but all examples in this post will use that subset so we can use both to refer to the same thing.&lt;/p&gt;
&lt;h2 id="so-keras-tensorflow-reticulate-what-are-they-for"&gt;So keras, tensorflow, reticulate, what are they for?&lt;/h2&gt;
&lt;p&gt;Firstly, nothing of this would be possible without &lt;code&gt;reticulate&lt;/code&gt;. &lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href="https://rstudio.github.io/reticulate/"&gt;reticulate&lt;/a&gt; is an R package designed to allow seemless interoperability between R and Python. If we absolutely wanted, we could construct a Keras model like this:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(reticulate)
tf &amp;lt;- import(&amp;quot;tensorflow&amp;quot;)
m &amp;lt;- tf$keras$models$Sequential()
m$`__class__`&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &amp;#39;tensorflow.python.keras.engine.sequential.Sequential&amp;#39;&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could go on adding layers …&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;m$add(tf$keras$layers$Dense(32, &amp;quot;relu&amp;quot;))
m$add(tf$keras$layers$Dense(1))
m$layers&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
&amp;lt;tensorflow.python.keras.layers.core.Dense&amp;gt;

[[2]]
&amp;lt;tensorflow.python.keras.layers.core.Dense&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But who would want to? If this were the only way, it’d be less cumbersome to directly write Python instead. Plus, as a user you’d have to know the complete Python-side module structure (now where do optimizers live, currently: &lt;code&gt;tf.keras.optimizers&lt;/code&gt;, &lt;code&gt;tf.optimizers&lt;/code&gt; …?), and keep up with all path and name changes in the Python API. &lt;a href="#fn4" class="footnote-ref" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is where &lt;code&gt;keras&lt;/code&gt; comes into play. &lt;code&gt;keras&lt;/code&gt; is where the TensorFlow-specific usability, re-usability, and convenience features live. &lt;a href="#fn5" class="footnote-ref" id="fnref5"&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; Functionality provided by &lt;code&gt;keras&lt;/code&gt; spans the whole range between boilerplate-avoidance over enabling elegant, R-like idioms to providing means of advanced feature usage. As an example for the first two, consider &lt;code&gt;layer_dense&lt;/code&gt; which, among others, converts its &lt;code&gt;units&lt;/code&gt; argument to an integer, and takes arguments in an order that allow it to be “pipe-added” to a model: Instead of&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential()
model$add(layer_dense(units = 32L))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we can just say&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential()
model %&amp;gt;% layer_dense(units = 32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While these are nice to have, there is more. Advanced functionality in (Python) Keras mostly depends on the ability to subclass objects. One example is custom callbacks. If you were using Python, you’d have to subclass &lt;code&gt;tf.keras.callbacks.Callback&lt;/code&gt;. From R, you can create an R6 class inheriting from &lt;code&gt;KerasCallback&lt;/code&gt;, like so&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;CustomCallback &amp;lt;- R6::R6Class(&amp;quot;CustomCallback&amp;quot;,
    inherit = KerasCallback,
    public = list(
      on_train_begin = function(logs) {
        # do something
      },
      on_train_end = function(logs) {
        # do something
      }
    )
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is because &lt;code&gt;keras&lt;/code&gt; defines an actual Python class, &lt;code&gt;RCallback&lt;/code&gt;, and maps your R6 class’ methods to it. Another example is &lt;a href="https://tensorflow.rstudio.com/keras/articles/custom_models.html"&gt;custom models&lt;/a&gt;, introduced on this blog &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-07-30-attention-layer/"&gt;about a year ago&lt;/a&gt;. These models can be trained with custom training loops. In R, you use &lt;code&gt;keras_model_custom&lt;/code&gt; to create one, for example, like this:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;m &amp;lt;- keras_model_custom(name = &amp;quot;mymodel&amp;quot;, function(self) {
  self$dense1 &amp;lt;- layer_dense(units = 32, activation = &amp;quot;relu&amp;quot;)
  self$dense2 &amp;lt;- layer_dense(units = 10, activation = &amp;quot;softmax&amp;quot;)
  
  function(inputs, mask = NULL) {
    self$dense1(inputs) %&amp;gt;%
      self$dense2()
  }
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, &lt;code&gt;keras&lt;/code&gt; will make sure an actual Python object is created which subclasses &lt;code&gt;tf.keras.Model&lt;/code&gt; and when called, runs the above anonymous &lt;code&gt;function()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;So that’s &lt;code&gt;keras&lt;/code&gt;. What about the &lt;code&gt;tensorflow&lt;/code&gt; package? As a user you only need it when you have to do advanced stuff, like configure TensorFlow device usage or (in TF 1.x) access elements of the &lt;code&gt;Graph&lt;/code&gt; or the &lt;code&gt;Session&lt;/code&gt;. Internally, it is used by &lt;code&gt;keras&lt;/code&gt; heavily. Essential internal functionality includes, e.g., implementations of S3 methods, like &lt;code&gt;print&lt;/code&gt;, &lt;code&gt;[&lt;/code&gt; or &lt;code&gt;+&lt;/code&gt;, on &lt;code&gt;Tensor&lt;/code&gt;s, so you can operate on them like on R vectors.&lt;/p&gt;
&lt;p&gt;Now that we know what each of the packages is “for”, let’s dig deeper into what makes this possible.&lt;/p&gt;
&lt;h2 id="show-me-the-magic-reticulate"&gt;Show me the magic: reticulate&lt;/h2&gt;
&lt;p&gt;Instead of exposing the topic top-down, we follow a by-example approach, building up complexity as we go. We’ll have three scenarios.&lt;/p&gt;
&lt;p&gt;First, we assume we already have a Python object (that has been constructed in whatever way) and need to convert that to R. Then, we’ll investigate how we can create a Python object, calling its constructor. Finally, we go the other way round: We ask how we can pass an R function to Python for later usage.&lt;/p&gt;
&lt;h3 id="scenario-1-r-to-python-conversion"&gt;Scenario 1: R-to-Python conversion&lt;/h3&gt;
&lt;p&gt;Let’s assume we have created a Python object in the global namespace, like this:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;py_run_string(&amp;quot;x = 1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So: There is a variable, called x, with value 1, living in Python world. Now how do we bring this thing into R?&lt;/p&gt;
&lt;p&gt;We know the main entry point to conversion is &lt;code&gt;py_to_r&lt;/code&gt;, defined as a generic in &lt;code&gt;conversion.R&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;py_to_r &amp;lt;- function(x) {
  ensure_python_initialized()
  UseMethod(&amp;quot;py_to_r&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;… with the default implementation calling a function named &lt;code&gt;py_ref_to_r&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;#&amp;#39; @export
py_to_r.default &amp;lt;- function(x) {
  [...]
  x &amp;lt;- py_ref_to_r(x)
  [...]
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To find out more about what is going on, debugging on the R level won’t get us far. We start &lt;code&gt;gdb&lt;/code&gt; so we can set breakpoints in C++ functions: &lt;a href="#fn6" class="footnote-ref" id="fnref6"&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ R -d gdb

GNU gdb (GDB) Fedora 8.3-6.fc30
[... some more gdb saying hello ...]
Reading symbols from /usr/lib64/R/bin/exec/R...
Reading symbols from /usr/lib/debug/usr/lib64/R/bin/exec/R-3.6.0-1.fc30.x86_64.debug...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now start R, load &lt;code&gt;reticulate&lt;/code&gt;, and execute the assignment we’re going to presuppose:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(gdb) run
Starting program: /usr/lib64/R/bin/exec/R 
[...]
R version 3.6.0 (2019-04-26) -- &amp;quot;Planting of a Tree&amp;quot;
Copyright (C) 2019 The R Foundation for Statistical Computing
[...]
&amp;gt; library(reticulate)
&amp;gt; py_run_string(&amp;quot;x = 1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So that set up our scenario, the Python object (named &lt;code&gt;x&lt;/code&gt;) we want to convert to R. Now, use Ctrl-C to “escape” to &lt;code&gt;gdb&lt;/code&gt;, set a breakpoint in &lt;code&gt;py_to_r&lt;/code&gt; and type &lt;code&gt;c&lt;/code&gt; to get back to R:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(gdb) b py_to_r
Breakpoint 1 at 0x7fffe48315d0 (2 locations)
(gdb) c&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now what are we going to see when we access that &lt;code&gt;x&lt;/code&gt;?&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; py$x

Thread 1 &amp;quot;R&amp;quot; hit Breakpoint 1, 0x00007fffe48315d0 in py_to_r(libpython::_object*, bool)@plt () from /home/key/R/x86_64-redhat-linux-gnu-library/3.6/reticulate/libs/reticulate.so&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the relevant (for our investigation) frames of the backtrace:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Thread 1 &amp;quot;R&amp;quot; hit Breakpoint 3, 0x00007fffe48315d0 in py_to_r(libpython::_object*, bool)@plt () from /home/key/R/x86_64-redhat-linux-gnu-library/3.6/reticulate/libs/reticulate.so
(gdb) bt
#0  0x00007fffe48315d0 in py_to_r(libpython::_object*, bool)@plt () from /home/key/R/x86_64-redhat-linux-gnu-library/3.6/reticulate/libs/reticulate.so
#1  0x00007fffe48588a0 in py_ref_to_r_with_convert (x=..., convert=true) at reticulate_types.h:32
#2  0x00007fffe4858963 in py_ref_to_r (x=...) at /home/key/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include/RcppCommon.h:120
#3  0x00007fffe483d7a9 in _reticulate_py_ref_to_r (xSEXP=0x55555daa7e50) at /home/key/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include/Rcpp/as.h:151
...
...
#14 0x00007ffff7cc5fc7 in Rf_usemethod (generic=0x55555757ce70 &amp;quot;py_to_r&amp;quot;, obj=obj@entry=0x55555daa7e50, call=call@entry=0x55555a0fe198, args=args@entry=0x55555557c4e0, 
    rho=rho@entry=0x55555dab2ed0, callrho=0x55555dab48d8, defrho=0x5555575a4068, ans=0x7fffffff69e8) at objects.c:486&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ve removed a few intermediate frames related to (R-level) method dispatch.&lt;/p&gt;
&lt;p&gt;As we already saw in the source code, &lt;code&gt;py_to_r.default&lt;/code&gt; will delegate to a method called &lt;code&gt;py_ref_to_r&lt;/code&gt;, which we see appears in #2. But what is &lt;code&gt;_reticulate_py_ref_to_r&lt;/code&gt; in #3, the frame just below? Here is where the magic, unseen by the user, begins.&lt;/p&gt;
&lt;p&gt;Let’s look at this from a bird’s eye’s view. To translate an object from one language to another, we need to find a common ground, that is, a third language “spoken” by both of them. In the case of R and Python (as well as in a lot of other cases) this will be C / C++. So assuming we are going to write a C function to talk to Python, how can we use this function in R?&lt;/p&gt;
&lt;p&gt;While R users have the ability to call into C directly, using &lt;code&gt;.Call&lt;/code&gt; or &lt;code&gt;.External&lt;/code&gt; &lt;a href="#fn7" class="footnote-ref" id="fnref7"&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;, this is made much more convenient by &lt;a href="https://cran.r-project.org/web/packages/Rcpp/index.html"&gt;Rcpp&lt;/a&gt; &lt;a href="#fn8" class="footnote-ref" id="fnref8"&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;: You just write your C++ function, and Rcpp takes care of compilation and provides the glue code necessary to call this function from R.&lt;/p&gt;
&lt;p&gt;So &lt;code&gt;py_ref_to_r&lt;/code&gt; really is written in C++:&lt;/p&gt;
&lt;pre class="cpp"&gt;&lt;code&gt;// [[Rcpp::export]]
SEXP py_ref_to_r(PyObjectRef x) {
  return py_ref_to_r_with_convert(x, x.convert());
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;but the comment &lt;code&gt;// [[Rcpp::export]]&lt;/code&gt; tells Rcpp to generate an R wrapper, &lt;code&gt;py_ref_to_R&lt;/code&gt;, that itself calls a C++ wrapper, &lt;code&gt;_reticulate_py_ref_to_r&lt;/code&gt; …&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;py_ref_to_r &amp;lt;- function(x) {
  .Call(`_reticulate_py_ref_to_r`, x)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which finally wraps the “real” thing, the C++ function &lt;code&gt;py_ref_to_R&lt;/code&gt; we saw above.&lt;/p&gt;
&lt;p&gt;Via &lt;code&gt;py_ref_to_r_with_convert&lt;/code&gt; in #1, a one-liner that extracts an object’s “convert” feature (see below)&lt;/p&gt;
&lt;pre class="cpp"&gt;&lt;code&gt;// [[Rcpp::export]]
SEXP py_ref_to_r_with_convert(PyObjectRef x, bool convert) {
  return py_to_r(x, convert);
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we finally arrive at &lt;code&gt;py_to_r&lt;/code&gt; in #0.&lt;/p&gt;
&lt;p&gt;Before we look at that, let’s contemplate that C/C++ “bridge” from the other side - Python. While strictly, Python is a language specification, its reference implementation is CPython, with a core written in C and much more functionality built on top in Python. In CPython, every Python object (including integers or other numeric types) is a &lt;code&gt;PyObject&lt;/code&gt;. &lt;code&gt;PyObject&lt;/code&gt;s are allocated through and operated on using pointers; most C API functions return a pointer to one, &lt;code&gt;PyObject *&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;So this is what we expect to work with, from R. What then is &lt;code&gt;PyObjectRef&lt;/code&gt; doing in &lt;code&gt;py_ref_to_r&lt;/code&gt;? &lt;code&gt;PyObjectRef&lt;/code&gt; is not part of the C API, it is part of the functionality introduced by &lt;code&gt;reticulate&lt;/code&gt; to manage Python objects. Its main purpose is to make sure the Python object is automatically cleaned up when the R object (an &lt;code&gt;Rcpp::Environment&lt;/code&gt;) goes out of scope. Why use an R environment to wrap the Python-level pointer? This is because R environments can have finalizers: functions that are called before objects are garbage collected. We use this R-level finalizer to ensure the Python-side object gets finalized as well:&lt;/p&gt;
&lt;pre class="cpp"&gt;&lt;code&gt;Rcpp::RObject xptr = R_MakeExternalPtr((void*) object, R_NilValue, R_NilValue);
R_RegisterCFinalizer(xptr, python_object_finalize);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;python_object_finalize&lt;/code&gt; is interesting, as it tells us something crucial about Python – about CPython, to be precise: To find out if an object is still needed, or could be garbage collected, it uses reference counting, thus placing on the user the burden of correctly incrementing and decrementing references according to language semantics.&lt;/p&gt;
&lt;pre class="cpp"&gt;&lt;code&gt;inline void python_object_finalize(SEXP object) {
  PyObject* pyObject = (PyObject*)R_ExternalPtrAddr(object);
  if (pyObject != NULL)
    Py_DecRef(pyObject);
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Resuming on &lt;code&gt;PyObjectRef&lt;/code&gt;, note that it also stores the “convert” feature of the Python object, used to determine whether that object should be converted to R automatically.&lt;/p&gt;
&lt;p&gt;Back to &lt;code&gt;py_to_r&lt;/code&gt;. This one now really gets to work with (a pointer to the) Python object,&lt;/p&gt;
&lt;pre class="cpp"&gt;&lt;code&gt;SEXP py_to_r(PyObject* x, bool convert) {
  //...
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and – but wait. Didn’t &lt;code&gt;py_ref_to_r_with_convert&lt;/code&gt; pass it a &lt;code&gt;PyObjectRef&lt;/code&gt;? So how come it receives a &lt;code&gt;PyObject&lt;/code&gt; instead? This is because &lt;code&gt;PyObjectRef&lt;/code&gt; inherits from &lt;code&gt;Rcpp::Environment&lt;/code&gt;, and its implicit conversion operator is used to extract the Python object from the &lt;code&gt;Environment&lt;/code&gt;. Concretely, that operator tells the compiler that a &lt;code&gt;PyObjectRef&lt;/code&gt; can be used as though it were a &lt;code&gt;PyObject*&lt;/code&gt; in some concepts, and the associated code specifies how to convert from &lt;code&gt;PyObjectRef&lt;/code&gt; to &lt;code&gt;PyObject*&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="cpp"&gt;&lt;code&gt;operator PyObject*() const {
  return get();
}

PyObject* get() const {
  SEXP pyObject = getFromEnvironment(&amp;quot;pyobj&amp;quot;);
  if (pyObject != R_NilValue) {
    PyObject* obj = (PyObject*)R_ExternalPtrAddr(pyObject);
    if (obj != NULL)
      return obj;
  }
  Rcpp::stop(&amp;quot;Unable to access object (object is from previous session and is now invalid)&amp;quot;);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So &lt;code&gt;py_to_r&lt;/code&gt; works with a pointer to a Python object and returns what we want, an R object (a &lt;code&gt;SEXP&lt;/code&gt;). The function checks for the type of the object, and then uses Rcpp to construct the adequate R object, in our case, an integer:&lt;/p&gt;
&lt;pre class="cpp"&gt;&lt;code&gt;
else if (scalarType == INTSXP)
  return IntegerVector::create(PyInt_AsLong(x));&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For other objects, typically there’s more action required; but essentially, the function is “just” a big &lt;code&gt;if&lt;/code&gt;-&lt;code&gt;else&lt;/code&gt; tree.&lt;/p&gt;
&lt;p&gt;So this was scenario 1: converting a Python object to R. Now in scenario 2, we assume we still need to create that Python object.&lt;/p&gt;
&lt;h3 id="scenario-2"&gt;Scenario 2:&lt;/h3&gt;
&lt;p&gt;As this scenario is considerably more complex than the previous one, we will explicitly concentrate on some aspects and leave out others. Importantly, we’ll not go into module loading, which would deserve separate treatment of its own. Instead, we try to shed a light on what’s involved using a concrete example: the ubiquitous, in &lt;code&gt;keras&lt;/code&gt; code, &lt;code&gt;keras_model_sequential()&lt;/code&gt;. All this R function does is&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;function(layers = NULL, name = NULL) {
  keras$models$Sequential(layers = layers, name = name)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How can &lt;code&gt;keras$models$Sequential()&lt;/code&gt; give us an object? When in Python, you run the equivalent&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;tf.keras.models.Sequential()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;this calls the constructor, that is, the &lt;code&gt;__init__&lt;/code&gt; method of the class:&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;class Sequential(training.Model):
  def __init__(self, layers=None, name=None):
    # ...
  # ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this time, before – as always, in the end – getting an R object back from Python, we need to call that constructor, that is, a Python &lt;em&gt;callable&lt;/em&gt;. (Python &lt;code&gt;callable&lt;/code&gt;s subsume functions, constructors, and objects created from a class that has a &lt;code&gt;call&lt;/code&gt; method.)&lt;/p&gt;
&lt;p&gt;So when &lt;code&gt;py_to_r&lt;/code&gt;, inspecting its argument’s type, sees it is a Python callable (wrapped in a &lt;code&gt;PyObjectRef&lt;/code&gt;, the &lt;code&gt;reticulate&lt;/code&gt;-specific subclass of &lt;code&gt;Rcpp::Environment&lt;/code&gt; we talked about above), it wraps it (the &lt;code&gt;PyObjectRef&lt;/code&gt;) in an R function, using Rcpp:&lt;/p&gt;
&lt;pre class="cpp"&gt;&lt;code&gt;Rcpp::Function f = py_callable_as_function(pyFunc, convert);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The cpython-side action starts when &lt;code&gt;py_callable_as_function&lt;/code&gt; then calls &lt;code&gt;py_call_impl&lt;/code&gt;. &lt;code&gt;py_call_impl&lt;/code&gt; executes the actual call and returns an R object, a &lt;code&gt;SEXP&lt;/code&gt;. Now you may be asking, how does the Python runtime know it shouldn’t deallocate that object, now that its work is done? This is taken of by the same &lt;code&gt;PyObjectRef&lt;/code&gt; class used to wrap instances of &lt;code&gt;PyObject *&lt;/code&gt;: It can wrap &lt;code&gt;SEXP&lt;/code&gt;s as well.&lt;/p&gt;
&lt;p&gt;While a lot more could be said about what happens before we finally get to work with that &lt;code&gt;Sequential&lt;/code&gt; model from R, let’s stop here and look at our third scenario.&lt;/p&gt;
&lt;h3 id="scenario-3-calling-r-from-python"&gt;Scenario 3: Calling R from Python&lt;/h3&gt;
&lt;p&gt;Not surprisingly, sometimes we need to pass R callbacks to Python. An example are R data generators that can be used with &lt;code&gt;keras&lt;/code&gt; models &lt;a href="#fn9" class="footnote-ref" id="fnref9"&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In general, for R objects to be passed to Python, the process is somewhat opposite to what we described in example 1. Say we type:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;py$a &amp;lt;- 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This assigns &lt;code&gt;1&lt;/code&gt; to a variable &lt;code&gt;a&lt;/code&gt; in the python main module. To enable assignment, &lt;code&gt;reticulate&lt;/code&gt; provides an implementation of the S3 generic &lt;code&gt;$&amp;lt;-&lt;/code&gt;, &lt;code&gt;$&amp;lt;-.python.builtin.object&lt;/code&gt;, which delegates to &lt;code&gt;py_set_attr&lt;/code&gt;, which then calls &lt;code&gt;py_set_attr_impl&lt;/code&gt; – yet another C++ function exported via Rcpp.&lt;/p&gt;
&lt;p&gt;Let’s focus on a different aspect here, though. A prerequisite for the assignment to happen is getting that &lt;code&gt;1&lt;/code&gt; converted to Python. (We’re using the simplest possible example, obviously; but you can imagine this getting a lot more complex if the object isn’t a simple number).&lt;/p&gt;
&lt;p&gt;For our “minimal example”, we see a stacktrace like the following&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#0 0x00007fffe4832010 in r_to_py_cpp(Rcpp::RObject_Impl&amp;lt;Rcpp::PreserveStorage&amp;gt;, bool)@plt () from /home/key/R/x86_64-redhat-linux-gnu-library/3.6/reticulate/libs/reticulate.so
#1  0x00007fffe4854f38 in r_to_py_impl (object=..., convert=convert@entry=true) at /home/key/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include/RcppCommon.h:120
#2  0x00007fffe48418f3 in _reticulate_r_to_py_impl (objectSEXP=0x55555ec88fa8, convertSEXP=&amp;lt;optimized out&amp;gt;) at /home/key/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include/Rcpp/as.h:151
...
#12 0x00007ffff7cc5c03 in dispatchMethod (sxp=0x55555d0cf1a0, dotClass=&amp;lt;optimized out&amp;gt;, cptr=cptr@entry=0x7ffffffeaae0, method=method@entry=0x55555bfe06c0, 
    generic=0x555557634458 &amp;quot;r_to_py&amp;quot;, rho=0x55555d1d98a8, callrho=0x5555555af2d0, defrho=0x555557947430, op=&amp;lt;optimized out&amp;gt;, op=&amp;lt;optimized out&amp;gt;) at objects.c:436
#13 0x00007ffff7cc5fc7 in Rf_usemethod (generic=0x555557634458 &amp;quot;r_to_py&amp;quot;, obj=obj@entry=0x55555ec88fa8, call=call@entry=0x55555c0317b8, args=args@entry=0x55555557cc60, 
    rho=rho@entry=0x55555d1d98a8, callrho=0x5555555af2d0, defrho=0x555557947430, ans=0x7ffffffe9928) at objects.c:486&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Whereas &lt;code&gt;r_to_py&lt;/code&gt; is a generic (like &lt;code&gt;py_to_r&lt;/code&gt; above), &lt;code&gt;r_to_py_impl&lt;/code&gt; is wrapped by Rcpp and &lt;code&gt;r_to_py_cpp&lt;/code&gt; is a C++ function that branches on the type of the object – basically the counterpart of the C++ &lt;code&gt;r_to_py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In addition to that general process, there is more going on when we call an R function from Python. As Python doesn’t “speak” R, we need to wrap the R function in CPython - basically, we are extending Python here! How to do this is described in the official &lt;a href="https://docs.python.org/3/extending/index.html#extending-index"&gt;Extending Python Guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In official terms, what &lt;code&gt;reticulate&lt;/code&gt; does it &lt;em&gt;embed&lt;/em&gt; and &lt;em&gt;extend&lt;/em&gt; Python. Embed, because it lets you use Python from inside R. Extend, because to enable Python to call back into R it needs to wrap R functions in C, so Python can understand them.&lt;/p&gt;
&lt;p&gt;As part of the former, the desired Python is loaded (&lt;code&gt;Py_Initialize()&lt;/code&gt;); as part of the latter, two functions are defined in a new module named &lt;code&gt;rpycall&lt;/code&gt;, that will be loaded when Python itself is loaded.&lt;/p&gt;
&lt;pre class="cpp"&gt;&lt;code&gt;PyImport_AppendInittab(&amp;quot;rpycall&amp;quot;, &amp;amp;initializeRPYCall);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These methods are &lt;code&gt;call_r_function&lt;/code&gt;, used by default, and &lt;code&gt;call_python_function_on_main_thread&lt;/code&gt;, used in cases where we need to make sure the R function is called on the main thread:&lt;/p&gt;
&lt;pre class="cpp"&gt;&lt;code&gt;PyMethodDef RPYCallMethods[] = {
  { &amp;quot;call_r_function&amp;quot;, (PyCFunction)call_r_function,
    METH_VARARGS | METH_KEYWORDS, &amp;quot;Call an R function&amp;quot; },
  { &amp;quot;call_python_function_on_main_thread&amp;quot;, (PyCFunction)call_python_function_on_main_thread,
    METH_VARARGS | METH_KEYWORDS, &amp;quot;Call a Python function on the main thread&amp;quot; },
  { NULL, NULL, 0, NULL }
};&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;call_python_function_on_main_thread&lt;/code&gt; is especially interesting. The R runtime is single-threaded; while the CPython implementation of Python effectively is as well, due to the &lt;a href="https://en.wikipedia.org/wiki/Global_interpreter_lock"&gt;Global Interpreter Lock&lt;/a&gt;, this is not automatically the case when other implementations are used, or C is used directly. So &lt;code&gt;call_python_function_on_main_thread&lt;/code&gt; makes sure that unless we can execute on the main thread, we wait.&lt;/p&gt;
&lt;p&gt;That’s it for our three “spotlights on &lt;code&gt;reticulate&lt;/code&gt;”.&lt;/p&gt;
&lt;h2 id="wrapup"&gt;Wrapup&lt;/h2&gt;
&lt;p&gt;It goes without saying that there’s a lot about &lt;code&gt;reticulate&lt;/code&gt; we didn’t cover in this article, such as memory management, initialization, or specifics of data conversion. Nonetheless, we hope we were able to shed a bit of light on the &lt;em&gt;magic&lt;/em&gt; involved in calling TensorFlow from R.&lt;/p&gt;
&lt;p&gt;R is a concise and elegant language, but to a high degree its power comes from its packages, including those that allow you to call into, and interact with, the outside world, such as deep learning frameworks or distributed processing engines. In this post, it was a special pleasure to focus on a central building block that makes much of this possible: &lt;code&gt;reticulate&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;or semi-official, dependent on the language; see the &lt;a href="https://www.tensorflow.org/api_docs"&gt;TensorFlow website&lt;/a&gt; to track status&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;but see the “note on terminology” below&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;Not without &lt;code&gt;Rcpp&lt;/code&gt; either, but we’ll save that for the “Digging deeper” section.&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;of which there are many, currently, accompanying the substantial changes related to the introduction of TF 2.0.&lt;a href="#fnref4" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn5"&gt;&lt;p&gt;It goes without saying that as a generic mediator between R and Python, &lt;code&gt;reticulate&lt;/code&gt; can not provide convenience features for all R packages that use it.&lt;a href="#fnref5" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn6"&gt;&lt;p&gt;For a very nice introduction to debugging R with a debugger like &lt;code&gt;gdb&lt;/code&gt;, see &lt;a href="http://kevinushey.github.io/blog/2015/04/13/debugging-with-lldb/."&gt;Kevin Ushey’s “Debugging with LLDB”&lt;/a&gt; That post uses &lt;code&gt;lldb&lt;/code&gt; which is the standard debugger on Macintosh, while here we’re using &lt;code&gt;gdb&lt;/code&gt; on linux; but mostly the behaviors are very similar.&lt;a href="#fnref6" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn7"&gt;&lt;p&gt;For a nice introduction, see &lt;a href="http://adv-r.had.co.nz/C-interface.html"&gt;version 1 of Advanced R&lt;/a&gt;.&lt;a href="#fnref7" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn8"&gt;&lt;p&gt;Not a copy-paste error: For a nice introduction, see &lt;a href="https://adv-r.hadley.nz/rcpp.html"&gt;version 2 of Advanced R&lt;/a&gt;.&lt;a href="#fnref8" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn9"&gt;&lt;p&gt;For performance reasons, it is often advisable to use &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/introduction.html"&gt;tfdatasets&lt;/a&gt; instead.&lt;a href="#fnref9" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">01015b42073172ddef579ec69105d187</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Meta</category>
      <category>Concepts</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-08-29-using-tf-from-r</guid>
      <pubDate>Thu, 29 Aug 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-08-29-using-tf-from-r/images/thumb.png" medium="image" type="image/png" width="739" height="516"/>
    </item>
    <item>
      <title>Image segmentation with U-Net</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Falbel</dc:creator>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-08-23-unet</link>
      <description>


&lt;p&gt;Sure, it is nice when I have a picture of some object, and a neural network can tell me what kind of object that is. More realistically, there might be several salient objects in that picture, and it tells me what they are, and where they are. The latter task (known as &lt;em&gt;object detection&lt;/em&gt;) seems especially prototypical of contemporary AI applications that at the same time are intellectually fascinating and ethically questionable. It’s different with the subject of this post: Successful &lt;em&gt;image segmentation&lt;/em&gt; has a lot of undeniably useful applications. For example, it is a sine qua non in medicine, neuroscience, biology and other life sciences.&lt;/p&gt;
&lt;p&gt;So what, technically, is image segmentation, and how can we train a neural network to do it?&lt;/p&gt;
&lt;h2 id="image-segmentation-in-a-nutshell"&gt;Image segmentation in a nutshell&lt;/h2&gt;
&lt;p&gt;Say we have an image with a bunch of cats in it. In &lt;em&gt;classification&lt;/em&gt;, the question is “what’s that?”, and the answer we want to hear is: “cat”. In &lt;em&gt;object detection&lt;/em&gt;, we again ask “what’s that”, but now that “what” is implicitly plural, and we expect an answer like “there’s a cat, a cat, and a cat, and they’re here, here, and here” (imagine the network pointing, by means of drawing &lt;em&gt;bounding boxes&lt;/em&gt;, i.e., rectangles around the detected objects). In &lt;em&gt;segmentation&lt;/em&gt;, we want more: We want the whole image covered by “boxes” – which aren’t boxes anymore, but unions of pixel-size “boxlets” – or put differently: &lt;strong&gt;We want the network to label every single pixel in the image.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Here’s an example from the paper we’re going to talk about in a second. On the left is the input image (HeLa cells), next up is the ground truth, and third is the learned segmentation mask.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-08-23-unet/images/mask.png" alt="Example segmentation from Ronneberger et al. 2015." width="493" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-1)Example segmentation from Ronneberger et al. 2015.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Technically, a distinction is made between &lt;em&gt;class segmentation&lt;/em&gt; and &lt;em&gt;instance segmentation&lt;/em&gt;. In class segmentation, referring to the “bunch of cats” example, there are two possible labels: Every pixel is either “cat” or “not cat”. Instance segmentation is more difficult: Here every cat gets their own label. (As an aside, why should that be more difficult? Presupposing human-like cognition, it wouldn’t be – if I have the concept of a cat, instead of just “cattiness”, I “see” there are two cats, not one. But depending on what a specific neural network relies on most – texture, color, isolated parts – those tasks may differ a lot in difficulty.)&lt;/p&gt;
&lt;p&gt;The network architecture used in this post is adequate for &lt;em&gt;class segmentation&lt;/em&gt; tasks and should be applicable to a vast number of practical, scientific as well as non-scientific applications. Speaking of network architecture, how should it look?&lt;/p&gt;
&lt;h2 id="introducing-u-net"&gt;Introducing U-Net&lt;/h2&gt;
&lt;p&gt;Given their success in image classification, can’t we just use a classic architecture like &lt;em&gt;Inception V[n]&lt;/em&gt;, &lt;em&gt;ResNet&lt;/em&gt;, &lt;em&gt;ResNext&lt;/em&gt; … , whatever? The problem is, our task at hand – labeling every pixel – does not fit so well with the classic idea of a CNN. With convnets, the idea is to apply successive layers of convolution and pooling to build up feature maps of decreasing granularity, to finally arrive at an abstract level where we just say: “yep, a cat”. The counterpart being, we lose detail information: To the final classification, it does not matter whether the five pixels in the top-left area are black or white.&lt;/p&gt;
&lt;p&gt;In practice, the classic architectures use (max) pooling or convolutions with &lt;code&gt;stride&lt;/code&gt; &amp;gt; 1 to achieve those successive abstractions – necessarily resulting in decreased spatial resolution. So how can we use a convnet and still preserve detail information? In their 2015 paper &lt;em&gt;U-Net: Convolutional Networks for Biomedical Image Segmentation&lt;/em&gt; &lt;span class="citation"&gt;(Ronneberger, Fischer, and Brox 2015)&lt;/span&gt;, Olaf Ronneberger et al. came up with what four years later, in 2019, is still the most popular approach. (Which is to say something, four years being a long time, in deep learning.)&lt;/p&gt;
&lt;p&gt;The idea is stunningly simple. While successive encoding (convolution / max pooling) steps, as usual, reduce resolution, the subsequent decoding – we have to arrive at an output of size same as the input, as we want to label every pixel! – does not simply upsample from the most compressed layer. Instead, during upsampling, at every step we feed in information from the corresponding, in resolution, layer in the downsizing chain.&lt;/p&gt;
&lt;p&gt;For U-Net, really a picture says more than many words:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-08-23-unet/images/unet.png" alt="U-Net architecture from Ronneberger et al. 2015." width="700" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-2)U-Net architecture from Ronneberger et al. 2015.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;At each upsampling stage we &lt;em&gt;concatenate&lt;/em&gt; the output from the previous layer with that from its counterpart in the compression stage. The final output is a &lt;em&gt;mask&lt;/em&gt; of size the original image, obtained via 1x1-convolution; no final dense layer is required, instead the output layer is just a convolutional layer with a single filter.&lt;/p&gt;
&lt;p&gt;Now let’s actually train a U-Net. We’re going to use the &lt;a href="https://github.com/r-tensorflow/unet"&gt;&lt;code&gt;unet&lt;/code&gt; package&lt;/a&gt; that lets you create a well-performing model in a single line:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;remotes::install_github(&amp;quot;r-tensorflow/unet&amp;quot;)
library(unet)

# takes additional parameters, including number of downsizing blocks, 
# number of filters to start with, and number of classes to identify
# see ?unet for more info
model &amp;lt;- unet(input_shape = c(128, 128, 3))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we have a model, and it looks like we’ll be wanting to feed it 128x128 RGB images. Now how do we get these images?&lt;/p&gt;
&lt;h2 id="the-data"&gt;The data&lt;/h2&gt;
&lt;p&gt;To illustrate how applications arise even outside the area of medical research, we’ll use as an example the Kaggle &lt;a href="https://www.kaggle.com/c/carvana-image-masking-challenge/overview"&gt;Carvana Image Masking Challenge&lt;/a&gt;. The task is to create a segmentation mask separating cars from background. For our current purpose, we only need &lt;code&gt;train.zip&lt;/code&gt; and &lt;code&gt;train_mask.zip&lt;/code&gt; from the &lt;a href="https://www.kaggle.com/c/6927/download-all"&gt;archive provided for download&lt;/a&gt;. In the following, we assume those have been extracted to a subdirectory called &lt;code&gt;data-raw&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let’s first take a look at some images and their associated segmentation masks.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# libraries we&amp;#39;re going to need later
library(keras)
library(tfdatasets)
library(tidyverse)
library(rsample)
library(reticulate)

images &amp;lt;- tibble(
  img = list.files(here::here(&amp;quot;data-raw/train&amp;quot;), full.names = TRUE),
  mask = list.files(here::here(&amp;quot;data-raw/train_masks&amp;quot;), full.names = TRUE)
  ) %&amp;gt;% 
  sample_n(2) %&amp;gt;% 
  map(. %&amp;gt;% magick::image_read() %&amp;gt;% magick::image_resize(&amp;quot;128x128&amp;quot;))

out &amp;lt;- magick::image_append(c(
  magick::image_append(images$img, stack = TRUE), 
  magick::image_append(images$mask, stack = TRUE)
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-08-23-unet/images/examples.png" width="350" /&gt;&lt;/p&gt;
&lt;p&gt;The photos are RGB-space JPEGs, while the masks are black-and-white GIFs.&lt;/p&gt;
&lt;p&gt;We split the data into a training and a validation set. We’ll use the latter to monitor generalization performance during training.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;data &amp;lt;- tibble(
  img = list.files(here::here(&amp;quot;data-raw/train&amp;quot;), full.names = TRUE),
  mask = list.files(here::here(&amp;quot;data-raw/train_masks&amp;quot;), full.names = TRUE)
)

data &amp;lt;- initial_split(data, prop = 0.8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To feed the data to the network, we’ll use &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/introduction.html"&gt;tfdatasets&lt;/a&gt;. All preprocessing will end up in a simple pipeline, but we’ll first go over the required actions step-by-step.&lt;/p&gt;
&lt;h2 id="preprocessing-pipeline"&gt;Preprocessing pipeline&lt;/h2&gt;
&lt;p&gt;The first step is to read in the images, making use of the appropriate functions in &lt;code&gt;tf$image&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;training_dataset &amp;lt;- training(data) %&amp;gt;%  
  tensor_slices_dataset() %&amp;gt;% 
  dataset_map(~.x %&amp;gt;% list_modify(
    # decode_jpeg yields a 3d tensor of shape (1280, 1918, 3)
    img = tf$image$decode_jpeg(tf$io$read_file(.x$img)),
    # decode_gif yields a 4d tensor of shape (1, 1280, 1918, 3),
    # so we remove the unneeded batch dimension and all but one 
    # of the 3 (identical) channels
    mask = tf$image$decode_gif(tf$io$read_file(.x$mask))[1,,,][,,1,drop=FALSE]
  ))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While constructing a preprocessing pipeline, it’s very useful to check intermediate results. It’s easy to do using &lt;code&gt;reticulate::as_iterator&lt;/code&gt; on the dataset:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;example &amp;lt;- training_dataset %&amp;gt;% as_iterator() %&amp;gt;% iter_next()
example&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$img
tf.Tensor(
[[[243 244 239]
  [243 244 239]
  [243 244 239]
  ...
 ...
  ...
  [175 179 178]
  [175 179 178]
  [175 179 178]]], shape=(1280, 1918, 3), dtype=uint8)

$mask
tf.Tensor(
[[[0]
  [0]
  [0]
  ...
 ...
  ...
  [0]
  [0]
  [0]]], shape=(1280, 1918, 1), dtype=uint8)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While the &lt;code&gt;uint8&lt;/code&gt; datatype makes RGB values easy to read for humans, the network is going to expect floating point numbers. The following code converts its input and additionally, scales values to the interval [0,1):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;training_dataset &amp;lt;- training_dataset %&amp;gt;% 
  dataset_map(~.x %&amp;gt;% list_modify(
    img = tf$image$convert_image_dtype(.x$img, dtype = tf$float32),
    mask = tf$image$convert_image_dtype(.x$mask, dtype = tf$float32)
  ))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To reduce computational cost, we resize the images to size &lt;code&gt;128x128&lt;/code&gt;. This will change the aspect ratio and thus, distort the images, but is not a problem with the given dataset.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;training_dataset &amp;lt;- training_dataset %&amp;gt;% 
  dataset_map(~.x %&amp;gt;% list_modify(
    img = tf$image$resize(.x$img, size = shape(128, 128)),
    mask = tf$image$resize(.x$mask, size = shape(128, 128))
  ))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, it’s well known that in deep learning, data augmentation is paramount. For segmentation, there’s one thing to consider, which is whether a transformation needs to be applied to the mask as well – this would be the case for e.g. rotations, or flipping. Here, results will be good enough applying just transformations that preserve positions:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;random_bsh &amp;lt;- function(img) {
  img %&amp;gt;% 
    tf$image$random_brightness(max_delta = 0.3) %&amp;gt;% 
    tf$image$random_contrast(lower = 0.5, upper = 0.7) %&amp;gt;% 
    tf$image$random_saturation(lower = 0.5, upper = 0.7) %&amp;gt;% 
    # make sure we still are between 0 and 1
    tf$clip_by_value(0, 1) 
}

training_dataset &amp;lt;- training_dataset %&amp;gt;% 
  dataset_map(~.x %&amp;gt;% list_modify(
    img = random_bsh(.x$img)
  ))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, we can use &lt;code&gt;as_iterator&lt;/code&gt; to see what these transformations do to our images:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;example &amp;lt;- training_dataset %&amp;gt;% as_iterator() %&amp;gt;% iter_next()
example$img %&amp;gt;% as.array() %&amp;gt;% as.raster() %&amp;gt;% plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-08-23-unet/images/transformed.png" width="400" /&gt;&lt;/p&gt;
&lt;p&gt;Here’s the complete preprocessing pipeline.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;create_dataset &amp;lt;- function(data, train, batch_size = 32L) {
  
  dataset &amp;lt;- data %&amp;gt;% 
    tensor_slices_dataset() %&amp;gt;% 
    dataset_map(~.x %&amp;gt;% list_modify(
      img = tf$image$decode_jpeg(tf$io$read_file(.x$img)),
      mask = tf$image$decode_gif(tf$io$read_file(.x$mask))[1,,,][,,1,drop=FALSE]
    )) %&amp;gt;% 
    dataset_map(~.x %&amp;gt;% list_modify(
      img = tf$image$convert_image_dtype(.x$img, dtype = tf$float32),
      mask = tf$image$convert_image_dtype(.x$mask, dtype = tf$float32)
    )) %&amp;gt;% 
    dataset_map(~.x %&amp;gt;% list_modify(
      img = tf$image$resize(.x$img, size = shape(128, 128)),
      mask = tf$image$resize(.x$mask, size = shape(128, 128))
    ))
  
  # data augmentation performed on training set only
  if (train) {
    dataset &amp;lt;- dataset %&amp;gt;% 
      dataset_map(~.x %&amp;gt;% list_modify(
        img = random_bsh(.x$img)
      )) 
  }
  
  # shuffling on training set only
  if (train) {
    dataset &amp;lt;- dataset %&amp;gt;% 
      dataset_shuffle(buffer_size = batch_size*128)
  }
  
  # train in batches; batch size might need to be adapted depending on
  # available memory
  dataset &amp;lt;- dataset %&amp;gt;% 
    dataset_batch(batch_size)
  
  dataset %&amp;gt;% 
    # output needs to be unnamed
    dataset_map(unname) 
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Training and test set creation now is just a matter of two function calls.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;training_dataset &amp;lt;- create_dataset(training(data), train = TRUE)
validation_dataset &amp;lt;- create_dataset(testing(data), train = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we’re ready to train the model.&lt;/p&gt;
&lt;h2 id="training-the-model"&gt;Training the model&lt;/h2&gt;
&lt;p&gt;We already showed how to create the model, but let’s repeat it here, and check model architecture:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- unet(input_shape = c(128, 128, 3))
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model: &amp;quot;model&amp;quot;
______________________________________________________________________________________________
Layer (type)                   Output Shape        Param #    Connected to                    
==============================================================================================
input_1 (InputLayer)           [(None, 128, 128, 3 0                                          
______________________________________________________________________________________________
conv2d (Conv2D)                (None, 128, 128, 64 1792       input_1[0][0]                   
______________________________________________________________________________________________
conv2d_1 (Conv2D)              (None, 128, 128, 64 36928      conv2d[0][0]                    
______________________________________________________________________________________________
max_pooling2d (MaxPooling2D)   (None, 64, 64, 64)  0          conv2d_1[0][0]                  
______________________________________________________________________________________________
conv2d_2 (Conv2D)              (None, 64, 64, 128) 73856      max_pooling2d[0][0]             
______________________________________________________________________________________________
conv2d_3 (Conv2D)              (None, 64, 64, 128) 147584     conv2d_2[0][0]                  
______________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D) (None, 32, 32, 128) 0          conv2d_3[0][0]                  
______________________________________________________________________________________________
conv2d_4 (Conv2D)              (None, 32, 32, 256) 295168     max_pooling2d_1[0][0]           
______________________________________________________________________________________________
conv2d_5 (Conv2D)              (None, 32, 32, 256) 590080     conv2d_4[0][0]                  
______________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D) (None, 16, 16, 256) 0          conv2d_5[0][0]                  
______________________________________________________________________________________________
conv2d_6 (Conv2D)              (None, 16, 16, 512) 1180160    max_pooling2d_2[0][0]           
______________________________________________________________________________________________
conv2d_7 (Conv2D)              (None, 16, 16, 512) 2359808    conv2d_6[0][0]                  
______________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D) (None, 8, 8, 512)   0          conv2d_7[0][0]                  
______________________________________________________________________________________________
dropout (Dropout)              (None, 8, 8, 512)   0          max_pooling2d_3[0][0]           
______________________________________________________________________________________________
conv2d_8 (Conv2D)              (None, 8, 8, 1024)  4719616    dropout[0][0]                   
______________________________________________________________________________________________
conv2d_9 (Conv2D)              (None, 8, 8, 1024)  9438208    conv2d_8[0][0]                  
______________________________________________________________________________________________
conv2d_transpose (Conv2DTransp (None, 16, 16, 512) 2097664    conv2d_9[0][0]                  
______________________________________________________________________________________________
concatenate (Concatenate)      (None, 16, 16, 1024 0          conv2d_7[0][0]                  
                                                              conv2d_transpose[0][0]          
______________________________________________________________________________________________
conv2d_10 (Conv2D)             (None, 16, 16, 512) 4719104    concatenate[0][0]               
______________________________________________________________________________________________
conv2d_11 (Conv2D)             (None, 16, 16, 512) 2359808    conv2d_10[0][0]                 
______________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTran (None, 32, 32, 256) 524544     conv2d_11[0][0]                 
______________________________________________________________________________________________
concatenate_1 (Concatenate)    (None, 32, 32, 512) 0          conv2d_5[0][0]                  
                                                              conv2d_transpose_1[0][0]        
______________________________________________________________________________________________
conv2d_12 (Conv2D)             (None, 32, 32, 256) 1179904    concatenate_1[0][0]             
______________________________________________________________________________________________
conv2d_13 (Conv2D)             (None, 32, 32, 256) 590080     conv2d_12[0][0]                 
______________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTran (None, 64, 64, 128) 131200     conv2d_13[0][0]                 
______________________________________________________________________________________________
concatenate_2 (Concatenate)    (None, 64, 64, 256) 0          conv2d_3[0][0]                  
                                                              conv2d_transpose_2[0][0]        
______________________________________________________________________________________________
conv2d_14 (Conv2D)             (None, 64, 64, 128) 295040     concatenate_2[0][0]             
______________________________________________________________________________________________
conv2d_15 (Conv2D)             (None, 64, 64, 128) 147584     conv2d_14[0][0]                 
______________________________________________________________________________________________
conv2d_transpose_3 (Conv2DTran (None, 128, 128, 64 32832      conv2d_15[0][0]                 
______________________________________________________________________________________________
concatenate_3 (Concatenate)    (None, 128, 128, 12 0          conv2d_1[0][0]                  
                                                              conv2d_transpose_3[0][0]        
______________________________________________________________________________________________
conv2d_16 (Conv2D)             (None, 128, 128, 64 73792      concatenate_3[0][0]             
______________________________________________________________________________________________
conv2d_17 (Conv2D)             (None, 128, 128, 64 36928      conv2d_16[0][0]                 
______________________________________________________________________________________________
conv2d_18 (Conv2D)             (None, 128, 128, 1) 65         conv2d_17[0][0]                 
==============================================================================================
Total params: 31,031,745
Trainable params: 31,031,745
Non-trainable params: 0
______________________________________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The “output shape” column shows the expected U-shape numerically: Width and height first go down, until we reach a minimum resolution of &lt;code&gt;8x8&lt;/code&gt;; they then go up again, until we’ve reached the original resolution. At the same time, the number of filters first goes up, then goes down again, until in the output layer we have a single filter. You can also see the &lt;code&gt;concatenate&lt;/code&gt; layers appending information that comes from “below” to information that comes “laterally.”&lt;/p&gt;
&lt;p&gt;What should be the loss function here? We’re labeling each pixel, so each pixel contributes to the loss. We have a binary problem – each pixel may be “car” or “background” – so we want each output to be close to either 0 or 1. This makes &lt;em&gt;binary_crossentropy&lt;/em&gt; the adequate loss function.&lt;/p&gt;
&lt;p&gt;During training, we keep track of classification accuracy as well as the &lt;a href="https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient"&gt;dice coefficient&lt;/a&gt;, the evaluation metric used in the competition. The dice coefficient is a way to measure the proportion of correct classifications:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;dice &amp;lt;- custom_metric(&amp;quot;dice&amp;quot;, function(y_true, y_pred, smooth = 1.0) {
  y_true_f &amp;lt;- k_flatten(y_true)
  y_pred_f &amp;lt;- k_flatten(y_pred)
  intersection &amp;lt;- k_sum(y_true_f * y_pred_f)
  (2 * intersection + smooth) / (k_sum(y_true_f) + k_sum(y_pred_f) + smooth)
})

model %&amp;gt;% compile(
  optimizer = optimizer_rmsprop(lr = 1e-5),
  loss = &amp;quot;binary_crossentropy&amp;quot;,
  metrics = list(dice, metric_binary_accuracy)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fitting the model takes some time – how much, of course, will depend on your hardware.&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; But the wait pays off: After five epochs, we saw a dice coefficient of ~ 0.87 on the validation set, and an accuracy of ~ 0.95.&lt;/p&gt;
&lt;h2 id="predictions"&gt;Predictions&lt;/h2&gt;
&lt;p&gt;Of course, what we’re ultimately interested in are predictions. Let’s see a few masks generated for items from the validation set:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;batch &amp;lt;- validation_dataset %&amp;gt;% as_iterator() %&amp;gt;% iter_next()
predictions &amp;lt;- predict(model, batch)

images &amp;lt;- tibble(
  image = batch[[1]] %&amp;gt;% array_branch(1),
  predicted_mask = predictions[,,,1] %&amp;gt;% array_branch(1),
  mask = batch[[2]][,,,1]  %&amp;gt;% array_branch(1)
) %&amp;gt;% 
  sample_n(2) %&amp;gt;% 
  map_depth(2, function(x) {
    as.raster(x) %&amp;gt;% magick::image_read()
  }) %&amp;gt;% 
  map(~do.call(c, .x))


out &amp;lt;- magick::image_append(c(
  magick::image_append(images$mask, stack = TRUE),
  magick::image_append(images$image, stack = TRUE), 
  magick::image_append(images$predicted_mask, stack = TRUE)
  )
)

plot(out)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-08-23-unet/images/predictions.png" alt="From left to right: ground truth, input image, and predicted mask from U-Net." width="400" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-19)From left to right: ground truth, input image, and predicted mask from U-Net.
&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;If there were a competition for the highest sum of usefulness and architectural transparency, U-Net would certainly be a contender. Without much tuning, it’s possible to obtain decent results. If you’re able to put this model to use in your work, or if you have problems using it, let us know! Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-RonnebergerFB15"&gt;
&lt;p&gt;Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” &lt;em&gt;CoRR&lt;/em&gt; abs/1505.04597. &lt;a href="http://arxiv.org/abs/1505.04597"&gt;http://arxiv.org/abs/1505.04597&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Expect up to half an hour on a laptop CPU.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">019b2c05e6dab5e49f4a94e526ea2ed2</distill:md5>
      <category>Image Recognition &amp; Image Processing</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-08-23-unet</guid>
      <pubDate>Fri, 23 Aug 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-08-23-unet/images/unet.png" medium="image" type="image/png" width="1400" height="932"/>
    </item>
    <item>
      <title>Modeling censored data with tfprobability</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-07-31-censored-data</link>
      <description>


&lt;p&gt;Nothing’s ever perfect, and data isn’t either. One type of “imperfection” is &lt;em&gt;missing data&lt;/em&gt;, where some features are unobserved for some subjects. (A topic for another post.) Another is &lt;em&gt;censored data&lt;/em&gt;, where an event whose characteristics we want to measure does not occur in the observation interval. The example in Richard McElreath’s &lt;em&gt;Statistical Rethinking&lt;/em&gt; is time to adoption of cats in an animal shelter. If we fix an interval and observe wait times for those cats that actually &lt;em&gt;did&lt;/em&gt; get adopted, our estimate will end up too optimistic: We don’t take into account those cats who weren’t adopted during this interval and thus, would have contributed wait times of length longer than the complete interval.&lt;/p&gt;
&lt;p&gt;In this post, we use a slightly less emotional example which nonetheless may be of interest, especially to R package developers: time to completion of &lt;code&gt;R CMD check&lt;/code&gt;, collected from CRAN and provided by the &lt;code&gt;parsnip&lt;/code&gt; package as &lt;code&gt;check_times&lt;/code&gt;. Here, the censored portion are those checks that errored out for whatever reason, i.e., for which the check did not complete.&lt;/p&gt;
&lt;p&gt;Why do we care about the censored portion? In the cat adoption scenario, this is pretty obvious: We want to be able to get a realistic estimate for any unknown cat, not just those cats that will turn out to be “lucky”. How about &lt;code&gt;check_times&lt;/code&gt;? Well, if your submission is one of those that errored out, you still care about how long you wait, so even though their percentage is low (&amp;lt; 1%) we don’t want to simply exclude them. Also, there is the possibility that the failing ones would have taken longer, had they run to completion, due to some intrinsic difference between both groups. Conversely, if failures were random, the longer-running checks would have a greater chance to get hit by an error. So here too, exluding the censored data may result in bias.&lt;/p&gt;
&lt;p&gt;How can we model durations for that censored portion, where the “true duration” is unknown? Taking one step back, how can we model durations in general? Making as few assumptions as possible, the &lt;a href="https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution"&gt;maximum entropy distribution&lt;/a&gt; for displacements (in space or time) is the exponential. Thus, for the checks that actually did complete, durations are assumed to be exponentially distributed.&lt;/p&gt;
&lt;p&gt;For the others, all we know is that in a virtual world where the check completed, it would take &lt;em&gt;at least as long&lt;/em&gt; as the given duration. This quantity can be modeled by the exponential complementary cumulative distribution function (CCDF). Why? A cumulative distribution function (CDF) indicates the probability that a value lower or equal to some reference point was reached; e.g., “the probability of durations &amp;lt;= 255 is 0.9”. Its complement, 1 - CDF, then gives the probability that a value will exceed than that reference point.&lt;/p&gt;
&lt;p&gt;Let’s see this in action.&lt;/p&gt;
&lt;h2 id="the-data"&gt;The data&lt;/h2&gt;
&lt;p&gt;The following code works with the current stable releases of TensorFlow and TensorFlow Probability, which are 1.14 and 0.7, respectively. If you don’t have &lt;code&gt;tfprobability&lt;/code&gt; installed, get it from Github:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;remotes::install_github(&amp;quot;rstudio/tfprobability&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These are the libraries we need. As of TensorFlow 1.14, we call &lt;code&gt;tf$compat$v2$enable_v2_behavior()&lt;/code&gt; to run with eager execution.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tensorflow)
library(tfprobability)
library(parsnip)
library(tidyverse)
library(zeallot)
library(gridExtra)
library(HDInterval)
library(tidymodels)
library(survival)

tf$compat$v2$enable_v2_behavior()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Besides the check durations we want to model, &lt;code&gt;check_times&lt;/code&gt; reports various features of the package in question, such as number of imported packages, number of dependencies, size of code and documentation files, etc. The &lt;code&gt;status&lt;/code&gt; variable indicates whether the check completed or errored out.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;df &amp;lt;- check_times %&amp;gt;% select(-package)
glimpse(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Observations: 13,626
Variables: 24
$ authors        &amp;lt;int&amp;gt; 1, 1, 1, 1, 5, 3, 2, 1, 4, 6, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,…
$ imports        &amp;lt;dbl&amp;gt; 0, 6, 0, 0, 3, 1, 0, 4, 0, 7, 0, 0, 0, 0, 3, 2, 14, 2, 2, 0…
$ suggests       &amp;lt;dbl&amp;gt; 2, 4, 0, 0, 2, 0, 2, 2, 0, 0, 2, 8, 0, 0, 2, 0, 1, 3, 0, 0,…
$ depends        &amp;lt;dbl&amp;gt; 3, 1, 6, 1, 1, 1, 5, 0, 1, 1, 6, 5, 0, 0, 0, 1, 1, 5, 0, 2,…
$ Roxygen        &amp;lt;dbl&amp;gt; 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,…
$ gh             &amp;lt;dbl&amp;gt; 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,…
$ rforge         &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
$ descr          &amp;lt;int&amp;gt; 217, 313, 269, 63, 223, 1031, 135, 344, 204, 335, 104, 163,…
$ r_count        &amp;lt;int&amp;gt; 2, 20, 8, 0, 10, 10, 16, 3, 6, 14, 16, 4, 1, 1, 11, 5, 7, 1…
$ r_size         &amp;lt;dbl&amp;gt; 0.029053, 0.046336, 0.078374, 0.000000, 0.019080, 0.032607,…
$ ns_import      &amp;lt;dbl&amp;gt; 3, 15, 6, 0, 4, 5, 0, 4, 2, 10, 5, 6, 1, 0, 2, 2, 1, 11, 0,…
$ ns_export      &amp;lt;dbl&amp;gt; 0, 19, 0, 0, 10, 0, 0, 2, 0, 9, 3, 4, 0, 1, 10, 0, 16, 0, 2…
$ s3_methods     &amp;lt;dbl&amp;gt; 3, 0, 11, 0, 0, 0, 0, 2, 0, 23, 0, 0, 2, 5, 0, 4, 0, 0, 0, …
$ s4_methods     &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
$ doc_count      &amp;lt;int&amp;gt; 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…
$ doc_size       &amp;lt;dbl&amp;gt; 0.000000, 0.019757, 0.038281, 0.000000, 0.007874, 0.000000,…
$ src_count      &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 3, 0, 0, 0, 0, 0, 0, 54, 0, 0…
$ src_size       &amp;lt;dbl&amp;gt; 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000,…
$ data_count     &amp;lt;int&amp;gt; 2, 0, 0, 3, 3, 1, 10, 0, 4, 2, 2, 146, 0, 0, 0, 0, 0, 10, 0…
$ data_size      &amp;lt;dbl&amp;gt; 0.025292, 0.000000, 0.000000, 4.885864, 4.595504, 0.006500,…
$ testthat_count &amp;lt;int&amp;gt; 0, 8, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0,…
$ testthat_size  &amp;lt;dbl&amp;gt; 0.000000, 0.002496, 0.000000, 0.000000, 0.000000, 0.000000,…
$ check_time     &amp;lt;dbl&amp;gt; 49, 101, 292, 21, 103, 46, 78, 91, 47, 196, 200, 169, 45, 2…
$ status         &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of these 13,626 observations, just 103 are censored:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;table(df$status)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0     1 
103 13523 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For better readability, we’ll work with a subset of the columns. We use &lt;code&gt;surv_reg&lt;/code&gt; to help us find a useful and interesting subset of predictors:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;survreg_fit &amp;lt;-
  surv_reg(dist = &amp;quot;exponential&amp;quot;) %&amp;gt;% 
  set_engine(&amp;quot;survreg&amp;quot;) %&amp;gt;% 
  fit(Surv(check_time, status) ~ ., 
      data = df)
tidy(survreg_fit) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 23 x 7
   term             estimate std.error statistic  p.value conf.low conf.high
   &amp;lt;chr&amp;gt;               &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
 1 (Intercept)     3.86      0.0219     176.     0.             NA        NA
 2 authors         0.0139    0.00580      2.40   1.65e- 2       NA        NA
 3 imports         0.0606    0.00290     20.9    7.49e-97       NA        NA
 4 suggests        0.0332    0.00358      9.28   1.73e-20       NA        NA
 5 depends         0.118     0.00617     19.1    5.66e-81       NA        NA
 6 Roxygen         0.0702    0.0209       3.36   7.87e- 4       NA        NA
 7 gh              0.00898   0.0217       0.414  6.79e- 1       NA        NA
 8 rforge          0.0232    0.0662       0.351  7.26e- 1       NA        NA
 9 descr           0.000138  0.0000337    4.10   4.18e- 5       NA        NA
10 r_count         0.00209   0.000525     3.98   7.03e- 5       NA        NA
11 r_size          0.481     0.0819       5.87   4.28e- 9       NA        NA
12 ns_import       0.00352   0.000896     3.93   8.48e- 5       NA        NA
13 ns_export      -0.00161   0.000308    -5.24   1.57e- 7       NA        NA
14 s3_methods      0.000449  0.000421     1.06   2.87e- 1       NA        NA
15 s4_methods     -0.00154   0.00206     -0.745  4.56e- 1       NA        NA
16 doc_count       0.0739    0.0117       6.33   2.44e-10       NA        NA
17 doc_size        2.86      0.517        5.54   3.08e- 8       NA        NA
18 src_count       0.0122    0.00127      9.58   9.96e-22       NA        NA
19 src_size       -0.0242    0.0181      -1.34   1.82e- 1       NA        NA
20 data_count      0.0000415 0.000980     0.0423 9.66e- 1       NA        NA
21 data_size       0.0217    0.0135       1.61   1.08e- 1       NA        NA
22 testthat_count -0.000128  0.00127     -0.101  9.20e- 1       NA        NA
23 testthat_size   0.0108    0.0139       0.774  4.39e- 1       NA        NA
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It seems that if we choose &lt;code&gt;imports&lt;/code&gt;, &lt;code&gt;depends&lt;/code&gt;, &lt;code&gt;r_size&lt;/code&gt;, &lt;code&gt;doc_size&lt;/code&gt;, &lt;code&gt;ns_import&lt;/code&gt; and &lt;code&gt;ns_export&lt;/code&gt; we end up with a mix of (comparatively) powerful predictors from different semantic spaces and of different scales.&lt;/p&gt;
&lt;p&gt;Before pruning the dataframe, we save away the target variable. In our model and training setup, it is convenient to have censored and uncensored data stored separately, so here we create &lt;em&gt;two&lt;/em&gt; target matrices instead of one:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# check times for failed checks
# _c stands for censored
check_time_c &amp;lt;- df %&amp;gt;%
  filter(status == 0) %&amp;gt;%
  select(check_time) %&amp;gt;%
  as.matrix()

# check times for successful checks 
check_time_nc &amp;lt;- df %&amp;gt;%
  filter(status == 1) %&amp;gt;%
  select(check_time) %&amp;gt;%
  as.matrix()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can zoom in on the variables of interest, setting up one dataframe for the censored data and one for the uncensored data each. All predictors are normalized to avoid overflow during sampling. &lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; We add a column of &lt;code&gt;1&lt;/code&gt;s for use as an intercept.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;df &amp;lt;- df %&amp;gt;% select(status,
                    depends,
                    imports,
                    doc_size,
                    r_size,
                    ns_import,
                    ns_export) %&amp;gt;%
  mutate_at(.vars = 2:7, .funs = function(x) (x - min(x))/(max(x)-min(x))) %&amp;gt;%
  add_column(intercept = rep(1, nrow(df)), .before = 1)

# dataframe of predictors for censored data  
df_c &amp;lt;- df %&amp;gt;% filter(status == 0) %&amp;gt;% select(-status)
# dataframe of predictors for non-censored data 
df_nc &amp;lt;- df %&amp;gt;% filter(status == 1) %&amp;gt;% select(-status)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s it for preparations. But of course we’re curious. Do check times look different? Do predictors – the ones we chose – look different?&lt;/p&gt;
&lt;p&gt;Comparing a few meaningful percentiles for both classes, we see that durations for uncompleted checks are higher than those for completed checks throughout, apart from the 100% percentile. It’s not surprising that given the enormous difference in sample size, maximum duration is higher for completed checks. Otherwise though, doesn’t it look like the errored-out package checks “were going to take longer”?&lt;/p&gt;
&lt;div class="l-body"&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th&gt;percentiles: &lt;em&gt;check time&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;10%&lt;/th&gt;
&lt;th&gt;30%&lt;/th&gt;
&lt;th&gt;50%&lt;/th&gt;
&lt;th&gt;70%&lt;/th&gt;
&lt;th&gt;90%&lt;/th&gt;
&lt;th&gt;100%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;completed&lt;/td&gt;
&lt;td&gt;36&lt;/td&gt;
&lt;td&gt;54&lt;/td&gt;
&lt;td&gt;79&lt;/td&gt;
&lt;td&gt;115&lt;/td&gt;
&lt;td&gt;211&lt;/td&gt;
&lt;td&gt;1343&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;not completed&lt;/td&gt;
&lt;td&gt;42&lt;/td&gt;
&lt;td&gt;71&lt;/td&gt;
&lt;td&gt;97&lt;/td&gt;
&lt;td&gt;143&lt;/td&gt;
&lt;td&gt;293&lt;/td&gt;
&lt;td&gt;696&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;How about the predictors? We don’t see any differences for &lt;code&gt;depends&lt;/code&gt;, the number of package dependencies (apart from, again, the higher maximum reached for packages whose check completed): &lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class="l-body"&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th&gt;percentiles: &lt;em&gt;depends&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;10%&lt;/th&gt;
&lt;th&gt;30%&lt;/th&gt;
&lt;th&gt;50%&lt;/th&gt;
&lt;th&gt;70%&lt;/th&gt;
&lt;th&gt;90%&lt;/th&gt;
&lt;th&gt;100%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;completed&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;not completed&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;But for all others, we see the same pattern as reported above for &lt;code&gt;check_time&lt;/code&gt;. Number of packages imported is higher for censored data at all percentiles besides the maximum:&lt;/p&gt;
&lt;div class="l-body"&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th&gt;percentiles: &lt;em&gt;imports&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;10%&lt;/th&gt;
&lt;th&gt;30%&lt;/th&gt;
&lt;th&gt;50%&lt;/th&gt;
&lt;th&gt;70%&lt;/th&gt;
&lt;th&gt;90%&lt;/th&gt;
&lt;th&gt;100%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;completed&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;43&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;not completed&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;22&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Same for &lt;code&gt;ns_export&lt;/code&gt;, the estimated number of exported functions or methods:&lt;/p&gt;
&lt;div class="l-body"&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th&gt;percentiles: &lt;em&gt;ns_export&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;10%&lt;/th&gt;
&lt;th&gt;30%&lt;/th&gt;
&lt;th&gt;50%&lt;/th&gt;
&lt;th&gt;70%&lt;/th&gt;
&lt;th&gt;90%&lt;/th&gt;
&lt;th&gt;100%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;completed&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;26&lt;/td&gt;
&lt;td&gt;2547&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;not completed&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;34&lt;/td&gt;
&lt;td&gt;336&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;As well as for &lt;code&gt;ns_import&lt;/code&gt;, the estimated number of imported functions or methods:&lt;/p&gt;
&lt;div class="l-body"&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th&gt;percentiles: &lt;em&gt;ns_import&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;10%&lt;/th&gt;
&lt;th&gt;30%&lt;/th&gt;
&lt;th&gt;50%&lt;/th&gt;
&lt;th&gt;70%&lt;/th&gt;
&lt;th&gt;90%&lt;/th&gt;
&lt;th&gt;100%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;completed&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;19&lt;/td&gt;
&lt;td&gt;312&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;not completed&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;23&lt;/td&gt;
&lt;td&gt;297&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Same pattern for &lt;code&gt;r_size&lt;/code&gt;, the size on disk of files in the &lt;code&gt;R&lt;/code&gt; directory:&lt;/p&gt;
&lt;div class="l-body"&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th&gt;percentiles: &lt;em&gt;r_size&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;10%&lt;/th&gt;
&lt;th&gt;30%&lt;/th&gt;
&lt;th&gt;50%&lt;/th&gt;
&lt;th&gt;70%&lt;/th&gt;
&lt;th&gt;90%&lt;/th&gt;
&lt;th&gt;100%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;completed&lt;/td&gt;
&lt;td&gt;0.005&lt;/td&gt;
&lt;td&gt;0.015&lt;/td&gt;
&lt;td&gt;0.031&lt;/td&gt;
&lt;td&gt;0.063&lt;/td&gt;
&lt;td&gt;0.176&lt;/td&gt;
&lt;td&gt;3.746&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;not completed&lt;/td&gt;
&lt;td&gt;0.008&lt;/td&gt;
&lt;td&gt;0.019&lt;/td&gt;
&lt;td&gt;0.041&lt;/td&gt;
&lt;td&gt;0.097&lt;/td&gt;
&lt;td&gt;0.217&lt;/td&gt;
&lt;td&gt;2.148&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;And finally, we see it for &lt;code&gt;doc_size&lt;/code&gt; too, where &lt;code&gt;doc_size&lt;/code&gt; is the size of &lt;code&gt;.Rmd&lt;/code&gt; and &lt;code&gt;.Rnw&lt;/code&gt; files:&lt;/p&gt;
&lt;div class="l-body"&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th&gt;percentiles: &lt;em&gt;doc_size&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;10%&lt;/th&gt;
&lt;th&gt;30%&lt;/th&gt;
&lt;th&gt;50%&lt;/th&gt;
&lt;th&gt;70%&lt;/th&gt;
&lt;th&gt;90%&lt;/th&gt;
&lt;th&gt;100%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;completed&lt;/td&gt;
&lt;td&gt;0.000&lt;/td&gt;
&lt;td&gt;0.000&lt;/td&gt;
&lt;td&gt;0.000&lt;/td&gt;
&lt;td&gt;0.000&lt;/td&gt;
&lt;td&gt;0.023&lt;/td&gt;
&lt;td&gt;0.988&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;not completed&lt;/td&gt;
&lt;td&gt;0.000&lt;/td&gt;
&lt;td&gt;0.000&lt;/td&gt;
&lt;td&gt;0.000&lt;/td&gt;
&lt;td&gt;0.011&lt;/td&gt;
&lt;td&gt;0.042&lt;/td&gt;
&lt;td&gt;0.114&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Given our task at hand – model check durations taking into account uncensored as well as censored data – we won’t dwell on differences between both groups any longer; nonetheless we thought it interesting to relate these numbers.&lt;/p&gt;
&lt;p&gt;So now, back to work. We need to create a model.&lt;/p&gt;
&lt;h2 id="the-model"&gt;The model&lt;/h2&gt;
&lt;p&gt;As explained in the introduction, for completed checks duration is modeled using an exponential PDF. This is as straightforward as adding &lt;a href="https://rstudio.github.io/tfprobability/reference/tfd_exponential.html"&gt;tfd_exponential()&lt;/a&gt; to the model function, &lt;a href="https://rstudio.github.io/tfprobability/reference/tfd_joint_distribution_sequential.html"&gt;tfd_joint_distribution_sequential()&lt;/a&gt;. &lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;For the censored portion, we need the exponential CCDF. This one is not, as of today, easily added to the model. What we can do though is calculate its value ourselves and add it to the “main” model likelihood. We’ll see this below when discussing sampling; for now it means the model definition ends up straightforward as it only covers the non-censored data. It is made of just the said exponential PDF and priors for the regression parameters.&lt;/p&gt;
&lt;p&gt;As for the latter, we use 0-centered, Gaussian priors for all parameters. Standard deviations of 1 turned out to work well. As the priors are all the same, instead of listing a bunch of &lt;code&gt;tfd_normal&lt;/code&gt;s, we can create them all at once as&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;tfd_sample_distribution(tfd_normal(0, 1), sample_shape = 7)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Mean check time is modeled as an affine combination of the six predictors and the intercept. Here then is the complete model, instantiated using the uncensored data only:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- function(data) {
  tfd_joint_distribution_sequential(
    list(
      tfd_sample_distribution(tfd_normal(0, 1), sample_shape = 7),
      function(betas)
        tfd_independent(
          tfd_exponential(
            rate = 1 / tf$math$exp(tf$transpose(
              tf$matmul(tf$cast(data, betas$dtype), tf$transpose(betas))))),
          reinterpreted_batch_ndims = 1)))
}

m &amp;lt;- model(df_nc %&amp;gt;% as.matrix())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Always, we test if samples from that model have the expected shapes:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;samples &amp;lt;- m %&amp;gt;% tfd_sample(2)
samples&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
tf.Tensor(
[[ 1.4184642   0.17583323 -0.06547955 -0.2512014   0.1862184  -1.2662812
   1.0231884 ]
 [-0.52142304 -1.0036682   2.2664437   1.29737     1.1123234   0.3810004
   0.1663677 ]], shape=(2, 7), dtype=float32)

[[2]]
tf.Tensor(
[[4.4954767  7.865639   1.8388556  ... 7.914391   2.8485563  3.859719  ]
 [1.549662   0.77833986 0.10015647 ... 0.40323067 3.42171    0.69368565]], shape=(2, 13523), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This looks fine: We have a list of length two, one element for each distribution in the model. For both tensors, dimension 1 reflects the batch size (which we arbitrarily set to 2 in this test), while dimension 2 is 7 for the number of normal priors and 13523 for the number of durations predicted.&lt;/p&gt;
&lt;p&gt;How likely are these samples?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;m %&amp;gt;% tfd_log_prob(samples)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor([-32464.521   -7693.4023], shape=(2,), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here too, the shape is correct, and the values look reasonable.&lt;/p&gt;
&lt;p&gt;The next thing to do is define the target we want to optimize.&lt;/p&gt;
&lt;h2 id="optimization-target"&gt;Optimization target&lt;/h2&gt;
&lt;p&gt;Abstractly, the thing to maximize is the log probility of the data – that is, the measured durations – under the model. Now here the data comes in two parts, and the target does as well. First, we have the non-censored data, for which&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;m %&amp;gt;% tfd_log_prob(list(betas, tf$cast(target_nc, betas$dtype)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will calculate the log probability. Second, to obtain log probability for the censored data we write a custom function that calculates the log of the exponential CCDF:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;get_exponential_lccdf &amp;lt;- function(betas, data, target) {
  e &amp;lt;-  tfd_independent(tfd_exponential(rate = 1 / tf$math$exp(tf$transpose(tf$matmul(
    tf$cast(data, betas$dtype), tf$transpose(betas)
  )))),
  reinterpreted_batch_ndims = 1)
  cum_prob &amp;lt;- e %&amp;gt;% tfd_cdf(tf$cast(target, betas$dtype))
  tf$math$log(1 - cum_prob)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both parts are combined in a little wrapper function that allows us to compare training including and excluding the censored data. We won’t do that in this post, but you might be interested to do it with your own data, especially if the ratio of censored and uncensored parts is a little less imbalanced.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;get_log_prob &amp;lt;-
  function(target_nc,
           censored_data = NULL,
           target_c = NULL) {
    log_prob &amp;lt;- function(betas) {
      log_prob &amp;lt;-
        m %&amp;gt;% tfd_log_prob(list(betas, tf$cast(target_nc, betas$dtype)))
      potential &amp;lt;-
        if (!is.null(censored_data) &amp;amp;&amp;amp; !is.null(target_c))
          get_exponential_lccdf(betas, censored_data, target_c)
      else
        0
      log_prob + potential
    }
    log_prob
  }

log_prob &amp;lt;-
  get_log_prob(
    check_time_nc %&amp;gt;% tf$transpose(),
    df_c %&amp;gt;% as.matrix(),
    check_time_c %&amp;gt;% tf$transpose()
  )&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="sampling"&gt;Sampling&lt;/h2&gt;
&lt;p&gt;With model and target defined, we’re ready to do sampling.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;n_chains &amp;lt;- 4
n_burnin &amp;lt;- 1000
n_steps &amp;lt;- 1000

# keep track of some diagnostic output, acceptance and step size
trace_fn &amp;lt;- function(state, pkr) {
  list(
    pkr$inner_results$is_accepted,
    pkr$inner_results$accepted_results$step_size
  )
}

# get shape of initial values 
# to start sampling without producing NaNs, we will feed the algorithm
# tf$zeros_like(initial_betas)
# instead 
initial_betas &amp;lt;- (m %&amp;gt;% tfd_sample(n_chains))[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the number of leapfrog steps and the step size, experimentation showed that a combination of 64 / 0.1 yielded reasonable results:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;hmc &amp;lt;- mcmc_hamiltonian_monte_carlo(
  target_log_prob_fn = log_prob,
  num_leapfrog_steps = 64,
  step_size = 0.1
) %&amp;gt;%
  mcmc_simple_step_size_adaptation(target_accept_prob = 0.8,
                                   num_adaptation_steps = n_burnin)

run_mcmc &amp;lt;- function(kernel) {
  kernel %&amp;gt;% mcmc_sample_chain(
    num_results = n_steps,
    num_burnin_steps = n_burnin,
    current_state = tf$ones_like(initial_betas),
    trace_fn = trace_fn
  )
}

# important for performance: run HMC in graph mode
run_mcmc &amp;lt;- tf_function(run_mcmc)

res &amp;lt;- hmc %&amp;gt;% run_mcmc()
samples &amp;lt;- res$all_states&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="results"&gt;Results&lt;/h2&gt;
&lt;p&gt;Before we inspect the chains, here is a quick look at the proportion of accepted steps and the per-parameter mean step size:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;accepted &amp;lt;- res$trace[[1]]
as.numeric(accepted) %&amp;gt;% mean()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.995&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;step_size &amp;lt;- res$trace[[2]]
as.numeric(step_size) %&amp;gt;% mean()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.004953894&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We also store away effective sample sizes and the &lt;em&gt;rhat&lt;/em&gt; metrics for later addition to the synopsis.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;effective_sample_size &amp;lt;- mcmc_effective_sample_size(samples) %&amp;gt;%
  as.matrix() %&amp;gt;%
  apply(2, mean)
potential_scale_reduction &amp;lt;- mcmc_potential_scale_reduction(samples) %&amp;gt;%
  as.numeric()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then convert the &lt;code&gt;samples&lt;/code&gt; tensor to an R array for use in postprocessing.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# 2-item list, where each item has dim (1000, 4)
samples &amp;lt;- as.array(samples) %&amp;gt;% array_branch(margin = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How well did the sampling work? The chains mix well, but for some parameters, autocorrelation is still pretty high.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;prep_tibble &amp;lt;- function(samples) {
  as_tibble(samples,
            .name_repair = ~ c(&amp;quot;chain_1&amp;quot;, &amp;quot;chain_2&amp;quot;, &amp;quot;chain_3&amp;quot;, &amp;quot;chain_4&amp;quot;)) %&amp;gt;%
    add_column(sample = 1:n_steps) %&amp;gt;%
    gather(key = &amp;quot;chain&amp;quot;, value = &amp;quot;value&amp;quot;,-sample)
}

plot_trace &amp;lt;- function(samples) {
  prep_tibble(samples) %&amp;gt;%
    ggplot(aes(x = sample, y = value, color = chain)) +
    geom_line() +
    theme_light() +
    theme(
      legend.position = &amp;quot;none&amp;quot;,
      axis.title = element_blank(),
      axis.text = element_blank(),
      axis.ticks = element_blank()
    )
}

plot_traces &amp;lt;- function(samples) {
  plots &amp;lt;- purrr::map(samples, plot_trace)
  do.call(grid.arrange, plots)
}

plot_traces(samples)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-07-31-censored-data/images/chains.png" alt="Trace plots for the 7 parameters." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-22)Trace plots for the 7 parameters.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now for a synopsis of posterior parameter statistics, including the usual per-parameter sampling indicators &lt;em&gt;effective sample size&lt;/em&gt; and &lt;em&gt;rhat&lt;/em&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;all_samples &amp;lt;- map(samples, as.vector)

means &amp;lt;- map_dbl(all_samples, mean)

sds &amp;lt;- map_dbl(all_samples, sd)

hpdis &amp;lt;- map(all_samples, ~ hdi(.x) %&amp;gt;% t() %&amp;gt;% as_tibble())

summary &amp;lt;- tibble(
  mean = means,
  sd = sds,
  hpdi = hpdis
) %&amp;gt;% unnest() %&amp;gt;%
  add_column(param = colnames(df_c), .after = FALSE) %&amp;gt;%
  add_column(
    n_effective = effective_sample_size,
    rhat = potential_scale_reduction
  )

summary&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 7 x 7
  param       mean     sd  lower upper n_effective  rhat
  &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
1 intercept  4.05  0.0158  4.02   4.08       508.   1.17
2 depends    1.34  0.0732  1.18   1.47      1000    1.00
3 imports    2.89  0.121   2.65   3.12      1000    1.00
4 doc_size   6.18  0.394   5.40   6.94       177.   1.01
5 r_size     2.93  0.266   2.42   3.46       289.   1.00
6 ns_import  1.54  0.274   0.987  2.06       387.   1.00
7 ns_export -0.237 0.675  -1.53   1.10        66.8  1.01&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-07-31-censored-data/images/synopsis.png" alt="Posterior means and HPDIs. " width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-24)Posterior means and HPDIs.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From the diagnostics and trace plots, the model seems to work reasonably well, but as there is no straightforward error metric involved, it’s hard to know if actual predictions would even land in an appropriate range.&lt;/p&gt;
&lt;p&gt;To make sure they do, we inspect predictions from our model as well as from &lt;code&gt;surv_reg&lt;/code&gt;. This time, we also split the data into training and test sets. Here first are the predictions from &lt;code&gt;surv_reg&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_test_split &amp;lt;- initial_split(check_times, strata = &amp;quot;status&amp;quot;)
check_time_train &amp;lt;- training(train_test_split)
check_time_test &amp;lt;- testing(train_test_split)

survreg_fit &amp;lt;-
  surv_reg(dist = &amp;quot;exponential&amp;quot;) %&amp;gt;% 
  set_engine(&amp;quot;survreg&amp;quot;) %&amp;gt;% 
  fit(Surv(check_time, status) ~ depends + imports + doc_size + r_size + 
        ns_import + ns_export, 
      data = check_time_train)
survreg_fit(sr_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 7 x 7
  term         estimate std.error statistic  p.value conf.low conf.high
  &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
1 (Intercept)  4.05      0.0174     234.    0.             NA        NA
2 depends      0.108     0.00701     15.4   3.40e-53       NA        NA
3 imports      0.0660    0.00327     20.2   1.09e-90       NA        NA
4 doc_size     7.76      0.543       14.3   2.24e-46       NA        NA
5 r_size       0.812     0.0889       9.13  6.94e-20       NA        NA
6 ns_import    0.00501   0.00103      4.85  1.22e- 6       NA        NA
7 ns_export   -0.000212  0.000375    -0.566 5.71e- 1       NA        NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;survreg_pred &amp;lt;- 
  predict(survreg_fit, check_time_test) %&amp;gt;% 
  bind_cols(check_time_test %&amp;gt;% select(check_time, status))  

ggplot(survreg_pred, aes(x = check_time, y = .pred, color = factor(status))) +
  geom_point() + 
  coord_cartesian(ylim = c(0, 1400))&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-07-31-censored-data/images/survreg_pred.png" alt="Test set predictions from surv_reg. One outlier (of value 160421) is excluded via coord_cartesian() to avoid distorting the plot." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-27)Test set predictions from surv_reg. One outlier (of value 160421) is excluded via coord_cartesian() to avoid distorting the plot.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;For the MCMC model, we re-train on just the training set and obtain the parameter summary. The code is analogous to the above and not shown here.&lt;/p&gt;
&lt;p&gt;We can now predict on the test set, for simplicity just using the posterior means:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;df &amp;lt;- check_time_test %&amp;gt;% select(
                    depends,
                    imports,
                    doc_size,
                    r_size,
                    ns_import,
                    ns_export) %&amp;gt;%
  add_column(intercept = rep(1, nrow(check_time_test)), .before = 1)

mcmc_pred &amp;lt;- df %&amp;gt;% as.matrix() %*% summary$mean %&amp;gt;% exp() %&amp;gt;% as.numeric()
mcmc_pred &amp;lt;- check_time_test %&amp;gt;% select(check_time, status) %&amp;gt;%
  add_column(.pred = mcmc_pred)

ggplot(mcmc_pred, aes(x = check_time, y = .pred, color = factor(status))) +
  geom_point() + 
  coord_cartesian(ylim = c(0, 1400)) &lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-07-31-censored-data/images/mcmc_preds.png" alt="Test set predictions from the mcmc model. No outliers, just using same scale as above for comparison." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-29)Test set predictions from the mcmc model. No outliers, just using same scale as above for comparison.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This looks good!&lt;/p&gt;
&lt;h2 id="wrapup"&gt;Wrapup&lt;/h2&gt;
&lt;p&gt;We’ve shown how to model censored data – or rather, a frequent subtype thereof involving durations – using &lt;code&gt;tfprobability&lt;/code&gt;. The &lt;code&gt;check_times&lt;/code&gt; data from &lt;code&gt;parsnip&lt;/code&gt; were a fun choice, but this modeling technique may be even more useful when censoring is more substantial. Hopefully his post has provided some guidance on how to handle censored data in your own work. Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;By itself, the predictors being on different scales isn’t a problem. Here we need to normalize due to the log link used in the model above.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;Here and in the following tables, we report the unnormalized, original values as contained in &lt;code&gt;check_times&lt;/code&gt;.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;For a first introduction to MCMC sampling with &lt;code&gt;tfprobability&lt;/code&gt;, see &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-05-06-tadpoles-on-tensorflow/"&gt;Tadpoles on TensorFlow: Hierarchical partial pooling with tfprobability&lt;/a&gt;&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">901c832b24ff30c62364b36055d9e3bf</distill:md5>
      <category>Bayesian Modeling</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-07-31-censored-data</guid>
      <pubDate>Wed, 31 Jul 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-07-31-censored-data/images/thumb_cropped.png" medium="image" type="image/png" width="955" height="396"/>
    </item>
    <item>
      <title>TensorFlow feature columns: Transforming your data recipes-style</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Falbel</dc:creator>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-07-09-feature-columns</link>
      <description>


&lt;p&gt;It’s 2019; no one doubts the effectiveness of deep learning in computer vision. Or natural language processing. With “normal”, Excel-style, a.k.a. &lt;em&gt;tabular&lt;/em&gt; data however, the situation is different.&lt;/p&gt;
&lt;p&gt;Basically there are two cases: One, you have numeric data only. Then, creating the network is straightforward, and all will be about optimization and hyperparameter search. Two, you have a mix of numeric and categorical data, where categorical could be anything from ordered-numeric to symbolic (e.g., text). In this latter case, with categorical data entering the picture, there is an extremely nice idea you can make use of: &lt;em&gt;embed&lt;/em&gt; what are equidistant symbols into a high-dimensional, numeric representation. In that new representation, we can define a distance metric that allows us to make statements like “cycling is closer to running than to baseball”, or “😃 is closer to 😂 than to 😠”. When not dealing with language data, this technique is referred to as &lt;em&gt;entity embeddings&lt;/em&gt;.&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Nice as this sounds, why don’t we see entity embeddings used all the time? Well, creating a Keras network that processes a mix of numeric and categorical data used to require a bit of an effort. With TensorFlow’s new &lt;em&gt;feature columns&lt;/em&gt;, usable from R through a combination of &lt;code&gt;tfdatasets&lt;/code&gt; and &lt;code&gt;keras&lt;/code&gt;, there is a much easier way to achieve this. What’s more, &lt;code&gt;tfdatasets&lt;/code&gt; follows the popular &lt;a href="https://cran.r-project.org/web/packages/recipes/index.html"&gt;recipes&lt;/a&gt; idiom to initialize, refine, and apply a feature specification &lt;code&gt;%&amp;gt;%&lt;/code&gt;-style. And finally, there are ready-made steps for bucketizing a numeric column, or hashing it, or creating &lt;em&gt;crossed columns&lt;/em&gt; to capture interactions.&lt;/p&gt;
&lt;p&gt;This post introduces feature specs starting from a scenario where they don’t exist: basically, the status quo until very recently. Imagine you have a dataset like that from the &lt;a href="https://www.kaggle.com/c/porto-seguro-safe-driver-prediction"&gt;Porto Seguro car insurance competition&lt;/a&gt; where some of the columns are numeric, and some are categorical. You want to train a fully connected network on it, with all categorical columns fed into embedding layers. How can you do that? We then contrast this with the feature spec way, which makes things &lt;em&gt;a lot&lt;/em&gt; easier – especially when there’s a lot of categorical columns. In a second applied example, we demonstrate the use of &lt;em&gt;crossed columns&lt;/em&gt; on the &lt;em&gt;rugged&lt;/em&gt; dataset from Richard McElreath’s &lt;em&gt;rethinking&lt;/em&gt; package. Here, we also direct attention to a few technical details that are worth knowing about.&lt;/p&gt;
&lt;h2 id="mixing-numeric-data-and-embeddings-the-pre-feature-spec-way"&gt;Mixing numeric data and embeddings, the pre-feature-spec way&lt;/h2&gt;
&lt;p&gt;Our first example dataset is taken from Kaggle. Two years ago, Brazilian car insurance company Porto Seguro asked participants to predict &lt;a href="https://www.kaggle.com/c/porto-seguro-safe-driver-prediction"&gt;how likely it is a car owner will file a claim&lt;/a&gt; based on a mix of characteristics collected during the previous year. The dataset is comparatively large – there are ~ 600,000 rows in the training set, with 57 predictors. Among others, features are named so as to indicate the type of the data – binary, categorical, or continuous/ordinal. While it’s common in competitions to try to reverse-engineer column meanings, here we just make use of the type of the data, and see how far that gets us.&lt;/p&gt;
&lt;p&gt;Concretely, this means we want to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;use binary features just the way they are, as zeroes and ones,&lt;/li&gt;
&lt;li&gt;scale the remaining numeric features to mean 0 and variance 1, and&lt;/li&gt;
&lt;li&gt;embed the categorical variables (each one by itself).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We’ll then define a dense network to predict &lt;code&gt;target&lt;/code&gt;, the binary outcome. So first, let’s see how we could get our data into shape, as well as build up the network, in a “manual”, pre-feature-columns way.&lt;/p&gt;
&lt;p&gt;When loading libraries, we already use the versions we’ll need very soon: Tensorflow 2 (&amp;gt;= beta 1), and the development (= Github) versions of &lt;code&gt;tfdatasets&lt;/code&gt; and &lt;code&gt;keras&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;tensorflow::install_tensorflow(version = &amp;quot;2.0.0-beta1&amp;quot;)

remotes::install_github(&amp;quot;rstudio/tfdatasets&amp;quot;)
remotes::install_github(&amp;quot;rstudio/keras&amp;quot;)

library(keras)
library(tfdatasets)
library(readr)
library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this first version of preparing the data, we make our lives easier by assigning different R types, based on what the features represent (categorical, binary, or numeric qualities):&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# downloaded from https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data
path &amp;lt;- &amp;quot;train.csv&amp;quot;

porto &amp;lt;- read_csv(path) %&amp;gt;%
  select(-id) %&amp;gt;%
  # to obtain number of unique levels, later
  mutate_at(vars(ends_with(&amp;quot;cat&amp;quot;)), factor) %&amp;gt;%
  # to easily keep them apart from the non-binary numeric data
  mutate_at(vars(ends_with(&amp;quot;bin&amp;quot;)), as.integer)

porto %&amp;gt;% glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Observations: 595,212
Variables: 58
$ target         &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,…
$ ps_ind_01      &amp;lt;dbl&amp;gt; 2, 1, 5, 0, 0, 5, 2, 5, 5, 1, 5, 2, 2, 1, 5, 5,…
$ ps_ind_02_cat  &amp;lt;fct&amp;gt; 2, 1, 4, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,…
$ ps_ind_03      &amp;lt;dbl&amp;gt; 5, 7, 9, 2, 0, 4, 3, 4, 3, 2, 2, 3, 1, 3, 11, 3…
$ ps_ind_04_cat  &amp;lt;fct&amp;gt; 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,…
$ ps_ind_05_cat  &amp;lt;fct&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
$ ps_ind_06_bin  &amp;lt;int&amp;gt; 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,…
$ ps_ind_07_bin  &amp;lt;int&amp;gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,…
$ ps_ind_08_bin  &amp;lt;int&amp;gt; 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,…
$ ps_ind_09_bin  &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…
$ ps_ind_10_bin  &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
$ ps_ind_11_bin  &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
$ ps_ind_12_bin  &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
$ ps_ind_13_bin  &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
$ ps_ind_14      &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
$ ps_ind_15      &amp;lt;dbl&amp;gt; 11, 3, 12, 8, 9, 6, 8, 13, 6, 4, 3, 9, 10, 12, …
$ ps_ind_16_bin  &amp;lt;int&amp;gt; 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,…
$ ps_ind_17_bin  &amp;lt;int&amp;gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
$ ps_ind_18_bin  &amp;lt;int&amp;gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,…
$ ps_reg_01      &amp;lt;dbl&amp;gt; 0.7, 0.8, 0.0, 0.9, 0.7, 0.9, 0.6, 0.7, 0.9, 0.…
$ ps_reg_02      &amp;lt;dbl&amp;gt; 0.2, 0.4, 0.0, 0.2, 0.6, 1.8, 0.1, 0.4, 0.7, 1.…
$ ps_reg_03      &amp;lt;dbl&amp;gt; 0.7180703, 0.7660777, -1.0000000, 0.5809475, 0.…
$ ps_car_01_cat  &amp;lt;fct&amp;gt; 10, 11, 7, 7, 11, 10, 6, 11, 10, 11, 11, 11, 6,…
$ ps_car_02_cat  &amp;lt;fct&amp;gt; 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,…
$ ps_car_03_cat  &amp;lt;fct&amp;gt; -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1…
$ ps_car_04_cat  &amp;lt;fct&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 0, 0, 0, 0, 9,…
$ ps_car_05_cat  &amp;lt;fct&amp;gt; 1, -1, -1, 1, -1, 0, 1, 0, 1, 0, -1, -1, -1, 1,…
$ ps_car_06_cat  &amp;lt;fct&amp;gt; 4, 11, 14, 11, 14, 14, 11, 11, 14, 14, 13, 11, …
$ ps_car_07_cat  &amp;lt;fct&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
$ ps_car_08_cat  &amp;lt;fct&amp;gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,…
$ ps_car_09_cat  &amp;lt;fct&amp;gt; 0, 2, 2, 3, 2, 0, 0, 2, 0, 2, 2, 0, 2, 2, 2, 0,…
$ ps_car_10_cat  &amp;lt;fct&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
$ ps_car_11_cat  &amp;lt;fct&amp;gt; 12, 19, 60, 104, 82, 104, 99, 30, 68, 104, 20, …
$ ps_car_11      &amp;lt;dbl&amp;gt; 2, 3, 1, 1, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 1, 2,…
$ ps_car_12      &amp;lt;dbl&amp;gt; 0.4000000, 0.3162278, 0.3162278, 0.3741657, 0.3…
$ ps_car_13      &amp;lt;dbl&amp;gt; 0.8836789, 0.6188165, 0.6415857, 0.5429488, 0.5…
$ ps_car_14      &amp;lt;dbl&amp;gt; 0.3708099, 0.3887158, 0.3472751, 0.2949576, 0.3…
$ ps_car_15      &amp;lt;dbl&amp;gt; 3.605551, 2.449490, 3.316625, 2.000000, 2.00000…
$ ps_calc_01     &amp;lt;dbl&amp;gt; 0.6, 0.3, 0.5, 0.6, 0.4, 0.7, 0.2, 0.1, 0.9, 0.…
$ ps_calc_02     &amp;lt;dbl&amp;gt; 0.5, 0.1, 0.7, 0.9, 0.6, 0.8, 0.6, 0.5, 0.8, 0.…
$ ps_calc_03     &amp;lt;dbl&amp;gt; 0.2, 0.3, 0.1, 0.1, 0.0, 0.4, 0.5, 0.1, 0.6, 0.…
$ ps_calc_04     &amp;lt;dbl&amp;gt; 3, 2, 2, 2, 2, 3, 2, 1, 3, 2, 2, 2, 4, 2, 3, 2,…
$ ps_calc_05     &amp;lt;dbl&amp;gt; 1, 1, 2, 4, 2, 1, 2, 2, 1, 2, 3, 2, 1, 1, 1, 1,…
$ ps_calc_06     &amp;lt;dbl&amp;gt; 10, 9, 9, 7, 6, 8, 8, 7, 7, 8, 8, 8, 8, 10, 8, …
$ ps_calc_07     &amp;lt;dbl&amp;gt; 1, 5, 1, 1, 3, 2, 1, 1, 3, 2, 2, 2, 4, 1, 2, 5,…
$ ps_calc_08     &amp;lt;dbl&amp;gt; 10, 8, 8, 8, 10, 11, 8, 6, 9, 9, 9, 10, 11, 8, …
$ ps_calc_09     &amp;lt;dbl&amp;gt; 1, 1, 2, 4, 2, 3, 3, 1, 4, 1, 4, 1, 1, 3, 3, 2,…
$ ps_calc_10     &amp;lt;dbl&amp;gt; 5, 7, 7, 2, 12, 8, 10, 13, 11, 11, 7, 8, 9, 8, …
$ ps_calc_11     &amp;lt;dbl&amp;gt; 9, 3, 4, 2, 3, 4, 3, 7, 4, 3, 6, 9, 6, 2, 4, 5,…
$ ps_calc_12     &amp;lt;dbl&amp;gt; 1, 1, 2, 2, 1, 2, 0, 1, 2, 5, 3, 2, 3, 0, 1, 2,…
$ ps_calc_13     &amp;lt;dbl&amp;gt; 5, 1, 7, 4, 1, 0, 0, 3, 1, 0, 3, 1, 3, 4, 3, 6,…
$ ps_calc_14     &amp;lt;dbl&amp;gt; 8, 9, 7, 9, 3, 9, 10, 6, 5, 6, 6, 10, 8, 3, 9, …
$ ps_calc_15_bin &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,…
$ ps_calc_16_bin &amp;lt;int&amp;gt; 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,…
$ ps_calc_17_bin &amp;lt;int&amp;gt; 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,…
$ ps_calc_18_bin &amp;lt;int&amp;gt; 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,…
$ ps_calc_19_bin &amp;lt;int&amp;gt; 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,…
$ ps_calc_20_bin &amp;lt;int&amp;gt; 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We split off 25% for validation.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# train-test split
id_training &amp;lt;- sample.int(nrow(porto), size = 0.75*nrow(porto))

x_train &amp;lt;- porto[id_training,] %&amp;gt;% select(-target)
x_test &amp;lt;- porto[-id_training,] %&amp;gt;% select(-target)
y_train &amp;lt;- porto[id_training, &amp;quot;target&amp;quot;]
y_test &amp;lt;- porto[-id_training, &amp;quot;target&amp;quot;] &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The only thing we want to do to the &lt;em&gt;data&lt;/em&gt; before defining the network is scaling the numeric features. Binary and categorical features can stay as is, with the minor correction that for the categorical ones, we’ll actually pass the network the numeric representation of the factor data.&lt;/p&gt;
&lt;p&gt;Here is the scaling.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_means &amp;lt;- colMeans(x_train[sapply(x_train, is.double)]) %&amp;gt;% unname()
train_sds &amp;lt;- apply(x_train[sapply(x_train, is.double)], 2, sd)  %&amp;gt;% unname()
train_sds[train_sds == 0] &amp;lt;- 0.000001

x_train[sapply(x_train, is.double)] &amp;lt;- sweep(
  x_train[sapply(x_train, is.double)],
  2,
  train_means
  ) %&amp;gt;%
  sweep(2, train_sds, &amp;quot;/&amp;quot;)
x_test[sapply(x_test, is.double)] &amp;lt;- sweep(
  x_test[sapply(x_test, is.double)],
  2,
  train_means
  ) %&amp;gt;%
  sweep(2, train_sds, &amp;quot;/&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When building the network, we need to specify the input and output dimensionalities for the embedding layers. Input dimensionality refers to the number of different symbols that “come in”; in NLP tasks this would be the vocabulary size while here, it’s simply the number of values a variable can take.&lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; Output dimensionality, the capacity of the internal representation, can then be calculated based on some heuristic. Below, we’ll follow a popular rule of thumb that takes the square root of the dimensionality of the input.&lt;/p&gt;
&lt;p&gt;So as part one of the network, here we build up the embedding layers in a loop, each wired to the input layer that feeds it:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# number of levels per factor, required to specify input dimensionality for
# the embedding layers
n_levels_in &amp;lt;- map(x_train %&amp;gt;% select_if(is.factor), compose(length, levels)) %&amp;gt;%
  unlist() 

# output dimensionality for the embedding layers, need +1 because Python is 0-based
n_levels_out &amp;lt;- n_levels_in %&amp;gt;% sqrt() %&amp;gt;% trunc() %&amp;gt;% `+`(1)

# each embedding layer gets its own input layer
cat_inputs &amp;lt;- map(n_levels_in, function(l) layer_input(shape = 1)) %&amp;gt;%
  unname()

# construct the embedding layers, connecting each to its input
embedding_layers &amp;lt;- vector(mode = &amp;quot;list&amp;quot;, length = length(cat_inputs))
for (i in 1:length(cat_inputs)) {
  embedding_layer &amp;lt;-  cat_inputs[[i]] %&amp;gt;% 
    layer_embedding(input_dim = n_levels_in[[i]] + 1, output_dim = n_levels_out[[i]]) %&amp;gt;%
    layer_flatten()
  embedding_layers[[i]] &amp;lt;- embedding_layer
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In case you were wondering about the &lt;code&gt;flatten&lt;/code&gt; layer following each embedding: We need to squeeze out the third dimension (introduced by the embedding layers) from the tensors, effectively rendering them rank-2. That is because we want to combine them with the rank-2 tensor coming out of the dense layer processing the numeric features.&lt;/p&gt;
&lt;p&gt;In order to be able to combine it with anything, we have to actually construct that dense layer first. It will be connected to a single input layer, of shape 43, that takes in the numeric features we scaled as well as the binary features we left untouched:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# create a single input and a dense layer for the numeric data
quant_input &amp;lt;- layer_input(shape = 43)
  
quant_dense &amp;lt;- quant_input %&amp;gt;% layer_dense(units = 64)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Are parts assembled, we wire them together using &lt;code&gt;layer_concatenate&lt;/code&gt;, and we’re good to call &lt;code&gt;keras_model&lt;/code&gt; to create the final graph.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;intermediate_layers &amp;lt;- list(embedding_layers, list(quant_dense)) %&amp;gt;% flatten()
inputs &amp;lt;- list(cat_inputs, list(quant_input)) %&amp;gt;% flatten()

l &amp;lt;- 0.25

output &amp;lt;- layer_concatenate(intermediate_layers) %&amp;gt;%
  layer_dense(units = 30, activation = &amp;quot;relu&amp;quot;, kernel_regularizer = regularizer_l2(l)) %&amp;gt;%
  layer_dropout(rate = 0.25) %&amp;gt;%
  layer_dense(units = 10, activation = &amp;quot;relu&amp;quot;, kernel_regularizer = regularizer_l2(l)) %&amp;gt;%
  layer_dropout(rate = 0.25) %&amp;gt;%
  layer_dense(units = 5, activation = &amp;quot;relu&amp;quot;, kernel_regularizer = regularizer_l2(l)) %&amp;gt;%
  layer_dropout(rate = 0.25) %&amp;gt;%
  layer_dense(units = 1, activation = &amp;quot;sigmoid&amp;quot;, kernel_regularizer = regularizer_l2(l))

model &amp;lt;- keras_model(inputs, output)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, if you’ve actually read through the whole of this part, you may wish for an easier way to get to this point. So let’s switch to feature specs for the rest of this post.&lt;/p&gt;
&lt;h2 id="feature-specs-to-the-rescue"&gt;Feature specs to the rescue&lt;/h2&gt;
&lt;p&gt;In spirit, the way feature specs are defined follows the example of the &lt;a href="https://cran.r-project.org/web/packages/recipes/index.html"&gt;recipes package&lt;/a&gt;. (It won’t make you hungry, though.) You initialize a feature spec with the prediction target – &lt;code&gt;feature_spec(target ~ .)&lt;/code&gt;, and then use the &lt;code&gt;%&amp;gt;%&lt;/code&gt; to tell it what to do with individual columns. “What to do” here signifies two things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First, how to “read in” the data. Are they numeric or categorical, and if categorical, what am I supposed to do with them? For example, should I treat all distinct symbols as distinct, resulting in, potentially, an enormous count of categories – or should I constrain myself to a fixed number of entities? Or hash them, even?&lt;/li&gt;
&lt;li&gt;Second, optional subsequent transformations. Numeric columns may be bucketized; categorical columns may be embedded. Or features could be combined to capture interaction.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this post, we demonstrate the use of a subset of &lt;code&gt;step_&lt;/code&gt; functions. The vignettes on &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/feature_columns.html"&gt;Feature columns&lt;/a&gt; and &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/feature_spec.html"&gt;Feature specs&lt;/a&gt; illustrate additional functions and their application.&lt;/p&gt;
&lt;p&gt;Starting from the beginning again, here is the complete code for data read-in and train-test split in the feature spec version.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
library(tfdatasets)
library(readr)
library(dplyr)

path &amp;lt;- &amp;quot;train.csv&amp;quot;

porto &amp;lt;- read_csv(path)

porto &amp;lt;- porto %&amp;gt;%
  mutate_at(vars(ends_with(&amp;quot;cat&amp;quot;)), as.character)

id_training &amp;lt;- sample.int(nrow(porto), size = 0.75*nrow(porto))
training &amp;lt;- porto[id_training,]
testing &amp;lt;- porto[-id_training,]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Data-prep-wise, recall what our goals are: leave alone if binary; scale if numeric; embed if categorical. Specifying all of this does not need more than a few lines of code:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ft_spec &amp;lt;- training %&amp;gt;%
  select(-id) %&amp;gt;%
  feature_spec(target ~ .) %&amp;gt;%
  step_numeric_column(ends_with(&amp;quot;bin&amp;quot;)) %&amp;gt;%
  step_numeric_column(-ends_with(&amp;quot;bin&amp;quot;),
                      -ends_with(&amp;quot;cat&amp;quot;),
                      normalizer_fn = scaler_standard()
                      ) %&amp;gt;%
  step_categorical_column_with_vocabulary_list(ends_with(&amp;quot;cat&amp;quot;)) %&amp;gt;%
  step_embedding_column(ends_with(&amp;quot;cat&amp;quot;),
                        dimension = function(vocab_size) as.integer(sqrt(vocab_size) + 1)
                        ) %&amp;gt;%
  fit()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note how here we are passing in the training set, and just like with &lt;code&gt;recipes&lt;/code&gt;, we won’t need to repeat any of the steps for the validation set. Scaling is taken care of by &lt;code&gt;scaler_standard()&lt;/code&gt;, an optional transformation function passed in to &lt;code&gt;step_numeric_column&lt;/code&gt;. Categorical columns are meant to make use of the complete vocabulary&lt;a href="#fn4" class="footnote-ref" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; and pipe their outputs into embedding layers.&lt;/p&gt;
&lt;p&gt;Now, what actually happened when we called &lt;code&gt;fit()&lt;/code&gt;? A lot – for us, as we got rid of a ton of manual preparation. For TensorFlow, nothing really – it just came to know about a few pieces in the graph we’ll ask it to construct.&lt;/p&gt;
&lt;p&gt;But wait, – don’t we still have to build up that graph ourselves, connecting and concatenating layers? Concretely, above, we had to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;create the correct number of input layers, of correct shape; and&lt;/li&gt;
&lt;li&gt;wire them to their matching embedding layers, of correct dimensionality.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So here comes the real magic, and it has two steps.&lt;/p&gt;
&lt;p&gt;First, we easily create the input layers by calling &lt;code&gt;layer_input_from_dataset&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;`&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;inputs &amp;lt;- layer_input_from_dataset(porto %&amp;gt;% select(-target))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And second, we can extract the features from the feature spec and have &lt;code&gt;layer_dense_features&lt;/code&gt; create the necessary layers based on that information:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;layer_dense_features(ft_spec$dense_features())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Without further ado, we add a few dense layers, and there is our model. Magic!&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;output &amp;lt;- inputs %&amp;gt;%
  layer_dense_features(ft_spec$dense_features()) %&amp;gt;%
  layer_dense(units = 30, activation = &amp;quot;relu&amp;quot;, kernel_regularizer = regularizer_l2(l)) %&amp;gt;%
  layer_dropout(rate = 0.25) %&amp;gt;%
  layer_dense(units = 10, activation = &amp;quot;relu&amp;quot;, kernel_regularizer = regularizer_l2(l)) %&amp;gt;%
  layer_dropout(rate = 0.25) %&amp;gt;%
  layer_dense(units = 5, activation = &amp;quot;relu&amp;quot;, kernel_regularizer = regularizer_l2(l)) %&amp;gt;%
  layer_dropout(rate = 0.25) %&amp;gt;%
  layer_dense(units = 1, activation = &amp;quot;sigmoid&amp;quot;, kernel_regularizer = regularizer_l2(l))

model &amp;lt;- keras_model(inputs, output)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we feed this model? In the non-feature-columns example, we would have had to feed each input separately, passing a list of tensors. Now we can just pass it the complete training set all at once:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% fit(x = training, y = training$target)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the Kaggle competition, submissions are &lt;a href="https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/overview/evaluation"&gt;evaluated using the normalized Gini coefficient&lt;/a&gt;, which we can calculate with the help of a new metric available in Keras, &lt;code&gt;tf$keras$metrics$AUC()&lt;/code&gt;. For training, we can use an approximation to the AUC due to Yan et al. (2003) &lt;span class="citation"&gt;(Yan et al. 2003)&lt;/span&gt;. Then training is as straightforward as:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;auc &amp;lt;- tf$keras$metrics$AUC()

gini &amp;lt;- custom_metric(name = &amp;quot;gini&amp;quot;, function(y_true, y_pred) {
  2*auc(y_true, y_pred) - 1
})

# Yan, L., Dodier, R., Mozer, M. C., &amp;amp; Wolniewicz, R. (2003). 
# Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.
roc_auc_score &amp;lt;- function(y_true, y_pred) {

  pos = tf$boolean_mask(y_pred, tf$cast(y_true, tf$bool))
  neg = tf$boolean_mask(y_pred, !tf$cast(y_true, tf$bool))

  pos = tf$expand_dims(pos, 0L)
  neg = tf$expand_dims(neg, 1L)

  # original paper suggests performance is robust to exact parameter choice
  gamma = 0.2
  p     = 3

  difference = tf$zeros_like(pos * neg) + pos - neg - gamma

  masked = tf$boolean_mask(difference, difference &amp;lt; 0.0)

  tf$reduce_sum(tf$pow(-masked, p))
}

model %&amp;gt;%
  compile(
    loss = roc_auc_score,
    optimizer = optimizer_adam(),
    metrics = list(auc, gini)
  )

model %&amp;gt;%
  fit(
    x = training,
    y = training$target,
    epochs = 50,
    validation_data = list(testing, testing$target),
    batch_size = 512
  )

predictions &amp;lt;- predict(model, testing)
Metrics::auc(testing$target, predictions)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After 50 epochs, we achieve an AUC of 0.64 on the validation set, or equivalently, a Gini coefficient of 0.27. Not a bad result for a simple fully connected network!&lt;/p&gt;
&lt;p&gt;We’ve seen how using feature columns automates away a number of steps in setting up the network, so we can spend more time on actually tuning it. This is most impressively demonstrated on a dataset like this, with more than a handful categorical columns. However, to explain a bit more what to pay attention to when using feature columns, it’s better to choose a smaller example where we can easily do some peeking around.&lt;/p&gt;
&lt;p&gt;Let’s move on to the second application.&lt;/p&gt;
&lt;h2 id="interactions-and-what-to-look-out-for"&gt;Interactions, and what to look out for&lt;/h2&gt;
&lt;p&gt;To demonstrate the use of &lt;code&gt;step_crossed_column&lt;/code&gt; to capture interactions, we make use of the &lt;code&gt;rugged&lt;/code&gt; dataset from Richard McElreath’s &lt;em&gt;rethinking&lt;/em&gt; package.&lt;/p&gt;
&lt;p&gt;We want to predict log GDP based on terrain ruggedness, for a number of countries (170, to be precise). However, the effect of ruggedness is different in Africa as opposed to other continents. Citing from &lt;em&gt;Statistical Rethinking&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It makes sense that ruggedness is associated with poorer countries, in most of the world. Rugged terrain means transport is difficult. Which means market access is hampered. Which means reduced gross domestic product. So the reversed relationship within Africa is puzzling. Why should difficult terrain be associated with higher GDP per capita?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;If this relationship is at all causal, it may be because rugged regions of Africa were protected against the Atlantic and Indian Ocean slave trades. Slavers preferred to raid easily accessed settlements, with easy routes to the sea. Those regions that suffered under the slave trade understandably continue to suffer economically, long after the decline of slave-trading markets. However, an outcome like GDP has many influences, and is furthermore a strange measure of economic activity. So it is hard to be sure what’s going on here.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While the causal situation is difficult, the purely technical one is easily described: We want to learn an interaction. We could rely on the network finding out by itself (in this case it probably will, if we just give it enough parameters). But it’s an excellent occasion to showcase the new &lt;code&gt;step_crossed_column&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Loading the dataset, zooming in on the variables of interest, and normalizing them the way it is done in &lt;em&gt;Rethinking&lt;/em&gt;, we have:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(rethinking)
library(keras)
library(tfdatasets)
library(tibble)

data(rugged)

d &amp;lt;- rugged
d &amp;lt;- d[complete.cases(d$rgdppc_2000), ]

df &amp;lt;- tibble(
  log_gdp = log(d$rgdppc_2000)/mean(log(d$rgdppc_2000)),
  rugged = d$rugged/max(d$rugged),
  africa = d$cont_africa
)

df %&amp;gt;% glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Observations: 170
Variables: 3
$ log_gdp &amp;lt;dbl&amp;gt; 0.8797119, 0.9647547, 1.1662705, 1.1044854, 0.9149038,…
$ rugged  &amp;lt;dbl&amp;gt; 0.1383424702, 0.5525636891, 0.1239922606, 0.1249596904…
$ africa  &amp;lt;int&amp;gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, …&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s first forget about the interaction and do the very minimal thing required to work with this data. &lt;code&gt;rugged&lt;/code&gt; should be a numeric column, while &lt;code&gt;africa&lt;/code&gt; is categorical in nature, which means we use one of the &lt;code&gt;step_categorical_[...]&lt;/code&gt; functions on it. (In this case we happen to know there are just two categories, Africa and not-Africa, so we could as well treat the column as numeric like in the previous example; but in other applications that won’t be the case, so here we show a method that generalizes to categorical features in general.)&lt;/p&gt;
&lt;p&gt;So we start out creating a feature spec and adding the two predictor columns. We check the result using &lt;code&gt;feature_spec&lt;/code&gt;’s &lt;code&gt;dense_features()&lt;/code&gt; method:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ft_spec &amp;lt;- training %&amp;gt;%
  feature_spec(log_gdp ~ .) %&amp;gt;%
  step_numeric_column(rugged) %&amp;gt;%
  step_categorical_column_with_identity(africa, num_buckets = 2) 
  fit()

ft_spec$dense_features()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$rugged
NumericColumn(key=&amp;#39;rugged&amp;#39;, shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hm, that doesn’t look too good. Where’d &lt;code&gt;africa&lt;/code&gt; go? In fact, there is one more thing we should have done: convert the categorical column to an &lt;em&gt;indicator column&lt;/em&gt;. Why?&lt;/p&gt;
&lt;p&gt;The rule of thumb is, whenever you have something &lt;em&gt;categorical&lt;/em&gt;, including &lt;em&gt;crossed&lt;/em&gt;, you need to then transform it into something &lt;em&gt;numeric&lt;/em&gt;, which includes &lt;em&gt;indicator&lt;/em&gt; and &lt;em&gt;embedding&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Being a heuristic, this rule works overall, and it matches our intuition. There’s one exception though, &lt;code&gt;step_bucketized_column&lt;/code&gt;, which although it “feels” categorical actually does not need that conversion.&lt;/p&gt;
&lt;p&gt;Therefore, it is best to supplement that intuition with a simple lookup diagram, which is also part of the &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/feature_columns.html"&gt;feature columns vignette&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;With this diagram, the simple rule is: &lt;em&gt;We always need to end up with something that inherits from &lt;code&gt;DenseColumn&lt;/code&gt;&lt;/em&gt;. So:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;step_numeric_column&lt;/code&gt;, &lt;code&gt;step_indicator_column&lt;/code&gt;, and &lt;code&gt;step_embedding_column&lt;/code&gt; are standalone;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;step_bucketized_column&lt;/code&gt; is, too, however categorical it “feels”; and&lt;/li&gt;
&lt;li&gt;all &lt;code&gt;step_categorical_column_[...]&lt;/code&gt;, as well as &lt;code&gt;step_crossed_column&lt;/code&gt;, need to be transformed using one the &lt;em&gt;dense&lt;/em&gt; column types.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-07-09-feature-columns/images/feature_cols_hier.png" alt="For use with Keras, all features need to end up inheriting from DenseColumn somehow." width="586" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-17)For use with Keras, all features need to end up inheriting from DenseColumn somehow.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Thus, we can fix the situation like so:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ft_spec &amp;lt;- training %&amp;gt;%
  feature_spec(log_gdp ~ .) %&amp;gt;%
  step_numeric_column(rugged) %&amp;gt;%
  step_categorical_column_with_identity(africa, num_buckets = 2) %&amp;gt;%
  step_indicator_column(africa) %&amp;gt;%
  fit()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and now &lt;code&gt;ft_spec$dense_features()&lt;/code&gt; will show us&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$rugged
NumericColumn(key=&amp;#39;rugged&amp;#39;, shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)

$indicator_africa
IndicatorColumn(categorical_column=IdentityCategoricalColumn(key=&amp;#39;africa&amp;#39;, number_buckets=2.0, default_value=None))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What we really wanted to do is capture the interaction between ruggedness and continent. To this end, we first &lt;em&gt;bucketize&lt;/em&gt; &lt;code&gt;rugged&lt;/code&gt;, and then cross it with – already binary – &lt;code&gt;africa&lt;/code&gt;. As per the rules, we finally transform into an &lt;em&gt;indicator&lt;/em&gt; column:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ft_spec &amp;lt;- training %&amp;gt;%
  feature_spec(log_gdp ~ .) %&amp;gt;%
  step_numeric_column(rugged) %&amp;gt;%
  step_categorical_column_with_identity(africa, num_buckets = 2) %&amp;gt;%
  step_indicator_column(africa) %&amp;gt;%
  step_bucketized_column(rugged,
                         boundaries = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.8)) %&amp;gt;%
  step_crossed_column(africa_rugged_interact = c(africa, bucketized_rugged),
                      hash_bucket_size = 16) %&amp;gt;%
  step_indicator_column(africa_rugged_interact) %&amp;gt;%
  fit()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at this code you may be asking yourself, now how many features do I have in the model? Let’s check.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ft_spec$dense_features()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$rugged
NumericColumn(key=&amp;#39;rugged&amp;#39;, shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)

$indicator_africa
IndicatorColumn(categorical_column=IdentityCategoricalColumn(key=&amp;#39;africa&amp;#39;, number_buckets=2.0, default_value=None))

$bucketized_rugged
BucketizedColumn(source_column=NumericColumn(key=&amp;#39;rugged&amp;#39;, shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.8))

$indicator_africa_rugged_interact
IndicatorColumn(categorical_column=CrossedColumn(keys=(IdentityCategoricalColumn(key=&amp;#39;africa&amp;#39;, number_buckets=2.0, default_value=None), BucketizedColumn(source_column=NumericColumn(key=&amp;#39;rugged&amp;#39;, shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.8))), hash_bucket_size=16.0, hash_key=None))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that all features, original or transformed, are kept, as long as they inherit from &lt;code&gt;DenseColumn&lt;/code&gt;. This means that, for example, the non-bucketized, continuous values of &lt;code&gt;rugged&lt;/code&gt; are used as well.&lt;/p&gt;
&lt;p&gt;Now setting up the training goes as expected.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;inputs &amp;lt;- layer_input_from_dataset(df %&amp;gt;% select(-log_gdp))

output &amp;lt;- inputs %&amp;gt;%
  layer_dense_features(ft_spec$dense_features()) %&amp;gt;%
  layer_dense(units = 8, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dense(units = 8, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dense(units = 1)

model &amp;lt;- keras_model(inputs, output)

model %&amp;gt;% compile(loss = &amp;quot;mse&amp;quot;, optimizer = &amp;quot;adam&amp;quot;, metrics = &amp;quot;mse&amp;quot;)

history &amp;lt;- model %&amp;gt;% fit(
  x = training,
  y = training$log_gdp,
  validation_data = list(testing, testing$log_gdp),
  epochs = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just as a sanity check, the final loss on the validation set for this code was ~ 0.014. But really this example did serve different purposes.&lt;/p&gt;
&lt;h2 id="in-a-nutshell"&gt;In a nutshell&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Feature specs&lt;/em&gt; are a convenient, elegant way of making categorical data available to Keras, as well as to chain useful transformations like bucketizing and creating crossed columns. The time you save data wrangling may go into tuning and experimentation. Enjoy, and thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-yan2003optimizing"&gt;
&lt;p&gt;Yan, Lian, Robert H Dodier, Michael Mozer, and Richard H Wolniewicz. 2003. “Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.” In &lt;em&gt;Proceedings of the 20th International Conference on Machine Learning (Icml-03)&lt;/em&gt;, 848–55.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;see e.g. &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-11-26-embeddings-fun-and-profit/"&gt;Entity embeddings for fun and profit&lt;/a&gt;&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;Having mentioned, above, that these really may be continuous or ordinal, from now on we’ll just call them numeric as we won’t make further use of the distinction. However, fitting ordinal data with neural networks is definitely a topic that would deserve its own post.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;In this dataset, every categorical variable is a column of singleton integers.&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;as indicated by leaving out the optional &lt;code&gt;vocabulary_list&lt;/code&gt;&lt;a href="#fnref4" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">df3c7e131361c3f6b2afe800d76521f0</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Tabular Data</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-07-09-feature-columns</guid>
      <pubDate>Tue, 09 Jul 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-07-09-feature-columns/images/feature_cols_hier.png" medium="image" type="image/png" width="1172" height="678"/>
    </item>
    <item>
      <title>Adding uncertainty estimates to Keras models with tfprobability</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability</link>
      <description>


&lt;p&gt;About six months ago, we showed &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-11-12-uncertainty_estimates_dropout/"&gt;how to create a custom wrapper&lt;/a&gt; to obtain uncertainty estimates from a Keras network. Today we present a less laborious, as well faster-running way using &lt;a href="https://rstudio.github.io/tfprobability/"&gt;tfprobability&lt;/a&gt;, the R wrapper to TensorFlow Probability. Like most posts on this blog, this one won’t be short, so let’s quickly state what you can expect in return of reading time.&lt;/p&gt;
&lt;h2 id="what-to-expect-from-this-post"&gt;What to expect from this post&lt;/h2&gt;
&lt;p&gt;Starting from what &lt;em&gt;not&lt;/em&gt; to expect: There won’t be a recipe that tells you how &lt;em&gt;exactly&lt;/em&gt; to set all parameters involved in order to report the “right” uncertainty measures. But then, what &lt;em&gt;are&lt;/em&gt; the “right” uncertainty measures? Unless you happen to work with a method that has no (hyper-)parameters to tweak, there will always be questions about how to report uncertainty.&lt;/p&gt;
&lt;p&gt;What you &lt;em&gt;can&lt;/em&gt; expect, though, is an introduction to obtaining uncertainty estimates for Keras networks, as well as an empirical report of how tweaking (hyper-)parameters may affect the results. As in the aforementioned post, we perform our tests on both a simulated and a real dataset, the &lt;a href="http://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant"&gt;Combined Cycle Power Plant Data Set&lt;/a&gt;. At the end, in place of strict rules, you should have acquired some intuition that will transfer to other real-world datasets.&lt;/p&gt;
&lt;p&gt;Did you notice our talking about &lt;em&gt;Keras networks&lt;/em&gt; above? Indeed this post has an additional goal: So far, we haven’t really discussed yet how &lt;code&gt;tfprobability&lt;/code&gt; goes together with &lt;code&gt;keras&lt;/code&gt;. Now we finally do (in short: they work together seemlessly).&lt;/p&gt;
&lt;p&gt;Finally, the notions of &lt;em&gt;aleatoric&lt;/em&gt; and &lt;em&gt;epistemic&lt;/em&gt; uncertainty, which may have stayed a bit abstract in the prior post, should get much more concrete here.&lt;/p&gt;
&lt;h2 id="aleatoric-vs.-epistemic-uncertainty"&gt;Aleatoric vs. epistemic uncertainty&lt;/h2&gt;
&lt;p&gt;Reminiscent somehow of the classic decomposition of generalization error into bias and variance, splitting uncertainty into its epistemic and aleatoric constituents separates an irreducible from a reducible part.&lt;/p&gt;
&lt;p&gt;The reducible part relates to imperfection in the model: In theory, if our model were perfect, epistemic uncertainty would vanish. Put differently, if the training data were unlimited – or if they comprised the whole population – we could just add capacity to the model until we’ve obtained a perfect fit.&lt;/p&gt;
&lt;p&gt;In contrast, normally there is variation in our measurements. There may be one true process that determines my resting heart rate; nonetheless, actual measurements will vary over time. There is nothing to be done about this: This is the aleatoric part that just remains, to be factored into our expectations.&lt;/p&gt;
&lt;p&gt;Now reading this, you might be thinking: “Wouldn’t a model that actually &lt;em&gt;were&lt;/em&gt; perfect capture those pseudo-random fluctuations?”. We’ll leave that phisosophical question be; instead, we’ll try to illustrate the usefulness of this distinction by example, in a practical way. In a nutshell, viewing a model’s &lt;em&gt;aleatoric&lt;/em&gt; uncertainty output should caution us to factor in appropriate deviations when making our predictions, while inspecting &lt;em&gt;epistemic&lt;/em&gt; uncertainty should help us re-think the appropriateness of the chosen model.&lt;/p&gt;
&lt;p&gt;Now let’s dive in and see how we may accomplish our goal with &lt;code&gt;tfprobability&lt;/code&gt;. We start with the simulated dataset.&lt;/p&gt;
&lt;h2 id="uncertainty-estimates-on-simulated-data"&gt;Uncertainty estimates on simulated data&lt;/h2&gt;
&lt;h3 id="dataset"&gt;Dataset&lt;/h3&gt;
&lt;p&gt;We re-use the dataset from the Google TensorFlow Probability team’s &lt;a href="https://medium.com/tensorflow/regression-with-probabilistic-layers-in-tensorflow-probability-e46ff5d37baf"&gt;blog post on the same subject&lt;/a&gt; &lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, with one exception: We extend the range of the independent variable a bit on the negative side, to better demonstrate the different methods’ behaviors.&lt;/p&gt;
&lt;p&gt;Here is the data-generating process. We also get library loading out of the way. Like the preceding posts on &lt;code&gt;tfprobability&lt;/code&gt;, this one too features recently added functionality, so please use the development versions of &lt;code&gt;tensorflow&lt;/code&gt; and &lt;code&gt;tfprobability&lt;/code&gt; as well as &lt;code&gt;keras&lt;/code&gt;. Call &lt;code&gt;install_tensorflow(version = "nightly")&lt;/code&gt; to obtain a current nightly build of TensorFlow and TensorFlow Probability:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# make sure we use the development versions of tensorflow, tfprobability and keras
devtools::install_github(&amp;quot;rstudio/tensorflow&amp;quot;)
devtools::install_github(&amp;quot;rstudio/tfprobability&amp;quot;)
devtools::install_github(&amp;quot;rstudio/keras&amp;quot;)

# and that we use a nightly build of TensorFlow and TensorFlow Probability
tensorflow::install_tensorflow(version = &amp;quot;nightly&amp;quot;)

library(tensorflow)
library(tfprobability)
library(keras)

library(dplyr)
library(tidyr)
library(ggplot2)

# make sure this code is compatible with TensorFlow 2.0
tf$compat$v1$enable_v2_behavior()

# generate the data
x_min &amp;lt;- -40
x_max &amp;lt;- 60
n &amp;lt;- 150
w0 &amp;lt;- 0.125
b0 &amp;lt;- 5

normalize &amp;lt;- function(x) (x - x_min) / (x_max - x_min)

# training data; predictor 
x &amp;lt;- x_min + (x_max - x_min) * runif(n) %&amp;gt;% as.matrix()

# training data; target
eps &amp;lt;- rnorm(n) * (3 * (0.25 + (normalize(x)) ^ 2))
y &amp;lt;- (w0 * x * (1 + sin(x)) + b0) + eps

# test data (predictor)
x_test &amp;lt;- seq(x_min, x_max, length.out = n) %&amp;gt;% as.matrix()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How does the data look?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggplot(data.frame(x = x, y = y), aes(x, y)) + geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-06-05-uncertainty-estimates-tfprobability/images/simdata.png" alt="Simulated data" width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-3)Simulated data
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The task here is single-predictor regression, which in principle we can achieve use Keras &lt;code&gt;dense&lt;/code&gt; layers. Let’s see how to enhance this by indicating uncertainty, starting from the aleatoric type.&lt;/p&gt;
&lt;h3 id="aleatoric-uncertainty"&gt;Aleatoric uncertainty&lt;/h3&gt;
&lt;p&gt;Aleatoric uncertainty, by definition, is not a statement about the model. So why not have the model learn the uncertainty inherent in the data?&lt;/p&gt;
&lt;p&gt;This is exactly how aleatoric uncertainty is operationalized in this approach. Instead of a single output per input – the predicted mean of the regression – here we have two outputs: one for the mean, and one for the standard deviation.&lt;/p&gt;
&lt;p&gt;How will we use these? Until shortly, we would have had to roll our own logic. Now with &lt;code&gt;tfprobability&lt;/code&gt;, we make the network output not tensors, but distributions – put differently, we make the last layer a &lt;a href="https://rstudio.github.io/tfprobability/reference/index.html#section-distribution-layers"&gt;distribution layer&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Distribution layers are Keras layers, but contributed by &lt;code&gt;tfprobability&lt;/code&gt;. The awesome thing is that we can train them with just tensors as targets, as usual: No need to compute probabilities ourselves.&lt;/p&gt;
&lt;p&gt;Several specialized distribution layers exist, such as &lt;a href="https://rstudio.github.io/tfprobability/reference/layer_kl_divergence_add_loss.html"&gt;layer_kl_divergence_add_loss&lt;/a&gt;, &lt;a href="https://rstudio.github.io/tfprobability/reference/layer_independent_bernoulli.html"&gt;layer_independent_bernoulli&lt;/a&gt;, or &lt;a href="https://rstudio.github.io/tfprobability/reference/layer_mixture_same_family.html"&gt;layer_mixture_same_family&lt;/a&gt;, but the most general is &lt;a href="https://rstudio.github.io/tfprobability/reference/layer_distribution_lambda.html"&gt;layer_distribution_lambda&lt;/a&gt;. &lt;code&gt;layer_distribution_lambda&lt;/code&gt; takes as inputs the preceding layer and outputs a distribution. In order to be able to do this, we need to tell it how to make use of the preceding layer’s activations.&lt;/p&gt;
&lt;p&gt;In our case, at some point we will want to have a &lt;code&gt;dense&lt;/code&gt; layer with two units.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;... %&amp;gt;% layer_dense(units = 2, activation = &amp;quot;linear&amp;quot;) %&amp;gt;%&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then &lt;code&gt;layer_distribution_lambda&lt;/code&gt; will use the first unit as the mean of a normal distribution, and the second as its standard deviation.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;layer_distribution_lambda(function(x)
    tfd_normal(loc = x[, 1, drop = FALSE],
               scale = 1e-3 + tf$math$softplus(x[, 2, drop = FALSE])
               )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the complete model we use. We insert an additional dense layer in front, with a &lt;code&gt;relu&lt;/code&gt; activation, to give the model a bit more freedom and capacity. We discuss this, as well as that &lt;code&gt;scale = ...&lt;/code&gt; foo, as soon as we’ve finished our walkthrough of model training.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_dense(units = 8, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dense(units = 2, activation = &amp;quot;linear&amp;quot;) %&amp;gt;%
  layer_distribution_lambda(function(x)
    tfd_normal(loc = x[, 1, drop = FALSE],
               # ignore on first read, we&amp;#39;ll come back to this
               # scale = 1e-3 + 0.05 * tf$math$softplus(x[, 2, drop = FALSE])
               scale = 1e-3 + tf$math$softplus(x[, 2, drop = FALSE])
               )
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For a model that outputs a distribution, the loss is the negative log likelihood given the target data.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;negloglik &amp;lt;- function(y, model) - (model %&amp;gt;% tfd_log_prob(y))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now compile and fit the model.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;learning_rate &amp;lt;- 0.01
model %&amp;gt;% compile(optimizer = optimizer_adam(lr = learning_rate), loss = negloglik)

model %&amp;gt;% fit(x, y, epochs = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now call the model on the test data to obtain the predictions. The predictions now actually are &lt;em&gt;distributions&lt;/em&gt;, and we have 150 of them, one for each datapoint:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;yhat &amp;lt;- model(tf$constant(x_test))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tfp.distributions.Normal(&amp;quot;sequential/distribution_lambda/Normal/&amp;quot;,
batch_shape=[150, 1], event_shape=[], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To obtain the means and standard deviations – the latter being that measure of aleatoric uncertainty we’re interested in – we just call &lt;a href="https://rstudio.github.io/tfprobability/reference/tfd_mean.html"&gt;tfd_mean&lt;/a&gt; and &lt;a href="https://rstudio.github.io/tfprobability/reference/tfd_stddev.html"&gt;tfd_stddev&lt;/a&gt; on these distributions. That will give us the predicted mean, as well as the predicted variance, &lt;em&gt;per datapoint&lt;/em&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;mean &amp;lt;- yhat %&amp;gt;% tfd_mean()
sd &amp;lt;- yhat %&amp;gt;% tfd_stddev()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s visualize this. Here are the actual test data points, the predicted means, as well as confidence bands indicating the mean estimate plus/minus two standard deviations.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggplot(data.frame(
  x = x,
  y = y,
  mean = as.numeric(mean),
  sd = as.numeric(sd)
),
aes(x, y)) +
  geom_point() +
  geom_line(aes(x = x_test, y = mean), color = &amp;quot;violet&amp;quot;, size = 1.5) +
  geom_ribbon(aes(
    x = x_test,
    ymin = mean - 2 * sd,
    ymax = mean + 2 * sd
  ),
  alpha = 0.2,
  fill = &amp;quot;grey&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-06-05-uncertainty-estimates-tfprobability/images/g_aleatoric_relu_8.png" alt="Aleatoric uncertainty on simulated data, using relu activation in the first dense layer." width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-12)Aleatoric uncertainty on simulated data, using relu activation in the first dense layer.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This looks pretty reasonable. What if we had used linear activation in the first layer? Meaning, what if the model had looked like this&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_dense(units = 8, activation = &amp;quot;linear&amp;quot;) %&amp;gt;%
  layer_dense(units = 2, activation = &amp;quot;linear&amp;quot;) %&amp;gt;%
  layer_distribution_lambda(function(x)
    tfd_normal(loc = x[, 1, drop = FALSE],
               scale = 1e-3 + 0.05 * tf$math$softplus(x[, 2, drop = FALSE])
               )
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This time, the model does not capture the “form” of the data that well, as we’ve disallowed any nonlinearities.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-06-05-uncertainty-estimates-tfprobability/images/g_aleatoric_linear_8.png" alt="Aleatoric uncertainty on simulated data, using linear activation in the first dense layer." width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-14)Aleatoric uncertainty on simulated data, using linear activation in the first dense layer.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Using linear activations only, we also need to do more experimenting with the &lt;code&gt;scale = ...&lt;/code&gt; line to get the result look “right”. With &lt;code&gt;relu&lt;/code&gt;, on the other hand, results are pretty robust to changes in how &lt;code&gt;scale&lt;/code&gt; is computed. Which activation do we choose? If our goal is to adequately model variation in the data, we can just choose &lt;code&gt;relu&lt;/code&gt; – and leave assessing &lt;em&gt;uncertainty in the model&lt;/em&gt; to a different technique (the &lt;em&gt;epistemic&lt;/em&gt; uncertainty that is up next).&lt;/p&gt;
&lt;p&gt;Overall, it seems like aleatoric uncertainty is the straightforward part. We want the network to learn the variation inherent in the data, which it does. What do we gain? Instead of obtaining just point estimates, which in this example might turn out pretty bad in the two fan-like areas of the data on the left and right sides, we learn about the spread as well. We’ll thus be appropriately cautious depending on what input range we’re making predictions for.&lt;/p&gt;
&lt;h3 id="epistemic-uncertainty"&gt;Epistemic uncertainty&lt;/h3&gt;
&lt;p&gt;Now our focus is on the model. Given a speficic model (e.g., one from the linear family), what kind of data does it say conforms to its expectations?&lt;/p&gt;
&lt;p&gt;To answer this question, we make use of a &lt;a href="https://rstudio.github.io/tfprobability/reference/layer_dense_variational.html"&gt;variational-dense layer&lt;/a&gt;. This is again a Keras layer provided by &lt;code&gt;tfprobability&lt;/code&gt;. Internally, it works by minimizing the &lt;em&gt;evidence lower bound&lt;/em&gt; (ELBO), thus striving to find an approximative posterior that does two things:&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;fit the actual data well (put differently: achieve high &lt;em&gt;log likelihood&lt;/em&gt;), and&lt;/li&gt;
&lt;li&gt;stay close to a &lt;em&gt;prior&lt;/em&gt; (as measured by &lt;a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"&gt;KL divergence&lt;/a&gt;).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As users, we actually specify the form of the posterior as well as that of the prior. Here is how a prior could look.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;prior_trainable &amp;lt;-
  function(kernel_size,
           bias_size = 0,
           dtype = NULL) {
    n &amp;lt;- kernel_size + bias_size
    keras_model_sequential() %&amp;gt;%
      # we&amp;#39;ll comment on this soon
      # layer_variable(n, dtype = dtype, trainable = FALSE) %&amp;gt;%
      layer_variable(n, dtype = dtype, trainable = TRUE) %&amp;gt;%
      layer_distribution_lambda(function(t) {
        tfd_independent(tfd_normal(loc = t, scale = 1),
                        reinterpreted_batch_ndims = 1)
      })
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This prior is itself a Keras model, containing a &lt;a href="https://rstudio.github.io/tfprobability/reference/layer_variable.html"&gt;layer that wraps a variable&lt;/a&gt; and a &lt;code&gt;layer_distribution_lambda&lt;/code&gt;, that type of distribution-yielding layer we’ve just encountered above. The variable layer could be fixed (non-trainable) or non-trainable, corresponding to a genuine prior or a prior learnt from the data in an &lt;em&gt;empirical Bayes&lt;/em&gt;-like way. The distribution layer outputs a normal distribution since we’re in a regression setting.&lt;/p&gt;
&lt;p&gt;The posterior too is a Keras model – definitely trainable this time. It too outputs a normal distribution:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;posterior_mean_field &amp;lt;-
  function(kernel_size,
           bias_size = 0,
           dtype = NULL) {
    n &amp;lt;- kernel_size + bias_size
    c &amp;lt;- log(expm1(1))
    keras_model_sequential(list(
      layer_variable(shape = 2 * n, dtype = dtype),
      layer_distribution_lambda(
        make_distribution_fn = function(t) {
          tfd_independent(tfd_normal(
            loc = t[1:n],
            scale = 1e-5 + tf$nn$softplus(c + t[(n + 1):(2 * n)])
            ), reinterpreted_batch_ndims = 1)
        }
      )
    ))
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we’ve defined both, we can set up the model’s layers. The first one, a variational-dense layer, has a single unit. The ensuing distribution layer then takes that unit’s output and uses it for the mean of a normal distribution – while the scale of that Normal is fixed at 1:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_dense_variational(
    units = 1,
    make_posterior_fn = posterior_mean_field,
    make_prior_fn = prior_trainable,
    kl_weight = 1 / n
  ) %&amp;gt;%
  layer_distribution_lambda(function(x)
    tfd_normal(loc = x, scale = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You may have noticed one argument to &lt;code&gt;layer_dense_variational&lt;/code&gt; we haven’t discussed yet, &lt;code&gt;kl_weight&lt;/code&gt;. This is used to scale the contribution to the total loss of the KL divergence, and normally should equal one over the number of data points.&lt;/p&gt;
&lt;p&gt;Training the model is straightforward. As users, we only specify the &lt;em&gt;negative log likelihood&lt;/em&gt; part of the loss; the KL divergence part is taken care of transparently by the framework.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;negloglik &amp;lt;- function(y, model) - (model %&amp;gt;% tfd_log_prob(y))
model %&amp;gt;% compile(optimizer = optimizer_adam(lr = learning_rate), loss = negloglik)
model %&amp;gt;% fit(x, y, epochs = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because of the stochasticity inherent in a variational-dense layer, each time we call this model, we obtain different results: different normal distributions, in this case. To obtain the uncertainty estimates we’re looking for, we therefore call the model a bunch of times – 100, say:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;yhats &amp;lt;- purrr::map(1:100, function(x) model(tf$constant(x_test)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now plot those 100 predictions – lines, in this case, as there are no nonlinearities:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;means &amp;lt;-
  purrr::map(yhats, purrr::compose(as.matrix, tfd_mean)) %&amp;gt;% abind::abind()

lines &amp;lt;- data.frame(cbind(x_test, means)) %&amp;gt;%
  gather(key = run, value = value,-X1)

mean &amp;lt;- apply(means, 1, mean)

ggplot(data.frame(x = x, y = y, mean = as.numeric(mean)), aes(x, y)) +
  geom_point() +
  geom_line(aes(x = x_test, y = mean), color = &amp;quot;violet&amp;quot;, size = 1.5) +
  geom_line(
    data = lines,
    aes(x = X1, y = value, color = run),
    alpha = 0.3,
    size = 0.5
  ) +
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-06-05-uncertainty-estimates-tfprobability/images/g_epistemic_linear_kl150.png" alt="Epistemic uncertainty on simulated data, using linear activation in the variational-dense layer." width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-21)Epistemic uncertainty on simulated data, using linear activation in the variational-dense layer.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;What we see here are essentially &lt;em&gt;different models&lt;/em&gt;, consistent with the assumptions built into the architecture. What we’re not accounting for is the spread in the data. Can we do both? We can; but first let’s comment on a few choices that were made and see how they affect the results.&lt;/p&gt;
&lt;p&gt;To prevent this post from growing to infinite size, we’ve refrained from performing a systematic experiment; please take what follows not as generalizable statements, but as &lt;em&gt;pointers to things you will want to keep in mind&lt;/em&gt; in your own ventures. Especially, each (hyper-)parameter is not an island; they could interact in unforeseen ways.&lt;/p&gt;
&lt;p&gt;After those words of caution, here are some things we noticed.&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;One question you might ask: Before, in the aleatoric uncertainty setup, we added an additional dense layer to the model, with &lt;code&gt;relu&lt;/code&gt; activation. What if we did this here? Firstly, we’re not adding any additional, non-variational layers in order to keep the setup “fully Bayesian” – we want priors at every level. As to using &lt;code&gt;relu&lt;/code&gt; in &lt;code&gt;layer_dense_variational&lt;/code&gt;, we did try that, and the results look pretty similar:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-06-05-uncertainty-estimates-tfprobability/images/g_epistemic_relu.png" alt="Epistemic uncertainty on simulated data, using relu activation in the variational-dense layer." width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-22)Epistemic uncertainty on simulated data, using relu activation in the variational-dense layer.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;However, things look pretty different if we drastically reduce training time… which brings us to the next observation.&lt;/p&gt;
&lt;ol start="2" style="list-style-type: decimal"&gt;
&lt;li&gt;Unlike in the aleatoric setup, the number of training epochs matter &lt;em&gt;a lot&lt;/em&gt;. If we train, &lt;em&gt;quote unquote&lt;/em&gt;, too long, the posterior estimates will get closer and closer to the posterior mean: we lose uncertainty. What happens if we train “too short” is even more notable. Here are the results for the linear-activation as well as the relu-activation cases:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-06-05-uncertainty-estimates-tfprobability/images/g_epistemic_100epochs.png" alt="Epistemic uncertainty on simulated data if we train for 100 epochs only. Left: linear activation. Right: relu activation." width="1010" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-23)Epistemic uncertainty on simulated data if we train for 100 epochs only. Left: linear activation. Right: relu activation.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Interestingly, both model families look very different now, and while the linear-activation family looks more reasonable at first, it still considers an overall negative slope consistent with the data.&lt;/p&gt;
&lt;p&gt;So how many epochs are “long enough”? From observation, we’d say that a working heuristic should probably be based on the rate of loss reduction. But certainly, it’ll make sense to try different numbers of epochs and check the effect on model behavior. As an aside, monitoring estimates over training time may even yield important insights into the assumptions built into a model (e.g., the effect of different activation functions).&lt;/p&gt;
&lt;ol start="3" style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;As important as the number of epochs trained, and similar in effect, is the &lt;em&gt;learning rate&lt;/em&gt;. If we replace the learning rate in this setup by &lt;code&gt;0.001&lt;/code&gt;, results will look similar to what we saw above for the &lt;code&gt;epochs = 100&lt;/code&gt; case. Again, we will want to try different learning rates and make sure we train the model “to completion” in some reasonable sense.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To conclude this section, let’s quickly look at what happens if we vary two other parameters. What if the prior were non-trainable (see the commented line above)? And what if we scaled the importance of the KL divergence (&lt;code&gt;kl_weight&lt;/code&gt; in &lt;code&gt;layer_dense_variational&lt;/code&gt;’s argument list) differently, replacing &lt;code&gt;kl_weight = 1/n&lt;/code&gt; by &lt;code&gt;kl_weight = 1&lt;/code&gt; (or equivalently, removing it)? Here are the respective results for an otherwise-default setup. They don’t lend themselves to generalization – on different (e.g., bigger!) datasets the outcomes will most certainly look different – but definitely interesting to observe.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-06-05-uncertainty-estimates-tfprobability/images/g_epistemic_100epochs.png" alt="Epistemic uncertainty on simulated data. Left: kl_weight = 1. Right: prior non-trainable." width="1010" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-24)Epistemic uncertainty on simulated data. Left: kl_weight = 1. Right: prior non-trainable.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now let’s come back to the question: We’ve modeled spread in the data, we’ve peeked into the heart of the model, – can we do both at the same time?&lt;/p&gt;
&lt;p&gt;We can, if we combine both approaches. We add an additional unit to the variational-dense layer and use this to learn the variance: once for each “sub-model” contained in the model.&lt;/p&gt;
&lt;h3 id="combining-both-aleatoric-and-epistemic-uncertainty"&gt;Combining both aleatoric and epistemic uncertainty&lt;/h3&gt;
&lt;p&gt;Reusing the prior and posterior from above, this is how the final model looks:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_dense_variational(
    units = 2,
    make_posterior_fn = posterior_mean_field,
    make_prior_fn = prior_trainable,
    kl_weight = 1 / n
  ) %&amp;gt;%
  layer_distribution_lambda(function(x)
    tfd_normal(loc = x[, 1, drop = FALSE],
               scale = 1e-3 + tf$math$softplus(0.01 * x[, 2, drop = FALSE])
               )
    )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We train this model just like the epistemic-uncertainty only one. We then obtain a measure of uncertainty &lt;em&gt;per predicted line&lt;/em&gt;. Or in the words we used above, we now have an ensemble of models each with its own indication of spread in the data. Here is a way we could display this – each colored line is the mean of a distribution, surrounded by a confidence band indicating +/- two standard deviations.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;yhats &amp;lt;- purrr::map(1:100, function(x) model(tf$constant(x_test)))
means &amp;lt;-
  purrr::map(yhats, purrr::compose(as.matrix, tfd_mean)) %&amp;gt;% abind::abind()
sds &amp;lt;-
  purrr::map(yhats, purrr::compose(as.matrix, tfd_stddev)) %&amp;gt;% abind::abind()

means_gathered &amp;lt;- data.frame(cbind(x_test, means)) %&amp;gt;%
  gather(key = run, value = mean_val,-X1)
sds_gathered &amp;lt;- data.frame(cbind(x_test, sds)) %&amp;gt;%
  gather(key = run, value = sd_val,-X1)

lines &amp;lt;-
  means_gathered %&amp;gt;% inner_join(sds_gathered, by = c(&amp;quot;X1&amp;quot;, &amp;quot;run&amp;quot;))
mean &amp;lt;- apply(means, 1, mean)

ggplot(data.frame(x = x, y = y, mean = as.numeric(mean)), aes(x, y)) +
  geom_point() +
  theme(legend.position = &amp;quot;none&amp;quot;) +
  geom_line(aes(x = x_test, y = mean), color = &amp;quot;violet&amp;quot;, size = 1.5) +
  geom_line(
    data = lines,
    aes(x = X1, y = mean_val, color = run),
    alpha = 0.6,
    size = 0.5
  ) +
  geom_ribbon(
    data = lines,
    aes(
      x = X1,
      ymin = mean_val - 2 * sd_val,
      ymax = mean_val + 2 * sd_val,
      group = run
    ),
    alpha = 0.05,
    fill = &amp;quot;grey&amp;quot;,
    inherit.aes = FALSE
  )&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-06-05-uncertainty-estimates-tfprobability/images/g_both_scale0.01.png" alt="Displaying both epistemic and aleatoric uncertainty on the simulated dataset." width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-27)Displaying both epistemic and aleatoric uncertainty on the simulated dataset.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Nice! This looks like something we could report.&lt;/p&gt;
&lt;p&gt;As you might imagine, this model, too, is sensitive to how long (think: number of epochs) or how fast (think: learning rate) we train it. And compared to the epistemic-uncertainty only model, there is an additional choice to be made here: the scaling of the previous layer’s activation – the &lt;code&gt;0.01&lt;/code&gt; in the &lt;code&gt;scale&lt;/code&gt; argument to &lt;code&gt;tfd_normal&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;scale = 1e-3 + tf$math$softplus(0.01 * x[, 2, drop = FALSE])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keeping everything else constant, here we vary that parameter between &lt;code&gt;0.01&lt;/code&gt; and &lt;code&gt;0.05&lt;/code&gt;:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-06-05-uncertainty-estimates-tfprobability/images/g_both_scale_all.png" alt="Epistemic plus aleatoric uncertainty on the simulated dataset: Varying the scale argument." width="1515" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-29)Epistemic plus aleatoric uncertainty on the simulated dataset: Varying the scale argument.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Evidently, this is another parameter we should be prepared to experiment with.&lt;/p&gt;
&lt;p&gt;Now that we’ve introduced all three types of presenting uncertainty – aleatoric only, epistemic only, or both – let’s see them on the aforementioned &lt;a href="http://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant"&gt;Combined Cycle Power Plant Data Set&lt;/a&gt;. Please see &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-11-12-uncertainty_estimates_dropout/"&gt;our previous post on uncertainty&lt;/a&gt; for a quick characterization, as well as visualization, of the dataset.&lt;/p&gt;
&lt;h2 id="combined-cycle-power-plant-data-set"&gt;Combined Cycle Power Plant Data Set&lt;/h2&gt;
&lt;p&gt;To keep this post at a digestible length, we’ll refrain from trying as many alternatives as with the simulated data and mainly stay with what worked well there. This should also give us an idea of how well these “defaults” generalize. We separately inspect two scenarios: The single-predictor setup (using each of the four available predictors alone), and the complete one (using all four predictors at once).&lt;/p&gt;
&lt;p&gt;The dataset is loaded just as in the previous post.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tensorflow)
library(tfprobability)
library(keras)

library(dplyr)
library(tidyr)
library(readxl)

# make sure this code is compatible with TensorFlow 2.0
tf$compat$v1$enable_v2_behavior()

df &amp;lt;- read_xlsx(&amp;quot;CCPP/Folds5x2_pp.xlsx&amp;quot;)

df_scaled &amp;lt;- scale(df)
centers &amp;lt;- attr(df_scaled, &amp;quot;scaled:center&amp;quot;)
scales &amp;lt;- attr(df_scaled, &amp;quot;scaled:scale&amp;quot;)

X &amp;lt;- df_scaled[, 1:4]
train_samples &amp;lt;- sample(1:nrow(df_scaled), 0.8 * nrow(X))
X_train &amp;lt;- X[train_samples, ]
X_val &amp;lt;- X[-train_samples, ]

y &amp;lt;- df_scaled[, 5] 
y_train &amp;lt;- y[train_samples] %&amp;gt;% as.matrix()
y_val &amp;lt;- y[-train_samples] %&amp;gt;% as.matrix()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First we look at the single-predictor case, starting from aleatoric uncertainty.&lt;/p&gt;
&lt;h3 id="single-predictor-aleatoric-uncertainty"&gt;Single predictor: Aleatoric uncertainty&lt;/h3&gt;
&lt;p&gt;Here is the “default” aleatoric model again. We also duplicate the plotting code here for the reader’s convenience.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;n &amp;lt;- nrow(X_train) # 7654
n_epochs &amp;lt;- 10 # we need fewer epochs because the dataset is so much bigger

batch_size &amp;lt;- 100

learning_rate &amp;lt;- 0.01

# variable to fit - change to 2,3,4 to get the other predictors
i &amp;lt;- 1

model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_dense(units = 16, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dense(units = 2, activation = &amp;quot;linear&amp;quot;) %&amp;gt;%
  layer_distribution_lambda(function(x)
    tfd_normal(loc = x[, 1, drop = FALSE],
               scale = tf$math$softplus(x[, 2, drop = FALSE])
               )
    )

negloglik &amp;lt;- function(y, model) - (model %&amp;gt;% tfd_log_prob(y))

model %&amp;gt;% compile(optimizer = optimizer_adam(lr = learning_rate), loss = negloglik)

hist &amp;lt;-
  model %&amp;gt;% fit(
    X_train[, i, drop = FALSE],
    y_train,
    validation_data = list(X_val[, i, drop = FALSE], y_val),
    epochs = n_epochs,
    batch_size = batch_size
  )

yhat &amp;lt;- model(tf$constant(X_val[, i, drop = FALSE]))

mean &amp;lt;- yhat %&amp;gt;% tfd_mean()
sd &amp;lt;- yhat %&amp;gt;% tfd_stddev()

ggplot(data.frame(
  x = X_val[, i],
  y = y_val,
  mean = as.numeric(mean),
  sd = as.numeric(sd)
),
aes(x, y)) +
  geom_point() +
  geom_line(aes(x = x, y = mean), color = &amp;quot;violet&amp;quot;, size = 1.5) +
  geom_ribbon(aes(
    x = x,
    ymin = mean - 2 * sd,
    ymax = mean + 2 * sd
  ),
  alpha = 0.4,
  fill = &amp;quot;grey&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How well does this work?&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-06-05-uncertainty-estimates-tfprobability/images/uci_aleatoric.png" alt="Aleatoric uncertainty on the Combined Cycle Power Plant Data Set; single predictors." width="1010" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-32)Aleatoric uncertainty on the Combined Cycle Power Plant Data Set; single predictors.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This looks pretty good we’d say! How about epistemic uncertainty?&lt;/p&gt;
&lt;h3 id="single-predictor-epistemic-uncertainty"&gt;Single predictor: Epistemic uncertainty&lt;/h3&gt;
&lt;p&gt;Here’s the code:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;posterior_mean_field &amp;lt;-
  function(kernel_size,
           bias_size = 0,
           dtype = NULL) {
    n &amp;lt;- kernel_size + bias_size
    c &amp;lt;- log(expm1(1))
    keras_model_sequential(list(
      layer_variable(shape = 2 * n, dtype = dtype),
      layer_distribution_lambda(
        make_distribution_fn = function(t) {
          tfd_independent(tfd_normal(
            loc = t[1:n],
            scale = 1e-5 + tf$nn$softplus(c + t[(n + 1):(2 * n)])
          ), reinterpreted_batch_ndims = 1)
        }
      )
    ))
  }

prior_trainable &amp;lt;-
  function(kernel_size,
           bias_size = 0,
           dtype = NULL) {
    n &amp;lt;- kernel_size + bias_size
    keras_model_sequential() %&amp;gt;%
      layer_variable(n, dtype = dtype, trainable = TRUE) %&amp;gt;%
      layer_distribution_lambda(function(t) {
        tfd_independent(tfd_normal(loc = t, scale = 1),
                        reinterpreted_batch_ndims = 1)
      })
  }

model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_dense_variational(
    units = 1,
    make_posterior_fn = posterior_mean_field,
    make_prior_fn = prior_trainable,
    kl_weight = 1 / n,
    activation = &amp;quot;linear&amp;quot;,
  ) %&amp;gt;%
  layer_distribution_lambda(function(x)
    tfd_normal(loc = x, scale = 1))

negloglik &amp;lt;- function(y, model) - (model %&amp;gt;% tfd_log_prob(y))
model %&amp;gt;% compile(optimizer = optimizer_adam(lr = learning_rate), loss = negloglik)
hist &amp;lt;-
  model %&amp;gt;% fit(
    X_train[, i, drop = FALSE],
    y_train,
    validation_data = list(X_val[, i, drop = FALSE], y_val),
    epochs = n_epochs,
    batch_size = batch_size
  )

yhats &amp;lt;- purrr::map(1:100, function(x)
  yhat &amp;lt;- model(tf$constant(X_val[, i, drop = FALSE])))
  
means &amp;lt;-
  purrr::map(yhats, purrr::compose(as.matrix, tfd_mean)) %&amp;gt;% abind::abind()

lines &amp;lt;- data.frame(cbind(X_val[, i], means)) %&amp;gt;%
  gather(key = run, value = value,-X1)

mean &amp;lt;- apply(means, 1, mean)
ggplot(data.frame(x = X_val[, i], y = y_val, mean = as.numeric(mean)), aes(x, y)) +
  geom_point() +
  geom_line(aes(x = X_val[, i], y = mean), color = &amp;quot;violet&amp;quot;, size = 1.5) +
  geom_line(
    data = lines,
    aes(x = X1, y = value, color = run),
    alpha = 0.3,
    size = 0.5
  ) +
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And this is the result.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-06-05-uncertainty-estimates-tfprobability/images/uci_epistemic.png" alt="Epistemic uncertainty on the Combined Cycle Power Plant Data Set; single predictors." width="1010" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-34)Epistemic uncertainty on the Combined Cycle Power Plant Data Set; single predictors.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As with the simulated data, the linear models seems to “do the right thing”. And here too, we think we will want to augment this with the spread in the data: Thus, on to way three.&lt;/p&gt;
&lt;h3 id="single-predictor-combining-both-types"&gt;Single predictor: Combining both types&lt;/h3&gt;
&lt;p&gt;Here we go. Again, &lt;code&gt;posterior_mean_field&lt;/code&gt; and &lt;code&gt;prior_trainable&lt;/code&gt; look just like in the epistemic-only case.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_dense_variational(
    units = 2,
    make_posterior_fn = posterior_mean_field,
    make_prior_fn = prior_trainable,
    kl_weight = 1 / n,
    activation = &amp;quot;linear&amp;quot;
  ) %&amp;gt;%
  layer_distribution_lambda(function(x)
    tfd_normal(loc = x[, 1, drop = FALSE],
               scale = 1e-3 + tf$math$softplus(0.01 * x[, 2, drop = FALSE])))


negloglik &amp;lt;- function(y, model)
  - (model %&amp;gt;% tfd_log_prob(y))
model %&amp;gt;% compile(optimizer = optimizer_adam(lr = learning_rate), loss = negloglik)
hist &amp;lt;-
  model %&amp;gt;% fit(
    X_train[, i, drop = FALSE],
    y_train,
    validation_data = list(X_val[, i, drop = FALSE], y_val),
    epochs = n_epochs,
    batch_size = batch_size
  )

yhats &amp;lt;- purrr::map(1:100, function(x)
  model(tf$constant(X_val[, i, drop = FALSE])))
means &amp;lt;-
  purrr::map(yhats, purrr::compose(as.matrix, tfd_mean)) %&amp;gt;% abind::abind()
sds &amp;lt;-
  purrr::map(yhats, purrr::compose(as.matrix, tfd_stddev)) %&amp;gt;% abind::abind()

means_gathered &amp;lt;- data.frame(cbind(X_val[, i], means)) %&amp;gt;%
  gather(key = run, value = mean_val,-X1)
sds_gathered &amp;lt;- data.frame(cbind(X_val[, i], sds)) %&amp;gt;%
  gather(key = run, value = sd_val,-X1)

lines &amp;lt;-
  means_gathered %&amp;gt;% inner_join(sds_gathered, by = c(&amp;quot;X1&amp;quot;, &amp;quot;run&amp;quot;))

mean &amp;lt;- apply(means, 1, mean)

#lines &amp;lt;- lines %&amp;gt;% filter(run==&amp;quot;X3&amp;quot; | run ==&amp;quot;X4&amp;quot;)

ggplot(data.frame(x = X_val[, i], y = y_val, mean = as.numeric(mean)), aes(x, y)) +
  geom_point() +
  theme(legend.position = &amp;quot;none&amp;quot;) +
  geom_line(aes(x = X_val[, i], y = mean), color = &amp;quot;violet&amp;quot;, size = 1.5) +
  geom_line(
    data = lines,
    aes(x = X1, y = mean_val, color = run),
    alpha = 0.2,
    size = 0.5
  ) +
geom_ribbon(
  data = lines,
  aes(
    x = X1,
    ymin = mean_val - 2 * sd_val,
    ymax = mean_val + 2 * sd_val,
    group = run
  ),
  alpha = 0.01,
  fill = &amp;quot;grey&amp;quot;,
  inherit.aes = FALSE
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the output?&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-06-05-uncertainty-estimates-tfprobability/images/uci_both.png" alt="Combined uncertainty on the Combined Cycle Power Plant Data Set; single predictors." width="1010" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-36)Combined uncertainty on the Combined Cycle Power Plant Data Set; single predictors.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This looks useful! Let’s wrap up with our final test case: Using all four predictors together.&lt;/p&gt;
&lt;h3 id="all-predictors"&gt;All predictors&lt;/h3&gt;
&lt;p&gt;The training code used in this scenario looks just like before, apart from our feeding all predictors to the model. For plotting, we resort to displaying the first principal component on the x-axis – this makes the plots look noisier than before. We also display fewer lines for the epistemic and epistemic-plus-aleatoric cases (20 instead of 100). Here are the results:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-06-05-uncertainty-estimates-tfprobability/images/uci_allpreds_all.png" alt="Uncertainty (aleatoric, epistemic, both) on the Combined Cycle Power Plant Data Set; all predictors." width="1515" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-37)Uncertainty (aleatoric, epistemic, both) on the Combined Cycle Power Plant Data Set; all predictors.
&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Where does this leave us? Compared to the learnable-dropout approach described in the prior post, the way presented here is a lot easier, faster, and more intuitively understandable. The methods per se are that easy to use that in this first introductory post, we could afford to explore alternatives already: something we had no time to do in that previous exposition.&lt;/p&gt;
&lt;p&gt;In fact, we hope this post leaves you in a position to do your own experiments, on your own data. Obviously, you will have to make decisions, but isn’t that the way it is in data science? There’s no way around making decisions; we just should be prepared to justify them … Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;see also the corresponding &lt;a href="https://colab.research.google.com/github/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Probabilistic_Layers_Regression.ipynb#scrollTo=5zCEYpzu7bDX"&gt;notebook&lt;/a&gt;&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;yes, we also use that other line for &lt;code&gt;scale&lt;/code&gt; that was commented before; more on that in a second&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">4e197d00e740be2fdfdf1bcd4398fef1</distill:md5>
      <category>Probabilistic ML/DL</category>
      <category>TensorFlow/Keras</category>
      <category>Concepts</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability</guid>
      <pubDate>Wed, 05 Jun 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/images/uci_both.png" medium="image" type="image/png" width="2020" height="1020"/>
    </item>
    <item>
      <title>Hierarchical partial pooling, continued: Varying slopes models with TensorFlow Probability</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-05-24-varying-slopes</link>
      <description>


&lt;p&gt;In a &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-01-08-getting-started-with-tf-probability/"&gt;previous post&lt;/a&gt;, we showed how to use &lt;a href="https://rstudio.github.io/tfprobability/"&gt;tfprobability&lt;/a&gt; – the R interface to TensorFlow Probability – to build a &lt;em&gt;multilevel&lt;/em&gt;, or &lt;em&gt;partial pooling&lt;/em&gt; model of tadpole survival in differently sized (and thus, differing in inhabitant number) tanks.&lt;/p&gt;
&lt;p&gt;A completely &lt;em&gt;pooled&lt;/em&gt; model would have resulted in a global estimate of survival count, irrespective of tank, while an &lt;em&gt;unpooled&lt;/em&gt; model would have learned to predict survival count for each tank separately. The former approach does not take into account different circumstances; the latter does not make use of common information. (Also, it clearly has no predictive use unless we want to make predictions for the very same entities we used to train the model.)&lt;/p&gt;
&lt;p&gt;In contrast, a &lt;em&gt;partially pooled&lt;/em&gt; model lets you make predictions for the familiar, as well as new entities: Just use the appropriate prior.&lt;/p&gt;
&lt;p&gt;Assuming we &lt;em&gt;are&lt;/em&gt; in fact interested in the same entities – why would we want to apply partial pooling? For the same reasons so much effort in machine learning goes into devising regularization mechanisms. We don’t want to overfit too much to actual measurements, be they related to the same entity or a class of entities. If I want to predict my heart rate as I wake up next morning, based on a single measurement I’m taking now (let’s say it’s evening and I’m frantically typing a blog post), I better take into account some facts about heart rate behavior in general (instead of just projecting into the future the exact value measured right now).&lt;/p&gt;
&lt;p&gt;In the tadpole example, this means we expect generalization to work better for tanks with many inhabitants, compared to more solitary environments. For the latter ones, we better take a peek at survival rates from other tanks, to supplement the sparse, idiosyncratic information available. Or using the technical term, in the latter case we hope for the model to &lt;em&gt;shrink&lt;/em&gt; its estimates toward the overall mean more noticeably than in the former.&lt;/p&gt;
&lt;p&gt;This type of information sharing is already very useful, but it gets better. The tadpole model is a &lt;em&gt;varying intercepts&lt;/em&gt; model, as McElreath calls it (or &lt;em&gt;random intercepts&lt;/em&gt;, as it is sometimes – confusingly – called &lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;) – &lt;em&gt;intercepts&lt;/em&gt; referring to the way we make predictions for entities (here: tanks), with no predictor variables present. So if we can pool information about intercepts, why not pool information about &lt;em&gt;slopes&lt;/em&gt; as well? This will allow us to, in addition, make use of &lt;em&gt;relationships&lt;/em&gt; between variables learnt on different entities in the training set.&lt;/p&gt;
&lt;p&gt;So as you might have guessed by now, &lt;em&gt;varying slopes&lt;/em&gt; (or &lt;em&gt;random slopes&lt;/em&gt;, if you will) is the topic of today’s post. Again, we take up an example from McElreath’s book, and show how to accomplish the same thing with &lt;code&gt;tfprobability&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="coffee-please"&gt;Coffee, please&lt;/h2&gt;
&lt;p&gt;Unlike the tadpole case, this time we work with simulated data. This is the data McElreath uses to introduce the &lt;em&gt;varying slopes&lt;/em&gt; modeling technique; he then goes on and applies it to one of the book’s most featured datasets, the &lt;em&gt;pro-social&lt;/em&gt; (or indifferent, rather!) chimpanzees. For today, we stay with the simulated data for two reasons: First, the subject matter per se is non-trivial enough; and second, we want to keep careful track of what our model does, and whether its output is sufficiently close to the results McElreath obtained from &lt;em&gt;Stan&lt;/em&gt; &lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So, the scenario is this. &lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; Cafés vary in how popular they are. In a popular café, when you order coffee, you’re likely to &lt;em&gt;wait&lt;/em&gt;. In a less popular café, you’ll likely be served much faster. That’s one thing. Second, all cafés tend to be more crowded in the mornings than in the afternoons. Thus in the morning, you’ll wait longer than in the afternoon – this goes for the popular as well as the less popular cafés.&lt;/p&gt;
&lt;p&gt;In terms of intercepts and slopes, we can picture the morning waits as intercepts, and the resultant afternoon waits as arising due to the slopes of the lines joining each morning and afternoon wait, respectively.&lt;/p&gt;
&lt;p&gt;So when we partially-pool &lt;em&gt;intercepts&lt;/em&gt;, we have one “intercept prior” (itself constrained by a prior, of course), and a set of café-specific intercepts that will vary around it. When we partially-pool &lt;em&gt;slopes&lt;/em&gt;, we have a “slope prior” reflecting the overall relationship between morning and afternoon waits, and a set of café-specific slopes reflecting the individual relationships. Cognitively, that means that if you have never been to the &lt;em&gt;Café Gerbeaud&lt;/em&gt; in Budapest but have been to cafés before, you might have a less-than-uninformed idea about how long you are going to wait; it also means that if you normally get your coffee in your favorite corner café in the mornings, and now you pass by there in the afternoon, you have an approximate idea how long it’s going to take (namely, fewer minutes than in the mornings).&lt;/p&gt;
&lt;p&gt;So is that all? Actually, no. In our scenario, intercepts and slopes are related. If, at a less popular café, I always get my coffee before two minutes have passed, there is little room for improvement. At a highly popular café though, if it could easily take ten minutes in the mornings, then there is quite some potential for decrease in waiting time in the afternoon. So in my prediction for this afternoon’s waiting time, I should factor in this interaction effect.&lt;/p&gt;
&lt;p&gt;So, now that we have an idea of what this is all about, let’s see how we can model these effects with &lt;code&gt;tfprobability&lt;/code&gt;. But first, we actually have to generate the data.&lt;/p&gt;
&lt;h2 id="simulate-the-data"&gt;Simulate the data&lt;/h2&gt;
&lt;p&gt;We directly follow McElreath in the way the data are generated.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;##### Inputs needed to generate the covariance matrix between intercepts and slopes #####

# average morning wait time
a &amp;lt;- 3.5
# average difference afternoon wait time
# we wait less in the afternoons
b &amp;lt;- -1
# standard deviation in the (café-specific) intercepts
sigma_a &amp;lt;- 1
# standard deviation in the (café-specific) slopes
sigma_b &amp;lt;- 0.5
# correlation between intercepts and slopes
# the higher the intercept, the more the wait goes down
rho &amp;lt;- -0.7


##### Generate the covariance matrix #####

# means of intercepts and slopes
mu &amp;lt;- c(a, b)
# standard deviations of means and slopes
sigmas &amp;lt;- c(sigma_a, sigma_b) 
# correlation matrix
# a correlation matrix has ones on the diagonal and the correlation in the off-diagonals
rho &amp;lt;- matrix(c(1, rho, rho, 1), nrow = 2) 
# now matrix multiply to get covariance matrix
cov_matrix &amp;lt;- diag(sigmas) %*% rho %*% diag(sigmas)


##### Generate the café-specific intercepts and slopes #####

# 20 cafés overall
n_cafes &amp;lt;- 20

library(MASS)
set.seed(5) # used to replicate example
# multivariate distribution of intercepts and slopes
vary_effects &amp;lt;- mvrnorm(n_cafes , mu ,cov_matrix)
# intercepts are in the first column
a_cafe &amp;lt;- vary_effects[ ,1]
# slopes are in the second
b_cafe &amp;lt;- vary_effects[ ,2]


##### Generate the actual wait times #####

set.seed(22)
# 10 visits per café
n_visits &amp;lt;- 10

# alternate values for mornings and afternoons in the data frame
afternoon &amp;lt;- rep(0:1, n_visits * n_cafes/2)
# data for each café are consecutive rows in the data frame
cafe_id &amp;lt;- rep(1:n_cafes, each = n_visits)

# the regression equation for the mean waiting time
mu &amp;lt;- a_cafe[cafe_id] + b_cafe[cafe_id] * afternoon
# standard deviation of waiting time within cafés
sigma &amp;lt;- 0.5 # std dev within cafes
# generate instances of waiting times
wait &amp;lt;- rnorm(n_visits * n_cafes, mu, sigma)

d &amp;lt;- data.frame(cafe = cafe_id, afternoon = afternoon, wait = wait)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Take a glimpse at the data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;d %&amp;gt;% glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Observations: 200
Variables: 3
$ cafe      &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3,...
$ afternoon &amp;lt;int&amp;gt; 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,...
$ wait      &amp;lt;dbl&amp;gt; 3.9678929, 3.8571978, 4.7278755, 2.7610133, 4.1194827, 3.54365,...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On to building the model.&lt;/p&gt;
&lt;h2 id="the-model"&gt;The model&lt;/h2&gt;
&lt;p&gt;As in the &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-05-06-tadpoles-on-tensorflow/"&gt;previous post on multi-level modeling&lt;/a&gt;, we use &lt;a href="https://rstudio.github.io/tfprobability/reference/tfd_joint_distribution_sequential.html"&gt;tfd_joint_distribution_sequential&lt;/a&gt; to define the model and &lt;a href="https://rstudio.github.io/tfprobability/reference/mcmc_hamiltonian_monte_carlo.html"&gt;Hamiltonian Monte Carlo&lt;/a&gt; for sampling. Consider taking a look at the first section of that post for a quick reminder of the overall procedure.&lt;/p&gt;
&lt;p&gt;Before we code the model, let’s quickly get library loading out of the way. Importantly, again just like in the previous post, we need to install a &lt;code&gt;master&lt;/code&gt; build of TensorFlow Probability, as we’re making use of very new features not yet available in the current release version. The same goes for the R packages &lt;code&gt;tensorflow&lt;/code&gt; and &lt;code&gt;tfprobability&lt;/code&gt;: Please install the respective development versions from github.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;devtools::install_github(&amp;quot;rstudio/tensorflow&amp;quot;)
devtools::install_github(&amp;quot;rstudio/tfprobability&amp;quot;)

# this will install the latest nightlies of TensorFlow as well as TensorFlow Probability
tensorflow::install_tensorflow(version = &amp;quot;nightly&amp;quot;)

library(tensorflow)
tf$compat$v1$enable_v2_behavior()

library(tfprobability)

library(tidyverse)
library(zeallot)
library(abind)
library(gridExtra)
library(HDInterval)
library(ellipse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now here is the model definition. We’ll go through it step by step in an instant.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- function(cafe_id) {
  tfd_joint_distribution_sequential(
      list(
        # rho, the prior for the correlation matrix between intercepts and slopes
        tfd_cholesky_lkj(2, 2), 
        # sigma, prior variance for the waiting time
        tfd_sample_distribution(tfd_exponential(rate = 1), sample_shape = 1),
        # sigma_cafe, prior of variances for intercepts and slopes (vector of 2)
        tfd_sample_distribution(tfd_exponential(rate = 1), sample_shape = 2), 
        # b, the prior mean for the slopes
        tfd_sample_distribution(tfd_normal(loc = -1, scale = 0.5), sample_shape = 1),
        # a, the prior mean for the intercepts
        tfd_sample_distribution(tfd_normal(loc = 5, scale = 2), sample_shape = 1), 
        # mvn, multivariate distribution of intercepts and slopes
        # shape: batch size, 20, 2
        function(a,b,sigma_cafe,sigma,chol_rho) 
          tfd_sample_distribution(
            tfd_multivariate_normal_tri_l(
              loc = tf$concat(list(a,b), axis = -1L),
              scale_tril = tf$linalg$LinearOperatorDiag(sigma_cafe)$matmul(chol_rho)),
            sample_shape = n_cafes),
        # waiting time
        # shape should be batch size, 200
        function(mvn, a, b, sigma_cafe, sigma)
          tfd_independent(
            # need to pull out the correct cafe_id in the middle column
            tfd_normal(
              loc = (tf$gather(mvn[ , , 1], cafe_id, axis = -1L) +
                       tf$gather(mvn[ , , 2], cafe_id, axis = -1L) * afternoon), 
              scale=sigma),  # Shape [batch,  1]
        reinterpreted_batch_ndims=1
        )
    )
  )
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first five distributions are priors. First, we have the prior for the correlation matrix. Basically, this would be an &lt;a href="https://rstudio.github.io/tfprobability/reference/tfd_lkj.html"&gt;LKJ distribution&lt;/a&gt; of shape &lt;code&gt;2x2&lt;/code&gt; and with &lt;em&gt;concentration&lt;/em&gt; parameter equal to 2.&lt;/p&gt;
&lt;p&gt;For performance reasons, we work with a version that inputs and outputs Cholesky factors instead:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# rho, the prior correlation matrix between intercepts and slopes
tfd_cholesky_lkj(2, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What kind of prior is this? As McElreath keeps reminding us, nothing is more instructive than sampling from the prior. For us to see what’s going on, we use the base LKJ distribution, not the Cholesky one:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;corr_prior &amp;lt;- tfd_lkj(2, 2)
correlation &amp;lt;- (corr_prior %&amp;gt;% tfd_sample(100))[ , 1, 2] %&amp;gt;% as.numeric()
library(ggplot2)
data.frame(correlation) %&amp;gt;% ggplot(aes(x = correlation)) + geom_density()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-05-24-varying-slopes/images/lkj.png" width="297" /&gt;&lt;/p&gt;
&lt;p&gt;So this prior is moderately skeptical about strong correlations, but pretty open to learning from data.&lt;/p&gt;
&lt;p&gt;The next distribution in line&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# sigma, prior variance for the waiting time
tfd_sample_distribution(tfd_exponential(rate = 1), sample_shape = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;is the prior for the variance of the waiting time, the very last distribution in the list.&lt;/p&gt;
&lt;p&gt;Next is the prior distribution of variances for the intercepts and slopes. This prior is the same for both cases, but we specify a &lt;code&gt;sample_shape&lt;/code&gt; of 2 to get two individual samples.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# sigma_cafe, prior of variances for intercepts and slopes (vector of 2)
tfd_sample_distribution(tfd_exponential(rate = 1), sample_shape = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have the respective prior variances, we move on to the prior means. Both are normal distributions.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# b, the prior mean for the slopes
tfd_sample_distribution(tfd_normal(loc = -1, scale = 0.5), sample_shape = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# a, the prior mean for the intercepts
tfd_sample_distribution(tfd_normal(loc = 5, scale = 2), sample_shape = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On to the heart of the model, where the partial pooling happens. We are going to construct partially-pooled intercepts and slopes for all of the cafés. Like we said above, intercepts and slopes are not independent; they interact. Thus, we need to use a multivariate normal distribution. The means are given by the prior means defined right above, while the covariance matrix is built from the above prior variances and the prior correlation matrix. The output shape here is determined by the number of cafés: We want an intercept and a slope for every café.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# mvn, multivariate distribution of intercepts and slopes
# shape: batch size, 20, 2
function(a,b,sigma_cafe,sigma,chol_rho) 
  tfd_sample_distribution(
    tfd_multivariate_normal_tri_l(
      loc = tf$concat(list(a,b), axis = -1L),
      scale_tril = tf$linalg$LinearOperatorDiag(sigma_cafe)$matmul(chol_rho)),
  sample_shape = n_cafes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we sample the actual waiting times. This code pulls out the correct intercepts and slopes from the multivariate normal and outputs the mean waiting time, dependent on what café we’re in and whether it’s morning or afternoon.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;        # waiting time
        # shape: batch size, 200
        function(mvn, a, b, sigma_cafe, sigma)
          tfd_independent(
            # need to pull out the correct cafe_id in the middle column
            tfd_normal(
              loc = (tf$gather(mvn[ , , 1], cafe_id, axis = -1L) +
                       tf$gather(mvn[ , , 2], cafe_id, axis = -1L) * afternoon), 
              scale=sigma), 
        reinterpreted_batch_ndims=1
        )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before running the sampling, it’s always a good idea to do a quick check on the model.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;n_cafes &amp;lt;- 20
cafe_id &amp;lt;- tf$cast((d$cafe - 1) %% 20, tf$int64)

afternoon &amp;lt;- d$afternoon
wait &amp;lt;- d$wait&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We sample from the model and then, check the log probability.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;m &amp;lt;- model(cafe_id)

s &amp;lt;- m %&amp;gt;% tfd_sample(3)
m %&amp;gt;% tfd_log_prob(s)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We want a scalar log probability per member in the batch, which is what we get.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor([-466.1392  -149.92587 -196.51688], shape=(3,), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="running-the-chains"&gt;Running the chains&lt;/h2&gt;
&lt;p&gt;The actual Monte Carlo sampling works just like in the previous post, with one exception. Sampling happens in unconstrained parameter space, but at the end we need to get valid correlation matrix parameters &lt;code&gt;rho&lt;/code&gt; and valid variances &lt;code&gt;sigma&lt;/code&gt; and &lt;code&gt;sigma_cafe&lt;/code&gt;. Conversion between spaces is done via TFP bijectors. Luckily, this is not something we have to do as users; all we need to specify are appropriate bijectors. For the normal distributions in the model, there is nothing to do.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;constraining_bijectors &amp;lt;- list(
  # make sure the rho[1:4] parameters are valid for a Cholesky factor
  tfb_correlation_cholesky(),
  # make sure variance is positive
  tfb_exp(),
  # make sure variance is positive
  tfb_exp(),
  tfb_identity(),
  tfb_identity(),
  tfb_identity()
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can set up the Hamiltonian Monte Carlo sampler.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;n_steps &amp;lt;- 500
n_burnin &amp;lt;- 500
n_chains &amp;lt;- 4

# set up the optimization objective
logprob &amp;lt;- function(rho, sigma, sigma_cafe, b, a, mvn)
  m %&amp;gt;% tfd_log_prob(list(rho, sigma, sigma_cafe, b, a, mvn, wait))

# initial states for the sampling procedure
c(initial_rho, initial_sigma, initial_sigma_cafe, initial_b, initial_a, initial_mvn, .) %&amp;lt;-% 
  (m %&amp;gt;% tfd_sample(n_chains))

# HMC sampler, with the above bijectors and step size adaptation
hmc &amp;lt;- mcmc_hamiltonian_monte_carlo(
  target_log_prob_fn = logprob,
  num_leapfrog_steps = 3,
  step_size = list(0.1, 0.1, 0.1, 0.1, 0.1, 0.1)
) %&amp;gt;%
  mcmc_transformed_transition_kernel(bijector = constraining_bijectors) %&amp;gt;%
  mcmc_simple_step_size_adaptation(target_accept_prob = 0.8,
                                   num_adaptation_steps = n_burnin)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, we can obtain additional diagnostics (here: step sizes and acceptance rates) by registering a trace function:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;trace_fn &amp;lt;- function(state, pkr) {
  list(pkr$inner_results$inner_results$is_accepted,
       pkr$inner_results$inner_results$accepted_results$step_size)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, then, is the sampling function. Note how we use &lt;code&gt;tf_function&lt;/code&gt; to put it on the graph. At least as of today, this makes a huge difference in sampling performance when using eager execution.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;run_mcmc &amp;lt;- function(kernel) {
  kernel %&amp;gt;% mcmc_sample_chain(
    num_results = n_steps,
    num_burnin_steps = n_burnin,
    current_state = list(initial_rho,
                         tf$ones_like(initial_sigma),
                         tf$ones_like(initial_sigma_cafe),
                         initial_b,
                         initial_a,
                         initial_mvn),
    trace_fn = trace_fn
  )
}

run_mcmc &amp;lt;- tf_function(run_mcmc)
res &amp;lt;- hmc %&amp;gt;% run_mcmc()

mcmc_trace &amp;lt;- res$all_states&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So how do our samples look, and what do we get in terms of posteriors? Let’s see.&lt;/p&gt;
&lt;h2 id="results"&gt;Results&lt;/h2&gt;
&lt;p&gt;At this moment, &lt;code&gt;mcmc_trace&lt;/code&gt; is a list of tensors of different shapes, dependent on how we defined the parameters. We need to do a bit of post-processing to be able to summarise and display the results.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# the actual mcmc samples
# for the trace plots, we want to have them in shape (500, 4, 49)
# that is: (number of steps, number of chains, number of parameters)
samples &amp;lt;- abind(
  # rho 1:4
  as.array(mcmc_trace[[1]] %&amp;gt;% tf$reshape(list(tf$cast(n_steps, tf$int32), tf$cast(n_chains, tf$int32), 4L))),
  # sigma
  as.array(mcmc_trace[[2]]),  
  # sigma_cafe 1:2
  as.array(mcmc_trace[[3]][ , , 1]),    
  as.array(mcmc_trace[[3]][ , , 2]), 
  # b
  as.array(mcmc_trace[[4]]),  
  # a
  as.array(mcmc_trace[[5]]),  
  # mvn 10:49
  as.array( mcmc_trace[[6]] %&amp;gt;% tf$reshape(list(tf$cast(n_steps, tf$int32), tf$cast(n_chains, tf$int32), 40L))),
  along = 3) 

# the effective sample sizes
# we want them in shape (4, 49), which is (number of chains * number of parameters)
ess &amp;lt;- mcmc_effective_sample_size(mcmc_trace) 
ess &amp;lt;- cbind(
  # rho 1:4
  as.matrix(ess[[1]] %&amp;gt;% tf$reshape(list(tf$cast(n_chains, tf$int32), 4L))),
  # sigma
  as.matrix(ess[[2]]),  
  # sigma_cafe 1:2
  as.matrix(ess[[3]][ , 1, drop = FALSE]),    
  as.matrix(ess[[3]][ , 2, drop = FALSE]), 
  # b
  as.matrix(ess[[4]]),  
  # a
  as.matrix(ess[[5]]),  
  # mvn 10:49
  as.matrix(ess[[6]] %&amp;gt;% tf$reshape(list(tf$cast(n_chains, tf$int32), 40L)))
  ) 

# the rhat values
# we want them in shape (49), which is (number of parameters)
rhat &amp;lt;- mcmc_potential_scale_reduction(mcmc_trace)
rhat &amp;lt;- c(
  # rho 1:4
  as.double(rhat[[1]] %&amp;gt;% tf$reshape(list(4L))),
  # sigma
  as.double(rhat[[2]]),  
  # sigma_cafe 1:2
  as.double(rhat[[3]][1]),    
  as.double(rhat[[3]][2]), 
  # b
  as.double(rhat[[4]]),  
  # a
  as.double(rhat[[5]]),  
  # mvn 10:49
  as.double(rhat[[6]] %&amp;gt;% tf$reshape(list(40L)))
  ) &lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="trace-plots"&gt;Trace plots&lt;/h3&gt;
&lt;p&gt;How well do the chains mix?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;prep_tibble &amp;lt;- function(samples) {
  as_tibble(samples, .name_repair = ~ c(&amp;quot;chain_1&amp;quot;, &amp;quot;chain_2&amp;quot;, &amp;quot;chain_3&amp;quot;, &amp;quot;chain_4&amp;quot;)) %&amp;gt;% 
    add_column(sample = 1:n_steps) %&amp;gt;%
    gather(key = &amp;quot;chain&amp;quot;, value = &amp;quot;value&amp;quot;, -sample)
}

plot_trace &amp;lt;- function(samples) {
  prep_tibble(samples) %&amp;gt;% 
    ggplot(aes(x = sample, y = value, color = chain)) +
    geom_line() + 
    theme_light() +
    theme(legend.position = &amp;quot;none&amp;quot;,
          axis.title = element_blank(),
          axis.text = element_blank(),
          axis.ticks = element_blank())
}

plot_traces &amp;lt;- function(sample_array, num_params) {
  plots &amp;lt;- purrr::map(1:num_params, ~ plot_trace(sample_array[ , , .x]))
  do.call(grid.arrange, plots)
}

plot_traces(samples, 49)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-05-24-varying-slopes/images/trace.png" width="600" /&gt;&lt;/p&gt;
&lt;p&gt;Awesome! (The first two parameters of &lt;code&gt;rho&lt;/code&gt;, the Cholesky factor of the correlation matrix, need to stay fixed at 1 and 0, respectively.)&lt;/p&gt;
&lt;p&gt;Now, on to some summary statistics on the posteriors of the parameters.&lt;/p&gt;
&lt;h3 id="parameters"&gt;Parameters&lt;/h3&gt;
&lt;p&gt;Like last time, we display posterior means and standard deviations, as well as the highest posterior density interval (HPDI). We add effective sample sizes and &lt;em&gt;rhat&lt;/em&gt; values.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;column_names &amp;lt;- c(
  paste0(&amp;quot;rho_&amp;quot;, 1:4),
  &amp;quot;sigma&amp;quot;,
  paste0(&amp;quot;sigma_cafe_&amp;quot;, 1:2),
  &amp;quot;b&amp;quot;,
  &amp;quot;a&amp;quot;,
  c(rbind(paste0(&amp;quot;a_cafe_&amp;quot;, 1:20), paste0(&amp;quot;b_cafe_&amp;quot;, 1:20)))
)

all_samples &amp;lt;- matrix(samples, nrow = n_steps * n_chains, ncol = 49)
all_samples &amp;lt;- all_samples %&amp;gt;%
  as_tibble(.name_repair = ~ column_names)

all_samples %&amp;gt;% glimpse()

means &amp;lt;- all_samples %&amp;gt;% 
  summarise_all(list (mean)) %&amp;gt;% 
  gather(key = &amp;quot;key&amp;quot;, value = &amp;quot;mean&amp;quot;)

sds &amp;lt;- all_samples %&amp;gt;% 
  summarise_all(list (sd)) %&amp;gt;% 
  gather(key = &amp;quot;key&amp;quot;, value = &amp;quot;sd&amp;quot;)

hpdis &amp;lt;-
  all_samples %&amp;gt;%
  summarise_all(list(~ list(hdi(.) %&amp;gt;% t() %&amp;gt;% as_tibble()))) %&amp;gt;% 
   unnest() 
 
 hpdis_lower &amp;lt;- hpdis %&amp;gt;% select(-contains(&amp;quot;upper&amp;quot;)) %&amp;gt;%
   rename(lower0 = lower) %&amp;gt;%
   gather(key = &amp;quot;key&amp;quot;, value = &amp;quot;lower&amp;quot;) %&amp;gt;% 
   arrange(as.integer(str_sub(key, 6))) %&amp;gt;%
   mutate(key = column_names)
 
 hpdis_upper &amp;lt;- hpdis %&amp;gt;% select(-contains(&amp;quot;lower&amp;quot;)) %&amp;gt;%
   rename(upper0 = upper) %&amp;gt;%
   gather(key = &amp;quot;key&amp;quot;, value = &amp;quot;upper&amp;quot;) %&amp;gt;% 
   arrange(as.integer(str_sub(key, 6))) %&amp;gt;%
   mutate(key = column_names)

summary &amp;lt;- means %&amp;gt;% 
  inner_join(sds, by = &amp;quot;key&amp;quot;) %&amp;gt;% 
  inner_join(hpdis_lower, by = &amp;quot;key&amp;quot;) %&amp;gt;%
  inner_join(hpdis_upper, by = &amp;quot;key&amp;quot;)

ess &amp;lt;- apply(ess, 2, mean)

summary_with_diag &amp;lt;- summary %&amp;gt;% add_column(ess = ess, rhat = rhat)
print(summary_with_diag, n = 49)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 49 x 7
   key            mean     sd  lower   upper   ess   rhat
   &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
 1 rho_1         1     0       1      1        NaN    NaN   
 2 rho_2         0     0       0      0       NaN     NaN   
 3 rho_3        -0.517 0.176  -0.831 -0.195   42.4   1.01
 4 rho_4         0.832 0.103   0.644  1.000   46.5   1.02
 5 sigma         0.473 0.0264  0.420  0.523  424.    1.00
 6 sigma_cafe_1  0.967 0.163   0.694  1.29    97.9   1.00
 7 sigma_cafe_2  0.607 0.129   0.386  0.861   42.3   1.03
 8 b            -1.14  0.141  -1.43  -0.864   95.1   1.00
 9 a             3.66  0.218   3.22   4.07    75.3   1.01
10 a_cafe_1      4.20  0.192   3.83   4.57    83.9   1.01
11 b_cafe_1     -1.13  0.251  -1.63  -0.664   63.6   1.02
12 a_cafe_2      2.17  0.195   1.79   2.54    59.3   1.01
13 b_cafe_2     -0.923 0.260  -1.42  -0.388   46.0   1.01
14 a_cafe_3      4.40  0.195   4.02   4.79    56.7   1.01
15 b_cafe_3     -1.97  0.258  -2.52  -1.51    43.9   1.01
16 a_cafe_4      3.22  0.199   2.80   3.57    58.7   1.02
17 b_cafe_4     -1.20  0.254  -1.70  -0.713   36.3   1.01
18 a_cafe_5      1.86  0.197   1.45   2.20    52.8   1.03
19 b_cafe_5     -0.113 0.263  -0.615  0.390   34.6   1.04
20 a_cafe_6      4.26  0.210   3.87   4.67    43.4   1.02
21 b_cafe_6     -1.30  0.277  -1.80  -0.713   41.4   1.05
22 a_cafe_7      3.61  0.198   3.23   3.98    44.9   1.01
23 b_cafe_7     -1.02  0.263  -1.51  -0.489   37.7   1.03
24 a_cafe_8      3.95  0.189   3.59   4.31    73.1   1.01
25 b_cafe_8     -1.64  0.248  -2.10  -1.13    60.7   1.02
26 a_cafe_9      3.98  0.212   3.57   4.37    76.3   1.03
27 b_cafe_9     -1.29  0.273  -1.83  -0.776   57.8   1.05
28 a_cafe_10     3.60  0.187   3.24   3.96   104.    1.01
29 b_cafe_10    -1.00  0.245  -1.47  -0.512   70.4   1.00
30 a_cafe_11     1.95  0.200   1.56   2.35    55.9   1.03
31 b_cafe_11    -0.449 0.266  -1.00   0.0619  42.5   1.04
32 a_cafe_12     3.84  0.195   3.46   4.22    76.0   1.02
33 b_cafe_12    -1.17  0.259  -1.65  -0.670   62.5   1.03
34 a_cafe_13     3.88  0.201   3.50   4.29    62.2   1.02
35 b_cafe_13    -1.81  0.270  -2.30  -1.29    48.3   1.03
36 a_cafe_14     3.19  0.212   2.82   3.61    65.9   1.07
37 b_cafe_14    -0.961 0.278  -1.49  -0.401   49.9   1.06
38 a_cafe_15     4.46  0.212   4.08   4.91    62.0   1.09
39 b_cafe_15    -2.20  0.290  -2.72  -1.59    47.8   1.11
40 a_cafe_16     3.41  0.193   3.02   3.78    62.7   1.02
41 b_cafe_16    -1.07  0.253  -1.54  -0.567   48.5   1.05
42 a_cafe_17     4.22  0.201   3.82   4.60    58.7   1.01
43 b_cafe_17    -1.24  0.273  -1.74  -0.703   43.8   1.01
44 a_cafe_18     5.77  0.210   5.34   6.18    66.0   1.02
45 b_cafe_18    -1.05  0.284  -1.61  -0.511   49.8   1.02
46 a_cafe_19     3.23  0.203   2.88   3.65    52.7   1.02
47 b_cafe_19    -0.232 0.276  -0.808  0.243   45.2   1.01
48 a_cafe_20     3.74  0.212   3.35   4.21    48.2   1.04
49 b_cafe_20    -1.09  0.281  -1.58  -0.506   36.5   1.05&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So what do we have? If you run this “live”, for the rows &lt;code&gt;a_cafe_n&lt;/code&gt; resp. &lt;code&gt;b_cafe_n&lt;/code&gt;, you see a nice alternation of white and red coloring: For all cafés, the inferred slopes are negative.&lt;/p&gt;
&lt;p&gt;The inferred slope prior (&lt;code&gt;b&lt;/code&gt;) is around -1.14, which is not too far off from the value we used for sampling: 1.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;rho&lt;/code&gt; posterior estimates, admittedly, are less useful unless you are accustomed to compose Cholesky factors in your head. We compute the resulting posterior correlations and their mean:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;rhos &amp;lt;- all_samples[ , 1:4] %&amp;gt;% tibble()

rhos &amp;lt;- rhos %&amp;gt;%
  apply(1, list) %&amp;gt;%
  unlist(recursive = FALSE) %&amp;gt;%
  lapply(function(x) matrix(x, byrow = TRUE, nrow = 2) %&amp;gt;% tcrossprod())

rho &amp;lt;- rhos %&amp;gt;% purrr::map(~ .x[1,2]) %&amp;gt;% unlist()

mean_rho &amp;lt;- mean(rho)
mean_rho&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;-0.5166775&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The value we used for sampling was -0.7, so we see the regularization effect. In case you’re wondering, for the same data &lt;em&gt;Stan&lt;/em&gt; yields an estimate of -0.5.&lt;/p&gt;
&lt;p&gt;Finally, let’s display equivalents to McElreath’s figures illustrating shrinkage on the parameter (café-specific intercepts and slopes) as well as the outcome (morning resp. afternoon waiting times) scales.&lt;/p&gt;
&lt;h2 id="shrinkage"&gt;Shrinkage&lt;/h2&gt;
&lt;p&gt;As expected, we see that the individual intercepts and slopes are pulled towards the mean – the more, the further away they are from the center.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# just like McElreath, compute unpooled estimates directly from data
a_empirical &amp;lt;- d %&amp;gt;% 
  filter(afternoon == 0) %&amp;gt;%
  group_by(cafe) %&amp;gt;% 
  summarise(a = mean(wait)) %&amp;gt;%
  select(a)

b_empirical &amp;lt;- d %&amp;gt;% 
  filter(afternoon == 1) %&amp;gt;%
  group_by(cafe) %&amp;gt;% 
  summarise(b = mean(wait)) %&amp;gt;%
  select(b) -
  a_empirical

empirical_estimates &amp;lt;- bind_cols(
  a_empirical,
  b_empirical,
  type = rep(&amp;quot;data&amp;quot;, 20))

posterior_estimates &amp;lt;- tibble(
  a = means %&amp;gt;% filter(
  str_detect(key, &amp;quot;^a_cafe&amp;quot;)) %&amp;gt;% select(mean) %&amp;gt;% pull(),
  b = means %&amp;gt;% filter(
    str_detect(key, &amp;quot;^b_cafe&amp;quot;)) %&amp;gt;% select(mean)  %&amp;gt;% pull(),
  type = rep(&amp;quot;posterior&amp;quot;, 20))
  
all_estimates &amp;lt;- bind_rows(empirical_estimates, posterior_estimates)

# compute posterior mean bivariate Gaussian
# again following McElreath
mu_est &amp;lt;- c(means[means$key == &amp;quot;a&amp;quot;, 2], means[means$key == &amp;quot;b&amp;quot;, 2]) %&amp;gt;% unlist()
rho_est &amp;lt;- mean_rho
sa_est &amp;lt;- means[means$key == &amp;quot;sigma_cafe_1&amp;quot;, 2] %&amp;gt;% unlist()
sb_est &amp;lt;- means[means$key == &amp;quot;sigma_cafe_2&amp;quot;, 2] %&amp;gt;% unlist()
cov_ab &amp;lt;- sa_est * sb_est * rho_est
sigma_est &amp;lt;- matrix(c(sa_est^2, cov_ab, cov_ab, sb_est^2), ncol=2) 

alpha_levels &amp;lt;- c(0.1, 0.3, 0.5, 0.8, 0.99)
names(alpha_levels) &amp;lt;- alpha_levels

contour_data &amp;lt;- plyr::ldply(
  alpha_levels,
  ellipse,
  x = sigma_est,
  scale = c(1, 1),
  centre = mu_est
)

ggplot() +
  geom_point(data = all_estimates, mapping = aes(x = a, y = b, color = type)) + 
  geom_path(data = contour_data, mapping = aes(x = x, y = y, group = .id))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-05-24-varying-slopes/images/shrinkage1.png" width="600" /&gt;&lt;/p&gt;
&lt;p&gt;The same behavior is visible on the outcome scale.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;wait_times  &amp;lt;- all_estimates %&amp;gt;%
  mutate(morning = a, afternoon = a + b)

# simulate from posterior means
v &amp;lt;- MASS::mvrnorm(1e4 , mu_est , sigma_est)
v[ ,2] &amp;lt;- v[ ,1] + v[ ,2] # calculate afternoon wait
# construct empirical covariance matrix
sigma_est2 &amp;lt;- cov(v)
mu_est2 &amp;lt;- mu_est
mu_est2[2] &amp;lt;- mu_est[1] + mu_est[2]

contour_data &amp;lt;- plyr::ldply(
  alpha_levels,
  ellipse,
  x = sigma_est2 %&amp;gt;% unname(),
  scale = c(1, 1),
  centre = mu_est2
)

ggplot() +
  geom_point(data = wait_times, mapping = aes(x = morning, y = afternoon, color = type)) + 
  geom_path(data = contour_data, mapping = aes(x = x, y = y, group = .id))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-05-24-varying-slopes/images/shrinkage2.png" width="600" /&gt;&lt;/p&gt;
&lt;h2 id="wrapping-up"&gt;Wrapping up&lt;/h2&gt;
&lt;p&gt;By now, we hope we have convinced you of the power inherent in Bayesian modeling, as well as conveyed some ideas on how this is achievable with TensorFlow Probability. As with every DSL though, it takes time to proceed from understanding worked examples to design your own models. And not just time – it helps to have seen a lot of different models, focusing on different tasks and applications. On this blog, we plan to loosely follow up on Bayesian modeling with TFP, picking up some of the tasks and challenges elaborated on in the later chapters of McElreath’s book. Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;cf. &lt;a href="https://en.wikipedia.org/wiki/Multilevel_model"&gt;the Wikipedia article on multilevel models&lt;/a&gt; for a collection of terms encountered when dealing with this subject, and e.g. &lt;a href="https://statmodeling.stat.columbia.edu/2005/01/25/why_i_dont_use/"&gt;Gelman’s&lt;/a&gt; dissection of various ways &lt;em&gt;random effects&lt;/em&gt; are defined&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;We won’t overload this post by explicitly comparing results here, but we did that when writing the code.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;Disclaimer: We have not verified whether this is an adequate model of the world, but it really doesn’t matter either.&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">32e4b2fae88b2d032cd4b8e177044a28</distill:md5>
      <category>Bayesian Modeling</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-05-24-varying-slopes</guid>
      <pubDate>Fri, 24 May 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-05-24-varying-slopes/images/thumb.png" medium="image" type="image/png" width="509" height="249"/>
    </item>
    <item>
      <title>Tadpoles on TensorFlow: Hierarchical partial pooling with tfprobability</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-05-06-tadpoles-on-tensorflow</link>
      <description>


&lt;p&gt;Before we jump into the technicalities: This post is, of course, dedicated to McElreath who wrote one of most intriguing books on Bayesian (or should we just say - scientific?) modeling we’re aware of. If you haven’t read &lt;a href="https://www.amazon.com/Statistical-Rethinking-Bayesian-Examples-Chapman/dp/1482253445"&gt;Statistical Rethinking&lt;/a&gt;, and are interested in modeling, you might definitely want to check it out. In this post, we’re not going to try to re-tell the story: Our clear focus will, instead, be a demonstration of how to do MCMC with &lt;a href="https://rstudio.github.io/tfprobability/"&gt;tfprobability&lt;/a&gt;.&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Concretely, this post has two parts. The first is a quick overview of how to use &lt;a href="https://rstudio.github.io/tfprobability/reference/tfd_joint_distribution_sequential.html"&gt;tfd_joint_sequential_distribution&lt;/a&gt; to construct a model, and then sample from it using Hamiltonian Monte Carlo. This part can be consulted for quick code look-up, or as a frugal template of the whole process. The second part then walks through a multi-level model in more detail, showing how to extract, post-process and visualize sampling as well as diagnostic outputs.&lt;/p&gt;
&lt;h2 id="reedfrogs"&gt;Reedfrogs&lt;/h2&gt;
&lt;p&gt;The data comes with the &lt;code&gt;rethinking&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(rethinking)

data(reedfrogs)
d &amp;lt;- reedfrogs
str(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   48 obs. of  5 variables:
 $ density : int  10 10 10 10 10 10 10 10 10 10 ...
 $ pred    : Factor w/ 2 levels &amp;quot;no&amp;quot;,&amp;quot;pred&amp;quot;: 1 1 1 1 1 1 1 1 2 2 ...
 $ size    : Factor w/ 2 levels &amp;quot;big&amp;quot;,&amp;quot;small&amp;quot;: 1 1 1 1 2 2 2 2 1 1 ...
 $ surv    : int  9 10 7 10 9 9 10 9 4 9 ...
 $ propsurv: num  0.9 1 0.7 1 0.9 0.9 1 0.9 0.4 0.9 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The task is modeling survivor counts among tadpoles, where tadpoles are held in tanks of different sizes (equivalently, different numbers of inhabitants). Each row in the dataset describes one tank, with its initial count of inhabitants (&lt;code&gt;density&lt;/code&gt;) and number of survivors (&lt;code&gt;surv&lt;/code&gt;). In the technical overview part, we build a simple unpooled model that describes every tank in isolation. Then, in the detailed walk-through, we’ll see how to construct a &lt;em&gt;varying intercepts&lt;/em&gt; model that allows for information sharing between tanks.&lt;/p&gt;
&lt;h2 id="constructing-models-with-tfd_joint_distribution_sequential"&gt;Constructing models with &lt;code&gt;tfd_joint_distribution_sequential&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;tfd_joint_distribution_sequential&lt;/code&gt; represents a model as a list of conditional distributions. This is easiest to see on a real example, so we’ll jump right in, creating an unpooled model of the tadpole data.&lt;/p&gt;
&lt;p&gt;This is the how the model specification would look in &lt;em&gt;Stan&lt;/em&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;model{
    vector[48] p;
    a ~ normal( 0 , 1.5 );
    for ( i in 1:48 ) {
        p[i] = a[tank[i]];
        p[i] = inv_logit(p[i]);
    }
    S ~ binomial( N , p );
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And here is &lt;code&gt;tfd_joint_distribution_sequential&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tensorflow)

# make sure you have at least version 0.7 of TensorFlow Probability 
# as of this writing, it is required of install the master branch:
# install_tensorflow(version = &amp;quot;nightly&amp;quot;)
library(tfprobability)

n_tadpole_tanks &amp;lt;- nrow(d)
n_surviving &amp;lt;- d$surv
n_start &amp;lt;- d$density

m1 &amp;lt;- tfd_joint_distribution_sequential(
  list(
    # normal prior of per-tank logits
    tfd_multivariate_normal_diag(
      loc = rep(0, n_tadpole_tanks),
      scale_identity_multiplier = 1.5),
    # binomial distribution of survival counts
    function(l)
      tfd_independent(
        tfd_binomial(total_count = n_start, logits = l),
        reinterpreted_batch_ndims = 1
      )
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model consists of two distributions: Prior means and variances for the 48 tadpole tanks are specified by &lt;code&gt;tfd_multivariate_normal_diag&lt;/code&gt;; then &lt;code&gt;tfd_binomial&lt;/code&gt; generates survival counts for each tank. Note how the first distribution is unconditional, while the second depends on the first. Note too how the second has to be wrapped in &lt;code&gt;tfd_independent&lt;/code&gt; to avoid wrong broadcasting. (This is an aspect of &lt;code&gt;tfd_joint_distribution_sequential&lt;/code&gt; usage that deserves to be documented more systematically, which is surely going to happen.&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; Just think that this functionality was added to TFP &lt;code&gt;master&lt;/code&gt; only three weeks ago!)&lt;/p&gt;
&lt;p&gt;As an aside, the model specification here ends up shorter than in &lt;em&gt;Stan&lt;/em&gt; as &lt;code&gt;tfd_binomial&lt;/code&gt; optionally takes logits as parameters.&lt;/p&gt;
&lt;p&gt;As with every TFP distribution, you can do a quick functionality check by sampling from the model:&lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# sample a batch of 2 values 
# we get samples for every distribution in the model
s &amp;lt;- m1 %&amp;gt;% tfd_sample(2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
Tensor(&amp;quot;MultivariateNormalDiag/sample/affine_linear_operator/forward/add:0&amp;quot;,
shape=(2, 48), dtype=float32)

[[2]]
Tensor(&amp;quot;IndependentJointDistributionSequential/sample/Beta/sample/Reshape:0&amp;quot;,
shape=(2, 48), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and computing log probabilities:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# we should get only the overall log probability of the model
m1 %&amp;gt;% tfd_log_prob(s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;t[[1]]
Tensor(&amp;quot;MultivariateNormalDiag/sample/affine_linear_operator/forward/add:0&amp;quot;,
shape=(2, 48), dtype=float32)

[[2]]
Tensor(&amp;quot;IndependentJointDistributionSequential/sample/Beta/sample/Reshape:0&amp;quot;,
shape=(2, 48), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s see how we can sample from this model using Hamiltonian Monte Carlo.&lt;/p&gt;
&lt;h2 id="running-hamiltonian-monte-carlo-in-tfp"&gt;Running Hamiltonian Monte Carlo in TFP&lt;/h2&gt;
&lt;p&gt;We define a Hamiltonian Monte Carlo kernel with dynamic step size adaptation based on a desired acceptance probability.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# number of steps to run burnin
n_burnin &amp;lt;- 500

# optimization target is the likelihood of the logits given the data
logprob &amp;lt;- function(l)
  m1 %&amp;gt;% tfd_log_prob(list(l, n_surviving))

hmc &amp;lt;- mcmc_hamiltonian_monte_carlo(
  target_log_prob_fn = logprob,
  num_leapfrog_steps = 3,
  step_size = 0.1,
) %&amp;gt;%
  mcmc_simple_step_size_adaptation(
    target_accept_prob = 0.8,
    num_adaptation_steps = n_burnin
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then run the sampler, passing in an initial state. If we want to run &lt;span class="math inline"&gt;\(n\)&lt;/span&gt; chains, that state has to be of length &lt;span class="math inline"&gt;\(n\)&lt;/span&gt;, for every parameter in the model (here we have just one).&lt;/p&gt;
&lt;p&gt;The sampling function, &lt;a href="https://rstudio.github.io/tfprobability/reference/mcmc_sample_chain.html"&gt;mcmc_sample_chain&lt;/a&gt;, may optionally be passed a &lt;code&gt;trace_fn&lt;/code&gt; that tells TFP which kinds of meta information to save. Here we save acceptance ratios and step sizes.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# number of steps after burnin
n_steps &amp;lt;- 500
# number of chains
n_chain &amp;lt;- 4

# get starting values for the parameters
# their shape implicitly determines the number of chains we will run
# see current_state parameter passed to mcmc_sample_chain below
c(initial_logits, .) %&amp;lt;-% (m1 %&amp;gt;% tfd_sample(n_chain))

# tell TFP to keep track of acceptance ratio and step size
trace_fn &amp;lt;- function(state, pkr) {
  list(pkr$inner_results$is_accepted,
       pkr$inner_results$accepted_results$step_size)
}

res &amp;lt;- hmc %&amp;gt;% mcmc_sample_chain(
  num_results = n_steps,
  num_burnin_steps = n_burnin,
  current_state = initial_logits,
  trace_fn = trace_fn
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When sampling is finished, we can access the samples as &lt;code&gt;res$all_states&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;mcmc_trace &amp;lt;- res$all_states
mcmc_trace&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Tensor(&amp;quot;mcmc_sample_chain/trace_scan/TensorArrayStack/TensorArrayGatherV3:0&amp;quot;,
shape=(500, 4, 48), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the shape of the samples for &lt;code&gt;l&lt;/code&gt;, the 48 per-tank logits: 500 samples times 4 chains times 48 parameters.&lt;/p&gt;
&lt;p&gt;From these samples, we can compute effective sample size and &lt;span class="math inline"&gt;\(rhat\)&lt;/span&gt; (alias &lt;code&gt;mcmc_potential_scale_reduction&lt;/code&gt;):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Tensor(&amp;quot;Mean:0&amp;quot;, shape=(48,), dtype=float32)
ess &amp;lt;- mcmc_effective_sample_size(mcmc_trace) %&amp;gt;% tf$reduce_mean(axis = 0L)

# Tensor(&amp;quot;potential_scale_reduction/potential_scale_reduction_single_state/sub_1:0&amp;quot;, shape=(48,), dtype=float32)
rhat &amp;lt;- mcmc_potential_scale_reduction(mcmc_trace)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Whereas diagnostic information is available in &lt;code&gt;res$trace&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Tensor(&amp;quot;mcmc_sample_chain/trace_scan/TensorArrayStack_1/TensorArrayGatherV3:0&amp;quot;,
# shape=(500, 4), dtype=bool)
is_accepted &amp;lt;- res$trace[[1]] 

# Tensor(&amp;quot;mcmc_sample_chain/trace_scan/TensorArrayStack_2/TensorArrayGatherV3:0&amp;quot;,
# shape=(500,), dtype=float32)
step_size &amp;lt;- res$trace[[2]] &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After this quick outline, let’s move on to the topic promised in the title: multi-level modeling, or partial pooling. This time, we’ll also take a closer look at sampling results and diagnostic outputs.&lt;/p&gt;
&lt;h2 id="multi-level-tadpoles"&gt;Multi-level tadpoles &lt;a href="#fn4" class="footnote-ref" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The multi-level model – or &lt;em&gt;varying intercepts model&lt;/em&gt;, in this case: we’ll get to &lt;em&gt;varying slopes&lt;/em&gt; in a later post – adds a &lt;em&gt;hyperprior&lt;/em&gt; to the model. Instead of deciding on a mean and variance of the normal prior the logits are drawn from, we let the model learn means and variances for individual tanks. These per-tank means, while being priors for the binomial logits, are assumed to be normally distributed, and are themselves regularized by a normal prior for the mean and an exponential prior for the variance.&lt;/p&gt;
&lt;p&gt;For the Stan-savvy, here is the Stan formulation of this model.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model{
    vector[48] p;
    sigma ~ exponential( 1 );
    a_bar ~ normal( 0 , 1.5 );
    a ~ normal( a_bar , sigma );
    for ( i in 1:48 ) {
        p[i] = a[tank[i]];
        p[i] = inv_logit(p[i]);
    }
    S ~ binomial( N , p );
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And here it is with TFP:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;m2 &amp;lt;- tfd_joint_distribution_sequential(
  list(
    # a_bar, the prior for the mean of the normal distribution of per-tank logits
    tfd_normal(loc = 0, scale = 1.5),
    # sigma, the prior for the variance of the normal distribution of per-tank logits
    tfd_exponential(rate = 1),
    # normal distribution of per-tank logits
    # parameters sigma and a_bar refer to the outputs of the above two distributions
    function(sigma, a_bar) 
      tfd_sample_distribution(
        tfd_normal(loc = a_bar, scale = sigma),
        sample_shape = list(n_tadpole_tanks)
      ), 
    # binomial distribution of survival counts
    # parameter l refers to the output of the normal distribution immediately above
    function(l)
      tfd_independent(
        tfd_binomial(total_count = n_start, logits = l),
        reinterpreted_batch_ndims = 1
      )
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Technically, dependencies in &lt;code&gt;tfd_joint_distribution_sequential&lt;/code&gt; are defined via spatial proximity in the list: In the learned prior for the logits&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;function(sigma, a_bar) 
      tfd_sample_distribution(
        tfd_normal(loc = a_bar, scale = sigma),
        sample_shape = list(n_tadpole_tanks)
      )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;sigma&lt;/code&gt; refers to the distribution immediately above, and &lt;code&gt;a_bar&lt;/code&gt; to the one above that.&lt;/p&gt;
&lt;p&gt;Analogously, in the distribution of survival counts&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;function(l)
      tfd_independent(
        tfd_binomial(total_count = n_start, logits = l),
        reinterpreted_batch_ndims = 1
      )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;l&lt;/code&gt; refers to the distribution immediately preceding its own definition.&lt;/p&gt;
&lt;p&gt;Again, let’s sample from this model to see if shapes are correct.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;s &amp;lt;- m2 %&amp;gt;% tfd_sample(2)
s &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;They are.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
Tensor(&amp;quot;Normal/sample_1/Reshape:0&amp;quot;, shape=(2,), dtype=float32)

[[2]]
Tensor(&amp;quot;Exponential/sample_1/Reshape:0&amp;quot;, shape=(2,), dtype=float32)

[[3]]
Tensor(&amp;quot;SampleJointDistributionSequential/sample_1/Normal/sample/Reshape:0&amp;quot;,
shape=(2, 48), dtype=float32)

[[4]]
Tensor(&amp;quot;IndependentJointDistributionSequential/sample_1/Beta/sample/Reshape:0&amp;quot;,
shape=(2, 48), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And to make sure we get one overall &lt;code&gt;log_prob&lt;/code&gt; per batch:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;m2 %&amp;gt;% tfd_log_prob(s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Tensor(&amp;quot;JointDistributionSequential/log_prob/add_3:0&amp;quot;, shape=(2,), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Training this model works like before, except that now the initial state comprises three parameters, &lt;em&gt;a_bar&lt;/em&gt;, &lt;em&gt;sigma&lt;/em&gt; and &lt;em&gt;l&lt;/em&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;c(initial_a, initial_s, initial_logits, .) %&amp;lt;-% (m2 %&amp;gt;% tfd_sample(n_chain))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the sampling routine:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# the joint log probability now is based on three parameters
logprob &amp;lt;- function(a, s, l)
  m2 %&amp;gt;% tfd_log_prob(list(a, s, l, n_surviving))

hmc &amp;lt;- mcmc_hamiltonian_monte_carlo(
  target_log_prob_fn = logprob,
  num_leapfrog_steps = 3,
  # one step size for each parameter
  step_size = list(0.1, 0.1, 0.1),
) %&amp;gt;%
  mcmc_simple_step_size_adaptation(target_accept_prob = 0.8,
                                   num_adaptation_steps = n_burnin)

run_mcmc &amp;lt;- function(kernel) {
  kernel %&amp;gt;% mcmc_sample_chain(
    num_results = n_steps,
    num_burnin_steps = n_burnin,
    current_state = list(initial_a, tf$ones_like(initial_s), initial_logits),
    trace_fn = trace_fn
  )
}

res &amp;lt;- hmc %&amp;gt;% run_mcmc()
 
mcmc_trace &amp;lt;- res$all_states&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This time, &lt;code&gt;mcmc_trace&lt;/code&gt; is a list of three: We have&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
Tensor(&amp;quot;mcmc_sample_chain/trace_scan/TensorArrayStack/TensorArrayGatherV3:0&amp;quot;,
shape=(500, 4), dtype=float32)

[[2]]
Tensor(&amp;quot;mcmc_sample_chain/trace_scan/TensorArrayStack_1/TensorArrayGatherV3:0&amp;quot;,
shape=(500, 4), dtype=float32)

[[3]]
Tensor(&amp;quot;mcmc_sample_chain/trace_scan/TensorArrayStack_2/TensorArrayGatherV3:0&amp;quot;,
shape=(500, 4, 48), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s create graph nodes for the results and information we’re interested in.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# as above, this is the raw result
mcmc_trace_ &amp;lt;- res$all_states

# we perform some reshaping operations directly in tensorflow
all_samples_ &amp;lt;-
  tf$concat(
    list(
      mcmc_trace_[[1]] %&amp;gt;% tf$expand_dims(axis = -1L),
      mcmc_trace_[[2]]  %&amp;gt;% tf$expand_dims(axis = -1L),
      mcmc_trace_[[3]]
    ),
    axis = -1L
  ) %&amp;gt;%
  tf$reshape(list(2000L, 50L))

# diagnostics, also as above
is_accepted_ &amp;lt;- res$trace[[1]]
step_size_ &amp;lt;- res$trace[[2]]

# effective sample size
# again we use tensorflow to get conveniently shaped outputs
ess_ &amp;lt;- mcmc_effective_sample_size(mcmc_trace) 
ess_ &amp;lt;- tf$concat(
  list(
    ess_[[1]] %&amp;gt;% tf$expand_dims(axis = -1L),
    ess_[[2]]  %&amp;gt;% tf$expand_dims(axis = -1L),
    ess_[[3]]
  ),
  axis = -1L
) 

# rhat, conveniently post-processed
rhat_ &amp;lt;- mcmc_potential_scale_reduction(mcmc_trace)
rhat_ &amp;lt;- tf$concat(
  list(
    rhat_[[1]] %&amp;gt;% tf$expand_dims(axis = -1L),
    rhat_[[2]]  %&amp;gt;% tf$expand_dims(axis = -1L),
    rhat_[[3]]
  ),
  axis = -1L
) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we’re ready to actually run the chains.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# so far, no sampling has been done!
# the actual sampling happens when we create a Session 
# and run the above-defined nodes
sess &amp;lt;- tf$Session()
eval &amp;lt;- function(...) sess$run(list(...))

c(mcmc_trace, all_samples, is_accepted, step_size, ess, rhat) %&amp;lt;-%
  eval(mcmc_trace_, all_samples_, is_accepted_, step_size_, ess_, rhat_)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This time, let’s actually inspect those results.&lt;/p&gt;
&lt;h2 id="multi-level-tadpoles-results"&gt;Multi-level tadpoles: Results&lt;/h2&gt;
&lt;p&gt;First, how do the chains behave?&lt;/p&gt;
&lt;h3 id="trace-plots"&gt;Trace plots&lt;/h3&gt;
&lt;p&gt;Extract the samples for &lt;code&gt;a_bar&lt;/code&gt; and &lt;code&gt;sigma&lt;/code&gt;, as well as one of the learned priors for the logits:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;a_bar &amp;lt;- mcmc_trace[[1]] %&amp;gt;% as.matrix()
sigma &amp;lt;- mcmc_trace[[2]] %&amp;gt;% as.matrix()
a_1 &amp;lt;- mcmc_trace[[3]][ , , 1] %&amp;gt;% as.matrix()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s a trace plot for &lt;code&gt;a_bar&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;prep_tibble &amp;lt;- function(samples) {
  as_tibble(samples, .name_repair = ~ c(&amp;quot;chain_1&amp;quot;, &amp;quot;chain_2&amp;quot;, &amp;quot;chain_3&amp;quot;, &amp;quot;chain_4&amp;quot;)) %&amp;gt;% 
    add_column(sample = 1:500) %&amp;gt;%
    gather(key = &amp;quot;chain&amp;quot;, value = &amp;quot;value&amp;quot;, -sample)
}

plot_trace &amp;lt;- function(samples, param_name) {
  prep_tibble(samples) %&amp;gt;% 
    ggplot(aes(x = sample, y = value, color = chain)) +
    geom_line() + 
    ggtitle(param_name)
}

plot_trace(a_bar, &amp;quot;a_bar&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-05-06-tadpoles-on-tensorflow/images/traceplot_a_bar.png" width="400" /&gt;&lt;/p&gt;
&lt;p&gt;And here for &lt;code&gt;sigma&lt;/code&gt; and &lt;code&gt;a_1&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-05-06-tadpoles-on-tensorflow/images/traceplot_sigma.png" width="400" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-05-06-tadpoles-on-tensorflow/images/traceplot_a_1.png" width="400" /&gt;&lt;/p&gt;
&lt;p&gt;How about the posterior distributions of the parameters, first and foremost, the varying intercepts &lt;code&gt;a_1&lt;/code&gt; … &lt;code&gt;a_48&lt;/code&gt;?&lt;/p&gt;
&lt;h3 id="posterior-distributions"&gt;Posterior distributions&lt;/h3&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot_posterior &amp;lt;- function(samples) {
  prep_tibble(samples) %&amp;gt;% 
    ggplot(aes(x = value, color = chain)) +
    geom_density() +
    theme_classic() +
    theme(legend.position = &amp;quot;none&amp;quot;,
          axis.title = element_blank(),
          axis.text = element_blank(),
          axis.ticks = element_blank())
    
}

plot_posteriors &amp;lt;- function(sample_array, num_params) {
  plots &amp;lt;- purrr::map(1:num_params, ~ plot_posterior(sample_array[ , , .x] %&amp;gt;% as.matrix()))
  do.call(grid.arrange, plots)
}

plot_posteriors(mcmc_trace[[3]], dim(mcmc_trace[[3]])[3])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-05-06-tadpoles-on-tensorflow/images/posteriors.png" width="400" /&gt;&lt;/p&gt;
&lt;p&gt;Now let’s see the corresponding posterior means and highest posterior density intervals. (The below code includes the hyperpriors in &lt;code&gt;summary&lt;/code&gt; as we’ll want to display a complete &lt;em&gt;precis&lt;/em&gt;-like output soon.)&lt;/p&gt;
&lt;h3 id="posterior-means-and-hpdis"&gt;Posterior means and HPDIs&lt;/h3&gt;
&lt;pre class="r"&gt;&lt;code&gt;all_samples &amp;lt;- all_samples %&amp;gt;%
  as_tibble(.name_repair = ~ c(&amp;quot;a_bar&amp;quot;, &amp;quot;sigma&amp;quot;, paste0(&amp;quot;a_&amp;quot;, 1:48))) 

means &amp;lt;- all_samples %&amp;gt;% 
  summarise_all(list (~ mean)) %&amp;gt;% 
  gather(key = &amp;quot;key&amp;quot;, value = &amp;quot;mean&amp;quot;)

sds &amp;lt;- all_samples %&amp;gt;% 
  summarise_all(list (~ sd)) %&amp;gt;% 
  gather(key = &amp;quot;key&amp;quot;, value = &amp;quot;sd&amp;quot;)

hpdis &amp;lt;-
  all_samples %&amp;gt;%
  summarise_all(list(~ list(hdi(.) %&amp;gt;% t() %&amp;gt;% as_tibble()))) %&amp;gt;% 
  unnest() 

hpdis_lower &amp;lt;- hpdis %&amp;gt;% select(-contains(&amp;quot;upper&amp;quot;)) %&amp;gt;%
  rename(lower0 = lower) %&amp;gt;%
  gather(key = &amp;quot;key&amp;quot;, value = &amp;quot;lower&amp;quot;) %&amp;gt;% 
  arrange(as.integer(str_sub(key, 6))) %&amp;gt;%
  mutate(key = c(&amp;quot;a_bar&amp;quot;, &amp;quot;sigma&amp;quot;, paste0(&amp;quot;a_&amp;quot;, 1:48)))

hpdis_upper &amp;lt;- hpdis %&amp;gt;% select(-contains(&amp;quot;lower&amp;quot;)) %&amp;gt;%
  rename(upper0 = upper) %&amp;gt;%
  gather(key = &amp;quot;key&amp;quot;, value = &amp;quot;upper&amp;quot;) %&amp;gt;% 
  arrange(as.integer(str_sub(key, 6))) %&amp;gt;%
  mutate(key = c(&amp;quot;a_bar&amp;quot;, &amp;quot;sigma&amp;quot;, paste0(&amp;quot;a_&amp;quot;, 1:48)))

summary &amp;lt;- means %&amp;gt;% 
  inner_join(sds, by = &amp;quot;key&amp;quot;) %&amp;gt;% 
  inner_join(hpdis_lower, by = &amp;quot;key&amp;quot;) %&amp;gt;%
  inner_join(hpdis_upper, by = &amp;quot;key&amp;quot;)


summary %&amp;gt;% 
  filter(!key %in% c(&amp;quot;a_bar&amp;quot;, &amp;quot;sigma&amp;quot;)) %&amp;gt;%
  mutate(key_fct = factor(key, levels = unique(key))) %&amp;gt;%
  ggplot(aes(x = key_fct, y = mean, ymin = lower, ymax = upper)) +
   geom_pointrange() + 
   coord_flip() +  
   xlab(&amp;quot;&amp;quot;) + ylab(&amp;quot;post. mean and HPDI&amp;quot;) +
   theme_minimal() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-05-06-tadpoles-on-tensorflow/images/forest_plot.png" width="400" /&gt;&lt;/p&gt;
&lt;p&gt;Now for an equivalent to &lt;em&gt;precis&lt;/em&gt;. We already computed means, standard deviations and the HPDI interval. Let’s add &lt;em&gt;n_eff&lt;/em&gt;, the effective number of samples, and &lt;em&gt;rhat&lt;/em&gt;, the Gelman-Rubin statistic.&lt;/p&gt;
&lt;h3 id="comprehensive-summary-a.k.a.-precis"&gt;Comprehensive summary (a.k.a. “precis”)&lt;/h3&gt;
&lt;pre class="r"&gt;&lt;code&gt;is_accepted &amp;lt;- is_accepted %&amp;gt;% as.integer() %&amp;gt;% mean()
step_size &amp;lt;- purrr::map(step_size, mean)

ess &amp;lt;- apply(ess, 2, mean)

summary_with_diag &amp;lt;- summary %&amp;gt;% add_column(ess = ess, rhat = rhat)
summary_with_diag&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 50 x 7
   key    mean    sd  lower upper   ess  rhat
   &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
 1 a_bar  1.35 0.266  0.792  1.87 405.   1.00
 2 sigma  1.64 0.218  1.23   2.05  83.6  1.00
 3 a_1    2.14 0.887  0.451  3.92  33.5  1.04
 4 a_2    3.16 1.13   1.09   5.48  23.7  1.03
 5 a_3    1.01 0.698 -0.333  2.31  65.2  1.02
 6 a_4    3.02 1.04   1.06   5.05  31.1  1.03
 7 a_5    2.11 0.843  0.625  3.88  49.0  1.05
 8 a_6    2.06 0.904  0.496  3.87  39.8  1.03
 9 a_7    3.20 1.27   1.11   6.12  14.2  1.02
10 a_8    2.21 0.894  0.623  4.18  44.7  1.04
# ... with 40 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the varying intercepts, effective sample sizes are pretty low, indicating we might want to investigate possible reasons.&lt;/p&gt;
&lt;p&gt;Let’s also display posterior survival probabilities, analogously to figure 13.2 in the book.&lt;/p&gt;
&lt;h3 id="posterior-survival-probabilities"&gt;Posterior survival probabilities&lt;/h3&gt;
&lt;pre class="r"&gt;&lt;code&gt;sim_tanks &amp;lt;- rnorm(8000, a_bar, sigma)
tibble(x = sim_tanks) %&amp;gt;% ggplot(aes(x = x)) + geom_density() + xlab(&amp;quot;distribution of per-tank logits&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-05-06-tadpoles-on-tensorflow/images/logits.png" width="400" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# our usual sigmoid by another name (undo the logit)
logistic &amp;lt;- function(x) 1/(1 + exp(-x))
probs &amp;lt;- map_dbl(sim_tanks, logistic)
tibble(x = probs) %&amp;gt;% ggplot(aes(x = x)) + geom_density() + xlab(&amp;quot;probability of survival&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-05-06-tadpoles-on-tensorflow/images/surv_probs.png" width="400" /&gt;&lt;/p&gt;
&lt;p&gt;Finally, we want to make sure we see the shrinkage behavior displayed in figure 13.1 in the book.&lt;/p&gt;
&lt;h3 id="shrinkage"&gt;Shrinkage&lt;/h3&gt;
&lt;pre class="r"&gt;&lt;code&gt;summary %&amp;gt;% 
  filter(!key %in% c(&amp;quot;a_bar&amp;quot;, &amp;quot;sigma&amp;quot;)) %&amp;gt;%
  select(key, mean) %&amp;gt;%
  mutate(est_survival = logistic(mean)) %&amp;gt;%
  add_column(act_survival = d$propsurv) %&amp;gt;%
  select(-mean) %&amp;gt;%
  gather(key = &amp;quot;type&amp;quot;, value = &amp;quot;value&amp;quot;, -key) %&amp;gt;%
  ggplot(aes(x = key, y = value, color = type)) +
  geom_point() +
  geom_hline(yintercept = mean(d$propsurv), size = 0.5, color = &amp;quot;cyan&amp;quot; ) +
  xlab(&amp;quot;&amp;quot;) +
  ylab(&amp;quot;&amp;quot;) +
  theme_minimal() +
  theme(axis.text.x = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-05-06-tadpoles-on-tensorflow/images/shrinkage.png" width="400" /&gt;&lt;/p&gt;
&lt;p&gt;We see results similar in spirit to McElreath’s: estimates are shrunken to the mean (the cyan-colored line). Also, shrinkage seems to be more active in smaller tanks, which are the lower-numbered ones on the left of the plot.&lt;/p&gt;
&lt;h2 id="outlook"&gt;Outlook&lt;/h2&gt;
&lt;p&gt;In this post, we saw how to construct a &lt;em&gt;varying intercepts&lt;/em&gt; model with &lt;code&gt;tfprobability&lt;/code&gt;, as well as how to extract sampling results and relevant diagnostics. In an upcoming post, we’ll move on to &lt;em&gt;varying slopes&lt;/em&gt;. With non-negligible probability, our example will build on one of Mc Elreath’s again… Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;For a supplementary introduction to Bayesian modeling focusing on complete coverage, yet starting from the very beginning, you might want to consult Ben Lambert’s &lt;a href="https://www.amazon.com/Students-Guide-Bayesian-Statistics/dp/1473916364/"&gt;Student’s Guide to Bayesian Statistics&lt;/a&gt;.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;As of today, lots of useful information is available in &lt;a href="https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Modeling_with_JointDistribution.ipynb"&gt;Modeling with JointDistribution&lt;/a&gt; and &lt;a href="https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Multilevel_Modeling_Primer.ipynb"&gt;Multilevel Modeling Primer&lt;/a&gt;, but some experimentation may needed to adapt the – numerous! – examples to your needs.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;&lt;strong&gt;Updated footnote, as of May 13th&lt;/strong&gt;: When this post was written, we were still experimenting with the use of &lt;code&gt;tf.function&lt;/code&gt; from R, so it seemed safest to code the complete example in graph mode. The next post on MCMC will use eager execution, and show how to achieve good performance by placing the actual sampling procedure on the graph.&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;yep, it’s a quote&lt;a href="#fnref4" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">5e4d10f453c88cebff985d52cf6cc273</distill:md5>
      <category>Bayesian Modeling</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-05-06-tadpoles-on-tensorflow</guid>
      <pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-05-06-tadpoles-on-tensorflow/images/thumb.png" medium="image" type="image/png" width="1612" height="659"/>
    </item>
    <item>
      <title>Experimenting with autoregressive flows in TensorFlow Probability</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-04-24-autoregressive-flows</link>
      <description>


&lt;p&gt;In the &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-04-05-bijectors-flows/"&gt;first part of this mini-series on autoregressive flow models&lt;/a&gt;, we looked at &lt;em&gt;bijectors&lt;/em&gt; in TensorFlow Probability (TFP), and saw how to use them for sampling and density estimation. We singled out the &lt;em&gt;affine bijector&lt;/em&gt; to demonstrate the mechanics of flow construction: We start from a distribution that is easy to sample from, and that allows for straightforward calculation of its density. Then, we attach some number of invertible transformations, optimizing for data-likelihood under the final transformed distribution. The efficiency of that (log)likelihood calculation is where normalizing flows excel: Loglikelihood under the (unknown) target distribution is obtained as a sum of the density under the base distribution of the inverse-transformed data plus the absolute log determinant of the inverse Jacobian.&lt;/p&gt;
&lt;p&gt;Now, an affine flow will seldom be powerful enough to model nonlinear, complex transformations. In constrast, autoregressive models have shown substantive success in density estimation as well as sample generation. Combined with more involved architectures, feature engineering, and extensive compute, the concept of autoregressivity has powered – and is powering – state-of-the-art architectures in areas such as image, speech and video modeling.&lt;/p&gt;
&lt;p&gt;This post will be concerned with the building blocks of autoregressive flows in TFP. While we won’t exactly be building state-of-the-art models, we’ll try to understand and play with some major ingredients, hopefully enabling the reader to do her own experiments on her own data.&lt;/p&gt;
&lt;p&gt;This post has three parts: First, we’ll look at autoregressivity and its implementation in TFP. Then, we try to (approximately) reproduce one of the experiments in the “MAF paper” (&lt;em&gt;Masked Autoregressive Flows for Distribution Estimation&lt;/em&gt; &lt;span class="citation"&gt;(Papamakarios, Pavlakou, and Murray 2017)&lt;/span&gt;) – essentially a proof of concept. Finally, for the third time on this blog, we come back to the task of analysing audio data, with mixed results.&lt;/p&gt;
&lt;h2 id="autoregressivity-and-masking"&gt;Autoregressivity and masking&lt;/h2&gt;
&lt;p&gt;In distribution estimation, autoregressivity enters the scene via the chain rule of probability that decomposes a joint density into a product of conditional densities:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
p(\mathbf{x}) = \prod_{i}p(\mathbf{x}_i|\mathbf{x}_{1:i−1})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In practice, this means that autoregressive models have to impose an order on the variables - an order which might or might not “make sense”. Approaches here include choosing orderings at random and/or using different orderings for each layer. While in recurrent neural networks, autoregressivity is conserved due to the recurrence relation inherent in state updating, it is not clear a priori how autoregressivity is to be achieved in a densely connected architecture. A computationally efficient solution was proposed in &lt;em&gt;MADE: Masked Autoencoder for Distribution Estimation&lt;/em&gt;&lt;span class="citation"&gt;(Germain et al. 2015)&lt;/span&gt;: Starting from a densely connected layer, mask out all connections that should not be allowed, i.e., all connections from input feature &lt;span class="math inline"&gt;\(i\)&lt;/span&gt; to said layer’s activations &lt;span class="math inline"&gt;\(1 ... i-1\)&lt;/span&gt;. Or expressed differently, activation &lt;span class="math inline"&gt;\(i\)&lt;/span&gt; may be connected to input features &lt;span class="math inline"&gt;\(1 ... i-1\)&lt;/span&gt; only. Then when adding more layers, care must be taken to ensure that all required connections are masked so that at the end, output &lt;span class="math inline"&gt;\(i\)&lt;/span&gt; will only ever have seen inputs &lt;span class="math inline"&gt;\(1 ... i-1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Thus masked autoregressive flows are a fusion of two major approaches - autoregressive models (which need not be flows) and flows (which need not be autoregressive). In TFP these are provided by &lt;code&gt;MaskedAutoregressiveFlow&lt;/code&gt;,&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; to be used as a bijector in a &lt;code&gt;TransformedDistribution&lt;/code&gt;.&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;While the documentation shows how to use this bijector, the step from theoretical understanding to coding a “black box” may seem wide. If you’re anything like the author, here you might feel the urge to “look under the hood” and verify that things really are the way you’re assuming. So let’s give in to curiosity and allow ourselves a little escapade into the source code.&lt;/p&gt;
&lt;p&gt;Peeking ahead, this is how we’ll construct a masked autoregressive flow in TFP (again using the still new-ish R bindings provided by &lt;a href="http://github.io/tfprobability"&gt;tfprobability&lt;/a&gt;):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tfprobability)

maf &amp;lt;- tfb_masked_autoregressive_flow(
    shift_and_log_scale_fn = tfb_masked_autoregressive_default_template(
      hidden_layers = list(num_hidden, num_hidden),
      activation = tf$nn$tanh)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pulling apart the relevant entities here, &lt;code&gt;tfb_masked_autoregressive_flow&lt;/code&gt; is a bijector, with the usual methods &lt;code&gt;tfb_forward()&lt;/code&gt;, &lt;code&gt;tfb_inverse()&lt;/code&gt;, &lt;code&gt;tfb_forward_log_det_jacobian()&lt;/code&gt; and &lt;code&gt;tfb_inverse_log_det_jacobian()&lt;/code&gt;. The default &lt;code&gt;shift_and_log_scale_fn&lt;/code&gt;, &lt;code&gt;tfb_masked_autoregressive_default_template&lt;/code&gt;, constructs a little neural network of its own, with a configurable number of hidden units per layer, a configurable activation function and optionally, other configurable parameters to be passed to the underlying &lt;code&gt;dense&lt;/code&gt; layers. It’s these dense layers that have to respect the autoregressive property. Can we take a look at how this is done? Yes we can, provided we’re not afraid of a little Python.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;masked_autoregressive_default_template&lt;/code&gt; (now leaving out the &lt;code&gt;tfb_&lt;/code&gt; as we’ve entered Python-land) uses &lt;code&gt;masked_dense&lt;/code&gt; to do what you’d suppose a thus-named function might be doing: construct a dense layer that has part of the weight matrix masked out. How? We’ll see after a few Python setup statements.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp
tfd = tfp.distributions
tfb = tfp.bijectors
tf.enable_eager_execution()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following code snippets are taken from &lt;code&gt;masked_dense&lt;/code&gt; (in its &lt;a href="https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/bijectors/masked_autoregressive.py"&gt;current form on master&lt;/a&gt;), and when possible, simplified for better readability, accommodating just the specifics of the chosen example - a toy matrix of shape 2x3:&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;# construct some toy input data (this line obviously not from the original code)
inputs = tf.constant(np.arange(1.,7), shape = (2, 3))

# (partly) determine shape of mask from shape of input
input_depth = tf.compat.dimension_value(inputs.shape.with_rank_at_least(1)[-1])
num_blocks = input_depth
num_blocks # 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our toy layer should have 4 units:&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;units = 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mask is initialized to all zeros. Considering it will be used to elementwise multiply the weight matrix, we’re a bit surprised at its shape (shouldn’t it be the other way round?). No worries; all will turn out correct in the end.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;mask = np.zeros([units, input_depth], dtype=tf.float32.as_numpy_dtype())
mask&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.]], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now to “whitelist” the allowed connections, we have to fill in ones whenever information flow &lt;em&gt;is&lt;/em&gt; allowed by the autoregressive property:&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;def _gen_slices(num_blocks, n_in, n_out):
  slices = []
  col = 0
  d_in = n_in // num_blocks
  d_out = n_out // num_blocks
  row = d_out 
  for _ in range(num_blocks):
    row_slice = slice(row, None)
    col_slice = slice(col, col + d_in)
    slices.append([row_slice, col_slice])
    col += d_in
    row += d_out
  return slices

slices = _gen_slices(num_blocks, input_depth, units)
for [row_slice, col_slice] in slices:
  mask[row_slice, col_slice] = 1

mask&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[0., 0., 0.],
       [1., 0., 0.],
       [1., 1., 0.],
       [1., 1., 1.]], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, does this look mirror-inverted? A transpose fixes shape and logic both:&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;mask = mask.t
mask&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[0., 1., 1., 1.],
       [0., 0., 1., 1.],
       [0., 0., 0., 1.]], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have the mask, we can create the layer (interestingly, as of this writing not (yet?) a &lt;code&gt;tf.keras&lt;/code&gt; layer):&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;layer = tf.compat.v1.layers.Dense(
        units,
        kernel_initializer=masked_initializer, # 1
        kernel_constraint=lambda x: mask * x   # 2
        )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we see masking going on in two ways. For one, the weight initializer is masked:&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;kernel_initializer = tf.compat.v1.glorot_normal_initializer()

def masked_initializer(shape, dtype=None, partition_info=None):
 return mask * kernel_initializer(shape, dtype, partition_info)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And secondly, a kernel constraint makes sure that after optimization, the relative units are zeroed out again:&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;kernel_constraint=lambda x: mask * x &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just for fun, let’s apply the layer to our toy input:&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;layer.apply(inputs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Tensor: id=30, shape=(2, 4), dtype=float64, numpy=
array([[ 0.        , -0.7489589 , -0.43329933,  1.42710014],
       [ 0.        , -2.9958356 , -1.71647246,  1.09258015]])&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Zeroes where expected. And double-checking on the weight matrix…&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;layer.kernel&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;tf.Variable &amp;#39;dense/kernel:0&amp;#39; shape=(3, 4) dtype=float64, numpy=
array([[ 0.        , -0.7489589 , -0.42214942, -0.6473454 ],
       [-0.        ,  0.        , -0.00557496, -0.46692933],
       [-0.        , -0.        , -0.        ,  1.00276807]])&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Good. Now hopefully after this little deep dive, things have become a bit more concrete. Of course in a bigger model, the autoregressive property has to be conserved between layers as well.&lt;/p&gt;
&lt;p&gt;On to the second topic, application of MAF to a real-world dataset.&lt;/p&gt;
&lt;h2 id="masked-autoregressive-flow"&gt;Masked Autoregressive Flow&lt;/h2&gt;
&lt;p&gt;The MAF paper&lt;span class="citation"&gt;(Papamakarios, Pavlakou, and Murray 2017)&lt;/span&gt; applied masked autoregressive flows (as well as single-layer-&lt;em&gt;MADE&lt;/em&gt;&lt;span class="citation"&gt;(Germain et al. 2015)&lt;/span&gt; and Real NVP &lt;span class="citation"&gt;(Dinh, Sohl-Dickstein, and Bengio 2016)&lt;/span&gt;) to a number of datasets, including MNIST, CIFAR-10 and several datasets from the &lt;a href="http://archive.ics.uci.edu/ml/index.html"&gt;UCI Machine Learning Repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We pick one of the UCI datasets: &lt;a href="http://archive.ics.uci.edu/ml/datasets/gas+sensors+for+home+activity+monitoring"&gt;Gas sensors for home activity monitoring&lt;/a&gt;. On this dataset, the MAF authors obtained the best results using a MAF with 10 flows, so this is what we will try.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-04-24-autoregressive-flows/images/maf_results.png" alt="Figure from Masked Autoregressive Flow for Density Estimation[@2017arXiv170507057P]" width="338" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-13)Figure from Masked Autoregressive Flow for Density Estimation&lt;span class="citation"&gt;(Papamakarios, Pavlakou, and Murray 2017)&lt;/span&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Collecting information from the paper, we know that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;data was included from the file &lt;em&gt;ethylene_CO.txt&lt;/em&gt; only;&lt;/li&gt;
&lt;li&gt;discrete columns were eliminated, as well as all columns with correlations &amp;gt; .98; and&lt;/li&gt;
&lt;li&gt;the remaining 8 columns&lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; were standardised (z-transformed).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Regarding the neural network architecture, we gather that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;each of the 10 MAF layers was followed by a batchnorm;&lt;/li&gt;
&lt;li&gt;as to feature order, the first MAF layer used the variable order that came with the dataset; then every consecutive layer reversed it;&lt;/li&gt;
&lt;li&gt;specifically for this dataset and as opposed to all other UCI datasets, &lt;em&gt;tanh&lt;/em&gt; was used for activation instead of &lt;em&gt;relu&lt;/em&gt;;&lt;/li&gt;
&lt;li&gt;the Adam optimizer was used, with a learning rate of 1e-4;&lt;/li&gt;
&lt;li&gt;there were two hidden layers for each MAF, with 100 units each;&lt;/li&gt;
&lt;li&gt;training went on until no improvement occurred for 30 consecutive epochs on the validation set; and&lt;/li&gt;
&lt;li&gt;the base distribution was a multivariate Gaussian.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is all useful information for our attempt to estimate this dataset, but the essential bit is this. In case you knew the dataset already, you might have been wondering how the authors would deal with the dimensionality of the data: It is a time series, and the MADE architecture explored above introduces autoregressivity between features, not time steps. So how is the additional temporal autoregressivity to be handled? The answer is: The time dimension is essentially removed. In the authors’ words,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[…] it is a time series but was treated as if each example were an i.i.d. sample from the marginal distribution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This undoubtedly is useful information for our present modeling attempt, but it also tells us something else: We might have to look beyond MADE layers for actual time series modeling.&lt;/p&gt;
&lt;p&gt;Now though let’s look at this example of using MAF for multivariate modeling, with no time or spatial dimension to be taken into account.&lt;/p&gt;
&lt;p&gt;Following the hints the authors gave us, this is what we do.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# load libraries -------------------------------------------------------------
library(tensorflow)
library(tfprobability)

tfe_enable_eager_execution(device_policy = &amp;quot;silent&amp;quot;)

library(tfdatasets)
library(dplyr)
library(readr)
library(purrr)
library(caret)

# read data ------------------------------------------------------------------
df &amp;lt;- read_table2(&amp;quot;ethylene_CO.txt&amp;quot;,
                  skip = 1,
                  col_names = FALSE)
glimpse(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Observations: 4,208,261
Variables: 19
$ X1  &amp;lt;dbl&amp;gt; 0.00, 0.01, 0.01, 0.03, 0.04, 0.05, 0.06, 0.07, 0.07, 0.09,...
$ X2  &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
$ X3  &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
$ X4  &amp;lt;dbl&amp;gt; -50.85, -49.40, -40.04, -47.14, -33.58, -48.59, -48.27, -47.14,... 
$ X5  &amp;lt;dbl&amp;gt; -1.95, -5.53, -16.09, -10.57, -20.79, -11.54, -9.11, -4.56,...
$ X6  &amp;lt;dbl&amp;gt; -41.82, -42.78, -27.59, -32.28, -33.25, -36.16, -31.31, -16.57,... 
$ X7  &amp;lt;dbl&amp;gt; 1.30, 0.49, 0.00, 4.40, 6.03, 6.03, 5.37, 4.40, 23.98, 2.77,...
$ X8  &amp;lt;dbl&amp;gt; -4.07, 3.58, -7.16, -11.22, 3.42, 0.33, -7.97, -2.28, -2.12,...
$ X9  &amp;lt;dbl&amp;gt; -28.73, -34.55, -42.14, -37.94, -34.22, -29.05, -30.34, -24.35,...
$ X10 &amp;lt;dbl&amp;gt; -13.49, -9.59, -12.52, -7.16, -14.46, -16.74, -8.62, -13.17,...
$ X11 &amp;lt;dbl&amp;gt; -3.25, 5.37, -5.86, -1.14, 8.31, -1.14, 7.00, -6.34, -0.81,...
$ X12 &amp;lt;dbl&amp;gt; 55139.95, 54395.77, 53960.02, 53047.71, 52700.28, 51910.52,...
$ X13 &amp;lt;dbl&amp;gt; 50669.50, 50046.91, 49299.30, 48907.00, 48330.96, 47609.00,...
$ X14 &amp;lt;dbl&amp;gt; 9626.26, 9433.20, 9324.40, 9170.64, 9073.64, 8982.88, 8860.51,...
$ X15 &amp;lt;dbl&amp;gt; 9762.62, 9591.21, 9449.81, 9305.58, 9163.47, 9021.08, 8966.48,...
$ X16 &amp;lt;dbl&amp;gt; 24544.02, 24137.13, 23628.90, 23101.66, 22689.54, 22159.12,...
$ X17 &amp;lt;dbl&amp;gt; 21420.68, 20930.33, 20504.94, 20101.42, 19694.07, 19332.57,...
$ X18 &amp;lt;dbl&amp;gt; 7650.61, 7498.79, 7369.67, 7285.13, 7156.74, 7067.61, 6976.13,...
$ X19 &amp;lt;dbl&amp;gt; 6928.42, 6800.66, 6697.47, 6578.52, 6468.32, 6385.31, 6300.97,...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# we don&amp;#39;t know if we&amp;#39;ll end up with the same columns as the authors did,
# but we try (at least we do end up with 8 columns)
df &amp;lt;- df[,-(1:3)]
hc &amp;lt;- findCorrelation(cor(df), cutoff = 0.985)
df2 &amp;lt;- df[,-c(hc)]

# scale
df2 &amp;lt;- scale(df2)
df2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 4,208,261 x 8
      X4     X5     X8    X9    X13    X16    X17   X18
   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
 1 -50.8  -1.95  -4.07 -28.7 50670. 24544. 21421. 7651.
 2 -49.4  -5.53   3.58 -34.6 50047. 24137. 20930. 7499.
 3 -40.0 -16.1   -7.16 -42.1 49299. 23629. 20505. 7370.
 4 -47.1 -10.6  -11.2  -37.9 48907  23102. 20101. 7285.
 5 -33.6 -20.8    3.42 -34.2 48331. 22690. 19694. 7157.
 6 -48.6 -11.5    0.33 -29.0 47609  22159. 19333. 7068.
 7 -48.3  -9.11  -7.97 -30.3 47047. 21932. 19028. 6976.
 8 -47.1  -4.56  -2.28 -24.4 46758. 21504. 18780. 6900.
 9 -42.3  -2.77  -2.12 -27.6 46197. 21125. 18439. 6827.
10 -44.6   3.58  -0.65 -35.5 45652. 20836. 18209. 6790.
# … with 4,208,251 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now set up the data generation process:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# train-test split
n_rows &amp;lt;- nrow(df2) # 4208261
train_ids &amp;lt;- sample(1:n_rows, 0.5 * n_rows)
x_train &amp;lt;- df2[train_ids, ]
x_test &amp;lt;- df2[-train_ids, ]

# create datasets
batch_size &amp;lt;- 100
train_dataset &amp;lt;- tf$cast(x_train, tf$float32) %&amp;gt;%
  tensor_slices_dataset %&amp;gt;%
  dataset_batch(batch_size)

test_dataset &amp;lt;- tf$cast(x_test, tf$float32) %&amp;gt;%
  tensor_slices_dataset %&amp;gt;%
  dataset_batch(nrow(x_test))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To construct the flow, the first thing needed is the base distribution.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;base_dist &amp;lt;- tfd_multivariate_normal_diag(loc = rep(0, ncol(df2)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now for the flow, by default constructed with batchnorm and permutation of feature order.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;num_hidden &amp;lt;- 100
dim &amp;lt;- ncol(df2)

use_batchnorm &amp;lt;- TRUE
use_permute &amp;lt;- TRUE
num_mafs &amp;lt;-10
num_layers &amp;lt;- 3 * num_mafs

bijectors &amp;lt;- vector(mode = &amp;quot;list&amp;quot;, length = num_layers)

for (i in seq(1, num_layers, by = 3)) {
  maf &amp;lt;- tfb_masked_autoregressive_flow(
    shift_and_log_scale_fn = tfb_masked_autoregressive_default_template(
      hidden_layers = list(num_hidden, num_hidden),
      activation = tf$nn$tanh))
  bijectors[[i]] &amp;lt;- maf
  if (use_batchnorm)
    bijectors[[i + 1]] &amp;lt;- tfb_batch_normalization()
  if (use_permute)
    bijectors[[i + 2]] &amp;lt;- tfb_permute((ncol(df2) - 1):0)
}

if (use_permute) bijectors &amp;lt;- bijectors[-num_layers]

flow &amp;lt;- bijectors %&amp;gt;%
  discard(is.null) %&amp;gt;%
  # tfb_chain expects arguments in reverse order of application
  rev() %&amp;gt;%
  tfb_chain()

target_dist &amp;lt;- tfd_transformed_distribution(
  distribution = base_dist,
  bijector = flow
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And configuring the optimizer:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;optimizer &amp;lt;- tf$train$AdamOptimizer(1e-4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Under that isotropic Gaussian we chose as a base distribution, how likely are the data?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;base_loglik &amp;lt;- base_dist %&amp;gt;% 
  tfd_log_prob(x_train) %&amp;gt;% 
  tf$reduce_mean()
base_loglik %&amp;gt;% as.numeric()        # -11.33871

base_loglik_test &amp;lt;- base_dist %&amp;gt;% 
  tfd_log_prob(x_test) %&amp;gt;% 
  tf$reduce_mean()
base_loglik_test %&amp;gt;% as.numeric()   # -11.36431&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And, just as a quick sanity check: What is the loglikelihood of the data under the transformed distribution &lt;em&gt;before any training&lt;/em&gt;?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;target_loglik_pre &amp;lt;-
  target_dist %&amp;gt;% tfd_log_prob(x_train) %&amp;gt;% tf$reduce_mean()
target_loglik_pre %&amp;gt;% as.numeric()        # -11.22097

target_loglik_pre_test &amp;lt;-
  target_dist %&amp;gt;% tfd_log_prob(x_test) %&amp;gt;% tf$reduce_mean()
target_loglik_pre_test %&amp;gt;% as.numeric()   # -11.36431&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The values match - good. Here now is the training loop. Being impatient, we already keep checking the loglikelihood on the (complete) test set to see if we’re making any progress.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;n_epochs &amp;lt;- 10

for (i in 1:n_epochs) {
  
  agg_loglik &amp;lt;- 0
  num_batches &amp;lt;- 0
  iter &amp;lt;- make_iterator_one_shot(train_dataset)
  
  until_out_of_range({
    batch &amp;lt;- iterator_get_next(iter)
    loss &amp;lt;-
      function()
        - tf$reduce_mean(target_dist %&amp;gt;% tfd_log_prob(batch))
    optimizer$minimize(loss)
    
    loglik &amp;lt;- tf$reduce_mean(target_dist %&amp;gt;% tfd_log_prob(batch))
    agg_loglik &amp;lt;- agg_loglik + loglik
    num_batches &amp;lt;- num_batches + 1
    
    test_iter &amp;lt;- make_iterator_one_shot(test_dataset)
    test_batch &amp;lt;- iterator_get_next(test_iter)
    loglik_test_current &amp;lt;- target_dist %&amp;gt;% tfd_log_prob(test_batch) %&amp;gt;% tf$reduce_mean()
    
    if (num_batches %% 100 == 1)
      cat(
        &amp;quot;Epoch &amp;quot;,
        i,
        &amp;quot;: &amp;quot;,
        &amp;quot;Batch &amp;quot;,
        num_batches,
        &amp;quot;: &amp;quot;,
        (agg_loglik %&amp;gt;% as.numeric()) / num_batches,
        &amp;quot; --- test: &amp;quot;,
        loglik_test_current %&amp;gt;% as.numeric(),
        &amp;quot;\n&amp;quot;
      )
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With both training and test sets amounting to over 2 million records each, we did not have the patience to run this model &lt;em&gt;until no improvement occurred for 30 consecutive epochs on the validation set&lt;/em&gt; (like the authors did). However, the picture we get from one complete epoch’s run is pretty clear: The setup seems to work pretty okay.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Epoch  1 :  Batch      1:  -8.212026  --- test:  -10.09264 
Epoch  1 :  Batch   1001:   2.222953  --- test:   1.894102 
Epoch  1 :  Batch   2001:   2.810996  --- test:   2.147804 
Epoch  1 :  Batch   3001:   3.136733  --- test:   3.673271 
Epoch  1 :  Batch   4001:   3.335549  --- test:   4.298822 
Epoch  1 :  Batch   5001:   3.474280  --- test:   4.502975 
Epoch  1 :  Batch   6001:   3.606634  --- test:   4.612468 
Epoch  1 :  Batch   7001:   3.695355  --- test:   4.146113 
Epoch  1 :  Batch   8001:   3.767195  --- test:   3.770533 
Epoch  1 :  Batch   9001:   3.837641  --- test:   4.819314 
Epoch  1 :  Batch  10001:   3.908756  --- test:   4.909763 
Epoch  1 :  Batch  11001:   3.972645  --- test:   3.234356 
Epoch  1 :  Batch  12001:   4.020613  --- test:   5.064850 
Epoch  1 :  Batch  13001:   4.067531  --- test:   4.916662 
Epoch  1 :  Batch  14001:   4.108388  --- test:   4.857317 
Epoch  1 :  Batch  15001:   4.147848  --- test:   5.146242 
Epoch  1 :  Batch  16001:   4.177426  --- test:   4.929565 
Epoch  1 :  Batch  17001:   4.209732  --- test:   4.840716 
Epoch  1 :  Batch  18001:   4.239204  --- test:   5.222693 
Epoch  1 :  Batch  19001:   4.264639  --- test:   5.279918 
Epoch  1 :  Batch  20001:   4.291542  --- test:   5.29119 
Epoch  1 :  Batch  21001:   4.314462  --- test:   4.872157 
Epoch  2 :  Batch      1:   5.212013  --- test:   4.969406 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With these training results, we regard the proof of concept as basically successful. However, from our experiments we also have to say that the choice of hyperparameters seems to matter a &lt;em&gt;lot&lt;/em&gt;. For example, use of the &lt;code&gt;relu&lt;/code&gt; activation function instead of &lt;code&gt;tanh&lt;/code&gt; resulted in the network basically learning nothing. (As per the authors, &lt;code&gt;relu&lt;/code&gt; worked fine on other datasets that had been z-transformed in just the same way.)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Batch normalization&lt;/em&gt; here was obligatory - and this might go for flows in general. The permutation bijectors, on the other hand, did not make much of a difference on this dataset. Overall the impression is that for flows, we might either need a “bag of tricks” (like is commonly said about GANs), or more involved architectures (see “Outlook” below).&lt;/p&gt;
&lt;p&gt;Finally, we wind up with an experiment, coming back to our favorite audio data, already featured in two posts: &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-06-06-simple-audio-classification-keras/"&gt;Simple Audio Classification with Keras&lt;/a&gt; and &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-02-07-audio-background/"&gt;Audio classification with Keras: Looking closer at the non-deep learning parts&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="analysing-audio-data-with-maf"&gt;Analysing audio data with MAF&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://storage.cloud.google.com/download.tensorflow.org/data/speech_commands_v0.01.tar.gz"&gt;dataset in question&lt;/a&gt; consists of recordings of 30 words, pronounced by a number of different speakers. In those previous posts, a convnet was trained to map spectrograms to those 30 classes. Now instead we want to try something different: Train an MAF on one of the classes - the word “zero”, say - and see if we can use the trained network to mark “non-zero” words as less likely: perform &lt;em&gt;anomaly detection&lt;/em&gt;, in a way. Spoiler alert: The results were not too encouraging, and if you are interested in a task like this, you might want to consider a different architecture (again, see “Outlook” below).&lt;/p&gt;
&lt;p&gt;Nonetheless, we quickly relate what was done, as this task is a nice example of handling data where features vary over more than one axis.&lt;/p&gt;
&lt;p&gt;Preprocessing starts as in the aforementioned previous posts. Here though, we explicitly use eager execution, and may sometimes hard-code known values to keep the code snippets short.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tensorflow)
library(tfprobability)

tfe_enable_eager_execution(device_policy = &amp;quot;silent&amp;quot;)

library(tfdatasets)
library(dplyr)
library(readr)
library(purrr)
library(caret)
library(stringr)

# make decode_wav() run with the current release 1.13.1 as well as with the current master branch
decode_wav &amp;lt;- function() if (reticulate::py_has_attr(tf, &amp;quot;audio&amp;quot;)) tf$audio$decode_wav
  else tf$contrib$framework$python$ops$audio_ops$decode_wav
# same for stft()
stft &amp;lt;- function() if (reticulate::py_has_attr(tf, &amp;quot;signal&amp;quot;)) tf$signal$stft else tf$spectral$stft

files &amp;lt;- fs::dir_ls(path = &amp;quot;audio/data_1/speech_commands_v0.01/&amp;quot;, # replace by yours
                    recursive = TRUE,
                    glob = &amp;quot;*.wav&amp;quot;)

files &amp;lt;- files[!str_detect(files, &amp;quot;background_noise&amp;quot;)]

df &amp;lt;- tibble(
  fname = files,
  class = fname %&amp;gt;%
    str_extract(&amp;quot;v0.01/.*/&amp;quot;) %&amp;gt;%
    str_replace_all(&amp;quot;v0.01/&amp;quot;, &amp;quot;&amp;quot;) %&amp;gt;%
    str_replace_all(&amp;quot;/&amp;quot;, &amp;quot;&amp;quot;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We train the MAF on pronunciations of the word “zero”.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;for (c in unique(df$class)) {
  assign(paste0(&amp;quot;df_&amp;quot;, c), df %&amp;gt;% filter(class == c) %&amp;gt;% select(fname))
}

df_ &amp;lt;- df_zero # 2 * 1178 rows
idx_train &amp;lt;- sample(1:nrow(df_), 0.5 * nrow(df_))
df_train &amp;lt;- df_[idx_train, ]
df_test &amp;lt;- df_[-idx_train, ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Following the approach detailed in &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-02-07-audio-background/"&gt;Audio classification with Keras: Looking closer at the non-deep learning parts&lt;/a&gt;, we’d like to train the network on spectrograms instead of the raw time domain data. Using the same settings for &lt;code&gt;frame_length&lt;/code&gt; and &lt;code&gt;frame_step&lt;/code&gt; of the Short Term Fourier Transform as in that post, we’d arrive at data shaped &lt;code&gt;number of frames x number of FFT coefficients&lt;/code&gt;. To make this work with the &lt;code&gt;masked_dense()&lt;/code&gt; employed in &lt;code&gt;tfb_masked_autoregressive_flow()&lt;/code&gt;, the data would then have to be flattened, yielding an impressive 25186 features in the joint distribution.&lt;/p&gt;
&lt;p&gt;With the architecture defined as above in the GAS example, this lead to the network not making much progress. Neither did leaving the data in time domain form, with 16000 features in the joint distribution. Thus, we decided to work with the FFT coefficients computed over the complete window instead, resulting in 257 joint features.&lt;a href="#fn4" class="footnote-ref" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;batch_size &amp;lt;- 100

sampling_rate &amp;lt;- 16000L
data_generator &amp;lt;- function(df,
                           batch_size) {
  
  ds &amp;lt;- tensor_slices_dataset(df) 
  
  ds &amp;lt;- ds %&amp;gt;%
    dataset_map(function(obs) {
      wav &amp;lt;-
        decode_wav()(tf$read_file(tf$reshape(obs$fname, list())))
      samples &amp;lt;- wav$audio[ ,1]
      
      # some wave files have fewer than 16000 samples
      padding &amp;lt;- list(list(0L, sampling_rate - tf$shape(samples)[1]))
      padded &amp;lt;- tf$pad(samples, padding)
      
      stft_out &amp;lt;- stft()(padded, 16000L, 1L, 512L)
      magnitude_spectrograms &amp;lt;- tf$abs(stft_out) %&amp;gt;% tf$squeeze()
    })
  
  ds %&amp;gt;% dataset_batch(batch_size)
  
}

ds_train &amp;lt;- data_generator(df_train, batch_size)
batch &amp;lt;- ds_train %&amp;gt;% 
  make_iterator_one_shot() %&amp;gt;%
  iterator_get_next()

dim(batch) # 100 x 257&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Training then proceeded as on the GAS dataset.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# define MAF
base_dist &amp;lt;-
  tfd_multivariate_normal_diag(loc = rep(0, dim(batch)[2]))

num_hidden &amp;lt;- 512 
use_batchnorm &amp;lt;- TRUE
use_permute &amp;lt;- TRUE
num_mafs &amp;lt;- 10 
num_layers &amp;lt;- 3 * num_mafs

# store bijectors in a list
bijectors &amp;lt;- vector(mode = &amp;quot;list&amp;quot;, length = num_layers)

# fill list, optionally adding batchnorm and permute bijectors
for (i in seq(1, num_layers, by = 3)) {
  maf &amp;lt;- tfb_masked_autoregressive_flow(
    shift_and_log_scale_fn = tfb_masked_autoregressive_default_template(
      hidden_layers = list(num_hidden, num_hidden),
      activation = tf$nn$tanh,
      ))
  bijectors[[i]] &amp;lt;- maf
  if (use_batchnorm)
    bijectors[[i + 1]] &amp;lt;- tfb_batch_normalization()
  if (use_permute)
    bijectors[[i + 2]] &amp;lt;- tfb_permute((dim(batch)[2] - 1):0)
}

if (use_permute) bijectors &amp;lt;- bijectors[-num_layers]
flow &amp;lt;- bijectors %&amp;gt;%
  # possibly clean out empty elements (if no batchnorm or no permute)
  discard(is.null) %&amp;gt;%
  rev() %&amp;gt;%
  tfb_chain()

target_dist &amp;lt;- tfd_transformed_distribution(distribution = base_dist,
                                            bijector = flow)

optimizer &amp;lt;- tf$train$AdamOptimizer(1e-3)

# train MAF
n_epochs &amp;lt;- 100
for (i in 1:n_epochs) {
  agg_loglik &amp;lt;- 0
  num_batches &amp;lt;- 0
  iter &amp;lt;- make_iterator_one_shot(ds_train)
  until_out_of_range({
    batch &amp;lt;- iterator_get_next(iter)
    loss &amp;lt;-
      function()
        - tf$reduce_mean(target_dist %&amp;gt;% tfd_log_prob(batch))
    optimizer$minimize(loss)
    
    loglik &amp;lt;- tf$reduce_mean(target_dist %&amp;gt;% tfd_log_prob(batch))
    agg_loglik &amp;lt;- agg_loglik + loglik
    num_batches &amp;lt;- num_batches + 1
    
    loglik_test_current &amp;lt;- 
      target_dist %&amp;gt;% tfd_log_prob(ds_test) %&amp;gt;% tf$reduce_mean()

    if (num_batches %% 20 == 1)
      cat(
        &amp;quot;Epoch &amp;quot;,
        i,
        &amp;quot;: &amp;quot;,
        &amp;quot;Batch &amp;quot;,
        num_batches,
        &amp;quot;: &amp;quot;,
        ((agg_loglik %&amp;gt;% as.numeric()) / num_batches) %&amp;gt;% round(1),
        &amp;quot; --- test: &amp;quot;,
        loglik_test_current %&amp;gt;% as.numeric() %&amp;gt;% round(1),
        &amp;quot;\n&amp;quot;
      )
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;During training, we also monitored loglikelihoods on three different classes, &lt;em&gt;cat&lt;/em&gt;, &lt;em&gt;bird&lt;/em&gt; and &lt;em&gt;wow&lt;/em&gt;&lt;a href="#fn5" class="footnote-ref" id="fnref5"&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;. Here are the loglikelihoods from the first 10 epochs. “Batch” refers to the current training batch (first batch in the epoch), all other values refer to complete datasets (the complete test set and the three sets selected for comparison).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;epoch   |   batch  |   test   |   &amp;quot;cat&amp;quot;  |   &amp;quot;bird&amp;quot;  |   &amp;quot;wow&amp;quot;  |
--------|----------|----------|----------|-----------|----------|
1       |   1443.5 |   1455.2 |   1398.8 |    1434.2 |   1546.0 |
2       |   1935.0 |   2027.0 |   1941.2 |    1952.3 |   2008.1 | 
3       |   2004.9 |   2073.1 |   2003.5 |    2000.2 |   2072.1 |
4       |   2063.5 |   2131.7 |   2056.0 |    2061.0 |   2116.4 |        
5       |   2120.5 |   2172.6 |   2096.2 |    2085.6 |   2150.1 |
6       |   2151.3 |   2206.4 |   2127.5 |    2110.2 |   2180.6 | 
7       |   2174.4 |   2224.8 |   2142.9 |    2163.2 |   2195.8 |
8       |   2203.2 |   2250.8 |   2172.0 |    2061.0 |   2221.8 |        
9       |   2224.6 |   2270.2 |   2186.6 |    2193.7 |   2241.8 |
10      |   2236.4 |   2274.3 |   2191.4 |    2199.7 |   2243.8 |        &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While this does not look too bad, a complete comparison against all twenty-nine non-target classes had “zero” outperformed by seven other classes, with the remaining twenty-two lower in loglikelihood. We don’t have a model for anomaly detection, as yet.&lt;/p&gt;
&lt;h2 id="outlook"&gt;Outlook&lt;/h2&gt;
&lt;p&gt;As already alluded to several times, for data with temporal and/or spatial orderings more evolved architectures may prove useful. The very successful &lt;em&gt;PixelCNN&lt;/em&gt; family is based on masked convolutions, with more recent developments bringing further refinements (e.g. &lt;em&gt;Gated PixelCNN&lt;/em&gt; &lt;span class="citation"&gt;(Oord et al. 2016)&lt;/span&gt;, &lt;em&gt;PixelCNN++&lt;/em&gt; &lt;span class="citation"&gt;(Salimans et al. 2017)&lt;/span&gt;. &lt;strong&gt;Attention&lt;/strong&gt;, too, may be masked and thus rendered autoregressive, as employed in the hybrid &lt;em&gt;PixelSNAIL&lt;/em&gt; &lt;span class="citation"&gt;(Chen et al. 2017)&lt;/span&gt; and the - not surprisingly given its name - transformer-based &lt;em&gt;ImageTransformer&lt;/em&gt; &lt;span class="citation"&gt;(Parmar et al. 2018)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To conclude, - while this post was interested in the intersection of flows and autoregressivity - and last not least the use therein of TFP bijectors - an upcoming one might dive deeper into autoregressive models specifically… and who knows, perhaps come back to the audio data for a fourth time.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-abs-1712-09763"&gt;
&lt;p&gt;Chen, Xi, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. 2017. “PixelSNAIL: An Improved Autoregressive Generative Model.” &lt;em&gt;CoRR&lt;/em&gt; abs/1712.09763. &lt;a href="http://arxiv.org/abs/1712.09763"&gt;http://arxiv.org/abs/1712.09763&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-DinhSB16"&gt;
&lt;p&gt;Dinh, Laurent, Jascha Sohl-Dickstein, and Samy Bengio. 2016. “Density Estimation Using Real Nvp.” &lt;em&gt;CoRR&lt;/em&gt; abs/1605.08803. &lt;a href="http://arxiv.org/abs/1605.08803"&gt;http://arxiv.org/abs/1605.08803&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-GermainGML15"&gt;
&lt;p&gt;Germain, Mathieu, Karol Gregor, Iain Murray, and Hugo Larochelle. 2015. “MADE: Masked Autoencoder for Distribution Estimation.” &lt;em&gt;CoRR&lt;/em&gt; abs/1502.03509. &lt;a href="http://arxiv.org/abs/1502.03509"&gt;http://arxiv.org/abs/1502.03509&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-OordKVEGK16"&gt;
&lt;p&gt;Oord, Aaron van den, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. 2016. “Conditional Image Generation with Pixelcnn Decoders.” &lt;em&gt;CoRR&lt;/em&gt; abs/1606.05328. &lt;a href="http://arxiv.org/abs/1606.05328"&gt;http://arxiv.org/abs/1606.05328&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-2017arXiv170507057P"&gt;
&lt;p&gt;Papamakarios, George, Theo Pavlakou, and Iain Murray. 2017. “Masked Autoregressive Flow for Density Estimation.” &lt;em&gt;arXiv E-Prints&lt;/em&gt;, May, arXiv:1705.07057. &lt;a href="http://arxiv.org/abs/1705.07057"&gt;http://arxiv.org/abs/1705.07057&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-abs-1802-05751"&gt;
&lt;p&gt;Parmar, Niki, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, and Alexander Ku. 2018. “Image Transformer.” &lt;em&gt;CoRR&lt;/em&gt; abs/1802.05751. &lt;a href="http://arxiv.org/abs/1802.05751"&gt;http://arxiv.org/abs/1802.05751&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-SalimansKCK17"&gt;
&lt;p&gt;Salimans, Tim, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. 2017. “PixelCNN++: Improving the Pixelcnn with Discretized Logistic Mixture Likelihood and Other Modifications.” &lt;em&gt;CoRR&lt;/em&gt; abs/1701.05517. &lt;a href="http://arxiv.org/abs/1701.05517"&gt;http://arxiv.org/abs/1701.05517&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;&lt;code&gt;tfb_masked_autoregressive_flow&lt;/code&gt;, in R&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;For a comparison of &lt;em&gt;Masked Autoregressive Flow&lt;/em&gt; to its siblings &lt;em&gt;Real NVP&lt;/em&gt; (also available as a TFP bijector) and &lt;em&gt;Inverse Autogressive Flow&lt;/em&gt; (to be obtained as an inverse of MAF in TFP), see Eric Jang’s excellent &lt;a href="https://blog.evjang.com/2018/01/nf2.html"&gt;tutorial&lt;/a&gt;.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;not specified individually in the paper&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;We quickly experimented with a higher number of FFT coefficients, but the approach did not seem that promising.&lt;a href="#fnref4" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn5"&gt;&lt;p&gt;to avoid clutter we don’t show the respective code&lt;a href="#fnref5" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">68c4fe7e15984d764a6ab9a615b550c4</distill:md5>
      <category>Probabilistic ML/DL</category>
      <category>Unsupervised Learning</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-04-24-autoregressive-flows</guid>
      <pubDate>Wed, 24 Apr 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-04-24-autoregressive-flows/images/made.png" medium="image" type="image/png" width="686" height="398"/>
    </item>
    <item>
      <title>Auto-Keras: Tuning-free deep learning from R</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Juan Cruz Rodriguez</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-04-16-autokeras</link>
      <description>


&lt;pre class="r"&gt;&lt;code&gt;knitr::opts_chunk$set(echo = TRUE, eval = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Today, we’re happy to feature a guest post written by Juan Cruz, showing how to use Auto-Keras from R. Juan holds a master’s degree in Computer Science. Currently, he is finishing his master’s degree in Applied Statistics, as well as a Ph.D. in Computer Science, at the Universidad Nacional de Córdoba. He started his R journey almost six years ago, applying statistical methods to biology data. He enjoys software projects focused on making machine learning and data science available to everyone.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In the past few years, artificial intelligence has been a subject of intense media hype. Machine learning, deep learning, and artificial intelligence come up in countless articles, often outside of technology-minded publications. For most any topic, a brief search on the web yields dozens of texts suggesting the application of one or the other deep learning model.&lt;/p&gt;
&lt;p&gt;However, tasks such as feature engineering, hyperparameter tuning, or network design, are by no means easy for people without a rich computer science background. Lately, research started to emerge in the area of what is known as Neural Architecture Search (NAS) &lt;span class="citation"&gt;(Baker et al. 2016; Pham et al. 2018; Zoph and Le 2016; Luo et al. 2018; Liu et al. 2017; Real et al. 2018; Jin, Song, and Hu 2018)&lt;/span&gt;. The main goal of NAS algorithms is, given a specific tagged dataset, to search for the most optimal neural network to perform a certain task on that dataset. In this sense, NAS algorithms allow the user to not have to worry about any task related to data science engineering. In other words, given a tagged dataset and a task, e.g., image classification, or text classification among others, the NAS algorithm will train several high-performance deep learning models and return the one that outperforms the rest.&lt;/p&gt;
&lt;p&gt;Several NAS algorithms were developed on different platforms (e.g. &lt;a href="https://cloud.google.com/automl/"&gt;Google Cloud AutoML&lt;/a&gt;), or as libraries of certain programming languages (e.g. &lt;a href="https://autokeras.com/"&gt;Auto-Keras&lt;/a&gt;, &lt;a href="https://epistasislab.github.io/tpot/"&gt;TPOT&lt;/a&gt;, &lt;a href="https://www.automl.org/automl/auto-sklearn/"&gt;Auto-Sklearn&lt;/a&gt;). However, for a language that brings together experts from such diverse disciplines as is the R programming language, to the best of our knowledge, there is no NAS tool to this day. In this post, we present the Auto-Keras R package, an interface from R to the &lt;a href="https://autokeras.com/"&gt;Auto-Keras Python library&lt;/a&gt; &lt;span class="citation"&gt;(Jin, Song, and Hu 2018)&lt;/span&gt;. Thanks to the use of Auto-Keras, R programmers with few lines of code will be able to train several deep learning models for their data and get the one that outperforms the others.&lt;/p&gt;
&lt;p&gt;Let’s dive into Auto-Keras!&lt;/p&gt;
&lt;h2 id="auto-keras"&gt;Auto-Keras&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; the Python Auto-Keras library is only compatible with Python 3.6. So make sure this version is currently installed, and correctly set to be used by the &lt;a href="https://rstudio.github.io/reticulate/"&gt;&lt;code&gt;reticulate&lt;/code&gt;&lt;/a&gt; R library.&lt;/p&gt;
&lt;h3 id="installation"&gt;Installation&lt;/h3&gt;
&lt;p&gt;To begin, install the autokeras R package from GitHub as follows:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;if (!require(&amp;quot;remotes&amp;quot;)) {
  install.packages(&amp;quot;remotes&amp;quot;)
}
remotes::install_github(&amp;quot;jcrodriguez1989/autokeras&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Auto-Keras R interface uses the Keras and TensorFlow backend engines by default. To install both the core Auto-Keras library as well as the Keras and TensorFlow backends use the &lt;code&gt;install_autokeras()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(&amp;quot;autokeras&amp;quot;)
install_autokeras()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will provide you with default CPU-based installations of Keras and TensorFlow. If you want a more customized installation, e.g. if you want to take advantage of NVIDIA GPUs, see the documentation for &lt;a href="https://keras.rstudio.com/reference/install_keras.html"&gt;&lt;code&gt;install_keras()&lt;/code&gt;&lt;/a&gt; from the &lt;code&gt;keras&lt;/code&gt; R library.&lt;/p&gt;
&lt;h3 id="mnist-example"&gt;MNIST Example&lt;/h3&gt;
&lt;p&gt;We can learn the basics of Auto-Keras by walking through a simple example: recognizing handwritten digits from the &lt;a href="https://en.wikipedia.org/wiki/MNIST_database"&gt;MNIST&lt;/a&gt; dataset. MNIST consists of 28 x 28 grayscale images of handwritten digits like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-04-16-autokeras/images/mnist_ggplotted_num.png" /&gt;&lt;/p&gt;
&lt;p&gt;The dataset also includes labels for each image, telling us which digit it is. For example, the label for the above image is 2.&lt;/p&gt;
&lt;h4 id="loading-the-data"&gt;Loading the Data&lt;/h4&gt;
&lt;p&gt;The MNIST dataset is included with Keras and can be accessed using the &lt;a href="https://keras.rstudio.com/reference/index.html#section-datasets"&gt;&lt;code&gt;dataset_mnist()&lt;/code&gt;&lt;/a&gt; function from the &lt;code&gt;keras&lt;/code&gt; R library. Here we load the dataset, and then create variables for our test and training data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(&amp;quot;keras&amp;quot;)
mnist &amp;lt;- dataset_mnist() # load mnist dataset
c(x_train, y_train) %&amp;lt;-% mnist$train # get train
c(x_test, y_test) %&amp;lt;-% mnist$test # and test data&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;x&lt;/code&gt; data is a 3-d array &lt;code&gt;(images,width,height)&lt;/code&gt; of grayscale integer values ranging between 0 to 255.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;x_train[1, 14:20, 14:20] # show some pixels from the first image&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;     [,1] [,2] [,3] [,4] [,5] [,6] [,7]
[1,]  241  225  160  108    1    0    0
[2,]   81  240  253  253  119   25    0
[3,]    0   45  186  253  253  150   27
[4,]    0    0   16   93  252  253  187
[5,]    0    0    0    0  249  253  249
[6,]    0   46  130  183  253  253  207
[7,]  148  229  253  253  253  250  182&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;y&lt;/code&gt; data is an integer vector with values ranging from 0 to 9.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;n_imgs &amp;lt;- 8
head(y_train, n = n_imgs) # show first 8 labels&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 5 0 4 1 9 2 1 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each of these images can be plotted in R:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(&amp;quot;ggplot2&amp;quot;)
library(&amp;quot;tidyr&amp;quot;)
# get each of the first n_imgs from the x_train dataset and
# convert them to wide format
mnist_to_plot &amp;lt;-
  do.call(rbind, lapply(seq_len(n_imgs), function(i) {
    samp_img &amp;lt;- x_train[i, , ] %&amp;gt;%
      as.data.frame()
    colnames(samp_img) &amp;lt;- seq_len(ncol(samp_img))
    data.frame(
      img = i,
      gather(samp_img, &amp;quot;x&amp;quot;, &amp;quot;value&amp;quot;, convert = TRUE),
      y = seq_len(nrow(samp_img))
    )
  }))
ggplot(mnist_to_plot, aes(x = x, y = y, fill = value)) + geom_tile() +
  scale_fill_gradient(low = &amp;quot;black&amp;quot;, high = &amp;quot;white&amp;quot;, na.value = NA) +
  scale_y_reverse() + theme_minimal() + theme(panel.grid = element_blank()) +
  theme(aspect.ratio = 1) + xlab(&amp;quot;&amp;quot;) + ylab(&amp;quot;&amp;quot;) + facet_wrap(~img, nrow = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-04-16-autokeras/images/some_mnist_nums.png" /&gt;&lt;/p&gt;
&lt;h4 id="data-ready-lets-get-the-model"&gt;Data ready, let’s get the model!&lt;/h4&gt;
&lt;p&gt;Data pre-processing? Model definition? Metrics, epochs definition, anyone? No, none of them are required by Auto-Keras. For image classification tasks, it is enough for Auto-Keras to be passed the &lt;code&gt;x_train&lt;/code&gt; and &lt;code&gt;y_train&lt;/code&gt; objects as defined above.&lt;/p&gt;
&lt;p&gt;So, to train several deep learning models for two hours, it is enough to run:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# train an Image Classifier for two hours
clf &amp;lt;- model_image_classifier(verbose = TRUE) %&amp;gt;%
  fit(x_train, y_train, time_limit = 2 * 60 * 60)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Saving Directory: /tmp/autokeras_ZOG76O
Preprocessing the images.
Preprocessing finished.

Initializing search.
Initialization finished.


+----------------------------------------------+
|               Training model 0               |
+----------------------------------------------+

No loss decrease after 5 epochs.


Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           0            |  0.19463148526847363   |   0.9843999999999999   |
+--------------------------------------------------------------------------+


+----------------------------------------------+
|               Training model 1               |
+----------------------------------------------+

No loss decrease after 5 epochs.


Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           1            |   0.210642946138978    |         0.984          |
+--------------------------------------------------------------------------+&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Evaluate it:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;clf %&amp;gt;% evaluate(x_test, y_test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.9866&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then just get the best-trained model with:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;clf %&amp;gt;% final_fit(x_train, y_train, x_test, y_test, retrain = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;No loss decrease after 30 epochs.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Evaluate the final model:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;clf %&amp;gt;% evaluate(x_test, y_test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.9918&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the model can be saved to take it into production with:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;clf %&amp;gt;% export_autokeras_model(&amp;quot;./myMnistModel.pkl&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="conclusions"&gt;Conclusions&lt;/h3&gt;
&lt;p&gt;In this post, the Auto-Keras R package was presented. It was shown that, with almost no deep learning knowledge, it is possible to train models and get the one that returns the best results for the desired task. Here we trained models for two hours. However, we have also tried training for 24 hours, resulting in 15 models being trained, to a final accuracy of 0.9928. Although Auto-Keras will not return a model as efficient as one generated manually by an expert, this new library has its place as an excellent starting point in the world of deep learning. Auto-Keras is an open-source R package, and is freely available in &lt;a href="https://github.com/jcrodriguez1989/autokeras/"&gt;https://github.com/jcrodriguez1989/autokeras/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Although the Python Auto-Keras library is currently in a pre-release version and comes with not too many types of training tasks, this is likely to change soon, as the project it was recently added to the &lt;em&gt;keras-team&lt;/em&gt; set of repositories. This will undoubtedly further its progress a lot. So stay tuned, and thanks for reading!&lt;/p&gt;
&lt;h3 id="reproducibility"&gt;Reproducibility&lt;/h3&gt;
&lt;p&gt;To correctly reproduce the results of this post, we recommend using the Auto-Keras docker image by typing:&lt;/p&gt;
&lt;pre class="bash"&gt;&lt;code&gt;docker pull jcrodriguez1989/r-autokeras:0.1.0
docker run -it jcrodriguez1989/r-autokeras:0.1.0 /bin/bash&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-baker2016designing"&gt;
&lt;p&gt;Baker, Bowen, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. 2016. “Designing Neural Network Architectures Using Reinforcement Learning.” &lt;em&gt;arXiv Preprint arXiv:1611.02167&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-jin2018efficient"&gt;
&lt;p&gt;Jin, Haifeng, Qingquan Song, and Xia Hu. 2018. “Auto-Keras: An Efficient Neural Architecture Search System.” &lt;em&gt;arXiv Preprint arXiv:1806.10282&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-liu2017hierarchical"&gt;
&lt;p&gt;Liu, Hanxiao, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu. 2017. “Hierarchical Representations for Efficient Architecture Search.” &lt;em&gt;arXiv Preprint arXiv:1711.00436&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-luo2018neural"&gt;
&lt;p&gt;Luo, Renqian, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. 2018. “Neural Architecture Optimization.” In &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;, 7816–27.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-pham2018efficient"&gt;
&lt;p&gt;Pham, Hieu, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeff Dean. 2018. “Efficient Neural Architecture Search via Parameter Sharing.” &lt;em&gt;arXiv Preprint arXiv:1802.03268&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-real2018regularized"&gt;
&lt;p&gt;Real, Esteban, Alok Aggarwal, Yanping Huang, and Quoc V Le. 2018. “Regularized Evolution for Image Classifier Architecture Search.” &lt;em&gt;arXiv Preprint arXiv:1802.01548&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-zoph2016neural"&gt;
&lt;p&gt;Zoph, Barret, and Quoc V Le. 2016. “Neural Architecture Search with Reinforcement Learning.” &lt;em&gt;arXiv Preprint arXiv:1611.01578&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">25bc177d120925e2c39826f9222308cf</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-04-16-autokeras</guid>
      <pubDate>Tue, 16 Apr 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-04-16-autokeras/images/thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Getting into the flow: Bijectors in TensorFlow Probability</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-04-05-bijectors-flows</link>
      <description>


&lt;p&gt;As of today, deep learning’s greatest successes have taken place in the realm of supervised learning, requiring lots and lots of annotated training data. However, data does not (normally) come with annotations or labels. Also, &lt;em&gt;unsupervised learning&lt;/em&gt; is attractive because of the analogy to human cognition.&lt;/p&gt;
&lt;p&gt;On this blog so far, we have seen two major architectures for unsupervised learning: &lt;a href="https://blogs.rstudio.com/tensorflow/#Variational_autoencoders_(VAEs)"&gt;variational autoencoders&lt;/a&gt; and &lt;a href="https://blogs.rstudio.com/tensorflow/#GAN"&gt;generative adversarial networks&lt;/a&gt;. Lesser known, but appealing for conceptual as well as for performance reasons are &lt;em&gt;normalizing flows&lt;/em&gt; &lt;span class="citation"&gt;(Jimenez Rezende and Mohamed 2015)&lt;/span&gt;. In this and the next post, we’ll introduce flows, focusing on how to implement them using &lt;em&gt;TensorFlow Probability&lt;/em&gt; (TFP).&lt;/p&gt;
&lt;p&gt;In contrast to &lt;a href="https://blogs.rstudio.com/tensorflow/#Probability_and_statistics"&gt;previous posts involving TFP&lt;/a&gt; that accessed its functionality using low-level &lt;code&gt;$&lt;/code&gt;-syntax, we now make use of &lt;a href="https://rstudio.github.io/tfprobability/"&gt;tfprobability&lt;/a&gt;, an R wrapper in the style of &lt;code&gt;keras&lt;/code&gt;, &lt;code&gt;tensorflow&lt;/code&gt; and &lt;code&gt;tfdatasets&lt;/code&gt;. A note regarding this package: It is still under heavy development and the API may change. As of this writing, wrappers do not yet exist for all TFP modules, but all TFP functionality is available using &lt;code&gt;$&lt;/code&gt;-syntax if need be.&lt;/p&gt;
&lt;h2 id="density-estimation-and-sampling"&gt;Density estimation and sampling&lt;/h2&gt;
&lt;p&gt;Back to unsupervised learning, and specifically thinking of variational autoencoders, what are the main things they give us? One thing that’s seldom missing from papers on generative methods are pictures of super-real-looking faces (or bed rooms, or animals …). So evidently &lt;em&gt;sampling&lt;/em&gt; (or: generation) is an important part. If we can sample from a model and obtain real-seeming entities, this means the model has learned something about how things are distributed in the world: it has learned a &lt;em&gt;distribution&lt;/em&gt;. In the case of variational autoencoders, there is more: The entities are supposed to be determined by a set of distinct, disentangled (hopefully!) latent factors. But this is not the assumption in the case of normalizing flows, so we are not going to elaborate on this here.&lt;/p&gt;
&lt;p&gt;As a recap, how do we sample from a VAE? We draw from &lt;span class="math inline"&gt;\(z\)&lt;/span&gt;, the latent variable, and run the decoder network on it. The result should - we hope - look like it comes from the empirical data distribution. It should not, however, look &lt;em&gt;exactly&lt;/em&gt; like any of the items used to train the VAE, or else we have not learned anything useful.&lt;/p&gt;
&lt;p&gt;The second thing we may get from a VAE is an assessment of the plausibility of individual data, to be used, for example, in anomaly detection. Here “plausibility” is vague on purpose: With VAE, we don’t have a means to compute an actual density under the posterior.&lt;/p&gt;
&lt;p&gt;What if we want, or need, both: generation of samples as well as density estimation? This is where &lt;em&gt;normalizing flows&lt;/em&gt; come in.&lt;/p&gt;
&lt;h2 id="normalizing-flows"&gt;Normalizing flows&lt;/h2&gt;
&lt;p&gt;A &lt;em&gt;flow&lt;/em&gt; is a sequence of differentiable, invertible mappings from data to a “nice” distribution, something we can easily sample from and use to calculate a density. Let’s take as example the canonical way to generate samples from some distribution, the exponential, say.&lt;/p&gt;
&lt;p&gt;We start by asking our random number generator for some number between 0 and 1:&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;u &amp;lt;- runif(1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This number we treat as coming from a &lt;em&gt;cumulative probability distribution&lt;/em&gt; (CDF) - from an &lt;em&gt;exponential&lt;/em&gt; CDF, to be precise. Now that we have a value from the CDF, all we need to do is map that “back” to a value. That mapping &lt;code&gt;CDF -&amp;gt; value&lt;/code&gt; we’re looking for is just the inverse of the CDF of an exponential distribution, the CDF being&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[F(x) = 1 - e^{-\lambda x}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The inverse then is&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
F^{-1}(u) = -\frac{1}{\lambda} ln (1 - u)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which means we may get our exponential sample doing&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;lambda &amp;lt;- 0.5 # pick some lambda
x &amp;lt;- -1/lambda * log(1-u)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see the CDF is actually a &lt;em&gt;flow&lt;/em&gt; (or a building block thereof, if we picture most flows as comprising several transformations), since&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It maps data to a uniform distribution between 0 and 1, allowing to assess data likelihood.&lt;/li&gt;
&lt;li&gt;Conversely, it maps a probability to an actual value, thus allowing to generate samples.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;From this example, we see why a flow should be invertible, but we don’t yet see why it should be &lt;em&gt;differentiable&lt;/em&gt;. This will become clear shortly, but first let’s take a look at how flows are available in &lt;code&gt;tfprobability&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="bijectors"&gt;Bijectors&lt;/h2&gt;
&lt;p&gt;TFP comes with a treasure trove of transformations, called &lt;code&gt;bijectors&lt;/code&gt;, ranging from simple computations like &lt;a href="https://rstudio.github.io/tfprobability/reference/tfb_exp.html"&gt;exponentiation&lt;/a&gt; to more complex ones like the &lt;a href="https://rstudio.github.io/tfprobability/reference/tfb_discrete_cosine_transform.html"&gt;discrete cosine transform&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To get started, let’s use &lt;code&gt;tfprobability&lt;/code&gt; to generate samples from the normal distribution. There is a bijector &lt;code&gt;tfb_normal_cdf()&lt;/code&gt; that takes input data to the interval &lt;span class="math inline"&gt;\([0,1]\)&lt;/span&gt;. Its inverse transform then yields a random variable with the standard normal distribution:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tfprobability)
library(tensorflow)
tfe_enable_eager_execution()

library(ggplot2)

b &amp;lt;- tfb_normal_cdf()
u &amp;lt;- runif(1000)
x &amp;lt;- b %&amp;gt;% tfb_inverse(u) %&amp;gt;% as.numeric()

x %&amp;gt;% data.frame(x = .) %&amp;gt;% ggplot(aes(x = x)) + geom_density()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-04-05-bijectors-flows/images/normal_samples.png" /&gt;&lt;/p&gt;
&lt;p&gt;Conversely, we can use this bijector to determine the (log) probability of a sample from the normal distribution. We’ll check against a straightforward use of &lt;code&gt;tfd_normal&lt;/code&gt; in the &lt;code&gt;distributions&lt;/code&gt; module:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;x &amp;lt;- 2.01
d_n &amp;lt;- tfd_normal(loc = 0, scale = 1) 

d_n %&amp;gt;% tfd_log_prob(x) %&amp;gt;% as.numeric() # -2.938989&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To obtain that same log probability from the bijector, we add two components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Firstly, we run the sample through the &lt;code&gt;forward&lt;/code&gt; transformation and compute log probability under the uniform distribution.&lt;/li&gt;
&lt;li&gt;Secondly, as we’re using the uniform distribution to determine probability of a normal sample, we need to track how probability changes under this transformation. This is done by calling &lt;code&gt;tfb_forward_log_det_jacobian&lt;/code&gt; (to be further elaborated on below).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;b &amp;lt;- tfb_normal_cdf()
d_u &amp;lt;- tfd_uniform()

l &amp;lt;- d_u %&amp;gt;% tfd_log_prob(b %&amp;gt;% tfb_forward(x))
j &amp;lt;- b %&amp;gt;% tfb_forward_log_det_jacobian(x, event_ndims = 0)

(l + j) %&amp;gt;% as.numeric() # -2.938989&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why does this work? Let’s get some background.&lt;/p&gt;
&lt;h2 id="probability-mass-is-conserved"&gt;Probability mass is conserved&lt;/h2&gt;
&lt;p&gt;Flows are based on the principle that under transformation, probability mass is conserved. Say we have a flow from &lt;span class="math inline"&gt;\(x\)&lt;/span&gt; to &lt;span class="math inline"&gt;\(z\)&lt;/span&gt;: &lt;span class="math display"&gt;\[z = f(x)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Suppose we sample from &lt;span class="math inline"&gt;\(z\)&lt;/span&gt; and then, compute the inverse transform to obtain &lt;span class="math inline"&gt;\(x\)&lt;/span&gt;. We know the probability of &lt;span class="math inline"&gt;\(z\)&lt;/span&gt;. What is the probability that &lt;span class="math inline"&gt;\(x\)&lt;/span&gt;, the transformed sample, lies between &lt;span class="math inline"&gt;\(x_0\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(x_0 + dx\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;This probability is &lt;span class="math inline"&gt;\(p(x) \ dx\)&lt;/span&gt;, the density times the length of the interval. This has to equal the probability that &lt;span class="math inline"&gt;\(z\)&lt;/span&gt; lies between &lt;span class="math inline"&gt;\(f(x)\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(f(x + dx)\)&lt;/span&gt;. That new interval has length &lt;span class="math inline"&gt;\(f&amp;#39;(x) dx\)&lt;/span&gt;, so:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[p(x) dx = p(z) f&amp;#39;(x) dx\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Or equivalently&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[p(x) = p(z) * dz/dx\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus, the sample probability &lt;span class="math inline"&gt;\(p(x)\)&lt;/span&gt; is determined by the base probability &lt;span class="math inline"&gt;\(p(z)\)&lt;/span&gt; of the transformed distribution, multiplied by how much the flow stretches space.&lt;/p&gt;
&lt;p&gt;The same goes in higher dimensions: Again, the flow is about the change in probability volume between the &lt;span class="math inline"&gt;\(z\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(y\)&lt;/span&gt; spaces:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[p(x) =  p(z) \frac{vol(dz)}{vol(dx)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In higher dimensions, the Jacobian replaces the derivative. Then, the change in volume is captured by the absolute value of its determinant:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[p(\mathbf{x}) = p(f(\mathbf{x})) \ \bigg|det\frac{\partial f({\mathbf{x})}}{\partial{\mathbf{x}}}\bigg|\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In practice, we work with log probabilities, so&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[log \ p(\mathbf{x}) = log \ p(f(\mathbf{x})) + log \ \bigg|det\frac{\partial f({\mathbf{x})}}{\partial{\mathbf{x}}}\bigg| \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s see this with another &lt;code&gt;bijector&lt;/code&gt; example, &lt;code&gt;tfb_affine_scalar&lt;/code&gt;. Below, we construct a mini-flow that maps a few arbitrary chosen &lt;span class="math inline"&gt;\(x\)&lt;/span&gt; values to double their value (&lt;code&gt;scale = 2&lt;/code&gt;):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;x &amp;lt;- c(0, 0.5, 1)
b &amp;lt;- tfb_affine_scalar(shift = 0, scale = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To compare densities under the flow, we choose the normal distribution, and look at the log densities:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;d_n &amp;lt;- tfd_normal(loc = 0, scale = 1)
d_n %&amp;gt;% tfd_log_prob(x) %&amp;gt;% as.numeric() # -0.9189385 -1.0439385 -1.4189385&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now apply the flow and compute the new log densities as a sum of the log densities of the corresponding &lt;span class="math inline"&gt;\(x\)&lt;/span&gt; values and the log determinant of the Jacobian:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;z &amp;lt;- b %&amp;gt;% tfb_forward(x)

(d_n  %&amp;gt;% tfd_log_prob(b %&amp;gt;% tfb_inverse(z))) +
  (b %&amp;gt;% tfb_inverse_log_det_jacobian(z, event_ndims = 0)) %&amp;gt;%
  as.numeric() # -1.6120857 -1.7370857 -2.1120858&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that as the values get stretched in space (we multiply by 2), the individual log densities go down. We can verify the cumulative probability stays the same using &lt;code&gt;tfd_transformed_distribution()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;d_t &amp;lt;- tfd_transformed_distribution(distribution = d_n, bijector = b)
d_n %&amp;gt;% tfd_cdf(x) %&amp;gt;% as.numeric()  # 0.5000000 0.6914625 0.8413447

d_t %&amp;gt;% tfd_cdf(y) %&amp;gt;% as.numeric()  # 0.5000000 0.6914625 0.8413447&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So far, the flows we saw were static - how does this fit into the framework of neural networks?&lt;/p&gt;
&lt;h2 id="training-a-flow"&gt;Training a flow&lt;/h2&gt;
&lt;p&gt;Given that flows are bidirectional, there are two ways to think about them. Above, we have mostly stressed the inverse mapping: We want a simple distribution we can sample from, and which we can use to compute a density. In that line, flows are sometimes called “mappings from data to noise” - &lt;em&gt;noise&lt;/em&gt; mostly being an isotropic Gaussian. However in practice, we don’t have that “noise” yet, we just have data. So in practice, we have to &lt;em&gt;learn&lt;/em&gt; a flow that does such a mapping. We do this by using &lt;code&gt;bijectors&lt;/code&gt; with trainable parameters. We’ll see a very simple example here, and leave “real world flows” to the next post.&lt;/p&gt;
&lt;p&gt;The example is based on part 1 of &lt;a href="https://blog.evjang.com/2018/01/nf1.html"&gt;Eric Jang’s introduction to normalizing flows&lt;/a&gt;. The main difference (apart from simplification to show the basic pattern) is that we’re using eager execution.&lt;/p&gt;
&lt;p&gt;We start from a two-dimensional, isotropic Gaussian, and we want to model data that’s also normal, but with a mean of 1 and a variance of 2 (in both dimensions).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tensorflow)
library(tfprobability)

tfe_enable_eager_execution(device_policy = &amp;quot;silent&amp;quot;)

library(tfdatasets)

# where we start from
base_dist &amp;lt;- tfd_multivariate_normal_diag(loc = c(0, 0))

# where we want to go
target_dist &amp;lt;- tfd_multivariate_normal_diag(loc = c(1, 1), scale_identity_multiplier = 2)

# create training data from the target distribution
target_samples &amp;lt;- target_dist %&amp;gt;% tfd_sample(1000) %&amp;gt;% tf$cast(tf$float32)

batch_size &amp;lt;- 100
dataset &amp;lt;- tensor_slices_dataset(target_samples) %&amp;gt;%
  dataset_shuffle(buffer_size = dim(target_samples)[1]) %&amp;gt;%
  dataset_batch(batch_size)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’ll build a tiny neural network, consisting of an affine transformation and a nonlinearity. For the former, we can make use of &lt;code&gt;tfb_affine&lt;/code&gt;, the multi-dimensional relative of &lt;code&gt;tfb_affine_scalar&lt;/code&gt;. As to nonlinearities, currently TFP comes with &lt;code&gt;tfb_sigmoid&lt;/code&gt; and &lt;code&gt;tfb_tanh&lt;/code&gt;, but we can build our own parameterized ReLU using &lt;code&gt;tfb_inline&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# alpha is a learnable parameter
bijector_leaky_relu &amp;lt;- function(alpha) {
  
  tfb_inline(
    # forward transform leaves positive values untouched and scales negative ones by alpha
    forward_fn = function(x)
      tf$where(tf$greater_equal(x, 0), x, alpha * x),
    # inverse transform leaves positive values untouched and scales negative ones by 1/alpha
    inverse_fn = function(y)
      tf$where(tf$greater_equal(y, 0), y, 1/alpha * y),
    # volume change is 0 when positive and 1/alpha when negative
    inverse_log_det_jacobian_fn = function(y) {
      I &amp;lt;- tf$ones_like(y)
      J_inv &amp;lt;- tf$where(tf$greater_equal(y, 0), I, 1/alpha * I)
      log_abs_det_J_inv &amp;lt;- tf$log(tf$abs(J_inv))
      tf$reduce_sum(log_abs_det_J_inv, axis = 1L)
    },
    forward_min_event_ndims = 1
  )
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define the learnable variables for the affine and the PReLU layers:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;d &amp;lt;- 2 # dimensionality
r &amp;lt;- 2 # rank of update

# shift of affine bijector
shift &amp;lt;- tf$get_variable(&amp;quot;shift&amp;quot;, d)
# scale of affine bijector
L &amp;lt;- tf$get_variable(&amp;#39;L&amp;#39;, c(d * (d + 1) / 2))
# rank-r update
V &amp;lt;- tf$get_variable(&amp;quot;V&amp;quot;, c(d, r))

# scaling factor of parameterized relu
alpha &amp;lt;- tf$abs(tf$get_variable(&amp;#39;alpha&amp;#39;, list())) + 0.01&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With eager execution, the variables have to be used inside the loss function, so that is where we define the bijectors. Our little flow now is a &lt;code&gt;tfb_chain&lt;/code&gt; of bijectors, and we wrap it in a &lt;em&gt;TransformedDistribution&lt;/em&gt; (&lt;code&gt;tfd_transformed_distribution&lt;/code&gt;) that links source and target distributions.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;loss &amp;lt;- function() {
  
 affine &amp;lt;- tfb_affine(
        scale_tril = tfb_fill_triangular() %&amp;gt;% tfb_forward(L),
        scale_perturb_factor = V,
        shift = shift
      )
 lrelu &amp;lt;- bijector_leaky_relu(alpha = alpha)  
 
 flow &amp;lt;- list(lrelu, affine) %&amp;gt;% tfb_chain()
 
 dist &amp;lt;- tfd_transformed_distribution(distribution = base_dist,
                          bijector = flow)
  
 l &amp;lt;- -tf$reduce_mean(dist$log_prob(batch))
 # keep track of progress
 print(round(as.numeric(l), 2))
 l
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can actually run the training!&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;optimizer &amp;lt;- tf$train$AdamOptimizer(1e-4)

n_epochs &amp;lt;- 100
for (i in 1:n_epochs) {
  iter &amp;lt;- make_iterator_one_shot(dataset)
  until_out_of_range({
    batch &amp;lt;- iterator_get_next(iter)
    optimizer$minimize(loss)
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Outcomes will differ depending on random initialization, but you should see a steady (if slow) progress. Using bijectors, we have actually trained and defined a little neural network.&lt;/p&gt;
&lt;h2 id="outlook"&gt;Outlook&lt;/h2&gt;
&lt;p&gt;Undoubtedly, this flow is too simple to model complex data, but it’s instructive to have seen the basic principles before delving into more complex flows. In the next post, we’ll check out &lt;em&gt;autoregressive flows&lt;/em&gt;, again using TFP and &lt;code&gt;tfprobability&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-2015arXiv150505770J"&gt;
&lt;p&gt;Jimenez Rezende, Danilo, and Shakir Mohamed. 2015. “Variational Inference with Normalizing Flows.” &lt;em&gt;arXiv E-Prints&lt;/em&gt;, May, arXiv:1505.05770. &lt;a href="http://arxiv.org/abs/1505.05770"&gt;http://arxiv.org/abs/1505.05770&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Yes, using &lt;code&gt;runif()&lt;/code&gt;. Just imagine there were no corresponding &lt;code&gt;rexp()&lt;/code&gt; in R…&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">3472080b1e805f11b811f879ed823776</distill:md5>
      <category>Probabilistic ML/DL</category>
      <category>TensorFlow/Keras</category>
      <category>Concepts</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-04-05-bijectors-flows</guid>
      <pubDate>Fri, 05 Apr 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-04-05-bijectors-flows/images/flows.png" medium="image" type="image/png" width="904" height="325"/>
    </item>
    <item>
      <title>Audio classification with Keras: Looking closer at the non-deep learning parts</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-02-07-audio-background</link>
      <description>


&lt;p&gt;About half a year ago, this blog featured a post, written by Daniel Falbel, on how to use Keras to classify pieces of spoken language. The article got a lot of attention and not surprisingly, questions arose how to apply that code to different datasets. We’ll take this as a motivation to explore in more depth the preprocessing done in that post: If we know why the input to the network looks the way it looks, we will be able to modify the model specification appropriately if need be.&lt;/p&gt;
&lt;p&gt;In case you have a background in speech recognition, or even general signal processing, for you the introductory part of this post will probably not contain much news. However, you might still be interested in the code part, which shows how to do things like creating spectrograms with current versions of TensorFlow.&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; If you don’t have that background, we’re inviting you on a (hopefully) fascinating journey, slightly touching on one of the greater mysteries of this universe.&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We’ll use the same dataset as Daniel did in his post, that is, &lt;a href="https://storage.cloud.google.com/download.tensorflow.org/data/speech_commands_v0.01.tar.gz"&gt;version 1 of the Google speech commands dataset&lt;/a&gt;&lt;span class="citation"&gt;(Warden 2018)&lt;/span&gt; The dataset consists of ~ 65,000 WAV files, of length one second or less. Each file is a recording of one of thirty words, uttered by different speakers.&lt;/p&gt;
&lt;p&gt;The goal then is to train a network to discriminate between spoken words. How should the input to the network look? The WAV files contain amplitudes of sound waves over time. Here are a few examples, corresponding to the words &lt;em&gt;bird&lt;/em&gt;, &lt;em&gt;down&lt;/em&gt;, &lt;em&gt;sheila&lt;/em&gt;, and &lt;em&gt;visual&lt;/em&gt;:&lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-02-07-audio-background/images/waves2.png" /&gt;&lt;/p&gt;
&lt;h1 id="time-domain-and-frequency-domain"&gt;Time domain and frequency domain&lt;/h1&gt;
&lt;p&gt;A sound wave is a signal extending in &lt;em&gt;time&lt;/em&gt;, analogously to how what enters our visual system extends in &lt;em&gt;space&lt;/em&gt;. At each point in time, the current signal is dependent on its past. The obvious architecture to use in modeling it thus seems to be a recurrent neural network.&lt;/p&gt;
&lt;p&gt;However, the information contained in the sound wave can be represented in an alternative way: namely, using the &lt;em&gt;frequencies&lt;/em&gt; that make up the signal.&lt;/p&gt;
&lt;p&gt;Here we see a sound wave (top) and its frequency representation (bottom).&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-02-07-audio-background/images/sin8_16_32_64_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;In the time representation (referred to as the &lt;em&gt;time domain&lt;/em&gt;), the signal is composed of consecutive amplitudes over time. In the frequency domain, it is represented as magnitudes of different frequencies. It may appear as one of the greatest mysteries in this world that you can convert between those two without loss of information, that is: Both representations are essentially equivalent!&lt;/p&gt;
&lt;p&gt;Conversion from the time domain to the frequency domain is done using the &lt;em&gt;Fourier transform&lt;/em&gt;; to convert back, the &lt;em&gt;Inverse Fourier Transform&lt;/em&gt; is used. There exist different types of Fourier transforms depending on whether time is viewed as continuous or discrete, and whether the signal itself is continuous or discrete. In the “real world”, where usually for us, real means virtual as we’re working with digitized signals, the time domain as well as the signal are represented as discrete and so, the &lt;em&gt;Discrete Fourier Transform&lt;/em&gt; (DFT) is used. The DFT itself is computed using the FFT (&lt;em&gt;Fast Fourier Transform&lt;/em&gt;) algorithm, resulting in significant speedup over a naive implementation.&lt;/p&gt;
&lt;p&gt;Looking back at the above example sound wave, it is a compound of four sine waves, of frequencies 8Hz, 16Hz, 32Hz, and 64Hz, whose amplitudes are added and displayed over time. The compound wave here is assumed to extend infinitely in time. Unlike speech, which changes over time, it can be characterized by a single enumeration of the magnitudes of the frequencies it is composed of. So here the &lt;em&gt;spectrogram&lt;/em&gt;, the characterization of a signal by magnitudes of constituent frequencies varying over time, looks essentially one-dimensional.&lt;/p&gt;
&lt;p&gt;However, when we ask &lt;em&gt;Praat&lt;/em&gt; to create a spectrogram of one of our example sounds (a &lt;em&gt;seven&lt;/em&gt;), it could look like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-02-07-audio-background/images/seven2.png" /&gt;&lt;/p&gt;
&lt;p&gt;Here we see a two-dimensional &lt;em&gt;image&lt;/em&gt; of frequency magnitudes over time (higher magnitudes indicated by darker coloring). This two-dimensional representation may be fed to a network, in place of the one-dimensional amplitudes. Accordingly, if we decide to do so we’ll use a convnet instead of an RNN.&lt;/p&gt;
&lt;p&gt;Spectrograms will look different depending on how we create them. We’ll take a look at the essential options in a minute. First though, let’s see what we &lt;em&gt;can’t&lt;/em&gt; always do: ask for &lt;em&gt;all&lt;/em&gt; frequencies that were contained in the analog signal.&lt;a href="#fn4" class="footnote-ref" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="sampling"&gt;Sampling&lt;/h1&gt;
&lt;p&gt;Above, we said that both representations, &lt;em&gt;time domain&lt;/em&gt; and &lt;em&gt;frequency domain&lt;/em&gt;, were essentially equivalent. In our virtual real world, this is only true if the signal we’re working with has been digitized correctly, or as this is commonly phrased, if it has been “properly sampled”.&lt;/p&gt;
&lt;p&gt;Take speech as an example: As an analog signal, speech per se is continuous in time; for us to be able to work with it on a computer, it needs to be converted to happen in discrete time. This conversion of the independent variable (time in our case, space in e.g. image processing) from continuous to discrete is called &lt;em&gt;sampling&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In this process of discretization, a crucial decision to be made is the &lt;em&gt;sampling rate&lt;/em&gt; to use. The sampling rate has to be at least double the highest frequency in the signal. If it’s not, loss of information will occur. The way this is most often put is the other way round: To preserve all information, the analog signal may not contain frequencies above one-half the sampling rate. This frequency - half the sampling rate - is called the &lt;em&gt;Nyquist rate&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;If the sampling rate is too low, &lt;em&gt;aliasing&lt;/em&gt; takes place: Higher frequencies &lt;em&gt;alias&lt;/em&gt; themselves as lower frequencies. This means that not only can’t we get them, they also corrupt the magnitudes of corresponding lower frequencies they are being added to. Here’s a schematic example of how a high-frequency signal could alias itself as being lower-frequency. Imagine the high-frequency wave being sampled at integer points (grey circles) only:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-02-07-audio-background/images/aliasing.png" /&gt;&lt;/p&gt;
&lt;p&gt;In the case of the speech commands dataset, all sound waves have been sampled at 16 kHz. This means that when we ask &lt;em&gt;Praat&lt;/em&gt; for a spectogram, we should not ask for frequencies higher than 8kHz. Here is what happens if we ask for frequencies up to 16kHz instead - we just don’t get them:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-02-07-audio-background/images/seven_16000_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;Now let’s see what options we &lt;em&gt;do&lt;/em&gt; have when creating spectrograms.&lt;/p&gt;
&lt;h1 id="spectrogram-creation-options"&gt;Spectrogram creation options&lt;/h1&gt;
&lt;p&gt;In the above simple sine wave example, the signal stayed constant over time. However in speech utterances, the magnitudes of constituent frequencies change over time. Ideally thus, we’d have an exact frequency representation for every point in time. As an approximation to this ideal, the signal is divided into overlapping windows, and the Fourier transform is computed for each time slice separately. This is called the &lt;em&gt;Short Time Fourier Transform&lt;/em&gt; (STFT).&lt;/p&gt;
&lt;p&gt;When we compute the spectrogram via the STFT, we need to tell it what size windows to use, and how big to make the overlap. The longer the windows we use, the better the resolution we get in the frequency domain. However, what we gain in resolution there, we lose in the time domain, as we’ll have fewer windows representing the signal. This is a general principle in signal processing: Resolution in the time and frequency domains are inversely related.&lt;/p&gt;
&lt;p&gt;To make this more concrete, let’s again look at a simple example. Here is the spectrogram of a synthetic sine wave, composed of two components at 1000 Hz and 1200 Hz. The window length was left at its (&lt;em&gt;Praat&lt;/em&gt;) default, 5 milliseconds:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-02-07-audio-background/images/bandwidth_1_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;We see that with a short window like that, the two different frequencies are mangled into one in the spectrogram. Now enlarge the window to 30 milliseconds, and they are clearly differentiated:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-02-07-audio-background/images/bandwidth_2_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;The above spectrogram of the word “seven” was produced using Praats default of 5 milliseconds. What happens if we use 30 milliseconds instead?&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-02-07-audio-background/images/seven_30_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;We get better frequency resolution, but at the price of lower resolution in the time domain. The window length used during preprocessing is a parameter we might want to experiment with later, when training a network.&lt;/p&gt;
&lt;p&gt;Another input to the STFT to play with is the type of window used to weight the samples in a time slice. Here again are three spectrograms of the above recording of &lt;em&gt;seven&lt;/em&gt;, using, respectively, a Hamming, a Hann, and a Gaussian window:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-02-07-audio-background/images/windows2.png" /&gt;&lt;/p&gt;
&lt;p&gt;While the spectrograms using the Hann and Gaussian windows don’t look much different, the Hamming window seems to have introduced some artifacts.&lt;/p&gt;
&lt;h1 id="beyond-the-spectrogram-mel-scale-and-mel-frequency-cepstral-coefficients-mfccs"&gt;Beyond the spectrogram: Mel scale and Mel-Frequency Cepstral Coefficients (MFCCs)&lt;/h1&gt;
&lt;p&gt;Preprocessing options don’t end with the spectrogram. A popular transformation applied to the spectrogram is conversion to &lt;em&gt;mel scale&lt;/em&gt;, a scale based on how humans actually perceive differences in pitch. We don’t elaborate further on this here,&lt;a href="#fn5" class="footnote-ref" id="fnref5"&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; but we do briefly comment on the respective TensorFlow code below, in case you’d like to experiment with this. In the past, coefficients transformed to Mel scale have sometimes been further processed to obtain the so-called Mel-Frequency Cepstral Coefficients (MFCCs). Again, we just show the code. For excellent reading on Mel scale conversion and MFCCs (including the reason why MFCCs are less often used nowadays) see &lt;a href="https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html"&gt;this post&lt;/a&gt; by Haytham Fayek.&lt;/p&gt;
&lt;p&gt;Back to our original task of speech classification. Now that we’ve gained a bit of insight in what is involved, let’s see how to perform these transformations in TensorFlow.&lt;/p&gt;
&lt;h1 id="preprocessing-for-audio-classification-using-tensorflow"&gt;Preprocessing for audio classification using TensorFlow&lt;/h1&gt;
&lt;p&gt;Code will be represented in snippets according to the functionality it provides, so we may directly map it to what was explained conceptually above. A complete example is available &lt;a href="https://github.com/skeydan/audio_classification/blob/master/audio_classification_tf.R"&gt;here&lt;/a&gt;. The complete example builds on Daniel’s &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-06-06-simple-audio-classification-keras/"&gt;original code&lt;/a&gt; as much as possible,&lt;a href="#fn6" class="footnote-ref" id="fnref6"&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; with two exceptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The code runs in eager as well as in static graph mode. If you decide you only ever need eager mode, there are a few places that can be simplified. This is partly related to the fact that in eager mode, TensorFlow operations in place of tensors return values, which we can directly pass on to TensorFlow functions expecting values, not tensors. In addition, less conversion code is needed when manipulating intermediate values in R.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;With TensorFlow 1.13 being released any day, and preparations for TF 2.0 running at full speed, we want the code to necessitate as few modifications as possible to run on the next major version of TF. One big difference is that there will no longer be a &lt;code&gt;contrib&lt;/code&gt; module. In the original post, &lt;code&gt;contrib&lt;/code&gt; was used to read in the &lt;code&gt;.wav&lt;/code&gt; files as well as compute the spectrograms. Here, we will use functionality from &lt;code&gt;tf.audio&lt;/code&gt; and &lt;code&gt;tf.signal&lt;/code&gt; instead.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All operations shown below will run inside &lt;code&gt;tf.dataset&lt;/code&gt; code, which on the R side is accomplished using the &lt;code&gt;tfdatasets&lt;/code&gt; package. To explain the individual operations, we look at a single file, but later we’ll also display the data generator as a whole.&lt;/p&gt;
&lt;p&gt;For stepping through individual lines, it’s always helpful to have eager mode enabled, independently of whether ultimately we’ll execute in eager or graph mode:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
use_implementation(&amp;quot;tensorflow&amp;quot;)

library(tensorflow)
tfe_enable_eager_execution(device_policy = &amp;quot;silent&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We pick a random &lt;code&gt;.wav&lt;/code&gt; file and decode it using &lt;code&gt;tf$audio$decode_wav&lt;/code&gt;.&lt;a href="#fn7" class="footnote-ref" id="fnref7"&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;This will give us access to two tensors: the samples themselves, and the sampling rate.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;fname &amp;lt;- &amp;quot;data/speech_commands_v0.01/bird/00b01445_nohash_0.wav&amp;quot;
wav &amp;lt;- tf$audio$decode_wav(tf$read_file(fname))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;wav$sample_rate&lt;/code&gt; contains the sampling rate. As expected, it is 16000, or 16kHz:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;sampling_rate &amp;lt;- wav$sample_rate %&amp;gt;% as.numeric()
sampling_rate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;16000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The samples themselves are accessible as &lt;code&gt;wav$audio&lt;/code&gt;, but their shape is (16000, 1), so we have to transpose the tensor to get the usual (&lt;em&gt;batch_size&lt;/em&gt;, &lt;em&gt;number of samples&lt;/em&gt;) format we need for further processing.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;samples &amp;lt;- wav$audio
samples &amp;lt;- samples %&amp;gt;% tf$transpose(perm = c(1L, 0L))
samples&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor(
[[-0.00750732  0.04653931  0.02041626 ... -0.01004028 -0.01300049
  -0.00250244]], shape=(1, 16000), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="computing-the-spectogram"&gt;Computing the spectogram&lt;/h4&gt;
&lt;p&gt;To compute the spectrogram, we use &lt;code&gt;tf$signal$stft&lt;/code&gt; (where &lt;em&gt;stft&lt;/em&gt; stands for &lt;em&gt;Short Time Fourier Transform&lt;/em&gt;). &lt;code&gt;stft&lt;/code&gt; expects three non-default arguments: Besides the input signal itself, there are the window size, &lt;code&gt;frame_length&lt;/code&gt;, and the stride to use when determining the overlapping windows, &lt;code&gt;frame_step&lt;/code&gt;. Both are expressed in units of &lt;code&gt;number of samples&lt;/code&gt;. So if we decide on a window length of 30 milliseconds and a stride of 10 milliseconds …&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;window_size_ms &amp;lt;- 30
window_stride_ms &amp;lt;- 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;… we arrive at the following call:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;samples_per_window &amp;lt;- sampling_rate * window_size_ms/1000 
stride_samples &amp;lt;-  sampling_rate * window_stride_ms/1000 

stft_out &amp;lt;- tf$signal$stft(
  samples,
  frame_length = as.integer(samples_per_window),
  frame_step = as.integer(stride_samples)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inspecting the tensor we got back, &lt;code&gt;stft_out&lt;/code&gt;, we see, for our single input wave, a matrix of 98 x 257 complex values:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor(
[[[ 1.03279948e-04+0.00000000e+00j -1.95371482e-04-6.41121820e-04j
   -1.60833192e-03+4.97534114e-04j ... -3.61620914e-05-1.07343149e-04j
   -2.82576875e-05-5.88812982e-05j  2.66879797e-05+0.00000000e+00j] 
   ... 
   ]],
shape=(1, 98, 257), dtype=complex64)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here 98 is the number of periods, which we can compute in advance, based on the number of samples in a window and the size of the stride:&lt;a href="#fn8" class="footnote-ref" id="fnref8"&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;n_periods &amp;lt;- length(seq(samples_per_window/2, sampling_rate - samples_per_window/2, stride_samples))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;257 is the number of frequencies we obtained magnitudes for. By default, &lt;code&gt;stft&lt;/code&gt; will apply a Fast Fourier Transform of size &lt;em&gt;smallest power of 2 greater or equal to the number of samples in a window&lt;/em&gt;,&lt;a href="#fn9" class="footnote-ref" id="fnref9"&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt; and then return the &lt;em&gt;fft_length / 2 + 1&lt;/em&gt; unique components of the FFT: the zero-frequency term and the positive-frequency terms.&lt;a href="#fn10" class="footnote-ref" id="fnref10"&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In our case, the number of samples in a window is 480. The nearest enclosing power of 2 being 512, we end up with 512/2 + 1 = 257 coefficients. This too we can compute in advance:&lt;a href="#fn11" class="footnote-ref" id="fnref11"&gt;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;fft_size &amp;lt;- as.integer(2^trunc(log(samples_per_window, 2)) + 1) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Back to the output of the STFT. Taking the elementwise magnitude of the complex values, we obtain an energy spectrogram:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;magnitude_spectrograms &amp;lt;- tf$abs(stft_out)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we stop preprocessing here, we will usually want to log transform the values to better match the sensitivity of the human auditory system:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;log_magnitude_spectrograms = tf$log(magnitude_spectrograms + 1e-6)&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="mel-spectrograms-and-mel-frequency-cepstral-coefficients-mfccs"&gt;Mel spectrograms and Mel-Frequency Cepstral Coefficients (MFCCs)&lt;/h4&gt;
&lt;p&gt;If instead we choose to use Mel spectrograms, we can obtain a transformation matrix that will convert the original spectrograms to Mel scale:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;lower_edge_hertz &amp;lt;- 0
upper_edge_hertz &amp;lt;- 2595 * log10(1 + (sampling_rate/2)/700)
num_mel_bins &amp;lt;- 64L
num_spectrogram_bins &amp;lt;- magnitude_spectrograms$shape[-1]$value

linear_to_mel_weight_matrix &amp;lt;- tf$signal$linear_to_mel_weight_matrix(
  num_mel_bins,
  num_spectrogram_bins,
  sampling_rate,
  lower_edge_hertz,
  upper_edge_hertz
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Applying that matrix, we obtain a tensor of size &lt;em&gt;(batch_size, number of periods, number of Mel coefficients)&lt;/em&gt; which again, we can log-compress if we want:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;mel_spectrograms &amp;lt;- tf$tensordot(magnitude_spectrograms, linear_to_mel_weight_matrix, 1L)
log_mel_spectrograms &amp;lt;- tf$log(mel_spectrograms + 1e-6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just for completeness’ sake, finally we show the TensorFlow code used to further compute MFCCs. We don’t include this in the complete example as with MFCCs, we would need a different network architecture.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;num_mfccs &amp;lt;- 13
mfccs &amp;lt;- tf$signal$mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[, , 1:num_mfccs]&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="accommodating-different-length-inputs"&gt;Accommodating different-length inputs&lt;/h4&gt;
&lt;p&gt;In our complete example, we determine the sampling rate from the first file read, thus assuming all recordings have been sampled at the same rate. We do allow for different lengths though. For example in our dataset, had we used this file, just 0.65 seconds long, for demonstration purposes:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;fname &amp;lt;- &amp;quot;data/speech_commands_v0.01/bird/1746d7b6_nohash_0.wav&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we’d have ended up with just 63 periods in the spectrogram. As we have to define a fixed &lt;code&gt;input_size&lt;/code&gt; for the first conv layer, we need to pad the corresponding dimension to the maximum possible length, which is &lt;code&gt;n_periods&lt;/code&gt; computed above. The padding actually takes place as part of dataset definition. Let’s quickly see dataset definition as a whole, leaving out the possible generation of Mel spectrograms.&lt;a href="#fn12" class="footnote-ref" id="fnref12"&gt;&lt;sup&gt;12&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;data_generator &amp;lt;- function(df,
                           window_size_ms,
                           window_stride_ms) {
  
  # assume sampling rate is the same in all samples
  sampling_rate &amp;lt;-
    tf$audio$decode_wav(tf$read_file(tf$reshape(df$fname[[1]], list()))) %&amp;gt;% .$sample_rate
  
  samples_per_window &amp;lt;- (sampling_rate * window_size_ms) %/% 1000L  
  stride_samples &amp;lt;-  (sampling_rate * window_stride_ms) %/% 1000L   
  
  n_periods &amp;lt;-
    tf$shape(
      tf$range(
        samples_per_window %/% 2L,
        16000L - samples_per_window %/% 2L,
        stride_samples
      )
    )[1] + 1L
  
  n_fft_coefs &amp;lt;-
    (2 ^ tf$ceil(tf$log(
      tf$cast(samples_per_window, tf$float32)
    ) / tf$log(2)) /
      2 + 1L) %&amp;gt;% tf$cast(tf$int32)
  
  ds &amp;lt;- tensor_slices_dataset(df) %&amp;gt;%
    dataset_shuffle(buffer_size = buffer_size)
  
  ds &amp;lt;- ds %&amp;gt;%
    dataset_map(function(obs) {
      wav &amp;lt;-
        tf$audio$decode_wav(tf$read_file(tf$reshape(obs$fname, list())))
      samples &amp;lt;- wav$audio
      samples &amp;lt;- samples %&amp;gt;% tf$transpose(perm = c(1L, 0L))
      
      stft_out &amp;lt;- tf$signal$stft(samples,
                                 frame_length = samples_per_window,
                                 frame_step = stride_samples)
      
      magnitude_spectrograms &amp;lt;- tf$abs(stft_out)
      log_magnitude_spectrograms &amp;lt;- tf$log(magnitude_spectrograms + 1e-6)
      
      response &amp;lt;- tf$one_hot(obs$class_id, 30L)

      input &amp;lt;- tf$transpose(log_magnitude_spectrograms, perm = c(1L, 2L, 0L))
      list(input, response)
    })
  
  ds &amp;lt;- ds %&amp;gt;%
    dataset_repeat()
  
  ds %&amp;gt;%
    dataset_padded_batch(
      batch_size = batch_size,
      padded_shapes = list(tf$stack(list(
        n_periods, n_fft_coefs,-1L
      )),
      tf$constant(-1L, shape = shape(1L))),
      drop_remainder = TRUE
    )
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The logic is the same as described above, only the code has been generalized to work in eager as well as graph mode. The padding is taken care of by &lt;em&gt;dataset_padded_batch()&lt;/em&gt;, which needs to be told the maximum number of periods and the maximum number of coefficients.&lt;/p&gt;
&lt;h4 id="time-for-experimentation"&gt;Time for experimentation&lt;/h4&gt;
&lt;p&gt;Building on the &lt;a href="https://github.com/skeydan/audio_classification/blob/master/audio_classification_tf.R"&gt;complete example&lt;/a&gt;, now is the time for experimentation: How do different window sizes affect classification accuracy? Does transformation to the mel scale yield improved results?&lt;a href="#fn13" class="footnote-ref" id="fnref13"&gt;&lt;sup&gt;13&lt;/sup&gt;&lt;/a&gt; You might also want to try passing a non-default &lt;code&gt;window_fn&lt;/code&gt; to &lt;code&gt;stft&lt;/code&gt; (the default being the Hann window) and see how that affects the results. And of course, the straightforward definition of the network leaves a lot of room for improvement.&lt;/p&gt;
&lt;h1 id="wrapping-up"&gt;Wrapping up&lt;/h1&gt;
&lt;p&gt;Speaking of the network: Now that we’ve gained more insight into what is contained in a spectrogram, we might start asking, is a convnet really an adequate solution here? Normally we use convnets on images: two-dimensional data where both dimensions represent the same kind of information. Thus with images, it is natural to have square filter kernels. In a spectrogram though, the time axis and the frequency axis represent fundamentally different types of information, and it is not clear at all that we should treat them equally. Also, whereas in images, the translation invariance of convnets is a desired feature, this is not the case for the frequency axis in a spectrogram.&lt;/p&gt;
&lt;p&gt;Closing the circle, we discover that due to deeper knowledge about the subject domain, we are in a better position to reason about (hopefully) successful network architectures. We leave it to the creativity of our readers to continue the search…&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-speechcommandsv2"&gt;
&lt;p&gt;Warden, P. 2018. “Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.” &lt;em&gt;ArXiv E-Prints&lt;/em&gt;, April. &lt;a href="https://arxiv.org/abs/1804.03209"&gt;https://arxiv.org/abs/1804.03209&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;As well as TensorFlow 2.0, more or less.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;Referring to the Fourier Transform. To cite an authority for this characterization, it is e.g. found in Brad Osgood’s lecture on &lt;a href="https://www.youtube.com/playlist?list=PLB24BC7956EE040CD"&gt;The Fourier Transform and its Applications&lt;/a&gt;.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;To display these sound waves, and later on to create spectrograms, we use &lt;a href="http://www.fon.hum.uva.nl/praat/"&gt;Praat&lt;/a&gt;, a speech analysis and synthesis program that has a &lt;em&gt;lot&lt;/em&gt; more functionality than what we’re making use of here.&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;In practice, working with datasets created for speech analysis, there will be no problems due to low sampling rates (the topic we talk about below). However, the topic is too essential - and interesting! - to skip over in an introductory post like this one.)&lt;a href="#fnref4" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn5"&gt;&lt;p&gt;Cf. &lt;a href="https://en.wikipedia.org/wiki/Mel_scale"&gt;discussions about the validity of the original experiments&lt;/a&gt;.&lt;a href="#fnref5" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn6"&gt;&lt;p&gt;In particular, we’re leaving the convnet and training code itself nearly unchanged.&lt;a href="#fnref6" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn7"&gt;&lt;p&gt;As of this writing, &lt;code&gt;tf.audio&lt;/code&gt; is only available in the TensorFlow nightly builds. If the &lt;code&gt;decode_wav&lt;/code&gt; line fails, simply replace &lt;code&gt;tf$audio&lt;/code&gt; by &lt;code&gt;tf$contrib$framework$python$ops$audio_ops&lt;/code&gt;.&lt;a href="#fnref7" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn8"&gt;&lt;p&gt;This code, taken from the original post, is applicable when executing eagerly or when working “on the R side”. The complete example, which dynamically determines the sampling rate and performs all operations so they work inside a static TensorFlow graph, has a more intimidating-looking equivalent that essentially does the same thing.&lt;a href="#fnref8" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn9"&gt;&lt;p&gt;In the former case, the samples dimension in the time domain will be padded with zeros.&lt;a href="#fnref9" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn10"&gt;&lt;p&gt;For real signals, the negative-frequency terms are redundant.&lt;a href="#fnref10" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn11"&gt;&lt;p&gt;Again, the code in the full example looks a bit more involved because it is supposed to be runnable on a static TensorFlow graph.&lt;a href="#fnref11" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn12"&gt;&lt;p&gt;Which is contained in the complete example code, though.&lt;a href="#fnref12" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn13"&gt;&lt;p&gt;For us, it didn’t.&lt;a href="#fnref13" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">dd166727f8728196d45d96cd44520caf</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Concepts</category>
      <category>Audio Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-02-07-audio-background</guid>
      <pubDate>Thu, 07 Feb 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-02-07-audio-background/images/seven2.png" medium="image" type="image/png" width="1714" height="846"/>
    </item>
    <item>
      <title>Discrete Representation Learning with VQ-VAE and TensorFlow Probability</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-01-24-vq-vae</link>
      <description>


&lt;p&gt;About two weeks ago, we &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-01-08-getting-started-with-tf-probability/"&gt;introduced TensorFlow Probability (TFP)&lt;/a&gt;, showing how to create and sample from &lt;em&gt;distributions&lt;/em&gt; and put them to use in a Variational Autoencoder (VAE) that learns its prior. Today, we move on to a different specimen in the VAE model zoo: the Vector Quantised Variational Autoencoder (VQ-VAE) described in &lt;em&gt;Neural Discrete Representation Learning&lt;/em&gt; &lt;span class="citation"&gt;(Oord, Vinyals, and Kavukcuoglu 2017)&lt;/span&gt;. This model differs from most VAEs in that its approximate posterior is not continuous, but discrete - hence the “quantised” in the article’s title. We’ll quickly look at what this means, and then dive directly into the code, combining Keras layers, eager execution, and TFP.&lt;/p&gt;
&lt;h1 id="discrete-codes"&gt;Discrete codes&lt;/h1&gt;
&lt;p&gt;Many phenomena are best thought of, and modeled, as discrete. This holds for phonemes and lexemes in language, higher-level structures in images (think objects instead of pixels),and tasks that necessitate reasoning and planning. The latent code used in most VAEs, however, is continuous - usually it’s a multivariate Gaussian. Continuous-space VAEs have been found very successful in reconstructing their input, but often they suffer from something called &lt;em&gt;posterior collapse&lt;/em&gt;: The decoder is so powerful that it may create realistic output given just &lt;em&gt;any&lt;/em&gt; input. This means there is no incentive to learn an expressive latent space.&lt;/p&gt;
&lt;p&gt;In VQ-VAE, however, each input sample gets mapped deterministically to one of a set of &lt;em&gt;embedding vectors&lt;/em&gt;.&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; Together, these embedding vectors constitute the prior for the latent space. As such, an embedding vector contains a lot more information than a mean and a variance, and thus, is much harder to ignore by the decoder.&lt;/p&gt;
&lt;p&gt;The question then is: Where is that magical hat, for us to pull out meaningful embeddings?&lt;/p&gt;
&lt;h1 id="learning-a-discrete-embedding-space"&gt;Learning a discrete embedding space&lt;/h1&gt;
&lt;p&gt;From the above conceptual description, we now have two questions to answer. First, by what mechanism do we assign input samples (that went through the encoder) to appropriate embedding vectors? And second: How can we learn embedding vectors that actually are useful representations - that when fed to a decoder, will result in entities perceived as belonging to the same species?&lt;/p&gt;
&lt;p&gt;As regards assignment, a tensor emitted from the encoder is simply mapped to its nearest neighbor in embedding space, using Euclidean distance. The embedding vectors are then updated using exponential moving averages.&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; As we’ll see soon, this means that they are actually not being learned using gradient descent - a feature worth pointing out as we don’t come across it every day in deep learning.&lt;/p&gt;
&lt;p&gt;Concretely, how then should the loss function and training process look? This will probably easiest be seen in code.&lt;/p&gt;
&lt;h1 id="coding-the-vq-vae"&gt;Coding the VQ-VAE&lt;/h1&gt;
&lt;p&gt;The complete code for this example, including utilities for model saving and image visualization, is &lt;a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/vq_vae.R"&gt;available on github&lt;/a&gt; as part of the Keras examples. Order of presentation here may differ from actual execution order for expository purposes, so please to actually run the code consider making use of the example on github.&lt;/p&gt;
&lt;h1 id="setup-and-data-loading"&gt;Setup and data loading&lt;/h1&gt;
&lt;p&gt;As in all our prior posts on VAEs, we use eager execution, which presupposes the TensorFlow implementation of Keras.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
use_implementation(&amp;quot;tensorflow&amp;quot;)
library(tensorflow)
tfe_enable_eager_execution(device_policy = &amp;quot;silent&amp;quot;)

tfp &amp;lt;- import(&amp;quot;tensorflow_probability&amp;quot;)
tfd &amp;lt;- tfp$distributions

library(tfdatasets)
library(dplyr)
library(glue)
# used for set_defaults; please get the development version:
# devtools::install_github(&amp;quot;thomasp85/curry&amp;quot;)
library(curry) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As in our previous post on doing VAE with TFP, we’ll use &lt;a href="https://github.com/rois-codh/kmnist"&gt;Kuzushiji-MNIST&lt;/a&gt;&lt;span class="citation"&gt;(Clanuwat et al. 2018)&lt;/span&gt; as input. Now is the time to look at &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2019-01-08-getting-started-with-tf-probability/images/results.png"&gt;what we ended up generating that time&lt;/a&gt; and place your bet: How will that compare against the discrete latent space of VQ-VAE?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;np &amp;lt;- import(&amp;quot;numpy&amp;quot;)
 
kuzushiji &amp;lt;- np$load(&amp;quot;kmnist-train-imgs.npz&amp;quot;)
kuzushiji &amp;lt;- kuzushiji$get(&amp;quot;arr_0&amp;quot;)

train_images &amp;lt;- kuzushiji %&amp;gt;%
  k_expand_dims() %&amp;gt;%
  k_cast(dtype = &amp;quot;float32&amp;quot;)

train_images &amp;lt;- train_images %&amp;gt;% `/`(255)

buffer_size &amp;lt;- 60000
batch_size &amp;lt;- 64
num_examples_to_generate &amp;lt;- batch_size

batches_per_epoch &amp;lt;- buffer_size / batch_size

train_dataset &amp;lt;- tensor_slices_dataset(train_images) %&amp;gt;%
  dataset_shuffle(buffer_size) %&amp;gt;%
  dataset_batch(batch_size, drop_remainder = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="hyperparameters"&gt;Hyperparameters&lt;/h2&gt;
&lt;p&gt;In addition to the “usual” hyperparameters we have in deep learning, the VQ-VAE infrastructure introduces a few model-specific ones. First of all, the embedding space is of dimensionality &lt;em&gt;number of embedding vectors&lt;/em&gt; times &lt;em&gt;embedding vector size&lt;/em&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# number of embedding vectors
num_codes &amp;lt;- 64L
# dimensionality of the embedding vectors
code_size &amp;lt;- 16L&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The latent space in our example will be of size one, that is, we have a single embedding vector representing the latent code for each input sample. This will be fine for our dataset, but it should be noted that van den Oord et al. used far higher-dimensional latent spaces on e.g. ImageNet and Cifar-10.&lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;latent_size &amp;lt;- 1&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="encoder-model"&gt;Encoder model&lt;/h2&gt;
&lt;p&gt;The encoder uses convolutional layers to extract image features. Its output is a 3-d tensor of shape &lt;em&gt;batchsize&lt;/em&gt; * 1 * &lt;em&gt;code_size&lt;/em&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;activation &amp;lt;- &amp;quot;elu&amp;quot;
# modularizing the code just a little bit
default_conv &amp;lt;- set_defaults(layer_conv_2d, list(padding = &amp;quot;same&amp;quot;, activation = activation))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;base_depth &amp;lt;- 32

encoder_model &amp;lt;- function(name = NULL,
                          code_size) {
  
  keras_model_custom(name = name, function(self) {
    
    self$conv1 &amp;lt;- default_conv(filters = base_depth, kernel_size = 5)
    self$conv2 &amp;lt;- default_conv(filters = base_depth, kernel_size = 5, strides = 2)
    self$conv3 &amp;lt;- default_conv(filters = 2 * base_depth, kernel_size = 5)
    self$conv4 &amp;lt;- default_conv(filters = 2 * base_depth, kernel_size = 5, strides = 2)
    self$conv5 &amp;lt;- default_conv(filters = 4 * latent_size, kernel_size = 7, padding = &amp;quot;valid&amp;quot;)
    self$flatten &amp;lt;- layer_flatten()
    self$dense &amp;lt;- layer_dense(units = latent_size * code_size)
    self$reshape &amp;lt;- layer_reshape(target_shape = c(latent_size, code_size))
    
    function (x, mask = NULL) {
      x %&amp;gt;% 
        # output shape:  7 28 28 32 
        self$conv1() %&amp;gt;% 
        # output shape:  7 14 14 32 
        self$conv2() %&amp;gt;% 
        # output shape:  7 14 14 64 
        self$conv3() %&amp;gt;% 
        # output shape:  7 7 7 64 
        self$conv4() %&amp;gt;% 
        # output shape:  7 1 1 4 
        self$conv5() %&amp;gt;% 
        # output shape:  7 4 
        self$flatten() %&amp;gt;% 
        # output shape:  7 16 
        self$dense() %&amp;gt;% 
        # output shape:  7 1 16
        self$reshape()
    }
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As always, let’s make use of the fact that we’re using eager execution, and see a few example outputs.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;iter &amp;lt;- make_iterator_one_shot(train_dataset)
batch &amp;lt;-  iterator_get_next(iter)

encoder &amp;lt;- encoder_model(code_size = code_size)
encoded  &amp;lt;- encoder(batch)
encoded&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor(
[[[ 0.00516277 -0.00746826  0.0268365  ... -0.012577   -0.07752544
   -0.02947626]]
...

 [[-0.04757921 -0.07282603 -0.06814402 ... -0.10861694 -0.01237121
    0.11455103]]], shape=(64, 1, 16), dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, each of these 16d vectors needs to be mapped to the embedding vector it is closest to. This mapping is taken care of by another model: &lt;code&gt;vector_quantizer&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="vector-quantizer-model"&gt;Vector quantizer model&lt;/h2&gt;
&lt;p&gt;This is how we will instantiate the vector quantizer:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;vector_quantizer &amp;lt;- vector_quantizer_model(num_codes = num_codes, code_size = code_size)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This model serves two purposes: First, it acts as a store for the embedding vectors. Second, it matches encoder output to available embeddings.&lt;/p&gt;
&lt;p&gt;Here, the current state of embeddings is stored in &lt;code&gt;codebook&lt;/code&gt;. &lt;code&gt;ema_means&lt;/code&gt; and &lt;code&gt;ema_count&lt;/code&gt; are for bookkeeping purposes only (note how they are set to be non-trainable). We’ll see them in use shortly.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;vector_quantizer_model &amp;lt;- function(name = NULL, num_codes, code_size) {
  
    keras_model_custom(name = name, function(self) {
      
      self$num_codes &amp;lt;- num_codes
      self$code_size &amp;lt;- code_size
      self$codebook &amp;lt;- tf$get_variable(
        &amp;quot;codebook&amp;quot;,
        shape = c(num_codes, code_size), 
        dtype = tf$float32
        )
      self$ema_count &amp;lt;- tf$get_variable(
        name = &amp;quot;ema_count&amp;quot;, shape = c(num_codes),
        initializer = tf$constant_initializer(0),
        trainable = FALSE
        )
      self$ema_means = tf$get_variable(
        name = &amp;quot;ema_means&amp;quot;,
        initializer = self$codebook$initialized_value(),
        trainable = FALSE
        )
      
      function (x, mask = NULL) { 
        
        # to be filled in shortly ...
        
      }
    })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In addition to the actual embeddings, in its &lt;code&gt;call&lt;/code&gt; method &lt;code&gt;vector_quantizer&lt;/code&gt; holds the assignment logic. First, we compute the Euclidean distance of each encoding to the vectors in the codebook (&lt;code&gt;tf$norm&lt;/code&gt;). We assign each encoding to the closest as by that distance embedding (&lt;code&gt;tf$argmin&lt;/code&gt;) and one-hot-encode the assignments (&lt;code&gt;tf$one_hot&lt;/code&gt;). Finally, we isolate the corresponding vector by masking out all others and summing up what’s left over (multiplication followed by &lt;code&gt;tf$reduce_sum&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Regarding the &lt;code&gt;axis&lt;/code&gt; argument used with many TensorFlow functions, please take into consideration that in contrast to their &lt;code&gt;k_*&lt;/code&gt; siblings, raw TensorFlow (&lt;code&gt;tf$*&lt;/code&gt;) functions expect axis numbering to be 0-based. We also have to add the &lt;code&gt;L&lt;/code&gt;’s after the numbers to conform to TensorFlow’s datatype requirements.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;vector_quantizer_model &amp;lt;- function(name = NULL, num_codes, code_size) {
  
    keras_model_custom(name = name, function(self) {
      
      # here we have the above instance fields
      
      function (x, mask = NULL) {
    
        # shape: bs * 1 * num_codes
         distances &amp;lt;- tf$norm(
          tf$expand_dims(x, axis = 2L) -
            tf$reshape(self$codebook, 
                       c(1L, 1L, self$num_codes, self$code_size)),
                       axis = 3L 
        )
        
        # bs * 1
        assignments &amp;lt;- tf$argmin(distances, axis = 2L)
        
        # bs * 1 * num_codes
        one_hot_assignments &amp;lt;- tf$one_hot(assignments, depth = self$num_codes)
        
        # bs * 1 * code_size
        nearest_codebook_entries &amp;lt;- tf$reduce_sum(
          tf$expand_dims(
            one_hot_assignments, -1L) * 
            tf$reshape(self$codebook, c(1L, 1L, self$num_codes, self$code_size)),
                       axis = 2L 
                       )
        list(nearest_codebook_entries, one_hot_assignments)
      }
    })
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we’ve seen how the codes are stored, let’s add functionality for updating them. As we said above, they are not learned via gradient descent. Instead, they are exponential moving averages, continually updated by whatever new “class member” they get assigned.&lt;/p&gt;
&lt;p&gt;So here is a function &lt;code&gt;update_ema&lt;/code&gt; that will take care of this.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;update_ema&lt;/code&gt; uses TensorFlow &lt;a href="https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage"&gt;moving_averages&lt;/a&gt; to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;first, keep track of the number of currently assigned samples per code (&lt;code&gt;updated_ema_count&lt;/code&gt;), and&lt;/li&gt;
&lt;li&gt;second, compute and assign the current exponential moving average (&lt;code&gt;updated_ema_means&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;moving_averages &amp;lt;- tf$python$training$moving_averages

# decay to use in computing exponential moving average
decay &amp;lt;- 0.99

update_ema &amp;lt;- function(
  vector_quantizer,
  one_hot_assignments,
  codes,
  decay) {
 
  updated_ema_count &amp;lt;- moving_averages$assign_moving_average(
    vector_quantizer$ema_count,
    tf$reduce_sum(one_hot_assignments, axis = c(0L, 1L)),
    decay,
    zero_debias = FALSE
  )

  updated_ema_means &amp;lt;- moving_averages$assign_moving_average(
    vector_quantizer$ema_means,
    # selects all assigned values (masking out the others) and sums them up over the batch
    # (will be divided by count later, so we get an average)
    tf$reduce_sum(
      tf$expand_dims(codes, 2L) *
        tf$expand_dims(one_hot_assignments, 3L), axis = c(0L, 1L)),
    decay,
    zero_debias = FALSE
  )

  updated_ema_count &amp;lt;- updated_ema_count + 1e-5
  updated_ema_means &amp;lt;-  updated_ema_means / tf$expand_dims(updated_ema_count, axis = -1L)
  
  tf$assign(vector_quantizer$codebook, updated_ema_means)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we look at the training loop, let’s quickly complete the scene adding in the last actor, the decoder.&lt;/p&gt;
&lt;h2 id="decoder-model"&gt;Decoder model&lt;/h2&gt;
&lt;p&gt;The decoder is pretty standard, performing a series of deconvolutions and finally, returning a probability for each image pixel.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;default_deconv &amp;lt;- set_defaults(
  layer_conv_2d_transpose,
  list(padding = &amp;quot;same&amp;quot;, activation = activation)
)

decoder_model &amp;lt;- function(name = NULL,
                          input_size,
                          output_shape) {
  
  keras_model_custom(name = name, function(self) {
    
    self$reshape1 &amp;lt;- layer_reshape(target_shape = c(1, 1, input_size))
    self$deconv1 &amp;lt;-
      default_deconv(
        filters = 2 * base_depth,
        kernel_size = 7,
        padding = &amp;quot;valid&amp;quot;
      )
    self$deconv2 &amp;lt;-
      default_deconv(filters = 2 * base_depth, kernel_size = 5)
    self$deconv3 &amp;lt;-
      default_deconv(
        filters = 2 * base_depth,
        kernel_size = 5,
        strides = 2
      )
    self$deconv4 &amp;lt;-
      default_deconv(filters = base_depth, kernel_size = 5)
    self$deconv5 &amp;lt;-
      default_deconv(filters = base_depth,
                     kernel_size = 5,
                     strides = 2)
    self$deconv6 &amp;lt;-
      default_deconv(filters = base_depth, kernel_size = 5)
    self$conv1 &amp;lt;-
      default_conv(filters = output_shape[3],
                   kernel_size = 5,
                   activation = &amp;quot;linear&amp;quot;)
    
    function (x, mask = NULL) {
      
      x &amp;lt;- x %&amp;gt;%
        # output shape:  7 1 1 16
        self$reshape1() %&amp;gt;%
        # output shape:  7 7 7 64
        self$deconv1() %&amp;gt;%
        # output shape:  7 7 7 64
        self$deconv2() %&amp;gt;%
        # output shape:  7 14 14 64
        self$deconv3() %&amp;gt;%
        # output shape:  7 14 14 32
        self$deconv4() %&amp;gt;%
        # output shape:  7 28 28 32
        self$deconv5() %&amp;gt;%
        # output shape:  7 28 28 32
        self$deconv6() %&amp;gt;%
        # output shape:  7 28 28 1
        self$conv1()
      
      tfd$Independent(tfd$Bernoulli(logits = x),
                      reinterpreted_batch_ndims = length(output_shape))
    }
  })
}

input_shape &amp;lt;- c(28, 28, 1)
decoder &amp;lt;- decoder_model(input_size = latent_size * code_size,
                         output_shape = input_shape)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’re ready to train. One thing we haven’t really talked about yet is the cost function: Given the differences in architecture (compared to standard VAEs), will the losses still look as expected (the usual add-up of reconstruction loss and KL divergence)? We’ll see that in a second.&lt;/p&gt;
&lt;h2 id="training-loop"&gt;Training loop&lt;/h2&gt;
&lt;p&gt;Here’s the optimizer we’ll use. Losses will be calculated inline.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;optimizer &amp;lt;- tf$train$AdamOptimizer(learning_rate = learning_rate)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The training loop, as usual, is a loop over epochs, where each iteration is a loop over batches obtained from the dataset. For each batch, we have a forward pass, recorded by a &lt;code&gt;gradientTape&lt;/code&gt;, based on which we calculate the loss. The tape will then determine the gradients of all trainable weights throughout the model, and the optimizer will use those gradients to update the weights.&lt;/p&gt;
&lt;p&gt;So far, all of this conforms to a scheme we’ve oftentimes seen before. One point to note though: In this same loop, we also call &lt;code&gt;update_ema&lt;/code&gt; to recalculate the moving averages, as those are not operated on during backprop. Here is the essential functionality:&lt;a href="#fn4" class="footnote-ref" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;num_epochs &amp;lt;- 20

for (epoch in seq_len(num_epochs)) {
  
  iter &amp;lt;- make_iterator_one_shot(train_dataset)
  
  until_out_of_range({
    
    x &amp;lt;-  iterator_get_next(iter)
    with(tf$GradientTape(persistent = TRUE) %as% tape, {
      
      # do forward pass
      # calculate losses
      
    })
    
    encoder_gradients &amp;lt;- tape$gradient(loss, encoder$variables)
    decoder_gradients &amp;lt;- tape$gradient(loss, decoder$variables)
    
    optimizer$apply_gradients(purrr::transpose(list(
      encoder_gradients, encoder$variables
    )),
    global_step = tf$train$get_or_create_global_step())
    
    optimizer$apply_gradients(purrr::transpose(list(
      decoder_gradients, decoder$variables
    )),
    global_step = tf$train$get_or_create_global_step())
    
    update_ema(vector_quantizer,
               one_hot_assignments,
               codes,
               decay)

    # periodically display some generated images
    # see code on github 
    # visualize_images(&amp;quot;kuzushiji&amp;quot;, epoch, reconstructed_images, random_images)
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, for the actual action. Inside the context of the gradient tape, we first determine which encoded input sample gets assigned to which embedding vector.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;codes &amp;lt;- encoder(x)
c(nearest_codebook_entries, one_hot_assignments) %&amp;lt;-% vector_quantizer(codes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, for this assignment operation there is no gradient. Instead what we can do is pass the gradients from decoder input straight through to encoder output. Here &lt;code&gt;tf$stop_gradient&lt;/code&gt; exempts &lt;code&gt;nearest_codebook_entries&lt;/code&gt; from the chain of gradients, so encoder and decoder are linked by &lt;code&gt;codes&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;codes_straight_through &amp;lt;- codes + tf$stop_gradient(nearest_codebook_entries - codes)
decoder_distribution &amp;lt;- decoder(codes_straight_through)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In sum, backprop will take care of the decoder’s as well as the encoder’s weights, whereas the latent embeddings are updated using moving averages, as we’ve seen already.&lt;/p&gt;
&lt;p&gt;Now we’re ready to tackle the losses. There are three components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First, the reconstruction loss, which is just the log probability of the actual input under the distribution learned by the decoder.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;reconstruction_loss &amp;lt;- -tf$reduce_mean(decoder_distribution$log_prob(x))&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Second, we have the &lt;em&gt;commitment loss&lt;/em&gt;, defined as the mean squared deviation of the encoded input samples from the nearest neighbors they’ve been assigned to: We want the network to “commit” to a concise set of latent codes!&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;commitment_loss &amp;lt;- tf$reduce_mean(tf$square(codes - tf$stop_gradient(nearest_codebook_entries)))&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Finally, we have the usual KL diverge to a prior. As, a priori, all assignments are equally probable, this component of the loss is constant and can oftentimes be dispensed of. We’re adding it here mainly for illustrative purposes.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;prior_dist &amp;lt;- tfd$Multinomial(
  total_count = 1,
  logits = tf$zeros(c(latent_size, num_codes))
  )
prior_loss &amp;lt;- -tf$reduce_mean(
  tf$reduce_sum(prior_dist$log_prob(one_hot_assignments), 1L)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Summing up all three components, we arrive at the overall loss:&lt;a href="#fn5" class="footnote-ref" id="fnref5"&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;beta &amp;lt;- 0.25
loss &amp;lt;- reconstruction_loss + beta * commitment_loss + prior_loss&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we look at the results, let’s see what happens inside &lt;code&gt;gradientTape&lt;/code&gt; at a single glance:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;with(tf$GradientTape(persistent = TRUE) %as% tape, {
      
  codes &amp;lt;- encoder(x)
  c(nearest_codebook_entries, one_hot_assignments) %&amp;lt;-% vector_quantizer(codes)
  codes_straight_through &amp;lt;- codes + tf$stop_gradient(nearest_codebook_entries - codes)
  decoder_distribution &amp;lt;- decoder(codes_straight_through)
      
  reconstruction_loss &amp;lt;- -tf$reduce_mean(decoder_distribution$log_prob(x))
  commitment_loss &amp;lt;- tf$reduce_mean(tf$square(codes - tf$stop_gradient(nearest_codebook_entries)))
  prior_dist &amp;lt;- tfd$Multinomial(
    total_count = 1,
    logits = tf$zeros(c(latent_size, num_codes))
  )
  prior_loss &amp;lt;- -tf$reduce_mean(tf$reduce_sum(prior_dist$log_prob(one_hot_assignments), 1L))
  
  loss &amp;lt;- reconstruction_loss + beta * commitment_loss + prior_loss
})&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="results"&gt;Results&lt;/h2&gt;
&lt;p&gt;And here we go. This time, we can’t have the 2d “morphing view” one generally likes to display with VAEs (there just is no 2d latent space). Instead, the two images below are (1) letters generated from random input and (2) reconstructed &lt;em&gt;actual&lt;/em&gt; letters, each saved after training for nine epochs.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-01-24-vq-vae/images/epoch_9.png" alt="" /&gt;
&lt;p class="caption"&gt;Left: letters generated from random input. Right: reconstructed input letters.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Two things jump to the eye: First, the generated letters are significantly sharper than their continuous-prior counterparts (from the previous post). And second, would you have been able to tell the random image from the reconstruction image?&lt;/p&gt;
&lt;h1 id="conclusion"&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;At this point, we’ve hopefully convinced you of the power and effectiveness of this discrete-latents approach. However, you might secretly have hoped we’d apply this to more complex data, such as the elements of speech we mentioned in the introduction, or higher-resolution images as found in ImageNet.&lt;a href="#fn6" class="footnote-ref" id="fnref6"&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The truth is that there’s a continuous tradeoff between the number of new and exciting techniques we can show, and the time we can spend on iterations to successfully apply these techniques to complex datasets. In the end it’s you, our readers, who will put these techniques to meaningful use on relevant, real world data.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-clanuwat2018deep"&gt;
&lt;p&gt;Clanuwat, Tarin, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. 2018. “Deep Learning for Classical Japanese Literature.” December 3, 2018. &lt;a href="http://arxiv.org/abs/cs.CV/1812.01718"&gt;http://arxiv.org/abs/cs.CV/1812.01718&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-abs-1711-00937"&gt;
&lt;p&gt;Oord, Aaron van den, Oriol Vinyals, and Koray Kavukcuoglu. 2017. “Neural Discrete Representation Learning.” &lt;em&gt;CoRR&lt;/em&gt; abs/1711.00937. &lt;a href="http://arxiv.org/abs/1711.00937"&gt;http://arxiv.org/abs/1711.00937&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Assuming a 1d latent space, that is. The authors actually used 1d, 2d and 3d spaces in their experiments.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;In the paper, the authors actually mention this as one of two ways to learn the prior, the other one being vector quantisation.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;To be specific, the authors indicate that they used a field of 32 x 32 latents for ImageNet, and 8 x 8 x 10 for CIFAR10.&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;The code on github additionally contains functionality to display generated images, output the losses, and save checkpoints.&lt;a href="#fnref4" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn5"&gt;&lt;p&gt;Here beta is a scaling parameter found surprisingly unimportant by the paper authors.&lt;a href="#fnref5" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn6"&gt;&lt;p&gt;Although we have to say we find that Kuzushiji-MNIST beats MNIST by far, in complexity and aesthetics!&lt;a href="#fnref6" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">e7f3e86f153b39f8bd73a1c84f5aa8cf</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Probabilistic ML/DL</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-01-24-vq-vae</guid>
      <pubDate>Thu, 24 Jan 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-01-24-vq-vae/images/thumb1.png" medium="image" type="image/png" width="510" height="287"/>
    </item>
    <item>
      <title>Getting started with TensorFlow Probability from R</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-01-08-getting-started-with-tf-probability</link>
      <description>


&lt;p&gt;With the abundance of great libraries, in R, for statistical computing, why would you be interested in TensorFlow Probability (&lt;em&gt;TFP&lt;/em&gt;, for short)? Well - let’s look at a list of its components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Distributions and bijectors (bijectors are reversible, composable maps)&lt;/li&gt;
&lt;li&gt;Probabilistic modeling (Edward2 and probabilistic network layers)&lt;/li&gt;
&lt;li&gt;Probabilistic inference (via MCMC or variational inference)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now imagine all these working seamlessly with the TensorFlow framework - core, Keras, contributed modules - and also, running distributed and on GPU. The field of possible applications is vast - and far too diverse to cover as a whole in an introductory blog post.&lt;/p&gt;
&lt;p&gt;Instead, our aim here is to provide a first introduction to &lt;em&gt;TFP&lt;/em&gt;, focusing on direct applicability to and interoperability with deep learning. We’ll quickly show how to get started with one of the basic building blocks: &lt;code&gt;distributions&lt;/code&gt;. Then, we’ll build a variational autoencoder similar to that in &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae/"&gt;Representation learning with MMD-VAE&lt;/a&gt;. This time though, we’ll make use of &lt;em&gt;TFP&lt;/em&gt; to sample from the prior and approximate posterior distributions.&lt;/p&gt;
&lt;p&gt;We’ll regard this post as a “proof on concept” for using &lt;em&gt;TFP&lt;/em&gt; with Keras - from R - and plan to follow up with more elaborate examples from the area of semi-supervised representation learning.&lt;/p&gt;
&lt;h1 id="installing-and-using-tfp"&gt;Installing and using &lt;em&gt;TFP&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;To install &lt;em&gt;TFP&lt;/em&gt; together with TensorFlow, simply append &lt;code&gt;tensorflow-probability&lt;/code&gt; to the default list of extra packages:&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tensorflow)
install_tensorflow(
  extra_packages = c(&amp;quot;keras&amp;quot;, &amp;quot;tensorflow-hub&amp;quot;, &amp;quot;tensorflow-probability&amp;quot;),
  version = &amp;quot;1.12&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now to use &lt;em&gt;TFP&lt;/em&gt;, all we need to do is import it and create some useful handles.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tensorflow)
tfp &amp;lt;- import(&amp;quot;tensorflow_probability&amp;quot;)
tfd &amp;lt;- tfp$distributions&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And here we go, sampling from a standard normal distribution.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;n &amp;lt;- tfd$Normal(loc = 0, scale = 1)
n$sample(6L)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor(
&amp;quot;Normal_1/sample/Reshape:0&amp;quot;, shape=(6,), dtype=float32
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that’s nice, but it’s 2019, we don’t want to have to create a session to evaluate these tensors anymore. In the variational autoencoder example below, we are going to see how &lt;em&gt;TFP&lt;/em&gt; and TF &lt;em&gt;eager execution&lt;/em&gt; are the perfect match, so why not start using it now.&lt;/p&gt;
&lt;p&gt;To use eager execution, we have to execute the following lines in a fresh (R) session:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
use_implementation(&amp;quot;tensorflow&amp;quot;)
library(tensorflow)
tfe_enable_eager_execution(device_policy = &amp;quot;silent&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;… and import &lt;em&gt;TFP&lt;/em&gt;, same as above.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;tfp &amp;lt;- import(&amp;quot;tensorflow_probability&amp;quot;)
tfd &amp;lt;- tfp$distributions&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s quickly look at &lt;em&gt;TFP&lt;/em&gt; distributions.&lt;/p&gt;
&lt;h2 id="using-distributions"&gt;Using distributions&lt;/h2&gt;
&lt;p&gt;Here’s that standard normal again.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;n &amp;lt;- tfd$Normal(loc = 0, scale = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Things commonly done with a distribution include sampling:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# just as in low-level tensorflow, we need to append L to indicate integer arguments
n$sample(6L) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor(
[-0.34403768 -0.14122334 -1.3832929   1.618252    1.364448   -1.1299014 ],
shape=(6,),
dtype=float32
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As well as getting the log probability. Here we do that simultaneously for three values.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;n$log_prob(c(-1, 0, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor(
[-1.4189385 -0.9189385 -1.4189385], shape=(3,), dtype=float32
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can do the same things with lots of other distributions, e.g., the Bernoulli:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;b &amp;lt;- tfd$Bernoulli(0.9)
b$sample(10L)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor(
[1 1 1 0 1 1 0 1 0 1], shape=(10,), dtype=int32
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;b$log_prob(c(0,1,0,0))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor(
[-1.2411538 -0.3411539 -1.2411538 -1.2411538], shape=(4,), dtype=float32
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that in the last chunk, we are asking for the log probabilities of four independent draws.&lt;/p&gt;
&lt;h2 id="batch-shapes-and-event-shapes"&gt;Batch shapes and event shapes&lt;/h2&gt;
&lt;p&gt;In &lt;em&gt;TFP&lt;/em&gt;, we can do the following.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ns &amp;lt;- tfd$Normal(
  loc = c(1, 10, -200),
  scale = c(0.1, 0.1, 1)
)
ns&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tfp.distributions.Normal(
&amp;quot;Normal/&amp;quot;, batch_shape=(3,), event_shape=(), dtype=float32
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Contrary to what it might look like, this is not a multivariate normal. As indicated by &lt;code&gt;batch_shape=(3,)&lt;/code&gt;, this is a “batch” of independent univariate distributions. The fact that these are univariate is seen in &lt;code&gt;event_shape=()&lt;/code&gt;: Each of them lives in one-dimensional &lt;em&gt;event space&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;If instead we create a single, two-dimensional multivariate normal:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;n &amp;lt;- tfd$MultivariateNormalDiag(loc = c(0, 10), scale_diag = c(1, 4))
n&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tfp.distributions.MultivariateNormalDiag(
&amp;quot;MultivariateNormalDiag/&amp;quot;, batch_shape=(), event_shape=(2,), dtype=float32
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we see &lt;code&gt;batch_shape=(), event_shape=(2,)&lt;/code&gt;, as expected.&lt;/p&gt;
&lt;p&gt;Of course, we can combine both, creating batches of multivariate distributions:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;nd_batch &amp;lt;- tfd$MultivariateNormalFullCovariance(
  loc = list(c(0., 0.), c(1., 1.), c(2., 2.)),
  covariance_matrix = list(
    matrix(c(1, .1, .1, 1), ncol = 2),
    matrix(c(1, .3, .3, 1), ncol = 2),
    matrix(c(1, .5, .5, 1), ncol = 2))
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This example defines a batch of three two-dimensional multivariate normal distributions.&lt;/p&gt;
&lt;h2 id="converting-between-batch-shapes-and-event-shapes"&gt;Converting between batch shapes and event shapes&lt;/h2&gt;
&lt;p&gt;Strange as it may sound, situations arise where we want to transform distribution shapes between these types - in fact, we’ll see such a case very soon.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;tfd$Independent&lt;/code&gt; is used to convert dimensions in &lt;code&gt;batch_shape&lt;/code&gt; to dimensions in &lt;code&gt;event_shape&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here is a batch of three independent Bernoulli distributions.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;bs &amp;lt;- tfd$Bernoulli(probs=c(.3,.5,.7))
bs&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tfp.distributions.Bernoulli(
&amp;quot;Bernoulli/&amp;quot;, batch_shape=(3,), event_shape=(), dtype=int32
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can convert this to a virtual “three-dimensional” Bernoulli like this:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;b &amp;lt;- tfd$Independent(bs, reinterpreted_batch_ndims = 1L)
b&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tfp.distributions.Independent(
&amp;quot;IndependentBernoulli/&amp;quot;, batch_shape=(), event_shape=(3,), dtype=int32
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here &lt;code&gt;reinterpreted_batch_ndims&lt;/code&gt; tells &lt;em&gt;TFP&lt;/em&gt; how many of the batch dimensions are being used for the event space, starting to count from the right of the shape list.&lt;/p&gt;
&lt;p&gt;With this basic understanding of &lt;em&gt;TFP&lt;/em&gt; distributions, we’re ready to see them used in a VAE.&lt;/p&gt;
&lt;h1 id="variational-autoencoder-using-tfp"&gt;Variational autoencoder using &lt;em&gt;TFP&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;We’ll take the (not so) deep convolutional architecture from &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae/"&gt;Representation learning with MMD-VAE&lt;/a&gt; and use &lt;code&gt;distributions&lt;/code&gt; for sampling and computing probabilities. Optionally, our new VAE will be able to &lt;em&gt;learn the prior distribution&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Concretely, the following exposition will consist of three parts. First, we present common code applicable to both a VAE with a static prior, and one that learns the parameters of the prior distribution. Then, we have the training loop for the first (static-prior) VAE. Finally, we discuss the training loop and additional model involved in the second (prior-learning) VAE.&lt;/p&gt;
&lt;p&gt;Presenting both versions one after the other leads to code duplications, but avoids scattering confusing if-else branches throughout the code.&lt;/p&gt;
&lt;p&gt;The second VAE is available as &lt;a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/tfprob_vae.R"&gt;part of the Keras examples&lt;/a&gt; so you don’t have to copy out code snippets. The code also contains additional functionality not discussed and replicated here, such as for saving model weights.&lt;/p&gt;
&lt;p&gt;So, let’s start with the common part.&lt;/p&gt;
&lt;p&gt;At the risk of repeating ourselves, here again are the preparatory steps (including a few additional library loads).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
use_implementation(&amp;quot;tensorflow&amp;quot;)
library(tensorflow)
tfe_enable_eager_execution(device_policy = &amp;quot;silent&amp;quot;)

tfp &amp;lt;- import(&amp;quot;tensorflow_probability&amp;quot;)
tfd &amp;lt;- tfp$distributions

library(tfdatasets)
library(dplyr)
library(glue)&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="dataset"&gt;Dataset&lt;/h3&gt;
&lt;p&gt;For a change from MNIST and Fashion-MNIST, we’ll use the brand new &lt;a href="https://github.com/rois-codh/kmnist"&gt;Kuzushiji-MNIST&lt;/a&gt;&lt;span class="citation"&gt;(Clanuwat et al. 2018)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-01-08-getting-started-with-tf-probability/images/kmnist_examples.png" alt="" /&gt;
&lt;p class="caption"&gt;From: Deep Learning for Classical Japanese Literature &lt;span class="citation"&gt;(Clanuwat et al. 2018)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;pre class="r"&gt;&lt;code&gt;np &amp;lt;- import(&amp;quot;numpy&amp;quot;)

kuzushiji &amp;lt;- np$load(&amp;quot;kmnist-train-imgs.npz&amp;quot;)
kuzushiji &amp;lt;- kuzushiji$get(&amp;quot;arr_0&amp;quot;)
 
train_images &amp;lt;- kuzushiji %&amp;gt;%
  k_expand_dims() %&amp;gt;%
  k_cast(dtype = &amp;quot;float32&amp;quot;)

train_images &amp;lt;- train_images %&amp;gt;% `/`(255)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As in that other post, we stream the data via &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/introduction.html"&gt;tfdatasets&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;buffer_size &amp;lt;- 60000
batch_size &amp;lt;- 256
batches_per_epoch &amp;lt;- buffer_size / batch_size

train_dataset &amp;lt;- tensor_slices_dataset(train_images) %&amp;gt;%
  dataset_shuffle(buffer_size) %&amp;gt;%
  dataset_batch(batch_size)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s see what changes in the encoder and decoder models.&lt;/p&gt;
&lt;h3 id="encoder"&gt;Encoder&lt;/h3&gt;
&lt;p&gt;The encoder differs from what we had without &lt;em&gt;TFP&lt;/em&gt; in that it does not return the approximate posterior means and variances directly as tensors. Instead, it returns a batch of multivariate normal distributions:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# you might want to change this depending on the dataset
latent_dim &amp;lt;- 2

encoder_model &amp;lt;- function(name = NULL) {

  keras_model_custom(name = name, function(self) {
  
    self$conv1 &amp;lt;-
      layer_conv_2d(
        filters = 32,
        kernel_size = 3,
        strides = 2,
        activation = &amp;quot;relu&amp;quot;
      )
    self$conv2 &amp;lt;-
      layer_conv_2d(
        filters = 64,
        kernel_size = 3,
        strides = 2,
        activation = &amp;quot;relu&amp;quot;
      )
    self$flatten &amp;lt;- layer_flatten()
    self$dense &amp;lt;- layer_dense(units = 2 * latent_dim)
    
    function (x, mask = NULL) {
      x &amp;lt;- x %&amp;gt;%
        self$conv1() %&amp;gt;%
        self$conv2() %&amp;gt;%
        self$flatten() %&amp;gt;%
        self$dense()
        
      tfd$MultivariateNormalDiag(
        loc = x[, 1:latent_dim],
        scale_diag = tf$nn$softplus(x[, (latent_dim + 1):(2 * latent_dim)] + 1e-5)
      )
    }
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s try this out.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;encoder &amp;lt;- encoder_model()

iter &amp;lt;- make_iterator_one_shot(train_dataset)
x &amp;lt;-  iterator_get_next(iter)

approx_posterior &amp;lt;- encoder(x)
approx_posterior&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tfp.distributions.MultivariateNormalDiag(
&amp;quot;MultivariateNormalDiag/&amp;quot;, batch_shape=(256,), event_shape=(2,), dtype=float32
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;approx_posterior$sample()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor(
[[ 5.77791929e-01 -1.64988488e-02]
 [ 7.93901443e-01 -1.00042784e+00]
 [-1.56279251e-01 -4.06365871e-01]
 ...
 ...
 [-6.47531569e-01  2.10889503e-02]], shape=(256, 2), dtype=float32)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We don’t know about you, but we still enjoy the ease of inspecting values with &lt;em&gt;eager execution&lt;/em&gt; - a lot.&lt;/p&gt;
&lt;p&gt;Now, on to the decoder, which too returns a distribution instead of a tensor.&lt;/p&gt;
&lt;h3 id="decoder"&gt;Decoder&lt;/h3&gt;
&lt;p&gt;In the decoder, we see why transformations between batch shape and event shape are useful. The output of &lt;code&gt;self$deconv3&lt;/code&gt; is four-dimensional. What we need is an on-off-probability for every pixel. Formerly, this was accomplished by feeding the tensor into a dense layer and applying a sigmoid activation. Here, we use &lt;code&gt;tfd$Independent&lt;/code&gt; to effectively tranform the tensor into a probability distribution over three-dimensional images (width, height, channel(s)).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;decoder_model &amp;lt;- function(name = NULL) {
  
  keras_model_custom(name = name, function(self) {
    
    self$dense &amp;lt;- layer_dense(units = 7 * 7 * 32, activation = &amp;quot;relu&amp;quot;)
    self$reshape &amp;lt;- layer_reshape(target_shape = c(7, 7, 32))
    self$deconv1 &amp;lt;-
      layer_conv_2d_transpose(
        filters = 64,
        kernel_size = 3,
        strides = 2,
        padding = &amp;quot;same&amp;quot;,
        activation = &amp;quot;relu&amp;quot;
      )
    self$deconv2 &amp;lt;-
      layer_conv_2d_transpose(
        filters = 32,
        kernel_size = 3,
        strides = 2,
        padding = &amp;quot;same&amp;quot;,
        activation = &amp;quot;relu&amp;quot;
      )
    self$deconv3 &amp;lt;-
      layer_conv_2d_transpose(
        filters = 1,
        kernel_size = 3,
        strides = 1,
        padding = &amp;quot;same&amp;quot;
      )
    
    function (x, mask = NULL) {
      x &amp;lt;- x %&amp;gt;%
        self$dense() %&amp;gt;%
        self$reshape() %&amp;gt;%
        self$deconv1() %&amp;gt;%
        self$deconv2() %&amp;gt;%
        self$deconv3()
      
      tfd$Independent(tfd$Bernoulli(logits = x),
                      reinterpreted_batch_ndims = 3L)
      
    }
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s try this out too.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;decoder &amp;lt;- decoder_model()
decoder_likelihood &amp;lt;- decoder(approx_posterior_sample)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tfp.distributions.Independent(
&amp;quot;IndependentBernoulli/&amp;quot;, batch_shape=(256,), event_shape=(28, 28, 1), dtype=int32
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This distribution will be used to generate the “reconstructions”, as well as determine the loglikelihood of the original samples.&lt;/p&gt;
&lt;h3 id="kl-loss-and-optimizer"&gt;KL loss and optimizer&lt;/h3&gt;
&lt;p&gt;Both VAEs discussed below will need an optimizer …&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;optimizer &amp;lt;- tf$train$AdamOptimizer(1e-4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;… and both will delegate to &lt;code&gt;compute_kl_loss&lt;/code&gt; to compute the KL part of the loss.&lt;/p&gt;
&lt;p&gt;This helper function simply subtracts the log likelihood of the samples&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; under the prior from their loglikelihood under the approximate posterior.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;compute_kl_loss &amp;lt;- function(
  latent_prior,
  approx_posterior,
  approx_posterior_sample) {
  
  kl_div &amp;lt;- approx_posterior$log_prob(approx_posterior_sample) -
    latent_prior$log_prob(approx_posterior_sample)
  avg_kl_div &amp;lt;- tf$reduce_mean(kl_div)
  avg_kl_div
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we’ve looked at the common parts, we first discuss how to train a VAE with a static prior.&lt;/p&gt;
&lt;h1 id="vae-with-static-prior"&gt;VAE with static prior&lt;/h1&gt;
&lt;p&gt;In this VAE, we use &lt;em&gt;TFP&lt;/em&gt; to create the usual isotropic Gaussian prior. We then directly sample from this distribution in the training loop.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;latent_prior &amp;lt;- tfd$MultivariateNormalDiag(
  loc  = tf$zeros(list(latent_dim)),
  scale_identity_multiplier = 1
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And here is the complete training loop. We’ll point out the crucial &lt;em&gt;TFP&lt;/em&gt;-related steps below.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;for (epoch in seq_len(num_epochs)) {
  iter &amp;lt;- make_iterator_one_shot(train_dataset)
  
  total_loss &amp;lt;- 0
  total_loss_nll &amp;lt;- 0
  total_loss_kl &amp;lt;- 0
  
  until_out_of_range({
    x &amp;lt;-  iterator_get_next(iter)
    
    with(tf$GradientTape(persistent = TRUE) %as% tape, {
      approx_posterior &amp;lt;- encoder(x)
      approx_posterior_sample &amp;lt;- approx_posterior$sample()
      decoder_likelihood &amp;lt;- decoder(approx_posterior_sample)
      
      nll &amp;lt;- -decoder_likelihood$log_prob(x)
      avg_nll &amp;lt;- tf$reduce_mean(nll)
      
      kl_loss &amp;lt;- compute_kl_loss(
        latent_prior,
        approx_posterior,
        approx_posterior_sample
      )

      loss &amp;lt;- kl_loss + avg_nll
    })
    
    total_loss &amp;lt;- total_loss + loss
    total_loss_nll &amp;lt;- total_loss_nll + avg_nll
    total_loss_kl &amp;lt;- total_loss_kl + kl_loss
    
    encoder_gradients &amp;lt;- tape$gradient(loss, encoder$variables)
    decoder_gradients &amp;lt;- tape$gradient(loss, decoder$variables)
    
    optimizer$apply_gradients(purrr::transpose(list(
      encoder_gradients, encoder$variables
    )),
    global_step = tf$train$get_or_create_global_step())
    optimizer$apply_gradients(purrr::transpose(list(
      decoder_gradients, decoder$variables
    )),
    global_step = tf$train$get_or_create_global_step())
 
  })
  
  cat(
    glue(
      &amp;quot;Losses (epoch): {epoch}:&amp;quot;,
      &amp;quot;  {(as.numeric(total_loss_nll)/batches_per_epoch) %&amp;gt;% round(4)} nll&amp;quot;,
      &amp;quot;  {(as.numeric(total_loss_kl)/batches_per_epoch) %&amp;gt;% round(4)} kl&amp;quot;,
      &amp;quot;  {(as.numeric(total_loss)/batches_per_epoch) %&amp;gt;% round(4)} total&amp;quot;
    ),
    &amp;quot;\n&amp;quot;
  )
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Above, playing around with the encoder and the decoder, we’ve already seen how&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;approx_posterior &amp;lt;- encoder(x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;gives us a distribution we can sample from. We use it to obtain samples from the approximate posterior:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;approx_posterior_sample &amp;lt;- approx_posterior$sample()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These samples, we take them and feed them to the decoder, who gives us on-off-likelihoods for image pixels.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;decoder_likelihood &amp;lt;- decoder(approx_posterior_sample)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the loss consists of the usual ELBO components: reconstruction loss and KL divergence. The reconstruction loss we directly obtain from &lt;em&gt;TFP&lt;/em&gt;, using the learned decoder distribution to assess the likelihood of the original input.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;nll &amp;lt;- -decoder_likelihood$log_prob(x)
avg_nll &amp;lt;- tf$reduce_mean(nll)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The KL loss we get from &lt;code&gt;compute_kl_loss&lt;/code&gt;, the helper function we saw above:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;kl_loss &amp;lt;- compute_kl_loss(
        latent_prior,
        approx_posterior,
        approx_posterior_sample
      )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We add both and arrive at the overall VAE loss:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;loss &amp;lt;- kl_loss + avg_nll&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apart from these changes due to using &lt;em&gt;TFP&lt;/em&gt;, the training process is just normal backprop, the way it looks using &lt;em&gt;eager execution&lt;/em&gt;.&lt;/p&gt;
&lt;h1 id="vae-with-learnable-prior-mixture-of-gaussians"&gt;VAE with learnable prior (mixture of Gaussians)&lt;/h1&gt;
&lt;p&gt;Now let’s see how instead of using the standard isotropic Gaussian, we could learn a mixture of Gaussians. The choice of number of distributions here is pretty arbitrary. Just as with &lt;code&gt;latent_dim&lt;/code&gt;, you might want to experiment and find out what works best on your dataset.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;mixture_components &amp;lt;- 16

learnable_prior_model &amp;lt;- function(name = NULL, latent_dim, mixture_components) {
  
  keras_model_custom(name = name, function(self) {
    
    self$loc &amp;lt;-
      tf$get_variable(
        name = &amp;quot;loc&amp;quot;,
        shape = list(mixture_components, latent_dim),
        dtype = tf$float32
      )
    self$raw_scale_diag &amp;lt;- tf$get_variable(
      name = &amp;quot;raw_scale_diag&amp;quot;,
      shape = c(mixture_components, latent_dim),
      dtype = tf$float32
    )
    self$mixture_logits &amp;lt;-
      tf$get_variable(
        name = &amp;quot;mixture_logits&amp;quot;,
        shape = c(mixture_components),
        dtype = tf$float32
      )
      
    function (x, mask = NULL) {
        tfd$MixtureSameFamily(
          components_distribution = tfd$MultivariateNormalDiag(
            loc = self$loc,
            scale_diag = tf$nn$softplus(self$raw_scale_diag)
          ),
          mixture_distribution = tfd$Categorical(logits = self$mixture_logits)
        )
      }
    })
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;em&gt;TFP&lt;/em&gt; terminology, &lt;code&gt;components_distribution&lt;/code&gt; is the underlying distribution type, and &lt;code&gt;mixture_distribution&lt;/code&gt; holds the probabilities that individual components are chosen.&lt;/p&gt;
&lt;p&gt;Note how &lt;code&gt;self$loc&lt;/code&gt;, &lt;code&gt;self$raw_scale_diag&lt;/code&gt; and &lt;code&gt;self$mixture_logits&lt;/code&gt; are TensorFlow &lt;code&gt;Variables&lt;/code&gt; and thus, persistent and updatable by backprop.&lt;/p&gt;
&lt;p&gt;Now we create the model.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;latent_prior_model &amp;lt;- learnable_prior_model(
  latent_dim = latent_dim,
  mixture_components = mixture_components
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we obtain a latent prior distribution we can sample from? A bit unusually, this model will be called without an input:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;latent_prior &amp;lt;- latent_prior_model(NULL)
latent_prior&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tfp.distributions.MixtureSameFamily(
&amp;quot;MixtureSameFamily/&amp;quot;, batch_shape=(), event_shape=(2,), dtype=float32
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here now is the complete training loop. Note how we have a third model to backprop through.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;for (epoch in seq_len(num_epochs)) {
  iter &amp;lt;- make_iterator_one_shot(train_dataset)
  
  total_loss &amp;lt;- 0
  total_loss_nll &amp;lt;- 0
  total_loss_kl &amp;lt;- 0
  
  until_out_of_range({
    x &amp;lt;-  iterator_get_next(iter)
    
    with(tf$GradientTape(persistent = TRUE) %as% tape, {
      approx_posterior &amp;lt;- encoder(x)
      
      approx_posterior_sample &amp;lt;- approx_posterior$sample()
      decoder_likelihood &amp;lt;- decoder(approx_posterior_sample)
      
      nll &amp;lt;- -decoder_likelihood$log_prob(x)
      avg_nll &amp;lt;- tf$reduce_mean(nll)
      
      latent_prior &amp;lt;- latent_prior_model(NULL)
      
      kl_loss &amp;lt;- compute_kl_loss(
        latent_prior,
        approx_posterior,
        approx_posterior_sample
      )

      loss &amp;lt;- kl_loss + avg_nll
    })
    
    total_loss &amp;lt;- total_loss + loss
    total_loss_nll &amp;lt;- total_loss_nll + avg_nll
    total_loss_kl &amp;lt;- total_loss_kl + kl_loss
    
    encoder_gradients &amp;lt;- tape$gradient(loss, encoder$variables)
    decoder_gradients &amp;lt;- tape$gradient(loss, decoder$variables)
    prior_gradients &amp;lt;-
      tape$gradient(loss, latent_prior_model$variables)
    
    optimizer$apply_gradients(purrr::transpose(list(
      encoder_gradients, encoder$variables
    )),
    global_step = tf$train$get_or_create_global_step())
    optimizer$apply_gradients(purrr::transpose(list(
      decoder_gradients, decoder$variables
    )),
    global_step = tf$train$get_or_create_global_step())
    optimizer$apply_gradients(purrr::transpose(list(
      prior_gradients, latent_prior_model$variables
    )),
    global_step = tf$train$get_or_create_global_step())
    
  })
  
  checkpoint$save(file_prefix = checkpoint_prefix)
  
  cat(
    glue(
      &amp;quot;Losses (epoch): {epoch}:&amp;quot;,
      &amp;quot;  {(as.numeric(total_loss_nll)/batches_per_epoch) %&amp;gt;% round(4)} nll&amp;quot;,
      &amp;quot;  {(as.numeric(total_loss_kl)/batches_per_epoch) %&amp;gt;% round(4)} kl&amp;quot;,
      &amp;quot;  {(as.numeric(total_loss)/batches_per_epoch) %&amp;gt;% round(4)} total&amp;quot;
    ),
    &amp;quot;\n&amp;quot;
  )
}  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that’s it! For us, both VAEs yielded similar results, and we did not experience great differences from experimenting with latent dimensionality and the number of mixture distributions. But again, we wouldn’t want to generalize to other datasets, architectures, etc.&lt;/p&gt;
&lt;p&gt;Speaking of results, how do they look? Here we see letters generated after 40 epochs of training. On the left are random letters, on the right, the usual VAE grid display of latent space.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2019-01-08-getting-started-with-tf-probability/images/results.png" /&gt;&lt;/p&gt;
&lt;h1 id="wrapping-up"&gt;Wrapping up&lt;/h1&gt;
&lt;p&gt;Hopefully, we’ve succeeded in showing that TensorFlow Probability, eager execution, and Keras make for an attractive combination! If you relate &lt;a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/tfprob_vae.R"&gt;total amount of code required&lt;/a&gt; to the complexity of the task, as well as depth of the concepts involved, this should appear as a pretty concise implementation.&lt;/p&gt;
&lt;p&gt;In the nearer future, we plan to follow up with more involved applications of TensorFlow Probability, mostly from the area of representation learning. Stay tuned!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-clanuwat2018deep"&gt;
&lt;p&gt;Clanuwat, Tarin, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. 2018. “Deep Learning for Classical Japanese Literature.” December 3, 2018. &lt;a href="http://arxiv.org/abs/cs.CV/1812.01718"&gt;http://arxiv.org/abs/cs.CV/1812.01718&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;At the time we’re publishing this post, we need to pass the “version” argument to enforce compatibility between TF and TFP. This is due to a quirk in TF’s handling of embedded Python that has us temporarily install TF 1.10 by default, until that quirk is to disappear with the upcoming release of TF 1.13.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;Just to be clear: by samples here we mean “samples from the approximate posterior”&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">9f3cb12a810ac2fda8353ebf2259ffc4</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Probabilistic ML/DL</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-01-08-getting-started-with-tf-probability</guid>
      <pubDate>Tue, 08 Jan 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-01-08-getting-started-with-tf-probability/images/thumb.png" medium="image" type="image/png" width="884" height="584"/>
    </item>
    <item>
      <title>Concepts in object detection</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-12-18-object-detection-concepts</link>
      <description>


&lt;p&gt;A few weeks ago, we provided an introduction to the task of &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-11-05-naming-locating-objects/"&gt;naming and locating objects in images&lt;/a&gt;. Crucially, we confined ourselves to detecting a single object in an image. Reading that article, you might have thought “can’t we just extend this approach to several objects”? The short answer is, not in a straightforward way. We’ll see a longer answer shortly.&lt;/p&gt;
&lt;p&gt;In this post, we want to detail one viable approach, explaining (and coding) the steps involved. We won’t, however, end up with a production-ready model. So if you read on, you won’t have a model you can export and put on your smartphone, for use in the wild. You should, however, have learned a bit about how this - object detection - is even possible. After all, it might look like magic!&lt;/p&gt;
&lt;p&gt;The code below is heavily based on &lt;a href="https://github.com/fastai/fastai/blob/master/courses/dl2/pascal-multi.ipynb"&gt;fast.ai’s implementation of SSD&lt;/a&gt;. While this is not the first time we’re “porting” &lt;em&gt;fast.ai&lt;/em&gt; models, in this case we found differences in execution models between PyTorch and TensorFlow to be especially striking, and we will briefly touch on this in our discussion.&lt;/p&gt;
&lt;h2 id="so-why-is-object-detection-hard"&gt;So why is object detection hard?&lt;/h2&gt;
&lt;p&gt;As we saw, we can classify and detect a single object as follows. We make use of a powerful feature extractor, such as &lt;em&gt;Resnet 50&lt;/em&gt;, add a few conv layers for specialization, and then, concatenate two outputs: one that indicates class, and one that has four coordinates specifying a bounding box.&lt;/p&gt;
&lt;p&gt;Now, to detect several objects, can’t we just have several class outputs, and several bounding boxes? Unfortunately we can’t. Assume there are two cute cats in the image, and we have just two bounding box detectors. How does each of them know which cat to detect? What happens in practice is that both of them try to designate both cats, so we end up with two bounding boxes in the middle - where there’s no cat. It’s a bit like averaging a bimodal distribution.&lt;/p&gt;
&lt;p&gt;What can be done? Overall, there are three approaches to object detection, differing in performance in both common senses of the word: execution time and precision.&lt;/p&gt;
&lt;p&gt;Probably the first option you’d think of (if you haven’t been exposed to the topic before) is running the algorithm over the image piece by piece. This is called the &lt;em&gt;sliding windows&lt;/em&gt; approach, and even though in a naive implementation, it would require excessive time, it can be run effectively if making use of fully convolutional models (cf. &lt;em&gt;Overfeat&lt;/em&gt; &lt;span class="citation"&gt;(Sermanet et al. 2013)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Currently the best precision is gained from &lt;em&gt;region proposal&lt;/em&gt; approaches (R-CNN&lt;span class="citation"&gt;(Girshick et al. 2013)&lt;/span&gt;, Fast R-CNN&lt;span class="citation"&gt;(Girshick 2015)&lt;/span&gt;, Faster R-CNN&lt;span class="citation"&gt;(Ren et al. 2015)&lt;/span&gt;). These operate in two steps. A first step points out &lt;em&gt;regions of interest&lt;/em&gt; in an image. Then, a convnet classifies and localizes the objects in each region. In the first step, originally non-deep-learning algorithms were used. With Faster R-CNN though, a convnet takes care of region proposal as well, such that the method now is “fully deep learning”.&lt;/p&gt;
&lt;p&gt;Last but not least, there is the class of &lt;em&gt;single shot detectors&lt;/em&gt;, like YOLO&lt;span class="citation"&gt;(Redmon et al. 2015)&lt;/span&gt;&lt;span class="citation"&gt;(Redmon and Farhadi 2016)&lt;/span&gt;&lt;span class="citation"&gt;(Redmon and Farhadi 2018)&lt;/span&gt;and SSD&lt;span class="citation"&gt;(Liu et al. 2015)&lt;/span&gt;. Just as &lt;em&gt;Overfeat&lt;/em&gt;, these do a single pass only, but they add an additional feature that boosts precision: &lt;em&gt;anchor boxes&lt;/em&gt;.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-12-18-object-detection-concepts/images/anchors.png" class="external" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Use of anchor boxes in SSD. Figure from &lt;span class="citation"&gt;(Liu et al. 2015)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Anchor boxes are prototypical object shapes, arranged systematically over the image. In the simplest case, these can just be rectangles (squares) spread out systematically in a grid. A simple grid already solves the basic problem we started with, above: How does each detector know which object to detect? In a single-shot approach like SSD, each detector is mapped to - &lt;em&gt;responsible for&lt;/em&gt; - a specific anchor box. We’ll see how this can be achieved below.&lt;/p&gt;
&lt;p&gt;What if we have several objects in a grid cell? We can assign more than one anchor box to each cell. Anchor boxes are created with different aspect ratios, to provide a good match to entities of different proportions, such as people or trees on the one hand, and bicycles or balconies on the other. You can see these different anchor boxes in the above figure, in illustrations &lt;em&gt;b&lt;/em&gt; and &lt;em&gt;c&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Now, what if an object spans several grid cells, or even the whole image? It won’t have sufficient overlap with any of the boxes to allow for successful detection. For that reason, SSD puts detectors at several stages in the model - a set of detectors after each successive step of downscaling. We see 8x8 and 4x4 grids in the figure above.&lt;/p&gt;
&lt;p&gt;In this post, we show how to code a &lt;em&gt;very basic&lt;/em&gt; single-shot approach, inspired by SSD but not going to full lengths. We’ll have a basic 16x16 grid of uniform anchors, all applied at the same resolution. In the end, we indicate how to extend this to different aspect ratios and resolutions, focusing on the model architecture.&lt;/p&gt;
&lt;h2 id="a-basic-single-shot-detector"&gt;A basic single-shot detector&lt;/h2&gt;
&lt;p&gt;We’re using the same dataset as in &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-11-05-naming-locating-objects/"&gt;Naming and locating objects in images&lt;/a&gt; - Pascal VOC, the 2007 edition - and we start out with the same preprocessing steps, up and until we have an object &lt;code&gt;imageinfo&lt;/code&gt; that contains, in every row, information about a single object in an image.&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="further-preprocessing"&gt;Further preprocessing&lt;/h3&gt;
&lt;p&gt;To be able to detect multiple objects, we need to aggregate all information on a single image into a single row.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;imageinfo4ssd &amp;lt;- imageinfo %&amp;gt;%
  select(category_id,
         file_name,
         name,
         x_left,
         y_top,
         x_right,
         y_bottom,
         ends_with(&amp;quot;scaled&amp;quot;))

imageinfo4ssd &amp;lt;- imageinfo4ssd %&amp;gt;%
  group_by(file_name) %&amp;gt;%
  summarise(
    categories = toString(category_id),
    name = toString(name),
    xl = toString(x_left_scaled),
    yt = toString(y_top_scaled),
    xr = toString(x_right_scaled),
    yb = toString(y_bottom_scaled),
    xl_orig = toString(x_left),
    yt_orig = toString(y_top),
    xr_orig = toString(x_right),
    yb_orig = toString(y_bottom),
    cnt = n()
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s check we got this right.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;example &amp;lt;- imageinfo4ssd[5, ]
img &amp;lt;- image_read(file.path(img_dir, example$file_name))
name &amp;lt;- (example$name %&amp;gt;% str_split(pattern = &amp;quot;, &amp;quot;))[[1]]
x_left &amp;lt;- (example$xl_orig %&amp;gt;% str_split(pattern = &amp;quot;, &amp;quot;))[[1]]
x_right &amp;lt;- (example$xr_orig %&amp;gt;% str_split(pattern = &amp;quot;, &amp;quot;))[[1]]
y_top &amp;lt;- (example$yt_orig %&amp;gt;% str_split(pattern = &amp;quot;, &amp;quot;))[[1]]
y_bottom &amp;lt;- (example$yb_orig %&amp;gt;% str_split(pattern = &amp;quot;, &amp;quot;))[[1]]

img &amp;lt;- image_draw(img)
for (i in 1:example$cnt) {
  rect(x_left[i],
       y_bottom[i],
       x_right[i],
       y_top[i],
       border = &amp;quot;white&amp;quot;,
       lwd = 2)
  text(
    x = as.integer(x_right[i]),
    y = as.integer(y_top[i]),
    labels = name[i],
    offset = 1,
    pos = 2,
    cex = 1,
    col = &amp;quot;white&amp;quot;
  )
}
dev.off()
print(img)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-12-18-object-detection-concepts/images/aeroplanes.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;Now we construct the anchor boxes.&lt;/p&gt;
&lt;h3 id="anchors"&gt;Anchors&lt;/h3&gt;
&lt;p&gt;Like we said above, here we will have one anchor box per cell. Thus, grid cells and anchor boxes, in our case, are the same thing, and we’ll call them by both names, interchangingly, depending on the context. Just keep in mind that in more complex models, these will most probably be different entities.&lt;/p&gt;
&lt;p&gt;Our grid will be of size 4x4. We will need the cells’ coordinates, and we’ll start with a &lt;em&gt;center x - center y - height - width&lt;/em&gt; representation.&lt;/p&gt;
&lt;p&gt;Here, first, are the center coordinates.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;cells_per_row &amp;lt;- 4
gridsize &amp;lt;- 1/cells_per_row
anchor_offset &amp;lt;- 1 / (cells_per_row * 2) 

anchor_xs &amp;lt;- seq(anchor_offset, 1 - anchor_offset, length.out = 4) %&amp;gt;%
  rep(each = cells_per_row)
anchor_ys &amp;lt;- seq(anchor_offset, 1 - anchor_offset, length.out = 4) %&amp;gt;%
  rep(cells_per_row)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can plot them.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggplot(data.frame(x = anchor_xs, y = anchor_ys), aes(x, y)) +
  geom_point() +
  coord_cartesian(xlim = c(0,1), ylim = c(0,1)) +
  theme(aspect.ratio = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-12-18-object-detection-concepts/images/grid_centers.png" style="width: 400px; " /&gt;&lt;/p&gt;
&lt;p&gt;The center coordinates are supplemented by height and width:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;anchor_centers &amp;lt;- cbind(anchor_xs, anchor_ys)
anchor_height_width &amp;lt;- matrix(1 / cells_per_row, nrow = 16, ncol = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Combining centers, heights and widths gives us the first representation.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;anchors &amp;lt;- cbind(anchor_centers, anchor_height_width)
anchors&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;       [,1]  [,2] [,3] [,4]
 [1,] 0.125 0.125 0.25 0.25
 [2,] 0.125 0.375 0.25 0.25
 [3,] 0.125 0.625 0.25 0.25
 [4,] 0.125 0.875 0.25 0.25
 [5,] 0.375 0.125 0.25 0.25
 [6,] 0.375 0.375 0.25 0.25
 [7,] 0.375 0.625 0.25 0.25
 [8,] 0.375 0.875 0.25 0.25
 [9,] 0.625 0.125 0.25 0.25
[10,] 0.625 0.375 0.25 0.25
[11,] 0.625 0.625 0.25 0.25
[12,] 0.625 0.875 0.25 0.25
[13,] 0.875 0.125 0.25 0.25
[14,] 0.875 0.375 0.25 0.25
[15,] 0.875 0.625 0.25 0.25
[16,] 0.875 0.875 0.25 0.25&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In subsequent manipulations, we will sometimes we need a different representation: the corners (&lt;em&gt;top-left, top-right, bottom-right, bottom-left&lt;/em&gt;) of the grid cells.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;hw2corners &amp;lt;- function(centers, height_width) {
  cbind(centers - height_width / 2, centers + height_width / 2) %&amp;gt;% unname()
}

# cells are indicated by (xl, yt, xr, yb)
# successive rows first go down in the image, then to the right
anchor_corners &amp;lt;- hw2corners(anchor_centers, anchor_height_width)
anchor_corners&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      [,1] [,2] [,3] [,4]
 [1,] 0.00 0.00 0.25 0.25
 [2,] 0.00 0.25 0.25 0.50
 [3,] 0.00 0.50 0.25 0.75
 [4,] 0.00 0.75 0.25 1.00
 [5,] 0.25 0.00 0.50 0.25
 [6,] 0.25 0.25 0.50 0.50
 [7,] 0.25 0.50 0.50 0.75
 [8,] 0.25 0.75 0.50 1.00
 [9,] 0.50 0.00 0.75 0.25
[10,] 0.50 0.25 0.75 0.50
[11,] 0.50 0.50 0.75 0.75
[12,] 0.50 0.75 0.75 1.00
[13,] 0.75 0.00 1.00 0.25
[14,] 0.75 0.25 1.00 0.50
[15,] 0.75 0.50 1.00 0.75
[16,] 0.75 0.75 1.00 1.00&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take our sample image again and plot it, this time including the grid cells. Note that we display the scaled image now - the way the network is going to see it.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;example &amp;lt;- imageinfo4ssd[5, ]
name &amp;lt;- (example$name %&amp;gt;% str_split(pattern = &amp;quot;, &amp;quot;))[[1]]
x_left &amp;lt;- (example$xl %&amp;gt;% str_split(pattern = &amp;quot;, &amp;quot;))[[1]]
x_right &amp;lt;- (example$xr %&amp;gt;% str_split(pattern = &amp;quot;, &amp;quot;))[[1]]
y_top &amp;lt;- (example$yt %&amp;gt;% str_split(pattern = &amp;quot;, &amp;quot;))[[1]]
y_bottom &amp;lt;- (example$yb %&amp;gt;% str_split(pattern = &amp;quot;, &amp;quot;))[[1]]


img &amp;lt;- image_read(file.path(img_dir, example$file_name))
img &amp;lt;- image_resize(img, geometry = &amp;quot;224x224!&amp;quot;)
img &amp;lt;- image_draw(img)

for (i in 1:example$cnt) {
  rect(x_left[i],
       y_bottom[i],
       x_right[i],
       y_top[i],
       border = &amp;quot;white&amp;quot;,
       lwd = 2)
  text(
    x = as.integer(x_right[i]),
    y = as.integer(y_top[i]),
    labels = name[i],
    offset = 0,
    pos = 2,
    cex = 1,
    col = &amp;quot;white&amp;quot;
  )
}
for (i in 1:nrow(anchor_corners)) {
  rect(
    anchor_corners[i, 1] * 224,
    anchor_corners[i, 4] * 224,
    anchor_corners[i, 3] * 224,
    anchor_corners[i, 2] * 224,
    border = &amp;quot;cyan&amp;quot;,
    lwd = 1,
    lty = 3
  )
}

dev.off()
print(img)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-12-18-object-detection-concepts/images/img_wgrid.jpeg" style="width: 400px; " /&gt;&lt;/p&gt;
&lt;p&gt;Now it’s time to address the possibly greatest mystery when you’re new to object detection: How do you actually construct the ground truth input to the network?&lt;/p&gt;
&lt;p&gt;That is the so-called “matching problem”.&lt;/p&gt;
&lt;h3 id="matching-problem"&gt;Matching problem&lt;/h3&gt;
&lt;p&gt;To train the network, we need to assign the ground truth boxes to the grid cells/anchor boxes. We do this based on overlap between &lt;em&gt;bounding boxes&lt;/em&gt; on the one hand, and &lt;em&gt;anchor boxes&lt;/em&gt; on the other. Overlap is computed using &lt;em&gt;Intersection over Union&lt;/em&gt; (IoU, =&lt;em&gt;Jaccard Index&lt;/em&gt;), as usual.&lt;/p&gt;
&lt;p&gt;Assume we’ve already computed the Jaccard index for all ground truth box - grid cell combinations. We then use the following algorithm:&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;For each ground truth object, find the grid cell it maximally overlaps with.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For each grid cell, find the object it overlaps with most.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In both cases, identify the &lt;em&gt;entity&lt;/em&gt; of greatest overlap as well as the &lt;em&gt;amount&lt;/em&gt; of overlap.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When criterium (1) applies, it overrides criterium (2).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When criterium (1) applies, set the amount overlap to a constant, high value: 1.99.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Return the combined result, that is, for each grid cell, the object and amount of best (as per the above criteria) overlap.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here’s the implementation.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# overlaps shape is: number of ground truth objects * number of grid cells
map_to_ground_truth &amp;lt;- function(overlaps) {
  
  # for each ground truth object, find maximally overlapping cell (crit. 1)
  # measure of overlap, shape: number of ground truth objects
  prior_overlap &amp;lt;- apply(overlaps, 1, max)
  # which cell is this, for each object
  prior_idx &amp;lt;- apply(overlaps, 1, which.max)
  
  # for each grid cell, what object does it overlap with most (crit. 2)
  # measure of overlap, shape: number of grid cells
  gt_overlap &amp;lt;-  apply(overlaps, 2, max)
  # which object is this, for each cell
  gt_idx &amp;lt;- apply(overlaps, 2, which.max)
  
  # set all definitely overlapping cells to respective object (crit. 1)
  gt_overlap[prior_idx] &amp;lt;- 1.99
  
  # now still set all others to best match by crit. 2
  # actually it&amp;#39;s other way round, we start from (2) and overwrite with (1)
  for (i in 1:length(prior_idx)) {
    # iterate over all cells &amp;quot;absolutely assigned&amp;quot;
    p &amp;lt;- prior_idx[i] # get respective grid cell
    gt_idx[p] &amp;lt;- i # assign this cell the object number
  }
  
  # return: for each grid cell, object it overlaps with most + measure of overlap
  list(gt_overlap, gt_idx)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now here’s the IoU calculation we need for that. We can’t just use the &lt;code&gt;IoU&lt;/code&gt; function from the previous post because this time, we want to compute overlaps with all grid cells simultaneously. It’s easiest to do this using tensors, so we temporarily convert the R matrices to tensors:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# compute IOU
jaccard &amp;lt;- function(bbox, anchor_corners) {
  bbox &amp;lt;- k_constant(bbox)
  anchor_corners &amp;lt;- k_constant(anchor_corners)
  intersection &amp;lt;- intersect(bbox, anchor_corners)
  union &amp;lt;-
    k_expand_dims(box_area(bbox), axis = 2)  + k_expand_dims(box_area(anchor_corners), axis = 1) - intersection
    res &amp;lt;- intersection / union
  res %&amp;gt;% k_eval()
}

# compute intersection for IOU
intersect &amp;lt;- function(box1, box2) {
  box1_a &amp;lt;- box1[, 3:4] %&amp;gt;% k_expand_dims(axis = 2)
  box2_a &amp;lt;- box2[, 3:4] %&amp;gt;% k_expand_dims(axis = 1)
  max_xy &amp;lt;- k_minimum(box1_a, box2_a)
  
  box1_b &amp;lt;- box1[, 1:2] %&amp;gt;% k_expand_dims(axis = 2)
  box2_b &amp;lt;- box2[, 1:2] %&amp;gt;% k_expand_dims(axis = 1)
  min_xy &amp;lt;- k_maximum(box1_b, box2_b)
  
  intersection &amp;lt;- k_clip(max_xy - min_xy, min = 0, max = Inf)
  intersection[, , 1] * intersection[, , 2]
  
}

box_area &amp;lt;- function(box) {
  (box[, 3] - box[, 1]) * (box[, 4] - box[, 2]) 
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By now you might be wondering - when does all this happen? Interestingly, the example we’re following, &lt;a href="https://github.com/fastai/fastai/blob/master/courses/dl2/pascal-multi.ipynb"&gt;fast.ai’s object detection notebook&lt;/a&gt;, does all this as part of the loss calculation! In TensorFlow, this is possible in principle (requiring some juggling of &lt;code&gt;tf$cond&lt;/code&gt;, &lt;code&gt;tf$while_loop&lt;/code&gt; etc., as well as a bit of creativity finding replacements for non-differentiable operations). But, simple facts - like the Keras loss function expecting the same shapes for &lt;code&gt;y_true&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; - made it impossible to follow the &lt;em&gt;fast.ai&lt;/em&gt; approach. Instead, all matching will take place in the data generator.&lt;/p&gt;
&lt;h3 id="data-generator"&gt;Data generator&lt;/h3&gt;
&lt;p&gt;The generator has the familiar structure, known from the predecessor post. Here is the complete code - we’ll talk through the details immediately.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;batch_size &amp;lt;- 16
image_size &amp;lt;- target_width # same as height

threshold &amp;lt;- 0.4

class_background &amp;lt;- 21

ssd_generator &amp;lt;-
  function(data,
           target_height,
           target_width,
           shuffle,
           batch_size) {
    i &amp;lt;- 1
    function() {
      if (shuffle) {
        indices &amp;lt;- sample(1:nrow(data), size = batch_size)
      } else {
        if (i + batch_size &amp;gt;= nrow(data))
          i &amp;lt;&amp;lt;- 1
        indices &amp;lt;- c(i:min(i + batch_size - 1, nrow(data)))
        i &amp;lt;&amp;lt;- i + length(indices)
      }
      
      x &amp;lt;-
        array(0, dim = c(length(indices), target_height, target_width, 3))
      y1 &amp;lt;- array(0, dim = c(length(indices), 16))
      y2 &amp;lt;- array(0, dim = c(length(indices), 16, 4))
      
      for (j in 1:length(indices)) {
        x[j, , , ] &amp;lt;-
          load_and_preprocess_image(data[[indices[j], &amp;quot;file_name&amp;quot;]], target_height, target_width)
        
        class_string &amp;lt;- data[indices[j], ]$categories
        xl_string &amp;lt;- data[indices[j], ]$xl
        yt_string &amp;lt;- data[indices[j], ]$yt
        xr_string &amp;lt;- data[indices[j], ]$xr
        yb_string &amp;lt;- data[indices[j], ]$yb
        
        classes &amp;lt;-  str_split(class_string, pattern = &amp;quot;, &amp;quot;)[[1]]
        xl &amp;lt;-
          str_split(xl_string, pattern = &amp;quot;, &amp;quot;)[[1]] %&amp;gt;% as.double() %&amp;gt;% `/`(image_size)
        yt &amp;lt;-
          str_split(yt_string, pattern = &amp;quot;, &amp;quot;)[[1]] %&amp;gt;% as.double() %&amp;gt;% `/`(image_size)
        xr &amp;lt;-
          str_split(xr_string, pattern = &amp;quot;, &amp;quot;)[[1]] %&amp;gt;% as.double() %&amp;gt;% `/`(image_size)
        yb &amp;lt;-
          str_split(yb_string, pattern = &amp;quot;, &amp;quot;)[[1]] %&amp;gt;% as.double() %&amp;gt;% `/`(image_size)
    
        # rows are objects, columns are coordinates (xl, yt, xr, yb)
        # anchor_corners are 16 rows with corresponding coordinates
        bbox &amp;lt;- cbind(xl, yt, xr, yb)
        overlaps &amp;lt;- jaccard(bbox, anchor_corners)
        
        c(gt_overlap, gt_idx) %&amp;lt;-% map_to_ground_truth(overlaps)
        gt_class &amp;lt;- classes[gt_idx]
        
        pos &amp;lt;- gt_overlap &amp;gt; threshold
        gt_class[gt_overlap &amp;lt; threshold] &amp;lt;- 21
                
        # columns correspond to objects
        boxes &amp;lt;- rbind(xl, yt, xr, yb)
        # columns correspond to object boxes according to gt_idx
        gt_bbox &amp;lt;- boxes[, gt_idx]
        # set those with non-sufficient overlap to 0
        gt_bbox[, !pos] &amp;lt;- 0
        gt_bbox &amp;lt;- gt_bbox %&amp;gt;% t()
        
        y1[j, ] &amp;lt;- as.integer(gt_class) - 1
        y2[j, , ] &amp;lt;- gt_bbox
        
      }

      x &amp;lt;- x %&amp;gt;% imagenet_preprocess_input()
      y1 &amp;lt;- y1 %&amp;gt;% to_categorical(num_classes = class_background)
      list(x, list(y1, y2))
    }
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before the generator can trigger any calculations, it needs to first split apart the multiple classes and bounding box coordinates that come in one row of the dataset.&lt;/p&gt;
&lt;p&gt;To make this more concrete, we show what happens for the “2 people and 2 airplanes” image we just displayed.&lt;/p&gt;
&lt;p&gt;We copy out code chunk-by-chunk from the generator so results can actually be displayed for inspection.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;data &amp;lt;- imageinfo4ssd
indices &amp;lt;- 1:8

j &amp;lt;- 5 # this is our image

class_string &amp;lt;- data[indices[j], ]$categories
xl_string &amp;lt;- data[indices[j], ]$xl
yt_string &amp;lt;- data[indices[j], ]$yt
xr_string &amp;lt;- data[indices[j], ]$xr
yb_string &amp;lt;- data[indices[j], ]$yb
        
classes &amp;lt;-  str_split(class_string, pattern = &amp;quot;, &amp;quot;)[[1]]
xl &amp;lt;- str_split(xl_string, pattern = &amp;quot;, &amp;quot;)[[1]] %&amp;gt;% as.double() %&amp;gt;% `/`(image_size)
yt &amp;lt;- str_split(yt_string, pattern = &amp;quot;, &amp;quot;)[[1]] %&amp;gt;% as.double() %&amp;gt;% `/`(image_size)
xr &amp;lt;- str_split(xr_string, pattern = &amp;quot;, &amp;quot;)[[1]] %&amp;gt;% as.double() %&amp;gt;% `/`(image_size)
yb &amp;lt;- str_split(yb_string, pattern = &amp;quot;, &amp;quot;)[[1]] %&amp;gt;% as.double() %&amp;gt;% `/`(image_size)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So here are that image’s &lt;code&gt;classes&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;classes&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;1&amp;quot;  &amp;quot;1&amp;quot;  &amp;quot;15&amp;quot; &amp;quot;15&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And its left bounding box coordinates:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;xl&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.20535714 0.26339286 0.38839286 0.04910714&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can &lt;code&gt;cbind&lt;/code&gt; those vectors together to obtain a object (&lt;code&gt;bbox&lt;/code&gt;) where rows are objects, and coordinates are in the columns:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# rows are objects, columns are coordinates (xl, yt, xr, yb)
bbox &amp;lt;- cbind(xl, yt, xr, yb)
bbox&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;          xl        yt         xr        yb
[1,] 0.20535714 0.2723214 0.75000000 0.6473214
[2,] 0.26339286 0.3080357 0.39285714 0.4330357
[3,] 0.38839286 0.6383929 0.42410714 0.8125000
[4,] 0.04910714 0.6696429 0.08482143 0.8437500&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we’re ready to compute these boxes’ overlap with all of the 16 grid cells. Recall that &lt;code&gt;anchor_corners&lt;/code&gt; stores the grid cells in an analogous way, the cells being in the rows and the coordinates in the columns.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# anchor_corners are 16 rows with corresponding coordinates
overlaps &amp;lt;- jaccard(bbox, anchor_corners)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have the overlaps, we can call the matching logic:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;c(gt_overlap, gt_idx) %&amp;lt;-% map_to_ground_truth(overlaps)
gt_overlap&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] 0.00000000 0.03961473 0.04358353 1.99000000 0.00000000 1.99000000 1.99000000 0.03357313 0.00000000
[10] 0.27127662 0.16019417 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking for the value &lt;code&gt;1.99&lt;/code&gt; in the above - the value indicating maximal, by the above criteria, overlap of an object with a grid cell - we see that box 4 (counting in column-major order here like R does) got matched (to a person, as we’ll see soon), box 6 did (to an airplane), and box 7 did (to a person). How about the other airplane? It got lost in the matching.&lt;/p&gt;
&lt;p&gt;This is not a problem of the matching algorithm though - it would disappear if we had more than one anchor box per grid cell.&lt;/p&gt;
&lt;p&gt;Looking for the objects just mentioned in the class index, &lt;code&gt;gt_idx&lt;/code&gt;, we see that indeed box 4 got matched to object 4 (a person), box 6 got matched to object 2 (an airplane), and box 7 got matched to object 3 (the other person):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;gt_idx&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1 1 4 4 1 2 3 3 1 1 1 1 1 1 1 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By the way, don’t worry about the abundance of &lt;code&gt;1&lt;/code&gt;s here. These are remnants from using &lt;code&gt;which.max&lt;/code&gt; to determine maximal overlap, and will disappear soon.&lt;/p&gt;
&lt;p&gt;Instead of thinking in object numbers, we should think in object classes (the respective numerical codes, that is).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;gt_class &amp;lt;- classes[gt_idx]
gt_class&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] &amp;quot;1&amp;quot;  &amp;quot;1&amp;quot;  &amp;quot;15&amp;quot; &amp;quot;15&amp;quot; &amp;quot;1&amp;quot;  &amp;quot;1&amp;quot;  &amp;quot;15&amp;quot; &amp;quot;15&amp;quot; &amp;quot;1&amp;quot;  &amp;quot;1&amp;quot;  &amp;quot;1&amp;quot;  &amp;quot;1&amp;quot;  &amp;quot;1&amp;quot;  &amp;quot;1&amp;quot;  &amp;quot;1&amp;quot;  &amp;quot;1&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So far, we take into account even the very slightest overlap - of 0.1 percent, say. Of course, this makes no sense. We set all cells with an overlap &amp;lt; 0.4 to the background class:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pos &amp;lt;- gt_overlap &amp;gt; threshold
gt_class[gt_overlap &amp;lt; threshold] &amp;lt;- 21

gt_class&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;21&amp;quot; &amp;quot;21&amp;quot; &amp;quot;21&amp;quot; &amp;quot;15&amp;quot; &amp;quot;21&amp;quot; &amp;quot;1&amp;quot;  &amp;quot;15&amp;quot; &amp;quot;21&amp;quot; &amp;quot;21&amp;quot; &amp;quot;21&amp;quot; &amp;quot;21&amp;quot; &amp;quot;21&amp;quot; &amp;quot;21&amp;quot; &amp;quot;21&amp;quot; &amp;quot;21&amp;quot; &amp;quot;21&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, to construct the targets for learning, we need to put the mapping we found into a data structure.&lt;/p&gt;
&lt;p&gt;The following gives us a 16x4 matrix of cells and the boxes they are responsible for:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;orig_boxes &amp;lt;- rbind(xl, yt, xr, yb)
# columns correspond to object boxes according to gt_idx
gt_bbox &amp;lt;- orig_boxes[, gt_idx]
# set those with non-sufficient overlap to 0
gt_bbox[, !pos] &amp;lt;- 0
gt_bbox &amp;lt;- gt_bbox %&amp;gt;% t()

gt_bbox&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;              xl        yt         xr        yb
 [1,] 0.00000000 0.0000000 0.00000000 0.0000000
 [2,] 0.00000000 0.0000000 0.00000000 0.0000000
 [3,] 0.00000000 0.0000000 0.00000000 0.0000000
 [4,] 0.04910714 0.6696429 0.08482143 0.8437500
 [5,] 0.00000000 0.0000000 0.00000000 0.0000000
 [6,] 0.26339286 0.3080357 0.39285714 0.4330357
 [7,] 0.38839286 0.6383929 0.42410714 0.8125000
 [8,] 0.00000000 0.0000000 0.00000000 0.0000000
 [9,] 0.00000000 0.0000000 0.00000000 0.0000000
[10,] 0.00000000 0.0000000 0.00000000 0.0000000
[11,] 0.00000000 0.0000000 0.00000000 0.0000000
[12,] 0.00000000 0.0000000 0.00000000 0.0000000
[13,] 0.00000000 0.0000000 0.00000000 0.0000000
[14,] 0.00000000 0.0000000 0.00000000 0.0000000
[15,] 0.00000000 0.0000000 0.00000000 0.0000000
[16,] 0.00000000 0.0000000 0.00000000 0.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Together, &lt;code&gt;gt_bbox&lt;/code&gt; and &lt;code&gt;gt_class&lt;/code&gt; make up the network’s learning targets.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;y1[j, ] &amp;lt;- as.integer(gt_class) - 1
y2[j, , ] &amp;lt;- gt_bbox&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To summarize, our target is a list of two outputs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the bounding box ground truth of dimensionality &lt;em&gt;number of grid cells&lt;/em&gt; times &lt;em&gt;number of box coordinates&lt;/em&gt;, and&lt;/li&gt;
&lt;li&gt;the class ground truth of size &lt;em&gt;number of grid cells&lt;/em&gt; times &lt;em&gt;number of classes&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can verify this by asking the generator for a batch of inputs and targets:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_gen &amp;lt;- ssd_generator(
  imageinfo4ssd,
  target_height = target_height,
  target_width = target_width,
  shuffle = TRUE,
  batch_size = batch_size
)

batch &amp;lt;- train_gen()
c(x, c(y1, y2)) %&amp;lt;-% batch
dim(y1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 16 16 21&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;dim(y2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 16 16  4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we’re ready for the model.&lt;/p&gt;
&lt;h3 id="the-model"&gt;The model&lt;/h3&gt;
&lt;p&gt;We start from Resnet 50 as a feature extractor. This gives us tensors of size 7x7x2048.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;feature_extractor &amp;lt;- application_resnet50(
  include_top = FALSE,
  input_shape = c(224, 224, 3)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we append a few conv layers. Three of those layers are “just” there for capacity; the last one though has a additional task: By virtue of &lt;code&gt;strides = 2&lt;/code&gt;, it downsamples its input to from 7x7 to 4x4 in the height/width dimensions.&lt;/p&gt;
&lt;p&gt;This resolution of 4x4 gives us exactly the grid we need!&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;input &amp;lt;- feature_extractor$input

common &amp;lt;- feature_extractor$output %&amp;gt;%
  layer_conv_2d(
    filters = 256,
    kernel_size = 3,
    padding = &amp;quot;same&amp;quot;,
    activation = &amp;quot;relu&amp;quot;,
    name = &amp;quot;head_conv1_1&amp;quot;
  ) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_conv_2d(
    filters = 256,
    kernel_size = 3,
    padding = &amp;quot;same&amp;quot;,
    activation = &amp;quot;relu&amp;quot;,
    name = &amp;quot;head_conv1_2&amp;quot;
  ) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_conv_2d(
    filters = 256,
    kernel_size = 3,
    padding = &amp;quot;same&amp;quot;,
    activation = &amp;quot;relu&amp;quot;,
    name = &amp;quot;head_conv1_3&amp;quot;
  ) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_conv_2d(
    filters = 256,
    kernel_size = 3,
    strides = 2,
    padding = &amp;quot;same&amp;quot;,
    activation = &amp;quot;relu&amp;quot;,
    name = &amp;quot;head_conv2&amp;quot;
  ) %&amp;gt;%
  layer_batch_normalization() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can do as we did in that other post, attach one output for the bounding boxes and one for the classes.&lt;/p&gt;
&lt;p&gt;Note how we don’t aggregate over the spatial grid though. Instead, we reshape it so the 4x4 grid cells appear sequentially.&lt;/p&gt;
&lt;p&gt;Here first is the class output. We have 21 classes (the 20 classes from PASCAL, plus background), and we need to classify each cell. We thus end up with an output of size 16x21.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;class_output &amp;lt;-
  layer_conv_2d(
    common,
    filters = 21,
    kernel_size = 3,
    padding = &amp;quot;same&amp;quot;,
    name = &amp;quot;class_conv&amp;quot;
  ) %&amp;gt;%
  layer_reshape(target_shape = c(16, 21), name = &amp;quot;class_output&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the bounding box output, we apply a &lt;code&gt;tanh&lt;/code&gt; activation so that values lie between -1 and 1. This is because they are used to compute offsets to the grid cell centers.&lt;/p&gt;
&lt;p&gt;These computations happen in the &lt;code&gt;layer_lambda&lt;/code&gt;. We start from the actual anchor box centers, and move them around by a scaled-down version of the activations. We then convert these to anchor corners - same as we did above with the ground truth anchors, just operating on tensors, this time.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;bbox_output &amp;lt;-
  layer_conv_2d(
    common,
    filters = 4,
    kernel_size = 3,
    padding = &amp;quot;same&amp;quot;,
    name = &amp;quot;bbox_conv&amp;quot;
  ) %&amp;gt;%
  layer_reshape(target_shape = c(16, 4), name = &amp;quot;bbox_flatten&amp;quot;) %&amp;gt;%
  layer_activation(&amp;quot;tanh&amp;quot;) %&amp;gt;%
  layer_lambda(
    f = function(x) {
      activation_centers &amp;lt;-
        (x[, , 1:2] / 2 * gridsize) + k_constant(anchors[, 1:2])
      activation_height_width &amp;lt;-
        (x[, , 3:4] / 2 + 1) * k_constant(anchors[, 3:4])
      activation_corners &amp;lt;-
        k_concatenate(
          list(
            activation_centers - activation_height_width / 2,
            activation_centers + activation_height_width / 2
          )
        )
     activation_corners
    },
    name = &amp;quot;bbox_output&amp;quot;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have all layers, let’s quickly finish up the model definition:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model(
  inputs = input,
  outputs = list(class_output, bbox_output)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last ingredient missing, then, is the loss function.&lt;/p&gt;
&lt;h3 id="loss"&gt;Loss&lt;/h3&gt;
&lt;p&gt;To the model’s two outputs - a classification output and a regression output - correspond two losses, just as in the basic classification + localization model. Only this time, we have 16 grid cells to take care of.&lt;/p&gt;
&lt;p&gt;Class loss uses &lt;code&gt;tf$nn$sigmoid_cross_entropy_with_logits&lt;/code&gt; to compute the binary crossentropy between targets and unnormalized network activation, summing over grid cells and dividing by the number of classes.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# shapes are batch_size * 16 * 21
class_loss &amp;lt;- function(y_true, y_pred) {

  class_loss  &amp;lt;-
    tf$nn$sigmoid_cross_entropy_with_logits(labels = y_true, logits = y_pred)

  class_loss &amp;lt;-
    tf$reduce_sum(class_loss) / tf$cast(n_classes + 1, &amp;quot;float32&amp;quot;)
  
  class_loss
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Localization loss is calculated for all boxes where in fact there &lt;em&gt;is&lt;/em&gt; an object present in the ground truth. All other activations get masked out.&lt;/p&gt;
&lt;p&gt;The loss itself then is just mean absolute error, scaled by a multiplier designed to bring both loss components to similar magnitudes. In practice, it makes sense to experiment a bit here.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# shapes are batch_size * 16 * 4
bbox_loss &amp;lt;- function(y_true, y_pred) {

  # calculate localization loss for all boxes where ground truth was assigned some overlap
  # calculate mask
  pos &amp;lt;- y_true[, , 1] + y_true[, , 3] &amp;gt; 0
  pos &amp;lt;-
    pos %&amp;gt;% k_cast(tf$float32) %&amp;gt;% k_reshape(shape = c(batch_size, 16, 1))
  pos &amp;lt;-
    tf$tile(pos, multiples = k_constant(c(1L, 1L, 4L), dtype = tf$int32))
    
  diff &amp;lt;- y_pred - y_true
  # mask out irrelevant activations
  diff &amp;lt;- diff %&amp;gt;% tf$multiply(pos)
  
  loc_loss &amp;lt;- diff %&amp;gt;% tf$abs() %&amp;gt;% tf$reduce_mean()
  loc_loss * 100
}&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id="training"&gt;Training&lt;/h1&gt;
&lt;p&gt;Above, we’ve already defined the model but we still need to freeze the feature detector’s weights and compile it.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% freeze_weights()
model %&amp;gt;% unfreeze_weights(from = &amp;quot;head_conv1_1&amp;quot;)
model&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% compile(
  loss = list(class_loss, bbox_loss),
  optimizer = &amp;quot;adam&amp;quot;,
  metrics = list(
    class_output = custom_metric(&amp;quot;class_loss&amp;quot;, metric_fn = class_loss),
    bbox_output = custom_metric(&amp;quot;bbox_loss&amp;quot;, metric_fn = bbox_loss)
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we’re ready to train. Training this model is very time consuming, such that for applications “in the real world”, we might want to do optimize the program for memory consumption and runtime. Like we said above, in this post we’re really focusing on understanding the approach.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;steps_per_epoch &amp;lt;- nrow(imageinfo4ssd) / batch_size

model %&amp;gt;% fit_generator(
  train_gen,
  steps_per_epoch = steps_per_epoch,
  epochs = 5,
  callbacks = callback_model_checkpoint(
    &amp;quot;weights.{epoch:02d}-{loss:.2f}.hdf5&amp;quot;, 
    save_weights_only = TRUE
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After 5 epochs, this is what we get from the model. It’s on the right way, but it will need many more epochs to reach decent performance.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-12-18-object-detection-concepts/images/results.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;Apart from training for many more epochs, what could we do? We’ll wrap up the post with two directions for improvement, but won’t implement them completely.&lt;/p&gt;
&lt;p&gt;The first one actually &lt;em&gt;is&lt;/em&gt; quick to implement. Here we go.&lt;/p&gt;
&lt;h2 id="focal-loss"&gt;Focal loss&lt;/h2&gt;
&lt;p&gt;Above, we were using cross entropy for the classification loss. Let’s look at what that entails.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-12-18-object-detection-concepts/images/xent.png" alt="" /&gt;
&lt;p class="caption"&gt;Binary cross entropy for predictions when the ground truth equals 1&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The figure shows loss incurred when the correct answer is 1. We see that even though loss is highest when the network is very wrong, it still incurs significant loss when it’s “right for all practical purposes” - meaning, its output is just above 0.5.&lt;/p&gt;
&lt;p&gt;In cases of strong class imbalance, this behavior can be problematic. Much training energy is wasted on getting “even more right” on cases where the net is right already - as will happen with instances of the dominant class. Instead, the network should dedicate more effort to the hard cases - exemplars of the rarer classes.&lt;/p&gt;
&lt;p&gt;In object detection, the prevalent class is &lt;em&gt;background&lt;/em&gt; - no class, really. Instead of getting more and more proficient at predicting background, the network had better learn how to tell apart the actual object classes.&lt;/p&gt;
&lt;p&gt;An alternative was pointed out by the authors of the RetinaNet paper&lt;span class="citation"&gt;(Lin et al. 2017)&lt;/span&gt;: They introduced a parameter &lt;span class="math inline"&gt;\(\gamma\)&lt;/span&gt;&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; that results in decreasing loss for samples that already have been well classified.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-12-18-object-detection-concepts/images/focal.png" class="external" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Focal loss downweights contributions from well-classified examples. Figure from &lt;span class="citation"&gt;(Lin et al. 2017)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Different implementations are found on the net, as well as different settings for the hyperparameters. Here’s a direct port of the &lt;em&gt;fast.ai&lt;/em&gt; code:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;alpha &amp;lt;- 0.25
gamma &amp;lt;- 1

get_weights &amp;lt;- function(y_true, y_pred) {
  p &amp;lt;- y_pred %&amp;gt;% k_sigmoid()
  pt &amp;lt;-  y_true*p + (1-p)*(1-y_true)
  w &amp;lt;- alpha*y_true + (1-alpha)*(1-y_true)
  w &amp;lt;-  w * (1-pt)^gamma
  w
}

class_loss_focal  &amp;lt;- function(y_true, y_pred) {
  
  w &amp;lt;- get_weights(y_true, y_pred)
  cx &amp;lt;- tf$nn$sigmoid_cross_entropy_with_logits(labels = y_true, logits = y_pred)
  weighted_cx &amp;lt;- w * cx

  class_loss &amp;lt;-
   tf$reduce_sum(weighted_cx) / tf$cast(21, &amp;quot;float32&amp;quot;)
  
  class_loss
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From testing this loss, it seems to yield better performance, but does not render obsolete the need for substantive training time.&lt;/p&gt;
&lt;p&gt;Finally, let’s see what we’d have to do if we wanted to use several anchor boxes per grid cells.&lt;/p&gt;
&lt;h2 id="more-anchor-boxes"&gt;More anchor boxes&lt;/h2&gt;
&lt;p&gt;The “real SSD” has anchor boxes of different aspect ratios, and it puts detectors at different stages of the network. Let’s implement this.&lt;/p&gt;
&lt;h3 id="anchor-box-coordinates"&gt;Anchor box coordinates&lt;/h3&gt;
&lt;p&gt;We create anchor boxes as combinations of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;different scales, and&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;anchor_zooms &amp;lt;- c(0.7, 1, 1.3)
anchor_zooms&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.7 1.0 1.3&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;different aspect ratios:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;anchor_ratios &amp;lt;- matrix(c(1, 1, 1, 0.5, 0.5, 1), ncol = 2, byrow = TRUE)
anchor_ratios&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;     [,1] [,2]
[1,]  1.0  1.0
[2,]  1.0  0.5
[3,]  0.5  1.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, we have nine different combinations:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;anchor_scales &amp;lt;- rbind(
  anchor_ratios * anchor_zooms[1],
  anchor_ratios * anchor_zooms[2],
  anchor_ratios * anchor_zooms[3]
)

k &amp;lt;- nrow(anchor_scales)

anchor_scales&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      [,1] [,2]
 [1,] 0.70 0.70
 [2,] 0.70 0.35
 [3,] 0.35 0.70
 [4,] 1.00 1.00
 [5,] 1.00 0.50
 [6,] 0.50 1.00
 [7,] 1.30 1.30
 [8,] 1.30 0.65
 [9,] 0.65 1.30&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We place detectors at three stages. Resolutions will be 4x4 (as we had before) and additionally, 2x2 and 1x1:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;anchor_grids &amp;lt;- c(4,2,1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once that’s been determined, we can compute&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;x coordinates of the box centers:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;anchor_offsets &amp;lt;- 1/(anchor_grids * 2)

anchor_x &amp;lt;- map(
  1:3,
  function(x) rep(seq(anchor_offsets[x],
                      1 - anchor_offsets[x],
                      length.out = anchor_grids[x]),
                  each = anchor_grids[x])) %&amp;gt;%
  flatten() %&amp;gt;%
  unlist()&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;y coordinates of the box centers:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;anchor_y &amp;lt;- map(
  1:3,
  function(y) rep(seq(anchor_offsets[y],
                      1 - anchor_offsets[y],
                      length.out = anchor_grids[y]),
                  times = anchor_grids[y])) %&amp;gt;%
  flatten() %&amp;gt;%
  unlist()&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;the x-y representations of the centers:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;anchor_centers &amp;lt;- cbind(rep(anchor_x, each = k), rep(anchor_y, each = k))&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;the sizes of the boxes:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;anchor_sizes &amp;lt;- map(
  anchor_grids,
  function(x)
   matrix(rep(t(anchor_scales/x), x*x), ncol = 2, byrow = TRUE)
  ) %&amp;gt;%
  abind(along = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;the sizes of the base grids (0.25, 0.5, and 1):&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;grid_sizes &amp;lt;- c(rep(0.25, k * anchor_grids[1]^2),
                rep(0.5, k * anchor_grids[2]^2),
                rep(1, k * anchor_grids[3]^2)
                )&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;the centers-width-height representations of the anchor boxes:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;anchors &amp;lt;- cbind(anchor_centers, anchor_sizes)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;and finally, the &lt;em&gt;corners&lt;/em&gt; representation of the boxes!&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;hw2corners &amp;lt;- function(centers, height_width) {
  cbind(centers - height_width / 2, centers + height_width / 2) %&amp;gt;% unname()
}

anchor_corners &amp;lt;- hw2corners(anchors[ , 1:2], anchors[ , 3:4])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So here, then, is a plot of the (distinct) box centers: One in the middle, for the 9 large boxes, 4 for the 4 * 9 medium-size boxes, and 16 for the 16 * 9 small boxes.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-12-18-object-detection-concepts/images/box_centers.png" /&gt;&lt;/p&gt;
&lt;p&gt;Of course, even if we aren’t going to train this version, we at least need to see these in action!&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-12-18-object-detection-concepts/images/more_boxes.jpeg" style="width: 400px; " /&gt;&lt;/p&gt;
&lt;p&gt;How would a model look that could deal with these?&lt;/p&gt;
&lt;h3 id="model"&gt;Model&lt;/h3&gt;
&lt;p&gt;Again, we’d start from a feature detector …&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;feature_extractor &amp;lt;- application_resnet50(
  include_top = FALSE,
  input_shape = c(224, 224, 3)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;… and attach some custom conv layers.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;input &amp;lt;- feature_extractor$input

common &amp;lt;- feature_extractor$output %&amp;gt;%
  layer_conv_2d(
    filters = 256,
    kernel_size = 3,
    padding = &amp;quot;same&amp;quot;,
    activation = &amp;quot;relu&amp;quot;,
    name = &amp;quot;head_conv1_1&amp;quot;
  ) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_conv_2d(
    filters = 256,
    kernel_size = 3,
    padding = &amp;quot;same&amp;quot;,
    activation = &amp;quot;relu&amp;quot;,
    name = &amp;quot;head_conv1_2&amp;quot;
  ) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_conv_2d(
    filters = 256,
    kernel_size = 3,
    padding = &amp;quot;same&amp;quot;,
    activation = &amp;quot;relu&amp;quot;,
    name = &amp;quot;head_conv1_3&amp;quot;
  ) %&amp;gt;%
  layer_batch_normalization()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, things get different. We want to attach detectors (= output layers) to different stages in a pipeline of successive downsamplings. If that doesn’t call for the Keras functional API…&lt;/p&gt;
&lt;p&gt;Here’s the downsizing pipeline.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt; downscale_4x4 &amp;lt;- common %&amp;gt;%
  layer_conv_2d(
    filters = 256,
    kernel_size = 3,
    strides = 2,
    padding = &amp;quot;same&amp;quot;,
    activation = &amp;quot;relu&amp;quot;,
    name = &amp;quot;downscale_4x4&amp;quot;
  ) %&amp;gt;%
  layer_batch_normalization() &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;downscale_2x2 &amp;lt;- downscale_4x4 %&amp;gt;%
  layer_conv_2d(
    filters = 256,
    kernel_size = 3,
    strides = 2,
    padding = &amp;quot;same&amp;quot;,
    activation = &amp;quot;relu&amp;quot;,
    name = &amp;quot;downscale_2x2&amp;quot;
  ) %&amp;gt;%
  layer_batch_normalization() &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;downscale_1x1 &amp;lt;- downscale_2x2 %&amp;gt;%
  layer_conv_2d(
    filters = 256,
    kernel_size = 3,
    strides = 2,
    padding = &amp;quot;same&amp;quot;,
    activation = &amp;quot;relu&amp;quot;,
    name = &amp;quot;downscale_1x1&amp;quot;
  ) %&amp;gt;%
  layer_batch_normalization() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The bounding box output definitions get a little messier than before, as each output has to take into account its relative anchor box coordinates.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;create_bbox_output &amp;lt;- function(prev_layer, anchor_start, anchor_stop, suffix) {
  output &amp;lt;- layer_conv_2d(
    prev_layer,
    filters = 4 * k,
    kernel_size = 3,
    padding = &amp;quot;same&amp;quot;,
    name = paste0(&amp;quot;bbox_conv_&amp;quot;, suffix)
  ) %&amp;gt;%
  layer_reshape(target_shape = c(-1, 4), name = paste0(&amp;quot;bbox_flatten_&amp;quot;, suffix)) %&amp;gt;%
  layer_activation(&amp;quot;tanh&amp;quot;) %&amp;gt;%
  layer_lambda(
    f = function(x) {
      activation_centers &amp;lt;-
        (x[, , 1:2] / 2 * matrix(grid_sizes[anchor_start:anchor_stop], ncol = 1)) +
        k_constant(anchors[anchor_start:anchor_stop, 1:2])
      activation_height_width &amp;lt;-
        (x[, , 3:4] / 2 + 1) * k_constant(anchors[anchor_start:anchor_stop, 3:4])
      activation_corners &amp;lt;-
        k_concatenate(
          list(
            activation_centers - activation_height_width / 2,
            activation_centers + activation_height_width / 2
          )
        )
     activation_corners
    },
    name = paste0(&amp;quot;bbox_output_&amp;quot;, suffix)
  )
  output
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here they are: Each one attached to it’s respective stage of action in the pipeline.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;bbox_output_4x4 &amp;lt;- create_bbox_output(downscale_4x4, 1, 144, &amp;quot;4x4&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;bbox_output_2x2 &amp;lt;- create_bbox_output(downscale_2x2, 145, 180, &amp;quot;2x2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;bbox_output_1x1 &amp;lt;- create_bbox_output(downscale_1x1, 181, 189, &amp;quot;1x1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The same principle applies to the class outputs.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;create_class_output &amp;lt;- function(prev_layer, suffix) {
  output &amp;lt;-
  layer_conv_2d(
    prev_layer,
    filters = 21 * k,
    kernel_size = 3,
    padding = &amp;quot;same&amp;quot;,
    name = paste0(&amp;quot;class_conv_&amp;quot;, suffix)
  ) %&amp;gt;%
  layer_reshape(target_shape = c(-1, 21), name = paste0(&amp;quot;class_output_&amp;quot;, suffix))
  output
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;class_output_4x4 &amp;lt;- create_class_output(downscale_4x4, &amp;quot;4x4&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;class_output_2x2 &amp;lt;- create_class_output(downscale_2x2, &amp;quot;2x2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;class_output_1x1 &amp;lt;- create_class_output(downscale_1x1, &amp;quot;1x1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And glue it all together, to get the model.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model(
  inputs = input,
  outputs = list(
    bbox_output_1x1,
    bbox_output_2x2,
    bbox_output_4x4,
    class_output_1x1, 
    class_output_2x2, 
    class_output_4x4)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we will stop here. To run this, there is another element that has to be adjusted: the data generator. Our focus being on explaining the concepts though, we’ll leave that to the interested reader.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;While we haven’t ended up with a good-performing model for object detection, we do hope that we’ve managed to shed some light on the mystery of object detection. What’s a bounding box? What’s an anchor (resp. prior, rep. default) box? How do you match them up in practice?&lt;/p&gt;
&lt;p&gt;If you’ve “just” read the papers (YOLO, SSD), but never seen any code, it may seem like all actions happen in some wonderland beyond the horizon. They don’t. But coding them, as we’ve seen, can be cumbersome, even in the very basic versions we’ve implemented. To perform object detection in production, then, a lot more time has to be spent on training and tuning models. But sometimes just learning about how something works can be very satisfying.&lt;/p&gt;
&lt;p&gt;Finally, we’d again like to stress how much this post leans on what the &lt;em&gt;fast.ai&lt;/em&gt; guys did. Their work most definitely is enriching not just the PyTorch, but also the R-TensorFlow community!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-Girshick15"&gt;
&lt;p&gt;Girshick, Ross B. 2015. “Fast R-Cnn.” &lt;em&gt;CoRR&lt;/em&gt; abs/1504.08083. &lt;a href="http://arxiv.org/abs/1504.08083"&gt;http://arxiv.org/abs/1504.08083&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-GirshickDDM13"&gt;
&lt;p&gt;Girshick, Ross B., Jeff Donahue, Trevor Darrell, and Jitendra Malik. 2013. “Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.” &lt;em&gt;CoRR&lt;/em&gt; abs/1311.2524. &lt;a href="http://arxiv.org/abs/1311.2524"&gt;http://arxiv.org/abs/1311.2524&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-abs-1708-02002"&gt;
&lt;p&gt;Lin, Tsung-Yi, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Dollar. 2017. “Focal Loss for Dense Object Detection.” &lt;em&gt;CoRR&lt;/em&gt; abs/1708.02002. &lt;a href="http://arxiv.org/abs/1708.02002"&gt;http://arxiv.org/abs/1708.02002&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-LiuAESR15"&gt;
&lt;p&gt;Liu, Wei, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott E. Reed, Cheng-Yang Fu, and Alexander C. Berg. 2015. “SSD: Single Shot Multibox Detector.” &lt;em&gt;CoRR&lt;/em&gt; abs/1512.02325. &lt;a href="http://arxiv.org/abs/1512.02325"&gt;http://arxiv.org/abs/1512.02325&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-RedmonDGF15"&gt;
&lt;p&gt;Redmon, Joseph, Santosh Kumar Divvala, Ross B. Girshick, and Ali Farhadi. 2015. “You Only Look Once: Unified, Real-Time Object Detection.” &lt;em&gt;CoRR&lt;/em&gt; abs/1506.02640. &lt;a href="http://arxiv.org/abs/1506.02640"&gt;http://arxiv.org/abs/1506.02640&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-RedmonF16"&gt;
&lt;p&gt;Redmon, Joseph, and Ali Farhadi. 2016. “YOLO9000: Better, Faster, Stronger.” &lt;em&gt;CoRR&lt;/em&gt; abs/1612.08242. &lt;a href="http://arxiv.org/abs/1612.08242"&gt;http://arxiv.org/abs/1612.08242&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-abs-1804-02767"&gt;
&lt;p&gt;———. 2018. “YOLOv3: An Incremental Improvement.” &lt;em&gt;CoRR&lt;/em&gt; abs/1804.02767. &lt;a href="http://arxiv.org/abs/1804.02767"&gt;http://arxiv.org/abs/1804.02767&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-RenHG015"&gt;
&lt;p&gt;Ren, Shaoqing, Kaiming He, Ross B. Girshick, and Jian Sun. 2015. “Faster R-Cnn: Towards Real-Time Object Detection with Region Proposal Networks.” &lt;em&gt;CoRR&lt;/em&gt; abs/1506.01497. &lt;a href="http://arxiv.org/abs/1506.01497"&gt;http://arxiv.org/abs/1506.01497&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-SermanetEZMFL13"&gt;
&lt;p&gt;Sermanet, Pierre, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, and Yann LeCun. 2013. “OverFeat: Integrated Recognition, Localization and Detection Using Convolutional Networks.” &lt;em&gt;CoRR&lt;/em&gt; abs/1312.6229. &lt;a href="http://arxiv.org/abs/1312.6229"&gt;http://arxiv.org/abs/1312.6229&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;We won’t need &lt;code&gt;imageinfo_maxbb&lt;/code&gt; in this post.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;as well as &lt;span class="math inline"&gt;\(\alpha\)&lt;/span&gt;, not used in the figure&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">9e9e2ebf2bbb087d9506620acd26c554</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-12-18-object-detection-concepts</guid>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-12-18-object-detection-concepts/images/results.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Entity embeddings for fun and profit</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-11-26-embeddings-fun-and-profit</link>
      <description>


&lt;p&gt;What’s useful about embeddings? Depending on who you ask, answers may vary. For many, the most immediate association may be word vectors and their use in natural language processing (translation, summarization, question answering etc.) There, they are famous for modeling semantic and syntactic relationships, as exemplified by this diagram found in one of the most influential papers on word vectors&lt;span class="citation"&gt;(Mikolov et al. 2013)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-11-26-embeddings-fun-and-profit/images/mikolov.png" class="external" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Countries and their capital cities. Figure from &lt;span class="citation"&gt;(Mikolov et al. 2013)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Others will probably bring up &lt;em&gt;entity embeddings&lt;/em&gt;, the magic tool that helped win the Rossmann competition&lt;span class="citation"&gt;(Guo and Berkhahn 2016)&lt;/span&gt; and was greatly popularized by &lt;a href="https://course.fast.ai/"&gt;fast.ai’s deep learning course&lt;/a&gt;. Here, the idea is to make use of data that is not normally helpful in prediction, like high-dimensional categorical variables.&lt;/p&gt;
&lt;p&gt;Another (related) idea, also widely spread by fast.ai and explained in &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-09-26-embeddings-recommender/"&gt;this blog&lt;/a&gt;, is to apply embeddings to collaborative filtering. This basically builds up entity embeddings of users and items based on the criterion how well these “match” (as indicated by existing ratings).&lt;/p&gt;
&lt;p&gt;So what are embeddings good for? The way we see it, embeddings are what you make of them. The goal in this post is to provide examples of how to use embeddings to uncover relationships and improve prediction. The examples are just that - examples, chosen to demonstrate a method. The most interesting thing really will be what you make of these methods in &lt;em&gt;your&lt;/em&gt; area of work or interest.&lt;/p&gt;
&lt;h2 id="embeddings-for-fun-picturing-relationships"&gt;Embeddings for fun (picturing relationships)&lt;/h2&gt;
&lt;p&gt;Our first example will stress the “fun” part, but also show how to technically deal with categorical variables in a dataset.&lt;/p&gt;
&lt;p&gt;We’ll take this year’s &lt;a href="https://insights.stackoverflow.com/survey/2018"&gt;StackOverflow developer survey&lt;/a&gt; as a basis and pick a few categorical variables that seem interesting - stuff like “what do people value in a job” and of course, what languages and OSes do people use. Don’t take this too seriously, it’s meant to be fun and demonstrate a method, that’s all.&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="preparing-the-data"&gt;Preparing the data&lt;/h3&gt;
&lt;p&gt;Equipped with the libraries we’ll need:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(dplyr)
library(ggplot2)
library(readr)
library(keras)
library(purrr)
library(forcats)
library(ggrepel)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We load the data and zoom in on a few categorical variables. Two of them we intend to use as targets: &lt;code&gt;EthicsChoice&lt;/code&gt; and &lt;code&gt;JobSatisfaction&lt;/code&gt;. &lt;code&gt;EthicsChoice&lt;/code&gt; is one of four ethics-related questions and goes&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Imagine that you were asked to write code for a purpose or product that you consider extremely unethical. Do you write the code anyway?”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;With questions like this, it’s never clear what portion of a response should be attributed to social desirability - this question seemed like the least prone to that, which is why we chose it.&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;data &amp;lt;- read_csv(&amp;quot;survey_results_public.csv&amp;quot;)

data &amp;lt;- data %&amp;gt;% select(
  FormalEducation,
  UndergradMajor,
  starts_with(&amp;quot;AssessJob&amp;quot;),
  EthicsChoice,
  LanguageWorkedWith,
  OperatingSystem,
  EthicsChoice,
  JobSatisfaction
)

data &amp;lt;- data %&amp;gt;% mutate_if(is.character, factor)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The variables we are interested in show a tendency to have been left unanswered by quite a few respondents, so the easiest way to handle missing data here is to exclude the respective participants completely.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;data &amp;lt;- na.omit(data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That leaves us with ~48,000 completed (as far as we’re concerned) questionnaires. Looking at the variables’ contents, we see we’ll have to do something with them before we can start training.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;data %&amp;gt;% glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Observations: 48,610
Variables: 16
$ FormalEducation    &amp;lt;fct&amp;gt; Bachelor’s degree (BA, BS, B.Eng., etc.),...
$ UndergradMajor     &amp;lt;fct&amp;gt; Mathematics or statistics, A natural scie...
$ AssessJob1         &amp;lt;int&amp;gt; 10, 1, 8, 8, 5, 6, 6, 6, 9, 7, 3, 1, 6, 7...
$ AssessJob2         &amp;lt;int&amp;gt; 7, 7, 5, 5, 3, 5, 3, 9, 4, 4, 9, 7, 7, 10...
$ AssessJob3         &amp;lt;int&amp;gt; 8, 10, 7, 4, 9, 4, 7, 2, 10, 10, 10, 6, 1...
$ AssessJob4         &amp;lt;int&amp;gt; 1, 8, 1, 9, 4, 2, 4, 4, 3, 2, 6, 10, 4, 1...
$ AssessJob5         &amp;lt;int&amp;gt; 2, 2, 2, 1, 1, 7, 1, 3, 1, 1, 8, 9, 2, 4,...
$ AssessJob6         &amp;lt;int&amp;gt; 5, 5, 6, 3, 8, 8, 5, 5, 6, 5, 7, 4, 5, 5,...
$ AssessJob7         &amp;lt;int&amp;gt; 3, 4, 4, 6, 2, 10, 10, 8, 5, 3, 1, 2, 3, ...
$ AssessJob8         &amp;lt;int&amp;gt; 4, 3, 3, 2, 7, 1, 8, 7, 2, 6, 2, 3, 1, 3,...
$ AssessJob9         &amp;lt;int&amp;gt; 9, 6, 10, 10, 10, 9, 9, 10, 7, 9, 4, 8, 9...
$ AssessJob10        &amp;lt;int&amp;gt; 6, 9, 9, 7, 6, 3, 2, 1, 8, 8, 5, 5, 8, 9,...
$ EthicsChoice       &amp;lt;fct&amp;gt; No, Depends on what it is, No, Depends on...
$ LanguageWorkedWith &amp;lt;fct&amp;gt; JavaScript;Python;HTML;CSS, JavaScript;Py...
$ OperatingSystem    &amp;lt;fct&amp;gt; Linux-based, Linux-based, Windows, Linux-...
$ JobSatisfaction    &amp;lt;fct&amp;gt; Extremely satisfied, Moderately dissatisf...
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="target-variables"&gt;Target variables&lt;/h4&gt;
&lt;p&gt;We want to binarize both target variables. Let’s inspect them, starting with &lt;code&gt;EthicsChoice&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;jslevels &amp;lt;- levels(data$JobSatisfaction)
elevels &amp;lt;- levels(data$EthicsChoice)

data &amp;lt;- data %&amp;gt;% mutate(
  JobSatisfaction = JobSatisfaction %&amp;gt;% fct_relevel(
    jslevels[1],
    jslevels[3],
    jslevels[6],
    jslevels[5],
    jslevels[7],
    jslevels[4],
    jslevels[2]
  ),
  EthicsChoice = EthicsChoice %&amp;gt;% fct_relevel(
    elevels[2],
    elevels[1],
    elevels[3]
  ) 
)

ggplot(data, aes(EthicsChoice)) + geom_bar()&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-11-26-embeddings-fun-and-profit/images/ethicschoice.png" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Distribution of answers to: “Imagine that you were asked to write code for a purpose or product that you consider extremely unethical. Do you write the code anyway?”&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;You might agree that with a question containing the phrase &lt;em&gt;a purpose or product that you consider extremely unethical&lt;/em&gt;, the answer “depends on what it is” feels closer to “yes” than to “no”. If that seems like too skeptical a thought, it’s still the only binarization that achieves a sensible split.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;data &amp;lt;- data %&amp;gt;% mutate(
  EthicsChoice = if_else(as.numeric(EthicsChoice) == 2, 1, 0)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at our second target variable, &lt;code&gt;JobSatisfaction&lt;/code&gt;:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-11-26-embeddings-fun-and-profit/images/jobsatisfaction.png" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Distribution of answers to: "“How satisfied are you with your current job? If you work more than one job, please answer regarding the one you spend the most hours on.”&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We think that given the mode at “moderately satisfied”, a sensible way to binarize is a split into “moderately satisfied” and “extremely satisfied” on one side, all remaining options on the other:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;data &amp;lt;- data %&amp;gt;% mutate(
  JobSatisfaction = if_else(as.numeric(JobSatisfaction) &amp;gt; 5, 1, 0)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="predictors"&gt;Predictors&lt;/h4&gt;
&lt;p&gt;Among the predictors, &lt;code&gt;FormalEducation&lt;/code&gt;, &lt;code&gt;UndergradMajor&lt;/code&gt; and &lt;code&gt;OperatingSystem&lt;/code&gt; look pretty harmless - we already turned them into factors so it should be straightforward to one-hot-encode them. For curiosity’s sake, let’s look at how they’re distributed:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;data %&amp;gt;% group_by(FormalEducation) %&amp;gt;%
  summarise(count = n()) %&amp;gt;%
  arrange(desc(count))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  FormalEducation                                        count
  &amp;lt;fct&amp;gt;                                                  &amp;lt;int&amp;gt;
1 Bachelor’s degree (BA, BS, B.Eng., etc.)               25558
2 Master’s degree (MA, MS, M.Eng., MBA, etc.)            12865
3 Some college/university study without earning a degree  6474
4 Associate degree                                        1595
5 Other doctoral degree (Ph.D, Ed.D., etc.)               1395
6 Professional degree (JD, MD, etc.)                       723&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;data %&amp;gt;% group_by(UndergradMajor) %&amp;gt;%
  summarise(count = n()) %&amp;gt;%
  arrange(desc(count))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  UndergradMajor                                                  count
   &amp;lt;fct&amp;gt;                                                           &amp;lt;int&amp;gt;
 1 Computer science, computer engineering, or software engineering 30931
 2 Another engineering discipline (ex. civil, electrical, mechani…  4179
 3 Information systems, information technology, or system adminis…  3953
 4 A natural science (ex. biology, chemistry, physics)              2046
 5 Mathematics or statistics                                        1853
 6 Web development or web design                                    1171
 7 A business discipline (ex. accounting, finance, marketing)       1166
 8 A humanities discipline (ex. literature, history, philosophy)    1104
 9 A social science (ex. anthropology, psychology, political scie…   888
10 Fine arts or performing arts (ex. graphic design, music, studi…   791
11 I never declared a major                                          398
12 A health science (ex. nursing, pharmacy, radiology)               130&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;data %&amp;gt;% group_by(OperatingSystem) %&amp;gt;%
  summarise(count = n()) %&amp;gt;%
  arrange(desc(count))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  OperatingSystem count
  &amp;lt;fct&amp;gt;           &amp;lt;int&amp;gt;
1 Windows         23470
2 MacOS           14216
3 Linux-based     10837
4 BSD/Unix           87&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;LanguageWorkedWith&lt;/code&gt;, on the other hand, contains sequences of programming languages, concatenated by semicolon. One way to unpack these is using Keras’ &lt;code&gt;text_tokenizer&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;language_tokenizer &amp;lt;- text_tokenizer(split = &amp;quot;;&amp;quot;, filters = &amp;quot;&amp;quot;)
language_tokenizer %&amp;gt;% fit_text_tokenizer(data$LanguageWorkedWith)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have 38 languages overall. Actual usage counts aren’t too surprising:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;data.frame(
  name = language_tokenizer$word_counts %&amp;gt;% names(),
  count = language_tokenizer$word_counts %&amp;gt;% unlist() %&amp;gt;% unname()
) %&amp;gt;%
 arrange(desc(count))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                   name count
1            javascript 35224
2                  html 33287
3                   css 31744
4                   sql 29217
5                  java 21503
6            bash/shell 20997
7                python 18623
8                    c# 17604
9                   php 13843
10                  c++ 10846
11           typescript  9551
12                    c  9297
13                 ruby  5352
14                swift  4014
15                   go  3784
16          objective-c  3651
17               vb.net  3217
18                    r  3049
19             assembly  2699
20               groovy  2541
21                scala  2475
22               matlab  2465
23               kotlin  2305
24                  vba  2298
25                 perl  2164
26       visual basic 6  1729
27         coffeescript  1711
28                  lua  1556
29 delphi/object pascal  1174
30                 rust  1132
31              haskell  1058
32                   f#   764
33              clojure   696
34               erlang   560
35                cobol   317
36                ocaml   216
37                julia   215
38                 hack    94&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now &lt;code&gt;language_tokenizer&lt;/code&gt; will nicely create a one-hot representation of the multiple-choice column.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;langs &amp;lt;- language_tokenizer %&amp;gt;%
  texts_to_matrix(data$LanguageWorkedWith, mode = &amp;quot;count&amp;quot;)
langs[1:3, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; langs[1:3, ]
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]
[1,]    0    1    1    1    0    0    0    1    0     0     0     0     0     0     0     0     0     0     0     0     0
[2,]    0    1    0    0    0    0    1    1    0     0     0     0     0     0     0     0     0     0     0     0     0
[3,]    0    0    0    0    1    1    1    0    0     0     1     0     1     0     0     0     0     0     1     0     0
     [,22] [,23] [,24] [,25] [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38] [,39]
[1,]     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0
[2,]     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0
[3,]     0     1     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can simply append these columns to the dataframe (and do a little cleanup):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;data &amp;lt;- data %&amp;gt;% cbind(langs[, 2:39]) # the very first column is not useful
data &amp;lt;- data %&amp;gt;% rename_at(vars(`1`:`38`), funs(paste0(language_tokenizer$index_word[as.integer(.)])))
data &amp;lt;- data %&amp;gt;% select(-LanguageWorkedWith)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We still have the &lt;code&gt;AssessJob[n]&lt;/code&gt; columns to deal with. Here, StackOverflow had people rank what’s important to them about a job. These are the features that were to be ranked:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The industry that I’d be working in&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The financial performance or funding status of the company or organization&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The specific department or team I’d be working on&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The languages, frameworks, and other technologies I’d be working with&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The compensation and benefits offered&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The office environment or company culture&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The opportunity to work from home/remotely&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Opportunities for professional development&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The diversity of the company or organization&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;How widely used or impactful the product or service I’d be working on is&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Columns &lt;code&gt;AssessJob1&lt;/code&gt; to &lt;code&gt;AssessJob10&lt;/code&gt; contain the respective ranks, that is, values between 1 and 10.&lt;/p&gt;
&lt;p&gt;Based on introspection about the cognitive effort to actually establish an order among 10 items, we decided to pull out the three top-ranked features per person and treat them as equal. Technically, a first step extracts and concatenate these, yielding an intermediary result of e.g.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ job_vals&amp;lt;fct&amp;gt; languages_frameworks;compensation;remote, industry;compensation;development, languages_frameworks;compensation;development&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;data &amp;lt;- data %&amp;gt;% mutate(
  val_1 = if_else(
   AssessJob1 == 1, &amp;quot;industry&amp;quot;, if_else(
    AssessJob2 == 1, &amp;quot;company_financial_status&amp;quot;, if_else(
      AssessJob3 == 1, &amp;quot;department&amp;quot;, if_else(
        AssessJob4 == 1, &amp;quot;languages_frameworks&amp;quot;, if_else(
          AssessJob5 == 1, &amp;quot;compensation&amp;quot;, if_else(
            AssessJob6 == 1, &amp;quot;company_culture&amp;quot;, if_else(
              AssessJob7 == 1, &amp;quot;remote&amp;quot;, if_else(
                AssessJob8 == 1, &amp;quot;development&amp;quot;, if_else(
                  AssessJob10 == 1, &amp;quot;diversity&amp;quot;, &amp;quot;impact&amp;quot;))))))))),
  val_2 = if_else(
    AssessJob1 == 2, &amp;quot;industry&amp;quot;, if_else(
      AssessJob2 == 2, &amp;quot;company_financial_status&amp;quot;, if_else(
        AssessJob3 == 2, &amp;quot;department&amp;quot;, if_else(
          AssessJob4 == 2, &amp;quot;languages_frameworks&amp;quot;, if_else(
            AssessJob5 == 2, &amp;quot;compensation&amp;quot;, if_else(
              AssessJob6 == 2, &amp;quot;company_culture&amp;quot;, if_else(
                AssessJob7 == 1, &amp;quot;remote&amp;quot;, if_else(
                  AssessJob8 == 1, &amp;quot;development&amp;quot;, if_else(
                    AssessJob10 == 1, &amp;quot;diversity&amp;quot;, &amp;quot;impact&amp;quot;))))))))),
  val_3 = if_else(
    AssessJob1 == 3, &amp;quot;industry&amp;quot;, if_else(
      AssessJob2 == 3, &amp;quot;company_financial_status&amp;quot;, if_else(
        AssessJob3 == 3, &amp;quot;department&amp;quot;, if_else(
          AssessJob4 == 3, &amp;quot;languages_frameworks&amp;quot;, if_else(
            AssessJob5 == 3, &amp;quot;compensation&amp;quot;, if_else(
              AssessJob6 == 3, &amp;quot;company_culture&amp;quot;, if_else(
                AssessJob7 == 3, &amp;quot;remote&amp;quot;, if_else(
                  AssessJob8 == 3, &amp;quot;development&amp;quot;, if_else(
                    AssessJob10 == 3, &amp;quot;diversity&amp;quot;, &amp;quot;impact&amp;quot;)))))))))
  )

data &amp;lt;- data %&amp;gt;% mutate(
  job_vals = paste(val_1, val_2, val_3, sep = &amp;quot;;&amp;quot;) %&amp;gt;% factor()
)

data &amp;lt;- data %&amp;gt;% select(
  -c(starts_with(&amp;quot;AssessJob&amp;quot;), starts_with(&amp;quot;val_&amp;quot;))
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that column looks exactly like &lt;code&gt;LanguageWorkedWith&lt;/code&gt; looked before, so we can use the same method as above to produce a one-hot-encoded version.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;values_tokenizer &amp;lt;- text_tokenizer(split = &amp;quot;;&amp;quot;, filters = &amp;quot;&amp;quot;)
values_tokenizer %&amp;gt;% fit_text_tokenizer(data$job_vals)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So what actually do respondents value most?&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;                      name count
1              compensation 27020
2      languages_frameworks 24216
3           company_culture 20432
4               development 15981
5                    impact 14869
6                department 10452
7                    remote 10396
8                  industry  8294
9                 diversity  7594
10 company_financial_status  6576&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the same method as above&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;job_values &amp;lt;- values_tokenizer %&amp;gt;% texts_to_matrix(data$job_vals, mode = &amp;quot;count&amp;quot;)
data &amp;lt;- data %&amp;gt;% cbind(job_values[, 2:11])
data &amp;lt;- data %&amp;gt;% rename_at(vars(`1`:`10`), funs(paste0(values_tokenizer$index_word[as.integer(.)])))
data &amp;lt;- data %&amp;gt;% select(-job_vals)
data %&amp;gt;% glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we end up with a dataset that looks like this&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; data %&amp;gt;% glimpse()
Observations: 48,610
Variables: 53
$ FormalEducation          &amp;lt;fct&amp;gt; Bachelor’s degree (BA, BS, B.Eng., etc.), Bach...
$ UndergradMajor           &amp;lt;fct&amp;gt; Mathematics or statistics, A natural science (...
$ OperatingSystem          &amp;lt;fct&amp;gt; Linux-based, Linux-based, Windows, Linux-based...
$ JS                       &amp;lt;dbl&amp;gt; 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0...
$ EC                       &amp;lt;dbl&amp;gt; 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0...
$ javascript               &amp;lt;dbl&amp;gt; 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1...
$ html                     &amp;lt;dbl&amp;gt; 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1...
$ css                      &amp;lt;dbl&amp;gt; 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1...
$ sql                      &amp;lt;dbl&amp;gt; 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1...
$ java                     &amp;lt;dbl&amp;gt; 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1...
$ `bash/shell`             &amp;lt;dbl&amp;gt; 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1...
$ python                   &amp;lt;dbl&amp;gt; 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0...
$ `c#`                     &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0...
$ php                      &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1...
$ `c++`                    &amp;lt;dbl&amp;gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0...
$ typescript               &amp;lt;dbl&amp;gt; 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1...
$ c                        &amp;lt;dbl&amp;gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0...
$ ruby                     &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0...
$ swift                    &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1...
$ go                       &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0...
$ `objective-c`            &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
$ vb.net                   &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
$ r                        &amp;lt;dbl&amp;gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
$ assembly                 &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0...
$ groovy                   &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
$ scala                    &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
$ matlab                   &amp;lt;dbl&amp;gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
$ kotlin                   &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
$ vba                      &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
$ perl                     &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
$ `visual basic 6`         &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
$ coffeescript             &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0...
$ lua                      &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0...
$ `delphi/object pascal`   &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
$ rust                     &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
$ haskell                  &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
$ `f#`                     &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
$ clojure                  &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
$ erlang                   &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0...
$ cobol                    &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
$ ocaml                    &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
$ julia                    &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
$ hack                     &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
$ compensation             &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0...
$ languages_frameworks     &amp;lt;dbl&amp;gt; 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0...
$ company_culture          &amp;lt;dbl&amp;gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
$ development              &amp;lt;dbl&amp;gt; 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0...
$ impact                   &amp;lt;dbl&amp;gt; 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1...
$ department               &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0...
$ remote                   &amp;lt;dbl&amp;gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 1, 0...
$ industry                 &amp;lt;dbl&amp;gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1...
$ diversity                &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0...
$ company_financial_status &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which we further reduce to a design matrix &lt;code&gt;X&lt;/code&gt; removing the binarized target variables&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;X &amp;lt;- data %&amp;gt;% select(-c(JobSatisfaction, EthicsChoice))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From here on, different actions will ensue depending on whether we choose the road of working with a one-hot model or an embeddings model of the predictors.&lt;/p&gt;
&lt;p&gt;There is one other thing though to be done before: We want to work with the same train-test split in both cases.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_indices &amp;lt;- sample(1:nrow(X), 0.8 * nrow(X))&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="one-hot-model"&gt;One-hot model&lt;/h3&gt;
&lt;p&gt;Given this is a post about embeddings, why show a one-hot model? First, for instructional reasons - you don’t see many of examples of deep learning on categorical data in the wild. Second, … but we’ll turn to that after having shown both models.&lt;/p&gt;
&lt;p&gt;For the one-hot model, all that remains to be done is using Keras’ &lt;code&gt;to_categorical&lt;/code&gt; on the three remaining variables that are not yet in one-hot form.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;X_one_hot &amp;lt;- X %&amp;gt;% map_if(is.factor, ~ as.integer(.x) - 1) %&amp;gt;%
  map_at(&amp;quot;FormalEducation&amp;quot;, ~ to_categorical(.x) %&amp;gt;% 
           array_reshape(c(length(.x), length(levels(data$FormalEducation))))) %&amp;gt;%
  map_at(&amp;quot;UndergradMajor&amp;quot;, ~ to_categorical(.x) %&amp;gt;% 
           array_reshape(c(length(.x), length(levels(data$UndergradMajor))))) %&amp;gt;%
  map_at(&amp;quot;OperatingSystem&amp;quot;, ~ to_categorical(.x) %&amp;gt;%
           array_reshape(c(length(.x), length(levels(data$OperatingSystem))))) %&amp;gt;%
  abind::abind(along = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We divide up our dataset into train and validation parts&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;x_train &amp;lt;- X_one_hot[train_indices, ] %&amp;gt;% as.matrix()
x_valid &amp;lt;- X_one_hot[-train_indices, ] %&amp;gt;% as.matrix()
y_train &amp;lt;- data$EthicsChoice[train_indices] %&amp;gt;% as.matrix()
y_valid &amp;lt;- data$EthicsChoice[-train_indices] %&amp;gt;% as.matrix()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and define a pretty straightforward MLP.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_dense(
    units = 128,
    activation = &amp;quot;selu&amp;quot;
  ) %&amp;gt;%
  layer_dropout(0.5) %&amp;gt;%
  layer_dense(
    units = 128,
    activation = &amp;quot;selu&amp;quot;
  ) %&amp;gt;%
  layer_dropout(0.5) %&amp;gt;%
  layer_dense(
    units = 128,
    activation = &amp;quot;selu&amp;quot;
  ) %&amp;gt;%
  layer_dropout(0.5) %&amp;gt;%
  layer_dense(
    units = 128,
    activation = &amp;quot;selu&amp;quot;
  ) %&amp;gt;%
  layer_dropout(0.5) %&amp;gt;%
  layer_dense(units = 1, activation = &amp;quot;sigmoid&amp;quot;)

model %&amp;gt;% compile(
  loss = &amp;quot;binary_crossentropy&amp;quot;,
  optimizer = &amp;quot;adam&amp;quot;,
  metrics = &amp;quot;accuracy&amp;quot;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Training this model:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;history &amp;lt;- model %&amp;gt;% fit(
  x_train,
  y_train,
  validation_data = list(x_valid, y_valid),
  epochs = 20,
  batch_size = 100
)

plot(history)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;…results in an accuracy on the validation set of 0.64 - not an impressive number per se, but interesting given the small amount of predictors and the choice of target variable.&lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-11-26-embeddings-fun-and-profit/images/so_one_hot.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;h3 id="embeddings-model"&gt;Embeddings model&lt;/h3&gt;
&lt;p&gt;In the embeddings model, we don’t need to use &lt;code&gt;to_categorical&lt;/code&gt; on the remaining factors, as embedding layers can work with integer input data. We thus just convert the factors to integers:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;X_embed &amp;lt;- X %&amp;gt;%
  mutate_if(is.factor, compose(partial(`-`, 1, .first = FALSE), as.integer))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now for the model. Effectively we have five groups of entities here: formal education, undergrad major, operating system, languages worked with, and highest-counting values with respect to jobs. Each of these groups get embedded separately, so we need to use the Keras functional API and declare five different inputs.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;input_fe &amp;lt;- layer_input(shape = 1)        # formal education, encoded as integer
input_um &amp;lt;- layer_input(shape = 1)        # undergrad major, encoded as integer
input_os &amp;lt;- layer_input(shape = 1)        # operating system, encoded as integer
input_langs &amp;lt;- layer_input(shape = 38)    # languages worked with, multi-hot-encoded
input_vals &amp;lt;- layer_input(shape = 10)     # values, multi-hot-encoded&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Having embedded them separately, we concatenate the outputs for further common processing.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;concat &amp;lt;- layer_concatenate(
  list(
    input_fe %&amp;gt;%
      layer_embedding(
        input_dim = length(levels(data$FormalEducation)),
        output_dim = 64,
        name = &amp;quot;fe&amp;quot;
      ) %&amp;gt;%
      layer_flatten(),
    input_um %&amp;gt;%
      layer_embedding(
        input_dim = length(levels(data$UndergradMajor)),
        output_dim = 64,
        name = &amp;quot;um&amp;quot;
      ) %&amp;gt;%
      layer_flatten(),
    input_os %&amp;gt;%
      layer_embedding(
        input_dim = length(levels(data$OperatingSystem)),
        output_dim = 64,
        name = &amp;quot;os&amp;quot;
      ) %&amp;gt;%
      layer_flatten(),
    input_langs %&amp;gt;%
       layer_embedding(input_dim = 38, output_dim = 256,
                       name = &amp;quot;langs&amp;quot;)%&amp;gt;%
       layer_flatten(),
    input_vals %&amp;gt;%
      layer_embedding(input_dim = 10, output_dim = 128,
                      name = &amp;quot;vals&amp;quot;)%&amp;gt;%
      layer_flatten()
  )
)

output &amp;lt;- concat %&amp;gt;%
  layer_dense(
    units = 128,
    activation = &amp;quot;relu&amp;quot;
  ) %&amp;gt;%
  layer_dropout(0.5) %&amp;gt;%
  layer_dense(
    units = 128,
    activation = &amp;quot;relu&amp;quot;
  ) %&amp;gt;%
  layer_dropout(0.5) %&amp;gt;%
  layer_dense(
    units = 128,
    activation = &amp;quot;relu&amp;quot;
  ) %&amp;gt;%
  layer_dense(
    units = 128,
    activation = &amp;quot;relu&amp;quot;
  ) %&amp;gt;%
  layer_dropout(0.5) %&amp;gt;%
  layer_dense(units = 1, activation = &amp;quot;sigmoid&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So there go model definition and compilation:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model(list(input_fe, input_um, input_os, input_langs, input_vals), output)

model %&amp;gt;% compile(
  loss = &amp;quot;binary_crossentropy&amp;quot;,
  optimizer = &amp;quot;adam&amp;quot;,
  metrics = &amp;quot;accuracy&amp;quot;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now to pass the data to the model, we need to chop it up into ranges of columns matching the inputs.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;y_train &amp;lt;- data$EthicsChoice[train_indices] %&amp;gt;% as.matrix()
y_valid &amp;lt;- data$EthicsChoice[-train_indices] %&amp;gt;% as.matrix()

x_train &amp;lt;-
  list(
    X_embed[train_indices, 1, drop = FALSE] %&amp;gt;% as.matrix() ,
    X_embed[train_indices , 2, drop = FALSE] %&amp;gt;% as.matrix(),
    X_embed[train_indices , 3, drop = FALSE] %&amp;gt;% as.matrix(),
    X_embed[train_indices , 4:41, drop = FALSE] %&amp;gt;% as.matrix(),
    X_embed[train_indices , 42:51, drop = FALSE] %&amp;gt;% as.matrix()
  )
x_valid &amp;lt;- list(
  X_embed[-train_indices, 1, drop = FALSE] %&amp;gt;% as.matrix() ,
  X_embed[-train_indices , 2, drop = FALSE] %&amp;gt;% as.matrix(),
  X_embed[-train_indices , 3, drop = FALSE] %&amp;gt;% as.matrix(),
  X_embed[-train_indices , 4:41, drop = FALSE] %&amp;gt;% as.matrix(),
  X_embed[-train_indices , 42:51, drop = FALSE] %&amp;gt;% as.matrix()
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we’re ready to train.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% fit(
  x_train,
  y_train,
  validation_data = list(x_valid, y_valid),
  epochs = 20,
  batch_size = 100
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the same train-test split as before, this results in an accuracy of … ~0.64 (just as before). Now we said from the start that using embeddings could serve different purposes, and that in this first use case, we wanted to demonstrate their use for extracting latent relationships. And in any case you could argue that the task is too hard - probably there just is not much of a relationship between the predictors we chose and the target.&lt;/p&gt;
&lt;p&gt;But this also warrants a more general comment. With all current enthusiasm about using embeddings on tabular data, we are not aware of any systematic comparisons with one-hot-encoded data as regards the actual effect on performance, nor do we know of systematic analyses under what circumstances embeddings will probably be of help. Our working hypothesis is that in the setup we chose, the dimensionality of the original data is so low that the information can simply be encoded “as is” by the network - as long as we create it with sufficient capacity. Our second use case will therefore use data where - hopefully - this won’t be the case.&lt;/p&gt;
&lt;p&gt;But before, let’s get to the main purpose of this use case: How can we extract those latent relationships from the network?&lt;/p&gt;
&lt;h4 id="extracting-relationships-from-the-learned-embeddings"&gt;Extracting relationships from the learned embeddings&lt;/h4&gt;
&lt;p&gt;We’ll show the code here for the &lt;em&gt;job values&lt;/em&gt; embeddings, - it is directly transferable to the other ones. &lt;em&gt;The embeddings&lt;/em&gt;, that’s just the weight matrix of the respective layer, of dimension &lt;code&gt;number of different values&lt;/code&gt; times &lt;code&gt;embedding size&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;emb_vals &amp;lt;- (model$get_layer(&amp;quot;vals&amp;quot;) %&amp;gt;% get_weights())[[1]]
emb_vals %&amp;gt;% dim() # 10x128&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then perform dimensionality reduction on the raw values, e.g., PCA&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pca &amp;lt;- prcomp(emb_vals, center = TRUE, scale. = TRUE, rank = 2)$x[, c(&amp;quot;PC1&amp;quot;, &amp;quot;PC2&amp;quot;)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and plot the results.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pca %&amp;gt;%
  as.data.frame() %&amp;gt;%
  mutate(class = attr(values_tokenizer$word_index, &amp;quot;names&amp;quot;)) %&amp;gt;%
  ggplot(aes(x = PC1, y = PC2)) +
  geom_point() +
  geom_label_repel(aes(label = class))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is what we get (displaying four of the five variables we used embeddings on):&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-11-26-embeddings-fun-and-profit/images/out.png" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Two first principal components of the embeddings for undergrad major (top left), operating system (top right), programming language used (bottom left), and primary values with respect to jobs (bottom right)&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now we’ll definitely refrain from taking this too seriously, given the modest accuracy on the prediction task that lead to these embedding matrices.&lt;a href="#fn4" class="footnote-ref" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; Certainly when assessing the obtained factorization, performance on the main task has to be taken into account.&lt;/p&gt;
&lt;p&gt;But we’d like to point out something else too: In contrast to unsupervised and semi-supervised techniques like PCA or autoencoders, we made use of an extraneous variable (the ethical behavior to be predicted). So any learned relationships are never “absolute”, but always to be seen in relation to the way they were learned. This is why we chose an additional target variable, &lt;code&gt;JobSatisfaction&lt;/code&gt;, so we could compare the embeddings learned on two different tasks. We won’t refer the concrete results here as accuracy turned out to be even lower than with &lt;code&gt;EthicsChoice&lt;/code&gt;. We do, however, want to stress this inherent difference to representations learned by, e.g., autoencoders.&lt;/p&gt;
&lt;p&gt;Now let’s address the second use case.&lt;/p&gt;
&lt;h2 id="embedding-for-profit-improving-accuracy"&gt;Embedding for profit (improving accuracy)&lt;/h2&gt;
&lt;p&gt;Our second task here is about fraud detection. The dataset is contained in the &lt;code&gt;DMwR2&lt;/code&gt; package and is called &lt;code&gt;sales&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;data(sales, package = &amp;quot;DMwR2&amp;quot;)
sales&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 401,146 x 5
   ID    Prod  Quant   Val Insp 
   &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;
 1 v1    p1      182  1665 unkn 
 2 v2    p1     3072  8780 unkn 
 3 v3    p1    20393 76990 unkn 
 4 v4    p1      112  1100 unkn 
 5 v3    p1     6164 20260 unkn 
 6 v5    p2      104  1155 unkn 
 7 v6    p2      350  5680 unkn 
 8 v7    p2      200  4010 unkn 
 9 v8    p2      233  2855 unkn 
10 v9    p2      118  1175 unkn 
# ... with 401,136 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each row indicates a transaction reported by a salesperson, - &lt;code&gt;ID&lt;/code&gt; being the salesperson ID, &lt;code&gt;Prod&lt;/code&gt; a product ID, &lt;code&gt;Quant&lt;/code&gt; the quantity sold, &lt;code&gt;Val&lt;/code&gt; the amount of money it was sold for, and &lt;code&gt;Insp&lt;/code&gt; indicating one of three possibilities: (1) the transaction was examined and found fraudulent, (2) it was examined and found okay, and (3) it has not been examined (the vast majority of cases).&lt;/p&gt;
&lt;p&gt;While this dataset “cries” for semi-supervised techniques (to make use of the overwhelming amount of unlabeled data), we want to see if using embeddings can help us improve accuracy on a supervised task.&lt;/p&gt;
&lt;p&gt;We thus recklessly throw away incomplete data as well as all unlabeled entries&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;sales &amp;lt;- filter(sales, !(is.na(Quant)))
sales &amp;lt;- filter(sales, !(is.na(Val)))

sales &amp;lt;- droplevels(sales %&amp;gt;% filter(Insp != &amp;quot;unkn&amp;quot;))
nrow(sales)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which leaves us with 15546 transactions.&lt;/p&gt;
&lt;h3 id="one-hot-model-1"&gt;One-hot model&lt;/h3&gt;
&lt;p&gt;Now we prepare the data for the one-hot model we want to compare against:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;With 2821 levels, salesperson &lt;code&gt;ID&lt;/code&gt; is far too high-dimensional to work well with one-hot encoding, so we completely drop that column.&lt;/li&gt;
&lt;li&gt;Product id (&lt;code&gt;Prod&lt;/code&gt;) has “just” 797 levels, but with one-hot-encoding, that still results in significant memory demand. We thus zoom in on the 500 top-sellers.&lt;/li&gt;
&lt;li&gt;The continuous variables &lt;code&gt;Quant&lt;/code&gt; and &lt;code&gt;Val&lt;/code&gt; are normalized to values between 0 and 1 so they fit with the one-hot-encoded &lt;code&gt;Prod&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;sales_1hot &amp;lt;- sales

normalize &amp;lt;- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

top_n &amp;lt;- 500
top_prods &amp;lt;- sales_1hot %&amp;gt;% 
  group_by(Prod) %&amp;gt;% 
  summarise(cnt = n()) %&amp;gt;% 
  arrange(desc(cnt)) %&amp;gt;%
  head(top_n) %&amp;gt;%
  select(Prod) %&amp;gt;%
  pull()
sales_1hot &amp;lt;- droplevels(sales_1hot %&amp;gt;% filter(Prod %in% top_prods))

sales_1hot &amp;lt;- sales_1hot %&amp;gt;%
  select(-ID) %&amp;gt;%
  map_if(is.factor, ~ as.integer(.x) - 1) %&amp;gt;%
  map_at(&amp;quot;Prod&amp;quot;, ~ to_categorical(.x) %&amp;gt;% array_reshape(c(length(.x), top_n))) %&amp;gt;%
  map_at(&amp;quot;Quant&amp;quot;, ~ normalize(.x) %&amp;gt;% array_reshape(c(length(.x), 1))) %&amp;gt;%
  map_at(&amp;quot;Val&amp;quot;, ~ normalize(.x) %&amp;gt;% array_reshape(c(length(.x), 1))) %&amp;gt;%
  abind(along = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then perform the usual train-test split.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_indices &amp;lt;- sample(1:nrow(sales_1hot), 0.7 * nrow(sales_1hot))

X_train &amp;lt;- sales_1hot[train_indices, 1:502] 
y_train &amp;lt;-  sales_1hot[train_indices, 503] %&amp;gt;% as.matrix()

X_valid &amp;lt;- sales_1hot[-train_indices, 1:502] 
y_valid &amp;lt;-  sales_1hot[-train_indices, 503] %&amp;gt;% as.matrix()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For classification on this dataset, which will be the baseline to beat?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;xtab_train  &amp;lt;- y_train %&amp;gt;% table()
xtab_valid  &amp;lt;- y_valid %&amp;gt;% table()
list(xtab_train[1]/(xtab_train[1] + xtab_train[2]), xtab_valid[1]/(xtab_valid[1] + xtab_valid[2]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
        0 
0.9393547 

[[2]]
        0 
0.9384437 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So if we don’t get beyond 94% accuracy on both training and validation sets, we may just as well predict “okay” for every transaction.&lt;/p&gt;
&lt;p&gt;Here then is the model, plus the training routine and evaluation:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_dense(units = 256, activation = &amp;quot;selu&amp;quot;) %&amp;gt;%
  layer_dropout(dropout_rate) %&amp;gt;% 
  layer_dense(units = 256, activation = &amp;quot;selu&amp;quot;) %&amp;gt;%
  layer_dropout(dropout_rate) %&amp;gt;% 
  layer_dense(units = 256, activation = &amp;quot;selu&amp;quot;) %&amp;gt;%
  layer_dropout(dropout_rate) %&amp;gt;% 
  layer_dense(units = 256, activation = &amp;quot;selu&amp;quot;) %&amp;gt;%
  layer_dropout(dropout_rate) %&amp;gt;% 
  layer_dense(units = 1, activation = &amp;quot;sigmoid&amp;quot;)

model %&amp;gt;% compile(loss = &amp;quot;binary_crossentropy&amp;quot;, optimizer = &amp;quot;adam&amp;quot;, metrics = c(&amp;quot;accuracy&amp;quot;))

model %&amp;gt;% fit(
  X_train,
  y_train,
  validation_data = list(X_valid, y_valid),
  class_weights = list(&amp;quot;0&amp;quot; = 0.1, &amp;quot;1&amp;quot; = 0.9),
  batch_size = 128,
  epochs = 200
)

model %&amp;gt;% evaluate(X_train, y_train, batch_size = 100) 
model %&amp;gt;% evaluate(X_valid, y_valid, batch_size = 100) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This model achieved optimal validation accuracy at a dropout rate of 0.2. At that rate, training accuracy was &lt;code&gt;0.9761&lt;/code&gt;, and validation accuracy was &lt;code&gt;0.9507&lt;/code&gt;. At all dropout rates lower than 0.7, validation accuracy did indeed surpass the majority vote baseline.&lt;/p&gt;
&lt;p&gt;Can we further improve performance by embedding the product id?&lt;/p&gt;
&lt;h3 id="embeddings-model-1"&gt;Embeddings model&lt;/h3&gt;
&lt;p&gt;For better comparability, we again discard salesperson information and cap the number of different products at 500. Otherwise, data preparation goes as expected for this model:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;sales_embed &amp;lt;- sales

top_prods &amp;lt;- sales_embed %&amp;gt;% 
  group_by(Prod) %&amp;gt;% 
  summarise(cnt = n()) %&amp;gt;% 
  arrange(desc(cnt)) %&amp;gt;% 
  head(top_n) %&amp;gt;% 
  select(Prod) %&amp;gt;% 
  pull()

sales_embed &amp;lt;- droplevels(sales_embed %&amp;gt;% filter(Prod %in% top_prods))

sales_embed &amp;lt;- sales_embed %&amp;gt;%
  select(-ID) %&amp;gt;%
  mutate_if(is.factor, ~ as.integer(.x) - 1) %&amp;gt;%
  mutate(Quant = scale(Quant)) %&amp;gt;%
  mutate(Val = scale(Val))

X_train &amp;lt;- sales_embed[train_indices, 1:3] %&amp;gt;% as.matrix()
y_train &amp;lt;-  sales_embed[train_indices, 4] %&amp;gt;% as.matrix()

X_valid &amp;lt;- sales_embed[-train_indices, 1:3] %&amp;gt;% as.matrix()
y_valid &amp;lt;-  sales_embed[-train_indices, 4] %&amp;gt;% as.matrix()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model we define is as similar as possible to the one-hot alternative:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;prod_input &amp;lt;- layer_input(shape = 1)
cont_input &amp;lt;- layer_input(shape = 2)

prod_embed &amp;lt;- prod_input %&amp;gt;% 
  layer_embedding(input_dim = sales_embed$Prod %&amp;gt;% max() + 1,
                  output_dim = 256
                  ) %&amp;gt;%
  layer_flatten()
cont_dense &amp;lt;- cont_input %&amp;gt;% layer_dense(units = 256, activation = &amp;quot;selu&amp;quot;)

output &amp;lt;- layer_concatenate(
  list(prod_embed, cont_dense)) %&amp;gt;%
  layer_dropout(dropout_rate) %&amp;gt;% 
  layer_dense(units = 256, activation = &amp;quot;selu&amp;quot;) %&amp;gt;%
  layer_dropout(dropout_rate) %&amp;gt;% 
  layer_dense(units = 256, activation = &amp;quot;selu&amp;quot;) %&amp;gt;%
  layer_dropout(dropout_rate) %&amp;gt;% 
  layer_dense(units = 256, activation = &amp;quot;selu&amp;quot;) %&amp;gt;%
  layer_dropout(dropout_rate) %&amp;gt;% 
  layer_dense(units = 1, activation = &amp;quot;sigmoid&amp;quot;)
  
model &amp;lt;- keras_model(inputs = list(prod_input, cont_input), outputs = output)

model %&amp;gt;% compile(loss = &amp;quot;binary_crossentropy&amp;quot;, optimizer = &amp;quot;adam&amp;quot;, metrics = &amp;quot;accuracy&amp;quot;)

model %&amp;gt;% fit(
  list(X_train[ , 1], X_train[ , 2:3]),
  y_train,
  validation_data = list(list(X_valid[ , 1], X_valid[ , 2:3]), y_valid),
  class_weights = list(&amp;quot;0&amp;quot; = 0.1, &amp;quot;1&amp;quot; = 0.9),
  batch_size = 128,
  epochs = 200
)

model %&amp;gt;% evaluate(list(X_train[ , 1], X_train[ , 2:3]), y_train) 
model %&amp;gt;% evaluate(list(X_valid[ , 1], X_valid[ , 2:3]), y_valid)        &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This time, accuracies are in fact higher: At the optimal dropout rate (0.3 in this case), training resp. validation accuracy are at &lt;code&gt;0.9913&lt;/code&gt; and &lt;code&gt;0.9666&lt;/code&gt;, respectively. Quite a difference!&lt;/p&gt;
&lt;p&gt;So why did we choose this dataset? In contrast to our previous dataset, here the categorical variable is high-dimensional, so well suited for compression and densification. It is interesting that we can make such good use of an ID without knowing what it stands for!&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we’ve shown two use cases of embeddings in “simple” tabular data. As stated in the introduction, to us, embeddings are &lt;em&gt;what you make of them&lt;/em&gt;. In that vein, if you’ve used embeddings to accomplish things that mattered to your task at hand, please comment and tell us about it!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-GuoB16"&gt;
&lt;p&gt;Guo, Cheng, and Felix Berkhahn. 2016. “Entity Embeddings of Categorical Variables.” &lt;em&gt;CoRR&lt;/em&gt; abs/1604.06737. &lt;a href="http://arxiv.org/abs/1604.06737"&gt;http://arxiv.org/abs/1604.06737&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-MikolovSCCD13"&gt;
&lt;p&gt;Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Distributed Representations of Words and Phrases and Their Compositionality.” &lt;em&gt;CoRR&lt;/em&gt; abs/1310.4546. &lt;a href="http://arxiv.org/abs/1310.4546"&gt;http://arxiv.org/abs/1310.4546&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;We did think it prudent though to omit variables like &lt;em&gt;country&lt;/em&gt;, &lt;em&gt;ethnicity&lt;/em&gt; or &lt;em&gt;gender&lt;/em&gt;.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;at least given the way we binarized answers (more on that soon)&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;As usual when not working with one the “flagship areas” of deep learning, comparisons against other machine learning methods would be interesting. We did, however, not want to further elongate the post, nor distract from its main focus, namely, the use of embeddings with categorical data.&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;No, no, of course we’re not implying that for programming languages, the second principal component, with R and assembly at its extremes, stands for high-level vs. low-level language here.&lt;a href="#fnref4" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">27a8373deec0e73f2e863f283c1dde8c</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Tabular Data</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-11-26-embeddings-fun-and-profit</guid>
      <pubDate>Mon, 26 Nov 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-11-26-embeddings-fun-and-profit/images/thumb.png" medium="image" type="image/png" width="820" height="410"/>
    </item>
    <item>
      <title>You sure? A Bayesian approach to obtaining uncertainty estimates from neural networks</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-11-12-uncertainty_estimates_dropout</link>
      <description>


&lt;p&gt;If there were a set of survival rules for data scientists, among them would have to be this: &lt;em&gt;Always report uncertainty estimates with your predictions&lt;/em&gt;. However, here we are, working with neural networks, and unlike &lt;code&gt;lm&lt;/code&gt;, a Keras model does not conveniently output something like a &lt;em&gt;standard error&lt;/em&gt; for the weights. We might try to think of rolling your own uncertainty measure - for example, averaging predictions from networks trained from different random weight initializations, for different numbers of epochs, or on different subsets of the data. But we might still be worried that our method is quite a bit, well … &lt;em&gt;ad hoc&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In this post, we’ll see a both practical as well as theoretically grounded approach to obtaining uncertainty estimates from neural networks. First, however, let’s quickly talk about why uncertainty is that important - over and above its potential to save a data scientist’s job.&lt;/p&gt;
&lt;h2 id="why-uncertainty"&gt;Why uncertainty?&lt;/h2&gt;
&lt;p&gt;In a society where automated algorithms are - and will be - entrusted with more and more life-critical tasks, one answer immediately jumps to mind: If the algorithm correctly quantifies its uncertainty, we may have human experts inspect the more uncertain predictions and potentially revise them.&lt;/p&gt;
&lt;p&gt;This will only work if the network’s self-indicated uncertainty really is indicative of a higher probability of misclassification. Leibig et al.&lt;span class="citation"&gt;(Leibig et al. 2017)&lt;/span&gt; used a predecessor of the method described below to assess neural network uncertainty in detecting &lt;em&gt;diabetic retinopathy&lt;/em&gt;. They found that indeed, the distributions of uncertainty were different depending on whether the answer was correct or not:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-11-12-uncertainty_estimates_dropout/images/retinopathy.png" class="external" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Figure from Leibig et al. 2017 &lt;span class="citation"&gt;(Leibig et al. 2017)&lt;/span&gt;. Green: uncertainty estimates for wrong predictions. Blue: uncertainty estimates for correct predictions.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In addition to quantifying uncertainty, it can make sense to &lt;em&gt;qualify&lt;/em&gt; it. In the Bayesian deep learning literature, a distinction is commonly made between &lt;em&gt;epistemic uncertainty&lt;/em&gt; and &lt;em&gt;aleatoric uncertainty&lt;/em&gt; &lt;span class="citation"&gt;(Kendall and Gal 2017)&lt;/span&gt;. Epistemic uncertainty refers to imperfections in the model - in the limit of infinite data, this kind of uncertainty should be reducible to 0. Aleatoric uncertainty is due to data sampling and measurement processes and does not depend on the size of the dataset.&lt;/p&gt;
&lt;p&gt;Say we train a model for object detection. With more data, the model should become more sure about what makes a unicycle different from a mountainbike. However, let’s assume all that’s visible of the mountainbike is the front wheel, the fork and the head tube. Then it doesn’t look so different from a unicycle any more!&lt;/p&gt;
&lt;p&gt;What would be the consequences if we could distinguish both types of uncertainty? If epistemic uncertainty is high, we can try to get more training data. The remaining aleatoric uncertainty should then keep us cautioned to factor in safety margins in our application.&lt;/p&gt;
&lt;p&gt;Probably no further justifications are required of why we might want to assess model uncertainty - but how can we do this?&lt;/p&gt;
&lt;h2 id="uncertainty-estimates-through-bayesian-deep-learning"&gt;Uncertainty estimates through Bayesian deep learning&lt;/h2&gt;
&lt;p&gt;In a Bayesian world, in principle, uncertainty is for free as we don’t just get point estimates (the maximum aposteriori) but the full posterior distribution. Strictly speaking, in Bayesian deep learning, priors should be put over the weights, and the posterior be determined according to Bayes’ rule. To the deep learning practitioner, this sounds pretty arduous - and how do you do it using Keras?&lt;/p&gt;
&lt;p&gt;In 2016 though, Gal and Ghahramani &lt;span class="citation"&gt;(Gal and Ghahramani 2016)&lt;/span&gt; showed that when viewing a neural network as an approximation to a Gaussian process, uncertainty estimates can be obtained in a theoretically grounded yet very practical way: by training a network with dropout and then, using dropout at test time too. At test time, dropout lets us extract Monte Carlo samples from the posterior, which can then be used to approximate the true posterior distribution.&lt;/p&gt;
&lt;aside&gt;
Yarin Gal has a nice writeup of the why and how on his &lt;a href="http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html"&gt;blog&lt;/a&gt;.
&lt;/aside&gt;
&lt;p&gt;This is already good news, but it leaves one question open: How do we choose an appropriate dropout rate? The answer is: let the network learn it.&lt;/p&gt;
&lt;h2 id="learning-dropout-and-uncertainty"&gt;Learning dropout and uncertainty&lt;/h2&gt;
&lt;p&gt;In several 2017 papers &lt;span class="citation"&gt;(Gal, Hron, and Kendall 2017)&lt;/span&gt;,&lt;span class="citation"&gt;(Kendall and Gal 2017)&lt;/span&gt;, Gal and his coworkers demonstrated how a network can be trained to dynamically adapt the dropout rate so it is adequate for the amount and characteristics of the data given.&lt;/p&gt;
&lt;p&gt;Besides the predictive mean of the target variable, it can additionally be made to learn the variance. This means we can calculate both types of uncertainty, epistemic and aleatoric, independently, which is useful in the light of their different implications. We then add them up to obtain the overall predictive uncertainty.&lt;/p&gt;
&lt;p&gt;Let’s make this concrete and see how we can implement and test the intended behavior on simulated data. In the implementation, there are three things warranting our special attention:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The wrapper class used to add learnable-dropout behavior to a Keras layer;&lt;/li&gt;
&lt;li&gt;The loss function designed to minimize aleatoric uncertainty; and&lt;/li&gt;
&lt;li&gt;The ways we can obtain both uncertainties at test time.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s start with the wrapper.&lt;/p&gt;
&lt;h3 id="a-wrapper-for-learning-dropout"&gt;A wrapper for learning dropout&lt;/h3&gt;
&lt;p&gt;In this example, we’ll restrict ourselves to learning dropout for &lt;em&gt;dense&lt;/em&gt; layers. Technically, we’ll add a weight and a loss to every dense layer we want to use dropout with. This means we’ll create a &lt;a href="https://github.com/rstudio/keras/blob/master/vignettes/custom_wrappers.Rmd"&gt;custom wrapper&lt;/a&gt; class that has access to the underlying layer and can modify it.&lt;/p&gt;
&lt;p&gt;The logic implemented in the wrapper is derived mathematically in the &lt;em&gt;Concrete Dropout&lt;/em&gt; paper &lt;span class="citation"&gt;(Gal, Hron, and Kendall 2017)&lt;/span&gt;. The below code is a port to R of the Python Keras version found in the &lt;a href="https://github.com/yaringal/ConcreteDropout"&gt;paper’s companion github repo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So first, here is the wrapper class - we’ll see how to use it in just a second:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)

# R6 wrapper class, a subclass of KerasWrapper
ConcreteDropout &amp;lt;- R6::R6Class(&amp;quot;ConcreteDropout&amp;quot;,
  
  inherit = KerasWrapper,
  
  public = list(
    weight_regularizer = NULL,
    dropout_regularizer = NULL,
    init_min = NULL,
    init_max = NULL,
    is_mc_dropout = NULL,
    supports_masking = TRUE,
    p_logit = NULL,
    p = NULL,
    
    initialize = function(weight_regularizer,
                          dropout_regularizer,
                          init_min,
                          init_max,
                          is_mc_dropout) {
      self$weight_regularizer &amp;lt;- weight_regularizer
      self$dropout_regularizer &amp;lt;- dropout_regularizer
      self$is_mc_dropout &amp;lt;- is_mc_dropout
      self$init_min &amp;lt;- k_log(init_min) - k_log(1 - init_min)
      self$init_max &amp;lt;- k_log(init_max) - k_log(1 - init_max)
    },
    
    build = function(input_shape) {
      super$build(input_shape)
      
      self$p_logit &amp;lt;- super$add_weight(
        name = &amp;quot;p_logit&amp;quot;,
        shape = shape(1),
        initializer = initializer_random_uniform(self$init_min, self$init_max),
        trainable = TRUE
      )

      self$p &amp;lt;- k_sigmoid(self$p_logit)

      input_dim &amp;lt;- input_shape[[2]]

      weight &amp;lt;- private$py_wrapper$layer$kernel
      
      kernel_regularizer &amp;lt;- self$weight_regularizer * 
                            k_sum(k_square(weight)) / 
                            (1 - self$p)
      
      dropout_regularizer &amp;lt;- self$p * k_log(self$p)
      dropout_regularizer &amp;lt;- dropout_regularizer +  
                             (1 - self$p) * k_log(1 - self$p)
      dropout_regularizer &amp;lt;- dropout_regularizer * 
                             self$dropout_regularizer * 
                             k_cast(input_dim, k_floatx())

      regularizer &amp;lt;- k_sum(kernel_regularizer + dropout_regularizer)
      super$add_loss(regularizer)
    },
    
    concrete_dropout = function(x) {
      eps &amp;lt;- k_cast_to_floatx(k_epsilon())
      temp &amp;lt;- 0.1
      
      unif_noise &amp;lt;- k_random_uniform(shape = k_shape(x))
      
      drop_prob &amp;lt;- k_log(self$p + eps) - 
                   k_log(1 - self$p + eps) + 
                   k_log(unif_noise + eps) - 
                   k_log(1 - unif_noise + eps)
      drop_prob &amp;lt;- k_sigmoid(drop_prob / temp)
      
      random_tensor &amp;lt;- 1 - drop_prob
      
      retain_prob &amp;lt;- 1 - self$p
      x &amp;lt;- x * random_tensor
      x &amp;lt;- x / retain_prob
      x
    },

    call = function(x, mask = NULL, training = NULL) {
      if (self$is_mc_dropout) {
        super$call(self$concrete_dropout(x))
      } else {
        k_in_train_phase(
          function()
            super$call(self$concrete_dropout(x)),
          super$call(x),
          training = training
        )
      }
    }
  )
)

# function for instantiating custom wrapper
layer_concrete_dropout &amp;lt;- function(object, 
                                   layer,
                                   weight_regularizer = 1e-6,
                                   dropout_regularizer = 1e-5,
                                   init_min = 0.1,
                                   init_max = 0.1,
                                   is_mc_dropout = TRUE,
                                   name = NULL,
                                   trainable = TRUE) {
  create_wrapper(ConcreteDropout, object, list(
    layer = layer,
    weight_regularizer = weight_regularizer,
    dropout_regularizer = dropout_regularizer,
    init_min = init_min,
    init_max = init_max,
    is_mc_dropout = is_mc_dropout,
    name = name,
    trainable = trainable
  ))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The wrapper instantiator has default arguments, but two of them should be adapted to the data: &lt;code&gt;weight_regularizer&lt;/code&gt; and &lt;code&gt;dropout_regularizer&lt;/code&gt;. Following the authors’ recommendations, they should be set as follows.&lt;/p&gt;
&lt;p&gt;First, choose a value for hyperparameter &lt;span class="math inline"&gt;\(l\)&lt;/span&gt;. In this view of a neural network as an approximation to a Gaussian process, &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; is the &lt;em&gt;prior length-scale&lt;/em&gt;, our a priori assumption about the frequency characteristics of the data. Here, we follow Gal’s demo in setting &lt;code&gt;l := 1e-4&lt;/code&gt;. Then the initial values for &lt;code&gt;weight_regularizer&lt;/code&gt; and &lt;code&gt;dropout_regularizer&lt;/code&gt; are derived from the length-scale and the sample size.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# sample size (training data)
n_train &amp;lt;- 1000
# sample size (validation data)
n_val &amp;lt;- 1000
# prior length-scale
l &amp;lt;- 1e-4
# initial value for weight regularizer 
wd &amp;lt;- l^2/n_train
# initial value for dropout regularizer
dd &amp;lt;- 2/n_train&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s see how to use the wrapper in a model.&lt;/p&gt;
&lt;h3 id="dropout-model"&gt;Dropout model&lt;/h3&gt;
&lt;p&gt;In our demonstration, we’ll have a model with three hidden dense layers, each of which will have its dropout rate calculated by a dedicated wrapper.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# we use one-dimensional input data here, but this isn&amp;#39;t a necessity
input_dim &amp;lt;- 1
# this too could be &amp;gt; 1 if we wanted
output_dim &amp;lt;- 1
hidden_dim &amp;lt;- 1024

input &amp;lt;- layer_input(shape = input_dim)

output &amp;lt;- input %&amp;gt;% layer_concrete_dropout(
  layer = layer_dense(units = hidden_dim, activation = &amp;quot;relu&amp;quot;),
  weight_regularizer = wd,
  dropout_regularizer = dd
  ) %&amp;gt;% layer_concrete_dropout(
  layer = layer_dense(units = hidden_dim, activation = &amp;quot;relu&amp;quot;),
  weight_regularizer = wd,
  dropout_regularizer = dd
  ) %&amp;gt;% layer_concrete_dropout(
  layer = layer_dense(units = hidden_dim, activation = &amp;quot;relu&amp;quot;),
  weight_regularizer = wd,
  dropout_regularizer = dd
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, model output is interesting: We have the model yielding not just the &lt;em&gt;predictive (conditional) mean&lt;/em&gt;, but also the &lt;em&gt;predictive variance&lt;/em&gt; (&lt;span class="math inline"&gt;\(\tau^{-1}\)&lt;/span&gt; in Gaussian process parlance):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;mean &amp;lt;- output %&amp;gt;% layer_concrete_dropout(
  layer = layer_dense(units = output_dim),
  weight_regularizer = wd,
  dropout_regularizer = dd
)

log_var &amp;lt;- output %&amp;gt;% layer_concrete_dropout(
  layer_dense(units = output_dim),
  weight_regularizer = wd,
  dropout_regularizer = dd
)

output &amp;lt;- layer_concatenate(list(mean, log_var))

model &amp;lt;- keras_model(input, output)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The significant thing here is that we learn &lt;em&gt;different variances for different data points&lt;/em&gt;. We thus hope to be able to account for &lt;em&gt;heteroscedasticity&lt;/em&gt; (different degrees of variability) in the data.&lt;/p&gt;
&lt;h3 id="heteroscedastic-loss"&gt;Heteroscedastic loss&lt;/h3&gt;
&lt;p&gt;Accordingly, instead of mean squared error we use a cost function that does not treat all estimates alike&lt;span class="citation"&gt;(Kendall and Gal 2017)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\frac{1}{N} \sum_i{\frac{1}{2 \hat{\sigma}^2_i} \ (\mathbf{y}_i - \mathbf{\hat{y}}_i)^2 + \frac{1}{2} log \ \hat{\sigma}^2_i}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In addition to the obligatory target vs. prediction check, this cost function contains two regularization terms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First, &lt;span class="math inline"&gt;\(\frac{1}{2 \hat{\sigma}^2_i}\)&lt;/span&gt; downweights the high-uncertainty predictions in the loss function. Put plainly: The model is encouraged to indicate high uncertainty when its predictions are false.&lt;/li&gt;
&lt;li&gt;Second, &lt;span class="math inline"&gt;\(\frac{1}{2} log \ \hat{\sigma}^2_i\)&lt;/span&gt; makes sure the network does not simply indicate high uncertainty everywhere.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This logic maps directly to the code (except that as usual, we’re calculating with the log of the variance, for reasons of numerical stability):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;heteroscedastic_loss &amp;lt;- function(y_true, y_pred) {
    mean &amp;lt;- y_pred[, 1:output_dim]
    log_var &amp;lt;- y_pred[, (output_dim + 1):(output_dim * 2)]
    precision &amp;lt;- k_exp(-log_var)
    k_sum(precision * (y_true - mean) ^ 2 + log_var, axis = 2)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="training-on-simulated-data"&gt;Training on simulated data&lt;/h3&gt;
&lt;p&gt;Now we generate some test data and train the model.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;gen_data_1d &amp;lt;- function(n) {
  sigma &amp;lt;- 1
  X &amp;lt;- matrix(rnorm(n))
  w &amp;lt;- 2
  b &amp;lt;- 8
  Y &amp;lt;- matrix(X %*% w + b + sigma * rnorm(n))
  list(X, Y)
}

c(X, Y) %&amp;lt;-% gen_data_1d(n_train + n_val)

c(X_train, Y_train) %&amp;lt;-% list(X[1:n_train], Y[1:n_train])
c(X_val, Y_val) %&amp;lt;-% list(X[(n_train + 1):(n_train + n_val)], 
                          Y[(n_train + 1):(n_train + n_val)])

model %&amp;gt;% compile(
  optimizer = &amp;quot;adam&amp;quot;,
  loss = heteroscedastic_loss,
  metrics = c(custom_metric(&amp;quot;heteroscedastic_loss&amp;quot;, heteroscedastic_loss))
)

history &amp;lt;- model %&amp;gt;% fit(
  X_train,
  Y_train,
  epochs = 30,
  batch_size = 10
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With training finished, we turn to the validation set to obtain estimates on unseen data - including those uncertainty measures this is all about!&lt;/p&gt;
&lt;h3 id="obtain-uncertainty-estimates-via-monte-carlo-sampling"&gt;Obtain uncertainty estimates via Monte Carlo sampling&lt;/h3&gt;
&lt;p&gt;As often in a Bayesian setup, we construct the posterior (and thus, the posterior predictive) via Monte Carlo sampling. Unlike in traditional use of dropout, there is no change in behavior between training and test phases: Dropout stays “on”.&lt;/p&gt;
&lt;p&gt;So now we get an ensemble of model predictions on the validation set:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;num_MC_samples &amp;lt;- 20

MC_samples &amp;lt;- array(0, dim = c(num_MC_samples, n_val, 2 * output_dim))
for (k in 1:num_MC_samples) {
  MC_samples[k, , ] &amp;lt;- (model %&amp;gt;% predict(X_val))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember, our model predicts the mean as well as the variance. We’ll use the former for calculating epistemic uncertainty, while aleatoric uncertainty is obtained from the latter.&lt;/p&gt;
&lt;p&gt;First, we determine the predictive mean as an average of the MC samples’ &lt;em&gt;mean&lt;/em&gt; output:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# the means are in the first output column
means &amp;lt;- MC_samples[, , 1:output_dim]  
# average over the MC samples
predictive_mean &amp;lt;- apply(means, 2, mean) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To calculate epistemic uncertainty, we again use the &lt;em&gt;mean&lt;/em&gt; output, but this time we’re interested in the variance of the MC samples:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;epistemic_uncertainty &amp;lt;- apply(means, 2, var) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then aleatoric uncertainty is the average over the MC samples of the &lt;em&gt;variance&lt;/em&gt; output.&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;logvar &amp;lt;- MC_samples[, , (output_dim + 1):(output_dim * 2)]
aleatoric_uncertainty &amp;lt;- exp(colMeans(logvar))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note how this procedure gives us uncertainty estimates individually for every prediction. How do they look?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;df &amp;lt;- data.frame(
  x = X_val,
  y_pred = predictive_mean,
  e_u_lower = predictive_mean - sqrt(epistemic_uncertainty),
  e_u_upper = predictive_mean + sqrt(epistemic_uncertainty),
  a_u_lower = predictive_mean - sqrt(aleatoric_uncertainty),
  a_u_upper = predictive_mean + sqrt(aleatoric_uncertainty),
  u_overall_lower = predictive_mean - 
                    sqrt(epistemic_uncertainty) - 
                    sqrt(aleatoric_uncertainty),
  u_overall_upper = predictive_mean + 
                    sqrt(epistemic_uncertainty) + 
                    sqrt(aleatoric_uncertainty)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, first, is epistemic uncertainty, with shaded bands indicating one standard deviation above resp. below the predicted mean:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggplot(df, aes(x, y_pred)) + 
  geom_point() + 
  geom_ribbon(aes(ymin = e_u_lower, ymax = e_u_upper), alpha = 0.3)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-11-12-uncertainty_estimates_dropout/images/epistemic_1000.png" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Epistemic uncertainty on the validation set, train size = 1000.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This is interesting. The training data (as well as the validation data) were generated from a standard normal distribution, so the model has encountered many more examples close to the mean than outside two, or even three, standard deviations. So it correctly tells us that in those more exotic regions, it feels pretty unsure about its predictions.&lt;/p&gt;
&lt;p&gt;This is exactly the behavior we want: Risk in automatically applying machine learning methods arises due to unanticipated differences between the training and test (&lt;em&gt;real world&lt;/em&gt;) distributions. If the model were to tell us “ehm, not really seen anything like that before, don’t really know what to do” that’d be an enormously valuable outcome.&lt;/p&gt;
&lt;p&gt;So while epistemic uncertainty has the algorithm reflecting on its model of the world - potentially admitting its shortcomings - aleatoric uncertainty, by definition, is irreducible. Of course, that doesn’t make it any less valuable - we’d know we &lt;em&gt;always&lt;/em&gt; have to factor in a safety margin. So how does it look here?&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-11-12-uncertainty_estimates_dropout/images/aleatoric_1000.png" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Aleatoric uncertainty on the validation set, train size = 1000.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Indeed, the extent of uncertainty does not depend on the amount of data seen at training time.&lt;/p&gt;
&lt;p&gt;Finally, we add up both types to obtain the overall uncertainty when making predictions.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-11-12-uncertainty_estimates_dropout/images/overall_1000.png" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Overall predictive uncertainty on the validation set, train size = 1000.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now let’s try this method on a real-world dataset.&lt;/p&gt;
&lt;h2 id="combined-cycle-power-plant-electrical-energy-output-estimation"&gt;Combined cycle power plant electrical energy output estimation&lt;/h2&gt;
&lt;p&gt;This dataset is available from the &lt;a href="http://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant"&gt;UCI Machine Learning Repository&lt;/a&gt;. We explicitly chose a regression task with continuous variables exclusively, to make for a smooth transition from the simulated data.&lt;/p&gt;
&lt;p&gt;In the dataset providers’ &lt;a href="http://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant"&gt;own words&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The dataset contains 9568 data points collected from a Combined Cycle Power Plant over 6 years (2006-2011), when the power plant was set to work with full load. Features consist of hourly average ambient variables Temperature (T), Ambient Pressure (AP), Relative Humidity (RH) and Exhaust Vacuum (V) to predict the net hourly electrical energy output (EP) of the plant.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;A combined cycle power plant (CCPP) is composed of gas turbines (GT), steam turbines (ST) and heat recovery steam generators. In a CCPP, the electricity is generated by gas and steam turbines, which are combined in one cycle, and is transferred from one turbine to another. While the Vacuum is collected from and has effect on the Steam Turbine, the other three of the ambient variables effect the GT performance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We thus have four predictors and one target variable. We’ll train five models: four single-variable regressions and one making use of all four predictors. It probably goes without saying that our goal here is to inspect uncertainty information, not to fine-tune the model.&lt;/p&gt;
&lt;h3 id="setup"&gt;Setup&lt;/h3&gt;
&lt;p&gt;Let’s quickly inspect those five variables. Here &lt;code&gt;PE&lt;/code&gt; is energy output, the target variable.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(readxl)
library(dplyr)
library(GGally)

df &amp;lt;- read_xlsx(&amp;quot;CCPP/Folds5x2_pp.xlsx&amp;quot;)
ggscatmat(df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-11-12-uncertainty_estimates_dropout/images/scattermat.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;p&gt;We scale and divide up the data&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;df_scaled &amp;lt;- scale(df)

X &amp;lt;- df_scaled[, 1:4]
train_samples &amp;lt;- sample(1:nrow(df_scaled), 0.8 * nrow(X))
X_train &amp;lt;- X[train_samples,]
X_val &amp;lt;- X[-train_samples,]

y &amp;lt;- df_scaled[, 5] %&amp;gt;% as.matrix()
y_train &amp;lt;- y[train_samples,]
y_val &amp;lt;- y[-train_samples,]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and get ready for training a few models.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;n &amp;lt;- nrow(X_train)
n_epochs &amp;lt;- 100
batch_size &amp;lt;- 100
output_dim &amp;lt;- 1
num_MC_samples &amp;lt;- 20
l &amp;lt;- 1e-4
wd &amp;lt;- l^2/n
dd &amp;lt;- 2/n

get_model &amp;lt;- function(input_dim, hidden_dim) {
  
  input &amp;lt;- layer_input(shape = input_dim)
  output &amp;lt;-
    input %&amp;gt;% layer_concrete_dropout(
      layer = layer_dense(units = hidden_dim, activation = &amp;quot;relu&amp;quot;),
      weight_regularizer = wd,
      dropout_regularizer = dd
    ) %&amp;gt;% layer_concrete_dropout(
      layer = layer_dense(units = hidden_dim, activation = &amp;quot;relu&amp;quot;),
      weight_regularizer = wd,
      dropout_regularizer = dd
    ) %&amp;gt;% layer_concrete_dropout(
      layer = layer_dense(units = hidden_dim, activation = &amp;quot;relu&amp;quot;),
      weight_regularizer = wd,
      dropout_regularizer = dd
    )
  
  mean &amp;lt;-
    output %&amp;gt;% layer_concrete_dropout(
      layer = layer_dense(units = output_dim),
      weight_regularizer = wd,
      dropout_regularizer = dd
    )
  
  log_var &amp;lt;-
    output %&amp;gt;% layer_concrete_dropout(
      layer_dense(units = output_dim),
      weight_regularizer = wd,
      dropout_regularizer = dd
    )
  
  output &amp;lt;- layer_concatenate(list(mean, log_var))
  
  model &amp;lt;- keras_model(input, output)
  
  heteroscedastic_loss &amp;lt;- function(y_true, y_pred) {
    mean &amp;lt;- y_pred[, 1:output_dim]
    log_var &amp;lt;- y_pred[, (output_dim + 1):(output_dim * 2)]
    precision &amp;lt;- k_exp(-log_var)
    k_sum(precision * (y_true - mean) ^ 2 + log_var, axis = 2)
  }
  
  model %&amp;gt;% compile(optimizer = &amp;quot;adam&amp;quot;,
                    loss = heteroscedastic_loss,
                    metrics = c(&amp;quot;mse&amp;quot;))
  model
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll train each of the five models with a &lt;code&gt;hidden_dim&lt;/code&gt; of 64. We then obtain 20 Monte Carlo sample from the posterior predictive distribution and calculate the uncertainties as before.&lt;/p&gt;
&lt;p&gt;Here we show the code for the first predictor, “AT”. It is similar for all other cases.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- get_model(1, 64)
hist &amp;lt;- model %&amp;gt;% fit(
  X_train[ ,1],
  y_train,
  validation_data = list(X_val[ , 1], y_val),
  epochs = n_epochs,
  batch_size = batch_size
)

MC_samples &amp;lt;- array(0, dim = c(num_MC_samples, nrow(X_val), 2 * output_dim))
for (k in 1:num_MC_samples) {
  MC_samples[k, ,] &amp;lt;- (model %&amp;gt;% predict(X_val[ ,1]))
}

means &amp;lt;- MC_samples[, , 1:output_dim]  
predictive_mean &amp;lt;- apply(means, 2, mean) 
epistemic_uncertainty &amp;lt;- apply(means, 2, var) 
logvar &amp;lt;- MC_samples[, , (output_dim + 1):(output_dim * 2)]
aleatoric_uncertainty &amp;lt;- exp(colMeans(logvar))

preds &amp;lt;- data.frame(
  x1 = X_val[, 1],
  y_true = y_val,
  y_pred = predictive_mean,
  e_u_lower = predictive_mean - sqrt(epistemic_uncertainty),
  e_u_upper = predictive_mean + sqrt(epistemic_uncertainty),
  a_u_lower = predictive_mean - sqrt(aleatoric_uncertainty),
  a_u_upper = predictive_mean + sqrt(aleatoric_uncertainty),
  u_overall_lower = predictive_mean - 
                    sqrt(epistemic_uncertainty) - 
                    sqrt(aleatoric_uncertainty),
  u_overall_upper = predictive_mean + 
                    sqrt(epistemic_uncertainty) + 
                    sqrt(aleatoric_uncertainty)
)&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="result"&gt;Result&lt;/h3&gt;
&lt;p&gt;Now let’s see the uncertainty estimates for all five models!&lt;/p&gt;
&lt;p&gt;First, the single-predictor setup. Ground truth values are displayed in cyan, posterior predictive estimates are black, and the grey bands extend up resp. down by the square root of the calculated uncertainties.&lt;/p&gt;
&lt;p&gt;We’re starting with &lt;em&gt;ambient temperature&lt;/em&gt;, a low-variance predictor. We are surprised how confident the model is that it’s gotten the process logic correct, but high aleatoric uncertainty makes up for this (more or less).&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-11-12-uncertainty_estimates_dropout/images/uc_AT.png" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Uncertainties on the validation set using &lt;em&gt;ambient temperature&lt;/em&gt; as a single predictor.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now looking at the other predictors, where variance is much higher in the ground truth, it &lt;em&gt;does&lt;/em&gt; get a bit difficult to feel comfortable with the model’s confidence. Aleatoric uncertainty is high, but not high enough to capture the true variability in the data. And we certaintly would hope for higher epistemic uncertainty, especially in places where the model introduces arbitrary-looking deviations from linearity.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-11-12-uncertainty_estimates_dropout/images/uc_V.png" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Uncertainties on the validation set using &lt;em&gt;exhaust vacuum&lt;/em&gt; as a single predictor.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-11-12-uncertainty_estimates_dropout/images/uc_AP.png" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Uncertainties on the validation set using &lt;em&gt;ambient pressure&lt;/em&gt; as a single predictor.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-11-12-uncertainty_estimates_dropout/images/uc_RH.png" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Uncertainties on the validation set using &lt;em&gt;relative humidity&lt;/em&gt; as a single predictor.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now let’s see uncertainty output when we use all four predictors. We see that now, the Monte Carlo estimates vary a lot more, and accordingly, epistemic uncertainty is a lot higher. Aleatoric uncertainty, on the other hand, got a lot lower. Overall, predictive uncertainty captures the range of ground truth values pretty well.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-11-12-uncertainty_estimates_dropout/images/uc_4.png" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Uncertainties on the validation set using all 4 predictors.&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We’ve introduced a method to obtain theoretically grounded uncertainty estimates from neural networks. We find the approach intuitively attractive for several reasons: For one, the separation of different types of uncertainty is convincing and practically relevant. Second, uncertainty depends on the amount of data seen in the respective ranges. This is especially relevant when thinking of differences between training and test-time distributions. Third, the idea of having the network “become aware of its own uncertainty” is seductive.&lt;/p&gt;
&lt;p&gt;In practice though, there are open questions as to how to apply the method. From our real-world test above, we immediately ask: Why is the model so confident&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; when the ground truth data has high variance? And, thinking experimentally: How would that vary with different data sizes (rows), dimensionality (columns), and hyperparameter settings (including neural network hyperparameters like capacity, number of epochs trained, and activation functions, but also the Gaussian process prior length-scale &lt;span class="math inline"&gt;\(\tau\)&lt;/span&gt;)?&lt;/p&gt;
&lt;p&gt;For practical use, more experimentation with different datasets and hyperparameter settings is certainly warranted. Another direction to follow up is application to tasks in image recognition, such as semantic segmentation. Here we’d be interested in not just quantifying, but also localizing uncertainty, to see which visual aspects of a scene (occlusion, illumination, uncommon shapes) make objects hard to identify.&lt;/p&gt;
&lt;aside&gt;
This would require a slightly different wrapper class but again, an R implementation could follow the &lt;a href="https://github.com/yaringal/ConcreteDropout/blob/master/spatial-concrete-dropout-keras.ipynb"&gt;Python example in Yarin Gal’s repository&lt;/a&gt;.
&lt;/aside&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-GalG16"&gt;
&lt;p&gt;Gal, Yarin, and Zoubin Ghahramani. 2016. “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.” In &lt;em&gt;Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, Ny, Usa, June 19-24, 2016&lt;/em&gt;, 1050–9. &lt;a href="http://jmlr.org/proceedings/papers/v48/gal16.html"&gt;http://jmlr.org/proceedings/papers/v48/gal16.html&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-2017arXiv170507832G"&gt;
&lt;p&gt;Gal, Y., J. Hron, and A. Kendall. 2017. “Concrete Dropout.” &lt;em&gt;ArXiv E-Prints&lt;/em&gt;, May. &lt;a href="http://arxiv.org/abs/1705.07832"&gt;http://arxiv.org/abs/1705.07832&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-NIPS2017_7141"&gt;
&lt;p&gt;Kendall, Alex, and Yarin Gal. 2017. “What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?” In &lt;em&gt;Advances in Neural Information Processing Systems 30&lt;/em&gt;, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 5574–84. Curran Associates, Inc. &lt;a href="http://papers.nips.cc/paper/7141-what-uncertainties-do-we-need-in-bayesian-deep-learning-for-computer-vision.pdf"&gt;http://papers.nips.cc/paper/7141-what-uncertainties-do-we-need-in-bayesian-deep-learning-for-computer-vision.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Leibig084210"&gt;
&lt;p&gt;Leibig, Christian, Vaneeda Allken, Murat Seckin Ayhan, Philipp Berens, and Siegfried Wahl. 2017. “Leveraging Uncertainty Information from Deep Neural Networks for Disease Detection.” &lt;em&gt;bioRxiv&lt;/em&gt;. &lt;a href="https://doi.org/10.1101/084210"&gt;https://doi.org/10.1101/084210&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;exponentiated because we’ve really been working with the log of the variance&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;talking epistemic uncertainty&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">09bb54e7402089229da6cd60c3ee2ce8</distill:md5>
      <category>Image Recognition &amp; Image Processing</category>
      <category>Probabilistic ML/DL</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-11-12-uncertainty_estimates_dropout</guid>
      <pubDate>Mon, 12 Nov 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-11-12-uncertainty_estimates_dropout/images/thumb.png" medium="image" type="image/png" width="2046" height="872"/>
    </item>
    <item>
      <title>Naming and locating objects in images</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-11-05-naming-locating-objects</link>
      <description>


&lt;p&gt;We’ve all become used to deep learning’s success in image classification. &lt;em&gt;Greater Swiss Mountain dog&lt;/em&gt; or &lt;em&gt;Bernese mountain dog&lt;/em&gt;? &lt;em&gt;Red panda&lt;/em&gt; or &lt;em&gt;giant panda&lt;/em&gt;? No problem. However, in real life it’s not enough to name the single most salient object on a picture. Like it or not, one of the most compelling examples is autonomous driving: We don’t want the algorithm to recognize just that car in front of us, but also the pedestrian about to cross the street. And, just detecting the pedestrian is not sufficient. The exact &lt;em&gt;location&lt;/em&gt; of objects matters.&lt;/p&gt;
&lt;p&gt;The term &lt;em&gt;object detection&lt;/em&gt; is commonly used to refer to the task of naming and localizing multiple objects in an image frame. Object detection is difficult; we’ll build up to it in a loose series of posts, focusing on concepts instead of aiming for ultimate performance. Today, we’ll start with a few straightforward building blocks: Classification, both single and multiple; localization; and combining both classification and localization of a single object.&lt;/p&gt;
&lt;aside&gt;
The structure and approaches of these posts will follow the excellent &lt;a href="https://github.com/fastai/fastai/blob/master/courses/dl2/pascal-multi.ipynb"&gt;fast.ai notebook on object detection&lt;/a&gt;.
&lt;/aside&gt;
&lt;h2 id="dataset"&gt;Dataset&lt;/h2&gt;
&lt;p&gt;We’ll be using images and annotations from the &lt;em&gt;Pascal VOC dataset&lt;/em&gt; which can be downloaded from &lt;a href="https://pjreddie.com/projects/pascal-voc-dataset-mirror/"&gt;this mirror&lt;/a&gt;. Specifically, we’ll use data from the 2007 challenge and the same JSON annotation file as used in the &lt;em&gt;fast.ai&lt;/em&gt; course.&lt;/p&gt;
&lt;p&gt;Quick download/organization instructions, shamelessly taken from a &lt;a href="https://forums.fast.ai/t/quick-google-colab-setup-for-part-2-week-1-along-with-pascal-voc-dataset/13650"&gt;helpful post on the fast.ai wiki&lt;/a&gt;, are as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mkdir data &amp;amp;&amp;amp; cd data
# curl -OL http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar
# curl -OL https://storage.googleapis.com/coco-dataset/external/PASCAL_VOC.zip
# tar -xf VOCtrainval_06-Nov-2007.tar
# unzip PASCAL_VOC.zip
# mv PASCAL_VOC/*.json .
# rmdir PASCAL_VOC
# tar -xvf VOCtrainval_06-Nov-2007.tar&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In words, we take the images and the annotation file from different places:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar"&gt;http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar&lt;/a&gt; provides us with the images, and after unzipping all we care about is the &lt;code&gt;JPEGImages&lt;/code&gt; folder.&lt;/li&gt;
&lt;li&gt;From &lt;a href="https://storage.googleapis.com/coco-dataset/external/PASCAL_VOC.zip"&gt;https://storage.googleapis.com/coco-dataset/external/PASCAL_VOC.zip&lt;/a&gt; all we will be needing is the annotation file, &lt;code&gt;pascal_train2007.json&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Whether you’re executing the listed commands or arranging files manually, you should eventually end up with directories/files analogous to these:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;img_dir &amp;lt;- &amp;quot;data/VOCdevkit/VOC2007/JPEGImages&amp;quot;
annot_file &amp;lt;- &amp;quot;data/pascal_train2007.json&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we need to extract some information from that &lt;em&gt;json&lt;/em&gt; file.&lt;/p&gt;
&lt;h2 id="preprocessing"&gt;Preprocessing&lt;/h2&gt;
&lt;p&gt;Let’s quickly make sure we have all required libraries loaded.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
library(rjson)
library(magick)
library(purrr)
library(tibble)
library(tidyr)
library(dplyr)
library(ggplot2)
library(stringr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Annotations contain information about three types of things we’re interested in.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;annotations &amp;lt;- fromJSON(file = annot_file)
str(annotations, max.level = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;List of 4
 $ images     :List of 2501
 $ type       : chr &amp;quot;instances&amp;quot;
 $ annotations:List of 7844
 $ categories :List of 20&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, characteristics of the image itself (height and width) and where it’s stored. Not surprisingly, here it’s one entry per image.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;imageinfo &amp;lt;- annotations$images %&amp;gt;% {
  tibble(
    id = map_dbl(., &amp;quot;id&amp;quot;),
    file_name = map_chr(., &amp;quot;file_name&amp;quot;),
    image_height = map_dbl(., &amp;quot;height&amp;quot;),
    image_width = map_dbl(., &amp;quot;width&amp;quot;)
  )
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, object class ids and bounding box coordinates. There may be multiple of these per image. In Pascal VOC, there are 20 object classes, from ubiquitous vehicles (&lt;code&gt;car&lt;/code&gt;, &lt;code&gt;aeroplane&lt;/code&gt;) over indispensable animals (&lt;code&gt;cat&lt;/code&gt;, &lt;code&gt;sheep&lt;/code&gt;) to more rare (in popular datasets) types like &lt;code&gt;potted plant&lt;/code&gt; or &lt;code&gt;tv monitor&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;classes &amp;lt;- c(
  &amp;quot;aeroplane&amp;quot;,
  &amp;quot;bicycle&amp;quot;,
  &amp;quot;bird&amp;quot;,
  &amp;quot;boat&amp;quot;,
  &amp;quot;bottle&amp;quot;,
  &amp;quot;bus&amp;quot;,
  &amp;quot;car&amp;quot;,
  &amp;quot;cat&amp;quot;,
  &amp;quot;chair&amp;quot;,
  &amp;quot;cow&amp;quot;,
  &amp;quot;diningtable&amp;quot;,
  &amp;quot;dog&amp;quot;,
  &amp;quot;horse&amp;quot;,
  &amp;quot;motorbike&amp;quot;,
  &amp;quot;person&amp;quot;,
  &amp;quot;pottedplant&amp;quot;,
  &amp;quot;sheep&amp;quot;,
  &amp;quot;sofa&amp;quot;,
  &amp;quot;train&amp;quot;,
  &amp;quot;tvmonitor&amp;quot;
)

boxinfo &amp;lt;- annotations$annotations %&amp;gt;% {
  tibble(
    image_id = map_dbl(., &amp;quot;image_id&amp;quot;),
    category_id = map_dbl(., &amp;quot;category_id&amp;quot;),
    bbox = map(., &amp;quot;bbox&amp;quot;)
  )
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The bounding boxes are now stored in a list column and need to be unpacked.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;boxinfo &amp;lt;- boxinfo %&amp;gt;% 
  mutate(bbox = unlist(map(.$bbox, function(x) paste(x, collapse = &amp;quot; &amp;quot;))))
boxinfo &amp;lt;- boxinfo %&amp;gt;% 
  separate(bbox, into = c(&amp;quot;x_left&amp;quot;, &amp;quot;y_top&amp;quot;, &amp;quot;bbox_width&amp;quot;, &amp;quot;bbox_height&amp;quot;))
boxinfo &amp;lt;- boxinfo %&amp;gt;% mutate_all(as.numeric)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the bounding boxes, the annotation file provides &lt;code&gt;x_left&lt;/code&gt; and &lt;code&gt;y_top&lt;/code&gt; coordinates, as well as width and height. We will mostly be working with corner coordinates, so we create the missing &lt;code&gt;x_right&lt;/code&gt; and &lt;code&gt;y_bottom&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;As usual in image processing, the &lt;code&gt;y&lt;/code&gt; axis starts from the top.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;boxinfo &amp;lt;- boxinfo %&amp;gt;% 
  mutate(y_bottom = y_top + bbox_height - 1, x_right = x_left + bbox_width - 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we still need to match class ids to class names.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;catinfo &amp;lt;- annotations$categories %&amp;gt;%  {
  tibble(id = map_dbl(., &amp;quot;id&amp;quot;), name = map_chr(., &amp;quot;name&amp;quot;))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, putting it all together:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;imageinfo &amp;lt;- imageinfo %&amp;gt;%
  inner_join(boxinfo, by = c(&amp;quot;id&amp;quot; = &amp;quot;image_id&amp;quot;)) %&amp;gt;%
  inner_join(catinfo, by = c(&amp;quot;category_id&amp;quot; = &amp;quot;id&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that here still, we have several entries per image, each annotated object occupying its own row.&lt;/p&gt;
&lt;p&gt;There’s one step that will bitterly hurt our localization performance if we later forget it, so let’s do it now already: We need to scale all bounding box coordinates according to the actual image size we’ll use when we pass it to our network.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;target_height &amp;lt;- 224
target_width &amp;lt;- 224

imageinfo &amp;lt;- imageinfo %&amp;gt;% mutate(
  x_left_scaled = (x_left / image_width * target_width) %&amp;gt;% round(),
  x_right_scaled = (x_right / image_width * target_width) %&amp;gt;% round(),
  y_top_scaled = (y_top / image_height * target_height) %&amp;gt;% round(),
  y_bottom_scaled = (y_bottom / image_height * target_height) %&amp;gt;% round(),
  bbox_width_scaled =  (bbox_width / image_width * target_width) %&amp;gt;% round(),
  bbox_height_scaled = (bbox_height / image_height * target_height) %&amp;gt;% round()
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a glance at our data. Picking one of the early entries and displaying the original image together with the object annotation yields&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;img_data &amp;lt;- imageinfo[4,]
img &amp;lt;- image_read(file.path(img_dir, img_data$file_name))
img &amp;lt;- image_draw(img)
rect(
  img_data$x_left,
  img_data$y_bottom,
  img_data$x_right,
  img_data$y_top,
  border = &amp;quot;white&amp;quot;,
  lwd = 2
)
text(
  img_data$x_left,
  img_data$y_top,
  img_data$name,
  offset = 1,
  pos = 2,
  cex = 1.5,
  col = &amp;quot;white&amp;quot;
)
dev.off()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-11-05-naming-locating-objects/images/bicycle.jpeg" style="width:80.0%" /&gt;&lt;/p&gt;
&lt;p&gt;Now as indicated above, in this post we’ll mostly address handling a single object in an image. This means we have to decide, per image, which object to single out.&lt;/p&gt;
&lt;p&gt;A reasonable strategy seems to be choosing the object with the largest ground truth bounding box.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;imageinfo &amp;lt;- imageinfo %&amp;gt;% mutate(area = bbox_width_scaled * bbox_height_scaled)

imageinfo_maxbb &amp;lt;- imageinfo %&amp;gt;%
  group_by(id) %&amp;gt;%
  filter(which.max(area) == row_number())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After this operation, we only have 2501 images to work with - not many at all! For classification, we could simply use data augmentation as provided by Keras, but to work with localization we’d have to spin our own augmentation algorithm. We’ll leave this to a later occasion and for now, focus on the basics.&lt;/p&gt;
&lt;p&gt;Finally after train-test split&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_indices &amp;lt;- sample(1:n_samples, 0.8 * n_samples)
train_data &amp;lt;- imageinfo_maxbb[train_indices,]
validation_data &amp;lt;- imageinfo_maxbb[-train_indices,]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;our training set consists of 2000 images with one annotation each. We’re ready to start training, and we’ll start gently, with single-object classification.&lt;/p&gt;
&lt;h2 id="single-object-classification"&gt;Single-object classification&lt;/h2&gt;
&lt;p&gt;In all cases, we will use XCeption as a basic feature extractor. Having been trained on ImageNet, we don’t expect much fine tuning to be necessary to adapt to Pascal VOC, so we leave XCeption’s weights untouched&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;feature_extractor &amp;lt;-
  application_xception(
    include_top = FALSE,
    input_shape = c(224, 224, 3),
    pooling = &amp;quot;avg&amp;quot;
)

feature_extractor %&amp;gt;% freeze_weights()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and put just a few custom layers on top.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;%
  feature_extractor %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_dropout(rate = 0.25) %&amp;gt;%
  layer_dense(units = 512, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_dropout(rate = 0.5) %&amp;gt;%
  layer_dense(units = 20, activation = &amp;quot;softmax&amp;quot;)

model %&amp;gt;% compile(
  optimizer = &amp;quot;adam&amp;quot;,
  loss = &amp;quot;sparse_categorical_crossentropy&amp;quot;,
  metrics = list(&amp;quot;accuracy&amp;quot;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How should we pass our data to Keras? We could simple use Keras’ &lt;code&gt;image_data_generator&lt;/code&gt;, but given we will need custom generators soon, we’ll build a simple one ourselves. This one delivers images as well as the corresponding targets in a stream. Note how the targets are not one-hot-encoded, but integers - using &lt;code&gt;sparse_categorical_crossentropy&lt;/code&gt; as a loss function enables this convenience.&lt;/p&gt;
&lt;aside&gt;
See the &lt;a href="https://tensorflow.rstudio.com/learn/resources.html"&gt;Deep learning with R&lt;/a&gt; book for an introduction to writing data generators like this one.
&lt;/aside&gt;
&lt;pre class="r"&gt;&lt;code&gt;batch_size &amp;lt;- 10

load_and_preprocess_image &amp;lt;- function(image_name, target_height, target_width) {
  img_array &amp;lt;- image_load(
    file.path(img_dir, image_name),
    target_size = c(target_height, target_width)
    ) %&amp;gt;%
    image_to_array() %&amp;gt;%
    xception_preprocess_input() 
  dim(img_array) &amp;lt;- c(1, dim(img_array))
  img_array
}

classification_generator &amp;lt;-
  function(data,
           target_height,
           target_width,
           shuffle,
           batch_size) {
    i &amp;lt;- 1
    function() {
      if (shuffle) {
        indices &amp;lt;- sample(1:nrow(data), size = batch_size)
      } else {
        if (i + batch_size &amp;gt;= nrow(data))
          i &amp;lt;&amp;lt;- 1
        indices &amp;lt;- c(i:min(i + batch_size - 1, nrow(data)))
        i &amp;lt;&amp;lt;- i + length(indices)
      }
      x &amp;lt;-
        array(0, dim = c(length(indices), target_height, target_width, 3))
      y &amp;lt;- array(0, dim = c(length(indices), 1))
      
      for (j in 1:length(indices)) {
        x[j, , , ] &amp;lt;-
          load_and_preprocess_image(data[[indices[j], &amp;quot;file_name&amp;quot;]],
                                    target_height, target_width)
        y[j, ] &amp;lt;-
          data[[indices[j], &amp;quot;category_id&amp;quot;]] - 1
      }
      x &amp;lt;- x / 255
      list(x, y)
    }
  }

train_gen &amp;lt;- classification_generator(
  train_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = TRUE,
  batch_size = batch_size
)

valid_gen &amp;lt;- classification_generator(
  validation_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = FALSE,
  batch_size = batch_size
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now how does training go?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% fit_generator(
  train_gen,
  epochs = 20,
  steps_per_epoch = nrow(train_data) / batch_size,
  validation_data = valid_gen,
  validation_steps = nrow(validation_data) / batch_size,
  callbacks = list(
    callback_model_checkpoint(
      file.path(&amp;quot;class_only&amp;quot;, &amp;quot;weights.{epoch:02d}-{val_loss:.2f}.hdf5&amp;quot;)
    ),
    callback_early_stopping(patience = 2)
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For us, after 8 epochs, accuracies on the train resp. validation sets were at 0.68 and 0.74, respectively. Not too bad given given we’re trying to differentiate between 20 classes here.&lt;/p&gt;
&lt;p&gt;Now let’s quickly think what we’d change if we were to classify multiple objects in one image. Changes mostly concern preprocessing steps.&lt;/p&gt;
&lt;h2 id="multiple-object-classification"&gt;Multiple object classification&lt;/h2&gt;
&lt;p&gt;This time, we multi-hot-encode our data. For every image (as represented by its filename), here we have a vector of length 20 where 0 indicates absence, 1 means presence of the respective object class:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;image_cats &amp;lt;- imageinfo %&amp;gt;% 
  select(category_id) %&amp;gt;%
  mutate(category_id = category_id - 1) %&amp;gt;%
  pull() %&amp;gt;%
  to_categorical(num_classes = 20)

image_cats &amp;lt;- data.frame(image_cats) %&amp;gt;%
  add_column(file_name = imageinfo$file_name, .before = TRUE)

image_cats &amp;lt;- image_cats %&amp;gt;% 
  group_by(file_name) %&amp;gt;% 
  summarise_all(.funs = funs(max))

n_samples &amp;lt;- nrow(image_cats)
train_indices &amp;lt;- sample(1:n_samples, 0.8 * n_samples)
train_data &amp;lt;- image_cats[train_indices,]
validation_data &amp;lt;- image_cats[-train_indices,]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Correspondingly, we modify the generator to return a target of dimensions &lt;code&gt;batch_size&lt;/code&gt; * 20, instead of &lt;code&gt;batch_size&lt;/code&gt; * 1.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;classification_generator &amp;lt;- 
  function(data,
           target_height,
           target_width,
           shuffle,
           batch_size) {
    i &amp;lt;- 1
    function() {
      if (shuffle) {
        indices &amp;lt;- sample(1:nrow(data), size = batch_size)
      } else {
        if (i + batch_size &amp;gt;= nrow(data))
          i &amp;lt;&amp;lt;- 1
        indices &amp;lt;- c(i:min(i + batch_size - 1, nrow(data)))
        i &amp;lt;&amp;lt;- i + length(indices)
      }
      x &amp;lt;-
        array(0, dim = c(length(indices), target_height, target_width, 3))
      y &amp;lt;- array(0, dim = c(length(indices), 20))
      
      for (j in 1:length(indices)) {
        x[j, , , ] &amp;lt;-
          load_and_preprocess_image(data[[indices[j], &amp;quot;file_name&amp;quot;]], 
                                    target_height, target_width)
        y[j, ] &amp;lt;-
          data[indices[j], 2:21] %&amp;gt;% as.matrix()
      }
      x &amp;lt;- x / 255
      list(x, y)
    }
  }

train_gen &amp;lt;- classification_generator(
  train_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = TRUE,
  batch_size = batch_size
)

valid_gen &amp;lt;- classification_generator(
  validation_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = FALSE,
  batch_size = batch_size
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, the most interesting change is to the model - even though it’s a change to two lines only. Were we to use &lt;code&gt;categorical_crossentropy&lt;/code&gt; now (the non-sparse variant of the above), combined with a &lt;code&gt;softmax&lt;/code&gt; activation, we would effectively tell the model to pick just one, namely, the most probable object.&lt;/p&gt;
&lt;aside&gt;
See the &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-10-11-activations-intro/"&gt;introduction to loss functions and activations&lt;/a&gt; on this blog for a demonstration.
&lt;/aside&gt;
&lt;p&gt;Instead, we want to decide: For each object class, is it present in the image or not? Thus, instead of &lt;code&gt;softmax&lt;/code&gt; we use &lt;code&gt;sigmoid&lt;/code&gt;, paired with &lt;code&gt;binary_crossentropy&lt;/code&gt;, to obtain an independent verdict on every class.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;feature_extractor &amp;lt;-
  application_xception(
    include_top = FALSE,
    input_shape = c(224, 224, 3),
    pooling = &amp;quot;avg&amp;quot;
  )

feature_extractor %&amp;gt;% freeze_weights()

model &amp;lt;- keras_model_sequential() %&amp;gt;%
  feature_extractor %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_dropout(rate = 0.25) %&amp;gt;%
  layer_dense(units = 512, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_dropout(rate = 0.5) %&amp;gt;%
  layer_dense(units = 20, activation = &amp;quot;sigmoid&amp;quot;)

model %&amp;gt;% compile(optimizer = &amp;quot;adam&amp;quot;,
                  loss = &amp;quot;binary_crossentropy&amp;quot;,
                  metrics = list(&amp;quot;accuracy&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And finally, again, we fit the model:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% fit_generator(
  train_gen,
  epochs = 20,
  steps_per_epoch = nrow(train_data) / batch_size,
  validation_data = valid_gen,
  validation_steps = nrow(validation_data) / batch_size,
  callbacks = list(
    callback_model_checkpoint(
      file.path(&amp;quot;multiclass&amp;quot;, &amp;quot;weights.{epoch:02d}-{val_loss:.2f}.hdf5&amp;quot;)
    ),
    callback_early_stopping(patience = 2)
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This time, (binary) accuracy surpasses 0.95 after one epoch already, on both the train and validation sets. Not surprisingly, accuracy is significantly higher here than when we had to single out one of 20 classes (and that, with other confounding objects present in most cases!).&lt;/p&gt;
&lt;p&gt;Now, chances are that if you’ve done any deep learning before, you’ve done image classification in some form, perhaps even in the multiple-object variant. To build up in the direction of object detection, it is time we add a new ingredient: localization.&lt;/p&gt;
&lt;h2 id="single-object-localization"&gt;Single-object localization&lt;/h2&gt;
&lt;p&gt;From here on, we’re back to dealing with a single object per image. So the question now is, how do we learn bounding boxes? If you’ve never heard of this, the answer will sound unbelievably simple (naive even): We formulate this as a regression problem and aim to predict the actual coordinates. To set realistic expectations - we surely shouldn’t expect ultimate precision here. But in a way it’s amazing it does even work at all.&lt;/p&gt;
&lt;p&gt;What does this mean, formulate as a regression problem? Concretely, it means we’ll have a &lt;code&gt;dense&lt;/code&gt; output layer with 4 units, each corresponding to a corner coordinate.&lt;/p&gt;
&lt;p&gt;So let’s start with the model this time. Again, we use Xception, but there’s an important difference here: Whereas before, we said &lt;code&gt;pooling = "avg"&lt;/code&gt; to obtain an output tensor of dimensions &lt;code&gt;batch_size&lt;/code&gt; * number of filters, here we don’t do any averaging or flattening out of the spatial grid. This is because it’s exactly the spatial information we’re interested in!&lt;/p&gt;
&lt;p&gt;For Xception, the output resolution will be 7x7. So a priori, we shouldn’t expect high precision on objects much smaller than about 32x32 pixels (assuming the standard input size of 224x224).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;feature_extractor &amp;lt;- application_xception(
  include_top = FALSE,
  input_shape = c(224, 224, 3)
)

feature_extractor %&amp;gt;% freeze_weights()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we append our custom regression module.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;%
  feature_extractor %&amp;gt;%
  layer_flatten() %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_dropout(rate = 0.25) %&amp;gt;%
  layer_dense(units = 512, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_dropout(rate = 0.5) %&amp;gt;%
  layer_dense(units = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will train with one of the loss functions common in regression tasks, mean absolute error. But in tasks like object detection or segmentation, we’re also interested in a more tangible quantity: How much do estimate and ground truth overlap?&lt;/p&gt;
&lt;p&gt;Overlap is usually measured as &lt;em&gt;Intersection over Union&lt;/em&gt;, or &lt;em&gt;Jaccard distance&lt;/em&gt;. Intersection over Union is exactly what it says, a ratio between space shared by the objects and space occupied when we take them together.&lt;/p&gt;
&lt;p&gt;To assess the model’s progress, we can easily code this as a custom metric:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;metric_iou &amp;lt;- function(y_true, y_pred) {
  
  # order is [x_left, y_top, x_right, y_bottom]
  intersection_xmin &amp;lt;- k_maximum(y_true[ ,1], y_pred[ ,1])
  intersection_ymin &amp;lt;- k_maximum(y_true[ ,2], y_pred[ ,2])
  intersection_xmax &amp;lt;- k_minimum(y_true[ ,3], y_pred[ ,3])
  intersection_ymax &amp;lt;- k_minimum(y_true[ ,4], y_pred[ ,4])
  
  area_intersection &amp;lt;- (intersection_xmax - intersection_xmin) * 
                       (intersection_ymax - intersection_ymin)
  area_y &amp;lt;- (y_true[ ,3] - y_true[ ,1]) * (y_true[ ,4] - y_true[ ,2])
  area_yhat &amp;lt;- (y_pred[ ,3] - y_pred[ ,1]) * (y_pred[ ,4] - y_pred[ ,2])
  area_union &amp;lt;- area_y + area_yhat - area_intersection
  
  iou &amp;lt;- area_intersection/area_union
  k_mean(iou)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Model compilation then goes like&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% compile(
  optimizer = &amp;quot;adam&amp;quot;,
  loss = &amp;quot;mae&amp;quot;,
  metrics = list(custom_metric(&amp;quot;iou&amp;quot;, metric_iou))
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now modify the generator to return bounding box coordinates as targets…&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;localization_generator &amp;lt;-
  function(data,
           target_height,
           target_width,
           shuffle,
           batch_size) {
    i &amp;lt;- 1
    function() {
      if (shuffle) {
        indices &amp;lt;- sample(1:nrow(data), size = batch_size)
      } else {
        if (i + batch_size &amp;gt;= nrow(data))
          i &amp;lt;&amp;lt;- 1
        indices &amp;lt;- c(i:min(i + batch_size - 1, nrow(data)))
        i &amp;lt;&amp;lt;- i + length(indices)
      }
      x &amp;lt;-
        array(0, dim = c(length(indices), target_height, target_width, 3))
      y &amp;lt;- array(0, dim = c(length(indices), 4))
      
      for (j in 1:length(indices)) {
        x[j, , , ] &amp;lt;-
          load_and_preprocess_image(data[[indices[j], &amp;quot;file_name&amp;quot;]], 
                                    target_height, target_width)
        y[j, ] &amp;lt;-
          data[indices[j], c(&amp;quot;x_left_scaled&amp;quot;,
                             &amp;quot;y_top_scaled&amp;quot;,
                             &amp;quot;x_right_scaled&amp;quot;,
                             &amp;quot;y_bottom_scaled&amp;quot;)] %&amp;gt;% as.matrix()
      }
      x &amp;lt;- x / 255
      list(x, y)
    }
  }

train_gen &amp;lt;- localization_generator(
  train_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = TRUE,
  batch_size = batch_size
)

valid_gen &amp;lt;- localization_generator(
  validation_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = FALSE,
  batch_size = batch_size
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;… and we’re ready to go!&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% fit_generator(
  train_gen,
  epochs = 20,
  steps_per_epoch = nrow(train_data) / batch_size,
  validation_data = valid_gen,
  validation_steps = nrow(validation_data) / batch_size,
  callbacks = list(
    callback_model_checkpoint(
      file.path(&amp;quot;loc_only&amp;quot;, &amp;quot;weights.{epoch:02d}-{val_loss:.2f}.hdf5&amp;quot;)
    ),
    callback_early_stopping(patience = 2)
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After 8 epochs, IOU on both training and test sets is around 0.35. This number doesn’t look too good. To learn more about how training went, we need to see some predictions. Here’s a convenience function that displays an image, the ground truth box of the most salient object (as defined above), and if given, class and bounding box predictions.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot_image_with_boxes &amp;lt;- function(file_name,
                                  object_class,
                                  box,
                                  scaled = FALSE,
                                  class_pred = NULL,
                                  box_pred = NULL) {
  img &amp;lt;- image_read(file.path(img_dir, file_name))
  if(scaled) img &amp;lt;- image_resize(img, geometry = &amp;quot;224x224!&amp;quot;)
  img &amp;lt;- image_draw(img)
  x_left &amp;lt;- box[1]
  y_bottom &amp;lt;- box[2]
  x_right &amp;lt;- box[3]
  y_top &amp;lt;- box[4]
  rect(
    x_left,
    y_bottom,
    x_right,
    y_top,
    border = &amp;quot;cyan&amp;quot;,
    lwd = 2.5
  )
  text(
    x_left,
    y_top,
    object_class,
    offset = 1,
    pos = 2,
    cex = 1.5,
    col = &amp;quot;cyan&amp;quot;
  )
  if (!is.null(box_pred))
    rect(box_pred[1],
         box_pred[2],
         box_pred[3],
         box_pred[4],
         border = &amp;quot;yellow&amp;quot;,
         lwd = 2.5)
  if (!is.null(class_pred))
    text(
      box_pred[1],
      box_pred[2],
      class_pred,
      offset = 0,
      pos = 4,
      cex = 1.5,
      col = &amp;quot;yellow&amp;quot;)
  dev.off()
  img %&amp;gt;% image_write(paste0(&amp;quot;preds_&amp;quot;, file_name))
  plot(img)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, let’s see predictions on sample images from the training set.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_1_8 &amp;lt;- train_data[1:8, c(&amp;quot;file_name&amp;quot;,
                               &amp;quot;name&amp;quot;,
                               &amp;quot;x_left_scaled&amp;quot;,
                               &amp;quot;y_top_scaled&amp;quot;,
                               &amp;quot;x_right_scaled&amp;quot;,
                               &amp;quot;y_bottom_scaled&amp;quot;)]

for (i in 1:8) {
  preds &amp;lt;-
    model %&amp;gt;% predict(
      load_and_preprocess_image(train_1_8[i, &amp;quot;file_name&amp;quot;], 
                                target_height, target_width),
      batch_size = 1
  )
  plot_image_with_boxes(train_1_8$file_name[i],
                        train_1_8$name[i],
                        train_1_8[i, 3:6] %&amp;gt;% as.matrix(),
                        scaled = TRUE,
                        box_pred = preds)
}&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-11-05-naming-locating-objects/images/preds_train.jpg" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Sample bounding box predictions on the training set.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As you’d guess from looking, the cyan-colored boxes are the ground truth ones. Now looking at the predictions explains a lot about the mediocre IOU values! Let’s take the very first sample image - we wanted the model to focus on the sofa, but it picked the table, which is also a category in the dataset (although in the form of &lt;em&gt;dining&lt;/em&gt; &lt;em&gt;table&lt;/em&gt;). Similar with the image on the right of the first row - we wanted to it to pick just the dog but it included the person, too (by far the most frequently seen category in the dataset). So we actually made the task a lot more difficult than had we stayed with e.g., ImageNet where normally a single object is salient.&lt;/p&gt;
&lt;p&gt;Now check predictions on the validation set.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-11-05-naming-locating-objects/images/preds_valid.jpg" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Some bounding box predictions on the validation set.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Again, we get a similar impression: The model &lt;em&gt;did&lt;/em&gt; learn something, but the task is ill defined. Look at the third image in row 2: Isn’t it pretty consequent the model picks &lt;em&gt;all&lt;/em&gt; people instead of singling out some special guy?&lt;/p&gt;
&lt;p&gt;If single-object localization is that straightforward, how technically involved can it be to output a class label at the same time? As long as we stay with a single object, the answer indeed is: not much.&lt;/p&gt;
&lt;aside&gt;
As a caveat, please note we’re talking about mapping concepts to technical approaches here. Obtaining ultimate performance is a different thing.
&lt;/aside&gt;
&lt;p&gt;Let’s finish up today with a constrained combination of classification and localization: detection of a single object.&lt;/p&gt;
&lt;h2 id="single-object-detection"&gt;Single-object detection&lt;/h2&gt;
&lt;p&gt;Combining regression and classification into one means we’ll want to have two outputs in our model. We’ll thus use the functional API this time. Otherwise, there isn’t much new here: We start with an XCeption output of spatial resolution 7x7, append some custom processing and return two outputs, one for bounding box regression and one for classification.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;feature_extractor &amp;lt;- application_xception(
  include_top = FALSE,
  input_shape = c(224, 224, 3)
)

input &amp;lt;- feature_extractor$input
common &amp;lt;- feature_extractor$output %&amp;gt;%
  layer_flatten(name = &amp;quot;flatten&amp;quot;) %&amp;gt;%
  layer_activation_relu() %&amp;gt;%
  layer_dropout(rate = 0.25) %&amp;gt;%
  layer_dense(units = 512, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_batch_normalization() %&amp;gt;%
  layer_dropout(rate = 0.5)

regression_output &amp;lt;-
  layer_dense(common, units = 4, name = &amp;quot;regression_output&amp;quot;)
class_output &amp;lt;- layer_dense(
  common,
  units = 20,
  activation = &amp;quot;softmax&amp;quot;,
  name = &amp;quot;class_output&amp;quot;
)

model &amp;lt;- keras_model(
  inputs = input,
  outputs = list(regression_output, class_output)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When defining the losses (mean absolute error and categorical crossentropy, just as in the respective single tasks of regression and classification), we could weight them so they end up on approximately a common scale. In fact that didn’t make much of a difference so we show the respective code in commented form.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% freeze_weights(to = &amp;quot;flatten&amp;quot;)

model %&amp;gt;% compile(
  optimizer = &amp;quot;adam&amp;quot;,
  loss = list(&amp;quot;mae&amp;quot;, &amp;quot;sparse_categorical_crossentropy&amp;quot;),
  #loss_weights = list(
  #  regression_output = 0.05,
  #  class_output = 0.95),
  metrics = list(
    regression_output = custom_metric(&amp;quot;iou&amp;quot;, metric_iou),
    class_output = &amp;quot;accuracy&amp;quot;
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just like model outputs and losses are both lists, the data generator has to return the ground truth samples in a list. Fitting the model then goes as usual.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;loc_class_generator &amp;lt;-
  function(data,
           target_height,
           target_width,
           shuffle,
           batch_size) {
    i &amp;lt;- 1
    function() {
      if (shuffle) {
        indices &amp;lt;- sample(1:nrow(data), size = batch_size)
      } else {
        if (i + batch_size &amp;gt;= nrow(data))
          i &amp;lt;&amp;lt;- 1
        indices &amp;lt;- c(i:min(i + batch_size - 1, nrow(data)))
        i &amp;lt;&amp;lt;- i + length(indices)
      }
      x &amp;lt;-
        array(0, dim = c(length(indices), target_height, target_width, 3))
      y1 &amp;lt;- array(0, dim = c(length(indices), 4))
      y2 &amp;lt;- array(0, dim = c(length(indices), 1))
      
      for (j in 1:length(indices)) {
        x[j, , , ] &amp;lt;-
          load_and_preprocess_image(data[[indices[j], &amp;quot;file_name&amp;quot;]], 
                                    target_height, target_width)
        y1[j, ] &amp;lt;-
          data[indices[j], c(&amp;quot;x_left&amp;quot;, &amp;quot;y_top&amp;quot;, &amp;quot;x_right&amp;quot;, &amp;quot;y_bottom&amp;quot;)] 
            %&amp;gt;% as.matrix()
        y2[j, ] &amp;lt;-
          data[[indices[j], &amp;quot;category_id&amp;quot;]] - 1
      }
      x &amp;lt;- x / 255
      list(x, list(y1, y2))
    }
  }

train_gen &amp;lt;- loc_class_generator(
  train_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = TRUE,
  batch_size = batch_size
)

valid_gen &amp;lt;- loc_class_generator(
  validation_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = FALSE,
  batch_size = batch_size
)

model %&amp;gt;% fit_generator(
  train_gen,
  epochs = 20,
  steps_per_epoch = nrow(train_data) / batch_size,
  validation_data = valid_gen,
  validation_steps = nrow(validation_data) / batch_size,
  callbacks = list(
    callback_model_checkpoint(
      file.path(&amp;quot;loc_class&amp;quot;, &amp;quot;weights.{epoch:02d}-{val_loss:.2f}.hdf5&amp;quot;)
    ),
    callback_early_stopping(patience = 2)
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What about model predictions? A priori we might expect the bounding boxes to look better than in the regression-only model, as a significant part of the model is shared between classification and localization. Intuitively, I should be able to more precisely indicate the boundaries of &lt;em&gt;something&lt;/em&gt; if I have an idea what that &lt;em&gt;something&lt;/em&gt; is.&lt;/p&gt;
&lt;p&gt;Unfortunately, that didn’t quite happen. The model has become &lt;em&gt;very&lt;/em&gt; biased to detecting a &lt;em&gt;person&lt;/em&gt; everywhere, which might be advantageous (thinking safety) in an autonomous driving application but isn’t quite what we’d hoped for here.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-11-05-naming-locating-objects/images/preds_train_2.jpg" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Example class and bounding box predictions on the training set.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-11-05-naming-locating-objects/images/preds_valid_2.jpg" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Example class and bounding box predictions on the validation set.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Just to double-check this really has to do with class imbalance, here are the actual frequencies:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;imageinfo %&amp;gt;% group_by(name)
  %&amp;gt;% summarise(cnt = n()) 
  %&amp;gt;% arrange(desc(cnt))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 20 x 2
   name          cnt
   &amp;lt;chr&amp;gt;       &amp;lt;int&amp;gt;
 1 person       2705
 2 car           826
 3 chair         726
 4 bottle        338
 5 pottedplant   305
 6 bird          294
 7 dog           271
 8 sofa          218
 9 boat          208
10 horse         207
11 bicycle       202
12 motorbike     193
13 cat           191
14 sheep         191
15 tvmonitor     191
16 cow           185
17 train         158
18 aeroplane     156
19 diningtable   148
20 bus           131&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To get better performance, we’d need to find a successful way to deal with this. However, handling class imbalance in deep learning is a topic of its own, and here we want to build up in the direction of objection detection. So we’ll make a cut here and in an upcoming post, think about how we can classify and localize multiple objects in an image.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We have seen that single-object classification and localization are conceptually straightforward. The big question now is, are these approaches extensible to multiple objects? Or will new ideas have to come in? We’ll follow up on this giving a short overview of approaches and then, singling in on one of those and implementing it.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">7f4431c4a0a22fb1f38dc5f1c53c0076</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-11-05-naming-locating-objects</guid>
      <pubDate>Mon, 05 Nov 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-11-05-naming-locating-objects/images/preds_train.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Representation learning with MMD-VAE</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae</link>
      <description>


&lt;p&gt;Recently, we showed how to &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-08-26-eager-dcgan/"&gt;generate images&lt;/a&gt; using generative adversarial networks (GANs). GANs may yield amazing results, but the contract there basically is: what you see is what you get. Sometimes this may be all we want. In other cases, we may be more interested in actually modelling a domain. We don’t just want to generate realistic-looking samples - we want our samples to be located at specific coordinates in domain space.&lt;/p&gt;
&lt;p&gt;For example, imagine our domain to be the space of facial expressions. Then our latent space might be conceived as two-dimensional: In accordance with underlying emotional states, expressions vary on a positive-negative scale. At the same time, they vary in intensity. Now if we trained a VAE on a set of facial expressions adequately covering the ranges, and it did in fact “discover” our hypothesized dimensions, we could then use it to generate previously-nonexisting incarnations of points (faces, that is) in latent space.&lt;/p&gt;
&lt;p&gt;Variational autoencoders are similar to probabilistic graphical models in that they assume a latent space that is responsible for the observations, but unobservable. They are similar to plain autoencoders in that they compress, and then decompress again, the input domain. In contrast to plain autoencoders though, the crucial point here is to devise a loss function that allows to obtain informative representations in latent space.&lt;/p&gt;
&lt;h2 id="in-a-nutshell"&gt;In a nutshell&lt;/h2&gt;
&lt;p&gt;In standard VAEs &lt;span class="citation"&gt;(Kingma and Welling 2013)&lt;/span&gt;, the objective is to maximize the evidence lower bound (ELBO):&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[ELBO\ = \ E[log\ p(x|z)]\ -\ KL(q(z)||p(z))\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In plain words and expressed in terms of how we use it in practice, the first component is the &lt;em&gt;reconstruction loss&lt;/em&gt; we also see in plain (non-variational) autoencoders. The second is the Kullback-Leibler divergence between a prior imposed on the latent space (typically, a standard normal distribution) and the representation of latent space as learned from the data.&lt;/p&gt;
&lt;aside&gt;
For a well-written and intuitive introduction to VAEs, including the why and how of their optimization, see this &lt;a href="https://arxiv.org/abs/1606.05908"&gt;Tutorial on variational autoencoders&lt;/a&gt; &lt;span class="citation"&gt;(Doersch 2016)&lt;/span&gt;.
&lt;/aside&gt;
&lt;p&gt;A major criticism regarding the traditional VAE loss is that it results in uninformative latent space. Alternatives include &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt;-VAE&lt;span class="citation"&gt;(Burgess et al. 2018)&lt;/span&gt;, Info-VAE &lt;span class="citation"&gt;(Zhao, Song, and Ermon 2017)&lt;/span&gt;, and more. The MMD-VAE&lt;span class="citation"&gt;(Zhao, Song, and Ermon 2017)&lt;/span&gt; implemented below is a subtype of Info-VAE that instead of making each representation in latent space as similar as possible to the prior, coerces the respective &lt;em&gt;distributions&lt;/em&gt; to be as close as possible. Here MMD stands for &lt;em&gt;maximum mean discrepancy&lt;/em&gt;, a similarity measure for distributions based on matching their respective moments. We explain this in more detail below.&lt;/p&gt;
&lt;aside&gt;
The main author of the paper&lt;span class="citation"&gt;(Zhao, Song, and Ermon 2017)&lt;/span&gt; has a &lt;a href="http://szhao.me/2017/06/10/a-tutorial-on-mmd-variational-autoencoders.html"&gt;tutorial&lt;/a&gt; on his website explaining the reasons behind this choice of cost function in a very accessible way.
&lt;/aside&gt;
&lt;h2 id="our-objective-today"&gt;Our objective today&lt;/h2&gt;
&lt;p&gt;In this post, we are first going to implement a standard VAE that strives to maximize the ELBO. Then, we compare its performance to that of an Info-VAE using the MMD loss.&lt;/p&gt;
&lt;p&gt;Our focus will be on inspecting the latent spaces and see if, and how, they differ as a consequence of the optimization criteria used.&lt;/p&gt;
&lt;p&gt;The domain we’re going to model will be glamorous (fashion!), but for the sake of manageability, confined to size 28 x 28: We’ll compress and reconstruct images from the &lt;a href="https://github.com/zalandoresearch/fashion-mnist"&gt;Fashion MNIST&lt;/a&gt; dataset that has been developed as a drop-in to MNIST.&lt;/p&gt;
&lt;h2 id="a-standard-variational-autoencoder"&gt;A standard variational autoencoder&lt;/h2&gt;
&lt;p&gt;Seeing we haven’t used TensorFlow eager execution for some weeks, we’ll do the model in an eager way. If you’re new to eager execution, don’t worry: As every new technique, it needs some getting accustomed to, but you’ll quickly find that many tasks are made easier if you use it. A simple yet complete, template-like example is available as part of the &lt;a href="https://tensorflow.rstudio.com/tutorials/advanced/"&gt;Keras documentation&lt;/a&gt;&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;aside&gt;
For interesting applications using eager execution in combination with Keras, ranging from machine translation to neural style transfer, see the recent posts in the &lt;a href="https://blogs.rstudio.com/tensorflow/#Eager"&gt;Eager category&lt;/a&gt; on this blog.
&lt;/aside&gt;
&lt;h4 id="setup-and-data-preparation"&gt;Setup and data preparation&lt;/h4&gt;
&lt;p&gt;As usual, we start by making sure we’re using the TensorFlow implementation of Keras and enabling eager execution. Besides &lt;code&gt;tensorflow&lt;/code&gt; and &lt;code&gt;keras&lt;/code&gt;, we also load &lt;code&gt;tfdatasets&lt;/code&gt; for use in data streaming.&lt;/p&gt;
&lt;p&gt;By the way: No need to copy-paste any of the below code snippets. The two approaches are available among our Keras examples, namely, as &lt;a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/eager_cvae.R"&gt;eager_cvae.R&lt;/a&gt; and &lt;a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/mmd_cvae.R"&gt;mmd_cvae.R&lt;/a&gt;.&lt;/p&gt;
&lt;aside&gt;
You might find it interesting to compare non-eager Keras code implementing a variational autoencoder: see &lt;a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/variational_autoencoder_deconv.R"&gt;variational_autoencoder_deconv.R&lt;/a&gt;.
&lt;/aside&gt;
&lt;pre class="r"&gt;&lt;code&gt;# the following 5 lines have to be executed in this order
library(keras)
use_implementation(&amp;quot;tensorflow&amp;quot;)
library(tensorflow)
tfe_enable_eager_execution(device_policy = &amp;quot;silent&amp;quot;)

library(tfdatasets)

library(dplyr)
library(ggplot2)
library(glue)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data comes conveniently with &lt;code&gt;keras&lt;/code&gt;, all we need to do is the usual normalization and reshaping.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;fashion &amp;lt;- dataset_fashion_mnist()

c(train_images, train_labels) %&amp;lt;-% fashion$train
c(test_images, test_labels) %&amp;lt;-% fashion$test

train_x &amp;lt;- train_images %&amp;gt;%
  `/`(255) %&amp;gt;%
  k_reshape(c(60000, 28, 28, 1))

test_x &amp;lt;- test_images %&amp;gt;% `/`(255) %&amp;gt;%
  k_reshape(c(10000, 28, 28, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What do we need the test set for, given we are going to train an unsupervised (a better term being: &lt;em&gt;semi-supervised&lt;/em&gt;) model? We’ll use it to see how (previously unknown) data points cluster together in latent space.&lt;/p&gt;
&lt;p&gt;Now prepare for streaming the data to &lt;code&gt;keras&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;buffer_size &amp;lt;- 60000
batch_size &amp;lt;- 100
batches_per_epoch &amp;lt;- buffer_size / batch_size

train_dataset &amp;lt;- tensor_slices_dataset(train_x) %&amp;gt;%
  dataset_shuffle(buffer_size) %&amp;gt;%
  dataset_batch(batch_size)

test_dataset &amp;lt;- tensor_slices_dataset(test_x) %&amp;gt;%
  dataset_batch(10000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next up is defining the model.&lt;/p&gt;
&lt;h4 id="encoder-decoder-model"&gt;Encoder-decoder model&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;The model&lt;/em&gt; really is two models: the encoder and the decoder. As we’ll see shortly, in the standard version of the VAE there is a third component in between, performing the so-called &lt;em&gt;reparameterization trick&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The encoder is a &lt;a href="https://tensorflow.rstudio.com/keras/articles/custom_models.html"&gt;custom model&lt;/a&gt;, comprised of two convolutional layers and a dense layer. It returns the output of the dense layer split into two parts, one storing the mean of the latent variables, the other their variance.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;latent_dim &amp;lt;- 2

encoder_model &amp;lt;- function(name = NULL) {
  
  keras_model_custom(name = name, function(self) {
    self$conv1 &amp;lt;-
      layer_conv_2d(
        filters = 32,
        kernel_size = 3,
        strides = 2,
        activation = &amp;quot;relu&amp;quot;
      )
    self$conv2 &amp;lt;-
      layer_conv_2d(
        filters = 64,
        kernel_size = 3,
        strides = 2,
        activation = &amp;quot;relu&amp;quot;
      )
    self$flatten &amp;lt;- layer_flatten()
    self$dense &amp;lt;- layer_dense(units = 2 * latent_dim)
    
    function (x, mask = NULL) {
      x %&amp;gt;%
        self$conv1() %&amp;gt;%
        self$conv2() %&amp;gt;%
        self$flatten() %&amp;gt;%
        self$dense() %&amp;gt;%
        tf$split(num_or_size_splits = 2L, axis = 1L) 
    }
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We choose the latent space to be of dimension 2 - just because that makes visualization easy. With more complex data, you will probably benefit from choosing a higher dimensionality here.&lt;/p&gt;
&lt;p&gt;So the encoder compresses real data into estimates of mean and variance of the latent space. We then “indirectly” sample from this distribution (the so-called &lt;em&gt;reparameterization trick&lt;/em&gt;):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;reparameterize &amp;lt;- function(mean, logvar) {
  eps &amp;lt;- k_random_normal(shape = mean$shape, dtype = tf$float64)
  eps * k_exp(logvar * 0.5) + mean
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The sampled values will serve as input to the decoder, who will attempt to map them back to the original space. The decoder is basically a sequence of transposed convolutions, upsampling until we reach a resolution of 28x28.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;decoder_model &amp;lt;- function(name = NULL) {
  
  keras_model_custom(name = name, function(self) {
    
    self$dense &amp;lt;- layer_dense(units = 7 * 7 * 32, activation = &amp;quot;relu&amp;quot;)
    self$reshape &amp;lt;- layer_reshape(target_shape = c(7, 7, 32))
    self$deconv1 &amp;lt;-
      layer_conv_2d_transpose(
        filters = 64,
        kernel_size = 3,
        strides = 2,
        padding = &amp;quot;same&amp;quot;,
        activation = &amp;quot;relu&amp;quot;
      )
    self$deconv2 &amp;lt;-
      layer_conv_2d_transpose(
        filters = 32,
        kernel_size = 3,
        strides = 2,
        padding = &amp;quot;same&amp;quot;,
        activation = &amp;quot;relu&amp;quot;
      )
    self$deconv3 &amp;lt;-
      layer_conv_2d_transpose(
        filters = 1,
        kernel_size = 3,
        strides = 1,
        padding = &amp;quot;same&amp;quot;
      )
    
    function (x, mask = NULL) {
      x %&amp;gt;%
        self$dense() %&amp;gt;%
        self$reshape() %&amp;gt;%
        self$deconv1() %&amp;gt;%
        self$deconv2() %&amp;gt;%
        self$deconv3()
    }
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note how the final deconvolution does not have the sigmoid activation you might have expected. This is because we will be using &lt;code&gt;tf$nn$sigmoid_cross_entropy_with_logits&lt;/code&gt; when calculating the loss.&lt;/p&gt;
&lt;p&gt;Speaking of losses, let’s inspect them now.&lt;/p&gt;
&lt;h4 id="loss-calculations"&gt;Loss calculations&lt;/h4&gt;
&lt;p&gt;One way to implement the VAE loss is combining reconstruction loss (cross entropy, in the present case) and Kullback-Leibler divergence. In Keras, the latter is available directly as &lt;code&gt;loss_kullback_leibler_divergence&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here, we follow a recent &lt;a href="https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/cvae.ipynb"&gt;Google Colaboratory notebook&lt;/a&gt; in batch-estimating the complete ELBO instead (instead of just estimating reconstruction loss and computing the KL-divergence analytically):&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[ELBO \ batch \ estimate = log\ p(x_{batch}|z_{sampled})+log\ p(z)−log\ q(z_{sampled}|x_{batch})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Calculation of the normal loglikelihood is packaged into a function so we can reuse it during the training loop.&lt;/p&gt;
&lt;aside&gt;
Note that we’re calculating with the log of the variance, instead of the variance, for reasons of numerical stability.
&lt;/aside&gt;
&lt;pre class="r"&gt;&lt;code&gt;normal_loglik &amp;lt;- function(sample, mean, logvar, reduce_axis = 2) {
  loglik &amp;lt;- k_constant(0.5, dtype = tf$float64) *
    (k_log(2 * k_constant(pi, dtype = tf$float64)) +
    logvar +
    k_exp(-logvar) * (sample - mean) ^ 2)
  - k_sum(loglik, axis = reduce_axis)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Peeking ahead some, during training we will compute the above as follows.&lt;/p&gt;
&lt;p&gt;First,&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;crossentropy_loss &amp;lt;- tf$nn$sigmoid_cross_entropy_with_logits(
  logits = preds,
  labels = x
)
logpx_z &amp;lt;- - k_sum(crossentropy_loss)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;yields &lt;span class="math inline"&gt;\(log \ p(x|z)\)&lt;/span&gt;, the loglikelihood of the reconstructed samples given values sampled from latent space (a.k.a. reconstruction loss).&lt;/p&gt;
&lt;p&gt;Then,&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;logpz &amp;lt;- normal_loglik(
  z,
  k_constant(0, dtype = tf$float64),
  k_constant(0, dtype = tf$float64)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;gives &lt;span class="math inline"&gt;\(log \ p(z)\)&lt;/span&gt;, the prior loglikelihood of &lt;span class="math inline"&gt;\(z\)&lt;/span&gt;. The prior is assumed to be standard normal, as is most often the case with VAEs.&lt;/p&gt;
&lt;p&gt;Finally,&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;logqz_x &amp;lt;- normal_loglik(z, mean, logvar)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;vields &lt;span class="math inline"&gt;\(log \ q(z|x)\)&lt;/span&gt;, the loglikelihood of the samples &lt;span class="math inline"&gt;\(z\)&lt;/span&gt; given mean and variance computed from the observed samples &lt;span class="math inline"&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;From these three components, we will compute the final loss as&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;loss &amp;lt;- -k_mean(logpx_z + logpz - logqz_x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After this peaking ahead, let’s quickly finish the setup so we get ready for training.&lt;/p&gt;
&lt;h4 id="final-setup"&gt;Final setup&lt;/h4&gt;
&lt;p&gt;Besides the loss, we need an optimizer that will strive to diminish it.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;optimizer &amp;lt;- tf$train$AdamOptimizer(1e-4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We instantiate our models …&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;encoder &amp;lt;- encoder_model()
decoder &amp;lt;- decoder_model()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and set up checkpointing, so we can later restore trained weights.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;checkpoint_dir &amp;lt;- &amp;quot;./checkpoints_cvae&amp;quot;
checkpoint_prefix &amp;lt;- file.path(checkpoint_dir, &amp;quot;ckpt&amp;quot;)
checkpoint &amp;lt;- tf$train$Checkpoint(
  optimizer = optimizer,
  encoder = encoder,
  decoder = decoder
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the training loop, we will, in certain intervals, also call three functions not reproduced here (but available in the &lt;a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/eager_vae.R"&gt;code example&lt;/a&gt;): &lt;code&gt;generate_random_clothes&lt;/code&gt;, used to generate clothes from random samples from the latent space; &lt;code&gt;show_latent_space&lt;/code&gt;, that displays the complete test set in latent (2-dimensional, thus easily visualizable) space; and &lt;code&gt;show_grid&lt;/code&gt;, that generates clothes according to input values systematically spaced out in a grid.&lt;/p&gt;
&lt;p&gt;Let’s start training! Actually, before we do that, let’s have a look at what these functions display &lt;em&gt;before&lt;/em&gt; any training: Instead of clothes, we see random pixels. Latent space has no structure. And different types of clothes do not cluster together in latent space.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-10-22-mmd-vae/images/c_epoch_0.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;h4 id="training-loop"&gt;Training loop&lt;/h4&gt;
&lt;p&gt;We’re training for 50 epochs here. For each epoch, we loop over the training set in batches. For each batch, we follow the usual eager execution flow: Inside the context of a &lt;code&gt;GradientTape&lt;/code&gt;, apply the model and calculate the current loss; then outside this context calculate the gradients and let the optimizer perform backprop.&lt;/p&gt;
&lt;p&gt;What’s special here is that we have two models that both need their gradients calculated and weights adjusted. This can be taken care of by a single gradient tape, provided we create it &lt;code&gt;persistent&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;After each epoch, we save current weights and every ten epochs, we also save plots for later inspection.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;num_epochs &amp;lt;- 50

for (epoch in seq_len(num_epochs)) {
  iter &amp;lt;- make_iterator_one_shot(train_dataset)
  
  total_loss &amp;lt;- 0
  logpx_z_total &amp;lt;- 0
  logpz_total &amp;lt;- 0
  logqz_x_total &amp;lt;- 0
  
  until_out_of_range({
    x &amp;lt;-  iterator_get_next(iter)
    
    with(tf$GradientTape(persistent = TRUE) %as% tape, {
      
      c(mean, logvar) %&amp;lt;-% encoder(x)
      z &amp;lt;- reparameterize(mean, logvar)
      preds &amp;lt;- decoder(z)
      
      crossentropy_loss &amp;lt;-
        tf$nn$sigmoid_cross_entropy_with_logits(logits = preds, labels = x)
      logpx_z &amp;lt;-
        - k_sum(crossentropy_loss)
      logpz &amp;lt;-
        normal_loglik(z,
                      k_constant(0, dtype = tf$float64),
                      k_constant(0, dtype = tf$float64)
        )
      logqz_x &amp;lt;- normal_loglik(z, mean, logvar)
      loss &amp;lt;- -k_mean(logpx_z + logpz - logqz_x)
      
    })

    total_loss &amp;lt;- total_loss + loss
    logpx_z_total &amp;lt;- tf$reduce_mean(logpx_z) + logpx_z_total
    logpz_total &amp;lt;- tf$reduce_mean(logpz) + logpz_total
    logqz_x_total &amp;lt;- tf$reduce_mean(logqz_x) + logqz_x_total
    
    encoder_gradients &amp;lt;- tape$gradient(loss, encoder$variables)
    decoder_gradients &amp;lt;- tape$gradient(loss, decoder$variables)
    
    optimizer$apply_gradients(
      purrr::transpose(list(encoder_gradients, encoder$variables)),
      global_step = tf$train$get_or_create_global_step()
    )
    optimizer$apply_gradients(
      purrr::transpose(list(decoder_gradients, decoder$variables)),
      global_step = tf$train$get_or_create_global_step()
    )
    
  })
  
  checkpoint$save(file_prefix = checkpoint_prefix)
  
  cat(
    glue(
      &amp;quot;Losses (epoch): {epoch}:&amp;quot;,
      &amp;quot;  {(as.numeric(logpx_z_total)/batches_per_epoch) %&amp;gt;% round(2)} logpx_z_total,&amp;quot;,
      &amp;quot;  {(as.numeric(logpz_total)/batches_per_epoch) %&amp;gt;% round(2)} logpz_total,&amp;quot;,
      &amp;quot;  {(as.numeric(logqz_x_total)/batches_per_epoch) %&amp;gt;% round(2)} logqz_x_total,&amp;quot;,
      &amp;quot;  {(as.numeric(total_loss)/batches_per_epoch) %&amp;gt;% round(2)} total&amp;quot;
    ),
    &amp;quot;\n&amp;quot;
  )
  
  if (epoch %% 10 == 0) {
    generate_random_clothes(epoch)
    show_latent_space(epoch)
    show_grid(epoch)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="results"&gt;Results&lt;/h4&gt;
&lt;p&gt;How well did that work? Let’s see the kinds of clothes generated after 50 epochs.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-10-22-mmd-vae/images/cvae_clothes_epoch_50.png" style="width:66.0%" /&gt;&lt;/p&gt;
&lt;p&gt;Also, how disentangled (or not) are the different classes in latent space?&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-10-22-mmd-vae/images/cvae_latentspace_epoch_50.png" style="width:66.0%" /&gt;&lt;/p&gt;
&lt;p&gt;And now watch different clothes morph into one another.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-10-22-mmd-vae/images/cvae_grid_epoch_50.png" style="width:66.0%" /&gt;&lt;/p&gt;
&lt;p&gt;How good are these representations? This is hard to say when there is nothing to compare with.&lt;/p&gt;
&lt;p&gt;So let’s dive into MMD-VAE and see how it does on the same dataset.&lt;/p&gt;
&lt;h2 id="mmd-vae"&gt;MMD-VAE&lt;/h2&gt;
&lt;p&gt;MMD-VAE promises to generate more informative latent features, so we would hope to see different behavior especially in the clustering and morphing plots.&lt;/p&gt;
&lt;p&gt;Data setup is the same, and there are only very slight differences in the model. Please check out the complete code for this example, &lt;a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/mmd_vae.R"&gt;mmd_vae.R&lt;/a&gt;, as here we’ll just highlight the differences.&lt;/p&gt;
&lt;h4 id="differences-in-the-models"&gt;Differences in the model(s)&lt;/h4&gt;
&lt;p&gt;There are three differences as regards model architecture.&lt;/p&gt;
&lt;p&gt;One, the encoder does not have to return the variance, so there is no need for &lt;code&gt;tf$split&lt;/code&gt;. The encoder’s &lt;code&gt;call&lt;/code&gt; method now just is&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;function (x, mask = NULL) {
  x %&amp;gt;%
    self$conv1() %&amp;gt;%
    self$conv2() %&amp;gt;%
    self$flatten() %&amp;gt;%
    self$dense() 
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Between the encoder and the decoder, we don’t need the sampling step anymore, so there is no &lt;em&gt;reparameterization&lt;/em&gt;. And since we won’t use &lt;code&gt;tf$nn$sigmoid_cross_entropy_with_logits&lt;/code&gt; to compute the loss, we let the decoder apply the sigmoid in the last deconvolution layer:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;self$deconv3 &amp;lt;- layer_conv_2d_transpose(
  filters = 1,
  kernel_size = 3,
  strides = 1,
  padding = &amp;quot;same&amp;quot;,
  activation = &amp;quot;sigmoid&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="loss-calculations-1"&gt;Loss calculations&lt;/h4&gt;
&lt;p&gt;Now, as expected, the big novelty is in the loss function.&lt;/p&gt;
&lt;p&gt;The loss, &lt;em&gt;maximum mean discrepancy&lt;/em&gt; (MMD), is based on the idea that two distributions are identical if and only if all moments are identical. Concretely, MMD is estimated using a &lt;em&gt;kernel&lt;/em&gt;, such as the Gaussian kernel&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[k(z,z&amp;#39;)=\frac{e^{||z-z&amp;#39;||}}{2\sigma^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;to assess similarity between distributions.&lt;/p&gt;
&lt;p&gt;The idea then is that if two distributions are identical, the average similarity between samples from each distribution should be identical to the average similarity between mixed samples from both distributions:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[MMD(p(z)||q(z))=E_{p(z),p(z&amp;#39;)}[k(z,z&amp;#39;)]+E_{q(z),q(z&amp;#39;)}[k(z,z&amp;#39;)]−2E_{p(z),q(z&amp;#39;)}[k(z,z&amp;#39;)]\]&lt;/span&gt; The following code is a direct port of the author’s &lt;a href="https://github.com/ShengjiaZhao/MMD-Variational-Autoencoder"&gt;original TensorFlow code&lt;/a&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;compute_kernel &amp;lt;- function(x, y) {
  x_size &amp;lt;- k_shape(x)[1]
  y_size &amp;lt;- k_shape(y)[1]
  dim &amp;lt;- k_shape(x)[2]
  tiled_x &amp;lt;- k_tile(
    k_reshape(x, k_stack(list(x_size, 1, dim))),
    k_stack(list(1, y_size, 1))
  )
  tiled_y &amp;lt;- k_tile(
    k_reshape(y, k_stack(list(1, y_size, dim))),
    k_stack(list(x_size, 1, 1))
  )
  k_exp(-k_mean(k_square(tiled_x - tiled_y), axis = 3) /
          k_cast(dim, tf$float64))
}

compute_mmd &amp;lt;- function(x, y, sigma_sqr = 1) {
  x_kernel &amp;lt;- compute_kernel(x, x)
  y_kernel &amp;lt;- compute_kernel(y, y)
  xy_kernel &amp;lt;- compute_kernel(x, y)
  k_mean(x_kernel) + k_mean(y_kernel) - 2 * k_mean(xy_kernel)
}&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="training-loop-1"&gt;Training loop&lt;/h4&gt;
&lt;p&gt;The training loop differs from the standard VAE example only in the loss calculations. Here are the respective lines:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt; with(tf$GradientTape(persistent = TRUE) %as% tape, {
      
      mean &amp;lt;- encoder(x)
      preds &amp;lt;- decoder(mean)
      
      true_samples &amp;lt;- k_random_normal(
        shape = c(batch_size, latent_dim),
        dtype = tf$float64
      )
      loss_mmd &amp;lt;- compute_mmd(true_samples, mean)
      loss_nll &amp;lt;- k_mean(k_square(x - preds))
      loss &amp;lt;- loss_nll + loss_mmd
      
    })&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we simply compute MMD loss as well as reconstruction loss, and add them up. No sampling is involved in this version. Of course, we are curious to see how well that worked!&lt;/p&gt;
&lt;h4 id="results-1"&gt;Results&lt;/h4&gt;
&lt;p&gt;Again, let’s look at some generated clothes first. It seems like edges are much sharper here.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-10-22-mmd-vae/images/mmd_clothes_epoch_50.png" style="width:66.0%" /&gt;&lt;/p&gt;
&lt;p&gt;The clusters too look more nicely spread out in the two dimensions. And, they are centered at (0,0), as we would have hoped for.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-10-22-mmd-vae/images/mmd_latentspace_epoch_50.png" style="width:66.0%" /&gt;&lt;/p&gt;
&lt;p&gt;Finally, let’s see clothes morph into one another. Here, the smooth, continuous evolutions are impressive! Also, nearly all space is filled with meaningful objects, which hasn’t been the case above.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-10-22-mmd-vae/images/mmd_grid_epoch_50.png" style="width:66.0%" /&gt;&lt;/p&gt;
&lt;h2 id="mnist"&gt;MNIST&lt;/h2&gt;
&lt;p&gt;For curiosity’s sake, we generated the same kinds of plots after training on original MNIST. Here, there are hardly any differences visible in generated random digits after 50 epochs of training.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-10-22-mmd-vae/images/comp_digits.png" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Left: random digits as generated after training with ELBO loss. Right: MMD loss.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Also the differences in clustering are not &lt;em&gt;that&lt;/em&gt; big.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-10-22-mmd-vae/images/comp_lat.png" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Left: latent space as observed after training with ELBO loss. Right: MMD loss.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;But here too, the morphing looks much more organic with MMD-VAE.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-10-22-mmd-vae/images/comp_grid.png" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Left: Morphing as observed after training with ELBO loss. Right: MMD loss.&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;To us, this demonstrates impressively what big a difference the cost function can make when working with VAEs. Another component open to experimentation may be the prior used for the latent space - see &lt;a href="https://www.ics.uci.edu/~enalisni/nalisnick_openAI_talk.pdf"&gt;this talk&lt;/a&gt; for an overview of alternative priors and the “Variational Mixture of Posteriors” paper &lt;span class="citation"&gt;(Tomczak and Welling 2017)&lt;/span&gt; for a popular recent approach.&lt;/p&gt;
&lt;p&gt;For both cost functions and priors, we expect effective differences to become way bigger still when we leave the controlled environment of (Fashion) MNIST and work with real-world datasets.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-2018arXiv180403599B"&gt;
&lt;p&gt;Burgess, C. P., I. Higgins, A. Pal, L. Matthey, N. Watters, G. Desjardins, and A. Lerchner. 2018. “Understanding Disentangling in Beta-Vae.” &lt;em&gt;ArXiv E-Prints&lt;/em&gt;, April. &lt;a href="http://arxiv.org/abs/1804.03599"&gt;http://arxiv.org/abs/1804.03599&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-2016arXiv160605908D"&gt;
&lt;p&gt;Doersch, C. 2016. “Tutorial on Variational Autoencoders.” &lt;em&gt;ArXiv E-Prints&lt;/em&gt;, June. &lt;a href="http://arxiv.org/abs/1606.05908"&gt;http://arxiv.org/abs/1606.05908&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-KingmaW13"&gt;
&lt;p&gt;Kingma, Diederik P., and Max Welling. 2013. “Auto-Encoding Variational Bayes.” &lt;em&gt;CoRR&lt;/em&gt; abs/1312.6114.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-TomczakW17"&gt;
&lt;p&gt;Tomczak, Jakub M., and Max Welling. 2017. “VAE with a Vampprior.” &lt;em&gt;CoRR&lt;/em&gt; abs/1705.07120.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-ZhaoSE17b"&gt;
&lt;p&gt;Zhao, Shengjia, Jiaming Song, and Stefano Ermon. 2017. “InfoVAE: Information Maximizing Variational Autoencoders.” &lt;em&gt;CoRR&lt;/em&gt; abs/1706.02262. &lt;a href="http://arxiv.org/abs/1706.02262"&gt;http://arxiv.org/abs/1706.02262&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Note: this link was updated as of November 29th, 2019, to point to the most up-to-date version.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">e36f8809f83c30da146298766fe139de</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Unsupervised Learning</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae</guid>
      <pubDate>Mon, 22 Oct 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae/images/thumb.png" medium="image" type="image/png" width="468" height="178"/>
    </item>
    <item>
      <title>Winner takes all: A look at activations and cost functions</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-10-11-activations-intro</link>
      <description>


&lt;p&gt;You’re building a Keras model. If you haven’t been doing deep learning for so long, getting the output activations and cost function right might involve some memorization (or lookup). You might be trying to recall the general guidelines like so:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;So with my cats and dogs, I’m doing 2-class classification, so I have to use sigmoid activation in the output layer, right, and then, it’s binary crossentropy for the cost function…&lt;/em&gt; Or: &lt;em&gt;I’m doing classification on ImageNet, that’s multi-class, so that was softmax for activation, and then, cost should be categorical crossentropy…&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It’s fine to memorize stuff like this, but knowing a bit about the reasons behind often makes things easier. So we ask: Why is it that these output activations and cost functions go together? And, do they always have to?&lt;/p&gt;
&lt;h2 id="in-a-nutshell"&gt;In a nutshell&lt;/h2&gt;
&lt;p&gt;Put simply, we choose activations that make the network predict what we want it to predict. The cost function is then determined by the model.&lt;/p&gt;
&lt;p&gt;This is because neural networks are normally optimized using &lt;em&gt;maximum likelihood&lt;/em&gt;, and depending on the distribution we assume for the output units, maximum likelihood yields different optimization objectives. All of these objectives then minimize the cross entropy (pragmatically: mismatch) between the true distribution and the predicted distribution.&lt;/p&gt;
&lt;aside&gt;
For a more mathematical development of these topics, see sections 5.5 and 6.2 of Goodfellow et al., Deep Learning.&lt;span class="citation"&gt;(Goodfellow, Bengio, and Courville 2016)&lt;/span&gt;
&lt;/aside&gt;
&lt;p&gt;Let’s start with the simplest, the linear case.&lt;/p&gt;
&lt;h2 id="regression"&gt;Regression&lt;/h2&gt;
&lt;p&gt;For the botanists among us, here’s a super simple network meant to predict sepal width from sepal length:&lt;/p&gt;
&lt;aside&gt;
In case you’d like a more comprehensive introduction to doing regression with Keras, see the &lt;a href="https://tensorflow.rstudio.com/keras/articles/tutorial_basic_regression.html"&gt;tutorial&lt;/a&gt; on the Keras website.
&lt;/aside&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_dense(units = 32) %&amp;gt;%
  layer_dense(units = 1)

model %&amp;gt;% compile(
  optimizer = &amp;quot;adam&amp;quot;, 
  loss = &amp;quot;mean_squared_error&amp;quot;
)

model %&amp;gt;% fit(
  x = iris$Sepal.Length %&amp;gt;% as.matrix(),
  y = iris$Sepal.Width %&amp;gt;% as.matrix(),
  epochs = 50
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our model’s assumption here is that sepal width is normally distributed, given sepal length. Most often, we’re trying to predict the mean of a conditional Gaussian distribution:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[p(y|\mathbf{x} = N(y; \mathbf{w}^t\mathbf{h} + b)\]&lt;/span&gt;&lt;/p&gt;
&lt;aside&gt;
This formula assumes a single output unit.
&lt;/aside&gt;
&lt;p&gt;In that case, the cost function that minimizes cross entropy (equivalently: optimizes maximum likelihood) is &lt;em&gt;mean squared error&lt;/em&gt;. And that’s exactly what we’re using as a cost function above.&lt;/p&gt;
&lt;p&gt;Alternatively, we might wish to predict the median of that conditional distribution. In that case, we’d change the cost function to use mean absolute error:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% compile(
  optimizer = &amp;quot;adam&amp;quot;, 
  loss = &amp;quot;mean_absolute_error&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s move on beyond linearity.&lt;/p&gt;
&lt;h2 id="binary-classification"&gt;Binary classification&lt;/h2&gt;
&lt;p&gt;We’re enthusiastic bird watchers and want an application to notify us when there’s a bird in our garden - not when the neighbors landed their airplane, though. We’ll thus train a network to distinguish between two classes: birds and airplanes.&lt;/p&gt;
&lt;aside&gt;
For a more detailed introduction to classification with Keras, see the &lt;a href="https://tensorflow.rstudio.com/keras/articles/tutorial_basic_classification.html"&gt;tutorial&lt;/a&gt; on the Keras website.
&lt;/aside&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Using the CIFAR-10 dataset that conveniently comes with Keras.
cifar10 &amp;lt;- dataset_cifar10()

x_train &amp;lt;- cifar10$train$x / 255
y_train &amp;lt;- cifar10$train$y

is_bird &amp;lt;- cifar10$train$y == 2
x_bird &amp;lt;- x_train[is_bird, , ,]
y_bird &amp;lt;- rep(0, 5000)

is_plane &amp;lt;- cifar10$train$y == 0
x_plane &amp;lt;- x_train[is_plane, , ,]
y_plane &amp;lt;- rep(1, 5000)

x &amp;lt;- abind::abind(x_bird, x_plane, along = 1)
y &amp;lt;- c(y_bird, y_plane)

model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_conv_2d(
    filter = 8,
    kernel_size = c(3, 3),
    padding = &amp;quot;same&amp;quot;,
    input_shape = c(32, 32, 3),
    activation = &amp;quot;relu&amp;quot;
  ) %&amp;gt;%
  layer_max_pooling_2d(pool_size = c(2, 2)) %&amp;gt;%
  layer_conv_2d(
    filter = 8,
    kernel_size = c(3, 3),
    padding = &amp;quot;same&amp;quot;,
    activation = &amp;quot;relu&amp;quot;
  ) %&amp;gt;%
  layer_max_pooling_2d(pool_size = c(2, 2)) %&amp;gt;%
layer_flatten() %&amp;gt;%
  layer_dense(units = 32, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dense(units = 1, activation = &amp;quot;sigmoid&amp;quot;)

model %&amp;gt;% compile(
  optimizer = &amp;quot;adam&amp;quot;, 
  loss = &amp;quot;binary_crossentropy&amp;quot;, 
  metrics = &amp;quot;accuracy&amp;quot;
)

model %&amp;gt;% fit(
  x = x,
  y = y,
  epochs = 50
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although we normally talk about “binary classification”, the way the outcome is usually modeled is as a &lt;em&gt;Bernoulli random variable&lt;/em&gt;, conditioned on the input data. So:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[P(y = 1|\mathbf{x}) = p, \ 0\leq p\leq1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A Bernoulli random variable takes on values between &lt;span class="math inline"&gt;\(0\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(1\)&lt;/span&gt;. So that’s what our network should produce. One idea might be to just clip all values of &lt;span class="math inline"&gt;\(\mathbf{w}^t\mathbf{h} + b\)&lt;/span&gt; outside that interval. But if we do this, the gradient in these regions will be &lt;span class="math inline"&gt;\(0\)&lt;/span&gt;: The network cannot learn.&lt;/p&gt;
&lt;p&gt;A better way is to squish the complete incoming interval into the range (0,1), using the logistic &lt;em&gt;sigmoid&lt;/em&gt; function&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[ \sigma(x) = \frac{1}{1 + e^{(-x)}} \]&lt;/span&gt;&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-10-11-activations-intro/images/sigmoid.png" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;The sigmoid function squishes its input into the interval (0,1).&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As you can see, the sigmoid function saturates when its input gets very large, or very small. Is this problematic? It depends. In the end, what we care about is if the cost function saturates. Were we to choose mean squared error here, as in the regression task above, that’s indeed what could happen.&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;However, if we follow the general principle of maximum likelihood/cross entropy, the loss will be&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[- log P (y|\mathbf{x})\]&lt;/span&gt;&lt;/p&gt;
&lt;aside&gt;
If you need the details, see section 6.2.2.2 in Goodfellow et al.
&lt;/aside&gt;
&lt;p&gt;where the &lt;span class="math inline"&gt;\(log\)&lt;/span&gt; undoes the &lt;span class="math inline"&gt;\(exp\)&lt;/span&gt; in the sigmoid.&lt;/p&gt;
&lt;p&gt;In Keras, the corresponding loss function is &lt;code&gt;binary_crossentropy&lt;/code&gt;. For a single item, the loss will be&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math inline"&gt;\(- log(p)\)&lt;/span&gt; when the ground truth is 1&lt;/li&gt;
&lt;li&gt;&lt;span class="math inline"&gt;\(- log(1-p)\)&lt;/span&gt; when the ground truth is 0&lt;/li&gt;
&lt;/ul&gt;
&lt;aside&gt;
Here p is the predicted probability, i.e., the output activations of the network.
&lt;/aside&gt;
&lt;p&gt;Here, you can see that when for an individual example, the network predicts the wrong class &lt;em&gt;and&lt;/em&gt; is highly confident about it, this example will contributely very strongly to the loss.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-10-11-activations-intro/images/xent.png" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Cross entropy penalizes wrong predictions most when they are highly confident.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;What happens when we distinguish between more than two classes?&lt;/p&gt;
&lt;h2 id="multi-class-classification"&gt;Multi-class classification&lt;/h2&gt;
&lt;p&gt;CIFAR-10 has 10 classes; so now we want to decide which of 10 object classes is present in the image.&lt;/p&gt;
&lt;p&gt;Here first is the code: Not many differences to the above, but note the changes in activation and cost function.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;cifar10 &amp;lt;- dataset_cifar10()

x_train &amp;lt;- cifar10$train$x / 255
y_train &amp;lt;- cifar10$train$y

model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_conv_2d(
    filter = 8,
    kernel_size = c(3, 3),
    padding = &amp;quot;same&amp;quot;,
    input_shape = c(32, 32, 3),
    activation = &amp;quot;relu&amp;quot;
  ) %&amp;gt;%
  layer_max_pooling_2d(pool_size = c(2, 2)) %&amp;gt;%
  layer_conv_2d(
    filter = 8,
    kernel_size = c(3, 3),
    padding = &amp;quot;same&amp;quot;,
    activation = &amp;quot;relu&amp;quot;
  ) %&amp;gt;%
  layer_max_pooling_2d(pool_size = c(2, 2)) %&amp;gt;%
  layer_flatten() %&amp;gt;%
  layer_dense(units = 32, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dense(units = 10, activation = &amp;quot;softmax&amp;quot;)

model %&amp;gt;% compile(
  optimizer = &amp;quot;adam&amp;quot;,
  loss = &amp;quot;sparse_categorical_crossentropy&amp;quot;,
  metrics = &amp;quot;accuracy&amp;quot;
)

model %&amp;gt;% fit(
  x = x_train,
  y = y_train,
  epochs = 50
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So now we have &lt;em&gt;softmax&lt;/em&gt; combined with &lt;em&gt;categorical crossentropy&lt;/em&gt;. Why?&lt;/p&gt;
&lt;p&gt;Again, we want a valid probability distribution: Probabilities for all disjunct events should sum to 1.&lt;/p&gt;
&lt;p&gt;CIFAR-10 has one object per image; so events are disjunct. Then we have a single-draw multinomial distribution (popularly known as “Multinoulli”, mostly due to Murphy’s &lt;em&gt;Machine learning&lt;/em&gt;&lt;span class="citation"&gt;(Murphy 2012)&lt;/span&gt;) that can be modeled by the softmax activation:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[softmax(\mathbf{z})_i = \frac{e^{z_i}}{\sum_j{e^{z_j}}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Just as the sigmoid, the softmax can saturate. In this case, that will happen when &lt;em&gt;differences&lt;/em&gt; between outputs become very big. Also like with the sigmoid, a &lt;span class="math inline"&gt;\(log\)&lt;/span&gt; in the cost function undoes the &lt;span class="math inline"&gt;\(exp\)&lt;/span&gt; that’s responsible for saturation:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[log\ softmax(\mathbf{z})_i = z_i - log\sum_j{e^{z_j}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here &lt;span class="math inline"&gt;\(z_i\)&lt;/span&gt; is the class we’re estimating the probability of - we see that its contribution to the loss is linear and thus, can never saturate.&lt;/p&gt;
&lt;p&gt;In Keras, the loss function that does this for us is called &lt;code&gt;categorical_crossentropy&lt;/code&gt;. We use sparse_categorical_crossentropy in the code which is the same as &lt;code&gt;categorical_crossentropy&lt;/code&gt; but does not need conversion of integer labels to one-hot vectors.&lt;/p&gt;
&lt;p&gt;Let’s take a closer look at what softmax does. Assume these are the raw outputs of our 10 output units:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-10-11-activations-intro/images/softmax_pre.png" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Simulated output before application of softmax.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now this is what the normalized probability distribution looks like after taking the softmax:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-10-11-activations-intro/images/softmax_post.png" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Final output after softmax.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Do you see where the &lt;em&gt;winner takes all&lt;/em&gt; in the title comes from? This is an important point to keep in mind: Activation functions are not just there to produce certain desired distributions; they can also change relationships between values.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We started this post alluding to common heuristics, such as “for multi-class classification, we use softmax activation, combined with categorical crossentropy as the loss function”. Hopefully, we’ve succeeded in showing why these heuristics make sense.&lt;/p&gt;
&lt;p&gt;However, knowing that background, you can also infer when these rules do not apply. For example, say you want to detect several objects in an image. In that case, the &lt;em&gt;winner-takes-all&lt;/em&gt; strategy is not the most useful, as we don’t want to exaggerate differences between candidates. So here, we’d use &lt;em&gt;sigmoid&lt;/em&gt; on all output units instead, to determine a probability of presence &lt;em&gt;per object&lt;/em&gt;.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-Goodfellow-et-al-2016"&gt;
&lt;p&gt;Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. &lt;em&gt;Deep Learning&lt;/em&gt;. MIT Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Murphy-2012"&gt;
&lt;p&gt;Murphy, Kevin. 2012. &lt;em&gt;Machine Learning: A Probabilistic Perspective&lt;/em&gt;. MIT Press.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;The actual outcome depends on the task. In the above simple classification example, training with mean squared error will attain similar accuracy in similar time.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">3f06178a6db628e7ad30292e95080453</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Concepts</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-10-11-activations-intro</guid>
      <pubDate>Thu, 11 Oct 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-10-11-activations-intro/images/output.png" medium="image" type="image/png" width="800" height="384"/>
    </item>
    <item>
      <title>More flexible models with TensorFlow eager execution and Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-10-02-eager-wrapup</link>
      <description>


&lt;p&gt;If you have used Keras to create neural networks you are no doubt familiar with the &lt;a href="https://tensorflow.rstudio.com/keras/articles/sequential_model.html"&gt;Sequential API&lt;/a&gt;, which represents models as a linear stack of layers. The &lt;a href="https://tensorflow.rstudio.com/keras/articles/functional_api.html"&gt;Functional API&lt;/a&gt; gives you additional options: Using separate input layers, you can combine text input with tabular data. Using multiple outputs, you can perform regression and classification at the same time. Furthermore, you can reuse layers within and between models.&lt;/p&gt;
&lt;p&gt;With TensorFlow eager execution, you gain even more flexibility. Using &lt;a href="https://tensorflow.rstudio.com/keras/articles/custom_models.html"&gt;custom models&lt;/a&gt;, you define the forward pass through the model completely &lt;em&gt;ad libitum&lt;/em&gt;. This means that a lot of architectures get a lot easier to implement, including the applications mentioned above: generative adversarial networks, neural style transfer, various forms of sequence-to-sequence models. In addition, because you have direct access to values, not tensors, model development and debugging are greatly sped up.&lt;/p&gt;
&lt;h2 id="how-does-it-work"&gt;How does it work?&lt;/h2&gt;
&lt;p&gt;In eager execution, operations are not compiled into a graph, but directly defined in your R code. They return values, not symbolic handles to nodes in a computational graph - meaning, you don’t need access to a TensorFlow &lt;code&gt;session&lt;/code&gt; to evaluate them.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;m1 &amp;lt;- matrix(1:8, nrow = 2, ncol = 4)
m2 &amp;lt;- matrix(1:8, nrow = 4, ncol = 2)
tf$matmul(m1, m2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;tf.Tensor(
[[ 50 114]
 [ 60 140]], shape=(2, 2), dtype=int32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Eager execution, recent though it is, is already supported in the current CRAN releases of &lt;code&gt;keras&lt;/code&gt; and &lt;code&gt;tensorflow&lt;/code&gt;. The &lt;a href="https://tensorflow.rstudio.com/keras/articles/eager_guide.html"&gt;eager execution guide&lt;/a&gt; describes the workflow in detail.&lt;/p&gt;
&lt;p&gt;Here’s a quick outline: You define a &lt;a href="https://tensorflow.rstudio.com/keras/articles/custom_models.html"&gt;model&lt;/a&gt;, an optimizer, and a loss function. Data is streamed via &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/introduction.html"&gt;tfdatasets&lt;/a&gt;, including any preprocessing such as image resizing. Then, model training is just a loop over epochs, giving you complete freedom over when (and whether) to execute any actions.&lt;/p&gt;
&lt;p&gt;How does backpropagation work in this setup? The forward pass is recorded by a &lt;code&gt;GradientTape&lt;/code&gt;, and during the backward pass we explicitly calculate gradients of the loss with respect to the model’s weights. These weights are then adjusted by the optimizer.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;with(tf$GradientTape() %as% tape, {
     
  # run model on current batch
  preds &amp;lt;- model(x)
 
  # compute the loss
  loss &amp;lt;- mse_loss(y, preds, x)
  
})
    
# get gradients of loss w.r.t. model weights
gradients &amp;lt;- tape$gradient(loss, model$variables)

# update model weights
optimizer$apply_gradients(
  purrr::transpose(list(gradients, model$variables)),
  global_step = tf$train$get_or_create_global_step()
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See the &lt;a href="https://tensorflow.rstudio.com/keras/articles/eager_guide.html"&gt;eager execution guide&lt;/a&gt; for a complete example. Here, we want to answer the question: Why are we so excited about it? At least three things come to mind:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Things that used to be complicated become much easier to accomplish.&lt;/li&gt;
&lt;li&gt;Models are easier to develop, and easier to debug.&lt;/li&gt;
&lt;li&gt;There is a much better match between our mental models and the code we write.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We’ll illustrate these points using a set of eager execution case studies that have recently appeared on this blog.&lt;/p&gt;
&lt;h2 id="complicated-stuff-made-easier"&gt;Complicated stuff made easier&lt;/h2&gt;
&lt;p&gt;A good example of architectures that become much easier to define with eager execution are attention models. Attention is an important ingredient of sequence-to-sequence models, e.g. (but not only) in machine translation.&lt;/p&gt;
&lt;p&gt;When using LSTMs on both the encoding and the decoding sides, the decoder, being a recurrent layer, knows about the sequence it has generated so far. It also (in all but the simplest models) has access to the complete input sequence. But where in the input sequence is the piece of information it needs to generate the next output token? It is this question that attention is meant to address.&lt;/p&gt;
&lt;p&gt;Now consider implementing this in code. Each time it is called to produce a new token, the decoder needs to get current input from the attention mechanism. This means we can’t just squeeze an attention layer between the encoder and the decoder LSTM. Before the advent of eager execution, a solution would have been to implement this in low-level TensorFlow code. With eager execution and custom models, we can just &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-07-30-attention-layer/"&gt;use Keras&lt;/a&gt;.&lt;/p&gt;
&lt;aside&gt;
&lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-07-30-attention-layer/"&gt;&lt;img src="https://blogs.rstudio.com/tensorflow/posts/2018-07-30-attention-layer/images/attention.png" style="border: 1px solid rgba(0, 0, 0, 0.2);" width="150" /&gt;&lt;/a&gt;
&lt;/aside&gt;
&lt;p&gt;Attention is not just relevant to sequence-to-sequence problems, though. In &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning/"&gt;image captioning&lt;/a&gt;, the output is a sequence, while the input is a complete image. When generating a caption, attention is used to focus on parts of the image relevant to different time steps in the text-generating process.&lt;/p&gt;
&lt;aside&gt;
&lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning/"&gt;&lt;img src="https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning/images/showattendandtell.png" style="border: 1px solid rgba(0, 0, 0, 0.2);" width="150" /&gt;&lt;/a&gt;
&lt;/aside&gt;
&lt;h2 id="easy-inspection"&gt;Easy inspection&lt;/h2&gt;
&lt;p&gt;In terms of debuggability, just using custom models (without eager execution) already simplifies things. If we have a custom model like &lt;code&gt;simple_dot&lt;/code&gt; from the recent &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-09-26-embeddings-recommender"&gt;embeddings post&lt;/a&gt; and are unsure if we’ve got the shapes correct, we can simply add logging statements, like so:&lt;/p&gt;
&lt;aside&gt;
&lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-09-26-embeddings-recommender"&gt;&lt;img src="https://blogs.rstudio.com/tensorflow/posts/2018-09-26-embeddings-recommender/images/m.png" style="border: 1px solid rgba(0, 0, 0, 0.2);" width="150" /&gt;&lt;/a&gt;
&lt;/aside&gt;
&lt;pre class="r"&gt;&lt;code&gt;function(x, mask = NULL) {
  
  users &amp;lt;- x[, 1]
  movies &amp;lt;- x[, 2]
  
  user_embedding &amp;lt;- self$user_embedding(users)
  cat(dim(user_embedding), &amp;quot;\n&amp;quot;)
  
  movie_embedding &amp;lt;- self$movie_embedding(movies)
  cat(dim(movie_embedding), &amp;quot;\n&amp;quot;)
  
  dot &amp;lt;- self$dot(list(user_embedding, movie_embedding))
  cat(dim(dot), &amp;quot;\n&amp;quot;)
  dot
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With eager execution, things get even better: We can print the tensors’ values themselves.&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;But convenience does not end there. In the training loop we showed above, we can obtain losses, model weights, and gradients just by printing them. For example, add a line after the call to &lt;code&gt;tape$gradient&lt;/code&gt; to print the gradients for all layers as a list.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;gradients &amp;lt;- tape$gradient(loss, model$variables)
print(gradients)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="matching-the-mental-model"&gt;Matching the mental model&lt;/h2&gt;
&lt;p&gt;If you’ve read &lt;a href="https://www.amazon.com/Deep-Learning-R-Francois-Chollet/dp/161729554X"&gt;Deep Learning with R&lt;/a&gt;, you know that it’s possible to program less straightforward workflows, such as those required for training GANs or doing neural style transfer, using the Keras functional API. However, the graph code does not make it easy to keep track of where you are in the workflow.&lt;/p&gt;
&lt;p&gt;Now compare the example from the &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-08-26-eager-dcgan"&gt;generating digits with GANs&lt;/a&gt; post. Generator and discriminator each get set up as actors in a drama:&lt;/p&gt;
&lt;aside&gt;
&lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-08-26-eager-dcgan"&gt;&lt;img src="https://blogs.rstudio.com/tensorflow/posts/2018-08-26-eager-dcgan/images/thumb.png" style="border: 1px solid rgba(0, 0, 0, 0.2);" width="150" /&gt;&lt;/a&gt;
&lt;/aside&gt;
&lt;pre class="r"&gt;&lt;code&gt;generator &amp;lt;- function(name = NULL) {
  keras_model_custom(name = name, function(self) {
    # ...
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;discriminator &amp;lt;- function(name = NULL) {
  keras_model_custom(name = name, function(self) {
    # ...
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both are informed about their respective loss functions and optimizers.&lt;/p&gt;
&lt;p&gt;Then, the duel starts. The training loop is just a succession of generator actions, discriminator actions, and backpropagation through both models. No need to worry about freezing/unfreezing weights in the appropriate places.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;with(tf$GradientTape() %as% gen_tape, { with(tf$GradientTape() %as% disc_tape, {
  
 # generator action
 generated_images &amp;lt;- generator(# ...
   
 # discriminator assessments
 disc_real_output &amp;lt;- discriminator(# ... 
 disc_generated_output &amp;lt;- discriminator(# ...
      
 # generator loss
 gen_loss &amp;lt;- generator_loss(# ...                        
 # discriminator loss
 disc_loss &amp;lt;- discriminator_loss(# ...
   
})})
   
# calcucate generator gradients   
gradients_of_generator &amp;lt;- gen_tape$gradient(#...
  
# calcucate discriminator gradients   
gradients_of_discriminator &amp;lt;- disc_tape$gradient(# ...
 
# apply generator gradients to model weights       
generator_optimizer$apply_gradients(# ...

# apply discriminator gradients to model weights 
discriminator_optimizer$apply_gradients(# ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code ends up so close to how we mentally picture the situation that hardly any memorization is needed to keep in mind the overall design.&lt;/p&gt;
&lt;p&gt;Relatedly, this way of programming lends itself to extensive modularization. This is illustrated by the &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-09-20-eager-pix2pix/images/pix2pixlosses.png"&gt;second post on GANs&lt;/a&gt; that includes U-Net like downsampling and upsampling steps.&lt;/p&gt;
&lt;aside&gt;
&lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-09-20-eager-pix2pix"&gt;&lt;img src="https://blogs.rstudio.com/tensorflow/posts/2018-09-20-eager-pix2pix/images/pix2pixlosses.png" style="border: 1px solid rgba(0, 0, 0, 0.2);" width="150" /&gt;&lt;/a&gt;
&lt;/aside&gt;
&lt;p&gt;Here, the downsampling and upsampling layers are each factored out into their own models&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;downsample &amp;lt;- function(# ...
  keras_model_custom(name = NULL, function(self) { # ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;such that they can be readably composed in the generator’s call method:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# model fields
self$down1 &amp;lt;- downsample(# ...
self$down2 &amp;lt;- downsample(# ...
# ...
# ...

# call method
function(x, mask = NULL, training = TRUE) {       
     
  x1 &amp;lt;- x %&amp;gt;% self$down1(training = training)         
  x2 &amp;lt;- self$down2(x1, training = training)           
  # ...
  # ...&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="wrapping-up"&gt;Wrapping up&lt;/h2&gt;
&lt;p&gt;Eager execution is still a very recent feature and under development. We are convinced that many interesting use cases will still turn up as this paradigm gets adopted more widely among deep learning practitioners.&lt;/p&gt;
&lt;p&gt;However, now already we have a list of use cases illustrating the vast options, gains in usability, modularization and elegance offered by eager execution code.&lt;/p&gt;
&lt;p&gt;For quick reference, these cover:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-07-30-attention-layer/"&gt;Neural machine translation with attention&lt;/a&gt;. This post provides a detailed introduction to eager execution and its building blocks, as well as an in-depth explanation of the attention mechanism used. Together with the next one, it occupies a very special role in this list: It uses eager execution to solve a problem that otherwise could only be solved with hard-to-read, hard-to-write low-level code.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning/"&gt;Image captioning with attention&lt;/a&gt;. This post builds on the first in that it does not re-explain attention in detail; however, it ports the concept to spatial attention applied over image regions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-08-26-eager-dcgan"&gt;Generating digits with convolutional generative adversarial networks (DCGANs)&lt;/a&gt;. This post introduces using two custom models, each with their associated loss functions and optimizers, and having them go through forward- and backpropagation in sync. It is perhaps the most impressive example of how eager execution simplifies coding by better alignment to our mental model of the situation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-09-20-eager-pix2pix"&gt;Image-to-image translation with pix2pix&lt;/a&gt; is another application of generative adversarial networks, but uses a more complex architecture based on U-Net-like downsampling and upsampling. It nicely demonstrates how eager execution allows for modular coding, rendering the final program much more readable.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-09-10-eager-style-transfer/"&gt;Neural style transfer&lt;/a&gt;. Finally, this post reformulates the style transfer problem in an eager way, again resulting in readable, concise code.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When diving into these applications, it is a good idea to also refer to the &lt;a href="https://tensorflow.rstudio.com/keras/articles/eager_guide.html"&gt;eager execution guide&lt;/a&gt; so you don’t lose sight of the forest for the trees.&lt;/p&gt;
&lt;p&gt;We are excited about the use cases our readers will come up with!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Note that the embeddings example uses standard (graph) execution; refactoring would be needed in order to enable eager execution on it.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">29bee67968cdea8c6aec3285a4637f63</distill:md5>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-10-02-eager-wrapup</guid>
      <pubDate>Tue, 02 Oct 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-10-02-eager-wrapup/images/m.png" medium="image" type="image/png" width="384" height="126"/>
    </item>
    <item>
      <title>Collaborative filtering with embeddings</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-09-26-embeddings-recommender</link>
      <description>


&lt;p&gt;What’s your first association when you read the word &lt;em&gt;embeddings&lt;/em&gt;? For most of us, the answer will probably be &lt;em&gt;word embeddings&lt;/em&gt;, or &lt;em&gt;word vectors&lt;/em&gt;. A quick search for recent papers on &lt;a href="www.arxiv.org"&gt;arxiv&lt;/a&gt; shows what else can be embedded: equations&lt;span class="citation"&gt;(Krstovski and Blei 2018)&lt;/span&gt;, vehicle sensor data&lt;span class="citation"&gt;(Hallac et al. 2018)&lt;/span&gt;, graphs&lt;span class="citation"&gt;(Ahmed et al. 2018)&lt;/span&gt;, code&lt;span class="citation"&gt;(Alon et al. 2018)&lt;/span&gt;, spatial data&lt;span class="citation"&gt;(Jean et al. 2018)&lt;/span&gt;, biological entities&lt;span class="citation"&gt;(Zohra Smaili, Gao, and Hoehndorf 2018)&lt;/span&gt; … - and what not.&lt;/p&gt;
&lt;p&gt;What is so attractive about this concept? Embeddings incorporate the concept of &lt;em&gt;distributed representations&lt;/em&gt;, an encoding of information not at specialized locations (dedicated neurons, say), but as a pattern of activations spread out over a network. No better source to cite than Geoffrey Hinton, who played an important role in the development of the concept&lt;span class="citation"&gt;(Rumelhart, McClelland, and PDP Research Group 1986)&lt;/span&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Distributed representation&lt;/em&gt; means a many to many relationship between two types of representation (such as concepts and neurons). Each concept is represented by many neurons. Each neuron participates in the representation of many concepts.&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The advantages are manifold. Perhaps the most famous effect of using embeddings is that we can learn and make use of semantic similarity.&lt;/p&gt;
&lt;p&gt;Let’s take a task like sentiment analysis. Initially, what we feed the network are sequences of words, essentially encoded as factors. In this setup, all words are equidistant: &lt;em&gt;Orange&lt;/em&gt; is as different from &lt;em&gt;kiwi&lt;/em&gt; as it is from &lt;em&gt;thunderstorm&lt;/em&gt;. An ensuing embedding layer then maps these representations to dense vectors of floating point numbers, which can be checked for mutual similarity via various similarity measures such as &lt;em&gt;cosine distance&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We hope that when we feed these “meaningful” vectors to the next layer(s), better classification will result. In addition, we may be interested in exploring that semantic space for its own sake, or use it in multi-modal transfer learning &lt;span class="citation"&gt;(Frome et al. 2013)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In this post, we’d like to do two things: First, we want to show an interesting application of embeddings beyond natural language processing, namely, their use in collaborative filtering. In this, we follow ideas developed in &lt;a href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson5-movielens.ipynb"&gt;lesson5-movielens.ipynb&lt;/a&gt; which is part of fast.ai’s &lt;a href="http://course.fast.ai/"&gt;Deep Learning for Coders&lt;/a&gt; class. Second, to gather more intuition, we’d like to take a look “under the hood” at how a simple embedding layer can be implemented.&lt;/p&gt;
&lt;p&gt;So first, let’s jump into collaborative filtering. Just like the notebook that inspired us, we’ll predict movie ratings. We will use the 2016 &lt;a href="http://files.grouplens.org/datasets/movielens/ml-latest-small.zip"&gt;ml-latest-small&lt;/a&gt; dataset from &lt;a href="https://grouplens.org/datasets/movielens/"&gt;MovieLens&lt;/a&gt; that contains ~100000 ratings of ~9900 movies, rated by ~700 users.&lt;/p&gt;
&lt;h2 id="embeddings-for-collaborative-filtering"&gt;Embeddings for collaborative filtering&lt;/h2&gt;
&lt;p&gt;In collaborative filtering, we try to generate recommendations based not on elaborate knowledge about our users and not on detailed profiles of our products, but on how users and products go together. Is product &lt;span class="math inline"&gt;\(\mathbf{p}\)&lt;/span&gt; a fit for user &lt;span class="math inline"&gt;\(\mathbf{u}\)&lt;/span&gt;? If so, we’ll recommend it.&lt;/p&gt;
&lt;p&gt;Often, this is done via matrix factorization. See, for example, &lt;a href="https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf"&gt;this nice article&lt;/a&gt; by the winners of the &lt;a href="https://www.netflixprize.com/"&gt;2009 Netflix prize&lt;/a&gt;, introducing the why and how of matrix factorization techniques as used in collaborative filtering.&lt;/p&gt;
&lt;p&gt;Here’s the general principle. While other techniques like &lt;a href="https://en.wikipedia.org/wiki/Non-negative_matrix_factorization"&gt;non-negative matrix factorization&lt;/a&gt; may be more popular, this diagram of &lt;strong&gt;singular value decomposition&lt;/strong&gt; (SVD) found on &lt;a href="https://research.fb.com/fast-randomized-svd/"&gt;Facebook Research&lt;/a&gt; is particularly instructive.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-09-26-embeddings-recommender/images/svd.png" class="external" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Figure from &lt;a href="https://research.fb.com/fast-randomized-svd/" class="uri"&gt;https://research.fb.com/fast-randomized-svd/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The diagram takes its example from the context of text analysis, assuming a co-occurrence matrix of hashtags and users (&lt;span class="math inline"&gt;\(\mathbf{A}\)&lt;/span&gt;). As stated above, we’ll instead work with a dataset of movie ratings.&lt;/p&gt;
&lt;p&gt;Were we doing matrix factorization, we would need to somehow address the fact that not every user has rated every movie. As we’ll be using embeddings instead, we won’t have that problem. For the sake of argumentation, though, let’s assume for a moment the ratings were a matrix, not a dataframe in tidy format.&lt;/p&gt;
&lt;p&gt;In that case, &lt;span class="math inline"&gt;\(\mathbf{A}\)&lt;/span&gt; would store the ratings, with each row containing the ratings one user gave to all movies.&lt;/p&gt;
&lt;p&gt;This matrix then gets decomposed into three matrices:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math inline"&gt;\(\mathbf{\Sigma}\)&lt;/span&gt; stores the importance of the latent factors governing the relationship between users and movies.&lt;/li&gt;
&lt;li&gt;&lt;span class="math inline"&gt;\(\mathbf{U}\)&lt;/span&gt; contains information on how users score on these latent factors. It’s a representation (&lt;em&gt;embedding&lt;/em&gt;) of users by the ratings they gave to the movies.&lt;/li&gt;
&lt;li&gt;&lt;span class="math inline"&gt;\(\mathbf{V}\)&lt;/span&gt; stores how movies score on these same latent factors. It’s a representation (&lt;em&gt;embedding&lt;/em&gt;) of movies by how they got rated by said users.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As soon as we have a representation of movies  as well as users  in the same latent space, we can determine their mutual fit by a simple dot product &lt;span class="math inline"&gt;\(\mathbf{m^ t}\mathbf{u}\)&lt;/span&gt;. Assuming the user and movie vectors have been normalized to length 1, this is equivalent to calculating the &lt;em&gt;cosine similarity&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[cos(\theta) = \frac{\mathbf{x^ t}\mathbf{y}}{\mathbf{||x||}\space\mathbf{||y||}}\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id="what-does-all-this-have-to-do-with-embeddings"&gt;What does all this have to do with embeddings?&lt;/h3&gt;
&lt;p&gt;Well, the same overall principles apply when we work with user resp. movie embeddings, instead of vectors obtained from matrix factorization. We’ll have one &lt;code&gt;layer_embedding&lt;/code&gt; for users, one &lt;code&gt;layer_embedding&lt;/code&gt; for movies, and a &lt;code&gt;layer_lambda&lt;/code&gt; that calculates the dot product.&lt;/p&gt;
&lt;p&gt;Here’s a minimal &lt;a href="https://tensorflow.rstudio.com/keras/articles/custom_models.html"&gt;custom model&lt;/a&gt; that does exactly this:&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;simple_dot &amp;lt;- function(embedding_dim,
                       n_users,
                       n_movies,
                       name = &amp;quot;simple_dot&amp;quot;) {
  
  keras_model_custom(name = name, function(self) {
    self$user_embedding &amp;lt;-
      layer_embedding(
        input_dim = n_users + 1,
        output_dim = embedding_dim,
        embeddings_initializer = initializer_random_uniform(minval = 0, maxval = 0.05),
        name = &amp;quot;user_embedding&amp;quot;
      )
    self$movie_embedding &amp;lt;-
      layer_embedding(
        input_dim = n_movies + 1,
        output_dim = embedding_dim,
        embeddings_initializer = initializer_random_uniform(minval = 0, maxval = 0.05),
        name = &amp;quot;movie_embedding&amp;quot;
      )
    self$dot &amp;lt;-
      layer_lambda(
        f = function(x) {
          k_batch_dot(x[[1]], x[[2]], axes = 2)
        }
      )
    
    function(x, mask = NULL) {
      users &amp;lt;- x[, 1]
      movies &amp;lt;- x[, 2]
      user_embedding &amp;lt;- self$user_embedding(users)
      movie_embedding &amp;lt;- self$movie_embedding(movies)
      self$dot(list(user_embedding, movie_embedding))
    }
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’re still missing the data though! Let’s load it. Besides the ratings themselves, we’ll also get the titles from &lt;em&gt;movies.csv&lt;/em&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;data_dir &amp;lt;- &amp;quot;ml-latest-small&amp;quot;
movies &amp;lt;- read_csv(file.path(data_dir, &amp;quot;movies.csv&amp;quot;))
ratings &amp;lt;- read_csv(file.path(data_dir, &amp;quot;ratings.csv&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While user ids have no gaps in this sample, that’s different for movie ids. We therefore convert them to consecutive numbers, so we can later specify an adequate size for the lookup matrix.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;dense_movies &amp;lt;- ratings %&amp;gt;% select(movieId) %&amp;gt;% distinct() %&amp;gt;% rowid_to_column()
ratings &amp;lt;- ratings %&amp;gt;% inner_join(dense_movies) %&amp;gt;% rename(movieIdDense = rowid)
ratings &amp;lt;- ratings %&amp;gt;% inner_join(movies) %&amp;gt;% select(userId, movieIdDense, rating, title, genres)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a note, then, of how many users resp. movies we have.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;n_movies &amp;lt;- ratings %&amp;gt;% select(movieIdDense) %&amp;gt;% distinct() %&amp;gt;% nrow()
n_users &amp;lt;- ratings %&amp;gt;% select(userId) %&amp;gt;% distinct() %&amp;gt;% nrow()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll split off 20% of the data for validation. After training, probably all users will have been seen by the network, while very likely, not all movies will have occurred in the training sample.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_indices &amp;lt;- sample(1:nrow(ratings), 0.8 * nrow(ratings))
train_ratings &amp;lt;- ratings[train_indices,]
valid_ratings &amp;lt;- ratings[-train_indices,]

x_train &amp;lt;- train_ratings %&amp;gt;% select(c(userId, movieIdDense)) %&amp;gt;% as.matrix()
y_train &amp;lt;- train_ratings %&amp;gt;% select(rating) %&amp;gt;% as.matrix()
x_valid &amp;lt;- valid_ratings %&amp;gt;% select(c(userId, movieIdDense)) %&amp;gt;% as.matrix()
y_valid &amp;lt;- valid_ratings %&amp;gt;% select(rating) %&amp;gt;% as.matrix()&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="training-a-simple-dot-product-model"&gt;Training a simple dot product model&lt;/h3&gt;
&lt;p&gt;We’re ready to start the training process. Feel free to experiment with different embedding dimensionalities.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;embedding_dim &amp;lt;- 64

model &amp;lt;- simple_dot(embedding_dim, n_users, n_movies)

model %&amp;gt;% compile(
  loss = &amp;quot;mse&amp;quot;,
  optimizer = &amp;quot;adam&amp;quot;
)

history &amp;lt;- model %&amp;gt;% fit(
  x_train,
  y_train,
  epochs = 10,
  batch_size = 32,
  validation_data = list(x_valid, y_valid),
  callbacks = list(callback_early_stopping(patience = 2))
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How well does this work? Final RMSE (the square root of the MSE loss we were using) on the validation set is around 1.08 , while popular benchmarks (e.g., of the &lt;a href="https://www.librec.net/release/v1.3/example.html"&gt;LibRec recommender system&lt;/a&gt;) lie around 0.91. Also, we’re overfitting early. It looks like we need a slightly more sophisticated system.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-09-26-embeddings-recommender/images/simple_embedding.png" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Training curve for simple dot product model&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id="accounting-for-user-and-movie-biases"&gt;Accounting for user and movie biases&lt;/h3&gt;
&lt;p&gt;A problem with our method is that we attribute the rating as a whole to user-movie interaction. However, some users are intrinsically more critical, while others tend to be more lenient. Analogously, films differ by average rating. We hope to get better predictions when factoring in these biases.&lt;/p&gt;
&lt;p&gt;Conceptually, we then calculate a prediction like this:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[pred =  avg + bias_m + bias_u + \mathbf{m^ t}\mathbf{u}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The corresponding Keras model gets just slightly more complex. In addition to the user and movie embeddings we’ve already been working with, the below model embeds the &lt;em&gt;average&lt;/em&gt; user and the &lt;em&gt;average&lt;/em&gt; movie in 1-d space. We then add both biases to the dot product encoding user-movie interaction. A sigmoid activation normalizes to a value between 0 and 1, which then gets mapped back to the original space.&lt;/p&gt;
&lt;p&gt;Note how in this model, we also use dropout on the user and movie embeddings (again, the best dropout rate is open to experimentation).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;max_rating &amp;lt;- ratings %&amp;gt;% summarise(max_rating = max(rating)) %&amp;gt;% pull()
min_rating &amp;lt;- ratings %&amp;gt;% summarise(min_rating = min(rating)) %&amp;gt;% pull()

dot_with_bias &amp;lt;- function(embedding_dim,
                          n_users,
                          n_movies,
                          max_rating,
                          min_rating,
                          name = &amp;quot;dot_with_bias&amp;quot;
                          ) {
  keras_model_custom(name = name, function(self) {
    
    self$user_embedding &amp;lt;-
      layer_embedding(input_dim = n_users + 1,
                      output_dim = embedding_dim,
                      name = &amp;quot;user_embedding&amp;quot;)
    self$movie_embedding &amp;lt;-
      layer_embedding(input_dim = n_movies + 1,
                      output_dim = embedding_dim,
                      name = &amp;quot;movie_embedding&amp;quot;)
    self$user_bias &amp;lt;-
      layer_embedding(input_dim = n_users + 1,
                      output_dim = 1,
                      name = &amp;quot;user_bias&amp;quot;)
    self$movie_bias &amp;lt;-
      layer_embedding(input_dim = n_movies + 1,
                      output_dim = 1,
                      name = &amp;quot;movie_bias&amp;quot;)
    self$user_dropout &amp;lt;- layer_dropout(rate = 0.3)
    self$movie_dropout &amp;lt;- layer_dropout(rate = 0.6)
    self$dot &amp;lt;-
      layer_lambda(
        f = function(x)
          k_batch_dot(x[[1]], x[[2]], axes = 2),
        name = &amp;quot;dot&amp;quot;
      )
    self$dot_bias &amp;lt;-
      layer_lambda(
        f = function(x)
          k_sigmoid(x[[1]] + x[[2]] + x[[3]]),
        name = &amp;quot;dot_bias&amp;quot;
      )
    self$pred &amp;lt;- layer_lambda(
      f = function(x)
        x * (self$max_rating - self$min_rating) + self$min_rating,
      name = &amp;quot;pred&amp;quot;
    )
    self$max_rating &amp;lt;- max_rating
    self$min_rating &amp;lt;- min_rating
    
    function(x, mask = NULL) {
      
      users &amp;lt;- x[, 1]
      movies &amp;lt;- x[, 2]
      user_embedding &amp;lt;-
        self$user_embedding(users) %&amp;gt;% self$user_dropout()
      movie_embedding &amp;lt;-
        self$movie_embedding(movies) %&amp;gt;% self$movie_dropout()
      dot &amp;lt;- self$dot(list(user_embedding, movie_embedding))
      dot_bias &amp;lt;-
        self$dot_bias(list(dot, self$user_bias(users), self$movie_bias(movies)))
      self$pred(dot_bias)
    }
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How well does this model perform?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- dot_with_bias(embedding_dim,
                       n_users,
                       n_movies,
                       max_rating,
                       min_rating)

model %&amp;gt;% compile(
  loss = &amp;quot;mse&amp;quot;,
  optimizer = &amp;quot;adam&amp;quot;
)

history &amp;lt;- model %&amp;gt;% fit(
  x_train,
  y_train,
  epochs = 10,
  batch_size = 32,
  validation_data = list(x_valid, y_valid),
  callbacks = list(callback_early_stopping(patience = 2))
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not only does it overfit later, it actually reaches a way better RMSE of 0.88 on the validation set!&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-09-26-embeddings-recommender/images/bias_embedding.png" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Training curve for dot product model with biases&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Spending some time on hyperparameter optimization could very well lead to even better results. As this post focuses on the conceptual side though, we want to see what else we can do with those embeddings.&lt;/p&gt;
&lt;h3 id="embeddings-a-closer-look"&gt;Embeddings: a closer look&lt;/h3&gt;
&lt;p&gt;We can easily extract the embedding matrices from the respective layers. Let’s do this for movies now.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;movie_embeddings &amp;lt;- (model %&amp;gt;% get_layer(&amp;quot;movie_embedding&amp;quot;) %&amp;gt;% get_weights())[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How are they distributed? Here’s a heatmap of the first 20 movies. (Note how we increment the row indices by 1, because the very first row in the embedding matrix belongs to a movie id &lt;em&gt;0&lt;/em&gt; which does not exist in our dataset.) We see that the embeddings look rather uniformly distributed between -0.5 and 0.5.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;levelplot(
  t(movie_embeddings[2:21, 1:64]),
  xlab = &amp;quot;&amp;quot;,
  ylab = &amp;quot;&amp;quot;,
  scale = (list(draw = FALSE)))&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-09-26-embeddings-recommender/images/levelplot.png" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Embeddings for first 20 movies&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Naturally, we might be interested in dimensionality reduction, and see how specific movies score on the dominant factors. A possible way to achieve this is PCA:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;movie_pca &amp;lt;- movie_embeddings %&amp;gt;% prcomp(center = FALSE)
components &amp;lt;- movie_pca$x %&amp;gt;% as.data.frame() %&amp;gt;% rowid_to_column()

plot(movie_pca)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-09-26-embeddings-recommender/images/pca.png" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;PCA: Variance explained by component&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Let’s just look at the first principal component as the second one already explains much less variance.&lt;/p&gt;
&lt;p&gt;Here are the 10 movies (out of all that were rated at least 20 times) that scored lowest on the first factor:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ratings_with_pc12 &amp;lt;-
  ratings %&amp;gt;% inner_join(components %&amp;gt;% select(rowid, PC1, PC2),
                         by = c(&amp;quot;movieIdDense&amp;quot; = &amp;quot;rowid&amp;quot;))

ratings_grouped &amp;lt;-
  ratings_with_pc12 %&amp;gt;%
  group_by(title) %&amp;gt;%
  summarize(
    PC1 = max(PC1),
    PC2 = max(PC2),
    rating = mean(rating),
    genres = max(genres),
    num_ratings = n()
  )

ratings_grouped %&amp;gt;% filter(num_ratings &amp;gt; 20) %&amp;gt;% arrange(PC1) %&amp;gt;% print(n = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1,247 x 6
   title                                   PC1      PC2 rating genres                   num_ratings
   &amp;lt;chr&amp;gt;                                 &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                          &amp;lt;int&amp;gt;
 1 Starman (1984)                       -1.15  -0.400     3.45 Adventure|Drama|Romance…          22
 2 Bulworth (1998)                      -0.820  0.218     3.29 Comedy|Drama|Romance              31
 3 Cable Guy, The (1996)                -0.801 -0.00333   2.55 Comedy|Thriller                   59
 4 Species (1995)                       -0.772 -0.126     2.81 Horror|Sci-Fi                     55
 5 Save the Last Dance (2001)           -0.765  0.0302    3.36 Drama|Romance                     21
 6 Spanish Prisoner, The (1997)         -0.760  0.435     3.91 Crime|Drama|Mystery|Thr…          23
 7 Sgt. Bilko (1996)                    -0.757  0.249     2.76 Comedy                            29
 8 Naked Gun 2 1/2: The Smell of Fear,… -0.749  0.140     3.44 Comedy                            27
 9 Swordfish (2001)                     -0.694  0.328     2.92 Action|Crime|Drama                33
10 Addams Family Values (1993)          -0.693  0.251     3.15 Children|Comedy|Fantasy           73
# ... with 1,237 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And here, inversely, are those that scored highest:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ratings_grouped %&amp;gt;% filter(num_ratings &amp;gt; 20) %&amp;gt;% arrange(desc(PC1)) %&amp;gt;% print(n = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; A tibble: 1,247 x 6
   title                                PC1        PC2 rating genres                    num_ratings
   &amp;lt;chr&amp;gt;                              &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                           &amp;lt;int&amp;gt;
 1 Graduate, The (1967)                1.41  0.0432      4.12 Comedy|Drama|Romance               89
 2 Vertigo (1958)                      1.38 -0.0000246   4.22 Drama|Mystery|Romance|Th…          69
 3 Breakfast at Tiffany&amp;#39;s (1961)       1.28  0.278       3.59 Drama|Romance                      44
 4 Treasure of the Sierra Madre, The…  1.28 -0.496       4.3  Action|Adventure|Drama|W…          30
 5 Boot, Das (Boat, The) (1981)        1.26  0.238       4.17 Action|Drama|War                   51
 6 Flintstones, The (1994)             1.18  0.762       2.21 Children|Comedy|Fantasy            39
 7 Rock, The (1996)                    1.17 -0.269       3.74 Action|Adventure|Thriller         135
 8 In the Heat of the Night (1967)     1.15 -0.110       3.91 Drama|Mystery                      22
 9 Quiz Show (1994)                    1.14 -0.166       3.75 Drama                              90
10 Striptease (1996)                   1.14 -0.681       2.46 Comedy|Crime                       39
# ... with 1,237 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll leave it to the knowledgeable reader to name these factors, and proceed to our second topic: How does an embedding layer do what it does?&lt;/p&gt;
&lt;h2 id="do-it-yourself-embeddings"&gt;Do-it-yourself embeddings&lt;/h2&gt;
&lt;p&gt;You may have heard people say all an embedding layer did was just a lookup. Imagine you had a dataset that, in addition to continuous variables like temperature or barometric pressure, contained a categorical column &lt;em&gt;characterization&lt;/em&gt; consisting of tags like “foggy” or “cloudy”. Say &lt;em&gt;characterization&lt;/em&gt; had 7 possible values, encoded as a factor with levels 1-7.&lt;/p&gt;
&lt;p&gt;Were we going to feed this variable to a non-embedding layer, &lt;code&gt;layer_dense&lt;/code&gt; say, we’d have to take care that those numbers do not get taken for integers, thus falsely implying an interval (or at least ordered) scale. But when we use an embedding as the first layer in a Keras model, we feed in integers all the time! For example, in text classification, a sentence might get encoded as a vector padded with zeroes, like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;2  77   4   5 122   55  1  3   0   0  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The thing that makes this work is that the embedding layer actually &lt;em&gt;does&lt;/em&gt; perform a lookup. Below, you’ll find a very simple&lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href="https://tensorflow.rstudio.com/keras/articles/custom_layers.html"&gt;custom layer&lt;/a&gt; that does essentially the same thing as Keras’ &lt;code&gt;layer_embedding&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It has a weight matrix &lt;code&gt;self$embeddings&lt;/code&gt; that maps from an input space (movies, say) to the output space of latent factors (embeddings).&lt;/li&gt;
&lt;li&gt;When we call the layer, as in&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;x &amp;lt;- k_gather(self$embeddings, x)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;it looks up the passed-in row number in the weight matrix, thus retrieving an item’s distributed representation from the matrix.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;SimpleEmbedding &amp;lt;- R6::R6Class(
  &amp;quot;SimpleEmbedding&amp;quot;,
  
  inherit = KerasLayer,
  
  public = list(
    output_dim = NULL,
    emb_input_dim = NULL,
    embeddings = NULL,
    
    initialize = function(emb_input_dim, output_dim) {
      self$emb_input_dim &amp;lt;- emb_input_dim
      self$output_dim &amp;lt;- output_dim
    },
    
    build = function(input_shape) {
      self$embeddings &amp;lt;- self$add_weight(
        name = &amp;#39;embeddings&amp;#39;,
        shape = list(self$emb_input_dim, self$output_dim),
        initializer = initializer_random_uniform(),
        trainable = TRUE
      )
    },
    
    call = function(x, mask = NULL) {
      x &amp;lt;- k_cast(x, &amp;quot;int32&amp;quot;)
      k_gather(self$embeddings, x)
    },
    
    compute_output_shape = function(input_shape) {
      list(self$output_dim)
    }
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As usual with custom layers, we still need a wrapper that takes care of instantiation.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;layer_simple_embedding &amp;lt;-
  function(object,
           emb_input_dim,
           output_dim,
           name = NULL,
           trainable = TRUE) {
    create_layer(
      SimpleEmbedding,
      object,
      list(
        emb_input_dim = as.integer(emb_input_dim),
        output_dim = as.integer(output_dim),
        name = name,
        trainable = trainable
      )
    )
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Does this work? Let’s test it on the ratings prediction task! We’ll just substitute the custom layer in the simple dot product model we started out with, and check if we get out a similar RMSE.&lt;/p&gt;
&lt;h2 id="putting-the-custom-embedding-layer-to-test"&gt;Putting the custom embedding layer to test&lt;/h2&gt;
&lt;p&gt;Here’s the simple dot product model again, this time using our custom embedding layer.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;simple_dot2 &amp;lt;- function(embedding_dim,
                       n_users,
                       n_movies,
                       name = &amp;quot;simple_dot2&amp;quot;) {
  
  keras_model_custom(name = name, function(self) {
    self$embedding_dim &amp;lt;- embedding_dim
    
    self$user_embedding &amp;lt;-
      layer_simple_embedding(
        emb_input_dim = list(n_users + 1),
        output_dim = embedding_dim,
        name = &amp;quot;user_embedding&amp;quot;
      )
    self$movie_embedding &amp;lt;-
      layer_simple_embedding(
        emb_input_dim = list(n_movies + 1),
        output_dim = embedding_dim,
        name = &amp;quot;movie_embedding&amp;quot;
      )
    self$dot &amp;lt;-
      layer_lambda(
        output_shape = self$embedding_dim,
        f = function(x) {
          k_batch_dot(x[[1]], x[[2]], axes = 2)
        }
      )
    
    function(x, mask = NULL) {
      users &amp;lt;- x[, 1]
      movies &amp;lt;- x[, 2]
      user_embedding &amp;lt;- self$user_embedding(users)
      movie_embedding &amp;lt;- self$movie_embedding(movies)
      self$dot(list(user_embedding, movie_embedding))
    }
  })
}

model &amp;lt;- simple_dot2(embedding_dim, n_users, n_movies)

model %&amp;gt;% compile(
  loss = &amp;quot;mse&amp;quot;,
  optimizer = &amp;quot;adam&amp;quot;
)

history &amp;lt;- model %&amp;gt;% fit(
  x_train,
  y_train,
  epochs = 10,
  batch_size = 32,
  validation_data = list(x_valid, y_valid),
  callbacks = list(callback_early_stopping(patience = 2))
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We end up with a RMSE of 1.13 on the validation set, which is not far from the 1.08 we obtained when using &lt;code&gt;layer_embedding&lt;/code&gt;. At least, this should tell us that we successfully reproduced the approach.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Our goals in this post were twofold: Shed some light on how an embedding layer can be implemented, and show how embeddings calculated by a neural network can be used as a substitute for component matrices obtained from matrix decomposition. Of course, this is not the only thing that’s fascinating about embeddings!&lt;/p&gt;
&lt;p&gt;For example, a very practical question is how much actual predictions can be improved by using embeddings instead of one-hot vectors; another is how learned embeddings might differ depending on what task they were trained on. Last not least - how do latent factors learned via embeddings differ from those learned by an autoencoder?&lt;/p&gt;
&lt;p&gt;In that spirit, there is no lack of topics for exploration and poking around …&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-2018arXiv180202896A"&gt;
&lt;p&gt;Ahmed, N. K., R. Rossi, J. Boaz Lee, T. L. Willke, R. Zhou, X. Kong, and H. Eldardiry. 2018. “Learning Role-Based Graph Embeddings.” &lt;em&gt;ArXiv E-Prints&lt;/em&gt;, February. &lt;a href="http://arxiv.org/abs/1802.02896"&gt;http://arxiv.org/abs/1802.02896&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-abs-1803-09473"&gt;
&lt;p&gt;Alon, Uri, Meital Zilberstein, Omer Levy, and Eran Yahav. 2018. “Code2vec: Learning Distributed Representations of Code.” &lt;em&gt;CoRR&lt;/em&gt; abs/1803.09473. &lt;a href="http://arxiv.org/abs/1803.09473"&gt;http://arxiv.org/abs/1803.09473&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-FromeCSBDRM13"&gt;
&lt;p&gt;Frome, Andrea, Gregory S. Corrado, Jonathon Shlens, Samy Bengio, Jeffrey Dean, Marc’Aurelio Ranzato, and Tomas Mikolov. 2013. “DeViSE: A Deep Visual-Semantic Embedding Model.” In &lt;em&gt;NIPS&lt;/em&gt;, 2121–9.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-2018arXiv180604795H"&gt;
&lt;p&gt;Hallac, D., S. Bhooshan, M. Chen, K. Abida, R. Sosic, and J. Leskovec. 2018. “Drive2Vec: Multiscale State-Space Embedding of Vehicular Sensor Data.” &lt;em&gt;ArXiv E-Prints&lt;/em&gt;, June. &lt;a href="http://arxiv.org/abs/1806.04795"&gt;http://arxiv.org/abs/1806.04795&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-abs-1805-02855"&gt;
&lt;p&gt;Jean, Neal, Sherrie Wang, Anshul Samar, George Azzari, David B. Lobell, and Stefano Ermon. 2018. “Tile2Vec: Unsupervised Representation Learning for Spatially Distributed Data.” &lt;em&gt;CoRR&lt;/em&gt; abs/1805.02855. &lt;a href="http://arxiv.org/abs/1805.02855"&gt;http://arxiv.org/abs/1805.02855&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-2018arXiv180309123K"&gt;
&lt;p&gt;Krstovski, K., and D. M. Blei. 2018. “Equation Embeddings.” &lt;em&gt;ArXiv E-Prints&lt;/em&gt;, March. &lt;a href="http://arxiv.org/abs/1803.09123"&gt;http://arxiv.org/abs/1803.09123&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Rumelhart"&gt;
&lt;p&gt;Rumelhart, David E., James L. McClelland, and CORPORATE PDP Research Group, eds. 1986. &lt;em&gt;Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 2: Psychological and Biological Models&lt;/em&gt;. Cambridge, MA, USA: MIT Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-2018arXiv180200864Z"&gt;
&lt;p&gt;Zohra Smaili, F., X. Gao, and R. Hoehndorf. 2018. “Onto2Vec: Joint Vector-Based Representation of Biological Entities and Their Ontology-Based Annotations.” &lt;em&gt;ArXiv E-Prints&lt;/em&gt;, January. &lt;a href="http://arxiv.org/abs/1802.00864"&gt;http://arxiv.org/abs/1802.00864&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;From: &lt;a href="http://www.cs.toronto.edu/~bonner/courses/2014s/csc321/lectures/lec5.pdf" class="uri"&gt;http://www.cs.toronto.edu/~bonner/courses/2014s/csc321/lectures/lec5.pdf&lt;/a&gt;&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;Custom models are a recent Keras feature that allow for a flexible definition of the forward pass. While the current use case does not require using a custom model, it nicely illustrates how the network’s logic can quickly be grasped by looking at the &lt;em&gt;call&lt;/em&gt; method.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;It really &lt;em&gt;is&lt;/em&gt; simple; it only works with input length = 1.&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">89365f74b54480960d45d5fd095eef6e</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Tabular Data</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-09-26-embeddings-recommender</guid>
      <pubDate>Wed, 26 Sep 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-09-26-embeddings-recommender/images/m.png" medium="image" type="image/png" width="700" height="402"/>
    </item>
    <item>
      <title>Image-to-image translation with pix2pix</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-09-20-eager-pix2pix</link>
      <description>


&lt;p&gt;What do we need to train a neural network? A common answer is: a model, a cost function, and an optimization algorithm. (I know: I’m leaving out the most important thing here - the data.)&lt;/p&gt;
&lt;p&gt;As computer programs work with numbers, the cost function has to be pretty specific: We can’t just say &lt;em&gt;predict next month’s demand for lawn mowers please, and do your best&lt;/em&gt;, we have to say something like this: Minimize the squared deviation of the estimate from the target value.&lt;/p&gt;
&lt;p&gt;In some cases it may be straightforward to map a task to a measure of error, in others, it may not. Consider the task of generating non-existing objects of a certain type (like a face, a scene, or a video clip). How do we quantify success? The trick with &lt;em&gt;generative adversarial networks&lt;/em&gt; (GANs) is to let the network learn the cost function.&lt;/p&gt;
&lt;p&gt;As shown in &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-08-26-eager-dcgan/"&gt;Generating images with Keras and TensorFlow eager execution&lt;/a&gt;, in a simple GAN the setup is this: One agent, the &lt;em&gt;generator&lt;/em&gt;, keeps on producing fake objects. The other, the &lt;em&gt;discriminator&lt;/em&gt;, is tasked to tell apart the real objects from the fake ones. For the generator, loss is augmented when its fraud gets discovered, meaning that the generator’s cost function depends on what the discriminator does. For the discriminator, loss grows when it fails to correctly tell apart generated objects from authentic ones.&lt;/p&gt;
&lt;p&gt;In a GAN of the type just described, creation starts from white noise. However in the real world, what is required may be a form of transformation, not creation. Take, for example, colorization of black-and-white images, or conversion of aerials to maps. For applications like those, we &lt;em&gt;condition&lt;/em&gt; on additional input: Hence the name, &lt;em&gt;conditional adversarial networks&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Put concretely, this means the generator is passed not (or not only) white noise, but data of a certain input structure, such as edges or shapes. It then has to generate realistic-looking pictures of real objects having those shapes. The discriminator, too, may receive the shapes or edges as input, in addition to the fake and real objects it is tasked to tell apart.&lt;/p&gt;
&lt;p&gt;Here are a few examples of conditioning, taken from the paper we’ll be implementing (see below):&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-09-20-eager-pix2pix/images/pix2pix.png" class="external" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Figure from Image-to-Image Translation with Conditional Adversarial Networks &lt;span class="citation"&gt;Isola et al. (2016)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In this post, we port to R a &lt;a href="https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb"&gt;Google Colaboratory Notebook&lt;/a&gt; using Keras with eager execution. We’re implementing the basic architecture from &lt;em&gt;pix2pix&lt;/em&gt;, as described by Isola et al. in their 2016 paper&lt;span class="citation"&gt;(Isola et al. 2016)&lt;/span&gt;. It’s an interesting paper to read as it validates the approach on a bunch of different datasets, and shares outcomes of using different loss families, too:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-09-20-eager-pix2pix/images/pix2pixlosses.png" class="external" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Figure from Image-to-Image Translation with Conditional Adversarial Networks &lt;span class="citation"&gt;Isola et al. (2016)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="prerequisites"&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;The code shown here will work with the current CRAN versions of &lt;code&gt;tensorflow&lt;/code&gt;, &lt;code&gt;keras&lt;/code&gt;, and &lt;code&gt;tfdatasets&lt;/code&gt;. Also, be sure to check that you’re using at least version 1.9 of TensorFlow. If that isn’t the case, as of this writing, this&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tensorflow)
install_tensorflow()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will get you version 1.10.&lt;/p&gt;
&lt;p&gt;When loading libraries, please make sure you’re executing the first 4 lines in the exact order shown. We need to make sure we’re using the TensorFlow implementation of Keras (&lt;code&gt;tf.keras&lt;/code&gt; in Python land), and we have to enable eager execution before using TensorFlow in any way.&lt;/p&gt;
&lt;p&gt;No need to copy-paste any code snippets - you’ll find the complete code (in order necessary for execution) here: &lt;a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/eager-pix2pix.R"&gt;eager-pix2pix.R&lt;/a&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
use_implementation(&amp;quot;tensorflow&amp;quot;)
library(tensorflow)
tfe_enable_eager_execution(device_policy = &amp;quot;silent&amp;quot;)

library(tfdatasets)
library(purrr)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="dataset"&gt;Dataset&lt;/h2&gt;
&lt;p&gt;For this post, we’re working with one of the datasets used in the paper, a &lt;a href="https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/"&gt;preprocessed&lt;/a&gt; version of the &lt;a href="http://cmp.felk.cvut.cz/~tylecr1/facade/"&gt;CMP Facade Dataset&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Images contain the ground truth - that we’d wish for the generator to generate, and for the discriminator to correctly detect as authentic - and the input we’re conditioning on (a coarse segmention into object classes) next to each other in the same file.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-09-20-eager-pix2pix/images/105.jpg" class="external" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Figure from &lt;a href="https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/" class="uri"&gt;https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="preprocessing"&gt;Preprocessing&lt;/h2&gt;
&lt;p&gt;Obviously, our preprocessing will have to split the input images into parts. That’s the first thing that happens in the function below.&lt;/p&gt;
&lt;p&gt;After that, action depends on whether we’re in the training or testing phases. If we’re training, we perform random jittering, via upsizing the image to &lt;code&gt;286x286&lt;/code&gt; and then cropping to the original size of &lt;code&gt;256x256&lt;/code&gt;. In about 50% of the cases, we also flipping the image left-to-right.&lt;/p&gt;
&lt;p&gt;In both cases, training and testing, we normalize the image to the range between -1 and 1.&lt;/p&gt;
&lt;p&gt;Note the use of the &lt;code&gt;tf$image&lt;/code&gt; module for image -related operations. This is required as the images will be streamed via &lt;code&gt;tfdatasets&lt;/code&gt;, which works on TensorFlow graphs.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;img_width &amp;lt;- 256L
img_height &amp;lt;- 256L

load_image &amp;lt;- function(image_file, is_train) {

  image &amp;lt;- tf$read_file(image_file)
  image &amp;lt;- tf$image$decode_jpeg(image)
  
  w &amp;lt;- as.integer(k_shape(image)[2])
  w2 &amp;lt;- as.integer(w / 2L)
  real_image &amp;lt;- image[ , 1L:w2, ]
  input_image &amp;lt;- image[ , (w2 + 1L):w, ]
  
  input_image &amp;lt;- k_cast(input_image, tf$float32)
  real_image &amp;lt;- k_cast(real_image, tf$float32)

  if (is_train) {
    input_image &amp;lt;-
      tf$image$resize_images(input_image,
                             c(286L, 286L),
                             align_corners = TRUE,
                             method = 2)
    real_image &amp;lt;- tf$image$resize_images(real_image,
                                         c(286L, 286L),
                                         align_corners = TRUE,
                                         method = 2)
    
    stacked_image &amp;lt;-
      k_stack(list(input_image, real_image), axis = 1)
    cropped_image &amp;lt;-
      tf$random_crop(stacked_image, size = c(2L, img_height, img_width, 3L))
    c(input_image, real_image) %&amp;lt;-% 
      list(cropped_image[1, , , ], cropped_image[2, , , ])
    
    if (runif(1) &amp;gt; 0.5) {
      input_image &amp;lt;- tf$image$flip_left_right(input_image)
      real_image &amp;lt;- tf$image$flip_left_right(real_image)
    }
    
  } else {
    input_image &amp;lt;-
      tf$image$resize_images(
        input_image,
        size = c(img_height, img_width),
        align_corners = TRUE,
        method = 2
      )
    real_image &amp;lt;-
      tf$image$resize_images(
        real_image,
        size = c(img_height, img_width),
        align_corners = TRUE,
        method = 2
      )
  }
  
  input_image &amp;lt;- (input_image / 127.5) - 1
  real_image &amp;lt;- (real_image / 127.5) - 1
  
  list(input_image, real_image)
}&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="streaming-the-data"&gt;Streaming the data&lt;/h2&gt;
&lt;p&gt;The images will be streamed via &lt;code&gt;tfdatasets&lt;/code&gt;, using a batch size of 1. Note how the &lt;code&gt;load_image&lt;/code&gt; function we defined above is wrapped in &lt;code&gt;tf$py_func&lt;/code&gt; to enable accessing tensor values in the usual eager way (which by default, as of this writing, is not possible with the TensorFlow datasets API).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# change to where you unpacked the data
# there will be train, val and test subdirectories below
data_dir &amp;lt;- &amp;quot;facades&amp;quot;

buffer_size &amp;lt;- 400
batch_size &amp;lt;- 1
batches_per_epoch &amp;lt;- buffer_size / batch_size

train_dataset &amp;lt;-
  tf$data$Dataset$list_files(file.path(data_dir, &amp;quot;train/*.jpg&amp;quot;)) %&amp;gt;%
  dataset_shuffle(buffer_size) %&amp;gt;%
  dataset_map(function(image) {
    tf$py_func(load_image, list(image, TRUE), list(tf$float32, tf$float32))
  }) %&amp;gt;%
  dataset_batch(batch_size)

test_dataset &amp;lt;-
  tf$data$Dataset$list_files(file.path(data_dir, &amp;quot;test/*.jpg&amp;quot;)) %&amp;gt;%
  dataset_map(function(image) {
    tf$py_func(load_image, list(image, TRUE), list(tf$float32, tf$float32))
  }) %&amp;gt;%
  dataset_batch(batch_size)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="defining-the-actors"&gt;Defining the actors&lt;/h2&gt;
&lt;h3 id="generator"&gt;Generator&lt;/h3&gt;
&lt;p&gt;First, here’s the generator. Let’s start with a birds-eye view.&lt;/p&gt;
&lt;p&gt;The generator receives as input a coarse segmentation, of size 256x256, and should produce a nice color image of a facade. It first successively downsamples the input, up to a minimal size of 1x1. Then after maximal condensation, it starts upsampling again, until it has reached the required output resolution of 256x256.&lt;/p&gt;
&lt;p&gt;During downsampling, as spatial resolution decreases, the number of filters increases. During upsampling, it goes the opposite way.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;generator &amp;lt;- function(name = &amp;quot;generator&amp;quot;) {
  
  keras_model_custom(name = name, function(self) {
    
    self$down1 &amp;lt;- downsample(64, 4, apply_batchnorm = FALSE)
    self$down2 &amp;lt;- downsample(128, 4)
    self$down3 &amp;lt;- downsample(256, 4)
    self$down4 &amp;lt;- downsample(512, 4)
    self$down5 &amp;lt;- downsample(512, 4)
    self$down6 &amp;lt;- downsample(512, 4)
    self$down7 &amp;lt;- downsample(512, 4)
    self$down8 &amp;lt;- downsample(512, 4)
    
    self$up1 &amp;lt;- upsample(512, 4, apply_dropout = TRUE)
    self$up2 &amp;lt;- upsample(512, 4, apply_dropout = TRUE)
    self$up3 &amp;lt;- upsample(512, 4, apply_dropout = TRUE)
    self$up4 &amp;lt;- upsample(512, 4)
    self$up5 &amp;lt;- upsample(256, 4)
    self$up6 &amp;lt;- upsample(128, 4)
    self$up7 &amp;lt;- upsample(64, 4)
    
    self$last &amp;lt;- layer_conv_2d_transpose(
      filters = 3,
      kernel_size = 4,
      strides = 2,
      padding = &amp;quot;same&amp;quot;,
      kernel_initializer = initializer_random_normal(0, 0.2),
      activation = &amp;quot;tanh&amp;quot;
    )
    
    function(x, mask = NULL, training = TRUE) {           # x shape == (bs, 256, 256, 3)
     
      x1 &amp;lt;- x %&amp;gt;% self$down1(training = training)         # (bs, 128, 128, 64)
      x2 &amp;lt;- self$down2(x1, training = training)           # (bs, 64, 64, 128)
      x3 &amp;lt;- self$down3(x2, training = training)           # (bs, 32, 32, 256)
      x4 &amp;lt;- self$down4(x3, training = training)           # (bs, 16, 16, 512)
      x5 &amp;lt;- self$down5(x4, training = training)           # (bs, 8, 8, 512)
      x6 &amp;lt;- self$down6(x5, training = training)           # (bs, 4, 4, 512)
      x7 &amp;lt;- self$down7(x6, training = training)           # (bs, 2, 2, 512)
      x8 &amp;lt;- self$down8(x7, training = training)           # (bs, 1, 1, 512)

      x9 &amp;lt;- self$up1(list(x8, x7), training = training)   # (bs, 2, 2, 1024)
      x10 &amp;lt;- self$up2(list(x9, x6), training = training)  # (bs, 4, 4, 1024)
      x11 &amp;lt;- self$up3(list(x10, x5), training = training) # (bs, 8, 8, 1024)
      x12 &amp;lt;- self$up4(list(x11, x4), training = training) # (bs, 16, 16, 1024)
      x13 &amp;lt;- self$up5(list(x12, x3), training = training) # (bs, 32, 32, 512)
      x14 &amp;lt;- self$up6(list(x13, x2), training = training) # (bs, 64, 64, 256)
      x15 &amp;lt;-self$up7(list(x14, x1), training = training)  # (bs, 128, 128, 128)
      x16 &amp;lt;- self$last(x15)                               # (bs, 256, 256, 3)
      x16
    }
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How can spatial information be preserved if we downsample all the way down to a single pixel? The generator follows the general principle of a &lt;em&gt;U-Net&lt;/em&gt; &lt;span class="citation"&gt;(Ronneberger, Fischer, and Brox 2015)&lt;/span&gt;, where skip connections exist from layers earlier in the downsampling process to layers later on the way up.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-09-20-eager-pix2pix/images/unet.png" class="external" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Figure from &lt;span class="citation"&gt;(Ronneberger, Fischer, and Brox 2015)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Let’s take the line&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;x15 &amp;lt;-self$up7(list(x14, x1), training = training)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;from the &lt;code&gt;call&lt;/code&gt; method.&lt;/p&gt;
&lt;p&gt;Here, the inputs to &lt;code&gt;self$up&lt;/code&gt; are &lt;code&gt;x14&lt;/code&gt;, which went through all of the down- and upsampling, and &lt;code&gt;x1&lt;/code&gt;, the output from the very first downsampling step. The former has resolution 64x64, the latter, 128x128. How do they get combined?&lt;/p&gt;
&lt;p&gt;That’s taken care of by &lt;code&gt;upsample&lt;/code&gt;, technically a custom model of its own. As an aside, we remark how custom models let you pack your code into nice, reusable modules.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;upsample &amp;lt;- function(filters,
                     size,
                     apply_dropout = FALSE,
                     name = &amp;quot;upsample&amp;quot;) {
  
  keras_model_custom(name = NULL, function(self) {
    
    self$apply_dropout &amp;lt;- apply_dropout
    self$up_conv &amp;lt;- layer_conv_2d_transpose(
      filters = filters,
      kernel_size = size,
      strides = 2,
      padding = &amp;quot;same&amp;quot;,
      kernel_initializer = initializer_random_normal(),
      use_bias = FALSE
    )
    self$batchnorm &amp;lt;- layer_batch_normalization()
    if (self$apply_dropout) {
      self$dropout &amp;lt;- layer_dropout(rate = 0.5)
    }
    
    function(xs, mask = NULL, training = TRUE) {
      
      c(x1, x2) %&amp;lt;-% xs
      x &amp;lt;- self$up_conv(x1) %&amp;gt;% self$batchnorm(training = training)
      if (self$apply_dropout) {
        x %&amp;gt;% self$dropout(training = training)
      }
      x %&amp;gt;% layer_activation(&amp;quot;relu&amp;quot;)
      concat &amp;lt;- k_concatenate(list(x, x2))
      concat
    }
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;x14&lt;/code&gt; is upsampled to double its size, and &lt;code&gt;x1&lt;/code&gt; is appended as is. The axis of concatenation here is axis 4, the feature map / channels axis. &lt;code&gt;x1&lt;/code&gt; comes with 64 channels, &lt;code&gt;x14&lt;/code&gt; comes out of &lt;code&gt;layer_conv_2d_transpose&lt;/code&gt; with 64 channels, too (because &lt;code&gt;self$up7&lt;/code&gt; has been defined that way). So we end up with an image of resolution 128x128 and 128 feature maps for the output of step &lt;code&gt;x15&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Downsampling, too, is factored out to its own model. Here too, the number of filters is configurable.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;downsample &amp;lt;- function(filters,
                       size,
                       apply_batchnorm = TRUE,
                       name = &amp;quot;downsample&amp;quot;) {
  
  keras_model_custom(name = name, function(self) {
    
    self$apply_batchnorm &amp;lt;- apply_batchnorm
    self$conv1 &amp;lt;- layer_conv_2d(
      filters = filters,
      kernel_size = size,
      strides = 2,
      padding = &amp;#39;same&amp;#39;,
      kernel_initializer = initializer_random_normal(0, 0.2),
      use_bias = FALSE
    )
    if (self$apply_batchnorm) {
      self$batchnorm &amp;lt;- layer_batch_normalization()
    }
    
    function(x, mask = NULL, training = TRUE) {
      
      x &amp;lt;- self$conv1(x)
      if (self$apply_batchnorm) {
        x %&amp;gt;% self$batchnorm(training = training)
      }
      x %&amp;gt;% layer_activation_leaky_relu()
    }
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now for the discriminator.&lt;/p&gt;
&lt;h3 id="discriminator"&gt;Discriminator&lt;/h3&gt;
&lt;p&gt;Again, let’s start with a birds-eye view. The discriminator receives as input both the coarse segmentation and the ground truth. Both are concatenated and processed together. Just like the generator, the discriminator is thus conditioned on the segmentation.&lt;/p&gt;
&lt;p&gt;What does the discriminator return? The output of &lt;code&gt;self$last&lt;/code&gt; has one channel, but a spatial resolution of 30x30: We’re outputting a probability for each of 30x30 image &lt;em&gt;patches&lt;/em&gt; (which is why the authors are calling this a &lt;em&gt;PatchGAN&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;The discriminator thus working on small image patches means it only cares about local structure, and consequently, enforces correctness in the high frequencies only. Correctness in the low frequencies is taken care of by an additional L1 component in the discriminator loss that operates over the whole image (as we’ll see below).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;discriminator &amp;lt;- function(name = &amp;quot;discriminator&amp;quot;) {
  
  keras_model_custom(name = name, function(self) {
    
    self$down1 &amp;lt;- disc_downsample(64, 4, FALSE)
    self$down2 &amp;lt;- disc_downsample(128, 4)
    self$down3 &amp;lt;- disc_downsample(256, 4)
    self$zero_pad1 &amp;lt;- layer_zero_padding_2d()
    self$conv &amp;lt;- layer_conv_2d(
      filters = 512,
      kernel_size = 4,
      strides = 1,
      kernel_initializer = initializer_random_normal(),
      use_bias = FALSE
    )
    self$batchnorm &amp;lt;- layer_batch_normalization()
    self$zero_pad2 &amp;lt;- layer_zero_padding_2d()
    self$last &amp;lt;- layer_conv_2d(
      filters = 1,
      kernel_size = 4,
      strides = 1,
      kernel_initializer = initializer_random_normal()
    )
    
    function(x, y, mask = NULL, training = TRUE) {
      
      x &amp;lt;- k_concatenate(list(x, y)) %&amp;gt;%            # (bs, 256, 256, channels*2)
        self$down1(training = training) %&amp;gt;%         # (bs, 128, 128, 64)
        self$down2(training = training) %&amp;gt;%         # (bs, 64, 64, 128)
        self$down3(training = training) %&amp;gt;%         # (bs, 32, 32, 256)
        self$zero_pad1() %&amp;gt;%                        # (bs, 34, 34, 256)
        self$conv() %&amp;gt;%                             # (bs, 31, 31, 512)
        self$batchnorm(training = training) %&amp;gt;%
        layer_activation_leaky_relu() %&amp;gt;%
        self$zero_pad2() %&amp;gt;%                        # (bs, 33, 33, 512)
        self$last()                                 # (bs, 30, 30, 1)
      x
    }
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And here’s the factored-out downsampling functionality, again providing the means to configure the number of filters.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;disc_downsample &amp;lt;- function(filters,
                            size,
                            apply_batchnorm = TRUE,
                            name = &amp;quot;disc_downsample&amp;quot;) {
  
  keras_model_custom(name = name, function(self) {
    
    self$apply_batchnorm &amp;lt;- apply_batchnorm
    self$conv1 &amp;lt;- layer_conv_2d(
      filters = filters,
      kernel_size = size,
      strides = 2,
      padding = &amp;#39;same&amp;#39;,
      kernel_initializer = initializer_random_normal(0, 0.2),
      use_bias = FALSE
    )
    if (self$apply_batchnorm) {
      self$batchnorm &amp;lt;- layer_batch_normalization()
    }
    
    function(x, mask = NULL, training = TRUE) {
      x &amp;lt;- self$conv1(x)
      if (self$apply_batchnorm) {
        x %&amp;gt;% self$batchnorm(training = training)
      }
      x %&amp;gt;% layer_activation_leaky_relu()
    }
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="losses-and-optimizer"&gt;Losses and optimizer&lt;/h3&gt;
&lt;p&gt;As we said in the introduction, the idea of a GAN is to have the network learn the cost function. More concretely, the thing it should learn is the balance between two losses, the generator loss and the discriminator loss. Each of them individually, of course, has to be provided with a loss function, so there are still decisions to be made.&lt;/p&gt;
&lt;p&gt;For the generator, two things factor into the loss: First, does the discriminator debunk my creations as fake? Second, how big is the absolute deviation of the generated image from the target? The latter factor does not have to be present in a conditional GAN, but was included by the authors to further encourage proximity to the target, and empirically found to deliver better results.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;lambda &amp;lt;- 100 # value chosen by the authors of the paper
generator_loss &amp;lt;- function(disc_judgment, generated_output, target) {
    gan_loss &amp;lt;- tf$losses$sigmoid_cross_entropy(
      tf$ones_like(disc_judgment),
      disc_judgment
    )
    l1_loss &amp;lt;- tf$reduce_mean(tf$abs(target - generated_output))
    gan_loss + (lambda * l1_loss)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The discriminator loss looks as in a standard (un-conditional) GAN. Its first component is determined by how accurately it classifies real images as real, while the second depends on its competence in judging fake images as fake.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;discriminator_loss &amp;lt;- function(real_output, generated_output) {
  real_loss &amp;lt;- tf$losses$sigmoid_cross_entropy(
    multi_class_labels = tf$ones_like(real_output),
    logits = real_output
  )
  generated_loss &amp;lt;- tf$losses$sigmoid_cross_entropy(
    multi_class_labels = tf$zeros_like(generated_output),
    logits = generated_output
  )
  real_loss + generated_loss
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For optimization, we rely on Adam for both the generator and the discriminator.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;discriminator_optimizer &amp;lt;- tf$train$AdamOptimizer(2e-4, beta1 = 0.5)
generator_optimizer &amp;lt;- tf$train$AdamOptimizer(2e-4, beta1 = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="the-game"&gt;The game&lt;/h2&gt;
&lt;p&gt;We’re ready to have the generator and the discriminator play the game! Below, we use &lt;a href="https://www.tensorflow.org/api_docs/python/tf/contrib/eager/defun"&gt;defun&lt;/a&gt; to compile the respective R functions into TensorFlow graphs, to speed up computations.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;generator &amp;lt;- generator()
discriminator &amp;lt;- discriminator()

generator$call = tf$contrib$eager$defun(generator$call)
discriminator$call = tf$contrib$eager$defun(discriminator$call)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We also create a &lt;code&gt;tf$train$Checkpoint&lt;/code&gt; object that will allow us to save and restore training weights.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;checkpoint_dir &amp;lt;- &amp;quot;./checkpoints_pix2pix&amp;quot;
checkpoint_prefix &amp;lt;- file.path(checkpoint_dir, &amp;quot;ckpt&amp;quot;)
checkpoint &amp;lt;- tf$train$Checkpoint(
    generator_optimizer = generator_optimizer,
    discriminator_optimizer = discriminator_optimizer,
    generator = generator,
    discriminator = discriminator
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Training is a loop over epochs with an inner loop over batches yielded by the dataset. As usual with eager execution, &lt;code&gt;tf$GradientTape&lt;/code&gt; takes care of recording the forward pass and determining the gradients, while the optimizer - there are two of them in this setup - adjusts the networks’ weights.&lt;/p&gt;
&lt;p&gt;Every tenth epoch, we save the weights, and tell the generator to have a go at the first example of the test set, so we can monitor network progress. See &lt;code&gt;generate_images&lt;/code&gt; in the companion code for this functionality.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train &amp;lt;- function(dataset, num_epochs) {
  
  for (epoch in 1:num_epochs) {
    total_loss_gen &amp;lt;- 0
    total_loss_disc &amp;lt;- 0
    iter &amp;lt;- make_iterator_one_shot(train_dataset)
    
    until_out_of_range({
      batch &amp;lt;- iterator_get_next(iter)
      input_image &amp;lt;- batch[[1]]
      target &amp;lt;- batch[[2]]
      
      with(tf$GradientTape() %as% gen_tape, {
        with(tf$GradientTape() %as% disc_tape, {
          
          gen_output &amp;lt;- generator(input_image, training = TRUE)
          disc_real_output &amp;lt;-
            discriminator(input_image, target, training = TRUE)
          disc_generated_output &amp;lt;-
            discriminator(input_image, gen_output, training = TRUE)
          gen_loss &amp;lt;-
            generator_loss(disc_generated_output, gen_output, target)
          disc_loss &amp;lt;-
            discriminator_loss(disc_real_output, disc_generated_output)
          total_loss_gen &amp;lt;- total_loss_gen + gen_loss
          total_loss_disc &amp;lt;- total_loss_disc + disc_loss
        })
      })
      
      generator_gradients &amp;lt;- gen_tape$gradient(gen_loss,
                                               generator$variables)
      discriminator_gradients &amp;lt;- disc_tape$gradient(disc_loss,
                                                    discriminator$variables)
      
      generator_optimizer$apply_gradients(transpose(list(
        generator_gradients,
        generator$variables
      )))
      discriminator_optimizer$apply_gradients(transpose(
        list(discriminator_gradients,
             discriminator$variables)
      ))
      
    })
    
    cat(&amp;quot;Epoch &amp;quot;, epoch, &amp;quot;\n&amp;quot;)
    cat(&amp;quot;Generator loss: &amp;quot;,
        total_loss_gen$numpy() / batches_per_epoch,
        &amp;quot;\n&amp;quot;)
    cat(&amp;quot;Discriminator loss: &amp;quot;,
        total_loss_disc$numpy() / batches_per_epoch,
        &amp;quot;\n\n&amp;quot;)
    
    if (epoch %% 10 == 0) {
      test_iter &amp;lt;- make_iterator_one_shot(test_dataset)
      batch &amp;lt;- iterator_get_next(test_iter)
      input &amp;lt;- batch[[1]]
      target &amp;lt;- batch[[2]]
      generate_images(generator, input, target, paste0(&amp;quot;epoch_&amp;quot;, i))
    }
    
    if (epoch %% 10 == 0) {
      checkpoint$save(file_prefix = checkpoint_prefix)
    }
  }
}

if (!restore) {
  train(train_dataset, 200)
} &lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="the-results"&gt;The results&lt;/h2&gt;
&lt;p&gt;What has the network learned?&lt;/p&gt;
&lt;p&gt;Here’s a pretty typical result from the test set. It doesn’t look so bad.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-09-20-eager-pix2pix/images/pix2pix_test_10.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;p&gt;Here’s another one. Interestingly, the colors used in the fake image match the previous one’s pretty well, even though we used an additional L1 loss to penalize deviations from the original.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-09-20-eager-pix2pix/images/pix2pix_test_32.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;p&gt;This pick from the test set again shows similar hues, and it might already convey an impression one gets when going through the complete test set: The network has not just learned some balance between creatively turning a coarse mask into a detailed image on the one hand, and reproducing a concrete example on the other hand. It also has internalized the main architectural style present in the dataset.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-09-20-eager-pix2pix/images/pix2pix_test_82.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;p&gt;For an extreme example, take this. The mask leaves an enormous lot of freedom, while the target image is a pretty untypical (perhaps the most untypical) pick from the test set. The outcome is a structure that could represent a building, or part of a building, of specific texture and color shades.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-09-20-eager-pix2pix/images/pix2pix_test_92.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;When we say the network has internalized the dominant style of the training set, is this a bad thing? (We’re used to thinking in terms of overfitting on the training set.)&lt;/p&gt;
&lt;p&gt;With GANs though, one could say it all depends on the purpose. If it doesn’t fit our purpose, one thing we could try is training on several datasets at the same time.&lt;/p&gt;
&lt;p&gt;Again depending on what we want to achieve, another weakness could be the lack of stochasticity in the model, as stated by the authors of the paper themselves. This will be hard to avoid when working with paired datasets as the ones used in &lt;em&gt;pix2pix&lt;/em&gt;. An interesting alternative is CycleGAN&lt;span class="citation"&gt;(Zhu et al. 2017)&lt;/span&gt; that lets you transfer style between complete datasets without using paired instances:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-09-20-eager-pix2pix/images/cyclegan.png" class="external" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Figure from &lt;span class="citation"&gt;Zhu et al. (2017)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Finally closing on a more technical note, you may have noticed the prominent checkerboard effects in the above fake examples. This phenomenon (and ways to address it) is superbly explained in a 2016 article on &lt;a href="https://distill.pub/"&gt;distill.pub&lt;/a&gt; &lt;span class="citation"&gt;(Odena, Dumoulin, and Olah 2016)&lt;/span&gt;. In our case, it will mostly be due to the use of &lt;code&gt;layer_conv_2d_transpose&lt;/code&gt; for upsampling.&lt;/p&gt;
&lt;p&gt;As per the authors &lt;span class="citation"&gt;(Odena, Dumoulin, and Olah 2016)&lt;/span&gt;, a better alternative is upsizing followed by padding and (standard) convolution. If you’re interested, it should be straightforward to modify the example code to use &lt;code&gt;tf$image$resize_images&lt;/code&gt; (using &lt;code&gt;ResizeMethod.NEAREST_NEIGHBOR&lt;/code&gt; as recommended by the authors), &lt;code&gt;tf$pad&lt;/code&gt; and &lt;code&gt;layer_conv2d&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-IsolaZZE16"&gt;
&lt;p&gt;Isola, Phillip, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. 2016. “Image-to-Image Translation with Conditional Adversarial Networks.” &lt;em&gt;CoRR&lt;/em&gt; abs/1611.07004. &lt;a href="http://arxiv.org/abs/1611.07004"&gt;http://arxiv.org/abs/1611.07004&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-odena2016deconvolution"&gt;
&lt;p&gt;Odena, Augustus, Vincent Dumoulin, and Chris Olah. 2016. “Deconvolution and Checkerboard Artifacts.” &lt;em&gt;Distill&lt;/em&gt;. &lt;a href="https://doi.org/10.23915/distill.00003"&gt;https://doi.org/10.23915/distill.00003&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-RonnebergerFB15"&gt;
&lt;p&gt;Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” &lt;em&gt;CoRR&lt;/em&gt; abs/1505.04597. &lt;a href="http://arxiv.org/abs/1505.04597"&gt;http://arxiv.org/abs/1505.04597&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-ZhuPIE17"&gt;
&lt;p&gt;Zhu, Jun-Yan, Taesung Park, Phillip Isola, and Alexei A. Efros. 2017. “Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks.” &lt;em&gt;CoRR&lt;/em&gt; abs/1703.10593. &lt;a href="http://arxiv.org/abs/1703.10593"&gt;http://arxiv.org/abs/1703.10593&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">baa0fcaaec0434ecfb88f539a25b26f2</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Image Recognition &amp; Image Processing</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-09-20-eager-pix2pix</guid>
      <pubDate>Thu, 20 Sep 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-09-20-eager-pix2pix/images/pix2pixlosses.png" medium="image" type="image/png" width="842" height="536"/>
    </item>
    <item>
      <title>Attention-based Image Captioning with Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning</link>
      <description>


&lt;p&gt;In image captioning, an algorithm is given an image and tasked with producing a sensible caption. It is a challenging task for several reasons, not the least being that it involves a notion of &lt;em&gt;saliency&lt;/em&gt; or &lt;em&gt;relevance&lt;/em&gt;. This is why recent deep learning approaches mostly include some “attention” mechanism (sometimes even more than one) to help focusing on relevant image features.&lt;/p&gt;
&lt;p&gt;In this post, we demonstrate a formulation of image captioning as an encoder-decoder problem, enhanced by spatial attention over image grid cells. The idea comes from a recent paper on &lt;em&gt;Neural Image Caption Generation with Visual Attention&lt;/em&gt; &lt;span class="citation"&gt;(Xu et al. 2015)&lt;/span&gt;, and employs the same kind of attention algorithm as detailed in our post on &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-07-30-attention-layer/"&gt;machine translation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We’re porting Python code from a recent &lt;a href="https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb?linkId=54343050&amp;amp;pli=1#scrollTo=io7ws3ReRPGv"&gt;Google Colaboratory notebook&lt;/a&gt;, using Keras with TensorFlow eager execution to simplify our lives.&lt;/p&gt;
&lt;h2 id="prerequisites"&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;The code shown here will work with the current CRAN versions of &lt;code&gt;tensorflow&lt;/code&gt;, &lt;code&gt;keras&lt;/code&gt;, and &lt;code&gt;tfdatasets&lt;/code&gt;. Check that you’re using at least version 1.9 of TensorFlow. If that isn’t the case, as of this writing, this&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tensorflow)
install_tensorflow()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will get you version 1.10.&lt;/p&gt;
&lt;p&gt;When loading libraries, please make sure you’re executing the first 4 lines in this exact order. We need to make sure we’re using the TensorFlow implementation of Keras (&lt;code&gt;tf.keras&lt;/code&gt; in Python land), and we have to enable eager execution before using TensorFlow in any way.&lt;/p&gt;
&lt;p&gt;No need to copy-paste any code snippets - you’ll find the complete code (in order necessary for execution) here: &lt;a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/eager-image-captioning.R"&gt;eager-image-captioning.R&lt;/a&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
use_implementation(&amp;quot;tensorflow&amp;quot;)

library(tensorflow)
tfe_enable_eager_execution(device_policy = &amp;quot;silent&amp;quot;)

np &amp;lt;- import(&amp;quot;numpy&amp;quot;)

library(tfdatasets)
library(purrr)
library(stringr)
library(glue)
library(rjson)
library(rlang)
library(dplyr)
library(magick)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="the-dataset"&gt;The dataset&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://cocodataset.org"&gt;MS-COCO&lt;/a&gt; (“Common Objects in Context”) is one of, perhaps &lt;em&gt;the&lt;/em&gt;, reference dataset in image captioning (object detection and segmentation, too). We’ll be using the &lt;a href="http://images.cocodataset.org/zips/train2014.zip"&gt;training images&lt;/a&gt; and &lt;a href="http://images.cocodataset.org/annotations/annotations_trainval2014.zip"&gt;annotations&lt;/a&gt; from 2014 - be warned, depending on your location, the download can take a &lt;em&gt;long&lt;/em&gt; time.&lt;/p&gt;
&lt;p&gt;After unpacking, let’s define where the images and captions are.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;annotation_file &amp;lt;- &amp;quot;train2014/annotations/captions_train2014.json&amp;quot;
image_path &amp;lt;- &amp;quot;train2014/train2014&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The annotations are in JSON format, and there are 414113 of them! Luckily for us we didn’t have to download that many images - every image comes with 5 different captions, for better generalizability.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;annotations &amp;lt;- fromJSON(file = annotation_file)
annot_captions &amp;lt;- annotations[[4]]

num_captions &amp;lt;- length(annot_captions)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We store both annotations and image paths in lists, for later loading.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;all_captions &amp;lt;- vector(mode = &amp;quot;list&amp;quot;, length = num_captions)
all_img_names &amp;lt;- vector(mode = &amp;quot;list&amp;quot;, length = num_captions)

for (i in seq_len(num_captions)) {
  caption &amp;lt;- paste0(&amp;quot;&amp;lt;start&amp;gt; &amp;quot;,
                    annot_captions[[i]][[&amp;quot;caption&amp;quot;]],
                    &amp;quot; &amp;lt;end&amp;gt;&amp;quot;
                    )
  image_id &amp;lt;- annot_captions[[i]][[&amp;quot;image_id&amp;quot;]]
  full_coco_image_path &amp;lt;- sprintf(
    &amp;quot;%s/COCO_train2014_%012d.jpg&amp;quot;,
    image_path,
    image_id
  )
  all_img_names[[i]] &amp;lt;- full_coco_image_path
  all_captions[[i]] &amp;lt;- caption
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Depending on your computing environment, you will for sure want to restrict the number of examples used. This post will use 30000 captioned images, chosen randomly, and set aside 20% for validation.&lt;/p&gt;
&lt;p&gt;Below, we take random samples, split into training and validation parts. The companion code will also store the indices on disk, so you can pick up on verification and analysis later.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;num_examples &amp;lt;- 30000

random_sample &amp;lt;- sample(1:num_captions, size = num_examples)
train_indices &amp;lt;- sample(random_sample, size = length(random_sample) * 0.8)
validation_indices &amp;lt;- setdiff(random_sample, train_indices)

sample_captions &amp;lt;- all_captions[random_sample]
sample_images &amp;lt;- all_img_names[random_sample]
train_captions &amp;lt;- all_captions[train_indices]
train_images &amp;lt;- all_img_names[train_indices]
validation_captions &amp;lt;- all_captions[validation_indices]
validation_images &amp;lt;- all_img_names[validation_indices]&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="interlude"&gt;Interlude&lt;/h2&gt;
&lt;p&gt;Before really diving into the technical stuff, let’s take a moment to reflect on this task. In typical image-related deep learning walk-throughs, we’re used to seeing well-defined problems - even if in some cases, the solution may be hard. Take, for example, the stereotypical &lt;em&gt;dog vs. cat&lt;/em&gt; problem. Some dogs may look like cats and some cats may look like dogs, but that’s about it: All in all, in the usual world we live in, it should be a more or less binary question.&lt;/p&gt;
&lt;p&gt;If, on the other hand, we ask people to describe what they see in a scene, it’s to be expected from the outset that we’ll get different answers. Still, how much consensus there is will very much depend on the concrete dataset we’re using.&lt;/p&gt;
&lt;p&gt;Let’s take a look at some picks from the very first 20 training items sampled randomly above.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-09-17-eager-captioning/images/COCO_train2014_000000510592.jpg" class="external" style="width:50.0%" alt="" /&gt;
&lt;p class="caption"&gt;Figure from MS-COCO 2014&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now this image does not leave much room for decision what to focus on, and received a very factual caption indeed: “There is a plate with one slice of bacon a half of orange and bread”. If the dataset were all like this, we’d think a machine learning algorithm should do pretty well here.&lt;/p&gt;
&lt;p&gt;Picking another one from the first 20:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-09-17-eager-captioning/images/COCO_train2014_000000118167.jpg" class="external" style="width:50.0%" alt="" /&gt;
&lt;p class="caption"&gt;Figure from MS-COCO 2014&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;What would be salient information to you here? The caption provided goes “A smiling little boy has a checkered shirt”. Is the look of the shirt as important as that? You might as well focus on the scenery, - or even something on a completely different level: The age of the photo, or it being an analog one.&lt;/p&gt;
&lt;p&gt;Let’s take a final example.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-09-17-eager-captioning/images/COCO_train2014_000000089158.jpg" class="external" style="width:70.0%" alt="" /&gt;
&lt;p class="caption"&gt;From MS-COCO 2014&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;What would you say about this scene? The official label we sampled here is “A group of people posing in a funny way for the camera”. Well …&lt;/p&gt;
&lt;p&gt;Please don’t forget that for each image, the dataset includes five different captions (although our n = 30000 samples probably won’t). So this is not saying the dataset is biased - not at all. Instead, we want to point out the ambiguities and difficulties inherent in the task. Actually, given those difficulties, it’s all the more amazing that the task we’re tackling here - having a network automatically generate image captions - should be possible at all!&lt;/p&gt;
&lt;p&gt;Now let’s see how we can do this.&lt;/p&gt;
&lt;h2 id="extract-image-features"&gt;Extract image features&lt;/h2&gt;
&lt;p&gt;For the encoding part of our encoder-decoder network, we will make use of &lt;em&gt;InceptionV3&lt;/em&gt; to extract image features. In principle, which features to extract is up to experimentation, - here we just use the last layer before the fully connected top:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;image_model &amp;lt;- application_inception_v3(
  include_top = FALSE,
  weights = &amp;quot;imagenet&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For an image size of 299x299, the output will be of size &lt;code&gt;(batch_size, 8, 8, 2048)&lt;/code&gt;, that is, we are making use of 2048 feature maps.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;InceptionV3&lt;/em&gt; being a “big model”, where every pass through the model takes time, we want to precompute features in advance and store them on disk. We’ll use &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/introduction.html"&gt;tfdatasets&lt;/a&gt; to stream images to the model. This means all our preprocessing has to employ tensorflow functions: That’s why we’re not using the more familiar &lt;code&gt;image_load&lt;/code&gt; from keras below.&lt;/p&gt;
&lt;p&gt;Our custom &lt;code&gt;load_image&lt;/code&gt; will read in, resize and preprocess the images as required for use with &lt;em&gt;InceptionV3&lt;/em&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;load_image &amp;lt;- function(image_path) {
  img &amp;lt;-
    tf$read_file(image_path) %&amp;gt;%
    tf$image$decode_jpeg(channels = 3) %&amp;gt;%
    tf$image$resize_images(c(299L, 299L)) %&amp;gt;%
    tf$keras$applications$inception_v3$preprocess_input()
  list(img, image_path)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’re ready to save the extracted features to disk. The &lt;code&gt;(batch_size, 8, 8, 2048)&lt;/code&gt;-sized features will be flattened to &lt;code&gt;(batch_size, 64, 2048)&lt;/code&gt;. The latter shape is what our encoder, soon to be discussed, will receive as input.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;preencode &amp;lt;- unique(sample_images) %&amp;gt;% unlist() %&amp;gt;% sort()
num_unique &amp;lt;- length(preencode)

# adapt this according to your system&amp;#39;s capacities  
batch_size_4save &amp;lt;- 1
image_dataset &amp;lt;-
  tensor_slices_dataset(preencode) %&amp;gt;%
  dataset_map(load_image) %&amp;gt;%
  dataset_batch(batch_size_4save)
  
save_iter &amp;lt;- make_iterator_one_shot(image_dataset)
  
until_out_of_range({
  
  save_count &amp;lt;- save_count + batch_size_4save
  batch_4save &amp;lt;- save_iter$get_next()
  img &amp;lt;- batch_4save[[1]]
  path &amp;lt;- batch_4save[[2]]
  batch_features &amp;lt;- image_model(img)
  batch_features &amp;lt;- tf$reshape(
    batch_features,
    list(dim(batch_features)[1], -1L, dim(batch_features)[4]
  )
                               )
  for (i in 1:dim(batch_features)[1]) {
    np$save(path[i]$numpy()$decode(&amp;quot;utf-8&amp;quot;),
            batch_features[i, , ]$numpy())
  }
    
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we get to the encoder and decoder models though, we need to take care of the captions.&lt;/p&gt;
&lt;h2 id="processing-the-captions"&gt;Processing the captions&lt;/h2&gt;
&lt;p&gt;We’re using keras &lt;code&gt;text_tokenizer&lt;/code&gt; and the text processing functions &lt;code&gt;texts_to_sequences&lt;/code&gt; and &lt;code&gt;pad_sequences&lt;/code&gt; to transform ascii text into a matrix.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# we will use the 5000 most frequent words only
top_k &amp;lt;- 5000
tokenizer &amp;lt;- text_tokenizer(
  num_words = top_k,
  oov_token = &amp;quot;&amp;lt;unk&amp;gt;&amp;quot;,
  filters = &amp;#39;!&amp;quot;#$%&amp;amp;()*+.,-/:;=?@[\\]^_`{|}~ &amp;#39;)
tokenizer$fit_on_texts(sample_captions)

train_captions_tokenized &amp;lt;-
  tokenizer %&amp;gt;% texts_to_sequences(train_captions)
validation_captions_tokenized &amp;lt;-
  tokenizer %&amp;gt;% texts_to_sequences(validation_captions)

# pad_sequences will use 0 to pad all captions to the same length
tokenizer$word_index[&amp;quot;&amp;lt;pad&amp;gt;&amp;quot;] &amp;lt;- 0

# create a lookup dataframe that allows us to go in both directions
word_index_df &amp;lt;- data.frame(
  word = tokenizer$word_index %&amp;gt;% names(),
  index = tokenizer$word_index %&amp;gt;% unlist(use.names = FALSE),
  stringsAsFactors = FALSE
)
word_index_df &amp;lt;- word_index_df %&amp;gt;% arrange(index)

decode_caption &amp;lt;- function(text) {
  paste(map(text, function(number)
    word_index_df %&amp;gt;%
      filter(index == number) %&amp;gt;%
      select(word) %&amp;gt;%
      pull()),
    collapse = &amp;quot; &amp;quot;)
}

# pad all sequences to the same length (the maximum length, in our case)
# could experiment with shorter padding (truncating the very longest captions)
caption_lengths &amp;lt;- map(
  all_captions[1:num_examples],
  function(c) str_split(c,&amp;quot; &amp;quot;)[[1]] %&amp;gt;% length()
  ) %&amp;gt;% unlist()
max_length &amp;lt;- fivenum(caption_lengths)[5]

train_captions_padded &amp;lt;-  pad_sequences(
  train_captions_tokenized,
  maxlen = max_length,
  padding = &amp;quot;post&amp;quot;,
  truncating = &amp;quot;post&amp;quot;
)

validation_captions_padded &amp;lt;- pad_sequences(
  validation_captions_tokenized,
  maxlen = max_length,
  padding = &amp;quot;post&amp;quot;,
  truncating = &amp;quot;post&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="loading-the-data-for-training"&gt;Loading the data for training&lt;/h2&gt;
&lt;p&gt;Now that we’ve taken care of pre-extracting the features and preprocessing the captions, we need a way to stream them to our captioning model. For that, we’re using &lt;code&gt;tensor_slices_dataset&lt;/code&gt; from &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/introduction.html"&gt;tfdatasets&lt;/a&gt;, passing in the list of paths to the images and the preprocessed captions. Loading the images is then performed as a TensorFlow graph operation (using &lt;a href="https://www.tensorflow.org/api_docs/python/tf/py_func"&gt;tf$pyfunc&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The original Colab code also shuffles the data on every iteration. Depending on your hardware, this may take a long time, and given the size of the dataset it is not strictly necessary to get reasonable results. (The results reported below were obtained without shuffling.)&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;batch_size &amp;lt;- 10
buffer_size &amp;lt;- num_examples

map_func &amp;lt;- function(img_name, cap) {
  p &amp;lt;- paste0(img_name$decode(&amp;quot;utf-8&amp;quot;), &amp;quot;.npy&amp;quot;)
  img_tensor &amp;lt;- np$load(p)
  img_tensor &amp;lt;- tf$cast(img_tensor, tf$float32)
  list(img_tensor, cap)
}

train_dataset &amp;lt;-
  tensor_slices_dataset(list(train_images, train_captions_padded)) %&amp;gt;%
  dataset_map(
    function(item1, item2) tf$py_func(map_func, list(item1, item2), list(tf$float32, tf$int32))
  ) %&amp;gt;%
  # optionally shuffle the dataset
  # dataset_shuffle(buffer_size) %&amp;gt;%
  dataset_batch(batch_size)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="captioning-model"&gt;Captioning model&lt;/h2&gt;
&lt;p&gt;The model is basically the same as that discussed in the &lt;a href="https://blogs.rstudio.com/tensorflow/posts/2018-07-30-attention-layer/"&gt;machine translation post&lt;/a&gt;. Please refer to that article for an explanation of the concepts, as well as a detailed walk-through of the tensor shapes involved at every step. Here, we provide the tensor shapes as comments in the code snippets, for quick overview/comparison.&lt;/p&gt;
&lt;p&gt;However, if you develop your own models, with eager execution you can simply insert debugging/logging statements at arbitrary places in the code - even in model definitions. So you can have a function&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;maybecat &amp;lt;- function(context, x) {
  if (debugshapes) {
    name &amp;lt;- enexpr(x)
    dims &amp;lt;- paste0(dim(x), collapse = &amp;quot; &amp;quot;)
    cat(context, &amp;quot;: shape of &amp;quot;, name, &amp;quot;: &amp;quot;, dims, &amp;quot;\n&amp;quot;, sep = &amp;quot;&amp;quot;)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And if you now set&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;debugshapes &amp;lt;- FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;you can trace - not only tensor shapes, but actual tensor values through your models, as shown below for the encoder. (We don’t display any debugging statements after that, but the &lt;a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/eager-image-captioning.R"&gt;sample code&lt;/a&gt; has many more.)&lt;/p&gt;
&lt;h3 id="encoder"&gt;Encoder&lt;/h3&gt;
&lt;p&gt;Now it’s time to define some some sizing-related hyperparameters and housekeeping variables:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# for encoder output
embedding_dim &amp;lt;- 256
# decoder (LSTM) capacity
gru_units &amp;lt;- 512
# for decoder output
vocab_size &amp;lt;- top_k
# number of feature maps gotten from Inception V3
features_shape &amp;lt;- 2048
# shape of attention features (flattened from 8x8)
attention_features_shape &amp;lt;- 64&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The encoder in this case is just a fully connected layer, taking in the features extracted from Inception V3 (in flattened form, as they were written to disk), and embedding them in 256-dimensional space.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;cnn_encoder &amp;lt;- function(embedding_dim, name = NULL) {
    
  keras_model_custom(name = name, function(self) {
      
    self$fc &amp;lt;- layer_dense(units = embedding_dim, activation = &amp;quot;relu&amp;quot;)
      
    function(x, mask = NULL) {
      # input shape: (batch_size, 64, features_shape)
      maybecat(&amp;quot;encoder input&amp;quot;, x)
      # shape after fc: (batch_size, 64, embedding_dim)
      x &amp;lt;- self$fc(x)
      maybecat(&amp;quot;encoder output&amp;quot;, x)
      x
    }
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="attention-module"&gt;Attention module&lt;/h3&gt;
&lt;p&gt;Unlike in the machine translation post, here the attention module is separated out into its own custom model. The logic is the same though:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;attention_module &amp;lt;- function(gru_units, name = NULL) {
  
  keras_model_custom(name = name, function(self) {
    
    self$W1 = layer_dense(units = gru_units)
    self$W2 = layer_dense(units = gru_units)
    self$V = layer_dense(units = 1)
      
    function(inputs, mask = NULL) {
      features &amp;lt;- inputs[[1]]
      hidden &amp;lt;- inputs[[2]]
      # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)
      # hidden shape == (batch_size, gru_units)
      # hidden_with_time_axis shape == (batch_size, 1, gru_units)
      hidden_with_time_axis &amp;lt;- k_expand_dims(hidden, axis = 2)
        
      # score shape == (batch_size, 64, 1)
      score &amp;lt;- self$V(k_tanh(self$W1(features) + self$W2(hidden_with_time_axis)))
      # attention_weights shape == (batch_size, 64, 1)
      attention_weights &amp;lt;- k_softmax(score, axis = 2)
      # context_vector shape after sum == (batch_size, embedding_dim)
      context_vector &amp;lt;- k_sum(attention_weights * features, axis = 2)
        
      list(context_vector, attention_weights)
    }
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="decoder"&gt;Decoder&lt;/h3&gt;
&lt;p&gt;The decoder at each time step calls the attention module with the features it got from the encoder and its last hidden state, and receives back an attention vector. The attention vector gets concatenated with the current input and further processed by a GRU and two fully connected layers, the last of which gives us the (unnormalized) probabilities for the next word in the caption.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;current input&lt;/em&gt; at each time step here is the previous word: the correct one during training (&lt;em&gt;teacher forcing&lt;/em&gt;), the last generated one during inference.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;rnn_decoder &amp;lt;- function(embedding_dim, gru_units, vocab_size, name = NULL) {
    
  keras_model_custom(name = name, function(self) {
      
    self$gru_units &amp;lt;- gru_units
    self$embedding &amp;lt;- layer_embedding(input_dim = vocab_size, 
                                      output_dim = embedding_dim)
    self$gru &amp;lt;- if (tf$test$is_gpu_available()) {
      layer_cudnn_gru(
        units = gru_units,
        return_sequences = TRUE,
        return_state = TRUE,
        recurrent_initializer = &amp;#39;glorot_uniform&amp;#39;
      )
    } else {
      layer_gru(
        units = gru_units,
        return_sequences = TRUE,
        return_state = TRUE,
        recurrent_initializer = &amp;#39;glorot_uniform&amp;#39;
      )
    }
      
    self$fc1 &amp;lt;- layer_dense(units = self$gru_units)
    self$fc2 &amp;lt;- layer_dense(units = vocab_size)
      
    self$attention &amp;lt;- attention_module(self$gru_units)
      
    function(inputs, mask = NULL) {
      x &amp;lt;- inputs[[1]]
      features &amp;lt;- inputs[[2]]
      hidden &amp;lt;- inputs[[3]]
        
      c(context_vector, attention_weights) %&amp;lt;-% 
        self$attention(list(features, hidden))
        
      # x shape after passing through embedding == (batch_size, 1, embedding_dim)
      x &amp;lt;- self$embedding(x)
        
      # x shape after concatenation == (batch_size, 1, 2 * embedding_dim)
      x &amp;lt;- k_concatenate(list(k_expand_dims(context_vector, 2), x))
        
      # passing the concatenated vector to the GRU
      c(output, state) %&amp;lt;-% self$gru(x)
        
      # shape == (batch_size, 1, gru_units)
      x &amp;lt;- self$fc1(output)
        
      # x shape == (batch_size, gru_units)
      x &amp;lt;- k_reshape(x, c(-1, dim(x)[[3]]))
        
      # output shape == (batch_size, vocab_size)
      x &amp;lt;- self$fc2(x)
        
      list(x, state, attention_weights)
        
    }
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="loss-function-and-instantiating-it-all"&gt;Loss function, and instantiating it all&lt;/h3&gt;
&lt;p&gt;Now that we’ve defined our model (built of three custom models), we still need to actually instantiate it (being precise: the two classes we will access from outside, that is, the encoder and the decoder).&lt;/p&gt;
&lt;p&gt;We also need to instantiate an optimizer (Adam will do), and define our loss function (categorical crossentropy). Note that &lt;code&gt;tf$nn$sparse_softmax_cross_entropy_with_logits&lt;/code&gt; expects raw logits instead of softmax activations, and that we’re using the &lt;em&gt;sparse&lt;/em&gt; variant because our labels are not one-hot-encoded.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;encoder &amp;lt;- cnn_encoder(embedding_dim)
decoder &amp;lt;- rnn_decoder(embedding_dim, gru_units, vocab_size)

optimizer = tf$train$AdamOptimizer()

cx_loss &amp;lt;- function(y_true, y_pred) {
  mask &amp;lt;- 1 - k_cast(y_true == 0L, dtype = &amp;quot;float32&amp;quot;)
  loss &amp;lt;- tf$nn$sparse_softmax_cross_entropy_with_logits(
    labels = y_true,
    logits = y_pred
  ) * mask
  tf$reduce_mean(loss)
}&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="training"&gt;Training&lt;/h2&gt;
&lt;p&gt;Training the captioning model is a time-consuming process, and you will for sure want to save the model’s weights! How does this work with eager execution?&lt;/p&gt;
&lt;p&gt;We create a &lt;code&gt;tf$train$Checkpoint&lt;/code&gt; object, passing it the objects to be saved: In our case, the encoder, the decoder, and the optimizer. Later, at the end of each epoch, we will ask it to write the respective weights to disk.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;restore_checkpoint &amp;lt;- FALSE

checkpoint_dir &amp;lt;- &amp;quot;./checkpoints_captions&amp;quot;
checkpoint_prefix &amp;lt;- file.path(checkpoint_dir, &amp;quot;ckpt&amp;quot;)
checkpoint &amp;lt;- tf$train$Checkpoint(
  optimizer = optimizer,
  encoder = encoder,
  decoder = decoder
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we’re just starting to train the model, &lt;code&gt;restore_checkpoint&lt;/code&gt; is set to false. Later, restoring the weights will be as easy as&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;if (restore_checkpoint) {
  checkpoint$restore(tf$train$latest_checkpoint(checkpoint_dir))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The training loop is structured just like in the machine translation case: We loop over epochs, batches, and the training targets, feeding in the correct previous word at every timestep. Again, &lt;code&gt;tf$GradientTape&lt;/code&gt; takes care of recording the forward pass and calculating the gradients, and the optimizer applies the gradients to the model’s weights. As each epoch ends, we also save the weights.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;num_epochs &amp;lt;- 20

if (!restore_checkpoint) {
  for (epoch in seq_len(num_epochs)) {
    
    total_loss &amp;lt;- 0
    progress &amp;lt;- 0
    train_iter &amp;lt;- make_iterator_one_shot(train_dataset)
    
    until_out_of_range({
      
      batch &amp;lt;- iterator_get_next(train_iter)
      loss &amp;lt;- 0
      img_tensor &amp;lt;- batch[[1]]
      target_caption &amp;lt;- batch[[2]]
      
      dec_hidden &amp;lt;- k_zeros(c(batch_size, gru_units))
      
      dec_input &amp;lt;- k_expand_dims(
        rep(list(word_index_df[word_index_df$word == &amp;quot;&amp;lt;start&amp;gt;&amp;quot;, &amp;quot;index&amp;quot;]), 
            batch_size)
      )
      
      with(tf$GradientTape() %as% tape, {
        
        features &amp;lt;- encoder(img_tensor)
        
        for (t in seq_len(dim(target_caption)[2] - 1)) {
          c(preds, dec_hidden, weights) %&amp;lt;-%
            decoder(list(dec_input, features, dec_hidden))
          loss &amp;lt;- loss + cx_loss(target_caption[, t], preds)
          dec_input &amp;lt;- k_expand_dims(target_caption[, t])
        }
        
      })
      
      total_loss &amp;lt;-
        total_loss + loss / k_cast_to_floatx(dim(target_caption)[2])
      
      variables &amp;lt;- c(encoder$variables, decoder$variables)
      gradients &amp;lt;- tape$gradient(loss, variables)
      
      optimizer$apply_gradients(purrr::transpose(list(gradients, variables)),
                                global_step = tf$train$get_or_create_global_step()
      )
    })
    cat(paste0(
      &amp;quot;\n\nTotal loss (epoch): &amp;quot;,
      epoch,
      &amp;quot;: &amp;quot;,
      (total_loss / k_cast_to_floatx(buffer_size)) %&amp;gt;% as.double() %&amp;gt;% round(4),
      &amp;quot;\n&amp;quot;
    ))
    
    checkpoint$save(file_prefix = checkpoint_prefix)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="peeking-at-results"&gt;Peeking at results&lt;/h2&gt;
&lt;p&gt;Just like in the translation case, it’s interesting to look at model performance during training. The companion code has that functionality integrated, so you can watch model progress for yourself.&lt;/p&gt;
&lt;p&gt;The basic function here is &lt;code&gt;get_caption&lt;/code&gt;: It gets passed the path to an image, loads it, obtains its features from Inception V3, and then asks the encoder-decoder model to generate a caption. If at any point the model produces the &lt;code&gt;end&lt;/code&gt; symbol, we stop early. Otherwise, we continue until we hit the predefined maximum length.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;get_caption &amp;lt;-
  function(image) {
    attention_matrix &amp;lt;-
      matrix(0, nrow = max_length, ncol = attention_features_shape)
    temp_input &amp;lt;- k_expand_dims(load_image(image)[[1]], 1)
    img_tensor_val &amp;lt;- image_model(temp_input)
    img_tensor_val &amp;lt;- k_reshape(
      img_tensor_val,
      list(dim(img_tensor_val)[1], -1, dim(img_tensor_val)[4])
    )
    features &amp;lt;- encoder(img_tensor_val)
    
    dec_hidden &amp;lt;- k_zeros(c(1, gru_units))
    dec_input &amp;lt;-
      k_expand_dims(
        list(word_index_df[word_index_df$word == &amp;quot;&amp;lt;start&amp;gt;&amp;quot;, &amp;quot;index&amp;quot;])
      )
    
    result &amp;lt;- &amp;quot;&amp;quot;
    
    for (t in seq_len(max_length - 1)) {
      
      c(preds, dec_hidden, attention_weights) %&amp;lt;-%
        decoder(list(dec_input, features, dec_hidden))
      attention_weights &amp;lt;- k_reshape(attention_weights, c(-1))
      attention_matrix[t,] &amp;lt;- attention_weights %&amp;gt;% as.double()
      
      pred_idx &amp;lt;- tf$multinomial(exp(preds), num_samples = 1)[1, 1] 
                    %&amp;gt;% as.double()
      pred_word &amp;lt;-
        word_index_df[word_index_df$index == pred_idx, &amp;quot;word&amp;quot;]
      
      if (pred_word == &amp;quot;&amp;lt;end&amp;gt;&amp;quot;) {
        result &amp;lt;-
          paste(result, pred_word)
        attention_matrix &amp;lt;-
          attention_matrix[1:length(str_split(result, &amp;quot; &amp;quot;)[[1]]), , 
                           drop = FALSE]
        return (list(result, attention_matrix))
      } else {
        result &amp;lt;-
          paste(result, pred_word)
        dec_input &amp;lt;- k_expand_dims(list(pred_idx))
      }
    }
    
    list(str_trim(result), attention_matrix)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With that functionality, now let’s actually do that: peek at results while the network is learning!&lt;/p&gt;
&lt;p&gt;We’ve picked 3 examples each from the training and validation sets. Here they are.&lt;/p&gt;
&lt;p&gt;First, our picks from the training set:&lt;/p&gt;
&lt;div class="l-body-outset"&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-09-17-eager-captioning/images/training_examples.png" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Three picks from the training set&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Let’s see the target captions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;a herd of giraffe standing on top of a grass covered field&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;a view of cards driving down a street&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;the skateboarding flips his board off of the sidewalk&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Interestingly, here we also have a demonstration of how labeled datasets (like anything human) may contain errors. (The samples were not picked for that; instead, they were chosen - without too much screening - for being rather unequivocal in their visual content.)&lt;/p&gt;
&lt;p&gt;Now for the validation candidates.&lt;/p&gt;
&lt;div class="l-body-outset"&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-09-17-eager-captioning/images/validation_examples.png" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Three picks from the validation set&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;and their official captions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;a left handed pitcher throwing the base ball&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;a woman taking a bite of a slice of pizza in a restaraunt&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;a woman hitting swinging a tennis racket at a tennis ball on a tennis court&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(Again, any spelling peculiarities have not been introduced by us.)&lt;/p&gt;
&lt;h4 id="epoch-1"&gt;Epoch 1&lt;/h4&gt;
&lt;p&gt;Now, what does our network produce after the first epoch? Remember that this means, having seen each one of the 24000 training images once.&lt;/p&gt;
&lt;p&gt;First then, here are the captions for the train images:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;a group of sheep standing in the grass&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;a group of cars driving down a street&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;a man is standing on a street&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Not only is the syntax correct in every case, the content isn’t that bad either!&lt;/p&gt;
&lt;p&gt;How about the validation set?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;a baseball player is playing baseball uniform is holding a baseball bat&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;a man is holding a table with a table with a table with a table with a table with a table with a table with a table with a table with a table with a table with a table with a table with a table&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;a tennis player is holding a tennis court&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This certainly tells that the network has been able to generalize over - let’s not call them concepts, but mappings between visual and textual entities, say It’s true that it will have seen some of these images before, because images come with several captions. You could be more strict setting up your training and validation sets - but here, we don’t really care about objective performance scores and so, it does not really matter.&lt;/p&gt;
&lt;p&gt;Let’ skip directly to epoch 20, our last training epoch, and check for further improvements.&lt;/p&gt;
&lt;h4 id="epoch-20"&gt;Epoch 20&lt;/h4&gt;
&lt;p&gt;This is what we get for the training images:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;a group of many tall giraffe standing next to a sheep&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;a view of cards and white gloves on a street&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;a skateboarding flips his board&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And this, for the validation images.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;a baseball catcher and umpire hit a baseball game&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;a man is eating a sandwich&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;a female tennis player is in the court&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I think we might agree that this still leaves room for improvement - but then, we only trained for 20 epochs and on a very small portion of the dataset.&lt;/p&gt;
&lt;p&gt;In the above code snippets, you may have noticed the decoder returning an &lt;code&gt;attention_matrix&lt;/code&gt; - but we weren’t commenting on it. Now finally, just as in the translation example, have a look what we can make of that.&lt;/p&gt;
&lt;h2 id="where-does-the-network-look"&gt;Where does the network look?&lt;/h2&gt;
&lt;p&gt;We can visualize where the network is “looking” as it generates each word by overlaying the original image and the attention matrix. This example is taken from the 4th epoch.&lt;/p&gt;
&lt;p&gt;Here white-ish squares indicate areas receiving stronger focus. Compared to text-to-text translation though, the mapping is inherently less straightforward - where does one “look” when producing words like “and”, “the”, or “in”?&lt;/p&gt;
&lt;div class="l-body-outset"&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-09-17-eager-captioning/images/attention.png" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Attention over image areas&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;It probably goes without saying that much better results are to be expected when training on (much!) more data and for much more time.&lt;/p&gt;
&lt;p&gt;Apart from that, there are other options, though. The concept implemented here uses &lt;em&gt;spatial&lt;/em&gt; attention over a uniform grid, that is, the attention mechanism guides the decoder &lt;em&gt;where&lt;/em&gt; on the grid to look next when generating a caption.&lt;/p&gt;
&lt;p&gt;However, this is not the only way, and this is not how it works with humans. A much more plausible approach is a mix of top-down and bottom-up attention. E.g., &lt;span class="citation"&gt;(Anderson et al. 2017)&lt;/span&gt; use object detection techniques to bottom-up isolate interesting objects, and an LSTM stack wherein the first LSTM computes top-down attention guided by the output word generated by the second one.&lt;/p&gt;
&lt;p&gt;Another interesting approach involving attention is using a multimodal attentive translator &lt;span class="citation"&gt;(Liu et al. 2017)&lt;/span&gt;, where the image features are encoded and presented in a sequence, such that we end up with sequence models both on the encoding and the decoding sides.&lt;/p&gt;
&lt;p&gt;Another alternative is to add a learned &lt;em&gt;topic&lt;/em&gt; to the information input &lt;span class="citation"&gt;(Zhu, Xue, and Yuan 2018)&lt;/span&gt;, which again is a top-down feature found in human cognition.&lt;/p&gt;
&lt;p&gt;If you find one of these, or yet another, approach more convincing, an eager execution implementation, in the style of the above, will likely be a sound way of implementing it.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-AndersonHBTJGZ17"&gt;
&lt;p&gt;Anderson, Peter, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. 2017. “Bottom-up and Top-down Attention for Image Captioning and VQA.” &lt;em&gt;CoRR&lt;/em&gt; abs/1707.07998. &lt;a href="http://arxiv.org/abs/1707.07998"&gt;http://arxiv.org/abs/1707.07998&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-LiuSWWY17"&gt;
&lt;p&gt;Liu, Chang, Fuchun Sun, Changhu Wang, Feng Wang, and Alan L. Yuille. 2017. “A Multimodal Attentive Translator for Image Captioning.” &lt;em&gt;CoRR&lt;/em&gt; abs/1702.05658. &lt;a href="http://arxiv.org/abs/1702.05658"&gt;http://arxiv.org/abs/1702.05658&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-XuBKCCSZB15"&gt;
&lt;p&gt;Xu, Kelvin, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.” &lt;em&gt;CoRR&lt;/em&gt; abs/1502.03044. &lt;a href="http://arxiv.org/abs/1502.03044"&gt;http://arxiv.org/abs/1502.03044&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Zhu2018"&gt;
&lt;p&gt;Zhu, Zhihao, Zhan Xue, and Zejian Yuan. 2018. “A Topic-Guided Attention for Image Captioning.” &lt;em&gt;CoRR&lt;/em&gt; abs/1807.03514v1. &lt;a href="https://arxiv.org/abs/1807.03514v1"&gt;https://arxiv.org/abs/1807.03514v1&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">b6140087093fcff6888b4087a90073e5</distill:md5>
      <category>Natural Language Processing</category>
      <category>TensorFlow/Keras</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning</guid>
      <pubDate>Mon, 17 Sep 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning/images/showattendandtell.png" medium="image" type="image/png" width="627" height="269"/>
    </item>
    <item>
      <title>Neural style transfer with eager execution and Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-09-10-eager-style-transfer</link>
      <description>


&lt;p&gt;How would your summer holiday’s photos look had Edvard Munch painted them? (Perhaps it’s better not to know). Let’s take a more comforting example: How would a nice, summarly river landscape look if painted by Katsushika Hokusai?&lt;/p&gt;
&lt;p&gt;Style transfer on images is not new, but got a boost when Gatys, Ecker, and Bethge&lt;span class="citation"&gt;(Gatys, Ecker, and Bethge 2015)&lt;/span&gt; showed how to successfully do it with deep learning. The main idea is straightforward: Create a hybrid that is a tradeoff between the &lt;em&gt;content image&lt;/em&gt; we want to manipulate, and a &lt;em&gt;style image&lt;/em&gt; we want to imitate, by optimizing for maximal resemblance to both at the same time.&lt;/p&gt;
&lt;p&gt;If you’ve read the chapter on neural style transfer from &lt;a href="https://tensorflow.rstudio.com/learn/resources.html"&gt;Deep Learning with R&lt;/a&gt;, you may recognize some of the code snippets that follow. However, there is an important difference: This post uses TensorFlow &lt;a href="https://www.tensorflow.org/guide/eager"&gt;Eager Execution&lt;/a&gt;, allowing for an imperative way of coding that makes it easy to map concepts to code. Just like previous posts on eager execution on this blog, this is a port of a &lt;a href="https://colab.research.google.com/github/tensorflow/models/blob/master/research/nst_blogpost/4_Neural_Style_Transfer_with_Eager_Execution.ipynb"&gt;Google Colaboratory notebook&lt;/a&gt; that performs the same task in Python.&lt;/p&gt;
&lt;p&gt;As usual, please make sure you have the required package versions installed. And no need to copy the snippets - you’ll find the complete code among the &lt;a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/eager_styletransfer.R"&gt;Keras examples&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="prerequisites"&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;The code in this post depends on the most recent versions of several of the TensorFlow R packages. You can install these packages as follows:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;install.packages(c(&amp;quot;tensorflow&amp;quot;, &amp;quot;keras&amp;quot;, &amp;quot;tfdatasets&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should also be sure that you are running the very latest version of TensorFlow (v1.10), which you can install like so:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tensorflow)
install_tensorflow()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are additional requirements for using TensorFlow eager execution. First, we need to call &lt;code&gt;tfe_enable_eager_execution()&lt;/code&gt; right at the beginning of the program. Second, we need to use the implementation of Keras included in TensorFlow, rather than the base Keras implementation.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
use_implementation(&amp;quot;tensorflow&amp;quot;)

library(tensorflow)
tfe_enable_eager_execution(device_policy = &amp;quot;silent&amp;quot;)

library(purrr)
library(glue)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Prerequisites behind us, let’s get started!&lt;/p&gt;
&lt;h2 id="input-images"&gt;Input images&lt;/h2&gt;
&lt;p&gt;Here is our content image - replace by an image of your own:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# If you have enough memory on your GPU, no need to load the images
# at such small size.
# This is the size I found working for a 4G GPU.
img_shape &amp;lt;- c(128, 128, 3)

content_path &amp;lt;- &amp;quot;isar.jpg&amp;quot;

content_image &amp;lt;-  image_load(content_path, target_size = img_shape[1:2])
content_image %&amp;gt;% 
  image_to_array() %&amp;gt;%
  `/`(., 255) %&amp;gt;%
  as.raster() %&amp;gt;%
  plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-09-10-eager-style-transfer/images/isar.jpg" style="width:60.0%" /&gt;&lt;/p&gt;
&lt;p&gt;And here’s the style model, Hokusai’s &lt;em&gt;The Great Wave off Kanagawa&lt;/em&gt;, which you can download from &lt;a href="https://upload.wikimedia.org/wikipedia/commons/0/0a/The_Great_Wave_off_Kanagawa.jpg"&gt;Wikimedia Commons&lt;/a&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;style_path &amp;lt;- &amp;quot;The_Great_Wave_off_Kanagawa.jpg&amp;quot;

style_image &amp;lt;-  image_load(content_path, target_size = img_shape[1:2])
style_image %&amp;gt;% 
  image_to_array() %&amp;gt;%
  `/`(., 255) %&amp;gt;%
  as.raster() %&amp;gt;%
  plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-09-10-eager-style-transfer/images/The_Great_Wave_off_Kanagawa.jpg" style="width:60.0%" /&gt;&lt;/p&gt;
&lt;p&gt;We create a wrapper that loads and preprocesses the input images for us. As we will be working with VGG19, a network that has been trained on ImageNet, we need to transform our input images in the same way that was used training it. Later, we’ll apply the inverse transformation to our combination image before displaying it.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;load_and_preprocess_image &amp;lt;- function(path) {
  img &amp;lt;- image_load(path, target_size = img_shape[1:2]) %&amp;gt;%
    image_to_array() %&amp;gt;%
    k_expand_dims(axis = 1) %&amp;gt;%
    imagenet_preprocess_input()
}

deprocess_image &amp;lt;- function(x) {
  x &amp;lt;- x[1, , ,]
  # Remove zero-center by mean pixel
  x[, , 1] &amp;lt;- x[, , 1] + 103.939
  x[, , 2] &amp;lt;- x[, , 2] + 116.779
  x[, , 3] &amp;lt;- x[, , 3] + 123.68
  # &amp;#39;BGR&amp;#39;-&amp;gt;&amp;#39;RGB&amp;#39;
  x &amp;lt;- x[, , c(3, 2, 1)]
  x[x &amp;gt; 255] &amp;lt;- 255
  x[x &amp;lt; 0] &amp;lt;- 0
  x[] &amp;lt;- as.integer(x) / 255
  x
}&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="setting-the-scene"&gt;Setting the scene&lt;/h2&gt;
&lt;p&gt;We are going to use a neural network, but we won’t be training it. Neural style transfer is a bit uncommon in that we don’t optimize the network’s weights, but back propagate the loss to the input layer (the image), in order to move it in the desired direction.&lt;/p&gt;
&lt;p&gt;We will be interested in two kinds of outputs from the network, corresponding to our two goals. Firstly, we want to keep the combination image similar to the content image, on a high level. In a convnet, upper layers map to more holistic concepts, so we are picking a layer high up in the graph to compare outputs from the source and the combination.&lt;/p&gt;
&lt;p&gt;Secondly, the generated image should “look like” the style image. Style corresponds to lower level features like texture, shapes, strokes… So to compare the combination against the style example, we choose a set of lower level conv blocks for comparison and aggregate the results.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;content_layers &amp;lt;- c(&amp;quot;block5_conv2&amp;quot;)
style_layers &amp;lt;- c(&amp;quot;block1_conv1&amp;quot;,
                 &amp;quot;block2_conv1&amp;quot;,
                 &amp;quot;block3_conv1&amp;quot;,
                 &amp;quot;block4_conv1&amp;quot;,
                 &amp;quot;block5_conv1&amp;quot;)

num_content_layers &amp;lt;- length(content_layers)
num_style_layers &amp;lt;- length(style_layers)

get_model &amp;lt;- function() {
  vgg &amp;lt;- application_vgg19(include_top = FALSE, weights = &amp;quot;imagenet&amp;quot;)
  vgg$trainable &amp;lt;- FALSE
  style_outputs &amp;lt;- map(style_layers, function(layer) vgg$get_layer(layer)$output)
  content_outputs &amp;lt;- map(content_layers, function(layer) vgg$get_layer(layer)$output)
  model_outputs &amp;lt;- c(style_outputs, content_outputs)
  keras_model(vgg$input, model_outputs)
}&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="losses"&gt;Losses&lt;/h2&gt;
&lt;p&gt;When optimizing the input image, we will consider three types of losses. Firstly, the &lt;em&gt;content loss&lt;/em&gt;: How different is the combination image from the source? Here, we’re using the sum of the squared errors for comparison.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;content_loss &amp;lt;- function(content_image, target) {
  k_sum(k_square(target - content_image))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our second concern is having the styles match as closely as possible. Style is commonly operationalized as the &lt;a href="http://mathworld.wolfram.com/GramMatrix.html"&gt;&lt;em&gt;Gram matrix&lt;/em&gt;&lt;/a&gt; of flattened feature maps in a layer. We thus assume that style is related to how maps in a layer correlate with other.&lt;/p&gt;
&lt;p&gt;We therefore compute the Gram matrices of the layers we’re interested in (defined above), for the source image as well as the optimization candidate, and compare them, again using the sum of squared errors.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;gram_matrix &amp;lt;- function(x) {
  features &amp;lt;- k_batch_flatten(k_permute_dimensions(x, c(3, 1, 2)))
  gram &amp;lt;- k_dot(features, k_transpose(features))
  gram
}

style_loss &amp;lt;- function(gram_target, combination) {
  gram_comb &amp;lt;- gram_matrix(combination)
  k_sum(k_square(gram_target - gram_comb)) /
    (4 * (img_shape[3] ^ 2) * (img_shape[1] * img_shape[2]) ^ 2)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thirdly, we don’t want the combination image to look overly pixelated, thus we’re adding in a regularization component, the total variation in the image:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;total_variation_loss &amp;lt;- function(image) {
  y_ij  &amp;lt;- image[1:(img_shape[1] - 1L), 1:(img_shape[2] - 1L),]
  y_i1j &amp;lt;- image[2:(img_shape[1]), 1:(img_shape[2] - 1L),]
  y_ij1 &amp;lt;- image[1:(img_shape[1] - 1L), 2:(img_shape[2]),]
  a &amp;lt;- k_square(y_ij - y_i1j)
  b &amp;lt;- k_square(y_ij - y_ij1)
  k_sum(k_pow(a + b, 1.25))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The tricky thing is how to combine these losses. We’ve reached acceptable results with the following weightings, but feel free to play around as you see fit:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;content_weight &amp;lt;- 100
style_weight &amp;lt;- 0.8
total_variation_weight &amp;lt;- 0.01&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="get-model-outputs-for-the-content-and-style-images"&gt;Get model outputs for the content and style images&lt;/h2&gt;
&lt;p&gt;We need the model’s output for the content and style images, but here it suffices to do this just once. We concatenate both images along the batch dimension, pass that input to the model, and get back a list of outputs, where every element of the list is a 4-d tensor. For the style image, we’re interested in the style outputs at batch position 1, whereas for the content image, we need the content output at batch position 2.&lt;/p&gt;
&lt;p&gt;In the below comments, please note that the sizes of dimensions 2 and 3 will differ if you’re loading images at a different size.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;get_feature_representations &amp;lt;-
  function(model, content_path, style_path) {
    
    # dim == (1, 128, 128, 3)
    style_image &amp;lt;-
      load_and_process_image(style_path) %&amp;gt;% k_cast(&amp;quot;float32&amp;quot;)
    # dim == (1, 128, 128, 3)
    content_image &amp;lt;-
      load_and_process_image(content_path) %&amp;gt;% k_cast(&amp;quot;float32&amp;quot;)
    # dim == (2, 128, 128, 3)
    stack_images &amp;lt;- k_concatenate(list(style_image, content_image), axis = 1)
    
    # length(model_outputs) == 6
    # dim(model_outputs[[1]]) = (2, 128, 128, 64)
    # dim(model_outputs[[6]]) = (2, 8, 8, 512)
    model_outputs &amp;lt;- model(stack_images)
    
    style_features &amp;lt;- 
      model_outputs[1:num_style_layers] %&amp;gt;%
      map(function(batch) batch[1, , , ])
    content_features &amp;lt;- 
      model_outputs[(num_style_layers + 1):(num_style_layers + num_content_layers)] %&amp;gt;%
      map(function(batch) batch[2, , , ])
    
    list(style_features, content_features)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="computing-the-losses"&gt;Computing the losses&lt;/h2&gt;
&lt;p&gt;On every iteration, we need to pass the combination image through the model, obtain the style and content outputs, and compute the losses. Again, the code is extensively commented with tensor sizes for easy verification, but please keep in mind that the exact numbers presuppose you’re working with 128x128 images.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;compute_loss &amp;lt;-
  function(model, loss_weights, init_image, gram_style_features, content_features) {
    
    c(style_weight, content_weight) %&amp;lt;-% loss_weights
    model_outputs &amp;lt;- model(init_image)
    style_output_features &amp;lt;- model_outputs[1:num_style_layers]
    content_output_features &amp;lt;-
      model_outputs[(num_style_layers + 1):(num_style_layers + num_content_layers)]
    
    # style loss
    weight_per_style_layer &amp;lt;- 1 / num_style_layers
    style_score &amp;lt;- 0
    # dim(style_zip[[5]][[1]]) == (512, 512)
    style_zip &amp;lt;- transpose(list(gram_style_features, style_output_features))
    for (l in 1:length(style_zip)) {
      # for l == 1:
      # dim(target_style) == (64, 64)
      # dim(comb_style) == (1, 128, 128, 64)
      c(target_style, comb_style) %&amp;lt;-% style_zip[[l]]
      style_score &amp;lt;- style_score + weight_per_style_layer * 
        style_loss(target_style, comb_style[1, , , ])
    }
    
    # content loss
    weight_per_content_layer &amp;lt;- 1 / num_content_layers
    content_score &amp;lt;- 0
    content_zip &amp;lt;- transpose(list(content_features, content_output_features))
    for (l in 1:length(content_zip)) {
      # dim(comb_content) ==  (1, 8, 8, 512)
      # dim(target_content) == (8, 8, 512)
      c(target_content, comb_content) %&amp;lt;-% content_zip[[l]]
      content_score &amp;lt;- content_score + weight_per_content_layer *
        content_loss(comb_content[1, , , ], target_content)
    }
    
    # total variation loss
    variation_loss &amp;lt;- total_variation_loss(init_image[1, , ,])
    
    style_score &amp;lt;- style_score * style_weight
    content_score &amp;lt;- content_score * content_weight
    variation_score &amp;lt;- variation_loss * total_variation_weight
    
    loss &amp;lt;- style_score + content_score + variation_score
    list(loss, style_score, content_score, variation_score)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="computing-the-gradients"&gt;Computing the gradients&lt;/h2&gt;
&lt;p&gt;As soon as we have the losses, obtaining the gradients of the overall loss with respect to the input image is just a matter of calling &lt;code&gt;tape$gradient&lt;/code&gt; on the &lt;code&gt;GradientTape&lt;/code&gt;. Note that the nested call to &lt;code&gt;compute_loss&lt;/code&gt;, and thus the call of the model on our combination image, happens inside the &lt;code&gt;GradientTape&lt;/code&gt; context.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;compute_grads &amp;lt;- 
  function(model, loss_weights, init_image, gram_style_features, content_features) {
    with(tf$GradientTape() %as% tape, {
      scores &amp;lt;-
        compute_loss(model,
                     loss_weights,
                     init_image,
                     gram_style_features,
                     content_features)
    })
    total_loss &amp;lt;- scores[[1]]
    list(tape$gradient(total_loss, init_image), scores)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="training-phase"&gt;Training phase&lt;/h2&gt;
&lt;p&gt;Now it’s time to train! While the natural continuation of this sentence would have been “… the model”, the model we’re training here is not VGG19 (that one we’re just using as a tool), but a minimal setup of just:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a &lt;code&gt;Variable&lt;/code&gt; that holds our to-be-optimized image&lt;/li&gt;
&lt;li&gt;the loss functions we defined above&lt;/li&gt;
&lt;li&gt;an optimizer that will apply the calculated gradients to the image variable (&lt;code&gt;tf$train$AdamOptimizer&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Below, we get the style features (of the style image) and the content feature (of the content image) just once, then iterate over the optimization process, saving the output every 100 iterations.&lt;/p&gt;
&lt;p&gt;In contrast to the original article and the &lt;em&gt;Deep Learning with R&lt;/em&gt; book, but following the Google notebook instead, we’re not using L-BFGS for optimization, but Adam, as our goal here is to provide a concise introduction to eager execution. However, you could plug in another optimization method if you wanted, replacing &lt;code&gt;optimizer$apply_gradients(list(tuple(grads, init_image)))&lt;/code&gt; by an algorithm of your choice (and of course, assigning the result of the optimization to the &lt;code&gt;Variable&lt;/code&gt; holding the image).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;run_style_transfer &amp;lt;- function(content_path, style_path) {
  model &amp;lt;- get_model()
  walk(model$layers, function(layer) layer$trainable = FALSE)
  
  c(style_features, content_features) %&amp;lt;-% 
    get_feature_representations(model, content_path, style_path)
  # dim(gram_style_features[[1]]) == (64, 64)
  gram_style_features &amp;lt;- map(style_features, function(feature) gram_matrix(feature))
  
  init_image &amp;lt;- load_and_process_image(content_path)
  init_image &amp;lt;- tf$contrib$eager$Variable(init_image, dtype = &amp;quot;float32&amp;quot;)
  
  optimizer &amp;lt;- tf$train$AdamOptimizer(learning_rate = 1,
                                      beta1 = 0.99,
                                      epsilon = 1e-1)
  
  c(best_loss, best_image) %&amp;lt;-% list(Inf, NULL)
  loss_weights &amp;lt;- list(style_weight, content_weight)
  
  start_time &amp;lt;- Sys.time()
  global_start &amp;lt;- Sys.time()
  
  norm_means &amp;lt;- c(103.939, 116.779, 123.68)
  min_vals &amp;lt;- -norm_means
  max_vals &amp;lt;- 255 - norm_means
  
  for (i in seq_len(num_iterations)) {
    # dim(grads) == (1, 128, 128, 3)
    c(grads, all_losses) %&amp;lt;-% compute_grads(model,
                                            loss_weights,
                                            init_image,
                                            gram_style_features,
                                            content_features)
    c(loss, style_score, content_score, variation_score) %&amp;lt;-% all_losses
    optimizer$apply_gradients(list(tuple(grads, init_image)))
    clipped &amp;lt;- tf$clip_by_value(init_image, min_vals, max_vals)
    init_image$assign(clipped)
    
    end_time &amp;lt;- Sys.time()
    
    if (k_cast_to_floatx(loss) &amp;lt; best_loss) {
      best_loss &amp;lt;- k_cast_to_floatx(loss)
      best_image &amp;lt;- init_image
    }
    
    if (i %% 50 == 0) {
      glue(&amp;quot;Iteration: {i}&amp;quot;) %&amp;gt;% print()
      glue(
        &amp;quot;Total loss: {k_cast_to_floatx(loss)},
        style loss: {k_cast_to_floatx(style_score)},
        content loss: {k_cast_to_floatx(content_score)},
        total variation loss: {k_cast_to_floatx(variation_score)},
        time for 1 iteration: {(Sys.time() - start_time) %&amp;gt;% round(2)}&amp;quot;
      ) %&amp;gt;% print()
      
      if (i %% 100 == 0) {
        png(paste0(&amp;quot;style_epoch_&amp;quot;, i, &amp;quot;.png&amp;quot;))
        plot_image &amp;lt;- best_image$numpy()
        plot_image &amp;lt;- deprocess_image(plot_image)
        plot(as.raster(plot_image), main = glue(&amp;quot;Iteration {i}&amp;quot;))
        dev.off()
      }
    }
  }
  
  glue(&amp;quot;Total time: {Sys.time() - global_start} seconds&amp;quot;) %&amp;gt;% print()
  list(best_image, best_loss)
}&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="ready-to-run"&gt;Ready to run&lt;/h2&gt;
&lt;p&gt;Now, we’re ready to start the process:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;c(best_image, best_loss) %&amp;lt;-% run_style_transfer(content_path, style_path)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In our case, results didn’t change much after ~ iteration 1000, and this is how our river landscape was looking:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-09-10-eager-style-transfer/images/style_epoch_1000.png" width="240" /&gt;&lt;/p&gt;
&lt;p&gt;… definitely more inviting than had it been painted by Edvard Munch!&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;With neural style transfer, some fiddling around may be needed until you get the result you want. But as our example shows, this doesn’t mean the code has to be complicated. Additionally to being easy to grasp, eager execution also lets you add debugging output, and step through the code line-by-line to check on tensor shapes. Until next time in our eager execution series!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-GatysEB15a"&gt;
&lt;p&gt;Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. 2015. “A Neural Algorithm of Artistic Style.” &lt;em&gt;CoRR&lt;/em&gt; abs/1508.06576. &lt;a href="http://arxiv.org/abs/1508.06576"&gt;http://arxiv.org/abs/1508.06576&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">0279ce5c37a7a5985eabd499c06be55e</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Unsupervised Learning</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-09-10-eager-style-transfer</guid>
      <pubDate>Mon, 10 Sep 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-09-10-eager-style-transfer/images/preview.png" medium="image" type="image/png" width="344" height="231"/>
    </item>
    <item>
      <title>Getting started with deep learning in R</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-09-07-getting-started</link>
      <description>


&lt;p&gt;There are good reasons to get into deep learning: Deep learning has been outperforming the respective “classical” techniques in areas like image recognition and natural language processing for a while now, and it has the potential to bring interesting insights even to the analysis of tabular data. For many R users interested in deep learning, the hurdle is not so much the mathematical prerequisites (as many have a background in statistics or empirical sciences), but rather how to get started in an efficient way.&lt;/p&gt;
&lt;p&gt;This post will give an overview of some materials that should prove useful. In the case that you don’t have that background in statistics or similar, we will also present a few helpful resources to catch up with “the math”.&lt;/p&gt;
&lt;h2 id="keras-tutorials"&gt;Keras tutorials&lt;/h2&gt;
&lt;p&gt;The easiest way to get started is using the Keras API. It is a high-level, declarative (in feel) way of specifying a model, training and testing it, originally developed in &lt;a href="http://keras.io"&gt;Python&lt;/a&gt; by Francois Chollet and ported to R by JJ Allaire.&lt;/p&gt;
&lt;p&gt;Check out the tutorials on the &lt;a href="https://tensorflow.rstudio.com/keras/"&gt;Keras website&lt;/a&gt;: They introduce basic tasks like classification and regression, as well as basic workflow elements like saving and restoring models, or assessing model performance.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/tutorial_basic_classification.html"&gt;Basic classification&lt;/a&gt; gets you started doing image classification using the &lt;em&gt;Fashion MNIST&lt;/em&gt; dataset.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/tutorial_basic_text_classification.html"&gt;Text classification&lt;/a&gt; shows how to do sentiment analysis on movie reviews, and includes the important topic of how to preprocess text for deep learning.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/tutorial_basic_regression.html"&gt;Basic regression&lt;/a&gt; demonstrates the task of predicting a continuous variable by example of the famous Boston housing dataset that ships with Keras.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/tutorial_overfit_underfit.html"&gt;Overfitting and underfitting&lt;/a&gt; explains how you can assess if your model is under- or over-fitting, and what remedies to take.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Last but not least, &lt;a href="https://tensorflow.rstudio.com/keras/articles/tutorial_save_and_restore.html"&gt;Save and restore models&lt;/a&gt; shows how to save checkpoints during and after training, so you don’t lose the fruit of the network’s labor.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Once you’ve seen the basics, the website also has more advanced information on implementing custom logic, monitoring and tuning, as well as using and adapting pre-trained models.&lt;/p&gt;
&lt;h2 id="videos-and-book"&gt;Videos and book&lt;/h2&gt;
&lt;p&gt;If you want a bit more conceptual background, the &lt;a href="https://bit.ly/2oPtXWv"&gt;Deep Learning with R in motion&lt;/a&gt; video series provides a nice introduction to basic concepts of machine learning and deep learning, including things often taken for granted, such as derivatives and gradients.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://bit.ly/2oPtXWv"&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-09-07-getting-started/images/dl_in_motion.png" class="l-body" style="border: 1px solid rgba(0, 0, 0, 0.2);" alt="Example from Deep Learning with R in motion, video 2.7, From Derivatives to Gradients" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The first 2 components of the video series (&lt;a href="https://bit.ly/2oPtXWv"&gt;Getting Started&lt;/a&gt; and the &lt;a href="https://bit.ly/2MY6YHj"&gt;MNIST Case Study&lt;/a&gt;) are free. The remainder of the videos introduce different neural network architectures by way of detailed case studies.&lt;/p&gt;
&lt;p&gt;The series is a companion to the &lt;a href="https://www.amazon.com/Deep-Learning-R-Francois-Chollet/dp/161729554X"&gt;Deep Learning with R&lt;/a&gt; book by Francois Chollet and JJ Allaire. Like the videos, the book has excellent, high-level explanations of deep learning concepts. At the same time, it contains lots of ready-to-use code, presenting examples for all the major architectures and use cases (including fancy stuff like variational autoencoders and GANs).&lt;/p&gt;
&lt;aside&gt;
&lt;a href="https://www.amazon.com/Deep-Learning-R-Francois-Chollet/dp/161729554X"&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-09-07-getting-started/images/dlwR.png" style="border: 1px solid rgba(0, 0, 0, 0.2);" width="100" /&gt;&lt;/a&gt;
&lt;/aside&gt;
&lt;h2 id="inspiration"&gt;Inspiration&lt;/h2&gt;
&lt;p&gt;If you’re not pursuing a specific goal, but in general curious about what can be done with deep learning, a good place to follow is the &lt;a href="https://blogs.rstudio.com/tensorflow/"&gt;TensorFlow for R Blog&lt;/a&gt;. There, you’ll find applications of deep learning to business as well as scientific tasks, as well as technical expositions and introductions to new features.&lt;/p&gt;
&lt;p&gt;In addition, the &lt;a href="https://blogs.rstudio.com/tensorflow/gallery.html"&gt;TensorFlow for R Gallery&lt;/a&gt; highlights several case studies that have proven especially useful for getting started in various areas of application.&lt;/p&gt;
&lt;h2 id="reality"&gt;Reality&lt;/h2&gt;
&lt;p&gt;Once the ideas are there, realization should follow, and for most of us the question will be: Where can I actually &lt;em&gt;train&lt;/em&gt; that model? As soon as real-world-size images are involved, or other kinds of higher-dimensional data, you’ll need a modern, high performance GPU so training on your laptop won’t be an option any more.&lt;/p&gt;
&lt;p&gt;There are a few different ways you can train in the cloud:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;RStudio provides &lt;a href="https://tensorflow.rstudio.com/tools/cloud_server_gpu.html"&gt;Amazon EC2 AMIs for cloud GPU instances&lt;/a&gt;. The AMI has both RStudio Server and the R TensorFlow package suite preinstalled.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You can also try out &lt;a href="https://tensorflow.rstudio.com/tools/cloud_desktop_gpu.html"&gt;Paperspace cloud GPU desktops&lt;/a&gt; (again with the RStudio and the R TensorFlow package suite preinstalled).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;em&gt;cloudml&lt;/em&gt; package provides an &lt;a href="https://tensorflow.rstudio.com/tools/cloudml/articles/getting_started.html"&gt;interface to the Google Cloud Machine Learning engine&lt;/a&gt;, which makes it easy to submit batch GPU training jobs to CloudML.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="more-background"&gt;More background&lt;/h2&gt;
&lt;p&gt;If you don’t have a very “mathy” background, you might feel that you’d like to supplement the concepts-focused approach from &lt;em&gt;Deep Learning with R&lt;/em&gt; with a bit more low-level basics (just as some people feel the need to know at least a bit of C or Assembler when learning a high-level language).&lt;/p&gt;
&lt;p&gt;Personal recommendations for such cases would include Andrew Ng’s &lt;a href="https://www.coursera.org/specializations/deep-learning"&gt;deep learning specialization&lt;/a&gt; on Coursera (videos are free to watch), and the book(s) and recorded lectures on linear algebra by &lt;a href="https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/"&gt;Gilbert Strang&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Of course, the ultimate reference on deep learning, as of today, is the &lt;a href="https://www.deeplearningbook.org"&gt;Deep Learning&lt;/a&gt; textbook by Ian Goodfellow, Yoshua Bengio and Aaron Courville. The book covers everything from background in linear algebra, probability theory and optimization via basic architectures such as CNNs or RNNs, on to unsupervised models on the frontier of the very latest research.&lt;/p&gt;
&lt;h2 id="getting-help"&gt;Getting help&lt;/h2&gt;
&lt;p&gt;Last not least, should you encounter problems with the software (or with mapping your task to runnable code), a good idea is to create a GitHub issue in the respective repository, e.g., &lt;a href="https://github.com/rstudio/keras/"&gt;rstudio/keras&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Best of luck for your deep learning journey with R!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">d848cdf19d930a5139c0499c556e6d3f</distill:md5>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-09-07-getting-started</guid>
      <pubDate>Fri, 07 Sep 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-09-07-getting-started/images/digits.png" medium="image" type="image/png" width="557" height="317"/>
    </item>
    <item>
      <title>Generating images with Keras and TensorFlow eager execution</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-08-26-eager-dcgan</link>
      <description>


&lt;p&gt;The recent announcement of TensorFlow 2.0 names &lt;em&gt;eager execution&lt;/em&gt; as the number one central feature of the new major version. What does this mean for R users? As demonstrated in our recent post on neural machine translation, you can use eager execution from R now already, in combination with Keras custom models and the datasets API. It’s good to know you &lt;em&gt;can&lt;/em&gt; use it - but why should you? And in which cases?&lt;/p&gt;
&lt;p&gt;In this and a few upcoming posts, we want to show how eager execution can make developing models a lot easier. The degree of simplication will depend on the task - and just &lt;em&gt;how much&lt;/em&gt; easier you’ll find the new way might also depend on your experience using the functional API to model more complex relationships. Even if you think that GANs, encoder-decoder architectures, or neural style transfer didn’t pose any problems before the advent of eager execution, you might find that the alternative is a better fit to how we humans mentally picture problems.&lt;/p&gt;
&lt;p&gt;For this post, we are porting code from a recent &lt;a href="https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/dcgan.ipynb"&gt;Google Colaboratory notebook&lt;/a&gt; implementing the DCGAN architecture.&lt;span class="citation"&gt;(Radford, Metz, and Chintala 2015)&lt;/span&gt; No prior knowledge of GANs is required - we’ll keep this post practical (no maths) and focus on how to achieve your goal, mapping a simple and vivid concept into an astonishingly small number of lines of code.&lt;/p&gt;
&lt;p&gt;As in the post on machine translation with attention, we first have to cover some prerequisites. By the way, no need to copy out the code snippets - you’ll find the complete code in &lt;a href="https://github.com/rstudio/keras/tree/master/vignettes/examples/eager_dcgan.R"&gt;eager_dcgan.R&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id="prerequisites"&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;The code in this post depends on the newest CRAN versions of several of the TensorFlow R packages. You can install these packages as follows:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;install.packages(c(&amp;quot;tensorflow&amp;quot;, &amp;quot;keras&amp;quot;, &amp;quot;tfdatasets&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should also be sure that you are running the very latest version of TensorFlow (v1.10), which you can install like so:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tensorflow)
install_tensorflow()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are additional requirements for using TensorFlow eager execution. First, we need to call &lt;code&gt;tfe_enable_eager_execution()&lt;/code&gt; right at the beginning of the program. Second, we need to use the implementation of Keras included in TensorFlow, rather than the base Keras implementation.&lt;/p&gt;
&lt;p&gt;We’ll also use the &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/introduction.html"&gt;tfdatasets&lt;/a&gt; package for our input pipeline. So we end up with the following preamble to set things up:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
use_implementation(&amp;quot;tensorflow&amp;quot;)

library(tensorflow)
tfe_enable_eager_execution(device_policy = &amp;quot;silent&amp;quot;)

library(tfdatasets)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s it. Let’s get started.&lt;/p&gt;
&lt;h2 id="so-whats-a-gan"&gt;So what’s a GAN?&lt;/h2&gt;
&lt;p&gt;GAN stands for &lt;em&gt;Generative Adversarial Network&lt;/em&gt;&lt;span class="citation"&gt;(Goodfellow et al. 2014)&lt;/span&gt;. It is a setup of two agents, the &lt;em&gt;generator&lt;/em&gt; and the &lt;em&gt;discriminator&lt;/em&gt;, that act against each other (thus, &lt;em&gt;adversarial&lt;/em&gt;). It is &lt;em&gt;generative&lt;/em&gt; because the goal is to generate output (as opposed to, say, classification or regression).&lt;/p&gt;
&lt;p&gt;In human learning, feedback - direct or indirect - plays a central role. Say we wanted to forge a banknote (as long as those still exist). Assuming we can get away with unsuccessful trials, we would get better and better at forgery over time. Optimizing our technique, we would end up rich. This concept of optimizing from feedback is embodied in the first of the two agents, the &lt;em&gt;generator&lt;/em&gt;. It gets its feedback from the &lt;em&gt;discriminator&lt;/em&gt;, in an upside-down way: If it can fool the discriminator, making it believe that the banknote was real, all is fine; if the discriminator notices the fake, it has to do things differently. For a neural network, that means it has to update its weights.&lt;/p&gt;
&lt;p&gt;How does the discriminator know what is real and what is fake? It too has to be trained, on real banknotes (or whatever the kind of objects involved) and the fake ones produced by the generator. So the complete setup is two agents competing, one striving to generate realistic-looking fake objects, and the other, to disavow the deception. The purpose of training is to have both evolve and get better, in turn causing the other to get better, too.&lt;/p&gt;
&lt;p&gt;In this system, there is no objective minimum to the loss function: We want both components to learn and getter better “in lockstep”, instead of one winning out over the other. This makes optimization difficult. In practice therefore, tuning a GAN can seem more like alchemy than like science, and it often makes sense to lean on practices and “tricks” reported by others.&lt;/p&gt;
&lt;p&gt;In this example, just like in the Google notebook we’re porting, the goal is to generate MNIST digits. While that may not sound like the most exciting task one could imagine, it lets us focus on the mechanics, and allows us to keep computation and memory requirements (comparatively) low.&lt;/p&gt;
&lt;p&gt;Let’s load the data (training set needed only) and then, look at the first actor in our drama, the generator.&lt;/p&gt;
&lt;h2 id="training-data"&gt;Training data&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;mnist &amp;lt;- dataset_mnist()
c(train_images, train_labels) %&amp;lt;-% mnist$train

train_images &amp;lt;- train_images %&amp;gt;% 
  k_expand_dims() %&amp;gt;%
  k_cast(dtype = &amp;quot;float32&amp;quot;)

# normalize images to [-1, 1] because the generator uses tanh activation
train_images &amp;lt;- (train_images - 127.5) / 127.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our complete training set will be streamed once per epoch:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;buffer_size &amp;lt;- 60000
batch_size &amp;lt;- 256
batches_per_epoch &amp;lt;- (buffer_size / batch_size) %&amp;gt;% round()

train_dataset &amp;lt;- tensor_slices_dataset(train_images) %&amp;gt;%
  dataset_shuffle(buffer_size) %&amp;gt;%
  dataset_batch(batch_size)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This input will be fed to the discriminator only.&lt;/p&gt;
&lt;h2 id="generator"&gt;Generator&lt;/h2&gt;
&lt;p&gt;Both generator and discriminator are &lt;a href="https://keras.rstudio.com/articles/custom_models.html"&gt;Keras custom models&lt;/a&gt;. In contrast to custom layers, custom models allow you to construct models as independent units, complete with custom forward pass logic, backprop and optimization. The model-generating function defines the layers the model (&lt;code&gt;self&lt;/code&gt;) wants assigned, and returns the function that implements the forward pass.&lt;/p&gt;
&lt;p&gt;As we will soon see, the generator gets passed vectors of random noise for input. This vector is transformed to 3d (height, width, channels) and then, successively upsampled to the required output size of (28,28,3).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;generator &amp;lt;-
  function(name = NULL) {
    keras_model_custom(name = name, function(self) {
      
      self$fc1 &amp;lt;- layer_dense(units = 7 * 7 * 64, use_bias = FALSE)
      self$batchnorm1 &amp;lt;- layer_batch_normalization()
      self$leaky_relu1 &amp;lt;- layer_activation_leaky_relu()
      self$conv1 &amp;lt;-
        layer_conv_2d_transpose(
          filters = 64,
          kernel_size = c(5, 5),
          strides = c(1, 1),
          padding = &amp;quot;same&amp;quot;,
          use_bias = FALSE
        )
      self$batchnorm2 &amp;lt;- layer_batch_normalization()
      self$leaky_relu2 &amp;lt;- layer_activation_leaky_relu()
      self$conv2 &amp;lt;-
        layer_conv_2d_transpose(
          filters = 32,
          kernel_size = c(5, 5),
          strides = c(2, 2),
          padding = &amp;quot;same&amp;quot;,
          use_bias = FALSE
        )
      self$batchnorm3 &amp;lt;- layer_batch_normalization()
      self$leaky_relu3 &amp;lt;- layer_activation_leaky_relu()
      self$conv3 &amp;lt;-
        layer_conv_2d_transpose(
          filters = 1,
          kernel_size = c(5, 5),
          strides = c(2, 2),
          padding = &amp;quot;same&amp;quot;,
          use_bias = FALSE,
          activation = &amp;quot;tanh&amp;quot;
        )
      
      function(inputs, mask = NULL, training = TRUE) {
        self$fc1(inputs) %&amp;gt;%
          self$batchnorm1(training = training) %&amp;gt;%
          self$leaky_relu1() %&amp;gt;%
          k_reshape(shape = c(-1, 7, 7, 64)) %&amp;gt;%
          self$conv1() %&amp;gt;%
          self$batchnorm2(training = training) %&amp;gt;%
          self$leaky_relu2() %&amp;gt;%
          self$conv2() %&amp;gt;%
          self$batchnorm3(training = training) %&amp;gt;%
          self$leaky_relu3() %&amp;gt;%
          self$conv3()
      }
    })
  }&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="discriminator"&gt;Discriminator&lt;/h2&gt;
&lt;p&gt;The discriminator is just a pretty normal convolutional network outputting a score. Here, usage of “score” instead of “probability” is on purpose: If you look at the last layer, it is fully connected, of size 1 but lacking the usual sigmoid activation. This is because unlike Keras’ &lt;code&gt;loss_binary_crossentropy&lt;/code&gt;, the loss function we’ll be using here - &lt;code&gt;tf$losses$sigmoid_cross_entropy&lt;/code&gt; - works with the raw logits, not the outputs of the sigmoid.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;discriminator &amp;lt;-
  function(name = NULL) {
    keras_model_custom(name = name, function(self) {
      
      self$conv1 &amp;lt;- layer_conv_2d(
        filters = 64,
        kernel_size = c(5, 5),
        strides = c(2, 2),
        padding = &amp;quot;same&amp;quot;
      )
      self$leaky_relu1 &amp;lt;- layer_activation_leaky_relu()
      self$dropout &amp;lt;- layer_dropout(rate = 0.3)
      self$conv2 &amp;lt;-
        layer_conv_2d(
          filters = 128,
          kernel_size = c(5, 5),
          strides = c(2, 2),
          padding = &amp;quot;same&amp;quot;
        )
      self$leaky_relu2 &amp;lt;- layer_activation_leaky_relu()
      self$flatten &amp;lt;- layer_flatten()
      self$fc1 &amp;lt;- layer_dense(units = 1)
      
      function(inputs, mask = NULL, training = TRUE) {
        inputs %&amp;gt;% self$conv1() %&amp;gt;%
          self$leaky_relu1() %&amp;gt;%
          self$dropout(training = training) %&amp;gt;%
          self$conv2() %&amp;gt;%
          self$leaky_relu2() %&amp;gt;%
          self$flatten() %&amp;gt;%
          self$fc1()
      }
    })
  }&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="setting-the-scene"&gt;Setting the scene&lt;/h2&gt;
&lt;p&gt;Before we can start training, we need to create the usual components of a deep learning setup: the model (or models, in this case), the loss function(s), and the optimizer(s).&lt;/p&gt;
&lt;p&gt;Model creation is just a function call, with a little extra on top:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;generator &amp;lt;- generator()
discriminator &amp;lt;- discriminator()

# https://www.tensorflow.org/api_docs/python/tf/contrib/eager/defun
generator$call = tf$contrib$eager$defun(generator$call)
discriminator$call = tf$contrib$eager$defun(discriminator$call)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href="https://www.tensorflow.org/api_docs/python/tf/contrib/eager/defun"&gt;&lt;em&gt;defun&lt;/em&gt;&lt;/a&gt; compiles an R function (once per different combination of argument shapes and non-tensor objects values)) into a TensorFlow graph, and is used to speed up computations. This comes with side effects and possibly unexpected behavior - please consult the documentation for the details. Here, we were mainly curious in how much of a speedup we might notice when using this from R - in our example, it resulted in a speedup of 130%.&lt;/p&gt;
&lt;p&gt;On to the losses. Discriminator loss consists of two parts: Does it correctly identify real images as real, and does it correctly spot fake images as fake. Here &lt;code&gt;real_output&lt;/code&gt; and &lt;code&gt;generated_output&lt;/code&gt; contain the logits returned from the discriminator - that is, its judgment of whether the respective images are fake or real.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;discriminator_loss &amp;lt;- function(real_output, generated_output) {
  real_loss &amp;lt;- tf$losses$sigmoid_cross_entropy(
    multi_class_labels = k_ones_like(real_output),
    logits = real_output)
  generated_loss &amp;lt;- tf$losses$sigmoid_cross_entropy(
    multi_class_labels = k_zeros_like(generated_output),
    logits = generated_output)
  real_loss + generated_loss
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Generator loss depends on how the discriminator judged its creations: It would hope for them all to be seen as real.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;generator_loss &amp;lt;- function(generated_output) {
  tf$losses$sigmoid_cross_entropy(
    tf$ones_like(generated_output),
    generated_output)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we still need to define optimizers, one for each model.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;discriminator_optimizer &amp;lt;- tf$train$AdamOptimizer(1e-4)
generator_optimizer &amp;lt;- tf$train$AdamOptimizer(1e-4)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="training-loop"&gt;Training loop&lt;/h2&gt;
&lt;p&gt;There are two models, two loss functions and two optimizers, but there is just one training loop, as both models depend on each other. The training loop will be over MNIST images streamed in batches, but we still need input to the generator - a random vector of size 100, in this case.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;noise_dim &amp;lt;- 100&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take the training loop step by step. There will be an outer and an inner loop, one over epochs and one over batches. At the start of each epoch, we create a fresh iterator over the dataset:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;for (epoch in seq_len(num_epochs)) {
  start &amp;lt;- Sys.time()
  total_loss_gen &amp;lt;- 0
  total_loss_disc &amp;lt;- 0
  iter &amp;lt;- make_iterator_one_shot(train_dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now for every batch we obtain from the iterator, we are calling the generator and having it generate images from random noise. Then, we’re calling the dicriminator on real images as well as the fake images just generated. For the discriminator, its relative outputs are directly fed into the loss function. For the generator, its loss will depend on how the discriminator judged its creations:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;until_out_of_range({
  batch &amp;lt;- iterator_get_next(iter)
  noise &amp;lt;- k_random_normal(c(batch_size, noise_dim))
  with(tf$GradientTape() %as% gen_tape, { with(tf$GradientTape() %as% disc_tape, {
    generated_images &amp;lt;- generator(noise)
    disc_real_output &amp;lt;- discriminator(batch, training = TRUE)
    disc_generated_output &amp;lt;-
       discriminator(generated_images, training = TRUE)
    gen_loss &amp;lt;- generator_loss(disc_generated_output)
    disc_loss &amp;lt;- discriminator_loss(disc_real_output, disc_generated_output)
  }) })&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that all model calls happen inside &lt;code&gt;tf$GradientTape&lt;/code&gt; contexts. This is so the forward passes can be recorded and “played back” to back propagate the losses through the network.&lt;/p&gt;
&lt;p&gt;Obtain the gradients of the losses to the respective models’ variables (&lt;code&gt;tape$gradient&lt;/code&gt;) and have the optimizers apply them to the models’ weights (&lt;code&gt;optimizer$apply_gradients&lt;/code&gt;):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;gradients_of_generator &amp;lt;-
  gen_tape$gradient(gen_loss, generator$variables)
gradients_of_discriminator &amp;lt;-
  disc_tape$gradient(disc_loss, discriminator$variables)
      
generator_optimizer$apply_gradients(purrr::transpose(
  list(gradients_of_generator, generator$variables)
))
discriminator_optimizer$apply_gradients(purrr::transpose(
  list(gradients_of_discriminator, discriminator$variables)
))
      
total_loss_gen &amp;lt;- total_loss_gen + gen_loss
total_loss_disc &amp;lt;- total_loss_disc + disc_loss&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This ends the loop over batches. Finish off the loop over epochs displaying current losses and saving a few of the generator’s artwork:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;cat(&amp;quot;Time for epoch &amp;quot;, epoch, &amp;quot;: &amp;quot;, Sys.time() - start, &amp;quot;\n&amp;quot;)
cat(&amp;quot;Generator loss: &amp;quot;, total_loss_gen$numpy() / batches_per_epoch, &amp;quot;\n&amp;quot;)
cat(&amp;quot;Discriminator loss: &amp;quot;, total_loss_disc$numpy() / batches_per_epoch, &amp;quot;\n\n&amp;quot;)
if (epoch %% 10 == 0)
  generate_and_save_images(generator,
                           epoch,
                           random_vector_for_generation)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the training loop again, shown as a whole - even including the lines for reporting on progress, it is remarkably concise, and allows for a quick grasp of what is going on:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train &amp;lt;- function(dataset, epochs, noise_dim) {
  for (epoch in seq_len(num_epochs)) {
    start &amp;lt;- Sys.time()
    total_loss_gen &amp;lt;- 0
    total_loss_disc &amp;lt;- 0
    iter &amp;lt;- make_iterator_one_shot(train_dataset)
    
    until_out_of_range({
      batch &amp;lt;- iterator_get_next(iter)
      noise &amp;lt;- k_random_normal(c(batch_size, noise_dim))
      with(tf$GradientTape() %as% gen_tape, { with(tf$GradientTape() %as% disc_tape, {
        generated_images &amp;lt;- generator(noise)
        disc_real_output &amp;lt;- discriminator(batch, training = TRUE)
        disc_generated_output &amp;lt;-
          discriminator(generated_images, training = TRUE)
        gen_loss &amp;lt;- generator_loss(disc_generated_output)
        disc_loss &amp;lt;-
          discriminator_loss(disc_real_output, disc_generated_output)
      }) })
      
      gradients_of_generator &amp;lt;-
        gen_tape$gradient(gen_loss, generator$variables)
      gradients_of_discriminator &amp;lt;-
        disc_tape$gradient(disc_loss, discriminator$variables)
      
      generator_optimizer$apply_gradients(purrr::transpose(
        list(gradients_of_generator, generator$variables)
      ))
      discriminator_optimizer$apply_gradients(purrr::transpose(
        list(gradients_of_discriminator, discriminator$variables)
      ))
      
      total_loss_gen &amp;lt;- total_loss_gen + gen_loss
      total_loss_disc &amp;lt;- total_loss_disc + disc_loss
      
    })
    
    cat(&amp;quot;Time for epoch &amp;quot;, epoch, &amp;quot;: &amp;quot;, Sys.time() - start, &amp;quot;\n&amp;quot;)
    cat(&amp;quot;Generator loss: &amp;quot;, total_loss_gen$numpy() / batches_per_epoch, &amp;quot;\n&amp;quot;)
    cat(&amp;quot;Discriminator loss: &amp;quot;, total_loss_disc$numpy() / batches_per_epoch, &amp;quot;\n\n&amp;quot;)
    if (epoch %% 10 == 0)
      generate_and_save_images(generator,
                               epoch,
                               random_vector_for_generation)
    
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the function for saving generated images…&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;generate_and_save_images &amp;lt;- function(model, epoch, test_input) {
  predictions &amp;lt;- model(test_input, training = FALSE)
  png(paste0(&amp;quot;images_epoch_&amp;quot;, epoch, &amp;quot;.png&amp;quot;))
  par(mfcol = c(5, 5))
  par(mar = c(0.5, 0.5, 0.5, 0.5),
      xaxs = &amp;#39;i&amp;#39;,
      yaxs = &amp;#39;i&amp;#39;)
  for (i in 1:25) {
    img &amp;lt;- predictions[i, , , 1]
    img &amp;lt;- t(apply(img, 2, rev))
    image(
      1:28,
      1:28,
      img * 127.5 + 127.5,
      col = gray((0:255) / 255),
      xaxt = &amp;#39;n&amp;#39;,
      yaxt = &amp;#39;n&amp;#39;
    )
  }
  dev.off()
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;… and we’re ready to go!&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;num_epochs &amp;lt;- 150
train(train_dataset, num_epochs, noise_dim)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="results"&gt;Results&lt;/h2&gt;
&lt;p&gt;Here are some generated images after training for 150 epochs:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-08-26-eager-dcgan/images/images_epoch_150.png" /&gt;&lt;/p&gt;
&lt;p&gt;As they say, your results will most certainly vary!&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;While certainly tuning GANs will remain a challenge, we hope we were able to show that mapping concepts to code is not difficult when using eager execution. In case you’ve played around with GANs before, you may have found you needed to pay careful attention to set up the losses the right way, freeze the discriminator’s weights when needed, etc. This need goes away with eager execution. In upcoming posts, we will show further examples where using it makes model development easier.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-GoodfellowPMXWOCB14"&gt;
&lt;p&gt;Goodfellow, Ian J., Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. 2014. “Generative Adversarial Nets.” In &lt;em&gt;Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada&lt;/em&gt;, 2672–80. &lt;a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets"&gt;http://papers.nips.cc/paper/5423-generative-adversarial-nets&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-RadfordMC15"&gt;
&lt;p&gt;Radford, Alec, Luke Metz, and Soumith Chintala. 2015. “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.” &lt;em&gt;CoRR&lt;/em&gt; abs/1511.06434. &lt;a href="http://arxiv.org/abs/1511.06434"&gt;http://arxiv.org/abs/1511.06434&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">e3ac9f4904903b595428093bb51e5460</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Unsupervised Learning</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-08-26-eager-dcgan</guid>
      <pubDate>Sun, 26 Aug 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-08-26-eager-dcgan/images/thumb.png" medium="image" type="image/png" width="240" height="144"/>
    </item>
    <item>
      <title>Attention-based Neural Machine Translation with Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-07-30-attention-layer</link>
      <description>


&lt;p&gt;These days it is not difficult to find sample code that demonstrates sequence to sequence translation using Keras. However, within the past few years it has been established that depending on the task, incorporating an attention mechanism significantly improves performance. First and foremost, this was the case for neural machine translation (see &lt;span class="citation"&gt;(Bahdanau, Cho, and Bengio 2014)&lt;/span&gt; and &lt;span class="citation"&gt;(Luong, Pham, and Manning 2015)&lt;/span&gt; for prominent work). But other areas performing sequence to sequence translation were profiting from incorporating an attention mechanism, too: E.g., &lt;span class="citation"&gt;(Xu et al. 2015)&lt;/span&gt; applied attention to image captioning, and &lt;span class="citation"&gt;(Vinyals et al. 2014)&lt;/span&gt;, to parsing.&lt;/p&gt;
&lt;p&gt;Ideally, using Keras, we’d just have an attention layer managing this for us. Unfortunately, as can be seen googling for code snippets and blog posts, implementing attention in pure Keras is not that straightforward.&lt;/p&gt;
&lt;p&gt;Consequently, until a short time ago, the best thing to do seemed to be translating the &lt;a href="https://github.com/tensorflow/nmt"&gt;TensorFlow Neural Machine Translation Tutorial&lt;/a&gt; to R TensorFlow. Then, &lt;a href="https://www.tensorflow.org/guide/eager"&gt;TensorFlow eager execution&lt;/a&gt; happened, and turned out a game changer for a number of things that used to be difficult (not the least of which is debugging). With eager execution, tensor operations are executed immediately, as opposed to of building a graph to be evaluated later. This means we can immediately inspect the values in our tensors - and it also means we can imperatively code loops to perform interleavings of sorts that earlier were more challenging to accomplish.&lt;/p&gt;
&lt;p&gt;Under these circumstances, it is not surprising that the &lt;a href="https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb"&gt;interactive notebook on neural machine translation&lt;/a&gt;, published on Colaboratory, got a lot of attention for its straightforward implementation and highly intellegible explanations. Our goal here is to do the same thing from R. We will not end up with Keras code exactly the way we used to write it, but a hybrid of Keras layers and imperative code enabled by TensorFlow eager execution.&lt;/p&gt;
&lt;h2 id="prerequisites"&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;The code in this post depends on the development versions of several of the TensorFlow R packages. You can install these packages as follows:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;devtools::install_github(c(
  &amp;quot;rstudio/reticulate&amp;quot;,
  &amp;quot;rstudio/tensorflow&amp;quot;,
  &amp;quot;rstudio/keras&amp;quot;,
  &amp;quot;rstudio/tfdatasets&amp;quot;
))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should also be sure that you are running the very latest version of TensorFlow (v1.9), which you can install like so:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tensorflow)
install_tensorflow()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are additional requirements for using TensorFlow eager execution. First, we need to call &lt;code&gt;tfe_enable_eager_execution()&lt;/code&gt; right at the beginning of the program. Second, we need to use the implementation of Keras included in TensorFlow, rather than the base Keras implementation. This is because at a later point, we are going to access &lt;code&gt;model$variables&lt;/code&gt; which at this point does not exist in base Keras.&lt;/p&gt;
&lt;p&gt;We’ll also use the &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/introduction.html"&gt;tfdatasets&lt;/a&gt; package for our input pipeline. So we end up with the below libraries needed for this example.&lt;/p&gt;
&lt;p&gt;One more aside: Please don’t copy-paste the code from the snippets for execution - you’ll find the complete code for this post &lt;a href="https://github.com/rstudio/keras/tree/master/vignettes/examples/nmt_attention.R"&gt;here&lt;/a&gt;. In the post, we may deviate from required execution order for purposes of narrative.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
use_implementation(&amp;quot;tensorflow&amp;quot;)

library(tensorflow)
tfe_enable_eager_execution()

library(tfdatasets)

library(purrr)
library(stringr)
library(reshape2)
library(viridis)
library(ggplot2)
library(tibble)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="preparing-the-data"&gt;Preparing the data&lt;/h2&gt;
&lt;p&gt;As our focus is on implementing the attention mechanism, we’re going to do a quick pass through pre-preprocessing. All operations are contained in short functions that are independently testable (which also makes it easy should you want to experiment with different preprocessing actions).&lt;/p&gt;
&lt;p&gt;The site &lt;a href="https://www.manythings.org/anki/" class="uri"&gt;https://www.manythings.org/anki/&lt;/a&gt; is a great source for multilingual datasets. For variation, we’ll choose a different dataset from the colab notebook, and try to translate English to Dutch. I’m going to assume you have the unzipped file &lt;code&gt;nld.txt&lt;/code&gt; in a subdirectory called &lt;code&gt;data&lt;/code&gt; in your current directory. The file contains 28224 sentence pairs, of which we are going to use the first 10000. Under this restriction, sentences range from one-word exclamations&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Run!    Ren!
Wow!    Da&amp;#39;s niet gek!
Fire!   Vuur!&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;over short phrases&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Are you crazy?  Ben je gek?
Do cats dream?  Dromen katten?
Feed the bird!  Geef de vogel voer!&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to simple sentences such as&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;My brother will kill me.    Mijn broer zal me vermoorden.
No one knows the future.    Niemand kent de toekomst.
Please ask someone else.    Vraag alsjeblieft iemand anders.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;filepath &amp;lt;- file.path(&amp;quot;data&amp;quot;, &amp;quot;nld.txt&amp;quot;)

lines &amp;lt;- readLines(filepath, n = 10000)
sentences &amp;lt;- str_split(lines, &amp;quot;\t&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Basic preprocessing includes adding space before punctuation, replacing special characters, reducing multiple spaces to one, and adding &lt;code&gt;&amp;lt;start&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;stop&amp;gt;&lt;/code&gt; tokens at the beginnings resp. ends of the sentences.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;space_before_punct &amp;lt;- function(sentence) {
  str_replace_all(sentence, &amp;quot;([?.!])&amp;quot;, &amp;quot; \\1&amp;quot;)
}

replace_special_chars &amp;lt;- function(sentence) {
  str_replace_all(sentence, &amp;quot;[^a-zA-Z?.!,¿]+&amp;quot;, &amp;quot; &amp;quot;)
}

add_tokens &amp;lt;- function(sentence) {
  paste0(&amp;quot;&amp;lt;start&amp;gt; &amp;quot;, sentence, &amp;quot; &amp;lt;stop&amp;gt;&amp;quot;)
}
add_tokens &amp;lt;- Vectorize(add_tokens, USE.NAMES = FALSE)

preprocess_sentence &amp;lt;- compose(add_tokens,
                               str_squish,
                               replace_special_chars,
                               space_before_punct)

word_pairs &amp;lt;- map(sentences, preprocess_sentence)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As usual with text data, we need to create lookup indices to get from words to integers and vice versa: one index each for the source and target languages.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;create_index &amp;lt;- function(sentences) {
  unique_words &amp;lt;- sentences %&amp;gt;% unlist() %&amp;gt;% paste(collapse = &amp;quot; &amp;quot;) %&amp;gt;%
    str_split(pattern = &amp;quot; &amp;quot;) %&amp;gt;% .[[1]] %&amp;gt;% unique() %&amp;gt;% sort()
  index &amp;lt;- data.frame(
    word = unique_words,
    index = 1:length(unique_words),
    stringsAsFactors = FALSE
  ) %&amp;gt;%
    add_row(word = &amp;quot;&amp;lt;pad&amp;gt;&amp;quot;,
                    index = 0,
                    .before = 1)
  index
}

word2index &amp;lt;- function(word, index_df) {
  index_df[index_df$word == word, &amp;quot;index&amp;quot;]
}
index2word &amp;lt;- function(index, index_df) {
  index_df[index_df$index == index, &amp;quot;word&amp;quot;]
}

src_index &amp;lt;- create_index(map(word_pairs, ~ .[[1]]))
target_index &amp;lt;- create_index(map(word_pairs, ~ .[[2]]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Conversion of text to integers uses the above indices as well as Keras’ convenient &lt;code&gt;pad_sequences&lt;/code&gt; function, which leaves us with matrices of integers, padded up to maximum sentence length found in the source and target corpora, respectively.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;sentence2digits &amp;lt;- function(sentence, index_df) {
  map((sentence %&amp;gt;% str_split(pattern = &amp;quot; &amp;quot;))[[1]], function(word)
    word2index(word, index_df))
}

sentlist2diglist &amp;lt;- function(sentence_list, index_df) {
  map(sentence_list, function(sentence)
    sentence2digits(sentence, index_df))
}

src_diglist &amp;lt;-
  sentlist2diglist(map(word_pairs, ~ .[[1]]), src_index)
src_maxlen &amp;lt;- map(src_diglist, length) %&amp;gt;% unlist() %&amp;gt;% max()
src_matrix &amp;lt;-
  pad_sequences(src_diglist, maxlen = src_maxlen,  padding = &amp;quot;post&amp;quot;)

target_diglist &amp;lt;-
  sentlist2diglist(map(word_pairs, ~ .[[2]]), target_index)
target_maxlen &amp;lt;- map(target_diglist, length) %&amp;gt;% unlist() %&amp;gt;% max()
target_matrix &amp;lt;-
  pad_sequences(target_diglist, maxlen = target_maxlen, padding = &amp;quot;post&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All that remains to be done is the train-test split.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_indices &amp;lt;-
  sample(nrow(src_matrix), size = nrow(src_matrix) * 0.8)

validation_indices &amp;lt;- setdiff(1:nrow(src_matrix), train_indices)

x_train &amp;lt;- src_matrix[train_indices, ]
y_train &amp;lt;- target_matrix[train_indices, ]

x_valid &amp;lt;- src_matrix[validation_indices, ]
y_valid &amp;lt;- target_matrix[validation_indices, ]

buffer_size &amp;lt;- nrow(x_train)

# just for convenience, so we may get a glimpse at translation 
# performance during training
train_sentences &amp;lt;- sentences[train_indices]
validation_sentences &amp;lt;- sentences[validation_indices]
validation_sample &amp;lt;- sample(validation_sentences, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="creating-datasets-to-iterate-over"&gt;Creating datasets to iterate over&lt;/h2&gt;
&lt;p&gt;This section does not contain much code, but it shows an important technique: the use of datasets. Remember the olden times when we used to pass in hand-crafted generators to Keras models? With &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/introduction.html"&gt;tfdatasets&lt;/a&gt;, we can scalably feed data directly to the Keras &lt;code&gt;fit&lt;/code&gt; function, having various preparatory actions being performed directly in native code. In our case, we will not be using &lt;code&gt;fit&lt;/code&gt;, instead iterate directly over the tensors contained in the dataset.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_dataset &amp;lt;- 
  tensor_slices_dataset(keras_array(list(x_train, y_train)))  %&amp;gt;%
  dataset_shuffle(buffer_size = buffer_size) %&amp;gt;%
  dataset_batch(batch_size, drop_remainder = TRUE)

validation_dataset &amp;lt;-
  tensor_slices_dataset(keras_array(list(x_valid, y_valid))) %&amp;gt;%
  dataset_shuffle(buffer_size = buffer_size) %&amp;gt;%
  dataset_batch(batch_size, drop_remainder = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are ready to roll! In fact, before talking about that training loop we need to dive into the implementation of the core logic: the custom layers responsible for performing the attention operation.&lt;/p&gt;
&lt;h2 id="attention-encoder"&gt;Attention encoder&lt;/h2&gt;
&lt;p&gt;We will create two custom layers, only the second of which is going to incorporate attention logic.&lt;/p&gt;
&lt;p&gt;However, it’s worth introducing the encoder in detail too, because technically this is not a custom layer but a custom model, as described &lt;a href="https://github.com/rstudio/keras/blob/master/vignettes/custom_models.Rmd"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Custom models allow you to create member layers and then, specify custom functionality defining the operations to be performed on these layers.&lt;/p&gt;
&lt;p&gt;Let’s look at the complete code for the encoder.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;attention_encoder &amp;lt;-
  
  function(gru_units,
           embedding_dim,
           src_vocab_size,
           name = NULL) {
    
    keras_model_custom(name = name, function(self) {
      
      self$embedding &amp;lt;-
        layer_embedding(
          input_dim = src_vocab_size,
          output_dim = embedding_dim
        )
      
      self$gru &amp;lt;-
        layer_gru(
          units = gru_units,
          return_sequences = TRUE,
          return_state = TRUE
        )
      
      function(inputs, mask = NULL) {
        
        x &amp;lt;- inputs[[1]]
        hidden &amp;lt;- inputs[[2]]
        
        x &amp;lt;- self$embedding(x)
        c(output, state) %&amp;lt;-% self$gru(x, initial_state = hidden)
    
        list(output, state)
      }
    })
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The encoder has two layers, an embedding and a GRU layer. The ensuing anonymous function specifies what should happen when the layer is called. One thing that might look unexpected is the argument passed to that function: It is a list of tensors, where the first element are the inputs, and the second is the hidden state at the point the layer is called (in traditional Keras RNN usage, we are accustomed to seeing state manipulations being done transparently for us.) As the input to the call flows through the operations, let’s keep track of the shapes involved:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;x&lt;/code&gt;, the input, is of size &lt;code&gt;(batch_size, max_length_input)&lt;/code&gt;, where &lt;code&gt;max_length_input&lt;/code&gt; is the number of digits constituting a source sentence. (Remember we’ve padded them to be of uniform length.) In familiar RNN parlance, we could also speak of &lt;code&gt;timesteps&lt;/code&gt; here (we soon will).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;After the embedding step, the tensors will have an additional axis, as each timestep (token) will have been embedded as an &lt;code&gt;embedding_dim&lt;/code&gt;-dimensional vector. So our shapes are now &lt;code&gt;(batch_size, max_length_input, embedding_dim)&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Note how when calling the GRU, we’re passing in the hidden state we received as &lt;code&gt;initial_state&lt;/code&gt;. We get back a list: the GRU output and last hidden state.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At this point, it helps to look up RNN output shapes in the &lt;a href="https://tensorflow.rstudio.com/keras/reference/layer_simple_rnn.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We have specified our GRU to return sequences as well as the state. Our asking for the state means we’ll get back a list of tensors: the output, and the last state(s) - a single last state in this case as we’re using GRU. That state itself will be of shape &lt;code&gt;(batch_size, gru_units)&lt;/code&gt;. Our asking for sequences means the output will be of shape &lt;code&gt;(batch_size, max_length_input, gru_units)&lt;/code&gt;. So that’s that. We bundle output and last state in a list and pass it to the calling code.&lt;/p&gt;
&lt;p&gt;Before we show the decoder, we need to say a few things about attention.&lt;/p&gt;
&lt;h2 id="attention-in-a-nutshell"&gt;Attention in a nutshell&lt;/h2&gt;
&lt;p&gt;As T. Luong nicely puts it in his &lt;a href="https://github.com/lmthang/thesis/blob/master/thesis.pdf"&gt;thesis&lt;/a&gt;, the idea of the attention mechanism is&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;to provide a ‘random access memory’ of source hidden states which one can constantly refer to as translation progresses.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This means that at every timestep, the decoder receives not just the previous decoder hidden state, but also the complete output from the encoder. It then “makes up its mind” as to what part of the encoded input matters at the current point in time. Although various attention mechanisms exist, the basic procedure often goes like this.&lt;/p&gt;
&lt;aside&gt;
In our description, we’re closely following &lt;span class="citation"&gt;(Luong, Pham, and Manning 2015)&lt;/span&gt;, in accordance with the colaboratory notebook on NMT.
&lt;/aside&gt;
&lt;p&gt;First, we create a &lt;em&gt;score&lt;/em&gt; that relates the decoder hidden state at a given timestep to the encoder hidden states at every timestep.&lt;/p&gt;
&lt;p&gt;The score function can take different shapes; the following is commonly referred to as &lt;em&gt;Bahdanau style&lt;/em&gt; (additive) attention.&lt;/p&gt;
&lt;p&gt;Note that when referring to this as &lt;em&gt;Bahdanau style attention&lt;/em&gt;, we - like others - do not imply exact agreement with the formulae in &lt;span class="citation"&gt;(Bahdanau, Cho, and Bengio 2014)&lt;/span&gt;. It is about the general way encoder and decoder hidden states are combined - additively or multiplicatively.&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[score(\mathbf{h}_t,\bar{\mathbf{h}_s}) = \mathbf{v}_a^T tanh(\mathbf{W_1}\mathbf{h}_t + \mathbf{W_2}\bar{\mathbf{h}_s})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From these scores, we want to find the encoder states that matter most to the current decoder timestep. Basically, we just normalize the scores doing a softmax, which leaves us with a set of &lt;em&gt;attention weights&lt;/em&gt; (also called &lt;em&gt;alignment vectors&lt;/em&gt;):&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\alpha_{ts} = \frac{exp(score(\mathbf{h}_t,\bar{\mathbf{h}_s}))}{\sum_{s&amp;#39;=1}^{S}{score(\mathbf{h}_t,\bar{\mathbf{h}_{s&amp;#39;}})}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From these &lt;em&gt;attention weights&lt;/em&gt;, we create the &lt;em&gt;context vector&lt;/em&gt;. This is basically an average of the source hidden states, weighted by the &lt;em&gt;attention weights&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\mathbf{c}_t= \sum_s{\alpha_{ts} \bar{\mathbf{h}_s}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now we need to relate this to the state the decoder is in. We calculate the &lt;em&gt;attention vector&lt;/em&gt; from a concatenation of context vector and current decoder hidden state:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\mathbf{a}_t = tanh(\mathbf{W_c} [ \mathbf{c}_t ; \mathbf{h}_t])\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In sum, we see how at each timestep, the attention mechanism combines information from the sequence of encoder states, and the current decoder hidden state. We’ll soon see a third source of information entering the calculation, which will be dependent on whether we’re in the training or the prediction phase.&lt;/p&gt;
&lt;h2 id="attention-decoder"&gt;Attention decoder&lt;/h2&gt;
&lt;p&gt;Now let’s look at how the attention decoder implements the above logic. We will be following the colab notebook in presenting a slight simplification of the score function, which will not prevent the decoder from successfully translating our example sentences.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;attention_decoder &amp;lt;-
  function(object,
           gru_units,
           embedding_dim,
           target_vocab_size,
           name = NULL) {
    
    keras_model_custom(name = name, function(self) {
      
      self$gru &amp;lt;-
        layer_gru(
          units = gru_units,
          return_sequences = TRUE,
          return_state = TRUE
        )
      
      self$embedding &amp;lt;-
        layer_embedding(input_dim = target_vocab_size, 
                        output_dim = embedding_dim)
      
      gru_units &amp;lt;- gru_units
      self$fc &amp;lt;- layer_dense(units = target_vocab_size)
      self$W1 &amp;lt;- layer_dense(units = gru_units)
      self$W2 &amp;lt;- layer_dense(units = gru_units)
      self$V &amp;lt;- layer_dense(units = 1L)
 
      function(inputs, mask = NULL) {
        
        x &amp;lt;- inputs[[1]]
        hidden &amp;lt;- inputs[[2]]
        encoder_output &amp;lt;- inputs[[3]]
        
        hidden_with_time_axis &amp;lt;- k_expand_dims(hidden, 2)
        
        score &amp;lt;- self$V(k_tanh(self$W1(encoder_output) + 
                                self$W2(hidden_with_time_axis)))
        
        attention_weights &amp;lt;- k_softmax(score, axis = 2)
        
        context_vector &amp;lt;- attention_weights * encoder_output
        context_vector &amp;lt;- k_sum(context_vector, axis = 2)
    
        x &amp;lt;- self$embedding(x)
       
        x &amp;lt;- k_concatenate(list(k_expand_dims(context_vector, 2), x), axis = 3)
        
        c(output, state) %&amp;lt;-% self$gru(x)
   
        output &amp;lt;- k_reshape(output, c(-1, gru_units))
    
        x &amp;lt;- self$fc(output)
 
        list(x, state, attention_weights)
        
      }
      
    })
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Firstly, we notice that in addition to the usual embedding and GRU layers we’d expect in a decoder, there are a few additional dense layers. We’ll comment on those as we go.&lt;/p&gt;
&lt;p&gt;This time, the first argument to what is effectively the &lt;code&gt;call&lt;/code&gt; function consists of three parts: input, hidden state, and the output from the encoder.&lt;/p&gt;
&lt;p&gt;First we need to calculate the score, which basically means addition of two matrix multiplications. For that addition, the shapes have to match. Now &lt;code&gt;encoder_output&lt;/code&gt; is of shape &lt;code&gt;(batch_size, max_length_input, gru_units)&lt;/code&gt;, while &lt;code&gt;hidden&lt;/code&gt; has shape &lt;code&gt;(batch_size, gru_units)&lt;/code&gt;. We thus add an axis “in the middle”, obtaining &lt;code&gt;hidden_with_time_axis&lt;/code&gt;, of shape &lt;code&gt;(batch_size, 1, gru_units)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;After applying the &lt;code&gt;tanh&lt;/code&gt; and the fully connected layer to the result of the addition, &lt;code&gt;score&lt;/code&gt; will be of shape &lt;code&gt;(batch_size, max_length_input, 1)&lt;/code&gt;. The next step calculates the softmax, to get the &lt;em&gt;attention weights&lt;/em&gt;. Now softmax by default is applied on the last axis - but here we’re applying it on the second axis, since it is with respect to the input timesteps we want to normalize the scores.&lt;/p&gt;
&lt;p&gt;After normalization, the shape is still &lt;code&gt;(batch_size, max_length_input, 1)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Next up we compute the context vector, as a weighted average of encoder hidden states. Its shape is &lt;code&gt;(batch_size, gru_units)&lt;/code&gt;. Note that like with the softmax operation above, we sum over the second axis, which corresponds to the number of timesteps in the input received from the encoder.&lt;/p&gt;
&lt;p&gt;We still have to take care of the third source of information: the input. Having been passed through the embedding layer, its shape is &lt;code&gt;(batch_size, 1, embedding_dim)&lt;/code&gt;. Here, the second axis is of dimension 1 as we’re forecasting a single token at a time.&lt;/p&gt;
&lt;p&gt;Now, let’s concatenate the context vector and the embedded input, to arrive at the &lt;em&gt;attention vector&lt;/em&gt;. If you compare the code with the formula above, you’ll see that here we’re skipping the &lt;code&gt;tanh&lt;/code&gt; and the additional fully connected layer, and just leave it at the concatenation. After concatenation, the shape now is &lt;code&gt;(batch_size, 1, embedding_dim + gru_units)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The ensuing GRU operation, as usual, gives us back output and shape tensors. The output tensor is flattened to shape &lt;code&gt;(batch_size, gru_units)&lt;/code&gt; and passed through the final densely connected layer, after which the output has shape &lt;code&gt;(batch_size, target_vocab_size)&lt;/code&gt;. With that, we’re going to be able to forecast the next token for every input in the batch.&lt;/p&gt;
&lt;p&gt;Remains to return everything we’re interested in: the output (to be used for forecasting), the last GRU hidden state (to be passed back in to the decoder), and the &lt;em&gt;attention weights&lt;/em&gt; for this batch (for plotting). And that’s that!&lt;/p&gt;
&lt;h2 id="creating-the-model"&gt;Creating the “model”&lt;/h2&gt;
&lt;p&gt;We’re almost ready to train the model. The model? We don’t have a model yet. The next steps will feel a bit unusual if you’re accustomed to the traditional Keras &lt;em&gt;create model -&amp;gt; compile model -&amp;gt; fit model &lt;/em&gt; workflow. Let’s have a look.&lt;/p&gt;
&lt;p&gt;First, we need a few bookkeeping variables.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;batch_size &amp;lt;- 32
embedding_dim &amp;lt;- 64
gru_units &amp;lt;- 256

src_vocab_size &amp;lt;- nrow(src_index)
target_vocab_size &amp;lt;- nrow(target_index)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we create the encoder and decoder objects - it’s tempting to call them layers, but technically both are custom Keras models.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;encoder &amp;lt;- attention_encoder(
  gru_units = gru_units,
  embedding_dim = embedding_dim,
  src_vocab_size = src_vocab_size
)

decoder &amp;lt;- attention_decoder(
  gru_units = gru_units,
  embedding_dim = embedding_dim,
  target_vocab_size = target_vocab_size
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So as we’re going along, assembling a model “from pieces”, we still need a loss function, and an optimizer.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;optimizer &amp;lt;- tf$train$AdamOptimizer()

cx_loss &amp;lt;- function(y_true, y_pred) {
  mask &amp;lt;- ifelse(y_true == 0L, 0, 1)
  loss &amp;lt;-
    tf$nn$sparse_softmax_cross_entropy_with_logits(labels = y_true,
                                                   logits = y_pred) * mask
  tf$reduce_mean(loss)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’re ready to train.&lt;/p&gt;
&lt;h2 id="training-phase"&gt;Training phase&lt;/h2&gt;
&lt;p&gt;In the training phase, we’re using &lt;em&gt;teacher forcing&lt;/em&gt;, which is the established name for feeding the model the (correct) target at time &lt;span class="math inline"&gt;\(t\)&lt;/span&gt; as input for the next calculation step at time &lt;span class="math inline"&gt;\(t + 1\)&lt;/span&gt;. This is in contrast to the inference phase, when the decoder output is fed back as input to the next time step.&lt;/p&gt;
&lt;p&gt;The training phase consists of three loops: firstly, we’re looping over epochs, secondly, over the dataset, and thirdly, over the target sequence we’re predicting.&lt;/p&gt;
&lt;p&gt;For each batch, we’re encoding the source sequence, getting back the output sequence as well as the last hidden state. The hidden state we then use to initialize the decoder. Now, we enter the target sequence prediction loop. For each timestep to be predicted, we call the decoder with the input (which due to teacher forcing is the ground truth from the previous step), its previous hidden state, and the complete encoder output. At each step, the decoder returns predictions, its hidden state and the attention weights.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;n_epochs &amp;lt;- 50

encoder_init_hidden &amp;lt;- k_zeros(c(batch_size, gru_units))

for (epoch in seq_len(n_epochs)) {
  
  total_loss &amp;lt;- 0
  iteration &amp;lt;- 0
    
  iter &amp;lt;- make_iterator_one_shot(train_dataset)
    
  until_out_of_range({
    
    batch &amp;lt;- iterator_get_next(iter)
    loss &amp;lt;- 0
    x &amp;lt;- batch[[1]]
    y &amp;lt;- batch[[2]]
    iteration &amp;lt;- iteration + 1
      
    with(tf$GradientTape() %as% tape, {
      c(enc_output, enc_hidden) %&amp;lt;-% encoder(list(x, encoder_init_hidden))
 
      dec_hidden &amp;lt;- enc_hidden
      dec_input &amp;lt;-
        k_expand_dims(rep(list(
          word2index(&amp;quot;&amp;lt;start&amp;gt;&amp;quot;, target_index)
        ), batch_size))
        

      for (t in seq_len(target_maxlen - 1)) {
        c(preds, dec_hidden, weights) %&amp;lt;-%
          decoder(list(dec_input, dec_hidden, enc_output))
        loss &amp;lt;- loss + cx_loss(y[, t], preds)
     
        dec_input &amp;lt;- k_expand_dims(y[, t])
      }
      
    })
      
    total_loss &amp;lt;-
      total_loss + loss / k_cast_to_floatx(dim(y)[2])
      
      paste0(
        &amp;quot;Batch loss (epoch/batch): &amp;quot;,
        epoch,
        &amp;quot;/&amp;quot;,
        iter,
        &amp;quot;: &amp;quot;,
        (loss / k_cast_to_floatx(dim(y)[2])) %&amp;gt;% 
          as.double() %&amp;gt;% round(4),
        &amp;quot;\n&amp;quot;
      )
      
    variables &amp;lt;- c(encoder$variables, decoder$variables)
    gradients &amp;lt;- tape$gradient(loss, variables)
      
    optimizer$apply_gradients(
      purrr::transpose(list(gradients, variables)),
      global_step = tf$train$get_or_create_global_step()
    )
      
  })
    
    paste0(
      &amp;quot;Total loss (epoch): &amp;quot;,
      epoch,
      &amp;quot;: &amp;quot;,
      (total_loss / k_cast_to_floatx(buffer_size)) %&amp;gt;% 
        as.double() %&amp;gt;% round(4),
      &amp;quot;\n&amp;quot;
    )
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How does backpropagation work with this new flow? With eager execution, a &lt;code&gt;GradientTape&lt;/code&gt; records operations performed on the forward pass. This recording is then “played back” to perform backpropagation. Concretely put, during the forward pass, we have the tape recording the model’s actions, and we keep incrementally updating the loss. Then, outside the tape’s context, we ask the tape for the gradients of the accumulated loss with respect to the model’s variables. Once we know the gradients, we can have the optimizer apply them to those variables. This &lt;code&gt;variables&lt;/code&gt; slot, by the way, does not (as of this writing) exist in the base implementation of Keras, which is why we have to resort to the TensorFlow implementation.&lt;/p&gt;
&lt;h2 id="inference"&gt;Inference&lt;/h2&gt;
&lt;p&gt;As soon as we have a trained model, we can get translating! Actually, we don’t have to wait. We can integrate a few sample translations directly into the training loop, and watch the network progressing (hopefully!). The &lt;a href="https://github.com/rstudio/keras/tree/master/vignettes/examples/nmt_attention.R"&gt;complete code for this post&lt;/a&gt; does it like this, however here we’re arranging the steps in a more didactical order. The inference loop differs from the training procedure mainly it that it does not use teacher forcing. Instead, we feed back the current prediction as input to the next decoding timestep. The actual predicted word is chosen from the exponentiated raw scores returned by the decoder using a multinomial distribution. We also include a function to plot a heatmap that shows where in the source attention is being directed as the translation is produced.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;evaluate &amp;lt;-
  function(sentence) {
    attention_matrix &amp;lt;-
      matrix(0, nrow = target_maxlen, ncol = src_maxlen)
    
    sentence &amp;lt;- preprocess_sentence(sentence)
    input &amp;lt;- sentence2digits(sentence, src_index)
    input &amp;lt;-
      pad_sequences(list(input), maxlen = src_maxlen,  padding = &amp;quot;post&amp;quot;)
    input &amp;lt;- k_constant(input)
    
    result &amp;lt;- &amp;quot;&amp;quot;
    
    hidden &amp;lt;- k_zeros(c(1, gru_units))
    c(enc_output, enc_hidden) %&amp;lt;-% encoder(list(input, hidden))
    
    dec_hidden &amp;lt;- enc_hidden
    dec_input &amp;lt;-
      k_expand_dims(list(word2index(&amp;quot;&amp;lt;start&amp;gt;&amp;quot;, target_index)))
    
    for (t in seq_len(target_maxlen - 1)) {
      c(preds, dec_hidden, attention_weights) %&amp;lt;-%
        decoder(list(dec_input, dec_hidden, enc_output))
      attention_weights &amp;lt;- k_reshape(attention_weights, c(-1))
      attention_matrix[t, ] &amp;lt;- attention_weights %&amp;gt;% as.double()
      
      pred_idx &amp;lt;-
        tf$multinomial(k_exp(preds), num_samples = 1)[1, 1] %&amp;gt;% as.double()
      pred_word &amp;lt;- index2word(pred_idx, target_index)
      
      if (pred_word == &amp;#39;&amp;lt;stop&amp;gt;&amp;#39;) {
        result &amp;lt;-
          paste0(result, pred_word)
        return (list(result, sentence, attention_matrix))
      } else {
        result &amp;lt;-
          paste0(result, pred_word, &amp;quot; &amp;quot;)
        dec_input &amp;lt;- k_expand_dims(list(pred_idx))
      }
    }
    list(str_trim(result), sentence, attention_matrix)
  }

plot_attention &amp;lt;-
  function(attention_matrix,
           words_sentence,
           words_result) {
    melted &amp;lt;- melt(attention_matrix)
    ggplot(data = melted, aes(
      x = factor(Var2),
      y = factor(Var1),
      fill = value
    )) +
      geom_tile() + scale_fill_viridis() + guides(fill = FALSE) +
      theme(axis.ticks = element_blank()) +
      xlab(&amp;quot;&amp;quot;) +
      ylab(&amp;quot;&amp;quot;) +
      scale_x_discrete(labels = words_sentence, position = &amp;quot;top&amp;quot;) +
      scale_y_discrete(labels = words_result) + 
      theme(aspect.ratio = 1)
  }


translate &amp;lt;- function(sentence) {
  c(result, sentence, attention_matrix) %&amp;lt;-% evaluate(sentence)
  print(paste0(&amp;quot;Input: &amp;quot;,  sentence))
  print(paste0(&amp;quot;Predicted translation: &amp;quot;, result))
  attention_matrix &amp;lt;-
    attention_matrix[1:length(str_split(result, &amp;quot; &amp;quot;)[[1]]),
                     1:length(str_split(sentence, &amp;quot; &amp;quot;)[[1]])]
  plot_attention(attention_matrix,
                 str_split(sentence, &amp;quot; &amp;quot;)[[1]],
                 str_split(result, &amp;quot; &amp;quot;)[[1]])
}&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="learning-to-translate"&gt;Learning to translate&lt;/h2&gt;
&lt;p&gt;Using the &lt;a href="https://github.com/rstudio/keras/tree/master/vignettes/examples/nmt_attention.R"&gt;sample code&lt;/a&gt;, you can see yourself how learning progresses. This is how it worked in our case. (We are always looking at the same sentences - sampled from the training and test sets, respectively - so we can more easily see the evolution.)&lt;/p&gt;
&lt;p&gt;On completion of the very first epoch, our network starts every Dutch sentence with &lt;em&gt;Ik&lt;/em&gt;. No doubt, there must be many sentences starting in the first person in our corpus!&lt;/p&gt;
&lt;p&gt;(Note: these five sentences are all from the training set.)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Input: &amp;lt;start&amp;gt; I did that easily . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Ik . &amp;lt;stop&amp;gt;

Input: &amp;lt;start&amp;gt; Look in the mirror . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Ik . &amp;lt;stop&amp;gt;

Input: &amp;lt;start&amp;gt; Tom wanted revenge . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Ik . &amp;lt;stop&amp;gt;

Input: &amp;lt;start&amp;gt; It s very kind of you . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Ik . &amp;lt;stop&amp;gt;

Input: &amp;lt;start&amp;gt; I refuse to answer . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Ik . &amp;lt;stop&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One epoch later it seems to have picked up common phrases, although their use does not look related to the input. And definitely, it has problems to recognize when it’s over…&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Input: &amp;lt;start&amp;gt; I did that easily . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Ik ben een een een een een een een een een een

Input: &amp;lt;start&amp;gt; Look in the mirror . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Tom is een een een een een een een een een een

Input: &amp;lt;start&amp;gt; Tom wanted revenge . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Tom is een een een een een een een een een een

Input: &amp;lt;start&amp;gt; It s very kind of you . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Ik ben een een een een een een een een een een

Input: &amp;lt;start&amp;gt; I refuse to answer . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Ik ben een een een een een een een een een een&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Jumping ahead to epoch 7, the translations still are completely wrong, but somehow start capturing overall sentence structure (like the imperative in sentence 2).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Input: &amp;lt;start&amp;gt; I did that easily . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Ik heb je niet . &amp;lt;stop&amp;gt;

Input: &amp;lt;start&amp;gt; Look in the mirror . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Ga naar de buurt . &amp;lt;stop&amp;gt;

Input: &amp;lt;start&amp;gt; Tom wanted revenge . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Tom heeft Tom . &amp;lt;stop&amp;gt;

Input: &amp;lt;start&amp;gt; It s very kind of you . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Het is een auto . &amp;lt;stop&amp;gt;

Input: &amp;lt;start&amp;gt; I refuse to answer . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Ik heb de buurt . &amp;lt;stop&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fast forward to epoch 17. Samples from the training set are starting to look better:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Input: &amp;lt;start&amp;gt; I did that easily . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Ik heb dat hij gedaan . &amp;lt;stop&amp;gt;

Input: &amp;lt;start&amp;gt; Look in the mirror . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Kijk in de spiegel . &amp;lt;stop&amp;gt;

Input: &amp;lt;start&amp;gt; Tom wanted revenge . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Tom wilde dood . &amp;lt;stop&amp;gt;

Input: &amp;lt;start&amp;gt; It s very kind of you . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Het is erg goed voor je . &amp;lt;stop&amp;gt;

Input: &amp;lt;start&amp;gt; I refuse to answer . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Ik speel te antwoorden . &amp;lt;stop&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Whereas samples from the test set still look pretty random. Although interestingly, not random in the sense of not having syntactic or semantic structure! &lt;em&gt;Breng de televisie op&lt;/em&gt; is a perfectly reasonable sentence, if not the most lucky translation of &lt;em&gt;Think happy thoughts&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Input: &amp;lt;start&amp;gt; It s entirely my fault . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Het is het mijn woord . &amp;lt;stop&amp;gt;

Input: &amp;lt;start&amp;gt; You re trustworthy . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Je bent net . &amp;lt;stop&amp;gt;

Input: &amp;lt;start&amp;gt; I want to live in Italy . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Ik wil in een leugen . &amp;lt;stop&amp;gt;

Input: &amp;lt;start&amp;gt; He has seven sons . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Hij heeft Frans uit . &amp;lt;stop&amp;gt;

Input: &amp;lt;start&amp;gt; Think happy thoughts . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Breng de televisie op . &amp;lt;stop&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where are we at after 30 epochs? By now, the training samples have been pretty much memorized (the third sentence is suffering from political correctness though, matching &lt;em&gt;Tom wanted revenge&lt;/em&gt; to &lt;em&gt;Tom wilde vrienden&lt;/em&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Input: &amp;lt;start&amp;gt; I did that easily . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Ik heb dat zonder moeite gedaan . &amp;lt;stop&amp;gt;

Input: &amp;lt;start&amp;gt; Look in the mirror . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Kijk in de spiegel . &amp;lt;stop&amp;gt;

Input: &amp;lt;start&amp;gt; Tom wanted revenge . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Tom wilde vrienden . &amp;lt;stop&amp;gt;

Input: &amp;lt;start&amp;gt; It s very kind of you . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Het is erg aardig van je . &amp;lt;stop&amp;gt;

Input: &amp;lt;start&amp;gt; I refuse to answer . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Ik weiger te antwoorden . &amp;lt;stop&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How about the test sentences? They’ve started to look much better. One sentence (&lt;em&gt;Ik wil in Itali leven&lt;/em&gt;) has even been translated entirely correctly. And we see something like the concept of numerals appearing (&lt;em&gt;seven&lt;/em&gt; translated by &lt;em&gt;acht&lt;/em&gt;)…&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Input: &amp;lt;start&amp;gt; It s entirely my fault . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Het is bijna mijn beurt . &amp;lt;stop&amp;gt;

Input: &amp;lt;start&amp;gt; You re trustworthy . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Je bent zo zijn . &amp;lt;stop&amp;gt;

Input: &amp;lt;start&amp;gt; I want to live in Italy . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Ik wil in Itali leven . &amp;lt;stop&amp;gt;

Input: &amp;lt;start&amp;gt; He has seven sons . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Hij heeft acht geleden . &amp;lt;stop&amp;gt;

Input: &amp;lt;start&amp;gt; Think happy thoughts . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Zorg alstublieft goed uit . &amp;lt;stop&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you see it can be quite interesting watching the network’s “language capability” evolve. Now, how about subjecting our network to a little MRI scan? Since we’re collecting the attention weights, we can visualize what part of the source text the decoder is &lt;em&gt;attending to&lt;/em&gt; at every timestep.&lt;/p&gt;
&lt;h2 id="what-is-the-decoder-looking-at"&gt;What is the decoder looking at?&lt;/h2&gt;
&lt;p&gt;First, let’s take an example where word orders in both languages are the same.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Input: &amp;lt;start&amp;gt; It s very kind of you . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Het is erg aardig van je . &amp;lt;stop&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-07-30-attention-layer/images/train-4.png" width="500" /&gt;&lt;/p&gt;
&lt;p&gt;We see that overall, given a sample where respective sentences align very well, the decoder pretty much looks where it is supposed to. Let’s pick something a little more complicated.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Input: &amp;lt;start&amp;gt; I did that easily . &amp;lt;stop&amp;gt;&amp;quot;
Predicted translation: &amp;lt;start&amp;gt; Ik heb dat zonder moeite gedaan . &amp;lt;stop&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The translation is correct, but word order in both languages isn’t the same here: &lt;em&gt;did&lt;/em&gt; corresponds to the analytic perfect &lt;em&gt;heb … gedaan&lt;/em&gt;. Will we be able to see that in the attention plot?&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-07-30-attention-layer/images/train-1.png" width="500" /&gt;&lt;/p&gt;
&lt;p&gt;The answer is no. It would be interesting to check again after training for a couple more epochs.&lt;/p&gt;
&lt;p&gt;Finally, let’s investigate this translation from the test set (which is entirely correct):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Input: &amp;lt;start&amp;gt; I want to live in Italy . &amp;lt;stop&amp;gt;
Predicted translation: &amp;lt;start&amp;gt; Ik wil in Itali leven . &amp;lt;stop&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-07-30-attention-layer/images/validation-3.png" width="500" /&gt;&lt;/p&gt;
&lt;p&gt;These two sentences don’t align well. We see that Dutch &lt;em&gt;in&lt;/em&gt; correctly picks English &lt;em&gt;in&lt;/em&gt; (skipping over &lt;em&gt;to live&lt;/em&gt;), then &lt;em&gt;Itali&lt;/em&gt; attends to &lt;em&gt;Italy&lt;/em&gt;. Finally &lt;em&gt;leven&lt;/em&gt; is produced without us witnessing the decoder looking back to &lt;em&gt;live&lt;/em&gt;. Here again, it would be interesting to watch what happens a few epochs later!&lt;/p&gt;
&lt;h2 id="next-up"&gt;Next up&lt;/h2&gt;
&lt;p&gt;There are many ways to go from here. For one, we didn’t do any hyperparameter optimization. (See e.g. &lt;span class="citation"&gt;(Luong, Pham, and Manning 2015)&lt;/span&gt; for an extensive experiment on architectures and hyperparameters for NMT.) Second, provided you have access to the required hardware, you might be curious how good an algorithm like this can get when trained on a real big dataset, using a real big network. Third, alternative attention mechanisms have been suggested (see e.g. &lt;a href="https://github.com/lmthang/thesis/blob/master/thesis.pdf"&gt;T. Luong’s thesis&lt;/a&gt; which we followed rather closely in the description of attention above).&lt;/p&gt;
&lt;p&gt;Last not least, no one said attention need be useful only in the context of machine translation. Out there, a plenty of sequence prediction (time series) problems are waiting to be explored with respect to its potential usefulness…&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references hanging-indent"&gt;
&lt;div id="ref-BahdanauCB14"&gt;
&lt;p&gt;Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural Machine Translation by Jointly Learning to Align and Translate.” &lt;em&gt;CoRR&lt;/em&gt; abs/1409.0473. &lt;a href="http://arxiv.org/abs/1409.0473"&gt;http://arxiv.org/abs/1409.0473&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-LuongPM15"&gt;
&lt;p&gt;Luong, Minh-Thang, Hieu Pham, and Christopher D. Manning. 2015. “Effective Approaches to Attention-Based Neural Machine Translation.” &lt;em&gt;CoRR&lt;/em&gt; abs/1508.04025. &lt;a href="http://arxiv.org/abs/1508.04025"&gt;http://arxiv.org/abs/1508.04025&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-VinyalsKKPSH14"&gt;
&lt;p&gt;Vinyals, Oriol, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey E. Hinton. 2014. “Grammar as a Foreign Language.” &lt;em&gt;CoRR&lt;/em&gt; abs/1412.7449. &lt;a href="http://arxiv.org/abs/1412.7449"&gt;http://arxiv.org/abs/1412.7449&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-XuBKCCSZB15"&gt;
&lt;p&gt;Xu, Kelvin, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.” &lt;em&gt;CoRR&lt;/em&gt; abs/1502.03044. &lt;a href="http://arxiv.org/abs/1502.03044"&gt;http://arxiv.org/abs/1502.03044&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">a937254c9bdc59248a0904ad4b0212f7</distill:md5>
      <category>Natural Language Processing</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-07-30-attention-layer</guid>
      <pubDate>Mon, 30 Jul 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-07-30-attention-layer/images/attention.png" medium="image" type="image/png" width="606" height="448"/>
    </item>
    <item>
      <title>Predicting Sunspot Frequency with Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Matt Dancho</dc:creator>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-06-25-sunspots-lstm</link>
      <description>


&lt;h2 id="forecasting-sunspots-with-deep-learning"&gt;Forecasting sunspots with deep learning&lt;/h2&gt;
&lt;p&gt;In this post we will examine making time series predictions using the &lt;a href="https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/sunspot.month.html"&gt;sunspots&lt;/a&gt; dataset that ships with base R. Sunspots are dark spots on the sun, associated with lower temperature. Here’s an image from NASA showing the solar phenomenon.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-06-25-sunspots-lstm/images/sunspot_nasa.jpg" class="external" style="width:100.0%" alt="" /&gt;
&lt;p class="caption"&gt;Figure from &lt;a href="https://www.nasa.gov/content/goddard/largest-sunspot-of-solar-cycle" class="uri"&gt;https://www.nasa.gov/content/goddard/largest-sunspot-of-solar-cycle&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We’re using the monthly version of the dataset, &lt;code&gt;sunspots.month&lt;/code&gt; (there is a yearly version, too). It contains 265 years worth of data (from 1749 through 2013) on the number of sunspots per month.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-06-25-sunspots-lstm/images/sunspots_full.png" width="352" /&gt;&lt;/p&gt;
&lt;p&gt;Forecasting this dataset is challenging because of high short term variability as well as long-term irregularities evident in the cycles. For example, maximum amplitudes reached by the low frequency cycle differ a lot, as does the number of high frequency cycle steps needed to reach that maximum low frequency cycle height.&lt;/p&gt;
&lt;p&gt;Our post will focus on two dominant aspects: how to apply deep learning to time series forecasting, and how to properly apply cross validation in this domain. For the latter, we will use the &lt;a href="https://cran.r-project.org/package=rsample"&gt;rsample&lt;/a&gt; package that allows to do resampling on time series data. As to the former, our goal is not to reach utmost performance but to show the general course of action when using recurrent neural networks to model this kind of data.&lt;/p&gt;
&lt;h2 id="recurrent-neural-networks"&gt;Recurrent neural networks&lt;/h2&gt;
&lt;p&gt;When our data has a sequential structure, it is recurrent neural networks (RNNs) we use to model it.&lt;/p&gt;
&lt;p&gt;As of today, among RNNs, the best established architectures are the GRU (Gated Recurrent Unit) and the LSTM (Long Short Term Memory). For today, let’s not zoom in on what makes them special, but on what they have in common with the most stripped-down RNN: the basic recurrence structure.&lt;/p&gt;
&lt;p&gt;In contrast to the prototype of a neural network, often called Multilayer Perceptron (MLP), the RNN has a state that is carried on over time. This is nicely seen in this diagram from &lt;a href="http://www.deeplearningbook.org"&gt;Goodfellow et al.&lt;/a&gt;, a.k.a. the “bible of deep learning”:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-06-25-sunspots-lstm/images/rnn.png" class="external" alt="" /&gt;
&lt;p class="caption"&gt;Figure from: &lt;a href="http://www.deeplearningbook.org" class="uri"&gt;http://www.deeplearningbook.org&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;At each time, the state is a combination of the current input and the previous hidden state. This is reminiscent of autoregressive models, but with neural networks, there has to be some point where we halt the dependence.&lt;/p&gt;
&lt;p&gt;That’s because in order to determine the weights, we keep calculating how our loss changes as the input changes. Now if the input we have to consider, at an arbitrary timestep, ranges back indefinitely - then we will not be able to calculate all those gradients. In practice, then, our hidden state will, at every iteration, be carried forward through a fixed number of steps.&lt;/p&gt;
&lt;p&gt;We’ll come back to that as soon as we’ve loaded and pre-processed the data.&lt;/p&gt;
&lt;h2 id="setup-pre-processing-and-exploration"&gt;Setup, pre-processing, and exploration&lt;/h2&gt;
&lt;h3 id="libraries"&gt;Libraries&lt;/h3&gt;
&lt;p&gt;Here, first, are the libraries needed for this tutorial.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Core Tidyverse
library(tidyverse)
library(glue)
library(forcats)

# Time Series
library(timetk)
library(tidyquant)
library(tibbletime)

# Visualization
library(cowplot)

# Preprocessing
library(recipes)

# Sampling / Accuracy
library(rsample)
library(yardstick) 

# Modeling
library(keras)
library(tfruns)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you have not previously run Keras in R, you will need to install Keras using the &lt;code&gt;install_keras()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Install Keras if you have not installed before
install_keras()&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="data"&gt;Data&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;sunspot.month&lt;/code&gt; is a &lt;code&gt;ts&lt;/code&gt; class (not tidy), so we’ll convert to a tidy data set using the &lt;code&gt;tk_tbl()&lt;/code&gt; function from &lt;code&gt;timetk&lt;/code&gt;. We use this instead of &lt;code&gt;as.tibble()&lt;/code&gt; from &lt;code&gt;tibble&lt;/code&gt; to automatically preserve the time series index as a &lt;code&gt;zoo&lt;/code&gt; &lt;code&gt;yearmon&lt;/code&gt; index. Last, we’ll convert the &lt;code&gt;zoo&lt;/code&gt; index to date using &lt;code&gt;lubridate::as_date()&lt;/code&gt; (loaded with &lt;code&gt;tidyquant&lt;/code&gt;) and then change to a &lt;code&gt;tbl_time&lt;/code&gt; object to make time series operations easier.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;sun_spots &amp;lt;- datasets::sunspot.month %&amp;gt;%
    tk_tbl() %&amp;gt;%
    mutate(index = as_date(index)) %&amp;gt;%
    as_tbl_time(index = index)

sun_spots&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A time tibble: 3,177 x 2
# Index: index
   index      value
   &amp;lt;date&amp;gt;     &amp;lt;dbl&amp;gt;
 1 1749-01-01  58  
 2 1749-02-01  62.6
 3 1749-03-01  70  
 4 1749-04-01  55.7
 5 1749-05-01  85  
 6 1749-06-01  83.5
 7 1749-07-01  94.8
 8 1749-08-01  66.3
 9 1749-09-01  75.9
10 1749-10-01  75.5
# ... with 3,167 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="exploratory-data-analysis"&gt;Exploratory data analysis&lt;/h3&gt;
&lt;p&gt;The time series is long (265 years!). We can visualize the time series both in full, and zoomed in on the first 10 years to get a feel for the series.&lt;/p&gt;
&lt;h4 id="visualizing-sunspot-data-with-cowplot"&gt;Visualizing sunspot data with cowplot&lt;/h4&gt;
&lt;p&gt;We’ll make two &lt;code&gt;ggplot&lt;/code&gt;s and combine them using &lt;code&gt;cowplot::plot_grid()&lt;/code&gt;. Note that for the zoomed in plot, we make use of &lt;code&gt;tibbletime::time_filter()&lt;/code&gt;, which is an easy way to perform time-based filtering.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;p1 &amp;lt;- sun_spots %&amp;gt;%
    ggplot(aes(index, value)) +
    geom_point(color = palette_light()[[1]], alpha = 0.5) +
    theme_tq() +
    labs(
        title = &amp;quot;From 1749 to 2013 (Full Data Set)&amp;quot;
    )

p2 &amp;lt;- sun_spots %&amp;gt;%
    filter_time(&amp;quot;start&amp;quot; ~ &amp;quot;1800&amp;quot;) %&amp;gt;%
    ggplot(aes(index, value)) +
    geom_line(color = palette_light()[[1]], alpha = 0.5) +
    geom_point(color = palette_light()[[1]]) +
    geom_smooth(method = &amp;quot;loess&amp;quot;, span = 0.2, se = FALSE) +
    theme_tq() +
    labs(
        title = &amp;quot;1749 to 1759 (Zoomed In To Show Changes over the Year)&amp;quot;,
        caption = &amp;quot;datasets::sunspot.month&amp;quot;
    )

p_title &amp;lt;- ggdraw() + 
  draw_label(&amp;quot;Sunspots&amp;quot;, size = 18, fontface = &amp;quot;bold&amp;quot;, 
             colour = palette_light()[[1]])

plot_grid(p_title, p1, p2, ncol = 1, rel_heights = c(0.1, 1, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-06-25-sunspots-lstm/images/cowplot.png" width="352" /&gt;&lt;/p&gt;
&lt;h3 id="backtesting-time-series-cross-validation"&gt;Backtesting: time series cross validation&lt;/h3&gt;
&lt;p&gt;When doing cross validation on sequential data, the time dependencies on preceding samples must be preserved. We can create a cross validation sampling plan by offsetting the window used to select sequential sub-samples. In essence, we’re creatively dealing with the fact that there’s no future test data available by creating multiple synthetic “futures” - a process often, esp. in finance, called “backtesting”.&lt;/p&gt;
&lt;p&gt;As mentioned in the introduction, the &lt;a href="https://cran.r-project.org/package=rsample"&gt;rsample&lt;/a&gt; package includes facitlities for backtesting on time series. The vignette, &lt;a href="https://tidymodels.github.io/rsample/articles/Applications/Time_Series.html"&gt;“Time Series Analysis Example”&lt;/a&gt;, describes a procedure that uses the &lt;code&gt;rolling_origin()&lt;/code&gt; function to create samples designed for time series cross validation. We’ll use this approach.&lt;/p&gt;
&lt;h4 id="developing-a-backtesting-strategy"&gt;Developing a backtesting strategy&lt;/h4&gt;
&lt;p&gt;The sampling plan we create uses 100 years (&lt;code&gt;initial&lt;/code&gt; = 12 x 100 samples) for the training set and 50 years (&lt;code&gt;assess&lt;/code&gt; = 12 x 50) for the testing (validation) set. We select a &lt;code&gt;skip&lt;/code&gt; span of about 22 years (&lt;code&gt;skip&lt;/code&gt; = 12 x 22 - 1) to approximately evenly distribute the samples into 6 sets that span the entire 265 years of sunspots history. Last, we select &lt;code&gt;cumulative = FALSE&lt;/code&gt; to allow the origin to shift which ensures that models on more recent data are not given an unfair advantage (more observations) over those operating on less recent data. The tibble return contains the &lt;code&gt;rolling_origin_resamples&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;periods_train &amp;lt;- 12 * 100
periods_test  &amp;lt;- 12 * 50
skip_span     &amp;lt;- 12 * 22 - 1

rolling_origin_resamples &amp;lt;- rolling_origin(
  sun_spots,
  initial    = periods_train,
  assess     = periods_test,
  cumulative = FALSE,
  skip       = skip_span
)

rolling_origin_resamples&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Rolling origin forecast resampling 
# A tibble: 6 x 2
  splits       id    
  &amp;lt;list&amp;gt;       &amp;lt;chr&amp;gt; 
1 &amp;lt;S3: rsplit&amp;gt; Slice1
2 &amp;lt;S3: rsplit&amp;gt; Slice2
3 &amp;lt;S3: rsplit&amp;gt; Slice3
4 &amp;lt;S3: rsplit&amp;gt; Slice4
5 &amp;lt;S3: rsplit&amp;gt; Slice5
6 &amp;lt;S3: rsplit&amp;gt; Slice6&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="visualizing-the-backtesting-strategy"&gt;Visualizing the backtesting strategy&lt;/h4&gt;
&lt;p&gt;We can visualize the resamples with two custom functions. The first, &lt;code&gt;plot_split()&lt;/code&gt;, plots one of the resampling splits using &lt;code&gt;ggplot2&lt;/code&gt;. Note that an &lt;code&gt;expand_y_axis&lt;/code&gt; argument is added to expand the date range to the full &lt;code&gt;sun_spots&lt;/code&gt; dataset date range. This will become useful when we visualize all plots together.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Plotting function for a single split
plot_split &amp;lt;- function(split, expand_y_axis = TRUE, 
                       alpha = 1, size = 1, base_size = 14) {
    
    # Manipulate data
    train_tbl &amp;lt;- training(split) %&amp;gt;%
        add_column(key = &amp;quot;training&amp;quot;) 
    
    test_tbl  &amp;lt;- testing(split) %&amp;gt;%
        add_column(key = &amp;quot;testing&amp;quot;) 
    
    data_manipulated &amp;lt;- bind_rows(train_tbl, test_tbl) %&amp;gt;%
        as_tbl_time(index = index) %&amp;gt;%
        mutate(key = fct_relevel(key, &amp;quot;training&amp;quot;, &amp;quot;testing&amp;quot;))
        
    # Collect attributes
    train_time_summary &amp;lt;- train_tbl %&amp;gt;%
        tk_index() %&amp;gt;%
        tk_get_timeseries_summary()
    
    test_time_summary &amp;lt;- test_tbl %&amp;gt;%
        tk_index() %&amp;gt;%
        tk_get_timeseries_summary()
    
    # Visualize
    g &amp;lt;- data_manipulated %&amp;gt;%
        ggplot(aes(x = index, y = value, color = key)) +
        geom_line(size = size, alpha = alpha) +
        theme_tq(base_size = base_size) +
        scale_color_tq() +
        labs(
          title    = glue(&amp;quot;Split: {split$id}&amp;quot;),
          subtitle = glue(&amp;quot;{train_time_summary$start} to &amp;quot;, 
                          &amp;quot;{test_time_summary$end}&amp;quot;),
            y = &amp;quot;&amp;quot;, x = &amp;quot;&amp;quot;
        ) +
        theme(legend.position = &amp;quot;none&amp;quot;) 
    
    if (expand_y_axis) {
        
        sun_spots_time_summary &amp;lt;- sun_spots %&amp;gt;% 
            tk_index() %&amp;gt;% 
            tk_get_timeseries_summary()
        
        g &amp;lt;- g +
            scale_x_date(limits = c(sun_spots_time_summary$start, 
                                    sun_spots_time_summary$end))
    }
    
    g
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;plot_split()&lt;/code&gt; function takes one split (in this case Slice01), and returns a visual of the sampling strategy. We expand the axis to the range for the full dataset using &lt;code&gt;expand_y_axis = TRUE&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;rolling_origin_resamples$splits[[1]] %&amp;gt;%
    plot_split(expand_y_axis = TRUE) +
    theme(legend.position = &amp;quot;bottom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-06-25-sunspots-lstm/images/slice1.png" width="262" /&gt;&lt;/p&gt;
&lt;p&gt;The second function, &lt;code&gt;plot_sampling_plan()&lt;/code&gt;, scales the &lt;code&gt;plot_split()&lt;/code&gt; function to all of the samples using &lt;code&gt;purrr&lt;/code&gt; and &lt;code&gt;cowplot&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Plotting function that scales to all splits 
plot_sampling_plan &amp;lt;- function(sampling_tbl, expand_y_axis = TRUE, 
                               ncol = 3, alpha = 1, size = 1, base_size = 14, 
                               title = &amp;quot;Sampling Plan&amp;quot;) {
    
    # Map plot_split() to sampling_tbl
    sampling_tbl_with_plots &amp;lt;- sampling_tbl %&amp;gt;%
        mutate(gg_plots = map(splits, plot_split, 
                              expand_y_axis = expand_y_axis,
                              alpha = alpha, base_size = base_size))
    
    # Make plots with cowplot
    plot_list &amp;lt;- sampling_tbl_with_plots$gg_plots 
    
    p_temp &amp;lt;- plot_list[[1]] + theme(legend.position = &amp;quot;bottom&amp;quot;)
    legend &amp;lt;- get_legend(p_temp)
    
    p_body  &amp;lt;- plot_grid(plotlist = plot_list, ncol = ncol)
    
    p_title &amp;lt;- ggdraw() + 
        draw_label(title, size = 14, fontface = &amp;quot;bold&amp;quot;, 
                   colour = palette_light()[[1]])
    
    g &amp;lt;- plot_grid(p_title, p_body, legend, ncol = 1, 
                   rel_heights = c(0.05, 1, 0.05))
    
    g
    
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now visualize the entire backtesting strategy with &lt;code&gt;plot_sampling_plan()&lt;/code&gt;. We can see how the sampling plan shifts the sampling window with each progressive slice of the train/test splits.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;rolling_origin_resamples %&amp;gt;%
    plot_sampling_plan(expand_y_axis = T, ncol = 3, alpha = 1, size = 1, base_size = 10, 
                       title = &amp;quot;Backtesting Strategy: Rolling Origin Sampling Plan&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-06-25-sunspots-lstm/images/all_splits.png" width="400" /&gt;&lt;/p&gt;
&lt;p&gt;And, we can set &lt;code&gt;expand_y_axis = FALSE&lt;/code&gt; to zoom in on the samples.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;rolling_origin_resamples %&amp;gt;%
    plot_sampling_plan(expand_y_axis = F, ncol = 3, alpha = 1, size = 1, base_size = 10, 
                       title = &amp;quot;Backtesting Strategy: Zoomed In&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-06-25-sunspots-lstm/images/all_splits_zoomed.png" width="400" /&gt;&lt;/p&gt;
&lt;p&gt;We’ll use this backtesting strategy (6 samples from one time series each with 50/10 split in years and a ~20 year offset) when testing the veracity of the LSTM model on the sunspots dataset.&lt;/p&gt;
&lt;h2 id="the-lstm-model"&gt;The LSTM model&lt;/h2&gt;
&lt;p&gt;To begin, we’ll develop an LSTM model on a single sample from the backtesting strategy, namely, the most recent slice. We’ll then apply the model to all samples to investigate modeling performance.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;example_split    &amp;lt;- rolling_origin_resamples$splits[[6]]
example_split_id &amp;lt;- rolling_origin_resamples$id[[6]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can reuse the &lt;code&gt;plot_split()&lt;/code&gt; function to visualize the split. Set &lt;code&gt;expand_y_axis = FALSE&lt;/code&gt; to zoom in on the subsample.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot_split(example_split, expand_y_axis = FALSE, size = 0.5) +
    theme(legend.position = &amp;quot;bottom&amp;quot;) +
    ggtitle(glue(&amp;quot;Split: {example_split_id}&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-06-25-sunspots-lstm/images/slice6.png" /&gt;&lt;/p&gt;
&lt;h3 id="data-setup"&gt;Data setup&lt;/h3&gt;
&lt;p&gt;To aid hyperparameter tuning, besides the training set we also need a validation set. For example, we will use a callback, &lt;code&gt;callback_early_stopping&lt;/code&gt;, that stops training when no significant performance is seen on the validation set (what’s considered significant is up to you).&lt;/p&gt;
&lt;p&gt;We will dedicate 2 thirds of the analysis set to training, and 1 third to validation.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;df_trn &amp;lt;- analysis(example_split)[1:800, , drop = FALSE]
df_val &amp;lt;- analysis(example_split)[801:1200, , drop = FALSE]
df_tst &amp;lt;- assessment(example_split)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, let’s combine the training and testing data sets into a single data set with a column &lt;code&gt;key&lt;/code&gt; that specifies where they came from (either “training” or “testing)”. Note that the &lt;code&gt;tbl_time&lt;/code&gt; object will need to have the index respecified during the &lt;code&gt;bind_rows()&lt;/code&gt; step, but &lt;a href="https://github.com/tidyverse/dplyr/issues/3259"&gt;this issue&lt;/a&gt; should be corrected in &lt;code&gt;dplyr&lt;/code&gt; soon.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;df &amp;lt;- bind_rows(
  df_trn %&amp;gt;% add_column(key = &amp;quot;training&amp;quot;),
  df_val %&amp;gt;% add_column(key = &amp;quot;validation&amp;quot;),
  df_tst %&amp;gt;% add_column(key = &amp;quot;testing&amp;quot;)
) %&amp;gt;%
  as_tbl_time(index = index)

df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A time tibble: 1,800 x 3
# Index: index
   index      value key     
   &amp;lt;date&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   
 1 1849-06-01  81.1 training
 2 1849-07-01  78   training
 3 1849-08-01  67.7 training
 4 1849-09-01  93.7 training
 5 1849-10-01  71.5 training
 6 1849-11-01  99   training
 7 1849-12-01  97   training
 8 1850-01-01  78   training
 9 1850-02-01  89.4 training
10 1850-03-01  82.6 training
# ... with 1,790 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="preprocessing-with-recipes"&gt;Preprocessing with recipes&lt;/h3&gt;
&lt;p&gt;The LSTM algorithm will usually work better if the input data has been centered and scaled. We can conveniently accomplish this using the &lt;code&gt;recipes&lt;/code&gt; package. In addition to &lt;code&gt;step_center&lt;/code&gt; and &lt;code&gt;step_scale&lt;/code&gt;, we’re using &lt;code&gt;step_sqrt&lt;/code&gt; to reduce variance and remov outliers. The actual transformations are executed when we &lt;code&gt;bake&lt;/code&gt; the data according to the recipe:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;rec_obj &amp;lt;- recipe(value ~ ., df) %&amp;gt;%
    step_sqrt(value) %&amp;gt;%
    step_center(value) %&amp;gt;%
    step_scale(value) %&amp;gt;%
    prep()

df_processed_tbl &amp;lt;- bake(rec_obj, df)

df_processed_tbl&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1,800 x 3
   index      value key     
   &amp;lt;date&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;   
 1 1849-06-01 0.714 training
 2 1849-07-01 0.660 training
 3 1849-08-01 0.473 training
 4 1849-09-01 0.922 training
 5 1849-10-01 0.544 training
 6 1849-11-01 1.01  training
 7 1849-12-01 0.974 training
 8 1850-01-01 0.660 training
 9 1850-02-01 0.852 training
10 1850-03-01 0.739 training
# ... with 1,790 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, let’s capture the original center and scale so we can invert the steps after modeling. The square root step can then simply be undone by squaring the back-transformed data.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;center_history &amp;lt;- rec_obj$steps[[2]]$means[&amp;quot;value&amp;quot;]
scale_history  &amp;lt;- rec_obj$steps[[3]]$sds[&amp;quot;value&amp;quot;]

c(&amp;quot;center&amp;quot; = center_history, &amp;quot;scale&amp;quot; = scale_history)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;center.value  scale.value 
    6.694468     3.238935 &lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="reshaping-the-data"&gt;Reshaping the data&lt;/h3&gt;
&lt;p&gt;Keras LSTM expects the input as well as the target data to be in a specific shape. The input has to be a 3-d array of size &lt;code&gt;num_samples, num_timesteps, num_features&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here, &lt;code&gt;num_samples&lt;/code&gt; is the number of observations in the set. This will get fed to the model in portions of &lt;code&gt;batch_size&lt;/code&gt;. The second dimension, &lt;code&gt;num_timesteps&lt;/code&gt;, is the length of the hidden state we were talking about above. Finally, the third dimension is the number of predictors we’re using. For univariate time series, this is 1.&lt;/p&gt;
&lt;p&gt;How long should we choose the hidden state to be? This generally depends on the dataset and our goal. If we did one-step-ahead forecasts - thus, forecasting the following month only - our main concern would be choosing a state length that allows to learn any patterns present in the data.&lt;/p&gt;
&lt;p&gt;Now say we wanted to forecast 12 months instead, as does &lt;a href="http://sidc.be/silso/home"&gt;SILSO&lt;/a&gt;, the &lt;em&gt;World Data Center for the production, preservation and dissemination of the international sunspot number&lt;/em&gt;. The way we can do this, with Keras, is by wiring the LSTM hidden states to sets of consecutive outputs of the same length. Thus, if we want to produce predictions for 12 months, our LSTM should have a hidden state length of 12.&lt;/p&gt;
&lt;p&gt;These 12 time steps will then get wired to 12 linear predictor units using a &lt;code&gt;time_distributed()&lt;/code&gt; wrapper. That wrapper’s task is to apply the same calculation (i.e., the same weight matrix) to every state input it receives.&lt;/p&gt;
&lt;p&gt;Now, what’s the target array’s format supposed to be? As we’re forecasting several timesteps here, the target data again needs to be 3-dimensional. Dimension 1 again is the batch dimension, dimension 2 again corresponds to the number of timesteps (the forecasted ones), and dimension 3 is the size of the wrapped layer. In our case, the wrapped layer is a &lt;code&gt;layer_dense()&lt;/code&gt; of a single unit, as we want exactly one prediction per point in time.&lt;/p&gt;
&lt;p&gt;So, let’s reshape the data. The main action here is creating the sliding windows of 12 steps of input, followed by 12 steps of output each. This is easiest to understand with a shorter and simpler example. Say our input were the numbers from 1 to 10, and our chosen sequence length (state size) were 4. Tthis is how we would want our training input to look:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1,2,3,4
2,3,4,5
3,4,5,6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And our target data, correspondingly:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;5,6,7,8
6,7,8,9
7,8,9,10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll define a short function that does this reshaping on a given dataset. Then finally, we add the third axis that is formally needed (even though that axis is of size 1 in our case).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# these variables are being defined just because of the order in which
# we present things in this post (first the data, then the model)
# they will be superseded by FLAGS$n_timesteps, FLAGS$batch_size and n_predictions
# in the following snippet
n_timesteps &amp;lt;- 12
n_predictions &amp;lt;- n_timesteps
batch_size &amp;lt;- 10

# functions used
build_matrix &amp;lt;- function(tseries, overall_timesteps) {
  t(sapply(1:(length(tseries) - overall_timesteps + 1), function(x) 
    tseries[x:(x + overall_timesteps - 1)]))
}

reshape_X_3d &amp;lt;- function(X) {
  dim(X) &amp;lt;- c(dim(X)[1], dim(X)[2], 1)
  X
}

# extract values from data frame
train_vals &amp;lt;- df_processed_tbl %&amp;gt;%
  filter(key == &amp;quot;training&amp;quot;) %&amp;gt;%
  select(value) %&amp;gt;%
  pull()
valid_vals &amp;lt;- df_processed_tbl %&amp;gt;%
  filter(key == &amp;quot;validation&amp;quot;) %&amp;gt;%
  select(value) %&amp;gt;%
  pull()
test_vals &amp;lt;- df_processed_tbl %&amp;gt;%
  filter(key == &amp;quot;testing&amp;quot;) %&amp;gt;%
  select(value) %&amp;gt;%
  pull()


# build the windowed matrices
train_matrix &amp;lt;-
  build_matrix(train_vals, n_timesteps + n_predictions)
valid_matrix &amp;lt;-
  build_matrix(valid_vals, n_timesteps + n_predictions)
test_matrix &amp;lt;- build_matrix(test_vals, n_timesteps + n_predictions)

# separate matrices into training and testing parts
# also, discard last batch if there are fewer than batch_size samples
# (a purely technical requirement)
X_train &amp;lt;- train_matrix[, 1:n_timesteps]
y_train &amp;lt;- train_matrix[, (n_timesteps + 1):(n_timesteps * 2)]
X_train &amp;lt;- X_train[1:(nrow(X_train) %/% batch_size * batch_size), ]
y_train &amp;lt;- y_train[1:(nrow(y_train) %/% batch_size * batch_size), ]

X_valid &amp;lt;- valid_matrix[, 1:n_timesteps]
y_valid &amp;lt;- valid_matrix[, (n_timesteps + 1):(n_timesteps * 2)]
X_valid &amp;lt;- X_valid[1:(nrow(X_valid) %/% batch_size * batch_size), ]
y_valid &amp;lt;- y_valid[1:(nrow(y_valid) %/% batch_size * batch_size), ]

X_test &amp;lt;- test_matrix[, 1:n_timesteps]
y_test &amp;lt;- test_matrix[, (n_timesteps + 1):(n_timesteps * 2)]
X_test &amp;lt;- X_test[1:(nrow(X_test) %/% batch_size * batch_size), ]
y_test &amp;lt;- y_test[1:(nrow(y_test) %/% batch_size * batch_size), ]
# add on the required third axis
X_train &amp;lt;- reshape_X_3d(X_train)
X_valid &amp;lt;- reshape_X_3d(X_valid)
X_test &amp;lt;- reshape_X_3d(X_test)

y_train &amp;lt;- reshape_X_3d(y_train)
y_valid &amp;lt;- reshape_X_3d(y_valid)
y_test &amp;lt;- reshape_X_3d(y_test)&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="building-the-lstm-model"&gt;Building the LSTM model&lt;/h3&gt;
&lt;p&gt;Now that we have our data in the required form, let’s finally build the model. As always in deep learning, an important, and often time-consuming, part of the job is tuning hyperparameters. To keep this post self-contained, and considering this is primarily a tutorial on how to use LSTM in R, let’s assume the following settings were found after extensive experimentation (in reality experimentation &lt;em&gt;did&lt;/em&gt; take place, but not to a degree that performance couldn’t possibly be improved).&lt;/p&gt;
&lt;p&gt;Instead of hard coding the hyperparameters, we’ll use &lt;a href="https://tensorflow.rstudio.com/tools/tfruns/articles/tuning.html"&gt;tfruns&lt;/a&gt; to set up an environment where we could easily perform grid search.&lt;/p&gt;
&lt;p&gt;We’ll quickly comment on what these parameters do but mainly leave those topics to further posts.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;FLAGS &amp;lt;- flags(
  # There is a so-called &amp;quot;stateful LSTM&amp;quot; in Keras. While LSTM is stateful
  # per se, this adds a further tweak where the hidden states get 
  # initialized with values from the item at same position in the previous
  # batch. This is helpful just under specific circumstances, or if you want
  # to create an &amp;quot;infinite stream&amp;quot; of states, in which case you&amp;#39;d use 1 as 
  # the batch size. Below, we show how the code would have to be changed to
  # use this, but it won&amp;#39;t be further discussed here.
  flag_boolean(&amp;quot;stateful&amp;quot;, FALSE),
  # Should we use several layers of LSTM?
  # Again, just included for completeness, it did not yield any superior 
  # performance on this task.
  # This will actually stack exactly one additional layer of LSTM units.
  flag_boolean(&amp;quot;stack_layers&amp;quot;, FALSE),
  # number of samples fed to the model in one go
  flag_integer(&amp;quot;batch_size&amp;quot;, 10),
  # size of the hidden state, equals size of predictions
  flag_integer(&amp;quot;n_timesteps&amp;quot;, 12),
  # how many epochs to train for
  flag_integer(&amp;quot;n_epochs&amp;quot;, 100),
  # fraction of the units to drop for the linear transformation of the inputs
  flag_numeric(&amp;quot;dropout&amp;quot;, 0.2),
  # fraction of the units to drop for the linear transformation of the 
  # recurrent state
  flag_numeric(&amp;quot;recurrent_dropout&amp;quot;, 0.2),
  # loss function. Found to work better for this specific case than mean
  # squared error
  flag_string(&amp;quot;loss&amp;quot;, &amp;quot;logcosh&amp;quot;),
  # optimizer = stochastic gradient descent. Seemed to work better than adam 
  # or rmsprop here (as indicated by limited testing)
  flag_string(&amp;quot;optimizer_type&amp;quot;, &amp;quot;sgd&amp;quot;),
  # size of the LSTM layer
  flag_integer(&amp;quot;n_units&amp;quot;, 128),
  # learning rate
  flag_numeric(&amp;quot;lr&amp;quot;, 0.003),
  # momentum, an additional parameter to the SGD optimizer
  flag_numeric(&amp;quot;momentum&amp;quot;, 0.9),
  # parameter to the early stopping callback
  flag_integer(&amp;quot;patience&amp;quot;, 10)
)

# the number of predictions we&amp;#39;ll make equals the length of the hidden state
n_predictions &amp;lt;- FLAGS$n_timesteps
# how many features = predictors we have
n_features &amp;lt;- 1
# just in case we wanted to try different optimizers, we could add here
optimizer &amp;lt;- switch(FLAGS$optimizer_type,
                    sgd = optimizer_sgd(lr = FLAGS$lr, 
                                        momentum = FLAGS$momentum)
                    )

# callbacks to be passed to the fit() function
# We just use one here: we may stop before n_epochs if the loss on the
# validation set does not decrease (by a configurable amount, over a 
# configurable time)
callbacks &amp;lt;- list(
  callback_early_stopping(patience = FLAGS$patience)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After all these preparations, the code for constructing and training the model is rather short! Let’s first quickly view the “long version”, that would allow you to test stacking several LSTMs or use a stateful LSTM, then go through the final short version (that does neither) and comment on it.&lt;/p&gt;
&lt;p&gt;This, just for reference, is the complete code.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential()

model %&amp;gt;%
  layer_lstm(
    units = FLAGS$n_units,
    batch_input_shape = c(FLAGS$batch_size, FLAGS$n_timesteps, n_features),
    dropout = FLAGS$dropout,
    recurrent_dropout = FLAGS$recurrent_dropout,
    return_sequences = TRUE,
    stateful = FLAGS$stateful
  )

if (FLAGS$stack_layers) {
  model %&amp;gt;%
    layer_lstm(
      units = FLAGS$n_units,
      dropout = FLAGS$dropout,
      recurrent_dropout = FLAGS$recurrent_dropout,
      return_sequences = TRUE,
      stateful = FLAGS$stateful
    )
}
model %&amp;gt;% time_distributed(layer_dense(units = 1))

model %&amp;gt;%
  compile(
    loss = FLAGS$loss,
    optimizer = optimizer,
    metrics = list(&amp;quot;mean_squared_error&amp;quot;)
  )

if (!FLAGS$stateful) {
  model %&amp;gt;% fit(
    x          = X_train,
    y          = y_train,
    validation_data = list(X_valid, y_valid),
    batch_size = FLAGS$batch_size,
    epochs     = FLAGS$n_epochs,
    callbacks = callbacks
  )
  
} else {
  for (i in 1:FLAGS$n_epochs) {
    model %&amp;gt;% fit(
      x          = X_train,
      y          = y_train,
      validation_data = list(X_valid, y_valid),
      callbacks = callbacks,
      batch_size = FLAGS$batch_size,
      epochs     = 1,
      shuffle    = FALSE
    )
    model %&amp;gt;% reset_states()
  }
}

if (FLAGS$stateful)
  model %&amp;gt;% reset_states()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s step through the simpler, yet better (or equally) performing configuration below.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# create the model
model &amp;lt;- keras_model_sequential()

# add layers
# we have just two, the LSTM and the time_distributed 
model %&amp;gt;%
  layer_lstm(
    units = FLAGS$n_units, 
    # the first layer in a model needs to know the shape of the input data
    batch_input_shape  = c(FLAGS$batch_size, FLAGS$n_timesteps, n_features),
    dropout = FLAGS$dropout,
    recurrent_dropout = FLAGS$recurrent_dropout,
    # by default, an LSTM just returns the final state
    return_sequences = TRUE
  ) %&amp;gt;% time_distributed(layer_dense(units = 1))

model %&amp;gt;%
  compile(
    loss = FLAGS$loss,
    optimizer = optimizer,
    # in addition to the loss, Keras will inform us about current 
    # MSE while training
    metrics = list(&amp;quot;mean_squared_error&amp;quot;)
  )

history &amp;lt;- model %&amp;gt;% fit(
  x          = X_train,
  y          = y_train,
  validation_data = list(X_valid, y_valid),
  batch_size = FLAGS$batch_size,
  epochs     = FLAGS$n_epochs,
  callbacks = callbacks
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we see, training was stopped after ~55 epochs as validation loss did not decrease any more. We also see that performance on the validation set is way worse than performance on the training set - normally indicating overfitting.&lt;/p&gt;
&lt;p&gt;This topic too, we’ll leave to a separate discussion another time, but interestingly regularization using higher values of &lt;code&gt;dropout&lt;/code&gt; and &lt;code&gt;recurrent_dropout&lt;/code&gt; (combined with increasing model capacity) did not yield better generalization performance. This is probably related to the characteristics of this specific time series we mentioned in the introduction.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(history, metrics = &amp;quot;loss&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-06-25-sunspots-lstm/images/history.png" /&gt;&lt;/p&gt;
&lt;p&gt;Now let’s see how well the model was able to capture the characteristics of the training set.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pred_train &amp;lt;- model %&amp;gt;%
  predict(X_train, batch_size = FLAGS$batch_size) %&amp;gt;%
  .[, , 1]

# Retransform values to original scale
pred_train &amp;lt;- (pred_train * scale_history + center_history) ^2
compare_train &amp;lt;- df %&amp;gt;% filter(key == &amp;quot;training&amp;quot;)

# build a dataframe that has both actual and predicted values
for (i in 1:nrow(pred_train)) {
  varname &amp;lt;- paste0(&amp;quot;pred_train&amp;quot;, i)
  compare_train &amp;lt;-
    mutate(compare_train,!!varname := c(
      rep(NA, FLAGS$n_timesteps + i - 1),
      pred_train[i,],
      rep(NA, nrow(compare_train) - FLAGS$n_timesteps * 2 - i + 1)
    ))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We compute the average RSME over all sequences of predictions.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;coln &amp;lt;- colnames(compare_train)[4:ncol(compare_train)]
cols &amp;lt;- map(coln, quo(sym(.)))
rsme_train &amp;lt;-
  map_dbl(cols, function(col)
    rmse(
      compare_train,
      truth = value,
      estimate = !!col,
      na.rm = TRUE
    )) %&amp;gt;% mean()

rsme_train&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;21.01495&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do these predictions really look? As a visualization of all predicted sequences would look pretty crowded, we arbitrarily pick start points at regular intervals.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggplot(compare_train, aes(x = index, y = value)) + geom_line() +
  geom_line(aes(y = pred_train1), color = &amp;quot;cyan&amp;quot;) +
  geom_line(aes(y = pred_train50), color = &amp;quot;red&amp;quot;) +
  geom_line(aes(y = pred_train100), color = &amp;quot;green&amp;quot;) +
  geom_line(aes(y = pred_train150), color = &amp;quot;violet&amp;quot;) +
  geom_line(aes(y = pred_train200), color = &amp;quot;cyan&amp;quot;) +
  geom_line(aes(y = pred_train250), color = &amp;quot;red&amp;quot;) +
  geom_line(aes(y = pred_train300), color = &amp;quot;red&amp;quot;) +
  geom_line(aes(y = pred_train350), color = &amp;quot;green&amp;quot;) +
  geom_line(aes(y = pred_train400), color = &amp;quot;cyan&amp;quot;) +
  geom_line(aes(y = pred_train450), color = &amp;quot;red&amp;quot;) +
  geom_line(aes(y = pred_train500), color = &amp;quot;green&amp;quot;) +
  geom_line(aes(y = pred_train550), color = &amp;quot;violet&amp;quot;) +
  geom_line(aes(y = pred_train600), color = &amp;quot;cyan&amp;quot;) +
  geom_line(aes(y = pred_train650), color = &amp;quot;red&amp;quot;) +
  geom_line(aes(y = pred_train700), color = &amp;quot;red&amp;quot;) +
  geom_line(aes(y = pred_train750), color = &amp;quot;green&amp;quot;) +
  ggtitle(&amp;quot;Predictions on the training set&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-06-25-sunspots-lstm/images/pred_train.png" width="400" /&gt;&lt;/p&gt;
&lt;p&gt;This looks pretty good. From the validation loss, we don’t quite expect the same from the test set, though.&lt;/p&gt;
&lt;p&gt;Let’s see.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pred_test &amp;lt;- model %&amp;gt;%
  predict(X_test, batch_size = FLAGS$batch_size) %&amp;gt;%
  .[, , 1]

# Retransform values to original scale
pred_test &amp;lt;- (pred_test * scale_history + center_history) ^2
pred_test[1:10, 1:5] %&amp;gt;% print()
compare_test &amp;lt;- df %&amp;gt;% filter(key == &amp;quot;testing&amp;quot;)

# build a dataframe that has both actual and predicted values
for (i in 1:nrow(pred_test)) {
  varname &amp;lt;- paste0(&amp;quot;pred_test&amp;quot;, i)
  compare_test &amp;lt;-
    mutate(compare_test,!!varname := c(
      rep(NA, FLAGS$n_timesteps + i - 1),
      pred_test[i,],
      rep(NA, nrow(compare_test) - FLAGS$n_timesteps * 2 - i + 1)
    ))
}

compare_test %&amp;gt;% write_csv(str_replace(model_path, &amp;quot;.hdf5&amp;quot;, &amp;quot;.test.csv&amp;quot;))
compare_test[FLAGS$n_timesteps:(FLAGS$n_timesteps + 10), c(2, 4:8)] %&amp;gt;% print()

coln &amp;lt;- colnames(compare_test)[4:ncol(compare_test)]
cols &amp;lt;- map(coln, quo(sym(.)))
rsme_test &amp;lt;-
  map_dbl(cols, function(col)
    rmse(
      compare_test,
      truth = value,
      estimate = !!col,
      na.rm = TRUE
    )) %&amp;gt;% mean()

rsme_test&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;31.31616&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggplot(compare_test, aes(x = index, y = value)) + geom_line() +
  geom_line(aes(y = pred_test1), color = &amp;quot;cyan&amp;quot;) +
  geom_line(aes(y = pred_test50), color = &amp;quot;red&amp;quot;) +
  geom_line(aes(y = pred_test100), color = &amp;quot;green&amp;quot;) +
  geom_line(aes(y = pred_test150), color = &amp;quot;violet&amp;quot;) +
  geom_line(aes(y = pred_test200), color = &amp;quot;cyan&amp;quot;) +
  geom_line(aes(y = pred_test250), color = &amp;quot;red&amp;quot;) +
  geom_line(aes(y = pred_test300), color = &amp;quot;green&amp;quot;) +
  geom_line(aes(y = pred_test350), color = &amp;quot;cyan&amp;quot;) +
  geom_line(aes(y = pred_test400), color = &amp;quot;red&amp;quot;) +
  geom_line(aes(y = pred_test450), color = &amp;quot;green&amp;quot;) +  
  geom_line(aes(y = pred_test500), color = &amp;quot;cyan&amp;quot;) +
  geom_line(aes(y = pred_test550), color = &amp;quot;violet&amp;quot;) +
  ggtitle(&amp;quot;Predictions on test set&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-06-25-sunspots-lstm/images/pred_test.png" width="400" /&gt;&lt;/p&gt;
&lt;p&gt;That’s not as good as on the training set, but not bad either, given this time series is quite challenging.&lt;/p&gt;
&lt;p&gt;Having defined and run our model on a manually chosen example split, let’s now revert to our overall re-sampling frame.&lt;/p&gt;
&lt;h3 id="backtesting-the-model-on-all-splits"&gt;Backtesting the model on all splits&lt;/h3&gt;
&lt;p&gt;To obtain predictions on all splits, we move the above code into a function and apply it to all splits. First, here’s the function. It returns a list of two dataframes, one for the training and test sets each, that contain the model’s predictions together with the actual values.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;obtain_predictions &amp;lt;- function(split) {
  df_trn &amp;lt;- analysis(split)[1:800, , drop = FALSE]
  df_val &amp;lt;- analysis(split)[801:1200, , drop = FALSE]
  df_tst &amp;lt;- assessment(split)
  
  df &amp;lt;- bind_rows(
    df_trn %&amp;gt;% add_column(key = &amp;quot;training&amp;quot;),
    df_val %&amp;gt;% add_column(key = &amp;quot;validation&amp;quot;),
    df_tst %&amp;gt;% add_column(key = &amp;quot;testing&amp;quot;)
  ) %&amp;gt;%
    as_tbl_time(index = index)
  
  rec_obj &amp;lt;- recipe(value ~ ., df) %&amp;gt;%
    step_sqrt(value) %&amp;gt;%
    step_center(value) %&amp;gt;%
    step_scale(value) %&amp;gt;%
    prep()
  
  df_processed_tbl &amp;lt;- bake(rec_obj, df)
  
  center_history &amp;lt;- rec_obj$steps[[2]]$means[&amp;quot;value&amp;quot;]
  scale_history  &amp;lt;- rec_obj$steps[[3]]$sds[&amp;quot;value&amp;quot;]
  
  FLAGS &amp;lt;- flags(
    flag_boolean(&amp;quot;stateful&amp;quot;, FALSE),
    flag_boolean(&amp;quot;stack_layers&amp;quot;, FALSE),
    flag_integer(&amp;quot;batch_size&amp;quot;, 10),
    flag_integer(&amp;quot;n_timesteps&amp;quot;, 12),
    flag_integer(&amp;quot;n_epochs&amp;quot;, 100),
    flag_numeric(&amp;quot;dropout&amp;quot;, 0.2),
    flag_numeric(&amp;quot;recurrent_dropout&amp;quot;, 0.2),
    flag_string(&amp;quot;loss&amp;quot;, &amp;quot;logcosh&amp;quot;),
    flag_string(&amp;quot;optimizer_type&amp;quot;, &amp;quot;sgd&amp;quot;),
    flag_integer(&amp;quot;n_units&amp;quot;, 128),
    flag_numeric(&amp;quot;lr&amp;quot;, 0.003),
    flag_numeric(&amp;quot;momentum&amp;quot;, 0.9),
    flag_integer(&amp;quot;patience&amp;quot;, 10)
  )
  
  n_predictions &amp;lt;- FLAGS$n_timesteps
  n_features &amp;lt;- 1
  
  optimizer &amp;lt;- switch(FLAGS$optimizer_type,
                      sgd = optimizer_sgd(lr = FLAGS$lr, momentum = FLAGS$momentum))
  callbacks &amp;lt;- list(
    callback_early_stopping(patience = FLAGS$patience)
  )
  
  train_vals &amp;lt;- df_processed_tbl %&amp;gt;%
    filter(key == &amp;quot;training&amp;quot;) %&amp;gt;%
    select(value) %&amp;gt;%
    pull()
  valid_vals &amp;lt;- df_processed_tbl %&amp;gt;%
    filter(key == &amp;quot;validation&amp;quot;) %&amp;gt;%
    select(value) %&amp;gt;%
    pull()
  test_vals &amp;lt;- df_processed_tbl %&amp;gt;%
    filter(key == &amp;quot;testing&amp;quot;) %&amp;gt;%
    select(value) %&amp;gt;%
    pull()
  
  train_matrix &amp;lt;-
    build_matrix(train_vals, FLAGS$n_timesteps + n_predictions)
  valid_matrix &amp;lt;-
    build_matrix(valid_vals, FLAGS$n_timesteps + n_predictions)
  test_matrix &amp;lt;-
    build_matrix(test_vals, FLAGS$n_timesteps + n_predictions)
  
  X_train &amp;lt;- train_matrix[, 1:FLAGS$n_timesteps]
  y_train &amp;lt;-
    train_matrix[, (FLAGS$n_timesteps + 1):(FLAGS$n_timesteps * 2)]
  X_train &amp;lt;-
    X_train[1:(nrow(X_train) %/% FLAGS$batch_size * FLAGS$batch_size),]
  y_train &amp;lt;-
    y_train[1:(nrow(y_train) %/% FLAGS$batch_size * FLAGS$batch_size),]
  
  X_valid &amp;lt;- valid_matrix[, 1:FLAGS$n_timesteps]
  y_valid &amp;lt;-
    valid_matrix[, (FLAGS$n_timesteps + 1):(FLAGS$n_timesteps * 2)]
  X_valid &amp;lt;-
    X_valid[1:(nrow(X_valid) %/% FLAGS$batch_size * FLAGS$batch_size),]
  y_valid &amp;lt;-
    y_valid[1:(nrow(y_valid) %/% FLAGS$batch_size * FLAGS$batch_size),]
  
  X_test &amp;lt;- test_matrix[, 1:FLAGS$n_timesteps]
  y_test &amp;lt;-
    test_matrix[, (FLAGS$n_timesteps + 1):(FLAGS$n_timesteps * 2)]
  X_test &amp;lt;-
    X_test[1:(nrow(X_test) %/% FLAGS$batch_size * FLAGS$batch_size),]
  y_test &amp;lt;-
    y_test[1:(nrow(y_test) %/% FLAGS$batch_size * FLAGS$batch_size),]
  
  X_train &amp;lt;- reshape_X_3d(X_train)
  X_valid &amp;lt;- reshape_X_3d(X_valid)
  X_test &amp;lt;- reshape_X_3d(X_test)
  
  y_train &amp;lt;- reshape_X_3d(y_train)
  y_valid &amp;lt;- reshape_X_3d(y_valid)
  y_test &amp;lt;- reshape_X_3d(y_test)
  
  model &amp;lt;- keras_model_sequential()
  
  model %&amp;gt;%
    layer_lstm(
      units            = FLAGS$n_units,
      batch_input_shape  = c(FLAGS$batch_size, FLAGS$n_timesteps, n_features),
      dropout = FLAGS$dropout,
      recurrent_dropout = FLAGS$recurrent_dropout,
      return_sequences = TRUE
    )     %&amp;gt;% time_distributed(layer_dense(units = 1))
  
  model %&amp;gt;%
    compile(
      loss = FLAGS$loss,
      optimizer = optimizer,
      metrics = list(&amp;quot;mean_squared_error&amp;quot;)
    )
  
  model %&amp;gt;% fit(
    x          = X_train,
    y          = y_train,
    validation_data = list(X_valid, y_valid),
    batch_size = FLAGS$batch_size,
    epochs     = FLAGS$n_epochs,
    callbacks = callbacks
  )
  
  
  pred_train &amp;lt;- model %&amp;gt;%
    predict(X_train, batch_size = FLAGS$batch_size) %&amp;gt;%
    .[, , 1]
  
  # Retransform values
  pred_train &amp;lt;- (pred_train * scale_history + center_history) ^ 2
  compare_train &amp;lt;- df %&amp;gt;% filter(key == &amp;quot;training&amp;quot;)
  
  for (i in 1:nrow(pred_train)) {
    varname &amp;lt;- paste0(&amp;quot;pred_train&amp;quot;, i)
    compare_train &amp;lt;-
      mutate(compare_train, !!varname := c(
        rep(NA, FLAGS$n_timesteps + i - 1),
        pred_train[i, ],
        rep(NA, nrow(compare_train) - FLAGS$n_timesteps * 2 - i + 1)
      ))
  }
  
  pred_test &amp;lt;- model %&amp;gt;%
    predict(X_test, batch_size = FLAGS$batch_size) %&amp;gt;%
    .[, , 1]
  
  # Retransform values
  pred_test &amp;lt;- (pred_test * scale_history + center_history) ^ 2
  compare_test &amp;lt;- df %&amp;gt;% filter(key == &amp;quot;testing&amp;quot;)
  
  for (i in 1:nrow(pred_test)) {
    varname &amp;lt;- paste0(&amp;quot;pred_test&amp;quot;, i)
    compare_test &amp;lt;-
      mutate(compare_test, !!varname := c(
        rep(NA, FLAGS$n_timesteps + i - 1),
        pred_test[i, ],
        rep(NA, nrow(compare_test) - FLAGS$n_timesteps * 2 - i + 1)
      ))
  }
  list(train = compare_train, test = compare_test)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Mapping the function over all splits yields a list of predictions.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;all_split_preds &amp;lt;- rolling_origin_resamples %&amp;gt;%
     mutate(predict = map(splits, obtain_predictions))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Calculate RMSE on all splits:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;calc_rmse &amp;lt;- function(df) {
  coln &amp;lt;- colnames(df)[4:ncol(df)]
  cols &amp;lt;- map(coln, quo(sym(.)))
  map_dbl(cols, function(col)
    rmse(
      df,
      truth = value,
      estimate = !!col,
      na.rm = TRUE
    )) %&amp;gt;% mean()
}

all_split_preds &amp;lt;- all_split_preds %&amp;gt;% unnest(predict)
all_split_preds_train &amp;lt;- all_split_preds[seq(1, 11, by = 2), ]
all_split_preds_test &amp;lt;- all_split_preds[seq(2, 12, by = 2), ]

all_split_rmses_train &amp;lt;- all_split_preds_train %&amp;gt;%
  mutate(rmse = map_dbl(predict, calc_rmse)) %&amp;gt;%
  select(id, rmse)

all_split_rmses_test &amp;lt;- all_split_preds_test %&amp;gt;%
  mutate(rmse = map_dbl(predict, calc_rmse)) %&amp;gt;%
  select(id, rmse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How does it look? Here’s RMSE on the training set for the 6 splits.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;all_split_rmses_train&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 6 x 2
  id      rmse
  &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt;
1 Slice1  22.2
2 Slice2  20.9
3 Slice3  18.8
4 Slice4  23.5
5 Slice5  22.1
6 Slice6  21.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;all_split_rmses_test&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 6 x 2
  id      rmse
  &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt;
1 Slice1  21.6
2 Slice2  20.6
3 Slice3  21.3
4 Slice4  31.4
5 Slice5  35.2
6 Slice6  31.4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at these numbers, we see something interesting: Generalization performance is much better for the first three slices of the time series than for the latter ones. This confirms our impression, stated above, that there seems to be some hidden development going on, rendering forecasting more difficult.&lt;/p&gt;
&lt;p&gt;And here are visualizations of the predictions on the respective training and test sets.&lt;/p&gt;
&lt;p&gt;First, the training sets:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot_train &amp;lt;- function(slice, name) {
  ggplot(slice, aes(x = index, y = value)) + geom_line() +
    geom_line(aes(y = pred_train1), color = &amp;quot;cyan&amp;quot;) +
    geom_line(aes(y = pred_train50), color = &amp;quot;red&amp;quot;) +
    geom_line(aes(y = pred_train100), color = &amp;quot;green&amp;quot;) +
    geom_line(aes(y = pred_train150), color = &amp;quot;violet&amp;quot;) +
    geom_line(aes(y = pred_train200), color = &amp;quot;cyan&amp;quot;) +
    geom_line(aes(y = pred_train250), color = &amp;quot;red&amp;quot;) +
    geom_line(aes(y = pred_train300), color = &amp;quot;red&amp;quot;) +
    geom_line(aes(y = pred_train350), color = &amp;quot;green&amp;quot;) +
    geom_line(aes(y = pred_train400), color = &amp;quot;cyan&amp;quot;) +
    geom_line(aes(y = pred_train450), color = &amp;quot;red&amp;quot;) +
    geom_line(aes(y = pred_train500), color = &amp;quot;green&amp;quot;) +
    geom_line(aes(y = pred_train550), color = &amp;quot;violet&amp;quot;) +
    geom_line(aes(y = pred_train600), color = &amp;quot;cyan&amp;quot;) +
    geom_line(aes(y = pred_train650), color = &amp;quot;red&amp;quot;) +
    geom_line(aes(y = pred_train700), color = &amp;quot;red&amp;quot;) +
    geom_line(aes(y = pred_train750), color = &amp;quot;green&amp;quot;) +
    ggtitle(name)
}

train_plots &amp;lt;- map2(all_split_preds_train$predict, all_split_preds_train$id, plot_train)
p_body_train  &amp;lt;- plot_grid(plotlist = train_plots, ncol = 3)
p_title_train &amp;lt;- ggdraw() + 
  draw_label(&amp;quot;Backtested Predictions: Training Sets&amp;quot;, size = 18, fontface = &amp;quot;bold&amp;quot;)

plot_grid(p_title_train, p_body_train, ncol = 1, rel_heights = c(0.05, 1, 0.05))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-06-25-sunspots-lstm/images/backtested_train.png" width="400" /&gt;&lt;/p&gt;
&lt;p&gt;And the test sets:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot_test &amp;lt;- function(slice, name) {
  ggplot(slice, aes(x = index, y = value)) + geom_line() +
    geom_line(aes(y = pred_test1), color = &amp;quot;cyan&amp;quot;) +
    geom_line(aes(y = pred_test50), color = &amp;quot;red&amp;quot;) +
    geom_line(aes(y = pred_test100), color = &amp;quot;green&amp;quot;) +
    geom_line(aes(y = pred_test150), color = &amp;quot;violet&amp;quot;) +
    geom_line(aes(y = pred_test200), color = &amp;quot;cyan&amp;quot;) +
    geom_line(aes(y = pred_test250), color = &amp;quot;red&amp;quot;) +
    geom_line(aes(y = pred_test300), color = &amp;quot;green&amp;quot;) +
    geom_line(aes(y = pred_test350), color = &amp;quot;cyan&amp;quot;) +
    geom_line(aes(y = pred_test400), color = &amp;quot;red&amp;quot;) +
    geom_line(aes(y = pred_test450), color = &amp;quot;green&amp;quot;) +  
    geom_line(aes(y = pred_test500), color = &amp;quot;cyan&amp;quot;) +
    geom_line(aes(y = pred_test550), color = &amp;quot;violet&amp;quot;) +
    ggtitle(name)
}

test_plots &amp;lt;- map2(all_split_preds_test$predict, all_split_preds_test$id, plot_test)

p_body_test  &amp;lt;- plot_grid(plotlist = test_plots, ncol = 3)
p_title_test &amp;lt;- ggdraw() + 
  draw_label(&amp;quot;Backtested Predictions: Test Sets&amp;quot;, size = 18, fontface = &amp;quot;bold&amp;quot;)

plot_grid(p_title_test, p_body_test, ncol = 1, rel_heights = c(0.05, 1, 0.05))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-06-25-sunspots-lstm/images/backtested_test.png" width="400" /&gt;&lt;/p&gt;
&lt;p&gt;This has been a long post, and necessarily will have left a lot of questions open, first and foremost: How do we obtain good settings for the hyperparameters (learning rate, number of epochs, dropout)? How do we choose the length of the hidden state? Or even, can we have an intuition how well LSTM will perform on a given dataset (with its specific characteristics)? We will tackle questions like the above in upcoming posts.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">9cf5f1ba081de51d57a88e361a35c1b4</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-06-25-sunspots-lstm</guid>
      <pubDate>Mon, 25 Jun 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-06-25-sunspots-lstm/images/backtested_test.png" medium="image" type="image/png" width="800" height="416"/>
    </item>
    <item>
      <title>Simple Audio Classification with Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Falbel</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-06-06-simple-audio-classification-keras</link>
      <description>


&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this tutorial we will build a deep learning model to classify words. We will use &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/introduction.html"&gt;tfdatasets&lt;/a&gt; to handle data IO and pre-processing, and &lt;a href="https://keras.rstudio.com"&gt;Keras&lt;/a&gt; to build and train the model.&lt;/p&gt;
&lt;p&gt;We will use the &lt;a href="https://storage.cloud.google.com/download.tensorflow.org/data/speech_commands_v0.01.tar.gz"&gt;Speech Commands dataset&lt;/a&gt; which consists of 65,000 one-second audio files of people saying 30 different words. Each file contains a single spoken English word. The dataset was released by Google under CC License.&lt;/p&gt;
&lt;p&gt;Our model is a Keras port of the &lt;a href="https://www.tensorflow.org/tutorials/audio_recognition#top_of_page"&gt;TensorFlow tutorial on &lt;em&gt;Simple Audio Recognition&lt;/em&gt;&lt;/a&gt; which in turn was inspired by &lt;a href="http://www.isca-speech.org/archive/interspeech_2015/papers/i15_1478.pdf"&gt;&lt;em&gt;Convolutional Neural Networks for Small-footprint Keyword Spotting&lt;/em&gt;&lt;/a&gt;. There are other approaches to the speech recognition task, like &lt;a href="https://svds.com/tensorflow-rnn-tutorial/"&gt;recurrent neural networks&lt;/a&gt;, &lt;a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/"&gt;dilated (atrous) convolutions&lt;/a&gt; or &lt;a href="https://arxiv.org/abs/1711.10282"&gt;Learning from Between-class Examples for Deep Sound Recognition&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The model we will implement here is not the state of the art for audio recognition systems, which are way more complex, but is relatively simple and fast to train. Plus, we show how to efficiently use &lt;a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/introduction.html"&gt;tfdatasets&lt;/a&gt; to preprocess and serve data.&lt;/p&gt;
&lt;h2 id="audio-representation"&gt;Audio representation&lt;/h2&gt;
&lt;p&gt;Many deep learning models are end-to-end, i.e. we let the model learn useful representations directly from the raw data. However, audio data grows very fast - 16,000 samples per second with a very rich structure at many time-scales. In order to avoid having to deal with raw wave sound data, researchers usually use some kind of feature engineering.&lt;/p&gt;
&lt;p&gt;Every sound wave can be represented by its spectrum, and digitally it can be computed using the &lt;a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform"&gt;Fast Fourier Transform (FFT)&lt;/a&gt;.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://upload.wikimedia.org/wikipedia/commons/6/61/FFT-Time-Frequency-View.png" alt="" /&gt;
&lt;p class="caption"&gt;By Phonical - Own work, CC BY-SA 4.0, &lt;a href="https://commons.wikimedia.org/w/index.php?curid=64473578" class="uri"&gt;https://commons.wikimedia.org/w/index.php?curid=64473578&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;A common way to represent audio data is to break it into small chunks, which usually overlap. For each chunk we use the FFT to calculate the magnitude of the frequency spectrum. The spectra are then combined, side by side, to form what we call a &lt;a href="https://en.wikipedia.org/wiki/Spectrogram"&gt;&lt;strong&gt;spectrogram&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It’s also common for speech recognition systems to further transform the spectrum and compute the &lt;a href="https://en.wikipedia.org/wiki/Mel-frequency_cepstrum"&gt;Mel-Frequency Cepstral Coefficients&lt;/a&gt;. This transformation takes into account that the human ear can’t discern the difference between two closely spaced frequencies and smartly creates bins on the frequency axis. A great tutorial on MFCCs can be found &lt;a href="http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://upload.wikimedia.org/wikipedia/commons/c/c5/Spectrogram-19thC.png" alt="" /&gt;
&lt;p class="caption"&gt;By Aquegg - Own work, Public Domain, &lt;a href="https://commons.wikimedia.org/w/index.php?curid=5544473" class="uri"&gt;https://commons.wikimedia.org/w/index.php?curid=5544473&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;After this procedure, we have an image for each audio sample and we can use convolutional neural networks, the standard architecture type in image recognition models.&lt;/p&gt;
&lt;h2 id="downloading"&gt;Downloading&lt;/h2&gt;
&lt;p&gt;First, let’s download data to a directory in our project. You can either download from &lt;a href="http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz"&gt;this link&lt;/a&gt; (~1GB) or from R with:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;dir.create(&amp;quot;data&amp;quot;)

download.file(
  url = &amp;quot;http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz&amp;quot;, 
  destfile = &amp;quot;data/speech_commands_v0.01.tar.gz&amp;quot;
)

untar(&amp;quot;data/speech_commands_v0.01.tar.gz&amp;quot;, exdir = &amp;quot;data/speech_commands_v0.01&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inside the &lt;code&gt;data&lt;/code&gt; directory we will have a folder called &lt;code&gt;speech_commands_v0.01&lt;/code&gt;. The WAV audio files inside this directory are organised in sub-folders with the label names. For example, all one-second audio files of people speaking the word “bed” are inside the &lt;code&gt;bed&lt;/code&gt; directory. There are 30 of them and a special one called &lt;code&gt;_background_noise_&lt;/code&gt; which contains various patterns that could be mixed in to simulate background noise.&lt;/p&gt;
&lt;h2 id="importing"&gt;Importing&lt;/h2&gt;
&lt;p&gt;In this step we will list all audio .wav files into a &lt;code&gt;tibble&lt;/code&gt; with 3 columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;fname&lt;/code&gt;: the file name;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: the label for each audio file;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;class_id&lt;/code&gt;: a unique integer number starting from zero for each class - used to one-hot encode the classes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This will be useful to the next step when we will create a generator using the &lt;code&gt;tfdatasets&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(stringr)
library(dplyr)

files &amp;lt;- fs::dir_ls(
  path = &amp;quot;data/speech_commands_v0.01/&amp;quot;, 
  recursive = TRUE, 
  glob = &amp;quot;*.wav&amp;quot;
)

files &amp;lt;- files[!str_detect(files, &amp;quot;background_noise&amp;quot;)]

df &amp;lt;- data_frame(
  fname = files, 
  class = fname %&amp;gt;% str_extract(&amp;quot;1/.*/&amp;quot;) %&amp;gt;% 
    str_replace_all(&amp;quot;1/&amp;quot;, &amp;quot;&amp;quot;) %&amp;gt;%
    str_replace_all(&amp;quot;/&amp;quot;, &amp;quot;&amp;quot;),
  class_id = class %&amp;gt;% as.factor() %&amp;gt;% as.integer() - 1L
)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="generator"&gt;Generator&lt;/h2&gt;
&lt;p&gt;We will now create our &lt;code&gt;Dataset&lt;/code&gt;, which in the context of &lt;code&gt;tfdatasets&lt;/code&gt;, adds operations to the TensorFlow graph in order to read and pre-process data. Since they are TensorFlow ops, they are executed in C++ and in parallel with model training.&lt;/p&gt;
&lt;p&gt;The generator we will create will be responsible for reading the audio files from disk, creating the spectrogram for each one and batching the outputs.&lt;/p&gt;
&lt;p&gt;Let’s start by creating the dataset from slices of the &lt;code&gt;data.frame&lt;/code&gt; with audio file names and classes we just created.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tfdatasets)
ds &amp;lt;- tensor_slices_dataset(df) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s define the parameters for spectrogram creation. We need to define &lt;code&gt;window_size_ms&lt;/code&gt; which is the size in milliseconds of each chunk we will break the audio wave into, and &lt;code&gt;window_stride_ms&lt;/code&gt;, the distance between the centers of adjacent chunks:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;window_size_ms &amp;lt;- 30
window_stride_ms &amp;lt;- 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we will convert the window size and stride from milliseconds to samples. We are considering that our audio files have 16,000 samples per second (1000 ms).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;window_size &amp;lt;- as.integer(16000*window_size_ms/1000)
stride &amp;lt;- as.integer(16000*window_stride_ms/1000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will obtain other quantities that will be useful for spectrogram creation, like the number of chunks and the FFT size, i.e., the number of bins on the frequency axis. The function we are going to use to compute the spectrogram doesn’t allow us to change the FFT size and instead by default uses the first power of 2 greater than the window size.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;fft_size &amp;lt;- as.integer(2^trunc(log(window_size, 2)) + 1)
n_chunks &amp;lt;- length(seq(window_size/2, 16000 - window_size/2, stride))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will now use &lt;code&gt;dataset_map&lt;/code&gt; which allows us to specify a pre-processing function for each observation (line) of our dataset. It’s in this step that we read the raw audio file from disk and create its spectrogram and the one-hot encoded response vector.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# shortcuts to used TensorFlow modules.
audio_ops &amp;lt;- tf$contrib$framework$python$ops$audio_ops

ds &amp;lt;- ds %&amp;gt;%
  dataset_map(function(obs) {
    
    # a good way to debug when building tfdatsets pipelines is to use a print
    # statement like this:
    # print(str(obs))
    
    # decoding wav files
    audio_binary &amp;lt;- tf$read_file(tf$reshape(obs$fname, shape = list()))
    wav &amp;lt;- audio_ops$decode_wav(audio_binary, desired_channels = 1)
    
    # create the spectrogram
    spectrogram &amp;lt;- audio_ops$audio_spectrogram(
      wav$audio, 
      window_size = window_size, 
      stride = stride,
      magnitude_squared = TRUE
    )
    
    # normalization
    spectrogram &amp;lt;- tf$log(tf$abs(spectrogram) + 0.01)
    
    # moving channels to last dim
    spectrogram &amp;lt;- tf$transpose(spectrogram, perm = c(1L, 2L, 0L))
    
    # transform the class_id into a one-hot encoded vector
    response &amp;lt;- tf$one_hot(obs$class_id, 30L)
    
    list(spectrogram, response)
  }) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we will specify how we want batch observations from the dataset. We’re using &lt;code&gt;dataset_shuffle&lt;/code&gt; since we want to shuffle observations from the dataset, otherwise it would follow the order of the &lt;code&gt;df&lt;/code&gt; object. Then we use &lt;code&gt;dataset_repeat&lt;/code&gt; in order to tell TensorFlow that we want to keep taking observations from the dataset even if all observations have already been used. And most importantly here, we use &lt;code&gt;dataset_padded_batch&lt;/code&gt; to specify that we want batches of size 32, but they should be padded, ie. if some observation has a different size we pad it with zeroes. The padded shape is passed to &lt;code&gt;dataset_padded_batch&lt;/code&gt; via the &lt;code&gt;padded_shapes&lt;/code&gt; argument and we use &lt;code&gt;NULL&lt;/code&gt; to state that this dimension doesn’t need to be padded.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ds &amp;lt;- ds %&amp;gt;% 
  dataset_shuffle(buffer_size = 100) %&amp;gt;%
  dataset_repeat() %&amp;gt;%
  dataset_padded_batch(
    batch_size = 32, 
    padded_shapes = list(
      shape(n_chunks, fft_size, NULL), 
      shape(NULL)
    )
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is our dataset specification, but we would need to rewrite all the code for the validation data, so it’s good practice to wrap this into a function of the data and other important parameters like &lt;code&gt;window_size_ms&lt;/code&gt; and &lt;code&gt;window_stride_ms&lt;/code&gt;. Below, we will define a function called &lt;code&gt;data_generator&lt;/code&gt; that will create the generator depending on those inputs.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;data_generator &amp;lt;- function(df, batch_size, shuffle = TRUE, 
                           window_size_ms = 30, window_stride_ms = 10) {
  
  window_size &amp;lt;- as.integer(16000*window_size_ms/1000)
  stride &amp;lt;- as.integer(16000*window_stride_ms/1000)
  fft_size &amp;lt;- as.integer(2^trunc(log(window_size, 2)) + 1)
  n_chunks &amp;lt;- length(seq(window_size/2, 16000 - window_size/2, stride))
  
  ds &amp;lt;- tensor_slices_dataset(df)
  
  if (shuffle) 
    ds &amp;lt;- ds %&amp;gt;% dataset_shuffle(buffer_size = 100)  
  
  ds &amp;lt;- ds %&amp;gt;%
    dataset_map(function(obs) {
      
      # decoding wav files
      audio_binary &amp;lt;- tf$read_file(tf$reshape(obs$fname, shape = list()))
      wav &amp;lt;- audio_ops$decode_wav(audio_binary, desired_channels = 1)
      
      # create the spectrogram
      spectrogram &amp;lt;- audio_ops$audio_spectrogram(
        wav$audio, 
        window_size = window_size, 
        stride = stride,
        magnitude_squared = TRUE
      )
      
      spectrogram &amp;lt;- tf$log(tf$abs(spectrogram) + 0.01)
      spectrogram &amp;lt;- tf$transpose(spectrogram, perm = c(1L, 2L, 0L))
      
      # transform the class_id into a one-hot encoded vector
      response &amp;lt;- tf$one_hot(obs$class_id, 30L)
      
      list(spectrogram, response)
    }) %&amp;gt;%
    dataset_repeat()
  
  ds &amp;lt;- ds %&amp;gt;% 
    dataset_padded_batch(batch_size, list(shape(n_chunks, fft_size, NULL), shape(NULL)))
  
  ds
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we can define training and validation data generators. It’s worth noting that executing this won’t actually compute any spectrogram or read any file. It will only define in the TensorFlow graph how it should read and pre-process data.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;set.seed(6)
id_train &amp;lt;- sample(nrow(df), size = 0.7*nrow(df))

ds_train &amp;lt;- data_generator(
  df[id_train,], 
  batch_size = 32, 
  window_size_ms = 30, 
  window_stride_ms = 10
)
ds_validation &amp;lt;- data_generator(
  df[-id_train,], 
  batch_size = 32, 
  shuffle = FALSE, 
  window_size_ms = 30, 
  window_stride_ms = 10
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To actually get a batch from the generator we could create a TensorFlow session and ask it to run the generator. For example:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;sess &amp;lt;- tf$Session()
batch &amp;lt;- next_batch(ds_train)
str(sess$run(batch))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;List of 2
 $ : num [1:32, 1:98, 1:257, 1] -4.6 -4.6 -4.61 -4.6 -4.6 ...
 $ : num [1:32, 1:30] 0 0 0 0 0 0 0 0 0 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each time you run &lt;code&gt;sess$run(batch)&lt;/code&gt; you should see a different batch of observations.&lt;/p&gt;
&lt;h2 id="model-definition"&gt;Model definition&lt;/h2&gt;
&lt;p&gt;Now that we know how we will feed our data we can focus on the model definition. The spectrogram can be treated like an image, so architectures that are commonly used in image recognition tasks should work well with the spectrograms too.&lt;/p&gt;
&lt;p&gt;We will build a convolutional neural network similar to what we have built &lt;a href="https://keras.rstudio.com/articles/examples/mnist_cnn.html"&gt;here&lt;/a&gt; for the MNIST dataset.&lt;/p&gt;
&lt;p&gt;The input size is defined by the number of chunks and the FFT size. Like we explained earlier, they can be obtained from the &lt;code&gt;window_size_ms&lt;/code&gt; and &lt;code&gt;window_stride_ms&lt;/code&gt; used to generate the spectrogram.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;window_size &amp;lt;- as.integer(16000*window_size_ms/1000)
stride &amp;lt;- as.integer(16000*window_stride_ms/1000)
fft_size &amp;lt;- as.integer(2^trunc(log(window_size, 2)) + 1)
n_chunks &amp;lt;- length(seq(window_size/2, 16000 - window_size/2, stride))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will now define our model using the Keras sequential API:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential()
model %&amp;gt;%  
  layer_conv_2d(input_shape = c(n_chunks, fft_size, 1), 
                filters = 32, kernel_size = c(3,3), activation = &amp;#39;relu&amp;#39;) %&amp;gt;% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %&amp;gt;% 
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = &amp;#39;relu&amp;#39;) %&amp;gt;% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %&amp;gt;% 
  layer_conv_2d(filters = 128, kernel_size = c(3,3), activation = &amp;#39;relu&amp;#39;) %&amp;gt;% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %&amp;gt;% 
  layer_conv_2d(filters = 256, kernel_size = c(3,3), activation = &amp;#39;relu&amp;#39;) %&amp;gt;% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %&amp;gt;% 
  layer_dropout(rate = 0.25) %&amp;gt;% 
  layer_flatten() %&amp;gt;% 
  layer_dense(units = 128, activation = &amp;#39;relu&amp;#39;) %&amp;gt;% 
  layer_dropout(rate = 0.5) %&amp;gt;% 
  layer_dense(units = 30, activation = &amp;#39;softmax&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We used 4 layers of convolutions combined with max pooling layers to extract features from the spectrogram images and 2 dense layers at the top. Our network is comparatively simple when compared to more advanced architectures like ResNet or DenseNet that perform very well on image recognition tasks.&lt;/p&gt;
&lt;p&gt;Now let’s compile our model. We will use categorical cross entropy as the loss function and use the Adadelta optimizer. It’s also here that we define that we will look at the accuracy metric during training.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% compile(
  loss = loss_categorical_crossentropy,
  optimizer = optimizer_adadelta(),
  metrics = c(&amp;#39;accuracy&amp;#39;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="model-fitting"&gt;Model fitting&lt;/h2&gt;
&lt;p&gt;Now, we will fit our model. In Keras we can use TensorFlow Datasets as inputs to the &lt;code&gt;fit_generator&lt;/code&gt; function and we will do it here.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% fit_generator(
  generator = ds_train,
  steps_per_epoch = 0.7*nrow(df)/32,
  epochs = 10, 
  validation_data = ds_validation, 
  validation_steps = 0.3*nrow(df)/32
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch 1/10
1415/1415 [==============================] - 87s 62ms/step - loss: 2.0225 - acc: 0.4184 - val_loss: 0.7855 - val_acc: 0.7907
Epoch 2/10
1415/1415 [==============================] - 75s 53ms/step - loss: 0.8781 - acc: 0.7432 - val_loss: 0.4522 - val_acc: 0.8704
Epoch 3/10
1415/1415 [==============================] - 75s 53ms/step - loss: 0.6196 - acc: 0.8190 - val_loss: 0.3513 - val_acc: 0.9006
Epoch 4/10
1415/1415 [==============================] - 75s 53ms/step - loss: 0.4958 - acc: 0.8543 - val_loss: 0.3130 - val_acc: 0.9117
Epoch 5/10
1415/1415 [==============================] - 75s 53ms/step - loss: 0.4282 - acc: 0.8754 - val_loss: 0.2866 - val_acc: 0.9213
Epoch 6/10
1415/1415 [==============================] - 76s 53ms/step - loss: 0.3852 - acc: 0.8885 - val_loss: 0.2732 - val_acc: 0.9252
Epoch 7/10
1415/1415 [==============================] - 75s 53ms/step - loss: 0.3566 - acc: 0.8991 - val_loss: 0.2700 - val_acc: 0.9269
Epoch 8/10
1415/1415 [==============================] - 76s 54ms/step - loss: 0.3364 - acc: 0.9045 - val_loss: 0.2573 - val_acc: 0.9284
Epoch 9/10
1415/1415 [==============================] - 76s 53ms/step - loss: 0.3220 - acc: 0.9087 - val_loss: 0.2537 - val_acc: 0.9323
Epoch 10/10
1415/1415 [==============================] - 76s 54ms/step - loss: 0.2997 - acc: 0.9150 - val_loss: 0.2582 - val_acc: 0.9323&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model’s accuracy is 93.23%. Let’s learn how to make predictions and take a look at the confusion matrix.&lt;/p&gt;
&lt;h2 id="making-predictions"&gt;Making predictions&lt;/h2&gt;
&lt;p&gt;We can use the&lt;code&gt;predict_generator&lt;/code&gt; function to make predictions on a new dataset. Let’s make predictions for our validation dataset. The &lt;code&gt;predict_generator&lt;/code&gt; function needs a step argument which is the number of times the generator will be called.&lt;/p&gt;
&lt;p&gt;We can calculate the number of steps by knowing the batch size, and the size of the validation dataset.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;df_validation &amp;lt;- df[-id_train,]
n_steps &amp;lt;- nrow(df_validation)/32 + 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then use the &lt;code&gt;predict_generator&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;predictions &amp;lt;- predict_generator(
  model, 
  ds_validation, 
  steps = n_steps
  )
str(predictions)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;num [1:19424, 1:30] 1.22e-13 7.30e-19 5.29e-10 6.66e-22 1.12e-17 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will output a matrix with 30 columns - one for each word and n_steps*batch_size number of rows. Note that it starts repeating the dataset at the end to create a full batch.&lt;/p&gt;
&lt;p&gt;We can compute the predicted class by taking the column with the highest probability, for example.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;classes &amp;lt;- apply(predictions, 1, which.max) - 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A nice visualization of the confusion matrix is to create an alluvial diagram:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(dplyr)
library(alluvial)
x &amp;lt;- df_validation %&amp;gt;%
  mutate(pred_class_id = head(classes, nrow(df_validation))) %&amp;gt;%
  left_join(
    df_validation %&amp;gt;% distinct(class_id, class) %&amp;gt;% rename(pred_class = class),
    by = c(&amp;quot;pred_class_id&amp;quot; = &amp;quot;class_id&amp;quot;)
  ) %&amp;gt;%
  mutate(correct = pred_class == class) %&amp;gt;%
  count(pred_class, class, correct)

alluvial(
  x %&amp;gt;% select(class, pred_class),
  freq = x$n,
  col = ifelse(x$correct, &amp;quot;lightblue&amp;quot;, &amp;quot;red&amp;quot;),
  border = ifelse(x$correct, &amp;quot;lightblue&amp;quot;, &amp;quot;red&amp;quot;),
  alpha = 0.6,
  hide = x$n &amp;lt; 20
)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-06-06-simple-audio-classification-keras/images/alluvial.png" alt="" /&gt;
&lt;p class="caption"&gt;Alluvial Plot&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We can see from the diagram that the most relevant mistake our model makes is to classify “tree” as “three”. There are other common errors like classifying “go” as “no”, “up” as “off”. At 93% accuracy for 30 classes, and considering the errors we can say that this model is pretty reasonable.&lt;/p&gt;
&lt;p&gt;The saved model occupies 25Mb of disk space, which is reasonable for a desktop but may not be on small devices. We could train a smaller model, with fewer layers, and see how much the performance decreases.&lt;/p&gt;
&lt;p&gt;In speech recognition tasks its also common to do some kind of data augmentation by mixing a background noise to the spoken audio, making it more useful for real applications where it’s common to have other irrelevant sounds happening in the environment.&lt;/p&gt;
&lt;p&gt;The full code to reproduce this tutorial is available &lt;a href="https://github.com/dfalbel/speech-keras"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">5568434b5bb414f177ffc6d543b3d4df</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Audio Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-06-06-simple-audio-classification-keras</guid>
      <pubDate>Wed, 06 Jun 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://upload.wikimedia.org/wikipedia/commons/6/61/FFT-Time-Frequency-View.png" medium="image" type="image/png"/>
    </item>
    <item>
      <title>lime v0.4: The Kitten Picture Edition</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Thomas Lin Pedersen</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-03-09-lime-v04-the-kitten-picture-edition</link>
      <description>


&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I’m happy to report a new major release of &lt;code&gt;lime&lt;/code&gt; has landed on CRAN. &lt;code&gt;lime&lt;/code&gt; is an R port of the Python library of the same name by Marco Ribeiro that allows the user to pry open black box machine learning models and explain their outcomes on a per-observation basis. It works by modelling the outcome of the black box in the local neighborhood around the observation to explain and using this local model to explain why (not how) the black box did what it did. For more information about the theory of &lt;code&gt;lime&lt;/code&gt; I will direct you to the article &lt;a href="https://arxiv.org/abs/1602.04938"&gt;introducing the methodology&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="new-features"&gt;New features&lt;/h2&gt;
&lt;p&gt;The meat of this release centers around two new features that are somewhat linked: Native support for keras models and support for explaining image models.&lt;/p&gt;
&lt;h3 id="keras-and-images"&gt;keras and images&lt;/h3&gt;
&lt;p&gt;J.J. Allaire was kind enough to namedrop &lt;code&gt;lime&lt;/code&gt; during his keynote introduction of the &lt;code&gt;tensorflow&lt;/code&gt; and &lt;code&gt;keras&lt;/code&gt; packages and I felt compelled to support them natively. As keras is by far the most popular way to interface with tensorflow it is first in line for build-in support. The addition of keras means that &lt;code&gt;lime&lt;/code&gt; now directly supports models from the following packages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/topepo/caret"&gt;caret&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mlr-org/mlr"&gt;mlr&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dmlc/xgboost"&gt;xgboost&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/h2oai/h2o-3"&gt;h2o&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/rstudio/keras"&gt;keras&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you’re working on something too obscure or cutting edge to not be able to use these packages it is still possible to make your model &lt;code&gt;lime&lt;/code&gt; compliant by providing &lt;code&gt;predict_model()&lt;/code&gt; and &lt;code&gt;model_type()&lt;/code&gt; methods for it.&lt;/p&gt;
&lt;p&gt;keras models are used just like any other model, by passing it into the &lt;code&gt;lime()&lt;/code&gt; function along with the training data in order to create an explainer object. Because we’re soon going to talk about image models, we’ll be using one of the pre-trained ImageNet models that is available from keras itself:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
library(lime)
library(magick)

model &amp;lt;- application_vgg16(
  weights = &amp;quot;imagenet&amp;quot;,
  include_top = TRUE
)
model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model
______________________________________________________________________________________________
Layer (type)                              Output Shape                         Param #        
==============================================================================================
input_1 (InputLayer)                      (None, 224, 224, 3)                  0              
______________________________________________________________________________________________
block1_conv1 (Conv2D)                     (None, 224, 224, 64)                 1792           
______________________________________________________________________________________________
block1_conv2 (Conv2D)                     (None, 224, 224, 64)                 36928          
______________________________________________________________________________________________
block1_pool (MaxPooling2D)                (None, 112, 112, 64)                 0              
______________________________________________________________________________________________
block2_conv1 (Conv2D)                     (None, 112, 112, 128)                73856          
______________________________________________________________________________________________
block2_conv2 (Conv2D)                     (None, 112, 112, 128)                147584         
______________________________________________________________________________________________
block2_pool (MaxPooling2D)                (None, 56, 56, 128)                  0              
______________________________________________________________________________________________
block3_conv1 (Conv2D)                     (None, 56, 56, 256)                  295168         
______________________________________________________________________________________________
block3_conv2 (Conv2D)                     (None, 56, 56, 256)                  590080         
______________________________________________________________________________________________
block3_conv3 (Conv2D)                     (None, 56, 56, 256)                  590080         
______________________________________________________________________________________________
block3_pool (MaxPooling2D)                (None, 28, 28, 256)                  0              
______________________________________________________________________________________________
block4_conv1 (Conv2D)                     (None, 28, 28, 512)                  1180160        
______________________________________________________________________________________________
block4_conv2 (Conv2D)                     (None, 28, 28, 512)                  2359808        
______________________________________________________________________________________________
block4_conv3 (Conv2D)                     (None, 28, 28, 512)                  2359808        
______________________________________________________________________________________________
block4_pool (MaxPooling2D)                (None, 14, 14, 512)                  0              
______________________________________________________________________________________________
block5_conv1 (Conv2D)                     (None, 14, 14, 512)                  2359808        
______________________________________________________________________________________________
block5_conv2 (Conv2D)                     (None, 14, 14, 512)                  2359808        
______________________________________________________________________________________________
block5_conv3 (Conv2D)                     (None, 14, 14, 512)                  2359808        
______________________________________________________________________________________________
block5_pool (MaxPooling2D)                (None, 7, 7, 512)                    0              
______________________________________________________________________________________________
flatten (Flatten)                         (None, 25088)                        0              
______________________________________________________________________________________________
fc1 (Dense)                               (None, 4096)                         102764544      
______________________________________________________________________________________________
fc2 (Dense)                               (None, 4096)                         16781312       
______________________________________________________________________________________________
predictions (Dense)                       (None, 1000)                         4097000        
==============================================================================================
Total params: 138,357,544
Trainable params: 138,357,544
Non-trainable params: 0
______________________________________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The vgg16 model is an image classification model that has been build as part of the ImageNet competition where the goal is to classify pictures into 1000 categories with the highest accuracy. As we can see it is fairly complicated.&lt;/p&gt;
&lt;p&gt;In order to create an explainer we will need to pass in the training data as well. For image data the training data is really only used to tell lime that we are dealing with an image model, so any image will suffice. The format for the training data is simply the path to the images, and because the internet runs on kitten pictures we’ll use one of these:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;img &amp;lt;- image_read(&amp;#39;https://www.data-imaginist.com/assets/images/kitten.jpg&amp;#39;)
img_path &amp;lt;- file.path(tempdir(), &amp;#39;kitten.jpg&amp;#39;)
image_write(img, img_path)
plot(as.raster(img))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-03-09-lime-v04-the-kitten-picture-edition/images/unnamed-chunk-2-1.png" /&gt;&lt;/p&gt;
&lt;p&gt;As with text models the explainer will need to know how to prepare the input data for the model. For keras models this means formatting the image data as tensors. Thankfully keras comes with a lot of tools for reshaping image data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;image_prep &amp;lt;- function(x) {
  arrays &amp;lt;- lapply(x, function(path) {
    img &amp;lt;- image_load(path, target_size = c(224,224))
    x &amp;lt;- image_to_array(img)
    x &amp;lt;- array_reshape(x, c(1, dim(x)))
    x &amp;lt;- imagenet_preprocess_input(x)
  })
  do.call(abind::abind, c(arrays, list(along = 1)))
}
explainer &amp;lt;- lime(img_path, model, image_prep)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have an explainer model for understanding how the vgg16 neural network makes its predictions. Before we go along, lets see what the model think of our kitten:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;res &amp;lt;- predict(model, image_prep(img_path))
imagenet_decode_predictions(res)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
  class_name class_description      score
1  n02124075      Egyptian_cat 0.48913878
2  n02123045             tabby 0.15177219
3  n02123159         tiger_cat 0.10270492
4  n02127052              lynx 0.02638111
5  n03793489             mouse 0.00852214&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, it is pretty sure about the whole cat thing. The reason we need to use &lt;code&gt;imagenet_decode_predictions()&lt;/code&gt; is that the output of a keras model is always just a nameless tensor:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;dim(res)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1]    1 1000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;dimnames(res)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are used to classifiers knowing the class labels, but this is not the case for keras. Motivated by this, &lt;code&gt;lime&lt;/code&gt; now have a way to define/overwrite the class labels of a model, using the &lt;code&gt;as_classifier()&lt;/code&gt; function. Let’s redo our explainer:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model_labels &amp;lt;- readRDS(system.file(&amp;#39;extdata&amp;#39;, &amp;#39;imagenet_labels.rds&amp;#39;, package = &amp;#39;lime&amp;#39;))
explainer &amp;lt;- lime(img_path, as_classifier(model, model_labels), image_prep)&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;There is also an &lt;code&gt;as_regressor()&lt;/code&gt; function which tells &lt;code&gt;lime&lt;/code&gt;, without a doubt, that the model is a regression model. Most models can be introspected to see which type of model they are, but neural networks doesn’t really care. &lt;code&gt;lime&lt;/code&gt; guesses the model type from the activation used in the last layer (linear activation == regression), but if that heuristic fails then &lt;code&gt;as_regressor()&lt;/code&gt;/&lt;code&gt;as_classifier()&lt;/code&gt; can be used.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We are now ready to poke into the model and find out what makes it think our image is of an Egyptian cat. But… first I’ll have to talk about yet another concept: superpixels (I promise I’ll get to the explanation part in a bit).&lt;/p&gt;
&lt;p&gt;In order to create meaningful permutations of our image (remember, this is the central idea in &lt;code&gt;lime&lt;/code&gt;), we have to define how to do so. The permutations needs to be substantial enough to have an impact on the image, but not so much that the model completely fails to recognise the content in every case - further, they should lead to an interpretable result. The concept of superpixels lends itself well to these constraints. In short, a superpixel is a patch of an area with high homogeneity, and superpixel segmentation is a clustering of image pixels into a number of superpixels. By segmenting the image to explain into superpixels we can turn area of contextual similarity on and off during the permutations and find out if that area is important. It is still necessary to experiment a bit as the optimal number of superpixels depend on the content of the image. Remember, we need them to be large enough to have an impact but not so large that the class probability becomes effectively binary. &lt;code&gt;lime&lt;/code&gt; comes with a function to assess the superpixel segmentation before beginning the explanation and it is recommended to play with it a bit — with time you’ll likely get a feel for the right values:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# default
plot_superpixels(img_path)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-03-09-lime-v04-the-kitten-picture-edition/images/unnamed-chunk-7-1.png" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Changing some settings
plot_superpixels(img_path, n_superpixels = 200, weight = 40)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-03-09-lime-v04-the-kitten-picture-edition/images/unnamed-chunk-7-2.png" /&gt;&lt;/p&gt;
&lt;p&gt;The default is set to a pretty low number of superpixels — if the subject of interest is relatively small it may be necessary to increase the number of superpixels so that the full subject does not end up in one, or a few superpixels. The &lt;code&gt;weight&lt;/code&gt; parameter will allow you to make the segments more compact by weighting spatial distance higher than colour distance. For this example we’ll stick with the defaults.&lt;/p&gt;
&lt;p&gt;Be aware that explaining image models is much heavier than tabular or text data. In effect it will create 1000 new images per explanation (default permutation size for images) and run these through the model. As image classification models are often quite heavy, this will result in computation time measured in minutes. The permutation is batched (default to 10 permutations per batch), so you should not be afraid of running out of RAM or hard-drive space.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;explanation &amp;lt;- explain(img_path, explainer, n_labels = 2, n_features = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output of an image explanation is a data frame of the same format as that from tabular and text data. Each feature will be a superpixel and the pixel range of the superpixel will be used as its description. Usually the explanation will only make sense in the context of the image itself, so the new version of &lt;code&gt;lime&lt;/code&gt; also comes with a &lt;code&gt;plot_image_explanation()&lt;/code&gt; function to do just that. Let’s see what our explanation have to tell us:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot_image_explanation(explanation)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-03-09-lime-v04-the-kitten-picture-edition/images/unnamed-chunk-8-1.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can see that the model, for both the major predicted classes, focuses on the cat, which is nice since they are both different cat breeds. The plot function got a few different functions to help you tweak the visual, and it filters low scoring superpixels away by default. An alternative view that puts more focus on the relevant superpixels, but removes the context can be seen by using &lt;code&gt;display = 'block'&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot_image_explanation(explanation, display = &amp;#39;block&amp;#39;, threshold = 0.01)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-03-09-lime-v04-the-kitten-picture-edition/images/unnamed-chunk-9-1.png" /&gt;&lt;/p&gt;
&lt;p&gt;While not as common with image explanations it is also possible to look at the areas of an image that contradicts the class:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot_image_explanation(explanation, threshold = 0, show_negative = TRUE, fill_alpha = 0.6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-03-09-lime-v04-the-kitten-picture-edition/images/unnamed-chunk-10-1.png" /&gt;&lt;/p&gt;
&lt;p&gt;As each explanation takes longer time to create and needs to be tweaked on a per-image basis, image explanations are not something that you’ll create in large batches as you might do with tabular and text data. Still, a few explanations might allow you to understand your model better and be used for communicating the workings of your model. Further, as the time-limiting factor in image explanations are the image classifier and not lime itself, it is bound to improve as image classifiers becomes more performant.&lt;/p&gt;
&lt;h3 id="grab-back"&gt;Grab back&lt;/h3&gt;
&lt;p&gt;Apart from keras and image support, a slew of other features and improvements have been added. Here’s a quick overview:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All explanation plots now include the fit of the ridge regression used to make the explanation. This makes it easy to assess how good the assumptions about local linearity are kept.&lt;/li&gt;
&lt;li&gt;When explaining tabular data the default distance measure is now &lt;code&gt;'gower'&lt;/code&gt; from the &lt;code&gt;gower&lt;/code&gt; package. &lt;code&gt;gower&lt;/code&gt; makes it possible to measure distances between heterogeneous data without converting all features to numeric and experimenting with different exponential kernels.&lt;/li&gt;
&lt;li&gt;When explaining tabular data numerical features will no longer be sampled from a normal distribution during permutations, but from a kernel density defined by the training data. This should ensure that the permutations are more representative of the expected input.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="wrapping-up"&gt;Wrapping up&lt;/h2&gt;
&lt;p&gt;This release represents an important milestone for &lt;code&gt;lime&lt;/code&gt; in R. With the addition of image explanations the &lt;code&gt;lime&lt;/code&gt; package is now on par or above its Python relative, feature-wise. Further development will focus on improving the performance of the model, e.g. by adding parallelisation or improving the local model definition, as well as exploring alternative explanation types such as &lt;a href="https://homes.cs.washington.edu/%7Emarcotcr/aaai18.pdf"&gt;anchor&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Happy Explaining!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">6964f7338edb1b3c8129d1acdc1c2dba</distill:md5>
      <category>Packages/Releases</category>
      <category>TensorFlow/Keras</category>
      <category>Explainability</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-03-09-lime-v04-the-kitten-picture-edition</guid>
      <pubDate>Fri, 09 Mar 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-03-09-lime-v04-the-kitten-picture-edition/images/unnamed-chunk-8-1.png" medium="image" type="image/png" width="1344" height="672"/>
    </item>
    <item>
      <title>Deep Learning for Cancer Immunotherapy</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Leon Eyrich Jessen</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-01-29-dl-for-cancer-immunotherapy</link>
      <description>


&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In my research, I apply deep learning to unravel molecular interactions in the human immune system. One application of my research is within cancer immunotherapy (Immuno-oncology or Immunooncology) - a cancer treatment strategy, where the aim is to utilize the cancer patient’s own immune system to fight the cancer.&lt;/p&gt;
&lt;p&gt;The aim of this post is to illustrates how deep learning is successfully being applied to model key molecular interactions in the human immune system. Molecular interactions are highly context dependent and therefore non-linear. Deep learning is a powerful tool to capture non-linearity and has therefore proven invaluable and highly successful. In particular in modelling the molecular interaction between the Major Histocompability Complex type I (MHCI) and peptides (The state-of-the-art model &lt;a href="http://www.cbs.dtu.dk/services/NetMHCpan/"&gt;netMHCpan&lt;/a&gt; identifies 96.5% of natural peptides at a very high specificity of 98.5%).&lt;/p&gt;
&lt;h3 id="adoptive-t-cell-therapy"&gt;Adoptive T-cell therapy&lt;/h3&gt;
&lt;p&gt;Some brief background before diving in. Special immune cells (T-cells) patrol our body, scanning the cells to check if they are healthy. On the surface of our cells is the MHCI - a highly specialized molecular system, which reflects the health status inside our cells. This is done by displaying small fragments of proteins called peptides, thus reflecting the inside of the cell. T-cells probe these molecular displays to check if the peptides are from our own body (self) or foreign (non-self), e.g. from a virus infection or cancer. If a displayed peptide is non-self, the T-cells has the power to terminate the cell.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-29-dl-for-cancer-immunotherapy/images/Adoptive_T-cell_therapy.png" /&gt; &lt;a href="https://commons.wikimedia.org/wiki/User:Simoncaulton" style=""&gt;Simon Caulton&lt;/a&gt;, &lt;a href="https://commons.wikimedia.org/wiki/File:Adoptive_T-cell_therapy.png"&gt;Adoptive T-cell therapy&lt;/a&gt;, &lt;a href="https://creativecommons.org/licenses/by-sa/3.0/legalcode"&gt;CC BY-SA 3.0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Adoptive T-cell therapy is a form of cancer immunotherapy that aims to isolate tumor infiltrating T-cells from the tumor in the patient, possibly genetically engineer them to be cancer-specific, grow them in great numbers and reintroduce them into the body to fight the cancer. In order to terminate cancer cells, the T-cell needs to be activated by being exposed to tumor peptides bound to MHCI (pMHCI). By analyzing the tumor genetics, relevant peptides can be identified and depending on the patients particular type of MHCI, we can predict which pMHCI are likely to be present in the tumor in the patient and thus which pMHCIs should be used to activate the T-cells.&lt;/p&gt;
&lt;h2 id="peptide-classification-model"&gt;Peptide Classification Model&lt;/h2&gt;
&lt;p&gt;For this use case, we applied three models to classify whether a given peptide is a ‘strong binder’ &lt;code&gt;SB&lt;/code&gt;, ‘weak binder’ &lt;code&gt;WB&lt;/code&gt; or ‘non-binder’ &lt;code&gt;NB&lt;/code&gt;. to MHCI (Specific type: &lt;code&gt;HLA-A*02:01&lt;/code&gt;). Thereby, the classification uncovers which peptides, will be presented to the T-cells. The models we tested were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A deep feed forward fully connected ANN&lt;/li&gt;
&lt;li&gt;A convolutional ANN (connected to a FFN)&lt;/li&gt;
&lt;li&gt;A random forest (for comparison)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Next, we’ll dive into building the artificial neural network. If you want to a more detailed explanation of cancer immunotherapy and how it interacts with the human immune system before going further, see the &lt;a href="#primer-on-cancer-immunotherapy"&gt;primer on cancer immunotherapy&lt;/a&gt; at the end of the post.&lt;/p&gt;
&lt;h3 id="prerequisites"&gt;Prerequisites&lt;/h3&gt;
&lt;p&gt;This example utilizes the &lt;a href="https://tensorflow.rstudio.com/keras"&gt;keras&lt;/a&gt; package, several &lt;a href="https://tidyverse.org"&gt;tidyverse&lt;/a&gt; packages, as well as the &lt;a href="https://github.com/omarwagih/ggseqlogo"&gt;ggseqlogo&lt;/a&gt; and &lt;a href="https://github.com/leonjessen/PepTools"&gt;PepTools&lt;/a&gt; packages. You can install these packages as follows:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Keras + TensorFlow and it&amp;#39;s dependencies
install.packages(&amp;quot;keras&amp;quot;)
library(keras)
install_keras()

# Tidyverse (readr, ggplot2, etc.)
install.packages(&amp;quot;tidyverse&amp;quot;)

# Packages for sequence logos and peptides
devtools::install_github(&amp;quot;omarwagih/ggseqlogo&amp;quot;)
devtools::install_github(&amp;quot;leonjessen/PepTools&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now load all of the packages we need for this example:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
library(tidyverse)
library(PepTools)&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="peptide-data"&gt;Peptide Data&lt;/h3&gt;
&lt;p&gt;The input data for this use case was created by generating 1,000,000 random &lt;code&gt;9-mer&lt;/code&gt; peptides by sampling the one-letter code for the 20 amino acids, i.e. &lt;code&gt;ARNDCQEGHILKMFPSTWYV&lt;/code&gt;, and then submitting the peptides to MHCI binding prediction using the current state-of-the-art model &lt;a href="http://www.cbs.dtu.dk/services/NetMHCpan/"&gt;netMHCpan&lt;/a&gt;. Different variants of MHCI exists, so for this case we chose &lt;code&gt;HLA-A*02:01&lt;/code&gt;. This method assigns ‘strong binder’ &lt;code&gt;SB&lt;/code&gt;, ‘weak binder’ &lt;code&gt;WB&lt;/code&gt; or ‘non-binder’ &lt;code&gt;NB&lt;/code&gt; to each peptide.&lt;/p&gt;
&lt;p&gt;Since &lt;code&gt;n(SB) &amp;lt; n(WB) &amp;lt;&amp;lt; n(NB)&lt;/code&gt;, the data was subsequently balanced by down sampling, such that &lt;code&gt;n(SB) = n(WB) = n(NB) = 7,920&lt;/code&gt;. Thus, &lt;a href="https://raw.githubusercontent.com/leonjessen/tensorflow_rstudio_example/master/data/ran_peps_netMHCpan40_predicted_A0201_reduced_cleaned_balanced.tsv"&gt;a data set with a total of 23,760 data points was created&lt;/a&gt;. 10% of the data points were randomly assigned as &lt;code&gt;test&lt;/code&gt; data and the remainder as &lt;code&gt;train&lt;/code&gt; data. It should be noted that since the data set originates from a model, the outcome of this particular use case will be a model of a model. However, netMHCpan is very accurate (96.5% of natural ligands are identified at a very high specificity 98.5%).&lt;/p&gt;
&lt;p&gt;In the following each peptide will be encoded by assigning a vector of 20 values, where each value is the probability of the amino acid mutating into 1 of the 20 others as defined by the &lt;a href="https://en.wikipedia.org/wiki/BLOSUM#An_example_-_BLOSUM62"&gt;BLOSUM62 matrix&lt;/a&gt; using the &lt;code&gt;pep_encode()&lt;/code&gt; function from the &lt;a href="https://github.com/leonjessen/PepTools/"&gt;PepTools&lt;/a&gt; package. This way each peptide is converted to an ‘image’ matrix with 9 rows and 20 columns.&lt;/p&gt;
&lt;p&gt;Let’s load the data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pep_file &amp;lt;- get_file(
  &amp;quot;ran_peps_netMHCpan40_predicted_A0201_reduced_cleaned_balanced.tsv&amp;quot;, 
  origin = &amp;quot;https://git.io/vb3Xa&amp;quot;
) 
pep_dat &amp;lt;- read_tsv(file = pep_file)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The example peptide data looks like this:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pep_dat %&amp;gt;% head(5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 5 x 4
  peptide   label_chr label_num data_type
  &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;         &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;    
1 LLTDAQRIV WB                1 train    
2 LMAFYLYEV SB                2 train    
3 VMSPITLPT WB                1 test     
4 SLHLTNCFV WB                1 train    
5 RQFTCMIAV WB                1 train   &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where &lt;code&gt;peptide&lt;/code&gt; is the &lt;code&gt;9-mer&lt;/code&gt; peptides, &lt;code&gt;label_chr&lt;/code&gt; defines whether the peptide was predicted by &lt;code&gt;netMHCpan&lt;/code&gt; to be a strong-binder &lt;code&gt;SB&lt;/code&gt;, weak-binder &lt;code&gt;WB&lt;/code&gt; or &lt;code&gt;NB&lt;/code&gt; non-binder to &lt;code&gt;HLA-A*02:01&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;label_num&lt;/code&gt; is equivalent to &lt;code&gt;label_chr&lt;/code&gt;, such that &lt;code&gt;NB = 0&lt;/code&gt;, &lt;code&gt;WB = 1&lt;/code&gt; and &lt;code&gt;SB = 2&lt;/code&gt;. Finally &lt;code&gt;data_type&lt;/code&gt; defines whether the particular data point is part of the &lt;code&gt;train&lt;/code&gt; set used to build the model or the ~10% data left out &lt;code&gt;test&lt;/code&gt; set, which will be used for final performance evaluation.&lt;/p&gt;
&lt;p&gt;The data has been balanced, as shown in this summary:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pep_dat %&amp;gt;% group_by(label_chr, data_type) %&amp;gt;% summarise(n = n())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 6 x 3
# Groups:   label_chr [?]
  label_chr data_type     n
  &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt;
1 NB        test        782
2 NB        train      7138
3 SB        test        802
4 SB        train      7118
5 WB        test        792
6 WB        train      7128&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use the &lt;code&gt;ggseqlogo&lt;/code&gt; package to visualize the sequence motif for the strong binders using a sequence logo. This allows us to see which positions in the peptide and which amino acids are critical for the binding to MHC (Higher letters indicate more importance):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pep_dat %&amp;gt;% filter(label_chr==&amp;#39;SB&amp;#39;) %&amp;gt;% pull(peptide) %&amp;gt;% ggseqlogo()&lt;/code&gt;&lt;/pre&gt;
&lt;div class="l-body-outset"&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-29-dl-for-cancer-immunotherapy/images/sequence-motif.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From the sequence logo, it is evident, that &lt;code&gt;L,M,I,V&lt;/code&gt; are found often at &lt;code&gt;p2&lt;/code&gt; and &lt;code&gt;p9&lt;/code&gt; amongst the strong binders. In fact these position are referred to as the anchor positions, which interact with the MHCI. The T-cell on the other hand, will recognize &lt;code&gt;p3-p8&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id="data-preparation"&gt;Data Preparation&lt;/h3&gt;
&lt;p&gt;We are creating a model &lt;code&gt;f&lt;/code&gt;, where &lt;code&gt;x&lt;/code&gt; is the peptide and &lt;code&gt;y&lt;/code&gt; is one of three classes &lt;code&gt;SB&lt;/code&gt;, &lt;code&gt;WB&lt;/code&gt; and &lt;code&gt;NB&lt;/code&gt;, such that &lt;code&gt;f(x) = y&lt;/code&gt;. Each &lt;code&gt;x&lt;/code&gt; is encoded into a 2-dimensional ‘image’, which we can visualize using the &lt;code&gt;pep_plot_images()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pep_dat %&amp;gt;% filter(label_chr==&amp;#39;SB&amp;#39;) %&amp;gt;% head(1) %&amp;gt;% pull(peptide) %&amp;gt;% pep_plot_images&lt;/code&gt;&lt;/pre&gt;
&lt;div class="l-body-outset"&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-29-dl-for-cancer-immunotherapy/images/visualize-encoding.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;To feed data into a neural network we need to encode it as a multi-dimensional array (or “tensor”). For this dataset we can do this with the &lt;code&gt;PepTools::pep_encode()&lt;/code&gt; function, which takes a character vector of peptides and transforms them into a 3D array of ‘total number of peptides’ x ‘length of each peptide (9)’ x ‘number of unique amino acids (20)’. For example:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;str(pep_encode(c(&amp;quot;LLTDAQRIV&amp;quot;, &amp;quot;LLTDAQRIV&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; num [1:2, 1:9, 1:20] 0.0445 0.0445 0.0445 0.0445 0.073 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s how we transform the data frame into 3-D arrays of training and test data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;x_train &amp;lt;- pep_dat %&amp;gt;% filter(data_type == &amp;#39;train&amp;#39;) %&amp;gt;% pull(peptide)   %&amp;gt;% pep_encode
y_train &amp;lt;- pep_dat %&amp;gt;% filter(data_type == &amp;#39;train&amp;#39;) %&amp;gt;% pull(label_num) %&amp;gt;% array
x_test  &amp;lt;- pep_dat %&amp;gt;% filter(data_type == &amp;#39;test&amp;#39;)  %&amp;gt;% pull(peptide)   %&amp;gt;% pep_encode
y_test  &amp;lt;- pep_dat %&amp;gt;% filter(data_type == &amp;#39;test&amp;#39;)  %&amp;gt;% pull(label_num) %&amp;gt;% array&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To prepare the data for training we convert the 3-d arrays into matrices by reshaping width and height into a single dimension (9x20 peptide ‘images’ are flattened into vectors of lengths 180):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;x_train &amp;lt;- array_reshape(x_train, c(nrow(x_train), 9, 20, 1))
x_test  &amp;lt;- array_reshape(x_test, c(nrow(x_test), 9, 20, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The y data is an integer vector with values ranging from 0 to 2. To prepare this data for training we &lt;a href="https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science"&gt;one-hot encode&lt;/a&gt; the vectors into binary class matrices using the Keras &lt;code&gt;to_categorical&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;y_train &amp;lt;- to_categorical(y_train, num_classes = 3)
y_test  &amp;lt;- to_categorical(y_test,  num_classes = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="defining-the-model"&gt;Defining the Model&lt;/h3&gt;
&lt;p&gt;The core data structure of Keras is a model, a way to organize layers. The simplest type of model is the sequential model, a linear stack of layers. We begin by creating a sequential model and then adding layers using the pipe (&lt;code&gt;%&amp;gt;%&lt;/code&gt;) operator:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;% 
  layer_dense(units  = 180, activation = &amp;#39;relu&amp;#39;, input_shape = 180) %&amp;gt;% 
  layer_dropout(rate = 0.4) %&amp;gt;% 
  layer_dense(units  = 90, activation  = &amp;#39;relu&amp;#39;) %&amp;gt;%
  layer_dropout(rate = 0.3) %&amp;gt;%
  layer_dense(units  = 3, activation   = &amp;#39;softmax&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A &lt;a href="https://keras.rstudio.com/reference/layer_dense.html"&gt;dense layer&lt;/a&gt; is a standard neural network layer with each input node is connected to an output node. A &lt;a href="https://keras.rstudio.com/reference/layer_dropout.html"&gt;dropout layer&lt;/a&gt; sets a random proportion of activations from the previous layer to 0, which helps to prevent overfitting.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;input_shape&lt;/code&gt; argument to the first layer specifies the shape of the input data (a length 180 numeric vector representing a peptide ‘image’). The final layer outputs a length 3 numeric vector (probabilities for each class &lt;code&gt;SB&lt;/code&gt;, &lt;code&gt;WB&lt;/code&gt; and &lt;code&gt;NB&lt;/code&gt;) using a softmax activation function.&lt;/p&gt;
&lt;p&gt;We can use the &lt;code&gt;summary()&lt;/code&gt; function to print the details of the model:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Layer (type)                        Output Shape                    Param #     
================================================================================
dense_1 (Dense)                     (None, 180)                     32580       
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 180)                     0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 90)                      16290       
________________________________________________________________________________
dropout_2 (Dropout)                 (None, 90)                      0           
________________________________________________________________________________
dense_3 (Dense)                     (None, 3)                       273         
================================================================================
Total params: 49,143
Trainable params: 49,143
Non-trainable params: 0
________________________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we compile the model with appropriate loss function, optimizer, and metrics:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% compile(
  loss      = &amp;#39;categorical_crossentropy&amp;#39;,
  optimizer = optimizer_rmsprop(),
  metrics   = c(&amp;#39;accuracy&amp;#39;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="training-and-evaluation"&gt;Training and Evaluation&lt;/h3&gt;
&lt;p&gt;We use the &lt;code&gt;fit()&lt;/code&gt; function to train the model for 150 epochs using batches of 50 peptide ‘images’:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;history = model %&amp;gt;% fit(
  x_train, y_train, 
  epochs = 150, 
  batch_size = 50, 
  validation_split = 0.2
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can visualize the training progress by plotting the &lt;code&gt;history&lt;/code&gt; object returned from &lt;code&gt;fit()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(history)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="l-body-outset"&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-29-dl-for-cancer-immunotherapy/images/ffn-training-history.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We can now evaluate the model’s performance on the original ~10% left out test data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;perf = model %&amp;gt;% evaluate(x_test, y_test)
perf&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$loss
[1] 0.2449334

$acc
[1] 0.9461279&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also visualize the predictions on the test data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;acc     = perf$acc %&amp;gt;% round(3)*100
y_pred  = model %&amp;gt;% predict_classes(x_test)
y_real  = y_test %&amp;gt;% apply(1,function(x){ return( which(x==1) - 1) })
results = tibble(y_real = y_real %&amp;gt;% factor, y_pred = y_pred %&amp;gt;% factor,
                 Correct = ifelse(y_real == y_pred,&amp;quot;yes&amp;quot;,&amp;quot;no&amp;quot;) %&amp;gt;% factor)
title = &amp;#39;Performance on 10% unseen data - Feed Forward Neural Network&amp;#39;
xlab  = &amp;#39;Measured (Real class, as predicted by netMHCpan-4.0)&amp;#39;
ylab  = &amp;#39;Predicted (Class assigned by Keras/TensorFlow deep FFN)&amp;#39;
results %&amp;gt;%
  ggplot(aes(x = y_pred, y = y_real, colour = Correct)) +
  geom_point() +
  ggtitle(label = title, subtitle = paste0(&amp;quot;Accuracy = &amp;quot;, acc,&amp;quot;%&amp;quot;)) +
  xlab(xlab) +
  ylab(ylab) +
  scale_color_manual(labels = c(&amp;#39;No&amp;#39;, &amp;#39;Yes&amp;#39;),
                     values = c(&amp;#39;tomato&amp;#39;,&amp;#39;cornflowerblue&amp;#39;)) +
  geom_jitter() +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;div class="l-body-outset"&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-29-dl-for-cancer-immunotherapy/images/01_ffn_02_results_3_by_3_confusion_matrix.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The final result was a performance on the 10% unseen data of just short of 95% accuracy.&lt;/p&gt;
&lt;h2 id="convolutional-neural-network"&gt;Convolutional Neural Network&lt;/h2&gt;
&lt;p&gt;In order to test a more complex architecture, we also implemented a Convolutional Neural Network. To make the comparison, we repeated the data preparation as described above and only changed the architecture by including a single 2d convolutional layer and then feeding that into the same architecture as the &lt;code&gt;FFN&lt;/code&gt; above:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;%
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = &amp;#39;relu&amp;#39;,
                input_shape = c(9, 20, 1)) %&amp;gt;%
  layer_dropout(rate = 0.25) %&amp;gt;% 
  layer_flatten() %&amp;gt;% 
  layer_dense(units  = 180, activation = &amp;#39;relu&amp;#39;) %&amp;gt;% 
  layer_dropout(rate = 0.4) %&amp;gt;% 
  layer_dense(units  = 90, activation  = &amp;#39;relu&amp;#39;) %&amp;gt;%
  layer_dropout(rate = 0.3) %&amp;gt;%
  layer_dense(units  = 3, activation   = &amp;#39;softmax&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="l-body-outset"&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-29-dl-for-cancer-immunotherapy/images/02_cnn_02_results_3_by_3_confusion_matrix.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This resulted in a performance on the 10% unseen data of 92% accuracy.&lt;/p&gt;
&lt;p&gt;One might have expected the CNN to be able to better capture the information in the peptide ‘images’. There is however a crucial difference between the peptide ‘images’ and the e.g. &lt;code&gt;MNIST&lt;/code&gt; dataset. The peptide ‘images’ do not contain edges and spatially arranged continuous structures, rather they are a set of pixels with &lt;code&gt;p2&lt;/code&gt; always at &lt;code&gt;p2&lt;/code&gt; and likewise for &lt;code&gt;p9&lt;/code&gt;, which are determinants for binding.&lt;/p&gt;
&lt;h2 id="random-forest"&gt;Random Forest&lt;/h2&gt;
&lt;p&gt;Knowing that deep ;earning is not necessarily the right tool for all prediction tasks, we also created a random forest model on the exact same data using the &lt;code&gt;randomForest&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; training data was prepared slightly different using &lt;code&gt;PepTools::pep_encode_mat&lt;/code&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Setup training data
target  &amp;lt;- &amp;#39;train&amp;#39;
x_train &amp;lt;- pep_dat %&amp;gt;% filter(data_type==target) %&amp;gt;% pull(peptide) %&amp;gt;%
  pep_encode_mat %&amp;gt;% select(-peptide)
y_train &amp;lt;- pep_dat %&amp;gt;% filter(data_type==target) %&amp;gt;% pull(label_num) %&amp;gt;% factor

# Setup test data
target &amp;lt;- &amp;#39;test&amp;#39;
x_test &amp;lt;- pep_dat %&amp;gt;% filter(data_type==target) %&amp;gt;% pull(peptide) %&amp;gt;%
  pep_encode_mat %&amp;gt;% select(-peptide)
y_test &amp;lt;- pep_dat %&amp;gt;% filter(data_type==target) %&amp;gt;% pull(label_num) %&amp;gt;% factor&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The random forest model was then run using 100 trees like so:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;rf_classifier &amp;lt;- randomForest(x = x_train, y = y_train, ntree = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results of the model were collected as follows:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;y_pred    &amp;lt;- predict(rf_classifier, x_test)
n_correct &amp;lt;- table(observed = y_test, predicted = y_pred) %&amp;gt;% diag %&amp;gt;% sum
acc       &amp;lt;- (n_correct / length(y_test)) %&amp;gt;% round(3) * 100
results   &amp;lt;- tibble(y_real  = y_test,
                   y_pred  = y_pred,
                   Correct = ifelse(y_real == y_pred,&amp;quot;yes&amp;quot;,&amp;quot;no&amp;quot;) %&amp;gt;% factor)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then visualize the performance as we did with the &lt;code&gt;FFN&lt;/code&gt; and the &lt;code&gt;CNN&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;title = &amp;quot;Performance on 10% unseen data - Random Forest&amp;quot;
xlab  = &amp;quot;Measured (Real class, as predicted by netMHCpan-4.0)&amp;quot;
ylab  = &amp;quot;Predicted (Class assigned by random forest)&amp;quot;
f_out = &amp;quot;plots/03_rf_01_results_3_by_3_confusion_matrix.png&amp;quot;
results %&amp;gt;%
  ggplot(aes(x = y_pred, y = y_real, colour = Correct)) +
  geom_point() +
  xlab(xlab) +
  ylab(ylab) +
  ggtitle(label = title, subtitle = paste0(&amp;quot;Accuracy = &amp;quot;, acc,&amp;quot;%&amp;quot;)) +
  scale_color_manual(labels = c(&amp;#39;No&amp;#39;, &amp;#39;Yes&amp;#39;),
                     values = c(&amp;#39;tomato&amp;#39;,&amp;#39;cornflowerblue&amp;#39;)) +
  geom_jitter() +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;div class="l-body-outset"&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-29-dl-for-cancer-immunotherapy/images/03_rf_01_results_3_by_3_confusion_matrix.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post you have been shown how we build 3 models: A Feed Forward Neural Network (FFN), a Convolutional Neural Network (CNN) and a Random Forest (RF). Using the same data, we obtained performances of ~95%, ~92% and ~82% for the FFN, CNN and RF respectively. The R-code for these models are available here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/leonjessen/tensorflow_rstudio_example/master/R/01_FFN.R"&gt;Feed Forward Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/leonjessen/tensorflow_rstudio_example/master/R/02_cnn.R"&gt;Convolutional Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/leonjessen/tensorflow_rstudio_example/master/R/03_RF.R"&gt;Random Forest&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is evident that the deep learning models capture the information in the system much better than the random forest model. However, the CNN model didn’t not perform as well as the straightforward FFN. This illustrates one of the pitfalls of deep learning - blind alleys. There are a huge number of architectures available, and when combined with hyperparameter tuning the potential model space is breathtakingly large.&lt;/p&gt;
&lt;p&gt;To increase the likelihood of finding a good architecture and the right hyper-parameters it is important to know and understand the data you are modeling. Also, if possible include several sources of data. For the case of peptide-MHC interaction, we include not only information of the strength of the binding as measured in the laboratory, but also information from actual human cells, where peptide-MHC complexes are extracted and analysed.&lt;/p&gt;
&lt;p&gt;It should be noted that when we build models in the research group, a lot of work goes into creating balanced training and test sets. Models are also trained and evaluated using cross-validation, usually 5-fold. We then save each of the five models and create an ensemble prediction - wisdom-of-the-crowd. We are very careful to avoiding overfitting as this of course decreases the models extrapolation performance.&lt;/p&gt;
&lt;p&gt;There is no doubt that deep learning already plays a major role in unraveling the complexities of the human immune system and associated diseases. With the release of &lt;a href="https://www.tensorflow.org"&gt;TensorFlow&lt;/a&gt; by Google along with the &lt;a href="https://tensorflow.rstudio.com/keras/"&gt;keras&lt;/a&gt; and &lt;a href="https://tensorflow.rstudio.com/tensorflow/"&gt;tensorflow&lt;/a&gt; R packages we now have the tools available in R to explore this frontier.&lt;/p&gt;
&lt;h2 id="primer-on-cancer-immunotherapy"&gt;Primer on Cancer Immunotherapy&lt;/h2&gt;
&lt;p&gt;Here is an elaborated background on DNA, proteins and cancer &lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. However, brief and simplified as this is naturally a hugely complex subject.&lt;/p&gt;
&lt;h3 id="dna"&gt;DNA&lt;/h3&gt;
&lt;p&gt;The cell is the basic unit of life. Each cell in our body harbors ~2 meters (6 feet) of DNA, which is identical across all cells. DNA makes up the blue print for our body - our genetic code - using only four nucleic acids (hence the name DNA = DeoxyriboNucleic Acid). We can represent the genetic code, using: &lt;code&gt;a&lt;/code&gt;,&lt;code&gt;c&lt;/code&gt;,&lt;code&gt;g&lt;/code&gt; and &lt;code&gt;t&lt;/code&gt;. Each cell carries ~3,200,000,000 of these letters, which constitute the blue print for our entire body. The letters are organised into ~20,000 genes and from the genes we get proteins. In Bioinformatics, we represent DNA sequences as repeats of the four nucleotides, e.g. &lt;code&gt;ctccgacgaatttcatgttcagggatagct....&lt;/code&gt;&lt;/p&gt;
&lt;h3 id="proteins"&gt;Proteins&lt;/h3&gt;
&lt;p&gt;Comparing with a building - if DNA is the blue print of how to construct a building, then the proteins are the bricks, windows, chimney, plumbing etc. Some proteins are structural (like a brick), whereas others are functional (like a window you can open and close). All ~100,000 proteins in our body are made by of only 20 small molecules called amino acids. Like with DNA, we can represent these 20 amino acids using: &lt;code&gt;A&lt;/code&gt;,&lt;code&gt;R&lt;/code&gt;,&lt;code&gt;N&lt;/code&gt;,&lt;code&gt;D&lt;/code&gt;,&lt;code&gt;C&lt;/code&gt;,&lt;code&gt;Q&lt;/code&gt;,&lt;code&gt;E&lt;/code&gt;,&lt;code&gt;G&lt;/code&gt;,&lt;code&gt;H&lt;/code&gt;,&lt;code&gt;I&lt;/code&gt;,&lt;code&gt;L&lt;/code&gt;,&lt;code&gt;K&lt;/code&gt;,&lt;code&gt;M&lt;/code&gt;,&lt;code&gt;F&lt;/code&gt;,&lt;code&gt;P&lt;/code&gt;,&lt;code&gt;S&lt;/code&gt;,&lt;code&gt;T&lt;/code&gt;,&lt;code&gt;W&lt;/code&gt;,&lt;code&gt;Y&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; (note lowercase for DNA and uppercase for amino acids). The average size of a protein in the human body ~300 amino acids and the sequence is the combination of the 20 amino acids making up the protein written consecutively, e.g.: &lt;code&gt;MRYEMGYWTAFRRDCRCTKSVPSQWEAADN...&lt;/code&gt;. The attentive reader will notice, that I mentioned ~20,000 genes, from which we get ~100,000 proteins. This is due to the DNA in one gene being able to join in different ways and thus produce more than one protein.&lt;/p&gt;
&lt;h3 id="peptides"&gt;Peptides&lt;/h3&gt;
&lt;p&gt;A peptide is a small fragment of a protein of length ~5-15 amino acids. MHCI predominantly binds peptides containing 9 amino acids - A so called &lt;code&gt;9-mer&lt;/code&gt;. Peptides play a crucial role in the monitoring of cells in our body by the human immune system. The data used in this use case consist solely of &lt;code&gt;9-mers&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id="the-human-immune-system"&gt;The Human Immune System&lt;/h3&gt;
&lt;p&gt;Inside each cell, proteins are constantly being produced from DNA. In order not to clutter the cell, proteins are also constantly broken down into peptides which are then recycled to produce new proteins. Some of these peptides are caught by a system and bound to MHCI (Major Histocompatibility Complex type 1, MHCI) and transported from inside of the cell to the outside, where the peptide is displayed. The viewer of this display is the human immune system. Special immune cells (T-cells) patrol the body, looking for cells displaying unexpected peptides. If a displayed peptide is unexpected, the T-cells will terminate the cell. The T-cells have been educated to recognize foreign peptides (non-self) and ignore peptides which originate from our own body (self). This is the hallmark of the immune system - Protecting us by distinguishing self from non-self. I the immune system is not active enough and thus fails to recognize non-self arising from an infection it is potentially fatal. On the other hand if the immune system is too active and starts recognizing not only non-self, but also self, you get autoimmune disease, which likewise is potentially fatal.&lt;/p&gt;
&lt;h3 id="cancer"&gt;Cancer&lt;/h3&gt;
&lt;p&gt;Cancer arises when errors (mutations) occur inside the cell, resulting in changed proteins. This means that if the original protein was e.g. &lt;code&gt;MRYEMGYWTAFRRDCRCTKSVPSQWEAADN...&lt;/code&gt;, then the new erroneous protein could be e.g. &lt;code&gt;MRYEMGYWTAFRRDCRCTKSVPSQWEAADR...&lt;/code&gt;. The result of this is that the peptide displayed on the cell surface is altered. The T-cells will now recognize the peptide as unexpected and terminate the cell. However, the environment around a cancer tumor is very hostile to the T-cells, which are supposed to recognize and terminate the cell.&lt;/p&gt;
&lt;p&gt;Cancer Immunotherapy aims at taking a sample of the tumor and isolate the T-cells, grow them in great numbers and then reintroduce them into the body. Now, despite the hostile environment around the tumor, sheer numbers result in the T-cells out competing the tumor. A special branch of cancer immunotherapy aims at introducing T-cells, which have been specially engineered to recognize a tumor. However, in this case it is of utmost importance to ensure that the T-cell does indeed recognize the tumor and nothing else than the tumor. If introduced T-cells recognize healthy tissue, the outcome can be fatal. It is therefore extremely important to understand the molecular interaction between the sick cell, i.e. the peptide and the MHCI, and the T-cell.&lt;/p&gt;
&lt;p&gt;Our &lt;a href="#peptide-classification-model"&gt;peptide classification model&lt;/a&gt; illustrates how deep learning is being applied to increase our understanding of the molecular interactions governing the activation of the T-cells.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Inside Life Science, Genetics by the Numbers: &lt;a href="https://publications.nigms.nih.gov/insidelifescience/genetics-numbers.html" class="uri"&gt;https://publications.nigms.nih.gov/insidelifescience/genetics-numbers.html&lt;/a&gt;&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">ef9976d57a3cebb9a4b865f727227994</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Tabular Data</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-01-29-dl-for-cancer-immunotherapy</guid>
      <pubDate>Mon, 29 Jan 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-01-29-dl-for-cancer-immunotherapy/images/01_ffn_02_results_3_by_3_confusion_matrix.png" medium="image" type="image/png" width="3000" height="1800"/>
    </item>
    <item>
      <title>Predicting Fraud with Autoencoders and Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Falbel</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-01-24-keras-fraud-autoencoder</link>
      <description>


&lt;h2 id="overview"&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this post we will train an autoencoder to detect credit card fraud. We will also demonstrate how to train Keras models in the cloud using &lt;a href="https://tensorflow.rstudio.com/tools/cloudml/"&gt;CloudML&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The basis of our model will be the Kaggle &lt;a href="https://www.kaggle.com/mlg-ulb/creditcardfraud"&gt;Credit Card Fraud Detection&lt;/a&gt; dataset, which was collected during a research collaboration of Worldline and the &lt;a href="http://mlg.ulb.ac.be"&gt;Machine Learning Group&lt;/a&gt; of ULB (Université Libre de Bruxelles) on big data mining and fraud detection.&lt;/p&gt;
&lt;p&gt;The dataset contains credit card transactions by European cardholders made over a two day period in September 2013. There are 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for only 0.172% of all transactions.&lt;/p&gt;
&lt;h2 id="reading-the-data"&gt;Reading the data&lt;/h2&gt;
&lt;p&gt;After downloading the data from &lt;a href="https://www.kaggle.com/dalpozz/creditcardfraud"&gt;Kaggle&lt;/a&gt;, you can read it in to R with &lt;code&gt;read_csv()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(readr)
df &amp;lt;- read_csv(&amp;quot;data-raw/creditcard.csv&amp;quot;, col_types = list(Time = col_number()))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The input variables consist of only numerical values which are the result of a PCA transformation. In order to preserve confidentiality, no more information about the original features was provided. The features V1, …, V28 were obtained with PCA. There are however 2 features (&lt;em&gt;Time&lt;/em&gt; and &lt;em&gt;Amount&lt;/em&gt;) that were not transformed. &lt;em&gt;Time&lt;/em&gt; is the seconds elapsed between each transaction and the first transaction in the dataset. &lt;em&gt;Amount&lt;/em&gt; is the transaction amount and could be used for cost-sensitive learning. The &lt;em&gt;Class&lt;/em&gt; variable takes value 1 in case of fraud and 0 otherwise.&lt;/p&gt;
&lt;h2 id="autoencoders"&gt;Autoencoders&lt;/h2&gt;
&lt;p&gt;Since only 0.172% of the observations are frauds, we have a highly unbalanced classification problem. With this kind of problem, traditional classification approaches usually don’t work very well because we have only a very small sample of the rarer class.&lt;/p&gt;
&lt;p&gt;An &lt;a href="https://en.wikipedia.org/wiki/Autoencoder"&gt;autoencoder&lt;/a&gt; is a neural network that is used to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. For this problem we will train an autoencoder to encode non-fraud observations from our training set. Since frauds are supposed to have a different distribution then normal transactions, we expect that our autoencoder will have higher reconstruction errors on frauds then on normal transactions. This means that we can use the reconstruction error as a quantity that indicates if a transaction is fraudulent or not.&lt;/p&gt;
&lt;p&gt;If you want to learn more about autoencoders, a good starting point is this &lt;a href="https://www.youtube.com/watch?v=FzS3tMl4Nsc"&gt;video from Larochelle&lt;/a&gt; on YouTube and &lt;a href="http://www.deeplearningbook.org/contents/autoencoders.html"&gt;Chapter 14&lt;/a&gt; from the &lt;a href="http://www.deeplearningbook.org/"&gt;Deep Learning&lt;/a&gt; book by Goodfellow et al.&lt;/p&gt;
&lt;h2 id="visualization"&gt;Visualization&lt;/h2&gt;
&lt;p&gt;For an autoencoder to work well we have a strong initial assumption: that the distribution of variables for normal transactions is different from the distribution for fraudulent ones. Let’s make some plots to verify this. Variables were transformed to a &lt;code&gt;[0,1]&lt;/code&gt; interval for plotting.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tidyr)
library(dplyr)
library(ggplot2)
library(ggridges)
df %&amp;gt;%
  gather(variable, value, -Class) %&amp;gt;%
  ggplot(aes(y = as.factor(variable), 
             fill = as.factor(Class), 
             x = percent_rank(value))) +
  geom_density_ridges()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-24-keras-fraud-autoencoder/images/joy-division.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can see that distributions of variables for fraudulent transactions are very different then from normal ones, except for the &lt;em&gt;Time&lt;/em&gt; variable, which seems to have the exact same distribution.&lt;/p&gt;
&lt;h2 id="preprocessing"&gt;Preprocessing&lt;/h2&gt;
&lt;p&gt;Before the modeling steps we need to do some preprocessing. We will split the dataset into train and test sets and then we will &lt;a href="https://www.quora.com/What-is-the-meaning-of-min-max-normalization"&gt;Min-max normalize&lt;/a&gt; our data (this is done because neural networks work much better with small input values). We will also remove the &lt;em&gt;Time&lt;/em&gt; variable as it has the exact same distribution for normal and fraudulent transactions.&lt;/p&gt;
&lt;p&gt;Based on the &lt;em&gt;Time&lt;/em&gt; variable we will use the first 200,000 observations for training and the rest for testing. This is good practice because when using the model we want to predict future frauds based on transactions that happened before.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;df_train &amp;lt;- df %&amp;gt;% filter(row_number(Time) &amp;lt;= 200000) %&amp;gt;% select(-Time)
df_test &amp;lt;- df %&amp;gt;% filter(row_number(Time) &amp;gt; 200000) %&amp;gt;% select(-Time)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s work on normalization of inputs. We created 2 functions to help us. The first one gets descriptive statistics about the dataset that are used for scaling. Then we have a function to perform the min-max scaling. It’s important to note that we applied the same normalization constants for training and test sets.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(purrr)

#&amp;#39; Gets descriptive statistics for every variable in the dataset.
get_desc &amp;lt;- function(x) {
  map(x, ~list(
    min = min(.x),
    max = max(.x),
    mean = mean(.x),
    sd = sd(.x)
  ))
} 

#&amp;#39; Given a dataset and normalization constants it will create a min-max normalized
#&amp;#39; version of the dataset.
normalization_minmax &amp;lt;- function(x, desc) {
  map2_dfc(x, desc, ~(.x - .y$min)/(.y$max - .y$min))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s create normalized versions of our datasets. We also transformed our data frames to matrices since this is the format expected by Keras.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;desc &amp;lt;- df_train %&amp;gt;% 
  select(-Class) %&amp;gt;% 
  get_desc()

x_train &amp;lt;- df_train %&amp;gt;%
  select(-Class) %&amp;gt;%
  normalization_minmax(desc) %&amp;gt;%
  as.matrix()

x_test &amp;lt;- df_test %&amp;gt;%
  select(-Class) %&amp;gt;%
  normalization_minmax(desc) %&amp;gt;%
  as.matrix()

y_train &amp;lt;- df_train$Class
y_test &amp;lt;- df_test$Class&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id="model-definition"&gt;Model definition&lt;/h1&gt;
&lt;p&gt;We will now define our model in Keras, a symmetric autoencoder with 4 dense layers.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
model &amp;lt;- keras_model_sequential()
model %&amp;gt;%
  layer_dense(units = 15, activation = &amp;quot;tanh&amp;quot;, input_shape = ncol(x_train)) %&amp;gt;%
  layer_dense(units = 10, activation = &amp;quot;tanh&amp;quot;) %&amp;gt;%
  layer_dense(units = 15, activation = &amp;quot;tanh&amp;quot;) %&amp;gt;%
  layer_dense(units = ncol(x_train))

summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;___________________________________________________________________________________
Layer (type)                         Output Shape                     Param #      
===================================================================================
dense_1 (Dense)                      (None, 15)                       450          
___________________________________________________________________________________
dense_2 (Dense)                      (None, 10)                       160          
___________________________________________________________________________________
dense_3 (Dense)                      (None, 15)                       165          
___________________________________________________________________________________
dense_4 (Dense)                      (None, 29)                       464          
===================================================================================
Total params: 1,239
Trainable params: 1,239
Non-trainable params: 0
___________________________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will then compile our model, using the mean squared error loss and the Adam optimizer for training.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% compile(
  loss = &amp;quot;mean_squared_error&amp;quot;, 
  optimizer = &amp;quot;adam&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="training-the-model"&gt;Training the model&lt;/h2&gt;
&lt;p&gt;We can now train our model using the &lt;code&gt;fit()&lt;/code&gt; function. Training the model is reasonably fast (~ 14s per epoch on my laptop). We will only feed to our model the observations of normal (non-fraudulent) transactions.&lt;/p&gt;
&lt;p&gt;We will use &lt;code&gt;callback_model_checkpoint()&lt;/code&gt; in order to save our model after each epoch. By passing the argument &lt;code&gt;save_best_only = TRUE&lt;/code&gt; we will keep on disk only the epoch with smallest loss value on the test set. We will also use &lt;code&gt;callback_early_stopping()&lt;/code&gt; to stop training if the validation loss stops decreasing for 5 epochs.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;checkpoint &amp;lt;- callback_model_checkpoint(
  filepath = &amp;quot;model.hdf5&amp;quot;, 
  save_best_only = TRUE, 
  period = 1,
  verbose = 1
)

early_stopping &amp;lt;- callback_early_stopping(patience = 5)

model %&amp;gt;% fit(
  x = x_train[y_train == 0,], 
  y = x_train[y_train == 0,], 
  epochs = 100, 
  batch_size = 32,
  validation_data = list(x_test[y_test == 0,], x_test[y_test == 0,]), 
  callbacks = list(checkpoint, early_stopping)
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Train on 199615 samples, validate on 84700 samples
Epoch 1/100
199615/199615 [==============================] - 17s 83us/step - loss: 0.0036 - val_loss: 6.8522e-04d from inf to 0.00069, saving model to model.hdf5
Epoch 2/100
199615/199615 [==============================] - 17s 86us/step - loss: 4.7817e-04 - val_loss: 4.7266e-04d from 0.00069 to 0.00047, saving model to model.hdf5
Epoch 3/100
199615/199615 [==============================] - 19s 94us/step - loss: 3.7753e-04 - val_loss: 4.2430e-04d from 0.00047 to 0.00042, saving model to model.hdf5
Epoch 4/100
199615/199615 [==============================] - 19s 94us/step - loss: 3.3937e-04 - val_loss: 4.0299e-04d from 0.00042 to 0.00040, saving model to model.hdf5
Epoch 5/100
199615/199615 [==============================] - 19s 94us/step - loss: 3.2259e-04 - val_loss: 4.0852e-04 improve
Epoch 6/100
199615/199615 [==============================] - 18s 91us/step - loss: 3.1668e-04 - val_loss: 4.0746e-04 improve
...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After training we can get the final loss for the test set by using the &lt;code&gt;evaluate()&lt;/code&gt; fucntion.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;loss &amp;lt;- evaluate(model, x = x_test[y_test == 0,], y = x_test[y_test == 0,])
loss&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        loss 
0.0003534254 &lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="tuning-with-cloudml"&gt;Tuning with CloudML&lt;/h2&gt;
&lt;p&gt;We may be able to get better results by tuning our model hyperparameters. We can tune, for example, the normalization function, the learning rate, the activation functions and the size of hidden layers. CloudML uses Bayesian optimization to tune hyperparameters of models as described in &lt;a href="https://cloud.google.com/blog/big-data/2017/08/hyperparameter-tuning-in-cloud-machine-learning-engine-using-bayesian-optimization"&gt;this blog post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We can use the &lt;a href="https://tensorflow.rstudio.com/tools/cloudml/"&gt;cloudml package&lt;/a&gt; to tune our model, but first we need to prepare our project by creating a &lt;a href="https://tensorflow.rstudio.com/tools/training_flags.html"&gt;training flag&lt;/a&gt; for each hyperparameter and a &lt;code&gt;tuning.yml&lt;/code&gt; file that will tell CloudML what parameters we want to tune and how.&lt;/p&gt;
&lt;p&gt;The full script used for training on CloudML can be found at &lt;a href="https://github.com/dfalbel/fraud-autoencoder-example" class="uri"&gt;https://github.com/dfalbel/fraud-autoencoder-example&lt;/a&gt;. The most important modifications to the code were adding the training flags:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;FLAGS &amp;lt;- flags(
  flag_string(&amp;quot;normalization&amp;quot;, &amp;quot;minmax&amp;quot;, &amp;quot;One of minmax, zscore&amp;quot;),
  flag_string(&amp;quot;activation&amp;quot;, &amp;quot;relu&amp;quot;, &amp;quot;One of relu, selu, tanh, sigmoid&amp;quot;),
  flag_numeric(&amp;quot;learning_rate&amp;quot;, 0.001, &amp;quot;Optimizer Learning Rate&amp;quot;),
  flag_integer(&amp;quot;hidden_size&amp;quot;, 15, &amp;quot;The hidden layer size&amp;quot;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then used the &lt;code&gt;FLAGS&lt;/code&gt; variable inside the script to drive the hyperparameters of the model, for example:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% compile(
  optimizer = optimizer_adam(lr = FLAGS$learning_rate), 
  loss = &amp;#39;mean_squared_error&amp;#39;,
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We also created a &lt;code&gt;tuning.yml&lt;/code&gt; file describing how hyperparameters should be varied during training, as well as what metric we wanted to optimize (in this case it was the validation loss: &lt;code&gt;val_loss&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;tuning.yml&lt;/strong&gt;&lt;/p&gt;
&lt;pre class="markup"&gt;&lt;code&gt;trainingInput:
  scaleTier: CUSTOM
  masterType: standard_gpu
  hyperparameters:
    goal: MINIMIZE
    hyperparameterMetricTag: val_loss
    maxTrials: 10
    maxParallelTrials: 5
    params:
      - parameterName: normalization
        type: CATEGORICAL
        categoricalValues: [zscore, minmax]
      - parameterName: activation
        type: CATEGORICAL
        categoricalValues: [relu, selu, tanh, sigmoid]
      - parameterName: learning_rate
        type: DOUBLE
        minValue: 0.000001
        maxValue: 0.1
        scaleType: UNIT_LOG_SCALE
      - parameterName: hidden_size
        type: INTEGER
        minValue: 5
        maxValue: 50
        scaleType: UNIT_LINEAR_SCALE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We describe the type of machine we want to use (in this case a &lt;code&gt;standard_gpu&lt;/code&gt; instance), the metric we want to minimize while tuning, and the the maximum number of trials (i.e. number of combinations of hyperparameters we want to test). We then specify how we want to vary each hyperparameter during tuning.&lt;/p&gt;
&lt;p&gt;You can learn more about the tuning.yml file &lt;a href="https://tensorflow.rstudio.com/tools/cloudml/articles/tuning.html"&gt;at the Tensorflow for R documentation&lt;/a&gt; and at &lt;a href="https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs#HyperparameterSpec"&gt;Google’s official documentation on CloudML&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now we are ready to send the job to Google CloudML. We can do this by running:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(cloudml)
cloudml_train(&amp;quot;train.R&amp;quot;, config = &amp;quot;tuning.yml&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The cloudml package takes care of uploading the dataset and installing any R package dependencies required to run the script on CloudML. If you are using RStudio v1.1 or higher, it will also allow you to monitor your job in a background terminal. You can also monitor your job using the &lt;a href="https://console.cloud.google.com"&gt;Google Cloud Console&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;After the job is finished we can collect the job results with:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;job_collect()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will copy the files from the job with the best &lt;code&gt;val_loss&lt;/code&gt; performance on CloudML to your local system and open a report summarizing the training run.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-24-keras-fraud-autoencoder/images/cloudml_report.png" style="border: 1px solid rgba(0,0,0,0.1);" /&gt;&lt;/p&gt;
&lt;p&gt;Since we used a callback to save model checkpoints during training, the model file was also copied from Google CloudML. Files created during training are copied to the “runs” subdirectory of the working directory from which &lt;code&gt;cloudml_train()&lt;/code&gt; is called. You can determine this directory for the most recent run with:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;latest_run()$run_dir&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] runs/cloudml_2018_01_23_221244595-03&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also list all previous runs and their validation losses with:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ls_runs(order = metric_val_loss, decreasing = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                    run_dir metric_loss metric_val_loss
1 runs/2017-12-09T21-01-11Z      0.2577          0.1482
2 runs/2017-12-09T21-00-11Z      0.2655          0.1505
3 runs/2017-12-09T19-59-44Z      0.2597          0.1402
4 runs/2017-12-09T19-56-48Z      0.2610          0.1459

Use View(ls_runs()) to view all columns&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In our case the job downloaded from CloudML was saved to &lt;code&gt;runs/cloudml_2018_01_23_221244595-03/&lt;/code&gt;, so the saved model file is available at &lt;code&gt;runs/cloudml_2018_01_23_221244595-03/model.hdf5&lt;/code&gt;. We can now use our tuned model to make predictions.&lt;/p&gt;
&lt;h2 id="making-predictions"&gt;Making predictions&lt;/h2&gt;
&lt;p&gt;Now that we trained and tuned our model we are ready to generate predictions with our autoencoder. We are interested in the MSE for each observation and we expect that observations of fraudulent transactions will have higher MSE’s.&lt;/p&gt;
&lt;p&gt;First, let’s load our model.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- load_model_hdf5(&amp;quot;runs/cloudml_2018_01_23_221244595-03/model.hdf5&amp;quot;, 
                         compile = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s calculate the MSE for the training and test set observations.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pred_train &amp;lt;- predict(model, x_train)
mse_train &amp;lt;- apply((x_train - pred_train)^2, 1, sum)

pred_test &amp;lt;- predict(model, x_test)
mse_test &amp;lt;- apply((x_test - pred_test)^2, 1, sum)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A good measure of model performance in highly unbalanced datasets is the Area Under the ROC Curve (AUC). AUC has a nice interpretation for this problem, it’s the probability that a fraudulent transaction will have higher MSE then a normal one. We can calculate this using the &lt;a href="https://CRAN.R-project.org/package=Metrics"&gt;Metrics&lt;/a&gt; package, which implements a wide variety of common machine learning model performance metrics.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(Metrics)
auc(y_train, mse_train)
auc(y_test, mse_test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.9546814
[1] 0.9403554&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To use the model in practice for making predictions we need to find a threshold &lt;span class="math inline"&gt;\(k\)&lt;/span&gt; for the MSE, then if if &lt;span class="math inline"&gt;\(MSE &amp;gt; k\)&lt;/span&gt; we consider that transaction a fraud (otherwise we consider it normal). To define this value it’s useful to look at precision and recall while varying the threshold &lt;span class="math inline"&gt;\(k\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;possible_k &amp;lt;- seq(0, 0.5, length.out = 100)
precision &amp;lt;- sapply(possible_k, function(k) {
  predicted_class &amp;lt;- as.numeric(mse_test &amp;gt; k)
  sum(predicted_class == 1 &amp;amp; y_test == 1)/sum(predicted_class)
})

qplot(possible_k, precision, geom = &amp;quot;line&amp;quot;) 
  + labs(x = &amp;quot;Threshold&amp;quot;, y = &amp;quot;Precision&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-24-keras-fraud-autoencoder/images/precision.png" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;recall &amp;lt;- sapply(possible_k, function(k) {
  predicted_class &amp;lt;- as.numeric(mse_test &amp;gt; k)
  sum(predicted_class == 1 &amp;amp; y_test == 1)/sum(y_test)
})
qplot(possible_k, recall, geom = &amp;quot;line&amp;quot;) 
  + labs(x = &amp;quot;Threshold&amp;quot;, y = &amp;quot;Recall&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-24-keras-fraud-autoencoder/images/recall.png" /&gt;&lt;/p&gt;
&lt;p&gt;A good starting point would be to choose the threshold with maximum precision but we could also base our decision on how much money we might lose from fraudulent transactions.&lt;/p&gt;
&lt;p&gt;Suppose each manual verification of fraud costs us $1 but if we don’t verify a transaction and it’s a fraud we will lose this transaction amount. Let’s find for each threshold value how much money we would lose.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;cost_per_verification &amp;lt;- 1

lost_money &amp;lt;- sapply(possible_k, function(k) {
  predicted_class &amp;lt;- as.numeric(mse_test &amp;gt; k)
  sum(cost_per_verification * predicted_class + (predicted_class == 0) * y_test * df_test$Amount) 
})

qplot(possible_k, lost_money, geom = &amp;quot;line&amp;quot;) + labs(x = &amp;quot;Threshold&amp;quot;, y = &amp;quot;Lost Money&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-24-keras-fraud-autoencoder/images/money.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can find the best threshold in this case with:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;possible_k[which.min(lost_money)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.005050505&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we needed to manually verify all frauds, it would cost us ~$13,000. Using our model we can reduce this to ~$2,500.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">d314c293e7f0032daa01ead924a057e8</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Unsupervised Learning</category>
      <category>Cloud</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-01-24-keras-fraud-autoencoder</guid>
      <pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-01-24-keras-fraud-autoencoder/images/preview.png" medium="image" type="image/png" width="790" height="537"/>
    </item>
    <item>
      <title>Analyzing rtweet Data with kerasformula</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Pete Mohanty</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-01-24-analyzing-rtweet-data-with-kerasformula</link>
      <description>


&lt;h2 id="overview"&gt;Overview&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://cran.r-project.org/web/packages/kerasformula/index.html"&gt;kerasformula&lt;/a&gt; package offers a high-level interface for the R interface to &lt;a href="https://keras.rstudio.com"&gt;Keras&lt;/a&gt;. It’s main interface is the &lt;code&gt;kms&lt;/code&gt; function, a regression-style interface to &lt;code&gt;keras_model_sequential&lt;/code&gt; that uses formulas and sparse matrices.&lt;/p&gt;
&lt;p&gt;The kerasformula package is available on CRAN, and can be installed with:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install the kerasformula package
install.packages(&amp;quot;kerasformula&amp;quot;)    
# or devtools::install_github(&amp;quot;rdrr1990/kerasformula&amp;quot;)

library(kerasformula)

# install the core keras library (if you haven&amp;#39;t already done so)
# see ?install_keras() for options e.g. install_keras(tensorflow = &amp;quot;gpu&amp;quot;)
install_keras()&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="the-kms-function"&gt;The kms() function&lt;/h2&gt;
&lt;p&gt;Many classic machine learning tutorials assume that data come in a relatively homogenous form (e.g., pixels for digit recognition or word counts or ranks) which can make coding somewhat cumbersome when data is contained in a heterogenous data frame. &lt;code&gt;kms()&lt;/code&gt; takes advantage of the flexibility of R formulas to smooth this process.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kms&lt;/code&gt; builds dense neural nets and, after fitting them, returns a single object with predictions, measures of fit, and details about the function call. &lt;code&gt;kms&lt;/code&gt; accepts a number of parameters including the loss and activation functions found in &lt;code&gt;keras&lt;/code&gt;. &lt;code&gt;kms&lt;/code&gt; also accepts compiled &lt;code&gt;keras_model_sequential&lt;/code&gt; objects allowing for even further customization. This little demo shows how &lt;code&gt;kms&lt;/code&gt; can aid is model building and hyperparameter selection (e.g., batch size) starting with raw data gathered using &lt;code&gt;library(rtweet)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let’s look at #rstats tweets (excluding retweets) for a six-day period ending January 24, 2018 at 10:40. This happens to give us a nice reasonable number of observations to work with in terms of runtime (and the purpose of this document is to show syntax, not build particularly predictive models).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;rstats &amp;lt;- search_tweets(&amp;quot;#rstats&amp;quot;, n = 10000, include_rts = FALSE)
dim(rstats)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  [1] 2840   42&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Suppose our goal is to predict how popular tweets will be based on how often the tweet was retweeted and favorited (which correlate strongly).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;cor(rstats$favorite_count, rstats$retweet_count, method=&amp;quot;spearman&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    [1] 0.7051952&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since few tweeets go viral, the data are quite skewed towards zero.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-24-analyzing-rtweet-data-with-kerasformula/images/densities-1.png" /&gt;&lt;/p&gt;
&lt;h2 id="getting-the-most-out-of-formulas"&gt;Getting the most out of formulas&lt;/h2&gt;
&lt;p&gt;Let’s suppose we are interested in putting tweets into categories based on popularity but we’re not sure how finely-grained we want to make distinctions. Some of the data, like &lt;code&gt;rstats$mentions_screen_name&lt;/code&gt; comes in a list of varying lengths, so let’s write a helper function to count non-NA entries.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;n &amp;lt;- function(x) {
  unlist(lapply(x, function(y){length(y) - is.na(y[1])}))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s start with a dense neural net, the default of &lt;code&gt;kms&lt;/code&gt;. We can use base R functions to help clean the data–in this case, &lt;code&gt;cut&lt;/code&gt; to discretize the outcome, &lt;code&gt;grepl&lt;/code&gt; to look for key words, and &lt;code&gt;weekdays&lt;/code&gt; and &lt;code&gt;format&lt;/code&gt; to capture different aspects of the time the tweet was posted.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;breaks &amp;lt;- c(-1, 0, 1, 10, 100, 1000, 10000)
popularity &amp;lt;- kms(cut(retweet_count + favorite_count, breaks) ~ screen_name + 
                  source + n(hashtags) + n(mentions_screen_name) + 
                  n(urls_url) + nchar(text) +
                  grepl(&amp;#39;photo&amp;#39;, media_type) +
                  weekdays(created_at) + 
                  format(created_at, &amp;#39;%H&amp;#39;), rstats)
plot(popularity$history) 
  + ggtitle(paste(&amp;quot;#rstat popularity:&amp;quot;, 
            paste0(round(100*popularity$evaluations$acc, 1), &amp;quot;%&amp;quot;),
            &amp;quot;out-of-sample accuracy&amp;quot;)) 
  + theme_minimal()

popularity$confusion&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-24-analyzing-rtweet-data-with-kerasformula/images/first_model-1.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;popularity$confusion

                    (-1,0] (0,1] (1,10] (10,100] (100,1e+03] (1e+03,1e+04]
      (-1,0]            37    12     28        2           0             0
      (0,1]             14    19     72        1           0             0
      (1,10]             6    11    187       30           0             0
      (10,100]           1     3     54       68           0             0
      (100,1e+03]        0     0      4       10           0             0
      (1e+03,1e+04]      0     0      0        1           0             0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model only classifies about 55% of the out-of-sample data correctly and that predictive accuracy doesn’t improve after the first ten epochs. The confusion matrix suggests that model does best with tweets that are retweeted a handful of times but overpredicts the 1-10 level. The &lt;code&gt;history&lt;/code&gt; plot also suggests that out-of-sample accuracy is not very stable. We can easily change the breakpoints and number of epochs.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;breaks &amp;lt;- c(-1, 0, 1, 25, 50, 75, 100, 500, 1000, 10000)
popularity &amp;lt;- kms(cut(retweet_count + favorite_count, breaks) ~  
                  n(hashtags) + n(mentions_screen_name) + n(urls_url) +
                  nchar(text) +
                  screen_name + source +
                  grepl(&amp;#39;photo&amp;#39;, media_type) +
                  weekdays(created_at) + 
                  format(created_at, &amp;#39;%H&amp;#39;), rstats, Nepochs = 10)

plot(popularity$history) 
  + ggtitle(paste(&amp;quot;#rstat popularity (new breakpoints):&amp;quot;,
            paste0(round(100*popularity$evaluations$acc, 1), &amp;quot;%&amp;quot;),
            &amp;quot;out-of-sample accuracy&amp;quot;)) 
  + theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-24-analyzing-rtweet-data-with-kerasformula/images/change_breaks-1.png" /&gt;&lt;/p&gt;
&lt;p&gt;That helped some (about 5% additional predictive accuracy). Suppose we want to add a little more data. Let’s first store the input formula.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pop_input &amp;lt;- &amp;quot;cut(retweet_count + favorite_count, breaks) ~  
                          n(hashtags) + n(mentions_screen_name) + n(urls_url) +
                          nchar(text) +
                          screen_name + source +
                          grepl(&amp;#39;photo&amp;#39;, media_type) +
                          weekdays(created_at) + 
                          format(created_at, &amp;#39;%H&amp;#39;)&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we use &lt;code&gt;paste0&lt;/code&gt; to add to the formula by looping over user IDs adding something like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;grepl(&amp;quot;12233344455556&amp;quot;, mentions_user_id)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;mentions &amp;lt;- unlist(rstats$mentions_user_id)
mentions &amp;lt;- unique(mentions[which(table(mentions) &amp;gt; 5)]) # remove infrequent
mentions &amp;lt;- mentions[!is.na(mentions)] # drop NA

for(i in mentions)
  pop_input &amp;lt;- paste0(pop_input, &amp;quot; + &amp;quot;, &amp;quot;grepl(&amp;quot;, i, &amp;quot;, mentions_user_id)&amp;quot;)

popularity &amp;lt;- kms(pop_input, rstats)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-24-analyzing-rtweet-data-with-kerasformula/images/mentionsplot-1.png" /&gt;&lt;/p&gt;
&lt;p&gt;That helped a touch but the predictive accuracy is still fairly unstable across epochs…&lt;/p&gt;
&lt;h2 id="customizing-layers-with-kms"&gt;Customizing layers with kms()&lt;/h2&gt;
&lt;p&gt;We could add more data, perhaps add individual words from the text or some other summary stat (&lt;code&gt;mean(text %in% LETTERS)&lt;/code&gt; to see if all caps explains popularity). But let’s alter the neural net.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;input.formula&lt;/code&gt; is used to create a sparse model matrix. For example, &lt;code&gt;rstats$source&lt;/code&gt; (Twitter or Twitter-client application type) and &lt;code&gt;rstats$screen_name&lt;/code&gt; are character vectors that will be dummied out. How many columns does it have?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;popularity$P&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    [1] 1277&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Say we wanted to reshape the layers to transition more gradually from the input shape to the output.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;popularity &amp;lt;- kms(pop_input, rstats,
                  layers = list(
                    units = c(1024, 512, 256, 128, NA),
                    activation = c(&amp;quot;relu&amp;quot;, &amp;quot;relu&amp;quot;, &amp;quot;relu&amp;quot;, &amp;quot;relu&amp;quot;, &amp;quot;softmax&amp;quot;), 
                    dropout = c(0.5, 0.45, 0.4, 0.35, NA)
                  ))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-24-analyzing-rtweet-data-with-kerasformula/images/customplot-1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kms&lt;/code&gt; builds a &lt;code&gt;keras_sequential_model()&lt;/code&gt;, which is a stack of linear layers. The input shape is determined by the dimensionality of the model matrix (&lt;code&gt;popularity$P&lt;/code&gt;) but after that users are free to determine the number of layers and so on. The &lt;code&gt;kms&lt;/code&gt; argument &lt;code&gt;layers&lt;/code&gt; expects a list, the first entry of which is a vector &lt;code&gt;units&lt;/code&gt; with which to call &lt;code&gt;keras::layer_dense()&lt;/code&gt;. The first element the number of &lt;code&gt;units&lt;/code&gt; in the first layer, the second element for the second layer, and so on (&lt;code&gt;NA&lt;/code&gt; as the final element connotes to auto-detect the final number of units based on the observed number of outcomes). &lt;code&gt;activation&lt;/code&gt; is also passed to &lt;code&gt;layer_dense()&lt;/code&gt; and may take values such as &lt;code&gt;softmax&lt;/code&gt;, &lt;code&gt;relu&lt;/code&gt;, &lt;code&gt;elu&lt;/code&gt;, and &lt;code&gt;linear&lt;/code&gt;. (&lt;code&gt;kms&lt;/code&gt; also has a separate parameter to control the optimizer; by default &lt;code&gt;kms(... optimizer = 'rms_prop')&lt;/code&gt;.) The &lt;code&gt;dropout&lt;/code&gt; that follows each dense layer rate prevents overfitting (but of course isn’t applicable to the final layer).&lt;/p&gt;
&lt;h2 id="choosing-a-batch-size"&gt;Choosing a Batch Size&lt;/h2&gt;
&lt;p&gt;By default, &lt;code&gt;kms&lt;/code&gt; uses batches of 32. Suppose we were happy with our model but didn’t have any particular intuition about what the size should be.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;Nbatch &amp;lt;- c(16, 32, 64)
Nruns &amp;lt;- 4
accuracy &amp;lt;- matrix(nrow = Nruns, ncol = length(Nbatch))
colnames(accuracy) &amp;lt;- paste0(&amp;quot;Nbatch_&amp;quot;, Nbatch)

est &amp;lt;- list()
for(i in 1:Nruns){
  for(j in 1:length(Nbatch)){
   est[[i]] &amp;lt;- kms(pop_input, rstats, Nepochs = 2, batch_size = Nbatch[j])
   accuracy[i,j] &amp;lt;- est[[i]][[&amp;quot;evaluations&amp;quot;]][[&amp;quot;acc&amp;quot;]]
  }
}
  
colMeans(accuracy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    Nbatch_16 Nbatch_32 Nbatch_64 
    0.5088407 0.3820850 0.5556952 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the sake of curtailing runtime, the number of epochs was set arbitrarily short but, from those results, 64 is the best batch size.&lt;/p&gt;
&lt;h2 id="making-predictions-for-new-data"&gt;Making predictions for new data&lt;/h2&gt;
&lt;p&gt;Thus far, we have been using the default settings for &lt;code&gt;kms&lt;/code&gt; which first splits data into 80% training and 20% testing. Of the 80% training, a certain portion is set aside for validation and that’s what produces the epoch-by-epoch graphs of loss and accuracy. The 20% is only used at the end to assess predictive accuracy. But suppose you wanted to make predictions on a new data set…&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;popularity &amp;lt;- kms(pop_input, rstats[1:1000,])
predictions &amp;lt;- predict(popularity, rstats[1001:2000,])
predictions$accuracy&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    [1] 0.579&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# predictions$confusion&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because the formula creates a dummy variable for each screen name and mention, any given set of tweets is all but guaranteed to have different columns. &lt;code&gt;predict.kms_fit&lt;/code&gt; is an &lt;code&gt;S3 method&lt;/code&gt; that takes the new data and constructs a (sparse) model matrix that preserves the original structure of the training matrix. &lt;code&gt;predict&lt;/code&gt; then returns the predictions along with a confusion matrix and accuracy score.&lt;/p&gt;
&lt;p&gt;If your newdata has the same observed levels of y and columns of x_train (the model matrix), you can also use &lt;code&gt;keras::predict_classes&lt;/code&gt; on &lt;code&gt;object$model&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="using-a-compiled-keras-model"&gt;Using a compiled Keras model&lt;/h2&gt;
&lt;p&gt;This section shows how to input a model compiled in the fashion typical to &lt;code&gt;library(keras)&lt;/code&gt;, which is useful for more advanced models. Here is an example for &lt;code&gt;lstm&lt;/code&gt; analogous to the &lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/imdb_lstm.html"&gt;imbd with Keras example&lt;/a&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;k &amp;lt;- keras_model_sequential()
k %&amp;gt;%
  layer_embedding(input_dim = popularity$P, output_dim = popularity$P) %&amp;gt;% 
  layer_lstm(units = 512, dropout = 0.4, recurrent_dropout = 0.2) %&amp;gt;% 
  layer_dense(units = 256, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dropout(0.3) %&amp;gt;%
  layer_dense(units = 8, # number of levels observed on y (outcome)  
              activation = &amp;#39;sigmoid&amp;#39;)

k %&amp;gt;% compile(
  loss = &amp;#39;categorical_crossentropy&amp;#39;,
  optimizer = &amp;#39;rmsprop&amp;#39;,
  metrics = c(&amp;#39;accuracy&amp;#39;)
)

popularity_lstm &amp;lt;- kms(pop_input, rstats, k)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="questions-comments"&gt;Questions? Comments?&lt;/h2&gt;
&lt;p&gt;Drop me a line via the project’s &lt;a href="https://github.com/rdrr1990/kerasformula"&gt;Github repo&lt;/a&gt;. Special thanks to &lt;a href="https://github.com/dfalbel"&gt;@dfalbel&lt;/a&gt; and &lt;a href="https://github.com/jjallaire"&gt;@jjallaire&lt;/a&gt; for helpful suggestions!!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">865c2f4cc74fd7d624f42067b900e4bf</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Natural Language Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-01-24-analyzing-rtweet-data-with-kerasformula</guid>
      <pubDate>Wed, 24 Jan 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-01-24-analyzing-rtweet-data-with-kerasformula/images/densities-1.png" medium="image" type="image/png" width="672" height="480"/>
    </item>
    <item>
      <title>Deep Learning With Keras To Predict Customer Churn</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Matt Dancho</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn</link>
      <description>


&lt;style type="text/css"&gt;
strong {
  font-weight: normal;
}
&lt;/style&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Customer churn is a problem that all companies need to monitor, especially those that depend on subscription-based revenue streams&lt;/strong&gt;. The simple fact is that most organizations have data that can be used to target these individuals and to understand the key drivers of churn, and &lt;strong&gt;we now have Keras for Deep Learning available in R (Yes, in R!!), which predicted customer churn with 82% accuracy&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We’re super excited for this article because we are using the new &lt;a href="https://tensorflow.rstudio.com/keras/"&gt;keras&lt;/a&gt; package to produce an &lt;strong&gt;Artificial Neural Network (ANN)&lt;/strong&gt; model on the &lt;a href="https://www.ibm.com/communities/analytics/watson-analytics-blog/predictive-insights-in-the-telco-customer-churn-data-set/"&gt;IBM Watson Telco Customer Churn Data Set&lt;/a&gt;! As with most business problems, it’s equally important to &lt;strong&gt;explain what features drive the model&lt;/strong&gt;, which is why we’ll use the &lt;a href="https://github.com/thomasp85/lime"&gt;lime&lt;/a&gt; package for explainability. We cross-checked the LIME results with a Correlation Analysis using the &lt;a href="https://github.com/drsimonj/corrr"&gt;corrr&lt;/a&gt; package.&lt;/p&gt;
&lt;p&gt;In addition, we use &lt;strong&gt;three new packages to assist with Machine Learning (ML)&lt;/strong&gt;: &lt;a href="https://topepo.github.io/recipes"&gt;recipes&lt;/a&gt; for preprocessing, &lt;a href="https://topepo.github.io/rsample/"&gt;rsample&lt;/a&gt; for sampling data and &lt;a href="https://github.com/topepo/yardstick"&gt;yardstick&lt;/a&gt; for model metrics. These are relatively new additions to CRAN developed by &lt;a href="https://github.com/topepo"&gt;Max Kuhn&lt;/a&gt; at RStudio (creator of the &lt;a href="http://topepo.github.io/caret/index.html"&gt;caret&lt;/a&gt; package). It seems that &lt;em&gt;R is quickly developing ML tools that rival Python&lt;/em&gt;. Good news if you’re interested in applying Deep Learning in R! We are so let’s get going!!&lt;/p&gt;
&lt;h2 id="customer-churn-hurts-sales-hurts-company"&gt;Customer Churn: Hurts Sales, Hurts Company&lt;/h2&gt;
&lt;p&gt;Customer churn refers to the situation when a customer ends their relationship with a company, and it’s a costly problem. Customers are the fuel that powers a business. Loss of customers impacts sales. Further, it’s much more difficult and costly to gain new customers than it is to retain existing customers. As a result, &lt;strong&gt;organizations need to focus on reducing customer churn&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The good news is that &lt;strong&gt;machine learning can help&lt;/strong&gt;. For many businesses that offer subscription based services, it’s critical to both predict customer churn and explain what features relate to customer churn. Older techniques such as logistic regression can be less accurate than newer techniques such as deep learning, which is why &lt;strong&gt;we are going to show you how to model an ANN in R with the &lt;a href="https://tensorflow.rstudio.com/keras/"&gt;keras&lt;/a&gt; package&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id="churn-modeling-with-artificial-neural-networks-keras"&gt;Churn Modeling With Artificial Neural Networks (Keras)&lt;/h2&gt;
&lt;p&gt;Artificial Neural Networks (ANN) are now a staple within the sub-field of Machine Learning called Deep Learning. &lt;strong&gt;Deep learning algorithms can be vastly superior to traditional regression and classification methods&lt;/strong&gt; (e.g. linear and logistic regression) because of the ability to model interactions between features that would otherwise go undetected. The challenge becomes explainability, which is often needed to support the business case. The good news is we get the best of both worlds with &lt;code&gt;keras&lt;/code&gt; and &lt;code&gt;lime&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id="ibm-watson-dataset-where-we-got-the-data"&gt;IBM Watson Dataset (Where We Got The Data)&lt;/h3&gt;
&lt;p&gt;The dataset used for this tutorial is &lt;a href="https://www.ibm.com/communities/analytics/watson-analytics-blog/predictive-insights-in-the-telco-customer-churn-data-set/"&gt;IBM Watson Telco Dataset&lt;/a&gt;. According to IBM, the business challenge is…&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A telecommunications company [Telco] is concerned about the number of customers leaving their landline business for cable competitors. They need to understand who is leaving. Imagine that you’re an analyst at this company and you have to find out who is leaving and why.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The dataset includes information about:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Customers who left within the last month&lt;/strong&gt;: The column is called Churn&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Services that each customer has signed up for&lt;/strong&gt;: phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Customer account information&lt;/strong&gt;: how long they’ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Demographic info about customers&lt;/strong&gt;: gender, age range, and if they have partners and dependents&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="deep-learning-with-keras-what-we-did-with-the-data"&gt;Deep Learning With Keras (What We Did With The Data)&lt;/h3&gt;
&lt;p&gt;In this example we show you how to use &lt;a href="https://tensorflow.rstudio.com/keras/"&gt;keras&lt;/a&gt; to develop a sophisticated and highly accurate deep learning model in R. We walk you through the preprocessing steps, investing time into how to format the data for Keras. We inspect the various classification metrics, and show that &lt;strong&gt;an un-tuned ANN model can easily get 82% accuracy on the unseen data&lt;/strong&gt;. Here’s the deep learning training history visualization.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-11-keras-customer-churn/images/customer_churn_analysis.png" /&gt;&lt;/p&gt;
&lt;p&gt;We have some fun with preprocessing the data (&lt;em&gt;yes, preprocessing can actually be fun and easy!&lt;/em&gt;). We use the new &lt;a href="https://topepo.github.io/recipes"&gt;recipes&lt;/a&gt; package to simplify the preprocessing workflow.&lt;/p&gt;
&lt;p&gt;We end by showing you how to explain the ANN with the &lt;a href="https://github.com/thomasp85/lime"&gt;lime&lt;/a&gt; package. &lt;strong&gt;Neural networks used to be frowned upon because of the “black box” nature&lt;/strong&gt; meaning these sophisticated models (ANNs are highly accurate) are difficult to explain using traditional methods. &lt;strong&gt;Not any more with LIME!&lt;/strong&gt; Here’s the feature importance visualization.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-11-keras-customer-churn/images/customer_churn_analysis_lime.png" /&gt;&lt;/p&gt;
&lt;p&gt;We also cross-checked the LIME results with a &lt;strong&gt;Correlation Analysis&lt;/strong&gt; using the &lt;a href="https://github.com/drsimonj/corrr"&gt;corrr&lt;/a&gt; package. Here’s the correlation visualization.&lt;/p&gt;
&lt;div class="l-body-outset"&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-11-keras-customer-churn/images/customer_churn_analysis_corrr.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We even built a &lt;strong&gt;Shiny Application&lt;/strong&gt; with a &lt;strong&gt;Customer Scorecard&lt;/strong&gt; to monitor customer churn risk and to make recommendations on how to improve customer health! Feel free to take it for a spin.&lt;/p&gt;
&lt;div class="l-body-outset"&gt;
&lt;p&gt;&lt;a href="https://jjallaire.shinyapps.io/keras-customer-churn/"&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-11-keras-customer-churn/images/shiny-application.png" class="illustration" width="100%"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id="credits"&gt;Credits&lt;/h3&gt;
&lt;p&gt;We saw that just last week the same Telco customer churn dataset was used in the article, &lt;a href="https://datascienceplus.com/predict-customer-churn-logistic-regression-decision-tree-and-random-forest/"&gt;Predict Customer Churn – Logistic Regression, Decision Tree and Random Forest&lt;/a&gt;. We thought the article was excellent.&lt;/p&gt;
&lt;p&gt;This article takes a different approach with Keras, LIME, Correlation Analysis, and a few other cutting edge packages. We encourage the readers to check out both articles because, although the problem is the same, both solutions are beneficial to those learning data science and advanced modeling.&lt;/p&gt;
&lt;h2 id="prerequisites"&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;We use the following libraries in this tutorial:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://tensorflow.rstudio.com/keras/"&gt;keras&lt;/a&gt;: Library that ports Keras from Python enabling deep learning in R. Visit the &lt;a href="https://tensorflow.rstudio.com/keras/"&gt;documentation&lt;/a&gt; for more information.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/thomasp85/lime"&gt;lime&lt;/a&gt;: Used to explain the predictions of black box classifiers. Deep Learning falls into this category.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://business-science.github.io/tidyquant/"&gt;tidyquant&lt;/a&gt;: Loads the &lt;a href="https://www.tidyverse.org/"&gt;tidyverse&lt;/a&gt; (&lt;a href="http://dplyr.tidyverse.org"&gt;dplyr&lt;/a&gt;, &lt;a href="http://ggplot2.tidyverse.org"&gt;ggplot2&lt;/a&gt;, etc) and has nice visualization functions with &lt;a href="https://business-science.github.io/tidyquant/reference/theme_tq.html"&gt;theme_tq()&lt;/a&gt;. Visit the &lt;a href="https://business-science.github.io/tidyquant/"&gt;tidyquant documentation&lt;/a&gt; and the &lt;a href="https://www.tidyverse.org/"&gt;tidyverse documentation&lt;/a&gt; for more information on the individual packages.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://topepo.github.io/rsample/"&gt;rsample&lt;/a&gt;: New package for generating resamples. Visit the &lt;a href="https://topepo.github.io/rsample/"&gt;documentation&lt;/a&gt; for more information.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://topepo.github.io/recipes"&gt;recipes&lt;/a&gt;: New package for preprocessing machine learning data sets. Visit the &lt;a href="https://topepo.github.io/recipes/"&gt;documentation&lt;/a&gt; for more information.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/topepo/yardstick"&gt;yardstick&lt;/a&gt;: Tidy methods for measuring model performance. Visit the &lt;a href="https://github.com/topepo/yardstick"&gt;GitHub Page&lt;/a&gt; for more information.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/drsimonj/corrr"&gt;corrr&lt;/a&gt;: Tidy methods for correlation. Visit the &lt;a href="https://github.com/drsimonj/corrr"&gt;GitHub Page&lt;/a&gt; for more information.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Install the following packages with &lt;code&gt;install.packages()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pkgs &amp;lt;- c(&amp;quot;keras&amp;quot;, &amp;quot;lime&amp;quot;, &amp;quot;tidyquant&amp;quot;, &amp;quot;rsample&amp;quot;, &amp;quot;recipes&amp;quot;, &amp;quot;yardstick&amp;quot;, &amp;quot;corrr&amp;quot;)
install.packages(pkgs)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="load-libraries"&gt;Load Libraries&lt;/h2&gt;
&lt;p&gt;Load the libraries.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Load libraries
library(keras)
library(lime)
library(tidyquant)
library(rsample)
library(recipes)
library(yardstick)
library(corrr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you have not previously run Keras in R, you will need to install Keras using the &lt;code&gt;install_keras()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Install Keras if you have not installed before
install_keras()&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="import-data"&gt;Import Data&lt;/h2&gt;
&lt;p&gt;Download the &lt;a href="https://www.ibm.com/communities/analytics/watson-analytics-blog/predictive-insights-in-the-telco-customer-churn-data-set/"&gt;IBM Watson Telco Data Set here&lt;/a&gt;. Next, use &lt;code&gt;read_csv()&lt;/code&gt; to import the data into a nice tidy data frame. We use the &lt;code&gt;glimpse()&lt;/code&gt; function to quickly inspect the data. We have the target “Churn” and all other variables are potential predictors. The raw data set needs to be cleaned and preprocessed for ML.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;churn_data_raw &amp;lt;- read_csv(&amp;quot;WA_Fn-UseC_-Telco-Customer-Churn.csv&amp;quot;)

glimpse(churn_data_raw)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Observations: 7,043
Variables: 21
$ customerID       &amp;lt;chr&amp;gt; &amp;quot;7590-VHVEG&amp;quot;, &amp;quot;5575-GNVDE&amp;quot;, &amp;quot;3668-QPYBK&amp;quot;, &amp;quot;77...
$ gender           &amp;lt;chr&amp;gt; &amp;quot;Female&amp;quot;, &amp;quot;Male&amp;quot;, &amp;quot;Male&amp;quot;, &amp;quot;Male&amp;quot;, &amp;quot;Female&amp;quot;, &amp;quot;...
$ SeniorCitizen    &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
$ Partner          &amp;lt;chr&amp;gt; &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;N...
$ Dependents       &amp;lt;chr&amp;gt; &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;N...
$ tenure           &amp;lt;int&amp;gt; 1, 34, 2, 45, 2, 8, 22, 10, 28, 62, 13, 16, 5...
$ PhoneService     &amp;lt;chr&amp;gt; &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;...
$ MultipleLines    &amp;lt;chr&amp;gt; &amp;quot;No phone service&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No phone ser...
$ InternetService  &amp;lt;chr&amp;gt; &amp;quot;DSL&amp;quot;, &amp;quot;DSL&amp;quot;, &amp;quot;DSL&amp;quot;, &amp;quot;DSL&amp;quot;, &amp;quot;Fiber optic&amp;quot;, &amp;quot;F...
$ OnlineSecurity   &amp;lt;chr&amp;gt; &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, ...
$ OnlineBackup     &amp;lt;chr&amp;gt; &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, ...
$ DeviceProtection &amp;lt;chr&amp;gt; &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, ...
$ TechSupport      &amp;lt;chr&amp;gt; &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;N...
$ StreamingTV      &amp;lt;chr&amp;gt; &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;...
$ StreamingMovies  &amp;lt;chr&amp;gt; &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;N...
$ Contract         &amp;lt;chr&amp;gt; &amp;quot;Month-to-month&amp;quot;, &amp;quot;One year&amp;quot;, &amp;quot;Month-to-month...
$ PaperlessBilling &amp;lt;chr&amp;gt; &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;...
$ PaymentMethod    &amp;lt;chr&amp;gt; &amp;quot;Electronic check&amp;quot;, &amp;quot;Mailed check&amp;quot;, &amp;quot;Mailed c...
$ MonthlyCharges   &amp;lt;dbl&amp;gt; 29.85, 56.95, 53.85, 42.30, 70.70, 99.65, 89....
$ TotalCharges     &amp;lt;dbl&amp;gt; 29.85, 1889.50, 108.15, 1840.75, 151.65, 820....
$ Churn            &amp;lt;chr&amp;gt; &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, ...&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="preprocess-data"&gt;Preprocess Data&lt;/h2&gt;
&lt;p&gt;We’ll go through a few steps to preprocess the data for ML. First, we “prune” the data, which is nothing more than removing unnecessary columns and rows. Then we split into training and testing sets. After that we explore the training set to uncover transformations that will be needed for deep learning. We save the best for last. We end by preprocessing the data with the new &lt;a href="https://topepo.github.io/recipes"&gt;recipes&lt;/a&gt; package.&lt;/p&gt;
&lt;h3 id="prune-the-data"&gt;Prune The Data&lt;/h3&gt;
&lt;p&gt;The data has a few columns and rows we’d like to remove:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The “customerID” column is a unique identifier for each observation that isn’t needed for modeling. We can de-select this column.&lt;/li&gt;
&lt;li&gt;The data has 11 &lt;code&gt;NA&lt;/code&gt; values all in the “TotalCharges” column. Because it’s such a small percentage of the total population (99.8% complete cases), we can drop these observations with the &lt;code&gt;drop_na()&lt;/code&gt; function from &lt;a href="http://tidyr.tidyverse.org"&gt;tidyr&lt;/a&gt;. Note that these may be customers that have not yet been charged, and therefore an alternative is to replace with zero or -99 to segregate this population from the rest.&lt;/li&gt;
&lt;li&gt;My preference is to have the target in the first column so we’ll include a final select() ooperation to do so.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We’ll perform the cleaning operation with one tidyverse pipe (%&amp;gt;%) chain.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Remove unnecessary data
churn_data_tbl &amp;lt;- churn_data_raw %&amp;gt;%
  select(-customerID) %&amp;gt;%
  drop_na() %&amp;gt;%
  select(Churn, everything())
    
glimpse(churn_data_tbl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Observations: 7,032
Variables: 20
$ Churn            &amp;lt;chr&amp;gt; &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, ...
$ gender           &amp;lt;chr&amp;gt; &amp;quot;Female&amp;quot;, &amp;quot;Male&amp;quot;, &amp;quot;Male&amp;quot;, &amp;quot;Male&amp;quot;, &amp;quot;Female&amp;quot;, &amp;quot;...
$ SeniorCitizen    &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
$ Partner          &amp;lt;chr&amp;gt; &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;N...
$ Dependents       &amp;lt;chr&amp;gt; &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;N...
$ tenure           &amp;lt;int&amp;gt; 1, 34, 2, 45, 2, 8, 22, 10, 28, 62, 13, 16, 5...
$ PhoneService     &amp;lt;chr&amp;gt; &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;...
$ MultipleLines    &amp;lt;chr&amp;gt; &amp;quot;No phone service&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No phone ser...
$ InternetService  &amp;lt;chr&amp;gt; &amp;quot;DSL&amp;quot;, &amp;quot;DSL&amp;quot;, &amp;quot;DSL&amp;quot;, &amp;quot;DSL&amp;quot;, &amp;quot;Fiber optic&amp;quot;, &amp;quot;F...
$ OnlineSecurity   &amp;lt;chr&amp;gt; &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, ...
$ OnlineBackup     &amp;lt;chr&amp;gt; &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, ...
$ DeviceProtection &amp;lt;chr&amp;gt; &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, ...
$ TechSupport      &amp;lt;chr&amp;gt; &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;N...
$ StreamingTV      &amp;lt;chr&amp;gt; &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;...
$ StreamingMovies  &amp;lt;chr&amp;gt; &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;N...
$ Contract         &amp;lt;chr&amp;gt; &amp;quot;Month-to-month&amp;quot;, &amp;quot;One year&amp;quot;, &amp;quot;Month-to-month...
$ PaperlessBilling &amp;lt;chr&amp;gt; &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Yes&amp;quot;...
$ PaymentMethod    &amp;lt;chr&amp;gt; &amp;quot;Electronic check&amp;quot;, &amp;quot;Mailed check&amp;quot;, &amp;quot;Mailed c...
$ MonthlyCharges   &amp;lt;dbl&amp;gt; 29.85, 56.95, 53.85, 42.30, 70.70, 99.65, 89....
$ TotalCharges     &amp;lt;dbl&amp;gt; 29.85, 1889.50, 108.15, 1840.75, 151.65, 820..&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="split-into-traintest-sets"&gt;Split Into Train/Test Sets&lt;/h3&gt;
&lt;p&gt;We have a new package, &lt;a href="https://topepo.github.io/rsample/"&gt;rsample&lt;/a&gt;, which is very useful for sampling methods. It has the &lt;code&gt;initial_split()&lt;/code&gt; function for splitting data sets into training and testing sets. The return is a special &lt;code&gt;rsplit&lt;/code&gt; object.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Split test/training sets
set.seed(100)
train_test_split &amp;lt;- initial_split(churn_data_tbl, prop = 0.8)
train_test_split&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;5626/1406/7032&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can retrieve our training and testing sets using &lt;code&gt;training()&lt;/code&gt; and &lt;code&gt;testing()&lt;/code&gt; functions.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Retrieve train and test sets
train_tbl &amp;lt;- training(train_test_split)
test_tbl  &amp;lt;- testing(train_test_split) &lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="exploration-what-transformation-steps-are-needed-for-ml"&gt;Exploration: What Transformation Steps Are Needed For ML?&lt;/h3&gt;
&lt;p&gt;This phase of the analysis is often called exploratory analysis, but basically &lt;strong&gt;we are trying to answer the question, “What steps are needed to prepare for ML?” The key concept is knowing what transformations are needed to run the algorithm most effectively&lt;/strong&gt;. Artificial Neural Networks are best when the data is one-hot encoded, scaled and centered. In addition, other transformations may be beneficial as well to make relationships easier for the algorithm to identify. A full exploratory analysis is not practical in this article. With that said we’ll cover a few tips on transformations that can help as they relate to this dataset. In the next section, we will implement the preprocessing techniques.&lt;/p&gt;
&lt;h4 id="discretize-the-tenure-feature"&gt;Discretize The “tenure” Feature&lt;/h4&gt;
&lt;p&gt;Numeric features like age, years worked, length of time in a position can generalize a group (or cohort). We see this in marketing a lot (think “millennials”, which identifies a group born in a certain timeframe). The “tenure” feature falls into this category of numeric features that can be discretized into groups.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-11-keras-customer-churn/images/figure-html/unnamed-chunk-9-1.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can split into six cohorts that divide up the user base by tenure in roughly one year (12 month) increments. This should help the ML algorithm detect if a group is more/less susceptible to customer churn.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-11-keras-customer-churn/images/figure-html/unnamed-chunk-10-1.png" /&gt;&lt;/p&gt;
&lt;h4 id="transform-the-totalcharges-feature"&gt;Transform The “TotalCharges” Feature&lt;/h4&gt;
&lt;p&gt;What we don’t like to see is when a lot of observations are bunched within a small part of the range.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-11-keras-customer-churn/images/figure-html/unnamed-chunk-11-1.png" /&gt;&lt;/p&gt;
&lt;p&gt;We can use a log transformation to even out the data into more of a normal distribution. It’s not perfect, but it’s quick and easy to get our data spread out a bit more.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-11-keras-customer-churn/images/figure-html/unnamed-chunk-12-1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pro Tip: A quick test is to see if the log transformation increases the magnitude of the correlation&lt;/strong&gt; between “TotalCharges” and “Churn”. We’ll use a few &lt;a href="http://dplyr.tidyverse.org/"&gt;dplyr&lt;/a&gt; operations along with the &lt;a href="https://github.com/drsimonj/corrr"&gt;corrr&lt;/a&gt; package to perform a quick correlation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;correlate()&lt;/code&gt;: Performs tidy correlations on numeric data&lt;/li&gt;
&lt;li&gt;&lt;code&gt;focus()&lt;/code&gt;: Similar to &lt;code&gt;select()&lt;/code&gt;. Takes columns and focuses on only the rows/columns of importance.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fashion()&lt;/code&gt;: Makes the formatting aesthetically easier to read.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Determine if log transformation improves correlation 
# between TotalCharges and Churn
train_tbl %&amp;gt;%
  select(Churn, TotalCharges) %&amp;gt;%
  mutate(
      Churn = Churn %&amp;gt;% as.factor() %&amp;gt;% as.numeric(),
      LogTotalCharges = log(TotalCharges)
      ) %&amp;gt;%
  correlate() %&amp;gt;%
  focus(Churn) %&amp;gt;%
  fashion()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;          rowname Churn
1    TotalCharges  -.20
2 LogTotalCharges  -.25&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The correlation between “Churn” and “LogTotalCharges” is greatest in magnitude indicating the log transformation should improve the accuracy of the ANN model we build. Therefore, we should perform the log transformation.&lt;/p&gt;
&lt;h4 id="one-hot-encoding"&gt;One-Hot Encoding&lt;/h4&gt;
&lt;p&gt;One-hot encoding is the process of converting categorical data to sparse data, which has columns of only zeros and ones (this is also called creating “dummy variables” or a “design matrix”). All non-numeric data will need to be converted to dummy variables. This is simple for binary Yes/No data because we can simply convert to 1’s and 0’s. It becomes slightly more complicated with multiple categories, which requires creating new columns of 1’s and 0`s for each category (actually one less). We have four features that are multi-category: Contract, Internet Service, Multiple Lines, and Payment Method.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-11-keras-customer-churn/images/figure-html/unnamed-chunk-14-1.png" /&gt;&lt;/p&gt;
&lt;h4 id="feature-scaling"&gt;Feature Scaling&lt;/h4&gt;
&lt;p&gt;ANN’s typically perform faster and often times with higher accuracy when the features are scaled and/or normalized (aka centered and scaled, also known as standardizing). Because ANNs use gradient descent, weights tend to update faster. According to &lt;a href="https://sebastianraschka.com/"&gt;&lt;em&gt;Sebastian Raschka&lt;/em&gt;&lt;/a&gt;, an expert in the field of Deep Learning, several examples when feature scaling is important are:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;k-nearest neighbors with an Euclidean distance measure if want all features to contribute equally&lt;/li&gt;
&lt;li&gt;k-means (see k-nearest neighbors)&lt;/li&gt;
&lt;li&gt;logistic regression, SVMs, perceptrons, neural networks etc. if you are using gradient descent/ascent-based optimization, otherwise some weights will update much faster than others&lt;/li&gt;
&lt;li&gt;linear discriminant analysis, principal component analysis, kernel principal component analysis since you want to find directions of maximizing the variance (under the constraints that those directions/eigenvectors/principal components are orthogonal); you want to have features on the same scale since you’d emphasize variables on “larger measurement scales” more. There are many more cases than I can possibly list here … I always recommend you to think about the algorithm and what it’s doing, and then it typically becomes obvious whether we want to scale your features or not.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;The interested reader can read &lt;a href="http://sebastianraschka.com/Articles/2014_about_feature_scaling.html"&gt;Sebastian Raschka’s article&lt;/a&gt; for a full discussion on the scaling/normalization topic. &lt;strong&gt;Pro Tip: When in doubt, standardize the data&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id="preprocessing-with-recipes"&gt;Preprocessing With Recipes&lt;/h3&gt;
&lt;p&gt;Let’s implement the preprocessing steps/transformations uncovered during our exploration. Max Kuhn (creator of &lt;a href="http://topepo.github.io/caret/index.html"&gt;caret&lt;/a&gt;) has been putting some work into &lt;em&gt;Rlang ML tools&lt;/em&gt; lately, and the payoff is beginning to take shape. &lt;strong&gt;A new package, &lt;a href="https://topepo.github.io/recipes"&gt;recipes&lt;/a&gt;, makes creating ML data preprocessing workflows a breeze&lt;/strong&gt;! It takes a little getting used to, but I’ve found that it really helps manage the preprocessing steps. We’ll go over the nitty gritty as it applies to this problem.&lt;/p&gt;
&lt;h4 id="step-1-create-a-recipe"&gt;Step 1: Create A Recipe&lt;/h4&gt;
&lt;p&gt;A “recipe” is nothing more than a series of steps you would like to perform on the training, testing and/or validation sets. Think of preprocessing data like baking a cake (I’m not a baker but stay with me). The recipe is our steps to make the cake. It doesn’t do anything other than create the playbook for baking.&lt;/p&gt;
&lt;p&gt;We use the &lt;code&gt;recipe()&lt;/code&gt; function to implement our preprocessing steps. The function takes a familiar &lt;code&gt;object&lt;/code&gt; argument, which is a modeling function such as &lt;code&gt;object = Churn ~ .&lt;/code&gt; meaning “Churn” is the outcome (aka response, predictor, target) and all other features are predictors. The function also takes the &lt;code&gt;data&lt;/code&gt; argument, which gives the “recipe steps” perspective on how to apply during baking (next).&lt;/p&gt;
&lt;p&gt;A recipe is not very useful until we add “steps”, which are used to transform the data during baking. The package contains a number of useful “step functions” that can be applied. The entire list of &lt;a href="https://topepo.github.io/recipes/reference/index.html"&gt;Step Functions&lt;/a&gt; can be viewed here. For our model, we use:&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;code&gt;step_discretize()&lt;/code&gt; with the &lt;code&gt;option = list(cuts = 6)&lt;/code&gt; to cut the continuous variable for “tenure” (number of years as a customer) to group customers into cohorts.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;step_log()&lt;/code&gt; to log transform “TotalCharges”.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;step_dummy()&lt;/code&gt; to one-hot encode the categorical data. Note that this adds columns of one/zero for categorical data with three or more categories.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;step_center()&lt;/code&gt; to mean-center the data.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;step_scale()&lt;/code&gt; to scale the data.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The last step is to prepare the recipe with the &lt;code&gt;prep()&lt;/code&gt; function. This step is used to “estimate the required parameters from a training set that can later be applied to other data sets”. This is important for centering and scaling and other functions that use parameters defined from the training set.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Here’s how simple it is to implement the preprocessing steps that we went over!&lt;/strong&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Create recipe
rec_obj &amp;lt;- recipe(Churn ~ ., data = train_tbl) %&amp;gt;%
  step_discretize(tenure, options = list(cuts = 6)) %&amp;gt;%
  step_log(TotalCharges) %&amp;gt;%
  step_dummy(all_nominal(), -all_outcomes()) %&amp;gt;%
  step_center(all_predictors(), -all_outcomes()) %&amp;gt;%
  step_scale(all_predictors(), -all_outcomes()) %&amp;gt;%
  prep(data = train_tbl)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can print the recipe object if we ever forget what steps were used to prepare the data. &lt;strong&gt;Pro Tip: We can save the recipe object as an RDS file using &lt;code&gt;saveRDS()&lt;/code&gt;, and then use it to &lt;code&gt;bake()&lt;/code&gt; (discussed next) future raw data into ML-ready data in production!&lt;/strong&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Print the recipe object
rec_obj&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Data Recipe

Inputs:

      role #variables
   outcome          1
 predictor         19

Training data contained 5626 data points and no missing data.

Steps:

Dummy variables from tenure [trained]
Log transformation on TotalCharges [trained]
Dummy variables from ~gender, ~Partner, ... [trained]
Centering for SeniorCitizen, ... [trained]
Scaling for SeniorCitizen, ... [trained]&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="step-2-baking-with-your-recipe"&gt;Step 2: Baking With Your Recipe&lt;/h4&gt;
&lt;p&gt;Now for the fun part! We can apply the “recipe” to any data set with the &lt;code&gt;bake()&lt;/code&gt; function, and it processes the data following our recipe steps. We’ll apply to our training and testing data to convert from raw data to a machine learning dataset. Check our training set out with &lt;code&gt;glimpse()&lt;/code&gt;. &lt;strong&gt;Now that’s an ML-ready dataset prepared for ANN modeling!!&lt;/strong&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Predictors
x_train_tbl &amp;lt;- bake(rec_obj, newdata = train_tbl) %&amp;gt;% select(-Churn)
x_test_tbl  &amp;lt;- bake(rec_obj, newdata = test_tbl) %&amp;gt;% select(-Churn)

glimpse(x_train_tbl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Observations: 5,626
Variables: 35
$ SeniorCitizen                         &amp;lt;dbl&amp;gt; -0.4351959, -0.4351...
$ MonthlyCharges                        &amp;lt;dbl&amp;gt; -1.1575972, -0.2601...
$ TotalCharges                          &amp;lt;dbl&amp;gt; -2.275819130, 0.389...
$ gender_Male                           &amp;lt;dbl&amp;gt; -1.0016900, 0.99813...
$ Partner_Yes                           &amp;lt;dbl&amp;gt; 1.0262054, -0.97429...
$ Dependents_Yes                        &amp;lt;dbl&amp;gt; -0.6507747, -0.6507...
$ tenure_bin1                           &amp;lt;dbl&amp;gt; 2.1677790, -0.46121...
$ tenure_bin2                           &amp;lt;dbl&amp;gt; -0.4389453, -0.4389...
$ tenure_bin3                           &amp;lt;dbl&amp;gt; -0.4481273, -0.4481...
$ tenure_bin4                           &amp;lt;dbl&amp;gt; -0.4509837, 2.21698...
$ tenure_bin5                           &amp;lt;dbl&amp;gt; -0.4498419, -0.4498...
$ tenure_bin6                           &amp;lt;dbl&amp;gt; -0.4337508, -0.4337...
$ PhoneService_Yes                      &amp;lt;dbl&amp;gt; -3.0407367, 0.32880...
$ MultipleLines_No.phone.service        &amp;lt;dbl&amp;gt; 3.0407367, -0.32880...
$ MultipleLines_Yes                     &amp;lt;dbl&amp;gt; -0.8571364, -0.8571...
$ InternetService_Fiber.optic           &amp;lt;dbl&amp;gt; -0.8884255, -0.8884...
$ InternetService_No                    &amp;lt;dbl&amp;gt; -0.5272627, -0.5272...
$ OnlineSecurity_No.internet.service    &amp;lt;dbl&amp;gt; -0.5272627, -0.5272...
$ OnlineSecurity_Yes                    &amp;lt;dbl&amp;gt; -0.6369654, 1.56966...
$ OnlineBackup_No.internet.service      &amp;lt;dbl&amp;gt; -0.5272627, -0.5272...
$ OnlineBackup_Yes                      &amp;lt;dbl&amp;gt; 1.3771987, -0.72598...
$ DeviceProtection_No.internet.service  &amp;lt;dbl&amp;gt; -0.5272627, -0.5272...
$ DeviceProtection_Yes                  &amp;lt;dbl&amp;gt; -0.7259826, 1.37719...
$ TechSupport_No.internet.service       &amp;lt;dbl&amp;gt; -0.5272627, -0.5272...
$ TechSupport_Yes                       &amp;lt;dbl&amp;gt; -0.6358628, -0.6358...
$ StreamingTV_No.internet.service       &amp;lt;dbl&amp;gt; -0.5272627, -0.5272...
$ StreamingTV_Yes                       &amp;lt;dbl&amp;gt; -0.7917326, -0.7917...
$ StreamingMovies_No.internet.service   &amp;lt;dbl&amp;gt; -0.5272627, -0.5272...
$ StreamingMovies_Yes                   &amp;lt;dbl&amp;gt; -0.797388, -0.79738...
$ Contract_One.year                     &amp;lt;dbl&amp;gt; -0.5156834, 1.93882...
$ Contract_Two.year                     &amp;lt;dbl&amp;gt; -0.5618358, -0.5618...
$ PaperlessBilling_Yes                  &amp;lt;dbl&amp;gt; 0.8330334, -1.20021...
$ PaymentMethod_Credit.card..automatic. &amp;lt;dbl&amp;gt; -0.5231315, -0.5231...
$ PaymentMethod_Electronic.check        &amp;lt;dbl&amp;gt; 1.4154085, -0.70638...
$ PaymentMethod_Mailed.check            &amp;lt;dbl&amp;gt; -0.5517013, 1.81225...&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="step-3-dont-forget-the-target"&gt;Step 3: Don’t Forget The Target&lt;/h4&gt;
&lt;p&gt;One last step, we need to store the actual values (truth) as &lt;code&gt;y_train_vec&lt;/code&gt; and &lt;code&gt;y_test_vec&lt;/code&gt;, which are needed for modeling our ANN. We convert to a series of numeric ones and zeros which can be accepted by the Keras ANN modeling functions. We add “vec” to the name so we can easily remember the class of the object (it’s easy to get confused when working with tibbles, vectors, and matrix data types).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Response variables for training and testing sets
y_train_vec &amp;lt;- ifelse(pull(train_tbl, Churn) == &amp;quot;Yes&amp;quot;, 1, 0)
y_test_vec  &amp;lt;- ifelse(pull(test_tbl, Churn) == &amp;quot;Yes&amp;quot;, 1, 0)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="model-customer-churn-with-keras-deep-learning"&gt;Model Customer Churn With Keras (Deep Learning)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;This is super exciting!! Finally, Deep Learning with Keras in R!&lt;/strong&gt; The team at RStudio has done fantastic work recently to create the &lt;a href="https://tensorflow.rstudio.com/keras/"&gt;keras&lt;/a&gt; package, which implements &lt;a href="https://keras.io/"&gt;Keras&lt;/a&gt; in R. Very cool!&lt;/p&gt;
&lt;h3 id="background-on-artifical-neural-networks"&gt;Background On Artifical Neural Networks&lt;/h3&gt;
&lt;p&gt;For those unfamiliar with Neural Networks (and those that need a refresher), &lt;a href="https://www.xenonstack.com/blog/overview-of-artificial-neural-networks-and-its-applications"&gt;read this article&lt;/a&gt;. It’s very comprehensive, and you’ll leave with a general understanding of the types of deep learning and how they work.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-11-keras-customer-churn/images/Artificial-Neural-Network-Architecture.jpg" /&gt;&lt;/p&gt;
&lt;p class="text-center date"&gt;
Source: &lt;a href="https://www.xenonstack.com/blog/overview-of-artificial-neural-networks-and-its-applications"&gt;Xenon Stack&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Deep Learning has been available in R for some time, but the primary packages used in the wild have not (this includes Keras, Tensor Flow, Theano, etc, which are all Python libraries). It’s worth mentioning that a number of other Deep Learning packages exist in R including &lt;code&gt;h2o&lt;/code&gt;, &lt;code&gt;mxnet&lt;/code&gt;, and others. The interested reader can check out &lt;a href="http://www.rblog.uni-freiburg.de/2017/02/07/deep-learning-in-r/"&gt;this blog post for a comparison of deep learning packages in R&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="building-a-deep-learning-model"&gt;Building A Deep Learning Model&lt;/h3&gt;
&lt;p&gt;We’re going to build a special class of ANN called a &lt;a href="https://en.wikipedia.org/wiki/Multilayer_perceptron"&gt;Multi-Layer Perceptron (MLP)&lt;/a&gt;. MLPs are one of the simplest forms of deep learning, but they are both highly accurate and serve as a jumping-off point for more complex algorithms. MLPs are quite versatile as they can be used for regression, binary and multi classification (and are typically quite good at classification problems).&lt;/p&gt;
&lt;p&gt;We’ll build a three layer MLP with Keras. Let’s walk-through the steps before we implement in R.&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Initialize a sequential model&lt;/strong&gt;: The first step is to initialize a sequential model with &lt;code&gt;keras_model_sequential()&lt;/code&gt;, which is the beginning of our Keras model. The sequential model is composed of a linear stack of layers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Apply layers to the sequential model&lt;/strong&gt;: Layers consist of the input layer, hidden layers and an output layer. The input layer is the data and provided it’s formatted correctly there’s nothing more to discuss. The hidden layers and output layers are what controls the ANN inner workings.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Hidden Layers&lt;/strong&gt;: Hidden layers form the neural network nodes that enable non-linear activation using weights. The hidden layers are created using &lt;code&gt;layer_dense()&lt;/code&gt;. We’ll add two hidden layers. We’ll apply &lt;code&gt;units = 16&lt;/code&gt;, which is the number of nodes. We’ll select &lt;code&gt;kernel_initializer = "uniform"&lt;/code&gt; and &lt;code&gt;activation = "relu"&lt;/code&gt; for both layers. The first layer needs to have the &lt;code&gt;input_shape = 35&lt;/code&gt;, which is the number of columns in the training set. &lt;strong&gt;Key Point: While we are arbitrarily selecting the number of hidden layers, units, kernel initializers and activation functions, these parameters can be optimized through a process called hyperparameter tuning that is discussed in &lt;a href="#next-steps"&gt;Next Steps&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Dropout Layers&lt;/strong&gt;: Dropout layers are used to control overfitting. This eliminates weights below a cutoff threshold to prevent low weights from overfitting the layers. We use the &lt;code&gt;layer_dropout()&lt;/code&gt; function add two drop out layers with &lt;code&gt;rate = 0.10&lt;/code&gt; to remove weights below 10%.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Output Layer&lt;/strong&gt;: The output layer specifies the shape of the output and the method of assimilating the learned information. The output layer is applied using the &lt;code&gt;layer_dense()&lt;/code&gt;. For binary values, the shape should be &lt;code&gt;units = 1&lt;/code&gt;. For multi-classification, the &lt;code&gt;units&lt;/code&gt; should correspond to the number of classes. We set the &lt;code&gt;kernel_initializer = "uniform"&lt;/code&gt; and the &lt;code&gt;activation = "sigmoid"&lt;/code&gt; (common for binary classification).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Compile the model&lt;/strong&gt;: The last step is to compile the model with &lt;code&gt;compile()&lt;/code&gt;. We’ll use &lt;code&gt;optimizer = "adam"&lt;/code&gt;, which is one of the most popular optimization algorithms. We select &lt;code&gt;loss = "binary_crossentropy"&lt;/code&gt; since this is a binary classification problem. We’ll select &lt;code&gt;metrics = c("accuracy")&lt;/code&gt; to be evaluated during training and testing. &lt;strong&gt;Key Point: The optimizer is often included in the tuning process&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let’s codify the discussion above to build our Keras MLP-flavored ANN model.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Building our Artificial Neural Network
model_keras &amp;lt;- keras_model_sequential()

model_keras %&amp;gt;% 
  
  # First hidden layer
  layer_dense(
    units              = 16, 
    kernel_initializer = &amp;quot;uniform&amp;quot;, 
    activation         = &amp;quot;relu&amp;quot;, 
    input_shape        = ncol(x_train_tbl)) %&amp;gt;% 
  
  # Dropout to prevent overfitting
  layer_dropout(rate = 0.1) %&amp;gt;%
  
  # Second hidden layer
  layer_dense(
    units              = 16, 
    kernel_initializer = &amp;quot;uniform&amp;quot;, 
    activation         = &amp;quot;relu&amp;quot;) %&amp;gt;% 
  
  # Dropout to prevent overfitting
  layer_dropout(rate = 0.1) %&amp;gt;%
  
  # Output layer
  layer_dense(
    units              = 1, 
    kernel_initializer = &amp;quot;uniform&amp;quot;, 
    activation         = &amp;quot;sigmoid&amp;quot;) %&amp;gt;% 
  
  # Compile ANN
  compile(
    optimizer = &amp;#39;adam&amp;#39;,
    loss      = &amp;#39;binary_crossentropy&amp;#39;,
    metrics   = c(&amp;#39;accuracy&amp;#39;)
  )

keras_model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model
___________________________________________________________________________________________________
Layer (type)                                Output Shape                            Param #        
===================================================================================================
dense_1 (Dense)                             (None, 16)                              576            
___________________________________________________________________________________________________
dropout_1 (Dropout)                         (None, 16)                              0              
___________________________________________________________________________________________________
dense_2 (Dense)                             (None, 16)                              272            
___________________________________________________________________________________________________
dropout_2 (Dropout)                         (None, 16)                              0              
___________________________________________________________________________________________________
dense_3 (Dense)                             (None, 1)                               17             
===================================================================================================
Total params: 865
Trainable params: 865
Non-trainable params: 0
___________________________________________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use the &lt;code&gt;fit()&lt;/code&gt; function to run the ANN on our training data. The &lt;code&gt;object&lt;/code&gt; is our model, and &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are our training data in matrix and numeric vector forms, respectively. The &lt;code&gt;batch_size = 50&lt;/code&gt; sets the number samples per gradient update within each epoch. We set &lt;code&gt;epochs = 35&lt;/code&gt; to control the number training cycles. Typically we want to keep the batch size high since this decreases the error within each training cycle (epoch). We also want epochs to be large, which is important in visualizing the training history (discussed below). We set &lt;code&gt;validation_split = 0.30&lt;/code&gt; to include 30% of the data for model validation, which prevents overfitting. The training process should complete in 15 seconds or so.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Fit the keras model to the training data
history &amp;lt;- fit(
  object           = model_keras, 
  x                = as.matrix(x_train_tbl), 
  y                = y_train_vec,
  batch_size       = 50, 
  epochs           = 35,
  validation_split = 0.30
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can inspect the training history. We want to make sure there is minimal difference between the validation accuracy and the training accuracy.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Print a summary of the training history
print(history)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Trained on 3,938 samples, validated on 1,688 samples (batch_size=50, epochs=35)
Final epoch (plot to see history):
val_loss: 0.4215
 val_acc: 0.8057
    loss: 0.399
     acc: 0.8101&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can visualize the Keras training history using the &lt;code&gt;plot()&lt;/code&gt; function. What we want to see is the validation accuracy and loss leveling off, which means the model has completed training. We see that there is some divergence between training loss/accuracy and validation loss/accuracy. This model indicates we can possibly stop training at an earlier epoch. &lt;strong&gt;Pro Tip: Only use enough epochs to get a high validation accuracy. Once validation accuracy curve begins to flatten or decrease, it’s time to stop training.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Plot the training/validation history of our Keras model
plot(history) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-11-keras-customer-churn/images/figure-html/unnamed-chunk-23-1.png" /&gt;&lt;/p&gt;
&lt;h3 id="making-predictions"&gt;Making Predictions&lt;/h3&gt;
&lt;p&gt;We’ve got a good model based on the validation accuracy. Now let’s make some predictions from our &lt;a href="https://tensorflow.rstudio.com/keras/"&gt;keras&lt;/a&gt; model on the test data set, which was unseen during modeling (we use this for the true performance assessment). We have two functions to generate predictions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;predict_classes()&lt;/code&gt;: Generates class values as a matrix of ones and zeros. Since we are dealing with binary classification, we’ll convert the output to a vector.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;predict_proba()&lt;/code&gt;: Generates the class probabilities as a numeric matrix indicating the probability of being a class. Again, we convert to a numeric vector because there is only one column output.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Predicted Class
yhat_keras_class_vec &amp;lt;- predict_classes(object = model_keras, x = as.matrix(x_test_tbl)) %&amp;gt;%
    as.vector()

# Predicted Class Probability
yhat_keras_prob_vec  &amp;lt;- predict_proba(object = model_keras, x = as.matrix(x_test_tbl)) %&amp;gt;%
    as.vector()&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="inspect-performance-with-yardstick"&gt;Inspect Performance With Yardstick&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;yardstick&lt;/code&gt; package has a collection of handy functions for measuring performance of machine learning models. We’ll overview some metrics we can use to understand the performance of our model.&lt;/p&gt;
&lt;p&gt;First, let’s get the data formatted for &lt;code&gt;yardstick&lt;/code&gt;. We create a data frame with the truth (actual values as factors), estimate (predicted values as factors), and the class probability (probability of yes as numeric). We use the &lt;code&gt;fct_recode()&lt;/code&gt; function from the &lt;a href="http://forcats.tidyverse.org"&gt;forcats&lt;/a&gt; package to assist with recoding as Yes/No values.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Format test data and predictions for yardstick metrics
estimates_keras_tbl &amp;lt;- tibble(
  truth      = as.factor(y_test_vec) %&amp;gt;% fct_recode(yes = &amp;quot;1&amp;quot;, no = &amp;quot;0&amp;quot;),
  estimate   = as.factor(yhat_keras_class_vec) %&amp;gt;% fct_recode(yes = &amp;quot;1&amp;quot;, no = &amp;quot;0&amp;quot;),
  class_prob = yhat_keras_prob_vec
)

estimates_keras_tbl&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1,406 x 3
    truth estimate  class_prob
   &amp;lt;fctr&amp;gt;   &amp;lt;fctr&amp;gt;       &amp;lt;dbl&amp;gt;
 1    yes       no 0.328355074
 2    yes      yes 0.633630514
 3     no       no 0.004589651
 4     no       no 0.007402068
 5     no       no 0.049968336
 6     no       no 0.116824441
 7     no      yes 0.775479317
 8     no       no 0.492996633
 9     no       no 0.011550998
10     no       no 0.004276015
# ... with 1,396 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have the data formatted, we can take advantage of the &lt;code&gt;yardstick&lt;/code&gt; package. The only other thing we need to do is to set &lt;code&gt;options(yardstick.event_first = FALSE)&lt;/code&gt;. As pointed out by &lt;a href="https://github.com/ad1729"&gt;ad1729&lt;/a&gt; in &lt;a href="options(yardstick.event_first%20=%20FALSE)"&gt;GitHub Issue 13&lt;/a&gt;, the default is to classify 0 as the positive class instead of 1.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;options(yardstick.event_first = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="confusion-table"&gt;Confusion Table&lt;/h4&gt;
&lt;p&gt;We can use the &lt;code&gt;conf_mat()&lt;/code&gt; function to get the confusion table. We see that the model was by no means perfect, but it did a decent job of identifying customers likely to churn.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Confusion Table
estimates_keras_tbl %&amp;gt;% conf_mat(truth, estimate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;          Truth
Prediction  no yes
       no  950 161
       yes  99 196&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="accuracy"&gt;Accuracy&lt;/h4&gt;
&lt;p&gt;We can use the &lt;code&gt;metrics()&lt;/code&gt; function to get an accuracy measurement from the test set. We are getting roughly 82% accuracy.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Accuracy
estimates_keras_tbl %&amp;gt;% metrics(truth, estimate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1 x 1
   accuracy
      &amp;lt;dbl&amp;gt;
1 0.8150782&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="auc"&gt;AUC&lt;/h4&gt;
&lt;p&gt;We can also get the ROC Area Under the Curve (AUC) measurement. AUC is often a good metric used to compare different classifiers and to compare to randomly guessing (AUC_random = 0.50). Our model has AUC = 0.85, which is much better than randomly guessing. Tuning and testing different classification algorithms may yield even better results.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# AUC
estimates_keras_tbl %&amp;gt;% roc_auc(truth, class_prob)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.8523951&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="precision-and-recall"&gt;Precision And Recall&lt;/h4&gt;
&lt;p&gt;Precision is when the model predicts “yes”, how often is it actually “yes”. Recall (also true positive rate or specificity) is when the actual value is “yes” how often is the model correct. We can get &lt;code&gt;precision()&lt;/code&gt; and &lt;code&gt;recall()&lt;/code&gt; measurements using &lt;code&gt;yardstick&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Precision
tibble(
  precision = estimates_keras_tbl %&amp;gt;% precision(truth, estimate),
  recall    = estimates_keras_tbl %&amp;gt;% recall(truth, estimate)
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1 x 2
  precision    recall
      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
1 0.6644068 0.5490196&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Precision and recall are very important to the business case: The organization is concerned with &lt;strong&gt;balancing the cost of targeting and retaining customers at risk of leaving with the cost of inadvertently targeting customers that are not planning to leave&lt;/strong&gt; (and potentially decreasing revenue from this group). The threshold above which to predict Churn = “Yes” can be adjusted to optimize for the business problem. This becomes an &lt;strong&gt;Customer Lifetime Value optimization problem&lt;/strong&gt; that is discussed further in &lt;a href="#next-steps"&gt;Next Steps&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id="f1-score"&gt;F1 Score&lt;/h4&gt;
&lt;p&gt;We can also get the F1-score, which is a weighted average between the precision and recall. Machine learning classifier thresholds are often adjusted to maximize the F1-score. However, this is often not the optimal solution to the business problem.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# F1-Statistic
estimates_keras_tbl %&amp;gt;% f_meas(truth, estimate, beta = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.601227&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="explain-the-model-with-lime"&gt;Explain The Model With LIME&lt;/h2&gt;
&lt;p&gt;LIME stands for &lt;em&gt;Local Interpretable Model-agnostic Explanations&lt;/em&gt;, and is a method for explaining black-box machine learning model classifiers. For those new to LIME, this YouTube video does a really nice job explaining how LIME helps to identify feature importance with black box machine learning models (e.g. deep learning, stacked ensembles, random forest).&lt;/p&gt;
&lt;div class="l-body-outset"&gt;
&lt;iframe height="500" width="100%" align="center" src="https://www.youtube.com/embed/hUnRCxnydCc" frameborder="1" allowfullscreen&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;h4 id="setup"&gt;Setup&lt;/h4&gt;
&lt;p&gt;The &lt;a href="https://github.com/thomasp85/lime"&gt;lime&lt;/a&gt; package implements &lt;a href="https://github.com/marcotcr/lime"&gt;LIME&lt;/a&gt; in R. One thing to note is that it’s not setup out-of-the-box to work with &lt;code&gt;keras&lt;/code&gt;. The good news is with a few functions we can get everything working properly. We’ll need to make two custom functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;model_type&lt;/code&gt;: Used to tell &lt;code&gt;lime&lt;/code&gt; what type of model we are dealing with. It could be classification, regression, survival, etc.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;predict_model&lt;/code&gt;: Used to allow &lt;code&gt;lime&lt;/code&gt; to perform predictions that its algorithm can interpret.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first thing we need to do is identify the class of our model object. We do this with the &lt;code&gt;class()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;class(model_keras)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;keras.models.Sequential&amp;quot;        
[2] &amp;quot;keras.engine.training.Model&amp;quot;    
[3] &amp;quot;keras.engine.topology.Container&amp;quot;
[4] &amp;quot;keras.engine.topology.Layer&amp;quot;    
[5] &amp;quot;python.builtin.object&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we create our &lt;code&gt;model_type()&lt;/code&gt; function. It’s only input is &lt;code&gt;x&lt;/code&gt; the keras model. The function simply returns “classification”, which tells LIME we are classifying.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Setup lime::model_type() function for keras
model_type.keras.models.Sequential &amp;lt;- function(x, ...) {
  &amp;quot;classification&amp;quot;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can create our &lt;code&gt;predict_model()&lt;/code&gt; function, which wraps &lt;code&gt;keras::predict_proba()&lt;/code&gt;. The trick here is to realize that it’s inputs must be &lt;code&gt;x&lt;/code&gt; a model, &lt;code&gt;newdata&lt;/code&gt; a dataframe object (this is important), and &lt;code&gt;type&lt;/code&gt; which is not used but can be use to switch the output type. The output is also a little tricky because it &lt;em&gt;must be in the format of probabilities by classification&lt;/em&gt; (this is important; shown next).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Setup lime::predict_model() function for keras
predict_model.keras.models.Sequential &amp;lt;- function(x, newdata, type, ...) {
  pred &amp;lt;- predict_proba(object = x, x = as.matrix(newdata))
  data.frame(Yes = pred, No = 1 - pred)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run this next script to show you what the output looks like and to test our &lt;code&gt;predict_model()&lt;/code&gt; function. See how it’s the probabilities by classification. It must be in this form for &lt;code&gt;model_type = "classification"&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Test our predict_model() function
predict_model(x = model_keras, newdata = x_test_tbl, type = &amp;#39;raw&amp;#39;) %&amp;gt;%
  tibble::as_tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1,406 x 2
           Yes        No
         &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
 1 0.328355074 0.6716449
 2 0.633630514 0.3663695
 3 0.004589651 0.9954103
 4 0.007402068 0.9925979
 5 0.049968336 0.9500317
 6 0.116824441 0.8831756
 7 0.775479317 0.2245207
 8 0.492996633 0.5070034
 9 0.011550998 0.9884490
10 0.004276015 0.9957240
# ... with 1,396 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the fun part, we create an explainer using the &lt;code&gt;lime()&lt;/code&gt; function. Just pass the training data set without the “Attribution column”. The form must be a data frame, which is OK since our &lt;code&gt;predict_model&lt;/code&gt; function will switch it to an &lt;code&gt;keras&lt;/code&gt; object. Set &lt;code&gt;model = automl_leader&lt;/code&gt; our leader model, and &lt;code&gt;bin_continuous = FALSE&lt;/code&gt;. We could tell the algorithm to bin continuous variables, but this may not make sense for categorical numeric data that we didn’t change to factors.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Run lime() on training set
explainer &amp;lt;- lime::lime(
  x              = x_train_tbl, 
  model          = model_keras, 
  bin_continuous = FALSE
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we run the &lt;code&gt;explain()&lt;/code&gt; function, which returns our &lt;code&gt;explanation&lt;/code&gt;. This can take a minute to run so we limit it to just the first ten rows of the test data set. We set &lt;code&gt;n_labels = 1&lt;/code&gt; because we care about explaining a single class. Setting &lt;code&gt;n_features = 4&lt;/code&gt; returns the top four features that are critical to each case. Finally, setting &lt;code&gt;kernel_width = 0.5&lt;/code&gt; allows us to increase the “model_r2” value by shrinking the localized evaluation.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Run explain() on explainer
explanation &amp;lt;- lime::explain(
  x_test_tbl[1:10, ], 
  explainer    = explainer, 
  n_labels     = 1, 
  n_features   = 4,
  kernel_width = 0.5
)&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="feature-importance-visualization"&gt;Feature Importance Visualization&lt;/h4&gt;
&lt;p&gt;The payoff for the work we put in using LIME is this &lt;strong&gt;feature importance plot&lt;/strong&gt;. This allows us to visualize each of the first ten cases (observations) from the test data. The top four features for each case are shown. Note that they are not the same for each case. The green bars mean that the feature supports the model conclusion, and the red bars contradict. A few important features based on frequency in first ten cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tenure (7 cases)&lt;/li&gt;
&lt;li&gt;Senior Citizen (5 cases)&lt;/li&gt;
&lt;li&gt;Online Security (4 cases)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot_features(explanation) +
  labs(title = &amp;quot;LIME Feature Importance Visualization&amp;quot;,
       subtitle = &amp;quot;Hold Out (Test) Set, First 10 Cases Shown&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-11-keras-customer-churn/images/figure-html/unnamed-chunk-41-1.png" /&gt;&lt;/p&gt;
&lt;p&gt;Another excellent visualization can be performed using &lt;code&gt;plot_explanations()&lt;/code&gt;, which produces a facetted heatmap of all case/label/feature combinations. It’s a more condensed version of &lt;code&gt;plot_features()&lt;/code&gt;, but we need to be careful because it does not provide exact statistics and it makes it less easy to investigate binned features (Notice that “tenure” would not be identified as a contributor even though it shows up as a top feature in 7 of 10 cases).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot_explanations(explanation) +
    labs(title = &amp;quot;LIME Feature Importance Heatmap&amp;quot;,
         subtitle = &amp;quot;Hold Out (Test) Set, First 10 Cases Shown&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="l-body-outset"&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-11-keras-customer-churn/images/figure-html/unnamed-chunk-42-1.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="check-explanations-with-correlation-analysis"&gt;Check Explanations With Correlation Analysis&lt;/h2&gt;
&lt;p&gt;One thing we need to be careful with the LIME visualization is that we are only doing a sample of the data, in our case the first 10 test observations. Therefore, we are gaining a very localized understanding of how the ANN works. However, we also want to know on from a global perspective what drives feature importance.&lt;/p&gt;
&lt;p&gt;We can perform a &lt;strong&gt;correlation analysis&lt;/strong&gt; on the training set as well to help glean what features correlate globally to “Churn”. We’ll use the &lt;code&gt;corrr&lt;/code&gt; package, which performs tidy correlations with the function &lt;code&gt;correlate()&lt;/code&gt;. We can get the correlations as follows.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Feature correlations to Churn
corrr_analysis &amp;lt;- x_train_tbl %&amp;gt;%
  mutate(Churn = y_train_vec) %&amp;gt;%
  correlate() %&amp;gt;%
  focus(Churn) %&amp;gt;%
  rename(feature = rowname) %&amp;gt;%
  arrange(abs(Churn)) %&amp;gt;%
  mutate(feature = as_factor(feature)) 
corrr_analysis&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 35 x 2
                          feature        Churn
                           &amp;lt;fctr&amp;gt;        &amp;lt;dbl&amp;gt;
 1                    gender_Male -0.006690899
 2                    tenure_bin3 -0.009557165
 3 MultipleLines_No.phone.service -0.016950072
 4               PhoneService_Yes  0.016950072
 5              MultipleLines_Yes  0.032103354
 6                StreamingTV_Yes  0.066192594
 7            StreamingMovies_Yes  0.067643871
 8           DeviceProtection_Yes -0.073301197
 9                    tenure_bin4 -0.073371838
10     PaymentMethod_Mailed.check -0.080451164
# ... with 25 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The correlation visualization helps in distinguishing which features are relavant to Churn.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Correlation visualization
corrr_analysis %&amp;gt;%
  ggplot(aes(x = Churn, y = fct_reorder(feature, desc(Churn)))) +
  geom_point() +
  # Positive Correlations - Contribute to churn
  geom_segment(aes(xend = 0, yend = feature), 
               color = palette_light()[[2]], 
               data = corrr_analysis %&amp;gt;% filter(Churn &amp;gt; 0)) +
  geom_point(color = palette_light()[[2]], 
             data = corrr_analysis %&amp;gt;% filter(Churn &amp;gt; 0)) +
  # Negative Correlations - Prevent churn
  geom_segment(aes(xend = 0, yend = feature), 
               color = palette_light()[[1]], 
               data = corrr_analysis %&amp;gt;% filter(Churn &amp;lt; 0)) +
  geom_point(color = palette_light()[[1]], 
             data = corrr_analysis %&amp;gt;% filter(Churn &amp;lt; 0)) +
  # Vertical lines
  geom_vline(xintercept = 0, color = palette_light()[[5]], size = 1, linetype = 2) +
  geom_vline(xintercept = -0.25, color = palette_light()[[5]], size = 1, linetype = 2) +
  geom_vline(xintercept = 0.25, color = palette_light()[[5]], size = 1, linetype = 2) +
  # Aesthetics
  theme_tq() +
  labs(title = &amp;quot;Churn Correlation Analysis&amp;quot;,
       subtitle = paste(&amp;quot;Positive Correlations (contribute to churn),&amp;quot;,
                        &amp;quot;Negative Correlations (prevent churn)&amp;quot;)
       y = &amp;quot;Feature Importance&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="l-body-outset"&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-11-keras-customer-churn/images/figure-html/unnamed-chunk-44-1.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The correlation analysis helps us quickly disseminate which features that the LIME analysis may be excluding. We can see that the following features are highly correlated (magnitude &amp;gt; 0.25):&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Increases Likelihood of Churn (Red)&lt;/strong&gt;: - Tenure = Bin 1 (&amp;lt;12 Months) - Internet Service = “Fiber Optic” - Payment Method = “Electronic Check”&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Decreases Likelihood of Churn (Blue)&lt;/strong&gt;: - Contract = “Two Year” - Total Charges (Note that this may be a biproduct of additional services such as Online Security)&lt;/p&gt;
&lt;h2 id="feature-investigation"&gt;Feature Investigation&lt;/h2&gt;
&lt;p&gt;We can investigate features that are &lt;strong&gt;most frequent&lt;/strong&gt; in the LIME feature importance visualization along with those that the &lt;strong&gt;correlation analysis shows an above normal magnitude&lt;/strong&gt;. We’ll investigate:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tenure (7/10 LIME Cases, Highly Correlated)&lt;/li&gt;
&lt;li&gt;Contract (Highly Correlated)&lt;/li&gt;
&lt;li&gt;Internet Service (Highly Correlated)&lt;/li&gt;
&lt;li&gt;Payment Method (Highly Correlated)&lt;/li&gt;
&lt;li&gt;Senior Citizen (5/10 LIME Cases)&lt;/li&gt;
&lt;li&gt;Online Security (4/10 LIME Cases)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="tenure-710-lime-cases-highly-correlated"&gt;Tenure (7/10 LIME Cases, Highly Correlated)&lt;/h4&gt;
&lt;p&gt;LIME cases indicate that the ANN model is using this feature frequently and high correlation agrees that this is important. Investigating the feature distribution, it appears that customers with lower tenure (bin 1) are more likely to leave. &lt;strong&gt;Opportunity: Target customers with less than 12 month tenure.&lt;/strong&gt;&lt;/p&gt;
&lt;div class="l-body-outset"&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-11-keras-customer-churn/images/figure-html/unnamed-chunk-45-1.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h4 id="contract-highly-correlated"&gt;Contract (Highly Correlated)&lt;/h4&gt;
&lt;p&gt;While LIME did not indicate this as a primary feature in the first 10 cases, the feature is clearly correlated with those electing to stay. Customers with one and two year contracts are much less likely to churn. &lt;strong&gt;Opportunity: Offer promotion to switch to long term contracts.&lt;/strong&gt;&lt;/p&gt;
&lt;div class="l-body-outset"&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-11-keras-customer-churn/images/figure-html/unnamed-chunk-46-1.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h4 id="internet-service-highly-correlated"&gt;Internet Service (Highly Correlated)&lt;/h4&gt;
&lt;p&gt;While LIME did not indicate this as a primary feature in the first 10 cases, the feature is clearly correlated with those electing to stay. Customers with fiber optic service are more likely to churn while those with no internet service are less likely to churn. &lt;strong&gt;Improvement Area: Customers may be dissatisfied with fiber optic service.&lt;/strong&gt;&lt;/p&gt;
&lt;div class="l-body-outset"&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-11-keras-customer-churn/images/figure-html/unnamed-chunk-47-1.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h4 id="payment-method-highly-correlated"&gt;Payment Method (Highly Correlated)&lt;/h4&gt;
&lt;p&gt;While LIME did not indicate this as a primary feature in the first 10 cases, the feature is clearly correlated with those electing to stay. Customers with electronic check are more likely to leave. &lt;strong&gt;Opportunity: Offer customers a promotion to switch to automatic payments&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="l-body-outset"&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-11-keras-customer-churn/images/figure-html/unnamed-chunk-48-1.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h4 id="senior-citizen-510-lime-cases"&gt;Senior Citizen (5/10 LIME Cases)&lt;/h4&gt;
&lt;p&gt;Senior citizen appeared in several of the LIME cases indicating it was important to the ANN for the 10 samples. However, it was not highly correlated to Churn, which may indicate that the ANN is using in an more sophisticated manner (e.g. as an interaction). It’s difficult to say that senior citizens are more likely to leave, but non-senior citizens appear less at risk of churning. &lt;strong&gt;Opportunity: Target users in the lower age demographic.&lt;/strong&gt;&lt;/p&gt;
&lt;div class="l-body-outset"&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-11-keras-customer-churn/images/figure-html/unnamed-chunk-49-1.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h4 id="online-security-410-lime-cases"&gt;Online Security (4/10 LIME Cases)&lt;/h4&gt;
&lt;p&gt;Customers that did not sign up for online security were more likely to leave while customers with no internet service or online security were less likely to leave. &lt;strong&gt;Opportunity: Promote online security and other packages that increase retention rates.&lt;/strong&gt;&lt;/p&gt;
&lt;div class="l-body-outset"&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-11-keras-customer-churn/images/figure-html/unnamed-chunk-50-1.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="next-steps-business-science-university"&gt;Next Steps: Business Science University&lt;/h2&gt;
&lt;p&gt;We’ve just scratched the surface with the solution to this problem, but unfortunately there’s only so much ground we can cover in an article. Here are a few next steps that I’m pleased to announce will be covered in a &lt;a href="https://university.business-science.io/"&gt;&lt;strong&gt;Business Science University&lt;/strong&gt;&lt;/a&gt; course coming in 2018!&lt;/p&gt;
&lt;h3 id="customer-lifetime-value"&gt;Customer Lifetime Value&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Your organization needs to see the financial benefit so always tie your analysis to sales, profitability or ROI.&lt;/strong&gt; &lt;a href="https://en.wikipedia.org/wiki/Customer_lifetime_value"&gt;Customer Lifetime Value (&lt;em&gt;CLV&lt;/em&gt;)&lt;/a&gt; is a methodology that ties the business profitability to the retention rate. While we did not implement the CLV methodology herein, a full customer churn analysis would tie the churn to an classification cutoff (threshold) optimization to maximize the CLV with the predictive ANN model.&lt;/p&gt;
&lt;p&gt;The simplified CLV model is:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[ 
CLV=GC*\frac{1}{1+d-r} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;GC&lt;/em&gt; is the gross contribution per customer&lt;/li&gt;
&lt;li&gt;&lt;em&gt;d&lt;/em&gt; is the annual discount rate&lt;/li&gt;
&lt;li&gt;&lt;em&gt;r&lt;/em&gt; is the retention rate&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="ann-performance-evaluation-and-improvement"&gt;ANN Performance Evaluation and Improvement&lt;/h3&gt;
&lt;p&gt;The ANN model we built is good, but it could be better. How we understand our model accuracy and improve on it is through the combination of two techniques:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;K-Fold Cross-Fold Validation&lt;/strong&gt;: Used to obtain bounds for accuracy estimates.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hyper Parameter Tuning&lt;/strong&gt;: Used to improve model performance by searching for the best parameters possible.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We need to implement &lt;em&gt;K-Fold Cross Validation&lt;/em&gt; and &lt;em&gt;Hyper Parameter Tuning&lt;/em&gt; if we want a best-in-class model.&lt;/p&gt;
&lt;h3 id="distributing-analytics"&gt;Distributing Analytics&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;It’s critical to communicate data science insights to decision makers in the organization&lt;/strong&gt;. Most decision makers in organizations are not data scientists, but these individuals make important decisions on a day-to-day basis. The Shiny application below includes a &lt;strong&gt;Customer Scorecard&lt;/strong&gt; to monitor customer health (risk of churn).&lt;/p&gt;
&lt;div class="l-body-outset"&gt;
&lt;p&gt;&lt;a href="https://jjallaire.shinyapps.io/keras-customer-churn/"&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2018-01-11-keras-customer-churn/images/shiny-application.png" class="illustration" width="100%"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id="business-science-university"&gt;Business Science University&lt;/h3&gt;
&lt;p&gt;You’re probably wondering why we are going into so much detail on next steps. We are happy to announce a new project for 2018: &lt;a href="https://university.business-science.io/"&gt;&lt;strong&gt;Business Science University&lt;/strong&gt;&lt;/a&gt;, an online school dedicated to helping data science learners.&lt;/p&gt;
&lt;p&gt;Benefits to learners:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Build your own &lt;strong&gt;online GitHub portfolio&lt;/strong&gt; of data science projects to market your skills to future employers!&lt;/li&gt;
&lt;li&gt;Learn &lt;strong&gt;real-world applications&lt;/strong&gt; in People Analytics (HR), Customer Analytics, Marketing Analytics, Social Media Analytics, Text Mining and Natural Language Processing (NLP), Financial and Time Series Analytics, and more!&lt;/li&gt;
&lt;li&gt;Use &lt;strong&gt;advanced machine learning techniques&lt;/strong&gt; for both high accuracy modeling and explaining features that have an effect on the outcome!&lt;/li&gt;
&lt;li&gt;Create &lt;strong&gt;ML-powered web-applications&lt;/strong&gt; that can be distributed throughout an organization, enabling non-data scientists to benefit from algorithms in a user-friendly way!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Enrollment is open&lt;/strong&gt; so please signup for special perks. Just go to &lt;a href="https://university.business-science.io/"&gt;&lt;strong&gt;Business Science University&lt;/strong&gt;&lt;/a&gt; and select enroll.&lt;/p&gt;
&lt;h2 id="conclusions"&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Customer churn is a costly problem&lt;/strong&gt;. The good news is that &lt;strong&gt;machine learning can solve churn problems&lt;/strong&gt;, making the organization more profitable in the process. In this article, we saw how &lt;strong&gt;Deep Learning can be used to predict customer churn&lt;/strong&gt;. We built an ANN model using the new &lt;a href="https://tensorflow.rstudio.com/keras/"&gt;keras&lt;/a&gt; package that achieved &lt;strong&gt;82% predictive accuracy&lt;/strong&gt; (without tuning)! We used three new machine learning packages to help with preprocessing and measuring performance: &lt;a href="https://topepo.github.io/recipes"&gt;recipes&lt;/a&gt;, &lt;a href="https://topepo.github.io/rsample/"&gt;rsample&lt;/a&gt; and &lt;a href="https://github.com/topepo/yardstick"&gt;yardstick&lt;/a&gt;. Finally we used &lt;a href="https://github.com/thomasp85/lime"&gt;lime&lt;/a&gt; to explain the Deep Learning model, which &lt;strong&gt;traditionally was impossible&lt;/strong&gt;! We checked the LIME results with a &lt;strong&gt;Correlation Analysis&lt;/strong&gt;, which brought to light other features to investigate. For the IBM Telco dataset, tenure, contract type, internet service type, payment menthod, senior citizen status, and online security status were useful in diagnosing customer churn. We hope you enjoyed this article!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">da08226b780bfddb3c51ca0c5efa2c2e</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Tabular Data</category>
      <category>Explainability</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn</guid>
      <pubDate>Thu, 11 Jan 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn/images/customer_churn_analysis_corrr.png" medium="image" type="image/png" width="2696" height="1696"/>
    </item>
    <item>
      <title>Classifying Duplicate Questions from Quora with Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Falbel</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-01-09-keras-duplicate-questions-quora</link>
      <description>


&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this post we will use Keras to classify duplicated questions from Quora. The dataset first appeared in the Kaggle competition &lt;a href="https://www.kaggle.com/c/quora-question-pairs"&gt;Quora Question Pairs&lt;/a&gt; and consists of approximately 400,000 pairs of questions along with a column indicating if the question pair is considered a duplicate.&lt;/p&gt;
&lt;p&gt;Our implementation is inspired by the &lt;a href="https://dl.acm.org/citation.cfm?id=3016291"&gt;Siamese Recurrent Architecture&lt;/a&gt;, with modifications to the similarity measure and the embedding layers (the original paper uses pre-trained word vectors). Using this kind of architecture dates back to 2005 with &lt;a href="https://dl.acm.org/citation.cfm?id=1068961"&gt;Le Cun et al&lt;/a&gt; and is useful for verification tasks. The idea is to learn a function that maps input patterns into a target space such that a similarity measure in the target space approximates the “semantic” distance in the input space.&lt;/p&gt;
&lt;p&gt;After the competition, Quora also described their approach to this problem in this &lt;a href="https://engineering.quora.com/Semantic-Question-Matching-with-Deep-Learning"&gt;blog post&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="dowloading-data"&gt;Dowloading data&lt;/h2&gt;
&lt;p&gt;Data can be downloaded from the Kaggle &lt;a href="https://www.kaggle.com/quora/question-pairs-dataset"&gt;dataset webpage&lt;/a&gt; or from Quora’s &lt;a href="https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs"&gt;release of the dataset&lt;/a&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
quora_data &amp;lt;- get_file(
  &amp;quot;quora_duplicate_questions.tsv&amp;quot;,
  &amp;quot;https://qim.ec.quoracdn.net/quora_duplicate_questions.tsv&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are using the Keras &lt;code&gt;get_file()&lt;/code&gt; function so that the file download is cached.&lt;/p&gt;
&lt;h2 id="reading-and-preprocessing"&gt;Reading and preprocessing&lt;/h2&gt;
&lt;p&gt;We will first load data into R and do some preprocessing to make it easier to include in the model. After downloading the data, you can read it using the readr &lt;code&gt;read_tsv()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(readr)
df &amp;lt;- read_tsv(quora_data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will create a Keras &lt;code&gt;tokenizer&lt;/code&gt; to transform each word into an integer token. We will also specify a hyperparameter of our model: the vocabulary size. For now let’s use the 50,000 most common words (we’ll tune this parameter later). The tokenizer will be fit using all unique questions from the dataset.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;tokenizer &amp;lt;- text_tokenizer(num_words = 50000)
tokenizer %&amp;gt;% fit_text_tokenizer(unique(c(df$question1, df$question2)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s save the tokenizer to disk in order to use it for inference later.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;save_text_tokenizer(tokenizer, &amp;quot;tokenizer-question-pairs&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will now use the text tokenizer to transform each question into a list of integers.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;question1 &amp;lt;- texts_to_sequences(tokenizer, df$question1)
question2 &amp;lt;- texts_to_sequences(tokenizer, df$question2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at the number of words in each question. This will helps us to decide the padding length, another hyperparameter of our model. Padding the sequences normalizes them to the same size so that we can feed them to the Keras model.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(purrr)
questions_length &amp;lt;- c(
  map_int(question1, length),
  map_int(question2, length)
)

quantile(questions_length, c(0.8, 0.9, 0.95, 0.99))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;80% 90% 95% 99% 
 14  18  23  31 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that 99% of questions have at most length 31 so we’ll choose a padding length between 15 and 30. Let’s start with 20 (we’ll also tune this parameter later). The default padding value is 0, but we are already using this value for words that don’t appear within the 50,000 most frequent, so we’ll use 50,001 instead.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;question1_padded &amp;lt;- pad_sequences(question1, maxlen = 20, value = 50000 + 1)
question2_padded &amp;lt;- pad_sequences(question2, maxlen = 20, value = 50000 + 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have now finished the preprocessing steps. We will now run a simple benchmark model before moving on to the Keras model.&lt;/p&gt;
&lt;h2 id="simple-benchmark"&gt;Simple benchmark&lt;/h2&gt;
&lt;p&gt;Before creating a complicated model let’s take a simple approach. Let’s create two predictors: percentage of words from question1 that appear in the question2 and vice-versa. Then we will use a logistic regression to predict if the questions are duplicate.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;perc_words_question1 &amp;lt;- map2_dbl(question1, question2, ~mean(.x %in% .y))
perc_words_question2 &amp;lt;- map2_dbl(question2, question1, ~mean(.x %in% .y))

df_model &amp;lt;- data.frame(
  perc_words_question1 = perc_words_question1,
  perc_words_question2 = perc_words_question2,
  is_duplicate = df$is_duplicate
) %&amp;gt;%
  na.omit()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have our predictors, let’s create the logistic model. We will take a small sample for validation.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;val_sample &amp;lt;- sample.int(nrow(df_model), 0.1*nrow(df_model))
logistic_regression &amp;lt;- glm(
  is_duplicate ~ perc_words_question1 + perc_words_question2, 
  family = &amp;quot;binomial&amp;quot;,
  data = df_model[-val_sample,]
)
summary(logistic_regression)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Call:
glm(formula = is_duplicate ~ perc_words_question1 + perc_words_question2, 
    family = &amp;quot;binomial&amp;quot;, data = df_model[-val_sample, ])

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5938  -0.9097  -0.6106   1.1452   2.0292  

Coefficients:
                      Estimate Std. Error z value Pr(&amp;gt;|z|)    
(Intercept)          -2.259007   0.009668 -233.66   &amp;lt;2e-16 ***
perc_words_question1  1.517990   0.023038   65.89   &amp;lt;2e-16 ***
perc_words_question2  1.681410   0.022795   73.76   &amp;lt;2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 479158  on 363843  degrees of freedom
Residual deviance: 431627  on 363841  degrees of freedom
  (17 observations deleted due to missingness)
AIC: 431633

Number of Fisher Scoring iterations: 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s calculate the accuracy on our validation set.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pred &amp;lt;- predict(logistic_regression, df_model[val_sample,], type = &amp;quot;response&amp;quot;)
pred &amp;lt;- pred &amp;gt; mean(df_model$is_duplicate[-val_sample])
accuracy &amp;lt;- table(pred, df_model$is_duplicate[val_sample]) %&amp;gt;% 
  prop.table() %&amp;gt;% 
  diag() %&amp;gt;% 
  sum()
accuracy&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.6573577&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We got an accuracy of 65.7%. Not all that much better than random guessing. Now let’s create our model in Keras.&lt;/p&gt;
&lt;h2 id="model-definition"&gt;Model definition&lt;/h2&gt;
&lt;p&gt;We will use a Siamese network to predict whether the pairs are duplicated or not. The idea is to create a model that can embed the questions (sequence of words) into a vector. Then we can compare the vectors for each question using a similarity measure and tell if the questions are duplicated or not.&lt;/p&gt;
&lt;p&gt;First let’s define the inputs for the model.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;input1 &amp;lt;- layer_input(shape = c(20), name = &amp;quot;input_question1&amp;quot;)
input2 &amp;lt;- layer_input(shape = c(20), name = &amp;quot;input_question2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then let’s the define the part of the model that will embed the questions in a vector.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;word_embedder &amp;lt;- layer_embedding( 
  input_dim = 50000 + 2, # vocab size + UNK token + padding value
  output_dim = 128,      # hyperparameter - embedding size
  input_length = 20,     # padding size,
  embeddings_regularizer = regularizer_l2(0.0001) # hyperparameter - regularization 
)

seq_embedder &amp;lt;- layer_lstm(
  units = 128, # hyperparameter -- sequence embedding size
  kernel_regularizer = regularizer_l2(0.0001) # hyperparameter - regularization 
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we will define the relationship between the input vectors and the embeddings layers. Note that we use the same layers and weights on both inputs. That’s why this is called a Siamese network. It makes sense, because we don’t want to have different outputs if question1 is switched with question2.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;vector1 &amp;lt;- input1 %&amp;gt;% word_embedder() %&amp;gt;% seq_embedder()
vector2 &amp;lt;- input2 %&amp;gt;% word_embedder() %&amp;gt;% seq_embedder()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then define the similarity measure we want to optimize. We want duplicated questions to have higher values of similarity. In this example we’ll use the cosine similarity, but any similarity measure could be used. Remember that the cosine similarity is the normalized dot product of the vectors, but for training it’s not necessary to normalize the results.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;cosine_similarity &amp;lt;- layer_dot(list(vector1, vector2), axes = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we define a final sigmoid layer to output the probability of both questions being duplicated.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;output &amp;lt;- cosine_similarity %&amp;gt;% 
  layer_dense(units = 1, activation = &amp;quot;sigmoid&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that let’s define the Keras model in terms of it’s inputs and outputs and compile it. In the compilation phase we define our loss function and optimizer. Like in the Kaggle challenge, we will minimize the logloss (equivalent to minimizing the binary crossentropy). We will use the Adam optimizer.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model(list(input1, input2), output)
model %&amp;gt;% compile(
  optimizer = &amp;quot;adam&amp;quot;, 
  metrics = list(acc = metric_binary_accuracy), 
  loss = &amp;quot;binary_crossentropy&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then take a look at out model with the &lt;code&gt;summary&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;_______________________________________________________________________________________
Layer (type)                Output Shape       Param #    Connected to                 
=======================================================================================
input_question1 (InputLayer (None, 20)         0                                       
_______________________________________________________________________________________
input_question2 (InputLayer (None, 20)         0                                       
_______________________________________________________________________________________
embedding_1 (Embedding)     (None, 20, 128)    6400256    input_question1[0][0]        
                                                          input_question2[0][0]        
_______________________________________________________________________________________
lstm_1 (LSTM)               (None, 128)        131584     embedding_1[0][0]            
                                                          embedding_1[1][0]            
_______________________________________________________________________________________
dot_1 (Dot)                 (None, 1)          0          lstm_1[0][0]                 
                                                          lstm_1[1][0]                 
_______________________________________________________________________________________
dense_1 (Dense)             (None, 1)          2          dot_1[0][0]                  
=======================================================================================
Total params: 6,531,842
Trainable params: 6,531,842
Non-trainable params: 0
_______________________________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="model-fitting"&gt;Model fitting&lt;/h2&gt;
&lt;p&gt;Now we will fit and tune our model. However before proceeding let’s take a sample for validation.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;set.seed(1817328)
val_sample &amp;lt;- sample.int(nrow(question1_padded), size = 0.1*nrow(question1_padded))

train_question1_padded &amp;lt;- question1_padded[-val_sample,]
train_question2_padded &amp;lt;- question2_padded[-val_sample,]
train_is_duplicate &amp;lt;- df$is_duplicate[-val_sample]

val_question1_padded &amp;lt;- question1_padded[val_sample,]
val_question2_padded &amp;lt;- question2_padded[val_sample,]
val_is_duplicate &amp;lt;- df$is_duplicate[val_sample]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we use the &lt;code&gt;fit()&lt;/code&gt; function to train the model:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% fit(
  list(train_question1_padded, train_question2_padded),
  train_is_duplicate, 
  batch_size = 64, 
  epochs = 10, 
  validation_data = list(
    list(val_question1_padded, val_question2_padded), 
    val_is_duplicate
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Train on 363861 samples, validate on 40429 samples
Epoch 1/10
363861/363861 [==============================] - 89s 245us/step - loss: 0.5860 - acc: 0.7248 - val_loss: 0.5590 - val_acc: 0.7449
Epoch 2/10
363861/363861 [==============================] - 88s 243us/step - loss: 0.5528 - acc: 0.7461 - val_loss: 0.5472 - val_acc: 0.7510
Epoch 3/10
363861/363861 [==============================] - 88s 242us/step - loss: 0.5428 - acc: 0.7536 - val_loss: 0.5439 - val_acc: 0.7515
Epoch 4/10
363861/363861 [==============================] - 88s 242us/step - loss: 0.5353 - acc: 0.7595 - val_loss: 0.5358 - val_acc: 0.7590
Epoch 5/10
363861/363861 [==============================] - 88s 242us/step - loss: 0.5299 - acc: 0.7633 - val_loss: 0.5358 - val_acc: 0.7592
Epoch 6/10
363861/363861 [==============================] - 88s 242us/step - loss: 0.5256 - acc: 0.7662 - val_loss: 0.5309 - val_acc: 0.7631
Epoch 7/10
363861/363861 [==============================] - 88s 242us/step - loss: 0.5211 - acc: 0.7701 - val_loss: 0.5349 - val_acc: 0.7586
Epoch 8/10
363861/363861 [==============================] - 88s 242us/step - loss: 0.5173 - acc: 0.7733 - val_loss: 0.5278 - val_acc: 0.7667
Epoch 9/10
363861/363861 [==============================] - 88s 242us/step - loss: 0.5138 - acc: 0.7762 - val_loss: 0.5292 - val_acc: 0.7667
Epoch 10/10
363861/363861 [==============================] - 88s 242us/step - loss: 0.5092 - acc: 0.7794 - val_loss: 0.5313 - val_acc: 0.7654&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After training completes, we can save our model for inference with the &lt;code&gt;save_model_hdf5()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;save_model_hdf5(model, &amp;quot;model-question-pairs.hdf5&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="model-tuning"&gt;Model tuning&lt;/h2&gt;
&lt;p&gt;Now that we have a reasonable model, let’s tune the hyperparameters using the &lt;a href="https://tensorflow.rstudio.com/tools/tfruns"&gt;tfruns&lt;/a&gt; package. We’ll begin by adding &lt;code&gt;FLAGS&lt;/code&gt; declarations to our script for all hyperparameters we want to tune (&lt;code&gt;FLAGS&lt;/code&gt; allow us to vary hyperparmaeters without changing our source code):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;FLAGS &amp;lt;- flags(
  flag_integer(&amp;quot;vocab_size&amp;quot;, 50000),
  flag_integer(&amp;quot;max_len_padding&amp;quot;, 20),
  flag_integer(&amp;quot;embedding_size&amp;quot;, 256),
  flag_numeric(&amp;quot;regularization&amp;quot;, 0.0001),
  flag_integer(&amp;quot;seq_embedding_size&amp;quot;, 512)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this &lt;code&gt;FLAGS&lt;/code&gt; definition we can now write our code in terms of the flags. For example:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;input1 &amp;lt;- layer_input(shape = c(FLAGS$max_len_padding))
input2 &amp;lt;- layer_input(shape = c(FLAGS$max_len_padding))

embedding &amp;lt;- layer_embedding(
  input_dim = FLAGS$vocab_size + 2, 
  output_dim = FLAGS$embedding_size, 
  input_length = FLAGS$max_len_padding, 
  embeddings_regularizer = regularizer_l2(l = FLAGS$regularization)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The full source code of the script with &lt;code&gt;FLAGS&lt;/code&gt; can be found &lt;a href="https://gist.github.com/dfalbel/a5d63d6bffe683072cc4781d7c8420ff"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We additionally added an early stopping callback in the training step in order to stop training if validation loss doesn’t decrease for 5 epochs in a row. This will hopefully reduce training time for bad models. We also added a learning rate reducer to reduce the learning rate by a factor of 10 when the loss doesn’t decrease for 3 epochs (this technique typically increases model accuracy).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% fit(
  ...,
  callbacks = list(
    callback_early_stopping(patience = 5),
    callback_reduce_lr_on_plateau(patience = 3)
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now execute a tuning run to probe for the optimal combination of hyperparameters. We call the &lt;code&gt;tuning_run()&lt;/code&gt; function, passing a list with the possible values for each flag. The &lt;code&gt;tuning_run()&lt;/code&gt; function will be responsible for executing the script for all combinations of hyperparameters. We also specify the &lt;code&gt;sample&lt;/code&gt; parameter to train the model for only a random sample from all combinations (reducing training time significantly).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tfruns)

runs &amp;lt;- tuning_run(
  &amp;quot;question-pairs.R&amp;quot;, 
  flags = list(
    vocab_size = c(30000, 40000, 50000, 60000),
    max_len_padding = c(15, 20, 25),
    embedding_size = c(64, 128, 256),
    regularization = c(0.00001, 0.0001, 0.001),
    seq_embedding_size = c(128, 256, 512)
  ), 
  runs_dir = &amp;quot;tuning&amp;quot;, 
  sample = 0.2
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The tuning run will return a &lt;code&gt;data.frame&lt;/code&gt; with results for all runs. We found that the best run attained 84.9% accuracy using the combination of hyperparameters shown below, so we modify our training script to use these values as the defaults:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;FLAGS &amp;lt;- flags(
  flag_integer(&amp;quot;vocab_size&amp;quot;, 50000),
  flag_integer(&amp;quot;max_len_padding&amp;quot;, 20),
  flag_integer(&amp;quot;embedding_size&amp;quot;, 256),
  flag_numeric(&amp;quot;regularization&amp;quot;, 1e-4),
  flag_integer(&amp;quot;seq_embedding_size&amp;quot;, 512)
)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="making-predictions"&gt;Making predictions&lt;/h2&gt;
&lt;p&gt;Now that we have trained and tuned our model we can start making predictions. At prediction time we will load both the text tokenizer and the model we saved to disk earlier.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
model &amp;lt;- load_model_hdf5(&amp;quot;model-question-pairs.hdf5&amp;quot;, compile = FALSE)
tokenizer &amp;lt;- load_text_tokenizer(&amp;quot;tokenizer-question-pairs&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we won’t continue training the model, we specified the &lt;code&gt;compile = FALSE&lt;/code&gt; argument.&lt;/p&gt;
&lt;p&gt;Now let`s define a function to create predictions. In this function we we preprocess the input data in the same way we preprocessed the training data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;predict_question_pairs &amp;lt;- function(model, tokenizer, q1, q2) {
  q1 &amp;lt;- texts_to_sequences(tokenizer, list(q1))
  q2 &amp;lt;- texts_to_sequences(tokenizer, list(q2))
  
  q1 &amp;lt;- pad_sequences(q1, 20)
  q2 &amp;lt;- pad_sequences(q2, 20)
  
  as.numeric(predict(model, list(q1, q2)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now call it with new pairs of questions, for example:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;predict_question_pairs(
  model,
  tokenizer,
  &amp;quot;What&amp;#39;s R programming?&amp;quot;,
  &amp;quot;What&amp;#39;s R in programming?&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.9784008&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Prediction is quite fast (~40 milliseconds).&lt;/p&gt;
&lt;h2 id="deploying-the-model"&gt;Deploying the model&lt;/h2&gt;
&lt;p&gt;To demonstrate deployment of the trained model, we created a simple &lt;a href="https://shiny.rstudio.com"&gt;Shiny&lt;/a&gt; application, where you can paste 2 questions from Quora and find the probability of them being duplicated. Try changing the questions below or entering two entirely different questions.&lt;/p&gt;
&lt;iframe src="https://jjallaire.shinyapps.io/shiny-quora/" height="405"&gt;
&lt;/iframe&gt;
&lt;p&gt;The shiny application can be found at &lt;a href="https://jjallaire.shinyapps.io/shiny-quora/" class="uri"&gt;https://jjallaire.shinyapps.io/shiny-quora/&lt;/a&gt; and it’s source code at &lt;a href="https://github.com/dfalbel/shiny-quora-question-pairs" class="uri"&gt;https://github.com/dfalbel/shiny-quora-question-pairs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Note that when deploying a Keras model you only need to load the previously saved model file and tokenizer (no training data or model training steps are required).&lt;/p&gt;
&lt;h2 id="wrapping-up"&gt;Wrapping up&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We trained a Siamese LSTM that gives us reasonable accuracy (84%). Quora’s state of the art is 87%.&lt;/li&gt;
&lt;li&gt;We can improve our model by using pre-trained word embeddings on larger datasets. For example, try using what’s described in &lt;a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/pretrained_word_embeddings.R"&gt;this example&lt;/a&gt;. Quora uses their own complete corpus to train the word embeddings.&lt;/li&gt;
&lt;li&gt;After training we deployed our model as a Shiny application which given two Quora questions calculates the probability of their being duplicates.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">a67643f94949b4830be4ae193ccda499</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Natural Language Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-01-09-keras-duplicate-questions-quora</guid>
      <pubDate>Tue, 09 Jan 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-01-09-keras-duplicate-questions-quora/keras-duplicate-questions-quora.png" medium="image" type="image/png" width="1302" height="788"/>
    </item>
    <item>
      <title>Word Embeddings with Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Falbel</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2017-12-22-word-embeddings-with-keras</link>
      <description>


&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Word embedding is a method used to map words of a vocabulary to dense vectors of real numbers where semantically similar words are mapped to nearby points. Representing words in this vector space help algorithms achieve better performance in natural language processing tasks like syntactic parsing and sentiment analysis by grouping similar words. For example, we expect that in the embedding space “cats” and “dogs” are mapped to nearby points since they are both animals, mammals, pets, etc.&lt;/p&gt;
&lt;p&gt;In this tutorial we will implement the skip-gram model created by &lt;a href="https://arxiv.org/abs/1301.3781"&gt;Mikolov et al&lt;/a&gt; in R using the &lt;a href="https://keras.rstudio.com/"&gt;keras&lt;/a&gt; package. The skip-gram model is a flavor of word2vec, a class of computationally-efficient predictive models for learning word embeddings from raw text. We won’t address theoretical details about embeddings and the skip-gram model. If you want to get more details you can read the paper linked above. The TensorFlow &lt;a href="https://www.tensorflow.org/tutorials/word2vec"&gt;Vector Representation of Words&lt;/a&gt; tutorial includes additional details as does the &lt;em&gt;Deep Learning With R&lt;/em&gt; &lt;a href="https://jjallaire.github.io/deep-learning-with-r-notebooks/notebooks/6.1-using-word-embeddings.nb.html"&gt;notebook about embeddings&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are other ways to create vector representations of words. For example, GloVe Embeddings are implemented in the &lt;a href="https://cran.r-project.org/web/packages/text2vec/vignettes/glove.html"&gt;text2vec&lt;/a&gt; package by Dmitriy Selivanov. There’s also a tidy approach described in Julia Silge’s blog post &lt;a href="https://juliasilge.com/blog/tidy-word-vectors/"&gt;Word Vectors with Tidy Data Principles&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="getting-the-data"&gt;Getting the Data&lt;/h2&gt;
&lt;p&gt;We will use the &lt;a href="https://snap.stanford.edu/data/web-FineFoods.html"&gt;Amazon Fine Foods Reviews dataset&lt;/a&gt;. This dataset consists of reviews of fine foods from Amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and narrative text.&lt;/p&gt;
&lt;p&gt;Data can be downloaded (~116MB) by running:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;download.file(&amp;quot;https://snap.stanford.edu/data/finefoods.txt.gz&amp;quot;, &amp;quot;finefoods.txt.gz&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will now load the plain text reviews into R.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(readr)
library(stringr)
reviews &amp;lt;- read_lines(&amp;quot;finefoods.txt.gz&amp;quot;) 
reviews &amp;lt;- reviews[str_sub(reviews, 1, 12) == &amp;quot;review/text:&amp;quot;]
reviews &amp;lt;- str_sub(reviews, start = 14)
reviews &amp;lt;- iconv(reviews, to = &amp;quot;UTF-8&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at some reviews we have in the dataset.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;head(reviews, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;I have bought several of the Vitality canned dog food products ...
[2] &amp;quot;Product arrived labeled as Jumbo Salted Peanuts...the peanuts ... &lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="preprocessing"&gt;Preprocessing&lt;/h2&gt;
&lt;p&gt;We’ll begin with some text pre-processing using a keras &lt;code&gt;text_tokenizer()&lt;/code&gt;. The tokenizer will be responsible for transforming each review into a sequence of integer tokens (which will subsequently be used as input into the skip-gram model).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
tokenizer &amp;lt;- text_tokenizer(num_words = 20000)
tokenizer %&amp;gt;% fit_text_tokenizer(reviews)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the &lt;code&gt;tokenizer&lt;/code&gt; object is modified in place by the call to &lt;code&gt;fit_text_tokenizer()&lt;/code&gt;. An integer token will be assigned for each of the 20,000 most common words (the other words will be assigned to token 0).&lt;/p&gt;
&lt;h2 id="skip-gram-model"&gt;Skip-Gram Model&lt;/h2&gt;
&lt;p&gt;In the skip-gram model we will use each word as input to a log-linear classifier with a projection layer, then predict words within a certain range before and after this word. It would be very computationally expensive to output a probability distribution over all the vocabulary for each target word we input into the model. Instead, we are going to use negative sampling, meaning we will sample some words that don’t appear in the context and train a binary classifier to predict if the context word we passed is truly from the context or not.&lt;/p&gt;
&lt;p&gt;In more practical terms, for the skip-gram model we will input a 1d integer vector of the target word tokens and a 1d integer vector of sampled context word tokens. We will generate a prediction of 1 if the sampled word really appeared in the context and 0 if it didn’t.&lt;/p&gt;
&lt;p&gt;We will now define a generator function to yield batches for model training.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(reticulate)
library(purrr)
skipgrams_generator &amp;lt;- function(text, tokenizer, window_size, negative_samples) {
  gen &amp;lt;- texts_to_sequences_generator(tokenizer, sample(text))
  function() {
    skip &amp;lt;- generator_next(gen) %&amp;gt;%
      skipgrams(
        vocabulary_size = tokenizer$num_words, 
        window_size = window_size, 
        negative_samples = 1
      )
    x &amp;lt;- transpose(skip$couples) %&amp;gt;% map(. %&amp;gt;% unlist %&amp;gt;% as.matrix(ncol = 1))
    y &amp;lt;- skip$labels %&amp;gt;% as.matrix(ncol = 1)
    list(x, y)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A &lt;a href="https://keras.rstudio.com/articles/faq.html#how-can-i-use-keras-with-datasets-that-dont-fit-in-memory"&gt;generator function&lt;/a&gt; is a function that returns a different value each time it is called (generator functions are often used to provide streaming or dynamic data for training models). Our generator function will receive a vector of texts, a tokenizer and the arguments for the skip-gram (the size of the window around each target word we examine and how many negative samples we want to sample for each target word).&lt;/p&gt;
&lt;p&gt;Now let’s start defining the keras model. We will use the Keras &lt;a href="https://keras.rstudio.com/articles/functional_api.html"&gt;functional API&lt;/a&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;embedding_size &amp;lt;- 128  # Dimension of the embedding vector.
skip_window &amp;lt;- 5       # How many words to consider left and right.
num_sampled &amp;lt;- 1       # Number of negative examples to sample for each word.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will first write placeholders for the inputs using the &lt;code&gt;layer_input&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;input_target &amp;lt;- layer_input(shape = 1)
input_context &amp;lt;- layer_input(shape = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s define the embedding matrix. The embedding is a matrix with dimensions (vocabulary, embedding_size) that acts as lookup table for the word vectors.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;embedding &amp;lt;- layer_embedding(
  input_dim = tokenizer$num_words + 1, 
  output_dim = embedding_size, 
  input_length = 1, 
  name = &amp;quot;embedding&amp;quot;
)

target_vector &amp;lt;- input_target %&amp;gt;% 
  embedding() %&amp;gt;% 
  layer_flatten()

context_vector &amp;lt;- input_context %&amp;gt;%
  embedding() %&amp;gt;%
  layer_flatten()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next step is to define how the &lt;code&gt;target_vector&lt;/code&gt; will be related to the &lt;code&gt;context_vector&lt;/code&gt; in order to make our network output 1 when the context word really appeared in the context and 0 otherwise. We want &lt;code&gt;target_vector&lt;/code&gt; to be &lt;em&gt;similar&lt;/em&gt; to the &lt;code&gt;context_vector&lt;/code&gt; if they appeared in the same context. A typical measure of similarity is the &lt;a href="https://en.wikipedia.org/wiki/Cosine_similarity"&gt;cosine similarity&lt;/a&gt;. Give two vectors &lt;span class="math inline"&gt;\(A\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(B\)&lt;/span&gt; the cosine similarity is defined by the Euclidean Dot product of &lt;span class="math inline"&gt;\(A\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(B\)&lt;/span&gt; normalized by their magnitude. As we don’t need the similarity to be normalized inside the network, we will only calculate the dot product and then output a dense layer with sigmoid activation.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;dot_product &amp;lt;- layer_dot(list(target_vector, context_vector), axes = 1)
output &amp;lt;- layer_dense(dot_product, units = 1, activation = &amp;quot;sigmoid&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we will create the model and compile it.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model(list(input_target, input_context), output)
model %&amp;gt;% compile(loss = &amp;quot;binary_crossentropy&amp;quot;, optimizer = &amp;quot;adam&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see the full definition of the model by calling &lt;code&gt;summary&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;_________________________________________________________________________________________
Layer (type)                 Output Shape       Param #    Connected to                  
=========================================================================================
input_1 (InputLayer)         (None, 1)          0                                        
_________________________________________________________________________________________
input_2 (InputLayer)         (None, 1)          0                                        
_________________________________________________________________________________________
embedding (Embedding)        (None, 1, 128)     2560128    input_1[0][0]                 
                                                           input_2[0][0]                 
_________________________________________________________________________________________
flatten_1 (Flatten)          (None, 128)        0          embedding[0][0]               
_________________________________________________________________________________________
flatten_2 (Flatten)          (None, 128)        0          embedding[1][0]               
_________________________________________________________________________________________
dot_1 (Dot)                  (None, 1)          0          flatten_1[0][0]               
                                                           flatten_2[0][0]               
_________________________________________________________________________________________
dense_1 (Dense)              (None, 1)          2          dot_1[0][0]                   
=========================================================================================
Total params: 2,560,130
Trainable params: 2,560,130
Non-trainable params: 0
_________________________________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="model-training"&gt;Model Training&lt;/h2&gt;
&lt;p&gt;We will fit the model using the &lt;code&gt;fit_generator()&lt;/code&gt; function We need to specify the number of training steps as well as number of epochs we want to train. We will train for 100,000 steps for 5 epochs. This is quite slow (~1000 seconds per epoch on a modern GPU). Note that you may also get reasonable results with just one epoch of training.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;%
  fit_generator(
    skipgrams_generator(reviews, tokenizer, skip_window, negative_samples), 
    steps_per_epoch = 100000, epochs = 5
    )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch 1/1
100000/100000 [==============================] - 1092s - loss: 0.3749      
Epoch 2/5
100000/100000 [==============================] - 1094s - loss: 0.3548     
Epoch 3/5
100000/100000 [==============================] - 1053s - loss: 0.3630     
Epoch 4/5
100000/100000 [==============================] - 1020s - loss: 0.3737     
Epoch 5/5
100000/100000 [==============================] - 1017s - loss: 0.3823 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now extract the embeddings matrix from the model by using the &lt;code&gt;get_weights()&lt;/code&gt; function. We also added &lt;code&gt;row.names&lt;/code&gt; to our embedding matrix so we can easily find where each word is.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(dplyr)

embedding_matrix &amp;lt;- get_weights(model)[[1]]

words &amp;lt;- data_frame(
  word = names(tokenizer$word_index), 
  id = as.integer(unlist(tokenizer$word_index))
)

words &amp;lt;- words %&amp;gt;%
  filter(id &amp;lt;= tokenizer$num_words) %&amp;gt;%
  arrange(id)

row.names(embedding_matrix) &amp;lt;- c(&amp;quot;UNK&amp;quot;, words$word)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="understanding-the-embeddings"&gt;Understanding the Embeddings&lt;/h2&gt;
&lt;p&gt;We can now find words that are close to each other in the embedding. We will use the cosine similarity, since this is what we trained the model to minimize.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(text2vec)

find_similar_words &amp;lt;- function(word, embedding_matrix, n = 5) {
  similarities &amp;lt;- embedding_matrix[word, , drop = FALSE] %&amp;gt;%
    sim2(embedding_matrix, y = ., method = &amp;quot;cosine&amp;quot;)
  
  similarities[,1] %&amp;gt;% sort(decreasing = TRUE) %&amp;gt;% head(n)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;find_similar_words(&amp;quot;2&amp;quot;, embedding_matrix)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        2         4         3       two         6 
1.0000000 0.9830254 0.9777042 0.9765668 0.9722549 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;find_similar_words(&amp;quot;little&amp;quot;, embedding_matrix)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   little       bit       few     small     treat 
1.0000000 0.9501037 0.9478287 0.9309829 0.9286966 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;find_similar_words(&amp;quot;delicious&amp;quot;, embedding_matrix)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;delicious     tasty wonderful   amazing     yummy 
1.0000000 0.9632145 0.9619508 0.9617954 0.9529505 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;find_similar_words(&amp;quot;cats&amp;quot;, embedding_matrix)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;     cats      dogs      kids       cat       dog 
1.0000000 0.9844937 0.9743756 0.9676026 0.9624494 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;strong&gt;t-SNE&lt;/strong&gt; algorithm can be used to visualize the embeddings. Because of time constraints we will only use it with the first 500 words. To understand more about the &lt;em&gt;t-SNE&lt;/em&gt; method see the article &lt;a href="https://distill.pub/2016/misread-tsne/"&gt;How to Use t-SNE Effectively&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This plot may look like a mess, but if you zoom into the small groups you end up seeing some nice patterns. Try, for example, to find a group of web related words like &lt;code&gt;http&lt;/code&gt;, &lt;code&gt;href&lt;/code&gt;, etc. Another group that may be easy to pick out is the pronouns group: &lt;code&gt;she&lt;/code&gt;, &lt;code&gt;he&lt;/code&gt;, &lt;code&gt;her&lt;/code&gt;, etc.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(Rtsne)
library(ggplot2)
library(plotly)

tsne &amp;lt;- Rtsne(embedding_matrix[2:500,], perplexity = 50, pca = FALSE)

tsne_plot &amp;lt;- tsne$Y %&amp;gt;%
  as.data.frame() %&amp;gt;%
  mutate(word = row.names(embedding_matrix)[2:500]) %&amp;gt;%
  ggplot(aes(x = V1, y = V2, label = word)) + 
  geom_text(size = 3)
tsne_plot&lt;/code&gt;&lt;/pre&gt;
&lt;iframe src="https://rstudio-pubs-static.s3.amazonaws.com/343548_04348b5de4124a3eb05f62506c6c5827.html" width="100%" height="750" style="border: none;"&gt;
&lt;/iframe&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">be6c05ee4a13bce72d1a88b3288e404a</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Natural Language Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2017-12-22-word-embeddings-with-keras</guid>
      <pubDate>Fri, 22 Dec 2017 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2017-12-22-word-embeddings-with-keras/word-embeddings-with-keras.png" medium="image" type="image/png" width="700" height="450"/>
    </item>
    <item>
      <title>Time Series Forecasting with Recurrent Neural Networks</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">François Chollet</dc:creator>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">J.J. Allaire</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks</link>
      <description>


&lt;h2 id="overview"&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this post, we’ll review three advanced techniques for improving the performance and generalization power of recurrent neural networks. By the end of the section, you’ll know most of what there is to know about using recurrent networks with Keras. We’ll demonstrate all three concepts on a temperature-forecasting problem, where you have access to a time series of data points coming from sensors installed on the roof of a building, such as temperature, air pressure, and humidity, which you use to predict what the temperature will be 24 hours after the last data point. This is a fairly challenging problem that exemplifies many common difficulties encountered when working with time series.&lt;/p&gt;
&lt;p&gt;We’ll cover the following techniques:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Recurrent dropout&lt;/em&gt; — This is a specific, built-in way to use dropout to fight overfitting in recurrent layers.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Stacking recurrent layers&lt;/em&gt; — This increases the representational power of the network (at the cost of higher computational loads).&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Bidirectional recurrent layers&lt;/em&gt; — These present the same information to a recurrent network in different ways, increasing accuracy and mitigating forgetting issues.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="a-temperature-forecasting-problem"&gt;A temperature-forecasting problem&lt;/h2&gt;
&lt;p&gt;Until now, the only sequence data we’ve covered has been text data, such as the IMDB dataset and the Reuters dataset. But sequence data is found in many more problems than just language processing. In all the examples in this section, you’ll play with a &lt;a href="https://www.bgc-jena.mpg.de/wetter/"&gt;weather timeseries dataset&lt;/a&gt; recorded at the Weather Station at the Max Planck Institute for Biogeochemistry in Jena, Germany.&lt;/p&gt;
&lt;p&gt;In this dataset, 14 different quantities (such air temperature, atmospheric pressure, humidity, wind direction, and so on) were recorded every 10 minutes, over several years. The original data goes back to 2003, but this example is limited to data from 2009–2016. This dataset is perfect for learning to work with numerical time series. You’ll use it to build a model that takes as input some data from the recent past (a few days’ worth of data points) and predicts the air temperature 24 hours in the future.&lt;/p&gt;
&lt;p&gt;Download and uncompress the data as follows:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;dir.create(&amp;quot;~/Downloads/jena_climate&amp;quot;, recursive = TRUE)
download.file(
  &amp;quot;https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip&amp;quot;,
  &amp;quot;~/Downloads/jena_climate/jena_climate_2009_2016.csv.zip&amp;quot;
)
unzip(
  &amp;quot;~/Downloads/jena_climate/jena_climate_2009_2016.csv.zip&amp;quot;,
  exdir = &amp;quot;~/Downloads/jena_climate&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s look at the data.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tibble)
library(readr)

data_dir &amp;lt;- &amp;quot;~/Downloads/jena_climate&amp;quot;
fname &amp;lt;- file.path(data_dir, &amp;quot;jena_climate_2009_2016.csv&amp;quot;)
data &amp;lt;- read_csv(fname)

glimpse(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Observations: 420,551
Variables: 15
$ `Date Time`       &amp;lt;chr&amp;gt; &amp;quot;01.01.2009 00:10:00&amp;quot;, &amp;quot;01.01.2009 00:20:00&amp;quot;, &amp;quot;...
$ `p (mbar)`        &amp;lt;dbl&amp;gt; 996.52, 996.57, 996.53, 996.51, 996.51, 996.50,...
$ `T (degC)`        &amp;lt;dbl&amp;gt; -8.02, -8.41, -8.51, -8.31, -8.27, -8.05, -7.62...
$ `Tpot (K)`        &amp;lt;dbl&amp;gt; 265.40, 265.01, 264.91, 265.12, 265.15, 265.38,...
$ `Tdew (degC)`     &amp;lt;dbl&amp;gt; -8.90, -9.28, -9.31, -9.07, -9.04, -8.78, -8.30...
$ `rh (%)`          &amp;lt;dbl&amp;gt; 93.3, 93.4, 93.9, 94.2, 94.1, 94.4, 94.8, 94.4,...
$ `VPmax (mbar)`    &amp;lt;dbl&amp;gt; 3.33, 3.23, 3.21, 3.26, 3.27, 3.33, 3.44, 3.44,...
$ `VPact (mbar)`    &amp;lt;dbl&amp;gt; 3.11, 3.02, 3.01, 3.07, 3.08, 3.14, 3.26, 3.25,...
$ `VPdef (mbar)`    &amp;lt;dbl&amp;gt; 0.22, 0.21, 0.20, 0.19, 0.19, 0.19, 0.18, 0.19,...
$ `sh (g/kg)`       &amp;lt;dbl&amp;gt; 1.94, 1.89, 1.88, 1.92, 1.92, 1.96, 2.04, 2.03,...
$ `H2OC (mmol/mol)` &amp;lt;dbl&amp;gt; 3.12, 3.03, 3.02, 3.08, 3.09, 3.15, 3.27, 3.26,...
$ `rho (g/m**3)`    &amp;lt;dbl&amp;gt; 1307.75, 1309.80, 1310.24, 1309.19, 1309.00, 13...
$ `wv (m/s)`        &amp;lt;dbl&amp;gt; 1.03, 0.72, 0.19, 0.34, 0.32, 0.21, 0.18, 0.19,...
$ `max. wv (m/s)`   &amp;lt;dbl&amp;gt; 1.75, 1.50, 0.63, 0.50, 0.63, 0.63, 0.63, 0.50,...
$ `wd (deg)`        &amp;lt;dbl&amp;gt; 152.3, 136.1, 171.6, 198.0, 214.3, 192.7, 166.5...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the plot of temperature (in degrees Celsius) over time. On this plot, you can clearly see the yearly periodicity of temperature.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(ggplot2)
ggplot(data, aes(x = 1:nrow(data), y = `T (degC)`)) + geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks/images/jena_temp-r.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;p&gt;Here is a more narrow plot of the first 10 days of temperature data (see figure 6.15). Because the data is recorded every 10 minutes, you get 144 data points per day.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggplot(data[1:1440,], aes(x = 1:1440, y = `T (degC)`)) + geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks/images/jena_temp_first_10_days-r.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;p&gt;On this plot, you can see daily periodicity, especially evident for the last 4 days. Also note that this 10-day period must be coming from a fairly cold winter month.&lt;/p&gt;
&lt;p&gt;If you were trying to predict average temperature for the next month given a few months of past data, the problem would be easy, due to the reliable year-scale periodicity of the data. But looking at the data over a scale of days, the temperature looks a lot more chaotic. Is this time series predictable at a daily scale? Let’s find out.&lt;/p&gt;
&lt;h2 id="preparing-the-data"&gt;Preparing the data&lt;/h2&gt;
&lt;p&gt;The exact formulation of the problem will be as follows: given data going as far back as &lt;code&gt;lookback&lt;/code&gt; timesteps (a timestep is 10 minutes) and sampled every &lt;code&gt;steps&lt;/code&gt; timesteps, can you predict the temperature in &lt;code&gt;delay&lt;/code&gt; timesteps? You’ll use the following parameter values:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;lookback = 1440&lt;/code&gt; — Observations will go back 10 days.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;steps = 6&lt;/code&gt; — Observations will be sampled at one data point per hour.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;delay = 144&lt;/code&gt; — Targets will be 24 hours in the future.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To get started, you need to do two things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Preprocess the data to a format a neural network can ingest. This is easy: the data is already numerical, so you don’t need to do any vectorization. But each time series in the data is on a different scale (for example, temperature is typically between -20 and +30, but atmospheric pressure, measured in mbar, is around 1,000). You’ll normalize each time series independently so that they all take small values on a similar scale.&lt;/li&gt;
&lt;li&gt;Write a generator function that takes the current array of float data and yields batches of data from the recent past, along with a target temperature in the future. Because the samples in the dataset are highly redundant (sample &lt;em&gt;N&lt;/em&gt; and sample &lt;em&gt;N&lt;/em&gt; + 1 will have most of their timesteps in common), it would be wasteful to explicitly allocate every sample. Instead, you’ll generate the samples on the fly using the original data.&lt;/li&gt;
&lt;/ul&gt;
&lt;style type="text/css"&gt;
.note {
  border: solid 1px rgba(0, 0, 0, 0.3); 
  padding: 16px 25px; 
  margin-bottom: 20px;
  background-color: rgb(253,253,253);
}
&lt;/style&gt;
&lt;div id="understanding-generators" class="note"&gt;
&lt;p&gt;&lt;strong&gt;NOTE: Understanding generator functions&lt;/strong&gt;&lt;br /&gt;
&lt;br/&gt; A generator function is a special type of function that you call repeatedly to obtain a sequence of values from. Often generators need to maintain internal state, so they are typically constructed by calling another yet another function which returns the generator function (the environment of the function which returns the generator is then used to track state).&lt;/p&gt;
&lt;p&gt;For example, the &lt;code&gt;sequence_generator()&lt;/code&gt; function below returns a generator function that yields an infinite sequence of numbers:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;sequence_generator &amp;lt;- function(start) {
  value &amp;lt;- start - 1
  function() {
    value &amp;lt;&amp;lt;- value + 1
    value
  }
}

gen &amp;lt;- sequence_generator(10)
gen()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;gen()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 11&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The current state of the generator is the &lt;code&gt;value&lt;/code&gt; variable that is defined outside of the function. Note that superassignment (&lt;code&gt;&amp;lt;&amp;lt;-&lt;/code&gt;) is used to update this state from within the function.&lt;/p&gt;
&lt;p&gt;Generator functions can signal completion by returning the value &lt;code&gt;NULL&lt;/code&gt;. However, generator functions passed to Keras training methods (e.g. &lt;code&gt;fit_generator()&lt;/code&gt;) should always return values infinitely (the number of calls to the generator function is controlled by the &lt;code&gt;epochs&lt;/code&gt; and &lt;code&gt;steps_per_epoch&lt;/code&gt; parameters).&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;First, you’ll convert the R data frame which we read earlier into a matrix of floating point values (we’ll discard the first column which included a text timestamp):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;data &amp;lt;- data.matrix(data[,-1])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You’ll then preprocess the data by subtracting the mean of each time series and dividing by the standard deviation. You’re going to use the first 200,000 timesteps as training data, so compute the mean and standard deviation for normalization only on this fraction of the data.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_data &amp;lt;- data[1:200000,]
mean &amp;lt;- apply(train_data, 2, mean)
std &amp;lt;- apply(train_data, 2, sd)
data &amp;lt;- scale(data, center = mean, scale = std)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code for the data generator you’ll use is below. It yields a list &lt;code&gt;(samples, targets)&lt;/code&gt;, where &lt;code&gt;samples&lt;/code&gt; is one batch of input data and &lt;code&gt;targets&lt;/code&gt; is the corresponding array of target temperatures. It takes the following arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;data&lt;/code&gt; — The original array of floating-point data, which you normalized in listing 6.32.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lookback&lt;/code&gt; — How many timesteps back the input data should go.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;delay&lt;/code&gt; — How many timesteps in the future the target should be.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;min_index&lt;/code&gt; and &lt;code&gt;max_index&lt;/code&gt; — Indices in the &lt;code&gt;data&lt;/code&gt; array that delimit which timesteps to draw from. This is useful for keeping a segment of the data for validation and another for testing.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;shuffle&lt;/code&gt; — Whether to shuffle the samples or draw them in chronological order.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;batch_size&lt;/code&gt; — The number of samples per batch.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;step&lt;/code&gt; — The period, in timesteps, at which you sample data. You’ll set it 6 in order to draw one data point every hour.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;generator &amp;lt;- function(data, lookback, delay, min_index, max_index,
                      shuffle = FALSE, batch_size = 128, step = 6) {
  if (is.null(max_index))
    max_index &amp;lt;- nrow(data) - delay - 1
  i &amp;lt;- min_index + lookback
  function() {
    if (shuffle) {
      rows &amp;lt;- sample(c((min_index+lookback):max_index), size = batch_size)
    } else {
      if (i + batch_size &amp;gt;= max_index)
        i &amp;lt;&amp;lt;- min_index + lookback
      rows &amp;lt;- c(i:min(i+batch_size-1, max_index))
      i &amp;lt;&amp;lt;- i + length(rows)
    }

    samples &amp;lt;- array(0, dim = c(length(rows),
                                lookback / step,
                                dim(data)[[-1]]))
    targets &amp;lt;- array(0, dim = c(length(rows)))
                      
    for (j in 1:length(rows)) {
      indices &amp;lt;- seq(rows[[j]] - lookback, rows[[j]]-1,
                     length.out = dim(samples)[[2]])
      samples[j,,] &amp;lt;- data[indices,]
      targets[[j]] &amp;lt;- data[rows[[j]] + delay,2]
    }           
    list(samples, targets)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;i&lt;/code&gt; variable contains the state that tracks next window of data to return, so it is updated using superassignment (e.g. &lt;code&gt;i &amp;lt;&amp;lt;- i + length(rows)&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Now, let’s use the abstract &lt;code&gt;generator&lt;/code&gt; function to instantiate three generators: one for training, one for validation, and one for testing. Each will look at different temporal segments of the original data: the training generator looks at the first 200,000 timesteps, the validation generator looks at the following 100,000, and the test generator looks at the remainder.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;lookback &amp;lt;- 1440
step &amp;lt;- 6
delay &amp;lt;- 144
batch_size &amp;lt;- 128

train_gen &amp;lt;- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = 200000,
  shuffle = TRUE,
  step = step, 
  batch_size = batch_size
)

val_gen = generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 200001,
  max_index = 300000,
  step = step,
  batch_size = batch_size
)

test_gen &amp;lt;- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 300001,
  max_index = NULL,
  step = step,
  batch_size = batch_size
)

# How many steps to draw from val_gen in order to see the entire validation set
val_steps &amp;lt;- (300000 - 200001 - lookback) / batch_size

# How many steps to draw from test_gen in order to see the entire test set
test_steps &amp;lt;- (nrow(data) - 300001 - lookback) / batch_size&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="a-common-sense-non-machine-learning-baseline"&gt;A common-sense, non-machine-learning baseline&lt;/h2&gt;
&lt;p&gt;Before you start using black-box deep-learning models to solve the temperature-prediction problem, let’s try a simple, common-sense approach. It will serve as a sanity check, and it will establish a baseline that you’ll have to beat in order to demonstrate the usefulness of more-advanced machine-learning models. Such common-sense baselines can be useful when you’re approaching a new problem for which there is no known solution (yet). A classic example is that of unbalanced classification tasks, where some classes are much more common than others. If your dataset contains 90% instances of class A and 10% instances of class B, then a common-sense approach to the classification task is to always predict “A” when presented with a new sample. Such a classifier is 90% accurate overall, and any learning-based approach should therefore beat this 90% score in order to demonstrate usefulness. Sometimes, such elementary baselines can prove surprisingly hard to beat.&lt;/p&gt;
&lt;p&gt;In this case, the temperature time series can safely be assumed to be continuous (the temperatures tomorrow are likely to be close to the temperatures today) as well as periodical with a daily period. Thus a common-sense approach is to always predict that the temperature 24 hours from now will be equal to the temperature right now. Let’s evaluate this approach, using the mean absolute error (MAE) metric:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;mean(abs(preds - targets))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the evaluation loop.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
evaluate_naive_method &amp;lt;- function() {
  batch_maes &amp;lt;- c()
  for (step in 1:val_steps) {
    c(samples, targets) %&amp;lt;-% val_gen()
    preds &amp;lt;- samples[,dim(samples)[[2]],2]
    mae &amp;lt;- mean(abs(preds - targets))
    batch_maes &amp;lt;- c(batch_maes, mae)
  }
  print(mean(batch_maes))
}

evaluate_naive_method()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This yields an MAE of 0.29. Because the temperature data has been normalized to be centered on 0 and have a standard deviation of 1, this number isn’t immediately interpretable. It translates to an average absolute error of 0.29 x &lt;code&gt;temperature_std&lt;/code&gt; degrees Celsius: 2.57˚C.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;celsius_mae &amp;lt;- 0.29 * std[[2]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s a fairly large average absolute error. Now the game is to use your knowledge of deep learning to do better.&lt;/p&gt;
&lt;h2 id="a-basic-machine-learning-approach"&gt;A basic machine-learning approach&lt;/h2&gt;
&lt;p&gt;In the same way that it’s useful to establish a common-sense baseline before trying machine-learning approaches, it’s useful to try simple, cheap machine-learning models (such as small, densely connected networks) before looking into complicated and computationally expensive models such as RNNs. This is the best way to make sure any further complexity you throw at the problem is legitimate and delivers real benefits.&lt;/p&gt;
&lt;p&gt;The following listing shows a fully connected model that starts by flattening the data and then runs it through two dense layers. Note the lack of activation function on the last dense layer, which is typical for a regression problem. You use MAE as the loss. Because you evaluate on the exact same data and with the exact same metric you did with the common-sense approach, the results will be directly comparable.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)

model &amp;lt;- keras_model_sequential() %&amp;gt;% 
  layer_flatten(input_shape = c(lookback / step, dim(data)[-1])) %&amp;gt;% 
  layer_dense(units = 32, activation = &amp;quot;relu&amp;quot;) %&amp;gt;% 
  layer_dense(units = 1)

model %&amp;gt;% compile(
  optimizer = optimizer_rmsprop(),
  loss = &amp;quot;mae&amp;quot;
)

history &amp;lt;- model %&amp;gt;% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 20,
  validation_data = val_gen,
  validation_steps = val_steps
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s display the loss curves for validation and training.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks/images/6-3_loss_plot_first_ml_baseline-r.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;p&gt;Some of the validation losses are close to the no-learning baseline, but not reliably. This goes to show the merit of having this baseline in the first place: it turns out to be not easy to outperform. Your common sense contains a lot of valuable information that a machine-learning model doesn’t have access to.&lt;/p&gt;
&lt;p&gt;You may wonder, if a simple, well-performing model exists to go from the data to the targets (the common-sense baseline), why doesn’t the model you’re training find it and improve on it? Because this simple solution isn’t what your training setup is looking for. The space of models in which you’re searching for a solution – that is, your hypothesis space – is the space of all possible two-layer networks with the configuration you defined. These networks are already fairly complicated. When you’re looking for a solution with a space of complicated models, the simple, well-performing baseline may be unlearnable, even if it’s technically part of the hypothesis space. That is a pretty significant limitation of machine learning in general: unless the learning algorithm is hardcoded to look for a specific kind of simple model, parameter learning can sometimes fail to find a simple solution to a simple problem.&lt;/p&gt;
&lt;h2 id="a-first-recurrent-baseline"&gt;A first recurrent baseline&lt;/h2&gt;
&lt;p&gt;The first fully connected approach didn’t do well, but that doesn’t mean machine learning isn’t applicable to this problem. The previous approach first flattened the time series, which removed the notion of time from the input data. Let’s instead look at the data as what it is: a sequence, where causality and order matter. You’ll try a recurrent-sequence processing model – it should be the perfect fit for such sequence data, precisely because it exploits the temporal ordering of data points, unlike the first approach.&lt;/p&gt;
&lt;p&gt;Instead of the LSTM layer introduced in the previous section, you’ll use the &lt;a href="https://arxiv.org/abs/1412.3555"&gt;GRU layer&lt;/a&gt;, developed by Chung et al. in 2014. Gated recurrent unit (GRU) layers work using the same principle as LSTM, but they’re somewhat streamlined and thus cheaper to run (although they may not have as much representational power as LSTM). This trade-off between computational expensiveness and representational power is seen everywhere in machine learning.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;% 
  layer_gru(units = 32, input_shape = list(NULL, dim(data)[[-1]])) %&amp;gt;% 
  layer_dense(units = 1)

model %&amp;gt;% compile(
  optimizer = optimizer_rmsprop(),
  loss = &amp;quot;mae&amp;quot;
)

history &amp;lt;- model %&amp;gt;% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 20,
  validation_data = val_gen,
  validation_steps = val_steps
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results are plotted below. Much better! You can significantly beat the common-sense baseline, demonstrating the value of machine learning as well as the superiority of recurrent networks compared to sequence-flattening dense networks on this type of task.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks/images/6-3_loss_plot_gru_baseline-r.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;p&gt;The new validation MAE of ~0.265 (before you start significantly overfitting) translates to a mean absolute error of 2.35˚C after denormalization. That’s a solid gain on the initial error of 2.57˚C, but you probably still have a bit of a margin for improvement.&lt;/p&gt;
&lt;h2 id="using-recurrent-dropout-to-fight-overfitting"&gt;Using recurrent dropout to fight overfitting&lt;/h2&gt;
&lt;p&gt;It’s evident from the training and validation curves that the model is overfitting: the training and validation losses start to diverge considerably after a few epochs. You’re already familiar with a classic technique for fighting this phenomenon: dropout, which randomly zeros out input units of a layer in order to break happenstance correlations in the training data that the layer is exposed to. But how to correctly apply dropout in recurrent networks isn’t a trivial question. It has long been known that applying dropout before a recurrent layer hinders learning rather than helping with regularization. In 2015, Yarin Gal, as part of his &lt;a href="http://mlg.eng.cam.ac.uk/yarin/blog_2248.html"&gt;PhD thesis&lt;/a&gt; on Bayesian deep learning, determined the proper way to use dropout with a recurrent network: the same dropout mask (the same pattern of dropped units) should be applied at every timestep, instead of a dropout mask that varies randomly from timestep to timestep. What’s more, in order to regularize the representations formed by the recurrent gates of layers such as &lt;code&gt;layer_gru&lt;/code&gt; and &lt;code&gt;layer_lstm&lt;/code&gt;, a temporally constant dropout mask should be applied to the inner recurrent activations of the layer (a &lt;em&gt;recurrent&lt;/em&gt; dropout mask). Using the same dropout mask at every timestep allows the network to properly propagate its learning error through time; a temporally random dropout mask would disrupt this error signal and be harmful to the learning process.&lt;/p&gt;
&lt;p&gt;Yarin Gal did his research using Keras and helped build this mechanism directly into Keras recurrent layers. Every recurrent layer in Keras has two dropout-related arguments: &lt;code&gt;dropout&lt;/code&gt;, a float specifying the dropout rate for input units of the layer, and &lt;code&gt;recurrent_dropout&lt;/code&gt;, specifying the dropout rate of the recurrent units. Let’s add dropout and recurrent dropout to the &lt;code&gt;layer_gru&lt;/code&gt; and see how doing so impacts overfitting. Because networks being regularized with dropout always take longer to fully converge, you’ll train the network for twice as many epochs.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;% 
  layer_gru(units = 32, dropout = 0.2, recurrent_dropout = 0.2,
            input_shape = list(NULL, dim(data)[[-1]])) %&amp;gt;% 
  layer_dense(units = 1)

model %&amp;gt;% compile(
  optimizer = optimizer_rmsprop(),
  loss = &amp;quot;mae&amp;quot;
)

history &amp;lt;- model %&amp;gt;% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 40,
  validation_data = val_gen,
  validation_steps = val_steps
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plot below shows the results. Success! You’re no longer overfitting during the first 20 epochs. But although you have more stable evaluation scores, your best scores aren’t much lower than they were previously.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks/images/6-3_loss_plot_gru_dropout-r.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;h2 id="stacking-recurrent-layers"&gt;Stacking recurrent layers&lt;/h2&gt;
&lt;p&gt;Because you’re no longer overfitting but seem to have hit a performance bottleneck, you should consider increasing the capacity of the network. Recall the description of the universal machine-learning workflow: it’s generally a good idea to increase the capacity of your network until overfitting becomes the primary obstacle (assuming you’re already taking basic steps to mitigate overfitting, such as using dropout). As long as you aren’t overfitting too badly, you’re likely under capacity.&lt;/p&gt;
&lt;p&gt;Increasing network capacity is typically done by increasing the number of units in the layers or adding more layers. Recurrent layer stacking is a classic way to build more-powerful recurrent networks: for instance, what currently powers the Google Translate algorithm is a stack of seven large LSTM layers – that’s huge.&lt;/p&gt;
&lt;p&gt;To stack recurrent layers on top of each other in Keras, all intermediate layers should return their full sequence of outputs (a 3D tensor) rather than their output at the last timestep. This is done by specifying &lt;code&gt;return_sequences = TRUE&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;% 
  layer_gru(units = 32, 
            dropout = 0.1, 
            recurrent_dropout = 0.5,
            return_sequences = TRUE,
            input_shape = list(NULL, dim(data)[[-1]])) %&amp;gt;% 
  layer_gru(units = 64, activation = &amp;quot;relu&amp;quot;,
            dropout = 0.1,
            recurrent_dropout = 0.5) %&amp;gt;% 
  layer_dense(units = 1)

model %&amp;gt;% compile(
  optimizer = optimizer_rmsprop(),
  loss = &amp;quot;mae&amp;quot;
)

history &amp;lt;- model %&amp;gt;% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 40,
  validation_data = val_gen,
  validation_steps = val_steps
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The figure below shows the results. You can see that the added layer does improve the results a bit, though not significantly. You can draw two conclusions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Because you’re still not overfitting too badly, you could safely increase the size of your layers in a quest for validation-loss improvement. This has a non-negligible computational cost, though.&lt;/li&gt;
&lt;li&gt;Adding a layer didn’t help by a significant factor, so you may be seeing diminishing returns from increasing network capacity at this point.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks/images/6-3_loss_plot_stacked_gru-r.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;h2 id="using-bidirectional-rnns"&gt;Using bidirectional RNNs&lt;/h2&gt;
&lt;p&gt;The last technique introduced in this section is called &lt;em&gt;bidirectional RNNs&lt;/em&gt;. A bidirectional RNN is a common RNN variant that can offer greater performance than a regular RNN on certain tasks. It’s frequently used in natural-language processing – you could call it the Swiss Army knife of deep learning for natural-language processing.&lt;/p&gt;
&lt;p&gt;RNNs are notably order dependent, or time dependent: they process the timesteps of their input sequences in order, and shuffling or reversing the timesteps can completely change the representations the RNN extracts from the sequence. This is precisely the reason they perform well on problems where order is meaningful, such as the temperature-forecasting problem. A bidirectional RNN exploits the order sensitivity of RNNs: it consists of using two regular RNNs, such as the &lt;code&gt;layer_gru&lt;/code&gt; and &lt;code&gt;layer_lstm&lt;/code&gt; you’re already familiar with, each of which processes the input sequence in one direction (chronologically and antichronologically), and then merging their representations. By processing a sequence both ways, a bidirectional RNN can catch patterns that may be overlooked by a unidirectional RNN.&lt;/p&gt;
&lt;p&gt;Remarkably, the fact that the RNN layers in this section have processed sequences in chronological order (older timesteps first) may have been an arbitrary decision. At least, it’s a decision we made no attempt to question so far. Could the RNNs have performed well enough if they processed input sequences in antichronological order, for instance (newer timesteps first)? Let’s try this in practice and see what happens. All you need to do is write a variant of the data generator where the input sequences are reverted along the time dimension (replace the last line with &lt;code&gt;list(samples[,ncol(samples):1,], targets)&lt;/code&gt;). Training the same one-GRU-layer network that you used in the first experiment in this section, you get the results shown below.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks/images/6-3_loss_plot_reversed_gru-r.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;p&gt;The reversed-order GRU underperforms even the common-sense baseline, indicating that in this case, chronological processing is important to the success of your approach. This makes perfect sense: the underlying GRU layer will typically be better at remembering the recent past than the distant past, and naturally the more recent weather data points are more predictive than older data points for the problem (that’s what makes the common-sense baseline fairly strong). Thus the chronological version of the layer is bound to outperform the reversed-order version. Importantly, this isn’t true for many other problems, including natural language: intuitively, the importance of a word in understanding a sentence isn’t usually dependent on its position in the sentence. Let’s try the same trick on the LSTM IMDB example from section 6.2.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)

# Number of words to consider as features
max_features &amp;lt;- 10000  

# Cuts off texts after this number of words
maxlen &amp;lt;- 500

imdb &amp;lt;- dataset_imdb(num_words = max_features)
c(c(x_train, y_train), c(x_test, y_test)) %&amp;lt;-% imdb

# Reverses sequences
x_train &amp;lt;- lapply(x_train, rev)
x_test &amp;lt;- lapply(x_test, rev) 

# Pads sequences
x_train &amp;lt;- pad_sequences(x_train, maxlen = maxlen)  &amp;lt;4&amp;gt;
x_test &amp;lt;- pad_sequences(x_test, maxlen = maxlen)

model &amp;lt;- keras_model_sequential() %&amp;gt;% 
  layer_embedding(input_dim = max_features, output_dim = 128) %&amp;gt;% 
  layer_lstm(units = 32) %&amp;gt;% 
  layer_dense(units = 1, activation = &amp;quot;sigmoid&amp;quot;)

model %&amp;gt;% compile(
  optimizer = &amp;quot;rmsprop&amp;quot;,
  loss = &amp;quot;binary_crossentropy&amp;quot;,
  metrics = c(&amp;quot;acc&amp;quot;)
)
  
history &amp;lt;- model %&amp;gt;% fit(
  x_train, y_train,
  epochs = 10,
  batch_size = 128,
  validation_split = 0.2
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You get performance nearly identical to that of the chronological-order LSTM. Remarkably, on such a text dataset, reversed-order processing works just as well as chronological processing, confirming the hypothesis that, although word order &lt;em&gt;does&lt;/em&gt; matter in understanding language, &lt;em&gt;which&lt;/em&gt; order you use isn’t crucial. Importantly, an RNN trained on reversed sequences will learn different representations than one trained on the original sequences, much as you would have different mental models if time flowed backward in the real world – if you lived a life where you died on your first day and were born on your last day. In machine learning, representations that are &lt;em&gt;different&lt;/em&gt; yet &lt;em&gt;useful&lt;/em&gt; are always worth exploiting, and the more they differ, the better: they offer a new angle from which to look at your data, capturing aspects of the data that were missed by other approaches, and thus they can help boost performance on a task. This is the intuition behind &lt;em&gt;ensembling&lt;/em&gt;, a concept we’ll explore in chapter 7.&lt;/p&gt;
&lt;p&gt;A bidirectional RNN exploits this idea to improve on the performance of chronological-order RNNs. It looks at its input sequence both ways, obtaining potentially richer representations and capturing patterns that may have been missed by the chronological-order version alone.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks/images/bidirectional_rnn.png" /&gt;&lt;/p&gt;
&lt;p&gt;To instantiate a bidirectional RNN in Keras, you use the &lt;code&gt;bidirectional()&lt;/code&gt; function, which takes a recurrent layer instance as an argument. The &lt;code&gt;bidirectional()&lt;/code&gt; function creates a second, separate instance of this recurrent layer and uses one instance for processing the input sequences in chronological order and the other instance for processing the input sequences in reversed order. Let’s try it on the IMDB sentiment-analysis task.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;% 
  layer_embedding(input_dim = max_features, output_dim = 32) %&amp;gt;% 
  bidirectional(
    layer_lstm(units = 32)
  ) %&amp;gt;% 
  layer_dense(units = 1, activation = &amp;quot;sigmoid&amp;quot;)

model %&amp;gt;% compile(
  optimizer = &amp;quot;rmsprop&amp;quot;,
  loss = &amp;quot;binary_crossentropy&amp;quot;,
  metrics = c(&amp;quot;acc&amp;quot;)
)

history &amp;lt;- model %&amp;gt;% fit(
  x_train, y_train,
  epochs = 10,
  batch_size = 128,
  validation_split = 0.2
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It performs slightly better than the regular LSTM you tried in the previous section, achieving over 89% validation accuracy. It also seems to overfit more quickly, which is unsurprising because a bidirectional layer has twice as many parameters as a chronological LSTM. With some regularization, the bidirectional approach would likely be a strong performer on this task.&lt;/p&gt;
&lt;p&gt;Now let’s try the same approach on the temperature prediction task.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;% 
  bidirectional(
    layer_gru(units = 32), input_shape = list(NULL, dim(data)[[-1]])
  ) %&amp;gt;% 
  layer_dense(units = 1)

model %&amp;gt;% compile(
  optimizer = optimizer_rmsprop(),
  loss = &amp;quot;mae&amp;quot;
)

history &amp;lt;- model %&amp;gt;% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 40,
  validation_data = val_gen,
  validation_steps = val_steps
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This performs about as well as the regular &lt;code&gt;layer_gru&lt;/code&gt;. It’s easy to understand why: all the predictive capacity must come from the chronological half of the network, because the antichronological half is known to be severely underperforming on this task (again, because the recent past matters much more than the distant past in this case).&lt;/p&gt;
&lt;h2 id="going-even-further"&gt;Going even further&lt;/h2&gt;
&lt;p&gt;There are many other things you could try, in order to improve performance on the temperature-forecasting problem:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Adjust the number of units in each recurrent layer in the stacked setup. The current choices are largely arbitrary and thus probably suboptimal.&lt;/li&gt;
&lt;li&gt;Adjust the learning rate used by the &lt;code&gt;RMSprop&lt;/code&gt; optimizer.&lt;/li&gt;
&lt;li&gt;Try using &lt;code&gt;layer_lstm&lt;/code&gt; instead of &lt;code&gt;layer_gru&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Try using a bigger densely connected regressor on top of the recurrent layers: that is, a bigger dense layer or even a stack of dense layers.&lt;/li&gt;
&lt;li&gt;Don’t forget to eventually run the best-performing models (in terms of validation MAE) on the test set! Otherwise, you’ll develop architectures that are overfitting to the validation set.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As always, deep learning is more an art than a science. We can provide guidelines that suggest what is likely to work or not work on a given problem, but, ultimately, every problem is unique; you’ll have to evaluate different strategies empirically. There is currently no theory that will tell you in advance precisely what you should do to optimally solve a problem. You must iterate.&lt;/p&gt;
&lt;h2 id="wrapping-up"&gt;Wrapping up&lt;/h2&gt;
&lt;p&gt;Here’s what you should take away from this section:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As you first learned in chapter 4, when approaching a new problem, it’s good to first establish common-sense baselines for your metric of choice. If you don’t have a baseline to beat, you can’t tell whether you’re making real progress.&lt;/li&gt;
&lt;li&gt;Try simple models before expensive ones, to justify the additional expense. Sometimes a simple model will turn out to be your best option.&lt;/li&gt;
&lt;li&gt;When you have data where temporal ordering matters, recurrent networks are a great fit and easily outperform models that first flatten the temporal data.&lt;/li&gt;
&lt;li&gt;To use dropout with recurrent networks, you should use a time-constant dropout mask and recurrent dropout mask. These are built into Keras recurrent layers, so all you have to do is use the &lt;code&gt;dropout&lt;/code&gt; and &lt;code&gt;recurrent_dropout&lt;/code&gt; arguments of recurrent layers.&lt;/li&gt;
&lt;li&gt;Stacked RNNs provide more representational power than a single RNN layer. They’re also much more expensive and thus not always worth it. Although they offer clear gains on complex problems (such as machine translation), they may not always be relevant to smaller, simpler problems.&lt;/li&gt;
&lt;li&gt;Bidirectional RNNs, which look at a sequence both ways, are useful on natural-language processing problems. But they aren’t strong performers on sequence data where the recent past is much more informative than the beginning of the sequence.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id="markets" class="note"&gt;
&lt;p&gt;&lt;strong&gt;NOTE: Markets and machine learning&lt;/strong&gt;&lt;br /&gt;
&lt;br/&gt; Some readers are bound to want to take the techniques we’ve introduced here and try them on the problem of forecasting the future price of securities on the stock market (or currency exchange rates, and so on). Markets have &lt;em&gt;very different statistical characteristics&lt;/em&gt; than natural phenomena such as weather patterns. Trying to use machine learning to beat markets, when you only have access to publicly available data, is a difficult endeavor, and you’re likely to waste your time and resources with nothing to show for it.&lt;/p&gt;
&lt;p&gt;Always remember that when it comes to markets, past performance is &lt;em&gt;not&lt;/em&gt; a good predictor of future returns – looking in the rear-view mirror is a bad way to drive. Machine learning, on the other hand, is applicable to datasets where the past &lt;em&gt;is&lt;/em&gt; a good predictor of the future.&lt;/p&gt;
&lt;/div&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">a1a8d47ece5a70e7bb15916f299dc6f4</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks</guid>
      <pubDate>Wed, 20 Dec 2017 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks/images/jena_temp-r.png" medium="image" type="image/png" width="6000" height="4000"/>
    </item>
    <item>
      <title>Image Classification on Small Datasets with Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">François Chollet</dc:creator>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">J.J. Allaire</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2017-12-14-image-classification-on-small-datasets</link>
      <description>


&lt;h2 id="training-a-convnet-with-a-small-dataset"&gt;Training a convnet with a small dataset&lt;/h2&gt;
&lt;p&gt;Having to train an image-classification model using very little data is a common situation, which you’ll likely encounter in practice if you ever do computer vision in a professional context. A “few” samples can mean anywhere from a few hundred to a few tens of thousands of images. As a practical example, we’ll focus on classifying images as dogs or cats, in a dataset containing 4,000 pictures of cats and dogs (2,000 cats, 2,000 dogs). We’ll use 2,000 pictures for training – 1,000 for validation, and 1,000 for testing.&lt;/p&gt;
&lt;p&gt;In Chapter 5 of the &lt;a href="https://www.manning.com/books/deep-learning-with-r"&gt;Deep Learning with R&lt;/a&gt; book we review three techniques for tackling this problem. The first of these is training a small model from scratch on what little data you have (which achieves an accuracy of 82%). Subsequently we use &lt;em&gt;feature extraction with a pretrained network&lt;/em&gt; (resulting in an accuracy of 90%) and &lt;em&gt;fine-tuning a pretrained network&lt;/em&gt; (with a final accuracy of 97%). In this post we’ll cover only the second and third techniques.&lt;/p&gt;
&lt;h3 id="the-relevance-of-deep-learning-for-small-data-problems"&gt;The relevance of deep learning for small-data problems&lt;/h3&gt;
&lt;p&gt;You’ll sometimes hear that deep learning only works when lots of data is available. This is valid in part: one fundamental characteristic of deep learning is that it can find interesting features in the training data on its own, without any need for manual feature engineering, and this can only be achieved when lots of training examples are available. This is especially true for problems where the input samples are very high-dimensional, like images.&lt;/p&gt;
&lt;p&gt;But what constitutes lots of samples is relative – relative to the size and depth of the network you’re trying to train, for starters. It isn’t possible to train a convnet to solve a complex problem with just a few tens of samples, but a few hundred can potentially suffice if the model is small and well regularized and the task is simple. Because convnets learn local, translation-invariant features, they’re highly data efficient on perceptual problems. Training a convnet from scratch on a very small image dataset will still yield reasonable results despite a relative lack of data, without the need for any custom feature engineering. You’ll see this in action in this section.&lt;/p&gt;
&lt;p&gt;What’s more, deep-learning models are by nature highly repurposable: you can take, say, an image-classification or speech-to-text model trained on a large-scale dataset and reuse it on a significantly different problem with only minor changes. Specifically, in the case of computer vision, many pretrained models (usually trained on the ImageNet dataset) are now publicly available for download and can be used to bootstrap powerful vision models out of very little data. That’s what you’ll do in the next section. Let’s start by getting your hands on the data.&lt;/p&gt;
&lt;h2 id="downloading-the-data"&gt;Downloading the data&lt;/h2&gt;
&lt;p&gt;The Dogs vs. Cats dataset that you’ll use isn’t packaged with Keras. It was made available by Kaggle as part of a computer-vision competition in late 2013, back when convnets weren’t mainstream. You can download the original dataset from &lt;a href="https://www.kaggle.com/c/dogs-vs-cats/data" class="uri"&gt;https://www.kaggle.com/c/dogs-vs-cats/data&lt;/a&gt; (you’ll need to create a Kaggle account if you don’t already have one – don’t worry, the process is painless).&lt;/p&gt;
&lt;p&gt;The pictures are medium-resolution color JPEGs. Here are some examples:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2017-12-14-image-classification-on-small-datasets/images/cats_vs_dogs_samples.jpg" style="width:80.0%" /&gt;&lt;/p&gt;
&lt;p&gt;Unsurprisingly, the dogs-versus-cats Kaggle competition in 2013 was won by entrants who used convnets. The best entries achieved up to 95% accuracy. Below you’ll end up with a 97% accuracy, even though you’ll train your models on less than 10% of the data that was available to the competitors.&lt;/p&gt;
&lt;p&gt;This dataset contains 25,000 images of dogs and cats (12,500 from each class) and is 543 MB (compressed). After downloading and uncompressing it, you’ll create a new dataset containing three subsets: a training set with 1,000 samples of each class, a validation set with 500 samples of each class, and a test set with 500 samples of each class.&lt;/p&gt;
&lt;p&gt;Following is the code to do this:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;original_dataset_dir &amp;lt;- &amp;quot;~/Downloads/kaggle_original_data&amp;quot;

base_dir &amp;lt;- &amp;quot;~/Downloads/cats_and_dogs_small&amp;quot;
dir.create(base_dir)

train_dir &amp;lt;- file.path(base_dir, &amp;quot;train&amp;quot;)
dir.create(train_dir)
validation_dir &amp;lt;- file.path(base_dir, &amp;quot;validation&amp;quot;)
dir.create(validation_dir)
test_dir &amp;lt;- file.path(base_dir, &amp;quot;test&amp;quot;)
dir.create(test_dir)

train_cats_dir &amp;lt;- file.path(train_dir, &amp;quot;cats&amp;quot;)
dir.create(train_cats_dir)

train_dogs_dir &amp;lt;- file.path(train_dir, &amp;quot;dogs&amp;quot;)
dir.create(train_dogs_dir)

validation_cats_dir &amp;lt;- file.path(validation_dir, &amp;quot;cats&amp;quot;)
dir.create(validation_cats_dir)

validation_dogs_dir &amp;lt;- file.path(validation_dir, &amp;quot;dogs&amp;quot;)
dir.create(validation_dogs_dir)

test_cats_dir &amp;lt;- file.path(test_dir, &amp;quot;cats&amp;quot;)
dir.create(test_cats_dir)

test_dogs_dir &amp;lt;- file.path(test_dir, &amp;quot;dogs&amp;quot;)
dir.create(test_dogs_dir)

fnames &amp;lt;- paste0(&amp;quot;cat.&amp;quot;, 1:1000, &amp;quot;.jpg&amp;quot;)
file.copy(file.path(original_dataset_dir, fnames), 
          file.path(train_cats_dir)) 

fnames &amp;lt;- paste0(&amp;quot;cat.&amp;quot;, 1001:1500, &amp;quot;.jpg&amp;quot;)
file.copy(file.path(original_dataset_dir, fnames), 
          file.path(validation_cats_dir))

fnames &amp;lt;- paste0(&amp;quot;cat.&amp;quot;, 1501:2000, &amp;quot;.jpg&amp;quot;)
file.copy(file.path(original_dataset_dir, fnames),
          file.path(test_cats_dir))

fnames &amp;lt;- paste0(&amp;quot;dog.&amp;quot;, 1:1000, &amp;quot;.jpg&amp;quot;)
file.copy(file.path(original_dataset_dir, fnames),
          file.path(train_dogs_dir))

fnames &amp;lt;- paste0(&amp;quot;dog.&amp;quot;, 1001:1500, &amp;quot;.jpg&amp;quot;)
file.copy(file.path(original_dataset_dir, fnames),
          file.path(validation_dogs_dir)) 

fnames &amp;lt;- paste0(&amp;quot;dog.&amp;quot;, 1501:2000, &amp;quot;.jpg&amp;quot;)
file.copy(file.path(original_dataset_dir, fnames),
          file.path(test_dogs_dir))&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="using-a-pretrained-convnet"&gt;Using a pretrained convnet&lt;/h2&gt;
&lt;p&gt;A common and highly effective approach to deep learning on small image datasets is to use a pretrained network. A &lt;em&gt;pretrained network&lt;/em&gt; is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task. If this original dataset is large enough and general enough, then the spatial hierarchy of features learned by the pretrained network can effectively act as a generic model of the visual world, and hence its features can prove useful for many different computer-vision problems, even though these new problems may involve completely different classes than those of the original task. For instance, you might train a network on ImageNet (where classes are mostly animals and everyday objects) and then repurpose this trained network for something as remote as identifying furniture items in images. Such portability of learned features across different problems is a key advantage of deep learning compared to many older, shallow-learning approaches, and it makes deep learning very effective for small-data problems.&lt;/p&gt;
&lt;p&gt;In this case, let’s consider a large convnet trained on the ImageNet dataset (1.4 million labeled images and 1,000 different classes). ImageNet contains many animal classes, including different species of cats and dogs, and you can thus expect to perform well on the dogs-versus-cats classification problem.&lt;/p&gt;
&lt;p&gt;You’ll use the &lt;a href="https://arxiv.org/abs/1409.1556"&gt;VGG16 architecture&lt;/a&gt;, developed by Karen Simonyan and Andrew Zisserman in 2014; it’s a simple and widely used convnet architecture for ImageNet. Although it’s an older model, far from the current state of the art and somewhat heavier than many other recent models, I chose it because its architecture is similar to what you’re already familiar with and is easy to understand without introducing any new concepts. This may be your first encounter with one of these cutesy model names – VGG, ResNet, Inception, Inception-ResNet, Xception, and so on; you’ll get used to them, because they will come up frequently if you keep doing deep learning for computer vision.&lt;/p&gt;
&lt;p&gt;There are two ways to use a pretrained network: &lt;em&gt;feature extraction&lt;/em&gt; and &lt;em&gt;fine-tuning&lt;/em&gt;. We’ll cover both of them. Let’s start with feature extraction.&lt;/p&gt;
&lt;h3 id="feature-extraction"&gt;Feature extraction&lt;/h3&gt;
&lt;p&gt;Feature extraction consists of using the representations learned by a previous network to extract interesting features from new samples. These features are then run through a new classifier, which is trained from scratch.&lt;/p&gt;
&lt;p&gt;As you saw previously, convnets used for image classification comprise two parts: they start with a series of pooling and convolution layers, and they end with a densely connected classifier. The first part is called the &lt;em&gt;convolutional base&lt;/em&gt; of the model. In the case of convnets, feature extraction consists of taking the convolutional base of a previously trained network, running the new data through it, and training a new classifier on top of the output.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2017-12-14-image-classification-on-small-datasets/images/swapping_fc_classifier.png" style="width:80.0%" /&gt;&lt;/p&gt;
&lt;p&gt;Why only reuse the convolutional base? Could you reuse the densely connected classifier as well? In general, doing so should be avoided. The reason is that the representations learned by the convolutional base are likely to be more generic and therefore more reusable: the feature maps of a convnet are presence maps of generic concepts over a picture, which is likely to be useful regardless of the computer-vision problem at hand. But the representations learned by the classifier will necessarily be specific to the set of classes on which the model was trained – they will only contain information about the presence probability of this or that class in the entire picture. Additionally, representations found in densely connected layers no longer contain any information about &lt;em&gt;where&lt;/em&gt; objects are located in the input image: these layers get rid of the notion of space, whereas the object location is still described by convolutional feature maps. For problems where object location matters, densely connected features are largely useless.&lt;/p&gt;
&lt;p&gt;Note that the level of generality (and therefore reusability) of the representations extracted by specific convolution layers depends on the depth of the layer in the model. Layers that come earlier in the model extract local, highly generic feature maps (such as visual edges, colors, and textures), whereas layers that are higher up extract more-abstract concepts (such as “cat ear” or “dog eye”). So if your new dataset differs a lot from the dataset on which the original model was trained, you may be better off using only the first few layers of the model to do feature extraction, rather than using the entire convolutional base.&lt;/p&gt;
&lt;p&gt;In this case, because the ImageNet class set contains multiple dog and cat classes, it’s likely to be beneficial to reuse the information contained in the densely connected layers of the original model. But we’ll choose not to, in order to cover the more general case where the class set of the new problem doesn’t overlap the class set of the original model.&lt;/p&gt;
&lt;p&gt;Let’s put this in practice by using the convolutional base of the VGG16 network, trained on ImageNet, to extract interesting features from cat and dog images, and then train a dogs-versus-cats classifier on top of these features.&lt;/p&gt;
&lt;p&gt;The VGG16 model, among others, comes prepackaged with Keras. Here’s the list of image-classification models (all pretrained on the ImageNet dataset) that are available as part of Keras:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Xception&lt;/li&gt;
&lt;li&gt;Inception V3&lt;/li&gt;
&lt;li&gt;ResNet50&lt;/li&gt;
&lt;li&gt;VGG16&lt;/li&gt;
&lt;li&gt;VGG19&lt;/li&gt;
&lt;li&gt;MobileNet&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s instantiate the VGG16 model.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)

conv_base &amp;lt;- application_vgg16(
  weights = &amp;quot;imagenet&amp;quot;,
  include_top = FALSE,
  input_shape = c(150, 150, 3)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You pass three arguments to the function:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;weights&lt;/code&gt; specifies the weight checkpoint from which to initialize the model.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;include_top&lt;/code&gt; refers to including (or not) the densely connected classifier on top of the network. By default, this densely connected classifier corresponds to the 1,000 classes from ImageNet. Because you intend to use your own densely connected classifier (with only two classes: &lt;code&gt;cat&lt;/code&gt; and &lt;code&gt;dog&lt;/code&gt;), you don’t need to include it.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;input_shape&lt;/code&gt; is the shape of the image tensors that you’ll feed to the network. This argument is purely optional: if you don’t pass it, the network will be able to process inputs of any size.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here’s the detail of the architecture of the VGG16 convolutional base. It’s similar to the simple convnets you’re already familiar with:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;summary(conv_base)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Layer (type)                     Output Shape          Param #  
================================================================
input_1 (InputLayer)             (None, 150, 150, 3)   0       
________________________________________________________________
block1_conv1 (Convolution2D)     (None, 150, 150, 64)  1792     
________________________________________________________________
block1_conv2 (Convolution2D)     (None, 150, 150, 64)  36928    
________________________________________________________________
block1_pool (MaxPooling2D)       (None, 75, 75, 64)    0        
________________________________________________________________
block2_conv1 (Convolution2D)     (None, 75, 75, 128)   73856    
________________________________________________________________
block2_conv2 (Convolution2D)     (None, 75, 75, 128)   147584   
________________________________________________________________
block2_pool (MaxPooling2D)       (None, 37, 37, 128)   0        
________________________________________________________________
block3_conv1 (Convolution2D)     (None, 37, 37, 256)   295168   
________________________________________________________________
block3_conv2 (Convolution2D)     (None, 37, 37, 256)   590080   
________________________________________________________________
block3_conv3 (Convolution2D)     (None, 37, 37, 256)   590080   
________________________________________________________________
block3_pool (MaxPooling2D)       (None, 18, 18, 256)   0        
________________________________________________________________
block4_conv1 (Convolution2D)     (None, 18, 18, 512)   1180160  
________________________________________________________________
block4_conv2 (Convolution2D)     (None, 18, 18, 512)   2359808  
________________________________________________________________
block4_conv3 (Convolution2D)     (None, 18, 18, 512)   2359808  
________________________________________________________________
block4_pool (MaxPooling2D)       (None, 9, 9, 512)     0        
________________________________________________________________
block5_conv1 (Convolution2D)     (None, 9, 9, 512)     2359808  
________________________________________________________________
block5_conv2 (Convolution2D)     (None, 9, 9, 512)     2359808  
________________________________________________________________
block5_conv3 (Convolution2D)     (None, 9, 9, 512)     2359808  
________________________________________________________________
block5_pool (MaxPooling2D)       (None, 4, 4, 512)     0        
================================================================
Total params: 14,714,688
Trainable params: 14,714,688
Non-trainable params: 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final feature map has shape &lt;code&gt;(4, 4, 512)&lt;/code&gt;. That’s the feature on top of which you’ll stick a densely connected classifier.&lt;/p&gt;
&lt;p&gt;At this point, there are two ways you could proceed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Running the convolutional base over your dataset, recording its output to an array on disk, and then using this data as input to a standalone, densely connected classifier similar to those you saw in part 1 of this book. This solution is fast and cheap to run, because it only requires running the convolutional base once for every input image, and the convolutional base is by far the most expensive part of the pipeline. But for the same reason, this technique won’t allow you to use data augmentation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Extending the model you have (&lt;code&gt;conv_base&lt;/code&gt;) by adding dense layers on top, and running the whole thing end to end on the input data. This will allow you to use data augmentation, because every input image goes through the convolutional base every time it’s seen by the model. But for the same reason, this technique is far more expensive than the first.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this post we’ll cover the second technique in detail (in the book we cover both). Note that this technique is so expensive that you should only attempt it if you have access to a GPU – it’s absolutely intractable on a CPU.&lt;/p&gt;
&lt;h3 id="feature-extraction-with-data-augmentation"&gt;Feature extraction with data augmentation&lt;/h3&gt;
&lt;p&gt;Because models behave just like layers, you can add a model (like &lt;code&gt;conv_base&lt;/code&gt;) to a sequential model just like you would add a layer.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;% 
  conv_base %&amp;gt;% 
  layer_flatten() %&amp;gt;% 
  layer_dense(units = 256, activation = &amp;quot;relu&amp;quot;) %&amp;gt;% 
  layer_dense(units = 1, activation = &amp;quot;sigmoid&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is what the model looks like now:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Layer (type)                     Output Shape          Param #  
================================================================
vgg16 (Model)                    (None, 4, 4, 512)     14714688                                     
________________________________________________________________
flatten_1 (Flatten)              (None, 8192)          0        
________________________________________________________________
dense_1 (Dense)                  (None, 256)           2097408  
________________________________________________________________
dense_2 (Dense)                  (None, 1)             257      
================================================================
Total params: 16,812,353
Trainable params: 16,812,353
Non-trainable params: 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, the convolutional base of VGG16 has 14,714,688 parameters, which is very large. The classifier you’re adding on top has 2 million parameters.&lt;/p&gt;
&lt;p&gt;Before you compile and train the model, it’s very important to freeze the convolutional base. &lt;em&gt;Freezing&lt;/em&gt; a layer or set of layers means preventing their weights from being updated during training. If you don’t do this, then the representations that were previously learned by the convolutional base will be modified during training. Because the dense layers on top are randomly initialized, very large weight updates would be propagated through the network, effectively destroying the representations previously learned.&lt;/p&gt;
&lt;p&gt;In Keras, you freeze a network using the &lt;code&gt;freeze_weights()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;length(model$trainable_weights)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 30&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;freeze_weights(conv_base)
length(model$trainable_weights)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this setup, only the weights from the two dense layers that you added will be trained. That’s a total of four weight tensors: two per layer (the main weight matrix and the bias vector). Note that in order for these changes to take effect, you must first compile the model. If you ever modify weight trainability after compilation, you should then recompile the model, or these changes will be ignored.&lt;/p&gt;
&lt;h4 id="using-data-augmentation"&gt;Using data augmentation&lt;/h4&gt;
&lt;p&gt;Overfitting is caused by having too few samples to learn from, rendering you unable to train a model that can generalize to new data. Given infinite data, your model would be exposed to every possible aspect of the data distribution at hand: you would never overfit. Data augmentation takes the approach of generating more training data from existing training samples, by &lt;em&gt;augmenting&lt;/em&gt; the samples via a number of random transformations that yield believable-looking images. The goal is that at training time, your model will never see the exact same picture twice. This helps expose the model to more aspects of the data and generalize better.&lt;/p&gt;
&lt;p&gt;In Keras, this can be done by configuring a number of random transformations to be performed on the images read by an &lt;code&gt;image_data_generator()&lt;/code&gt;. For example:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_datagen = image_data_generator(
  rescale = 1/255,
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE,
  fill_mode = &amp;quot;nearest&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These are just a few of the options available (for more, see the Keras documentation). Let’s quickly go over this code:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;rotation_range&lt;/code&gt; is a value in degrees (0–180), a range within which to randomly rotate pictures.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;width_shift&lt;/code&gt; and &lt;code&gt;height_shift&lt;/code&gt; are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;shear_range&lt;/code&gt; is for randomly applying shearing transformations.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;zoom_range&lt;/code&gt; is for randomly zooming inside pictures.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;horizontal_flip&lt;/code&gt; is for randomly flipping half the images horizontally – relevant when there are no assumptions of horizontal asymmetry (for example, real-world pictures).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fill_mode&lt;/code&gt; is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now we can train our model using the image data generator:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Note that the validation data shouldn&amp;#39;t be augmented!
test_datagen &amp;lt;- image_data_generator(rescale = 1/255)  

train_generator &amp;lt;- flow_images_from_directory(
  train_dir,                  # Target directory  
  train_datagen,              # Data generator
  target_size = c(150, 150),  # Resizes all images to 150 × 150
  batch_size = 20,
  class_mode = &amp;quot;binary&amp;quot;       # binary_crossentropy loss for binary labels
)

validation_generator &amp;lt;- flow_images_from_directory(
  validation_dir,
  test_datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = &amp;quot;binary&amp;quot;
)

model %&amp;gt;% compile(
  loss = &amp;quot;binary_crossentropy&amp;quot;,
  optimizer = optimizer_rmsprop(lr = 2e-5),
  metrics = c(&amp;quot;accuracy&amp;quot;)
)

history &amp;lt;- model %&amp;gt;% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 30,
  validation_data = validation_generator,
  validation_steps = 50
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s plot the results. As you can see, you reach a validation accuracy of about 90%.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2017-12-14-image-classification-on-small-datasets/images/5-3_metrics_plot_feature_extraction_with_augmentation-r.png" /&gt;&lt;/p&gt;
&lt;h3 id="fine-tuning"&gt;Fine-tuning&lt;/h3&gt;
&lt;p&gt;Another widely used technique for model reuse, complementary to feature extraction, is &lt;em&gt;fine-tuning&lt;/em&gt; Fine-tuning consists of unfreezing a few of the top layers of a frozen model base used for feature extraction, and jointly training both the newly added part of the model (in this case, the fully connected classifier) and these top layers. This is called &lt;em&gt;fine-tuning&lt;/em&gt; because it slightly adjusts the more abstract representations of the model being reused, in order to make them more relevant for the problem at hand.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2017-12-14-image-classification-on-small-datasets/images/vgg16_fine_tuning.png" style="width:60.0%" /&gt;&lt;/p&gt;
&lt;p&gt;I stated earlier that it’s necessary to freeze the convolution base of VGG16 in order to be able to train a randomly initialized classifier on top. For the same reason, it’s only possible to fine-tune the top layers of the convolutional base once the classifier on top has already been trained. If the classifier isn’t already trained, then the error signal propagating through the network during training will be too large, and the representations previously learned by the layers being fine-tuned will be destroyed. Thus the steps for fine-tuning a network are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add your custom network on top of an already-trained base network.&lt;/li&gt;
&lt;li&gt;Freeze the base network.&lt;/li&gt;
&lt;li&gt;Train the part you added.&lt;/li&gt;
&lt;li&gt;Unfreeze some layers in the base network.&lt;/li&gt;
&lt;li&gt;Jointly train both these layers and the part you added.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You already completed the first three steps when doing feature extraction. Let’s proceed with step 4: you’ll unfreeze your &lt;code&gt;conv_base&lt;/code&gt; and then freeze individual layers inside it.&lt;/p&gt;
&lt;p&gt;As a reminder, this is what your convolutional base looks like:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;summary(conv_base)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Layer (type)                     Output Shape          Param #  
================================================================
input_1 (InputLayer)             (None, 150, 150, 3)   0        
________________________________________________________________
block1_conv1 (Convolution2D)     (None, 150, 150, 64)  1792     
________________________________________________________________
block1_conv2 (Convolution2D)     (None, 150, 150, 64)  36928    
________________________________________________________________
block1_pool (MaxPooling2D)       (None, 75, 75, 64)    0        
________________________________________________________________
block2_conv1 (Convolution2D)     (None, 75, 75, 128)   73856    
________________________________________________________________
block2_conv2 (Convolution2D)     (None, 75, 75, 128)   147584   
________________________________________________________________
block2_pool (MaxPooling2D)       (None, 37, 37, 128)   0        
________________________________________________________________
block3_conv1 (Convolution2D)     (None, 37, 37, 256)   295168   
________________________________________________________________
block3_conv2 (Convolution2D)     (None, 37, 37, 256)   590080   
________________________________________________________________
block3_conv3 (Convolution2D)     (None, 37, 37, 256)   590080   
________________________________________________________________
block3_pool (MaxPooling2D)       (None, 18, 18, 256)   0        
________________________________________________________________
block4_conv1 (Convolution2D)     (None, 18, 18, 512)   1180160  
________________________________________________________________
block4_conv2 (Convolution2D)     (None, 18, 18, 512)   2359808  
________________________________________________________________
block4_conv3 (Convolution2D)     (None, 18, 18, 512)   2359808  
________________________________________________________________
block4_pool (MaxPooling2D)       (None, 9, 9, 512)     0        
________________________________________________________________
block5_conv1 (Convolution2D)     (None, 9, 9, 512)     2359808  
________________________________________________________________
block5_conv2 (Convolution2D)     (None, 9, 9, 512)     2359808  
________________________________________________________________
block5_conv3 (Convolution2D)     (None, 9, 9, 512)     2359808  
________________________________________________________________
block5_pool (MaxPooling2D)       (None, 4, 4, 512)     0        
================================================================
Total params: 14714688&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You’ll fine-tune all of the layers from &lt;code&gt;block3_conv1&lt;/code&gt; and on. Why not fine-tune the entire convolutional base? You could. But you need to consider the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Earlier layers in the convolutional base encode more-generic, reusable features, whereas layers higher up encode more-specialized features. It’s more useful to fine-tune the more specialized features, because these are the ones that need to be repurposed on your new problem. There would be fast-decreasing returns in fine-tuning lower layers.&lt;/li&gt;
&lt;li&gt;The more parameters you’re training, the more you’re at risk of overfitting. The convolutional base has 15 million parameters, so it would be risky to attempt to train it on your small dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thus, in this situation, it’s a good strategy to fine-tune only some of the layers in the convolutional base. Let’s set this up, starting from where you left off in the previous example.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;unfreeze_weights(conv_base, from = &amp;quot;block3_conv1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you can begin fine-tuning the network. You’ll do this with the RMSProp optimizer, using a very low learning rate. The reason for using a low learning rate is that you want to limit the magnitude of the modifications you make to the representations of the three layers you’re fine-tuning. Updates that are too large may harm these representations.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% compile(
  loss = &amp;quot;binary_crossentropy&amp;quot;,
  optimizer = optimizer_rmsprop(lr = 1e-5),
  metrics = c(&amp;quot;accuracy&amp;quot;)
)

history &amp;lt;- model %&amp;gt;% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 100,
  validation_data = validation_generator,
  validation_steps = 50
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s plot our results:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2017-12-14-image-classification-on-small-datasets/images/5-3_metrics_plot_fine_tuning-r.png" /&gt;&lt;/p&gt;
&lt;p&gt;You’re seeing a nice 6% absolute improvement in accuracy, from about 90% to above 96%.&lt;/p&gt;
&lt;p&gt;Note that the loss curve doesn’t show any real improvement (in fact, it’s deteriorating). You may wonder, how could accuracy stay stable or improve if the loss isn’t decreasing? The answer is simple: what you display is an average of pointwise loss values; but what matters for accuracy is the distribution of the loss values, not their average, because accuracy is the result of a binary thresholding of the class probability predicted by the model. The model may still be improving even if this isn’t reflected in the average loss.&lt;/p&gt;
&lt;p&gt;You can now finally evaluate this model on the test data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;test_generator &amp;lt;- flow_images_from_directory(
  test_dir,
  test_datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = &amp;quot;binary&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% evaluate_generator(test_generator, steps = 50)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$loss
[1] 0.2158171

$acc
[1] 0.965&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here you get a test accuracy of 96.5%. In the original Kaggle competition around this dataset, this would have been one of the top results. But using modern deep-learning techniques, you managed to reach this result using only a small fraction of the training data available (about 10%). There is a huge difference between being able to train on 20,000 samples compared to 2,000 samples!&lt;/p&gt;
&lt;h2 id="take-aways-using-convnets-with-small-datasets"&gt;Take-aways: using convnets with small datasets&lt;/h2&gt;
&lt;p&gt;Here’s what you should take away from the exercises in the past two sections:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Convnets are the best type of machine-learning models for computer-vision tasks. It’s possible to train one from scratch even on a very small dataset, with decent results.&lt;/li&gt;
&lt;li&gt;On a small dataset, overfitting will be the main issue. Data augmentation is a powerful way to fight overfitting when you’re working with image data.&lt;/li&gt;
&lt;li&gt;It’s easy to reuse an existing convnet on a new dataset via feature extraction. This is a valuable technique for working with small image datasets.&lt;/li&gt;
&lt;li&gt;As a complement to feature extraction, you can use fine-tuning, which adapts to a new problem some of the representations previously learned by an existing model. This pushes performance a bit further.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now you have a solid set of tools for dealing with image-classification problems – in particular with small datasets.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">09c44e1a6db81ae71e6b2c5a978b6617</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2017-12-14-image-classification-on-small-datasets</guid>
      <pubDate>Thu, 14 Dec 2017 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2017-12-14-image-classification-on-small-datasets/images/swapping_fc_classifier.png" medium="image" type="image/png" width="678" height="453"/>
    </item>
    <item>
      <title>Deep Learning for Text Classification with Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">François Chollet</dc:creator>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">J.J. Allaire</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2017-12-07-text-classification-with-keras</link>
      <description>


&lt;h2 id="the-imdb-dataset"&gt;The IMDB dataset&lt;/h2&gt;
&lt;p&gt;In this example, we’ll work with the IMDB dataset: a set of 50,000 highly polarized reviews from the Internet Movie Database. They’re split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting of 50% negative and 50% positive reviews.&lt;/p&gt;
&lt;p&gt;Why use separate training and test sets? Because you should never test a machine-learning model on the same data that you used to train it! Just because a model performs well on its training data doesn’t mean it will perform well on data it has never seen; and what you care about is your model’s performance on new data (because you already know the labels of your training data – obviously you don’t need your model to predict those). For instance, it’s possible that your model could end up merely &lt;em&gt;memorizing&lt;/em&gt; a mapping between your training samples and their targets, which would be useless for the task of predicting targets for data the model has never seen before. We’ll go over this point in much more detail in the next chapter.&lt;/p&gt;
&lt;p&gt;Just like the MNIST dataset, the IMDB dataset comes packaged with Keras. It has already been preprocessed: the reviews (sequences of words) have been turned into sequences of integers, where each integer stands for a specific word in a dictionary.&lt;/p&gt;
&lt;p&gt;The following code will load the dataset (when you run it the first time, about 80 MB of data will be downloaded to your machine).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
imdb &amp;lt;- dataset_imdb(num_words = 10000)
train_data &amp;lt;- imdb$train$x
train_labels &amp;lt;- imdb$train$y
test_data &amp;lt;- imdb$test$x
test_labels &amp;lt;- imdb$test$y&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The argument &lt;code&gt;num_words = 10000&lt;/code&gt; means you’ll only keep the top 10,000 most frequently occurring words in the training data. Rare words will be discarded. This allows you to work with vector data of manageable size.&lt;/p&gt;
&lt;p&gt;The variables &lt;code&gt;train_data&lt;/code&gt; and &lt;code&gt;test_data&lt;/code&gt; are lists of reviews; each review is a list of word indices (encoding a sequence of words). &lt;code&gt;train_labels&lt;/code&gt; and &lt;code&gt;test_labels&lt;/code&gt; are lists of 0s and 1s, where 0 stands for &lt;em&gt;negative&lt;/em&gt; and 1 stands for &lt;em&gt;positive&lt;/em&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;str(train_data[[1]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;int [1:218] 1 14 22 16 43 530 973 1622 1385 65 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_labels[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because you’re restricting yourself to the top 10,000 most frequent words, no word index will exceed 10,000:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;max(sapply(train_data, max))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 9999&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For kicks, here’s how you can quickly decode one of these reviews back to English words:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Named list mapping words to an integer index.
word_index &amp;lt;- dataset_imdb_word_index()  
reverse_word_index &amp;lt;- names(word_index)
names(reverse_word_index) &amp;lt;- word_index

# Decodes the review. Note that the indices are offset by 3 because 0, 1, and 
# 2 are reserved indices for &amp;quot;padding,&amp;quot; &amp;quot;start of sequence,&amp;quot; and &amp;quot;unknown.&amp;quot;
decoded_review &amp;lt;- sapply(train_data[[1]], function(index) {
  word &amp;lt;- if (index &amp;gt;= 3) reverse_word_index[[as.character(index - 3)]]
  if (!is.null(word)) word else &amp;quot;?&amp;quot;
})
cat(decoded_review)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;? this film was just brilliant casting location scenery story direction
everyone&amp;#39;s really suited the part they played and you could just imagine
being there robert ? is an amazing actor and now the same being director
? father came from the same scottish island as myself so i loved the fact
there was a real connection with this film the witty remarks throughout
the film were great it was just brilliant so much that i bought the film
as soon as it was released for ? and would recommend it to everyone to 
watch and the fly fishing was amazing really cried at the end it was so
sad and you know what they say if you cry at a film it must have been 
good and this definitely was also ? to the two little boy&amp;#39;s that played&amp;#39;
the ? of norman and paul they were just brilliant children are often left
out of the ? list i think because the stars that play them all grown up
are such a big profile for the whole film but these children are amazing
and should be praised for what they have done don&amp;#39;t you think the whole
story was so lovely because it was true and was someone&amp;#39;s life after all
that was shared with us all&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="preparing-the-data"&gt;Preparing the data&lt;/h2&gt;
&lt;p&gt;You can’t feed lists of integers into a neural network. You have to turn your lists into tensors. There are two ways to do that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pad your lists so that they all have the same length, turn them into an integer tensor of shape &lt;code&gt;(samples, word_indices)&lt;/code&gt;, and then use as the first layer in your network a layer capable of handling such integer tensors (the “embedding” layer, which we’ll cover in detail later in the book).&lt;/li&gt;
&lt;li&gt;One-hot encode your lists to turn them into vectors of 0s and 1s. This would mean, for instance, turning the sequence &lt;code&gt;[3, 5]&lt;/code&gt; into a 10,000-dimensional vector that would be all 0s except for indices 3 and 5, which would be 1s. Then you could use as the first layer in your network a dense layer, capable of handling floating-point vector data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s go with the latter solution to vectorize the data, which you’ll do manually for maximum clarity.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;vectorize_sequences &amp;lt;- function(sequences, dimension = 10000) {
  # Creates an all-zero matrix of shape (length(sequences), dimension)
  results &amp;lt;- matrix(0, nrow = length(sequences), ncol = dimension) 
  for (i in 1:length(sequences))
    # Sets specific indices of results[i] to 1s
    results[i, sequences[[i]]] &amp;lt;- 1 
  results
}

x_train &amp;lt;- vectorize_sequences(train_data)
x_test &amp;lt;- vectorize_sequences(test_data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s what the samples look like now:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;str(x_train[1,])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; num [1:10000] 1 1 0 1 1 1 1 1 1 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should also convert your labels from integer to numeric, which is straightforward:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;y_train &amp;lt;- as.numeric(train_labels)
y_test &amp;lt;- as.numeric(test_labels)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the data is ready to be fed into a neural network.&lt;/p&gt;
&lt;h2 id="building-your-network"&gt;Building your network&lt;/h2&gt;
&lt;p&gt;The input data is vectors, and the labels are scalars (1s and 0s): this is the easiest setup you’ll ever encounter. A type of network that performs well on such a problem is a simple stack of fully connected (“dense”) layers with &lt;code&gt;relu&lt;/code&gt; activations: &lt;code&gt;layer_dense(units = 16, activation = "relu")&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The argument being passed to each dense layer (16) is the number of hidden units of the layer. A &lt;em&gt;hidden unit&lt;/em&gt; is a dimension in the representation space of the layer. You may remember from chapter 2 that each such dense layer with a &lt;code&gt;relu&lt;/code&gt; activation implements the following chain of tensor operations:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;output = relu(dot(W, input) + b)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Having 16 hidden units means the weight matrix &lt;code&gt;W&lt;/code&gt; will have shape &lt;code&gt;(input_dimension, 16)&lt;/code&gt;: the dot product with &lt;code&gt;W&lt;/code&gt; will project the input data onto a 16-dimensional representation space (and then you’ll add the bias vector &lt;code&gt;b&lt;/code&gt; and apply the &lt;code&gt;relu&lt;/code&gt; operation). You can intuitively understand the dimensionality of your representation space as “how much freedom you’re allowing the network to have when learning internal representations.” Having more hidden units (a higher-dimensional representation space) allows your network to learn more-complex representations, but it makes the network more computationally expensive and may lead to learning unwanted patterns (patterns that will improve performance on the training data but not on the test data).&lt;/p&gt;
&lt;p&gt;There are two key architecture decisions to be made about such stack of dense layers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How many layers to use&lt;/li&gt;
&lt;li&gt;How many hidden units to choose for each layer&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In chapter 4, you’ll learn formal principles to guide you in making these choices. For the time being, you’ll have to trust me with the following architecture choice:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Two intermediate layers with 16 hidden units each&lt;/li&gt;
&lt;li&gt;A third layer that will output the scalar prediction regarding the sentiment of the current review&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The intermediate layers will use &lt;code&gt;relu&lt;/code&gt; as their activation function, and the final layer will use a sigmoid activation so as to output a probability (a score between 0 and 1, indicating how likely the sample is to have the target “1”: how likely the review is to be positive). A &lt;code&gt;relu&lt;/code&gt; (rectified linear unit) is a function meant to zero out negative values.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2017-12-07-text-classification-with-keras/images/relu.png" style="width:60.0%" /&gt;&lt;/p&gt;
&lt;p&gt;A sigmoid “squashes” arbitrary values into the &lt;code&gt;[0, 1]&lt;/code&gt; interval, outputting something that can be interpreted as a probability.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2017-12-07-text-classification-with-keras/images/sigmoid.png" style="width:60.0%" /&gt;&lt;/p&gt;
&lt;p&gt;Here’s what the network looks like.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2017-12-07-text-classification-with-keras/images/3_layer_network.png" style="width:60.0%" /&gt;&lt;/p&gt;
&lt;p&gt;Here’s the Keras implementation, similar to the MNIST example you saw previously.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)

model &amp;lt;- keras_model_sequential() %&amp;gt;% 
  layer_dense(units = 16, activation = &amp;quot;relu&amp;quot;, input_shape = c(10000)) %&amp;gt;% 
  layer_dense(units = 16, activation = &amp;quot;relu&amp;quot;) %&amp;gt;% 
  layer_dense(units = 1, activation = &amp;quot;sigmoid&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="activation-functions"&gt;Activation Functions&lt;/h3&gt;
&lt;p&gt;Note that without an activation function like &lt;code&gt;relu&lt;/code&gt; (also called a &lt;em&gt;non-linearity&lt;/em&gt;), the dense layer would consist of two linear operations – a dot product and an addition:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;output = dot(W, input) + b&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;So the layer could only learn &lt;em&gt;linear transformations&lt;/em&gt; (affine transformations) of the input data: the &lt;em&gt;hypothesis space&lt;/em&gt; of the layer would be the set of all possible linear transformations of the input data into a 16-dimensional space. Such a hypothesis space is too restricted and wouldn’t benefit from multiple layers of representations, because a deep stack of linear layers would still implement a linear operation: adding more layers wouldn’t extend the hypothesis space.&lt;/p&gt;
&lt;p&gt;In order to get access to a much richer hypothesis space that would benefit from deep representations, you need a non-linearity, or activation function. &lt;code&gt;relu&lt;/code&gt; is the most popular activation function in deep learning, but there are many other candidates, which all come with similarly strange names: &lt;code&gt;prelu&lt;/code&gt;, &lt;code&gt;elu&lt;/code&gt;, and so on.&lt;/p&gt;
&lt;h3 id="loss-function-and-optimizer"&gt;Loss Function and Optimizer&lt;/h3&gt;
&lt;p&gt;Finally, you need to choose a loss function and an optimizer. Because you’re facing a binary classification problem and the output of your network is a probability (you end your network with a single-unit layer with a sigmoid activation), it’s best to use the &lt;code&gt;binary_crossentropy&lt;/code&gt; loss. It isn’t the only viable choice: you could use, for instance, &lt;code&gt;mean_squared_error&lt;/code&gt;. But crossentropy is usually the best choice when you’re dealing with models that output probabilities. &lt;em&gt;Crossentropy&lt;/em&gt; is a quantity from the field of Information Theory that measures the distance between probability distributions or, in this case, between the ground-truth distribution and your predictions.&lt;/p&gt;
&lt;p&gt;Here’s the step where you configure the model with the &lt;code&gt;rmsprop&lt;/code&gt; optimizer and the &lt;code&gt;binary_crossentropy&lt;/code&gt; loss function. Note that you’ll also monitor accuracy during training.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% compile(
  optimizer = &amp;quot;rmsprop&amp;quot;,
  loss = &amp;quot;binary_crossentropy&amp;quot;,
  metrics = c(&amp;quot;accuracy&amp;quot;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You’re passing your optimizer, loss function, and metrics as strings, which is possible because &lt;code&gt;rmsprop&lt;/code&gt;, &lt;code&gt;binary_crossentropy&lt;/code&gt;, and &lt;code&gt;accuracy&lt;/code&gt; are packaged as part of Keras. Sometimes you may want to configure the parameters of your optimizer or pass a custom loss function or metric function. The former can be done by passing an optimizer instance as the &lt;code&gt;optimizer&lt;/code&gt; argument:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% compile(
  optimizer = optimizer_rmsprop(lr=0.001),
  loss = &amp;quot;binary_crossentropy&amp;quot;,
  metrics = c(&amp;quot;accuracy&amp;quot;)
) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Custom loss and metrics functions can be provided by passing function objects as the &lt;code&gt;loss&lt;/code&gt; and/or &lt;code&gt;metrics&lt;/code&gt; arguments&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% compile(
  optimizer = optimizer_rmsprop(lr = 0.001),
  loss = loss_binary_crossentropy,
  metrics = metric_binary_accuracy
) &lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="validating-your-approach"&gt;Validating your approach&lt;/h3&gt;
&lt;p&gt;In order to monitor during training the accuracy of the model on data it has never seen before, you’ll create a validation set by setting apart 10,000 samples from the original training data.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;val_indices &amp;lt;- 1:10000

x_val &amp;lt;- x_train[val_indices,]
partial_x_train &amp;lt;- x_train[-val_indices,]

y_val &amp;lt;- y_train[val_indices]
partial_y_train &amp;lt;- y_train[-val_indices]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You’ll now train the model for 20 epochs (20 iterations over all samples in the &lt;code&gt;x_train&lt;/code&gt; and &lt;code&gt;y_train&lt;/code&gt; tensors), in mini-batches of 512 samples. At the same time, you’ll monitor loss and accuracy on the 10,000 samples that you set apart. You do so by passing the validation data as the &lt;code&gt;validation_data&lt;/code&gt; argument.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% compile(
  optimizer = &amp;quot;rmsprop&amp;quot;,
  loss = &amp;quot;binary_crossentropy&amp;quot;,
  metrics = c(&amp;quot;accuracy&amp;quot;)
)

history &amp;lt;- model %&amp;gt;% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On CPU, this will take less than 2 seconds per epoch – training is over in 20 seconds. At the end of every epoch, there is a slight pause as the model computes its loss and accuracy on the 10,000 samples of the validation data.&lt;/p&gt;
&lt;p&gt;Note that the call to &lt;code&gt;fit()&lt;/code&gt; returns a &lt;code&gt;history&lt;/code&gt; object. The &lt;code&gt;history&lt;/code&gt; object has a &lt;code&gt;plot()&lt;/code&gt; method that enables us to visualize the training and validation metrics by epoch:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(history)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2017-12-07-text-classification-with-keras/images/training-history.png" style="width:100.0%" /&gt;&lt;/p&gt;
&lt;p&gt;The accuracy is plotted on the top panel and the loss on the bottom panel. Note that your own results may vary slightly due to a different random initialization of your network.&lt;/p&gt;
&lt;p&gt;As you can see, the training loss decreases with every epoch, and the training accuracy increases with every epoch. That’s what you would expect when running a gradient-descent optimization – the quantity you’re trying to minimize should be less with every iteration. But that isn’t the case for the validation loss and accuracy: they seem to peak at the fourth epoch. This is an example of what we warned against earlier: a model that performs better on the training data isn’t necessarily a model that will do better on data it has never seen before. In precise terms, what you’re seeing is &lt;em&gt;overfitting&lt;/em&gt;: after the second epoch, you’re overoptimizing on the training data, and you end up learning representations that are specific to the training data and don’t generalize to data outside of the training set.&lt;/p&gt;
&lt;p&gt;In this case, to prevent overfitting, you could stop training after three epochs. In general, you can use a range of techniques to mitigate overfitting,which we’ll cover in chapter 4.&lt;/p&gt;
&lt;p&gt;Let’s train a new network from scratch for four epochs and then evaluate it on the test data.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() %&amp;gt;% 
  layer_dense(units = 16, activation = &amp;quot;relu&amp;quot;, input_shape = c(10000)) %&amp;gt;% 
  layer_dense(units = 16, activation = &amp;quot;relu&amp;quot;) %&amp;gt;% 
  layer_dense(units = 1, activation = &amp;quot;sigmoid&amp;quot;)

model %&amp;gt;% compile(
  optimizer = &amp;quot;rmsprop&amp;quot;,
  loss = &amp;quot;binary_crossentropy&amp;quot;,
  metrics = c(&amp;quot;accuracy&amp;quot;)
)

model %&amp;gt;% fit(x_train, y_train, epochs = 4, batch_size = 512)
results &amp;lt;- model %&amp;gt;% evaluate(x_test, y_test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;results&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$loss
[1] 0.2900235

$acc
[1] 0.88512&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This fairly naive approach achieves an accuracy of 88%. With state-of-the-art approaches, you should be able to get close to 95%.&lt;/p&gt;
&lt;h2 id="generating-predictions"&gt;Generating predictions&lt;/h2&gt;
&lt;p&gt;After having trained a network, you’ll want to use it in a practical setting. You can generate the likelihood of reviews being positive by using the &lt;code&gt;predict&lt;/code&gt; method:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% predict(x_test[1:10,])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1,] 0.92306918
 [2,] 0.84061098
 [3,] 0.99952853
 [4,] 0.67913240
 [5,] 0.73874789
 [6,] 0.23108074
 [7,] 0.01230567
 [8,] 0.04898361
 [9,] 0.99017477
[10,] 0.72034937&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, the network is confident for some samples (0.99 or more, or 0.01 or less) but less confident for others (0.7, 0.2).&lt;/p&gt;
&lt;h2 id="further-experiments"&gt;Further experiments&lt;/h2&gt;
&lt;p&gt;The following experiments will help convince you that the architecture choices you’ve made are all fairly reasonable, although there’s still room for improvement.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You used two hidden layers. Try using one or three hidden layers, and see how doing so affects validation and test accuracy.&lt;/li&gt;
&lt;li&gt;Try using layers with more hidden units or fewer hidden units: 32 units, 64 units, and so on.&lt;/li&gt;
&lt;li&gt;Try using the &lt;code&gt;mse&lt;/code&gt; loss function instead of &lt;code&gt;binary_crossentropy&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Try using the &lt;code&gt;tanh&lt;/code&gt; activation (an activation that was popular in the early days of neural networks) instead of &lt;code&gt;relu&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="wrapping-up"&gt;Wrapping up&lt;/h2&gt;
&lt;p&gt;Here’s what you should take away from this example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You usually need to do quite a bit of preprocessing on your raw data in order to be able to feed it – as tensors – into a neural network. Sequences of words can be encoded as binary vectors, but there are other encoding options, too.&lt;/li&gt;
&lt;li&gt;Stacks of dense layers with &lt;code&gt;relu&lt;/code&gt; activations can solve a wide range of problems (including sentiment classification), and you’ll likely use them frequently.&lt;/li&gt;
&lt;li&gt;In a binary classification problem (two output classes), your network should end with a dense layer with one unit and a &lt;code&gt;sigmoid&lt;/code&gt; activation: the output of your network should be a scalar between 0 and 1, encoding a probability.&lt;/li&gt;
&lt;li&gt;With such a scalar sigmoid output on a binary classification problem, the loss function you should use is &lt;code&gt;binary_crossentropy&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;rmsprop&lt;/code&gt; optimizer is generally a good enough choice, whatever your problem. That’s one less thing for you to worry about.&lt;/li&gt;
&lt;li&gt;As they get better on their training data, neural networks eventually start overfitting and end up obtaining increasingly worse results on data they’ve never seen before. Be sure to always monitor performance on data that is outside of the training set.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">10d047044ac8660d47f5f9a51f5dabe8</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Natural Language Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2017-12-07-text-classification-with-keras</guid>
      <pubDate>Thu, 07 Dec 2017 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2017-12-07-text-classification-with-keras/images/training-history.png" medium="image" type="image/png" width="1400" height="865"/>
    </item>
    <item>
      <title>Keras for R</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">J.J. Allaire</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2017-09-06-keras-for-r</link>
      <description>


&lt;p&gt;We are excited to announce that the &lt;a href="https://tensorflow.rstudio.com/keras"&gt;keras package&lt;/a&gt; is now available on CRAN. The package provides an R interface to &lt;a href="https://keras.io"&gt;Keras&lt;/a&gt;, a high-level neural networks API developed with a focus on enabling fast experimentation. Keras has the following key features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Allows the same code to run on CPU or on GPU, seamlessly.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;User-friendly API which makes it easy to quickly prototype deep learning models.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Built-in support for convolutional networks (for computer vision), recurrent networks (for sequence processing), and any combination of both.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Supports arbitrary network architectures: multi-input or multi-output models, layer sharing, model sharing, etc. This means that Keras is appropriate for building essentially any deep learning model, from a memory network to a neural Turing machine.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is capable of running on top of multiple back-ends including &lt;a href="https://github.com/tensorflow/tensorflow"&gt;TensorFlow&lt;/a&gt;, &lt;a href="https://github.com/Microsoft/cntk"&gt;CNTK&lt;/a&gt;, or &lt;a href="https://github.com/Theano/Theano"&gt;Theano&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you are already familiar with Keras and want to jump right in, check out &lt;a href="https://tensorflow.rstudio.com/keras" class="uri"&gt;https://tensorflow.rstudio.com/keras&lt;/a&gt; which has everything you need to get started including over 20 complete examples to learn from.&lt;/p&gt;
&lt;p&gt;To learn a bit more about Keras and why we’re so excited to announce the Keras interface for R, read on!&lt;/p&gt;
&lt;h2 id="keras-and-deep-learning"&gt;Keras and Deep Learning&lt;/h2&gt;
&lt;p&gt;Interest in deep learning has been accelerating rapidly over the past few years, and several deep learning frameworks have emerged over the same time frame. Of all the available frameworks, Keras has stood out for its productivity, flexibility and user-friendly API. At the same time, TensorFlow has emerged as a next-generation machine learning platform that is both extremely flexible and well-suited to production deployment.&lt;/p&gt;
&lt;p&gt;Not surprisingly, Keras and TensorFlow have of late been pulling away from other deep learning frameworks:&lt;/p&gt;
&lt;blockquote class="twitter-tweet" data-lang="en"&gt;
&lt;p lang="en" dir="ltr"&gt;
Google web search interest around deep learning frameworks over time. If you remember Q4 2015 and Q1-2 2016 as confusing, you weren't alone. &lt;a href="https://t.co/1f1VQVGr8n"&gt;pic.twitter.com/1f1VQVGr8n&lt;/a&gt;
&lt;/p&gt;
— François Chollet (&lt;span class="citation"&gt;@fchollet&lt;/span&gt;) &lt;a href="https://twitter.com/fchollet/status/871089784898310144"&gt;June 3, 2017&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
&lt;p&gt;The good news about Keras and TensorFlow is that you don’t need to choose between them! The default backend for Keras is TensorFlow and Keras can be &lt;a href="https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html"&gt;integrated seamlessly&lt;/a&gt; with TensorFlow workflows. There is also a pure-TensorFlow implementation of Keras with &lt;a href="https://www.youtube.com/watch?v=UeheTiBJ0Io&amp;amp;t=7s&amp;amp;index=8&amp;amp;list=PLOU2XLYxmsIKGc_NBoIhTn2Qhraji53cv"&gt;deeper integration&lt;/a&gt; on the roadmap for later this year.&lt;/p&gt;
&lt;p&gt;Keras and TensorFlow are the state of the art in deep learning tools and with the keras package you can now access both with a fluent R interface.&lt;/p&gt;
&lt;h2 id="getting-started"&gt;Getting Started&lt;/h2&gt;
&lt;h3 id="installation"&gt;Installation&lt;/h3&gt;
&lt;p&gt;To begin, install the keras R package from CRAN as follows:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;install.packages(&amp;quot;keras&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Keras R interface uses the &lt;a href="https://www.tensorflow.org/"&gt;TensorFlow&lt;/a&gt; backend engine by default. To install both the core Keras library as well as the TensorFlow backend use the &lt;code&gt;install_keras()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
install_keras()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will provide you with default CPU-based installations of Keras and TensorFlow. If you want a more customized installation, e.g. if you want to take advantage of NVIDIA GPUs, see the documentation for &lt;a href="https://tensorflow.rstudio.com/keras/reference/install_keras.html"&gt;&lt;code&gt;install_keras()&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="mnist-example"&gt;MNIST Example&lt;/h3&gt;
&lt;p&gt;We can learn the basics of Keras by walking through a simple example: recognizing handwritten digits from the &lt;a href="https://en.wikipedia.org/wiki/MNIST_database"&gt;MNIST&lt;/a&gt; dataset. MNIST consists of 28 x 28 grayscale images of handwritten digits like these:&lt;/p&gt;
&lt;p&gt;&lt;img style="width: 50%;" src="https://www.tensorflow.org/images/MNIST.png"&gt;&lt;/p&gt;
&lt;p&gt;The dataset also includes labels for each image, telling us which digit it is. For example, the labels for the above images are 5, 0, 4, and 1.&lt;/p&gt;
&lt;h4 id="preparing-the-data"&gt;Preparing the Data&lt;/h4&gt;
&lt;p&gt;The MNIST dataset is included with Keras and can be accessed using the &lt;code&gt;dataset_mnist()&lt;/code&gt; function. Here we load the dataset then create variables for our test and training data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(keras)
mnist &amp;lt;- dataset_mnist()
x_train &amp;lt;- mnist$train$x
y_train &amp;lt;- mnist$train$y
x_test &amp;lt;- mnist$test$x
y_test &amp;lt;- mnist$test$y&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;x&lt;/code&gt; data is a 3-d array &lt;code&gt;(images,width,height)&lt;/code&gt; of grayscale values. To prepare the data for training we convert the 3-d arrays into matrices by reshaping width and height into a single dimension (28x28 images are flattened into length 784 vectors). Then, we convert the grayscale values from integers ranging between 0 to 255 into floating point values ranging between 0 and 1:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# reshape
dim(x_train) &amp;lt;- c(nrow(x_train), 784)
dim(x_test) &amp;lt;- c(nrow(x_test), 784)
# rescale
x_train &amp;lt;- x_train / 255
x_test &amp;lt;- x_test / 255&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;y&lt;/code&gt; data is an integer vector with values ranging from 0 to 9. To prepare this data for training we &lt;a href="https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science"&gt;one-hot encode&lt;/a&gt; the vectors into binary class matrices using the Keras &lt;code&gt;to_categorical()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;y_train &amp;lt;- to_categorical(y_train, 10)
y_test &amp;lt;- to_categorical(y_test, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="defining-the-model"&gt;Defining the Model&lt;/h4&gt;
&lt;p&gt;The core data structure of Keras is a model, a way to organize layers. The simplest type of model is the &lt;a href="https://tensorflow.rstudio.com/keras/articles/sequential_model.html"&gt;sequential model&lt;/a&gt;, a linear stack of layers.&lt;/p&gt;
&lt;p&gt;We begin by creating a sequential model and then adding layers using the pipe (&lt;code&gt;%&amp;gt;%&lt;/code&gt;) operator:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential() 
model %&amp;gt;% 
  layer_dense(units = 256, activation = &amp;quot;relu&amp;quot;, input_shape = c(784)) %&amp;gt;% 
  layer_dropout(rate = 0.4) %&amp;gt;% 
  layer_dense(units = 128, activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dropout(rate = 0.3) %&amp;gt;%
  layer_dense(units = 10, activation = &amp;quot;softmax&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;input_shape&lt;/code&gt; argument to the first layer specifies the shape of the input data (a length 784 numeric vector representing a grayscale image). The final layer outputs a length 10 numeric vector (probabilities for each digit) using a &lt;a href="https://en.wikipedia.org/wiki/Softmax_function"&gt;softmax activation function&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Use the &lt;code&gt;summary()&lt;/code&gt; function to print the details of the model:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre style="box-shadow: none;"&gt;&lt;code&gt;Model
________________________________________________________________________________
Layer (type)                        Output Shape                    Param #     
================================================================================
dense_1 (Dense)                     (None, 256)                     200960      
________________________________________________________________________________
dropout_1 (Dropout)                 (None, 256)                     0           
________________________________________________________________________________
dense_2 (Dense)                     (None, 128)                     32896       
________________________________________________________________________________
dropout_2 (Dropout)                 (None, 128)                     0           
________________________________________________________________________________
dense_3 (Dense)                     (None, 10)                      1290        
================================================================================
Total params: 235,146
Trainable params: 235,146
Non-trainable params: 0
________________________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, compile the model with appropriate loss function, optimizer, and metrics:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% compile(
  loss = &amp;quot;categorical_crossentropy&amp;quot;,
  optimizer = optimizer_rmsprop(),
  metrics = c(&amp;quot;accuracy&amp;quot;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="training-and-evaluation"&gt;Training and Evaluation&lt;/h4&gt;
&lt;p&gt;Use the &lt;code&gt;fit()&lt;/code&gt; function to train the model for 30 epochs using batches of 128 images:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;history &amp;lt;- model %&amp;gt;% fit(
  x_train, y_train, 
  epochs = 30, batch_size = 128, 
  validation_split = 0.2
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;history&lt;/code&gt; object returned by &lt;code&gt;fit()&lt;/code&gt; includes loss and accuracy metrics which we can plot:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(history)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://keras.rstudio.com/images/training_history_ggplot2.png" /&gt;&lt;/p&gt;
&lt;p&gt;Evaluate the model’s performance on the test data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% evaluate(x_test, y_test,verbose = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$loss
[1] 0.1149

$acc
[1] 0.9807&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Generate predictions on new data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model %&amp;gt;% predict_classes(x_test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  [1] 7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7 2 7 1 2
 [40] 1 1 7 4 2 3 5 1 2 4 4 6 3 5 5 6 0 4 1 9 5 7 8 9 3 7 4 6 4 3 0 7 0 2 9 1 7 3 2
 [79] 9 7 7 6 2 7 8 4 7 3 6 1 3 6 9 3 1 4 1 7 6 9
 [ reached getOption(&amp;quot;max.print&amp;quot;) -- omitted 9900 entries ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keras provides a vocabulary for building deep learning models that is simple, elegant, and intuitive. Building a question answering system, an image classification model, a neural Turing machine, or any other model is just as straightforward.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://tensorflow.rstudio.com/keras/articles/sequential_model.html"&gt;Guide to the Sequential Model&lt;/a&gt; article describes the basics of Keras sequential models in more depth.&lt;/p&gt;
&lt;h2 id="examples"&gt;Examples&lt;/h2&gt;
&lt;p&gt;Over 20 complete examples are available (special thanks to &lt;span class="citation"&gt;[@dfalbel]&lt;/span&gt;(&lt;a href="https://github.com/dfalbel" class="uri"&gt;https://github.com/dfalbel&lt;/a&gt;) for his work on these!). The examples cover image classification, text generation with stacked LSTMs, question-answering with memory networks, transfer learning, variational encoding, and more.&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width="53%" /&gt;
&lt;col width="46%" /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th&gt;Example&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/addition_rnn.html"&gt;addition_rnn&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Implementation of sequence to sequence learning for performing addition of two numbers (as strings).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/babi_memnn.html"&gt;babi_memnn&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Trains a memory network on the bAbI dataset for reading comprehension.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/babi_rnn.html"&gt;babi_rnn&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Trains a two-branch recurrent network on the bAbI dataset for reading comprehension.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/cifar10_cnn.html"&gt;cifar10_cnn&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Trains a simple deep CNN on the CIFAR10 small images dataset.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/conv_lstm.html"&gt;conv_lstm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Demonstrates the use of a convolutional LSTM network.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/deep_dream.html"&gt;deep_dream&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Deep Dreams in Keras.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/imdb_bidirectional_lstm.html"&gt;imdb_bidirectional_lstm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Trains a Bidirectional LSTM on the IMDB sentiment classification task.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/imdb_cnn.html"&gt;imdb_cnn&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Demonstrates the use of Convolution1D for text classification.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/imdb_cnn_lstm.html"&gt;imdb_cnn_lstm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Trains a convolutional stack followed by a recurrent stack network on the IMDB sentiment classification task.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/imdb_fasttext.html"&gt;imdb_fasttext&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Trains a FastText model on the IMDB sentiment classification task.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/imdb_lstm.html"&gt;imdb_lstm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Trains a LSTM on the IMDB sentiment classification task.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/lstm_text_generation.html"&gt;lstm_text_generation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Generates text from Nietzsche’s writings.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/mnist_acgan.html"&gt;mnist_acgan&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Implementation of AC-GAN (Auxiliary Classifier GAN ) on the MNIST dataset&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/mnist_antirectifier.html"&gt;mnist_antirectifier&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Demonstrates how to write custom layers for Keras&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/mnist_cnn.html"&gt;mnist_cnn&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Trains a simple convnet on the MNIST dataset.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/mnist_irnn.html"&gt;mnist_irnn&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Reproduction of the IRNN experiment with pixel-by-pixel sequential MNIST in “A Simple Way to Initialize Recurrent Networks of Rectified Linear Units” by Le et al.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/mnist_mlp.html"&gt;mnist_mlp&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Trains a simple deep multi-layer perceptron on the MNIST dataset.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/mnist_hierarchical_rnn.html"&gt;mnist_hierarchical_rnn&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Trains a Hierarchical RNN (HRNN) to classify MNIST digits.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/mnist_transfer_cnn.html"&gt;mnist_transfer_cnn&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Transfer learning toy example.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/neural_style_transfer.html"&gt;neural_style_transfer&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Neural style transfer (generating an image with the same “content” as a base image, but with the “style” of a different picture).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/reuters_mlp.html"&gt;reuters_mlp&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Trains and evaluates a simple MLP on the Reuters newswire topic classification task.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/stateful_lstm.html"&gt;stateful_lstm&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Demonstrates how to use stateful RNNs to model long sequences efficiently.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/variational_autoencoder.html"&gt;variational_autoencoder&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Demonstrates how to build a variational autoencoder.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/examples/variational_autoencoder_deconv.html"&gt;variational_autoencoder_deconv&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Demonstrates how to build a variational autoencoder with Keras using deconvolution layers.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id="learning-more"&gt;Learning More&lt;/h2&gt;
&lt;p&gt;After you’ve become familiar with the basics, these articles are a good next step:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/sequential_model.html"&gt;Guide to the Sequential Model&lt;/a&gt;. The sequential model is a linear stack of layers and is the API most users should start with.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/functional_api.html"&gt;Guide to the Functional API&lt;/a&gt;. The Keras functional API is the way to go for defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/training_visualization.html"&gt;Training Visualization&lt;/a&gt;. There are a wide variety of tools available for visualizing training. These include plotting of training metrics, real time display of metrics within the RStudio IDE, and integration with the TensorBoard visualization tool included with TensorFlow.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/applications.html"&gt;Using Pre-Trained Models&lt;/a&gt;. Keras includes a number of deep learning models (Xception, VGG16, VGG19, ResNet50, InceptionVV3, and MobileNet) that are made available alongside pre-trained weights. These models can be used for prediction, feature extraction, and fine-tuning.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://tensorflow.rstudio.com/keras/articles/faq.html"&gt;Frequently Asked Questions&lt;/a&gt;. Covers many additional topics including streaming training data, saving models, training on GPUs, and more.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Keras provides a productive, highly flexible framework for developing deep learning models. We can’t wait to see what the R community will do with these tools!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">4005c12c2397234717b35c6a74ab1567</distill:md5>
      <category>TensorFlow/Keras</category>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2017-09-06-keras-for-r</guid>
      <pubDate>Tue, 05 Sep 2017 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2017-09-06-keras-for-r/preview.png" medium="image" type="image/png" width="669" height="414"/>
    </item>
  </channel>
</rss>
