<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>RStudio AI Blog</title>
    <link>https://blogs.rstudio.com/tensorflow/</link>
    <atom:link href="https://blogs.rstudio.com/tensorflow/index.xml" rel="self" type="application/rss+xml"/>
    <description>News, concepts, and applications as regards deep learning, probabilistic computation, distributed computing and machine learning automation from R.
</description>
    <image>
      <title>RStudio AI Blog</title>
      <url>https://blogs.rstudio.com/tensorflow/images/favicon.png</url>
      <link>https://blogs.rstudio.com/tensorflow/</link>
    </image>
    <generator>Distill</generator>
    <lastBuildDate>Mon, 07 Sep 2020 00:00:00 +0000</lastBuildDate>
    <item>
      <title>Introducing sparklyr.flint: A time-series extension for sparklyr</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yitao Li</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-09-07-sparklyr-flint</link>
      <description>


&lt;div class="container-fluid main-container"&gt;




&lt;div class="fluid-row" id="header"&gt;




&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;```&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="line-block"&gt;     In other words, given a timestamp &lt;code&gt;t&lt;/code&gt; and a row in the result having &lt;code&gt;time&lt;/code&gt; equal to &lt;code&gt;t&lt;/code&gt;, one can notice the &lt;code&gt;value_sum&lt;/code&gt; column of that row contains sum of &lt;code&gt;value&lt;/code&gt;s within the time window of &lt;code&gt;[t - 2, t]&lt;/code&gt; from &lt;code&gt;ts_rdd&lt;/code&gt;.&lt;/div&gt;
&lt;div id="intro-to-sparklyr.flint" class="section level1"&gt;
&lt;h1&gt;Intro to &lt;code&gt;sparklyr.flint&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;The purpose of &lt;code&gt;sparklyr.flint&lt;/code&gt; is to make time-series functionalities of &lt;code&gt;Flint&lt;/code&gt; easily accessible from &lt;code&gt;sparklyr&lt;/code&gt;. To see &lt;code&gt;sparklyr.flint&lt;/code&gt; in action, one can skim through the example in the previous section, go through the following to produce the exact R-equivalent of each step in that example, and then obtain the same summarization as the final result:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;First of all, install &lt;code&gt;sparklyr&lt;/code&gt; and &lt;code&gt;sparklyr.flint&lt;/code&gt; if you haven’t done so already.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;install.packages(&amp;quot;sparklyr&amp;quot;)
install.packages(&amp;quot;sparklyr.flint&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Connect to Apache Spark that is running locally from &lt;code&gt;sparklyr&lt;/code&gt;, but remember to attach &lt;code&gt;sparklyr.flint&lt;/code&gt; before running &lt;code&gt;sparklyr::spark_connect&lt;/code&gt;, and then import our example time-series data to Spark:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sparklyr)
library(sparklyr.flint)

sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;, version = &amp;quot;2.4&amp;quot;)
sdf &amp;lt;- copy_to(sc, data.frame(time = seq(4), value = seq(4)^2))&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Convert &lt;code&gt;sdf&lt;/code&gt; above into a &lt;code&gt;TimeSeriesRDD&lt;/code&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;ts_rdd &amp;lt;- fromSDF(sdf, is_sorted = TRUE, time_unit = &amp;quot;SECONDS&amp;quot;, time_column = &amp;quot;time&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;And finally, run the ‘sum’ summarizer to obtain a summation of &lt;code&gt;value&lt;/code&gt;s in all past-2-second time windows:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;result &amp;lt;- summarize_sum(ts_rdd, column = &amp;quot;value&amp;quot;, window = in_past(&amp;quot;2s&amp;quot;))

print(result %&amp;gt;% collect())&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;## # A tibble: 4 x 3
##   time                value value_sum
##   &amp;lt;dttm&amp;gt;              &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 1970-01-01 00:00:01     1         1
## 2 1970-01-01 00:00:02     4         5
## 3 1970-01-01 00:00:03     9        14
## 4 1970-01-01 00:00:04    16        29&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id="why-create-a-sparklyr-extension" class="section level1"&gt;
&lt;h1&gt;Why create a &lt;code&gt;sparklyr&lt;/code&gt; extension?&lt;/h1&gt;
&lt;p&gt;The alternative to making &lt;code&gt;sparklyr.flint&lt;/code&gt; a &lt;code&gt;sparklyr&lt;/code&gt; extension is to bundle all time-series functionalities it provides with &lt;code&gt;sparklyr&lt;/code&gt; itself. We decided that this would not be a good idea because of the following reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Not all &lt;code&gt;sparklyr&lt;/code&gt; users will need those time-series functionalities&lt;/li&gt;
&lt;li&gt;&lt;code&gt;com.twosigma:flint:0.6.0&lt;/code&gt; and all Maven packages it transitively relies on are quite heavy dependency-wise&lt;/li&gt;
&lt;li&gt;Implementing an intuitive R interface for &lt;code&gt;Flint&lt;/code&gt; also takes a non-trivial number of R source files, and making all of that part of &lt;code&gt;sparklyr&lt;/code&gt; itself would be too much&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, considering all of the above, building &lt;code&gt;sparklyr.flint&lt;/code&gt; as an extension of &lt;code&gt;sparklyr&lt;/code&gt; seems to be a much more reasonable choice.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="current-state-of-sparklyr.flint-and-its-future-directions" class="section level1"&gt;
&lt;h1&gt;Current state of &lt;code&gt;sparklyr.flint&lt;/code&gt; and its future directions&lt;/h1&gt;
&lt;p&gt;Recently &lt;code&gt;sparklyr.flint&lt;/code&gt; has had its first successful release on CRAN. At the moment, &lt;code&gt;sparklyr.flint&lt;/code&gt; only supports the &lt;code&gt;summarizeCycle&lt;/code&gt; and &lt;code&gt;summarizeWindow&lt;/code&gt; functionalities of &lt;code&gt;Flint&lt;/code&gt;, and does not yet support asof join and other useful time-series operations. While &lt;code&gt;sparklyr.flint&lt;/code&gt; contains R interfaces to most of the summarizers in &lt;code&gt;Flint&lt;/code&gt; (one can find the list of summarizers currently supported by &lt;code&gt;sparklyr.flint&lt;/code&gt; in &lt;a href="https://cran.r-project.org/web/packages/sparklyr.flint/sparklyr.flint.pdf"&gt;here&lt;/a&gt;), there are still a few of them missing (e.g., the support for &lt;code&gt;OLSRegressionSummarizer&lt;/code&gt;, among others).&lt;/p&gt;
&lt;p&gt;In general, the goal of building &lt;code&gt;sparklyr.flint&lt;/code&gt; is for it to be a thin “translation layer” between &lt;code&gt;sparklyr&lt;/code&gt; and &lt;code&gt;Flint&lt;/code&gt;. It should be as simple and intuitive as possibly can be, while supporting a rich set of &lt;code&gt;Flint&lt;/code&gt; time-series functionalities.&lt;/p&gt;
&lt;p&gt;We cordially welcome any open-source contribution towards &lt;code&gt;sparklyr.flint&lt;/code&gt;. Please visit &lt;a href="https://github.com/r-spark/sparklyr.flint/issues" class="uri"&gt;https://github.com/r-spark/sparklyr.flint/issues&lt;/a&gt; if you would like to initiate discussions, report bugs, or propose new features related to &lt;code&gt;sparklyr.flint&lt;/code&gt;, and &lt;a href="https://github.com/r-spark/sparklyr.flint/pulls" class="uri"&gt;https://github.com/r-spark/sparklyr.flint/pulls&lt;/a&gt; if you would like to send pull requests.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="acknowledgement" class="section level1"&gt;
&lt;h1&gt;Acknowledgement&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;First and foremost, the author wishes to thank Javier (&lt;a href="https://github.com/javierluraschi"&gt;@javierluraschi&lt;/a&gt;) for proposing the idea of creating &lt;code&gt;sparklyr.flint&lt;/code&gt; as the R interface for &lt;code&gt;Flint&lt;/code&gt;, and for his guidance on how to build it as an extension to &lt;code&gt;sparklyr&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Both Javier (&lt;a href="https://github.com/javierluraschi"&gt;@javierluraschi&lt;/a&gt;) and Daniel (&lt;a href="https://github.com/dfalbel"&gt;@dfalbel&lt;/a&gt;) have offered numerous helpful tips on making the initial submission of &lt;code&gt;sparklyr.flint&lt;/code&gt; to CRAN successful.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We really appreciate the enthusiasm from &lt;code&gt;sparklyr&lt;/code&gt; users who were willing to give &lt;code&gt;sparklyr.flint&lt;/code&gt; a try shortly after it was released on CRAN (and there were quite a few downloads of &lt;code&gt;sparklyr.flint&lt;/code&gt; in the past week according to CRAN stats, which was quite encouraging for us to see). We hope you enjoy using &lt;code&gt;sparklyr.flint&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The author is also grateful for valuable editorial suggestions from Mara (&lt;a href="https://github.com/batpigandme"&gt;@batpigandme&lt;/a&gt;), Sigrid (&lt;a href="https://github.com/skeydan"&gt;@skeydan&lt;/a&gt;), and Javier (&lt;a href="https://github.com/javierluraschi"&gt;@javierluraschi&lt;/a&gt;) on this blog post.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;/div&gt;




&lt;/div&gt;

&lt;script&gt;

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


&lt;/script&gt;

&lt;!-- tabsets --&gt;

&lt;script&gt;
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown &gt; .nav-tabs &gt; li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
&lt;/script&gt;

&lt;!-- code folding --&gt;


&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
&lt;/script&gt;

</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">8cb77d5ba37aa1a89decbcc5ef689257</distill:md5>
      <category>R</category>
      <category>Time Series</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-09-07-sparklyr-flint</guid>
      <pubDate>Mon, 07 Sep 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-09-07-sparklyr-flint/images/thumb.png" medium="image" type="image/png" width="126" height="77"/>
    </item>
    <item>
      <title>An introduction to weather forecasting with deep learning</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-09-01-weather-prediction</link>
      <description>A few weeks ago, we showed how to forecast chaotic dynamical systems with deep learning, augmented by a custom constraint derived from domain-specific insight. Global weather is a chaotic system, but of much higher complexity than many tasks commonly addressed with machine and/or deep learning. In this post, we provide a practical introduction featuring a simple deep learning baseline for atmospheric forecasting. While far away from being competitive, it serves to illustrate how more sophisticated and compute-intensive models may approach that formidable task by means of methods situated on the "black-box end" of the continuum.</description>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-09-01-weather-prediction</guid>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-09-01-weather-prediction/images/thumb.png" medium="image" type="image/png" width="1667" height="923"/>
    </item>
    <item>
      <title>Training ImageNet with R</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Javier Luraschi</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-08-24-training-imagenet-with-r</link>
      <description>This post explores how to train large datasets with TensorFlow and R. Specifically, we present how to download and repartition ImageNet, followed by training ImageNet across multiple GPUs in distributed environments using TensorFlow and Apache Spark.</description>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Distributed Computing</category>
      <category>Data Management</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-08-24-training-imagenet-with-r</guid>
      <pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-08-24-training-imagenet-with-r/images/fishing-net.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>FNN-VAE for noisy time series forecasting</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-07-31-fnn-vae-for-noisy-timeseries</link>
      <description>In the last part of this mini-series on forecasting with false nearest neighbors (FNN) loss, we replace the LSTM autoencoder from the previous post by a convolutional VAE, resulting in equivalent prediction performance but significantly lower training time. In addition, we find that FNN regularization is of great help when an underlying deterministic process is obscured by substantial noise.</description>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-07-31-fnn-vae-for-noisy-timeseries</guid>
      <pubDate>Fri, 31 Jul 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/kb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Time series prediction with FNN-LSTM</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm</link>
      <description>In a recent post, we showed how an LSTM autoencoder, regularized by false nearest neighbors (FNN) loss, can be used to reconstruct the attractor of a nonlinear, chaotic dynamical system. Here, we explore how that same technique assists in prediction. Matched up with a comparable, capacity-wise, "vanilla LSTM", FNN-LSTM improves performance on a set of very different, real-world datasets, especially for the initial steps in a multi-step forecast.</description>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm</guid>
      <pubDate>Mon, 20 Jul 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm/images/old_faithful.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Deep attractors: Where deep learning meets chaos</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors</link>
      <description>In nonlinear dynamics, when the state space is thought to be multidimensional but all we have for data is just a univariate time series, one may attempt to reconstruct the true space via delay coordinate embeddings. However, it is not clear a priori how to choose dimensionality and time lag of the reconstruction space. In this post, we show how to use an autoencoder architecture to circumvent the problem: Given just a scalar series of observations, the autoencoder directly learns to represent attractors of chaotic systems in adequate dimensionality.</description>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors</guid>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors/images/x_z.gif" medium="image" type="image/gif"/>
    </item>
    <item>
      <title>Easy PixelCNN with tfprobability</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn</link>
      <description>PixelCNN is a deep learning architecture - or bundle of architectures - designed to generate highly realistic-looking images. To use it, no reverse-engineering of arXiv papers or search for reference implementations is required: TensorFlow Probability and its R wrapper, tfprobability, now include a PixelCNN distribution that can be used to train a straightforwardly-defined neural network in a parameterizable way.</description>
      <category>R</category>
      <category>Image Recognition &amp; Image Processing</category>
      <category>TensorFlow/Keras</category>
      <category>Probabilistic ML/DL</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn</guid>
      <pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn/images/thumb.png" medium="image" type="image/png" width="400" height="203"/>
    </item>
    <item>
      <title>Hacking deep learning: model inversion attack by example</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks</link>
      <description>Compared to other applications, deep learning models might not seem too likely as victims of privacy attacks. However, methods exist to determine whether an entity was used in the training set (an adversarial attack called member inference), and techniques subsumed under "model inversion" allow to reconstruct raw data input given just model output (and sometimes, context information). This post shows an end-to-end example of model inversion, and explores mitigation strategies using TensorFlow Privacy.</description>
      <category>R</category>
      <category>Privacy &amp; Security</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks</guid>
      <pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks/images/results.png" medium="image" type="image/png" width="600" height="394"/>
    </item>
    <item>
      <title>Towards privacy: Encrypted deep learning with Syft and Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft</link>
      <description>Deep learning need not be irreconcilable with privacy protection. Federated learning enables on-device, distributed model training; encryption keeps model and gradient updates private; differential privacy prevents the training data from leaking. As of today, private and secure deep learning is an emerging technology. In this post, we introduce Syft, an open-source framework that integrates with PyTorch as well as TensorFlow. In an example use case, we obtain private predictions from a Keras model.</description>
      <category>R</category>
      <category>Privacy &amp; Security</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft</guid>
      <pubDate>Wed, 29 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft/images/thumb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>sparklyr 1.2: Foreach, Spark 3.0 and Databricks Connect</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yitao Li</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released</link>
      <description>A new sparklyr release is now available. This sparklyr 1.2 release features new functionalities such as support for Databricks Connect, a Spark backend for the 'foreach' package, inter-op improvements for working with Spark 3.0 preview, as well as a number of bug fixes and improvements addressing user-visible pain points.</description>
      <category>R</category>
      <category>Packages/Releases</category>
      <category>Distributed Computing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released</guid>
      <pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released/images/sparklyr.png" medium="image" type="image/png" width="1241" height="307"/>
    </item>
    <item>
      <title>pins 0.4: Versioning</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Javier Luraschi</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04</link>
      <description>A new release of pins is available on CRAN today. This release adds support to time travel across dataset versions, which improves collaboration and protects your code from breaking when remote resources change unexpectedly.</description>
      <category>R</category>
      <category>Packages/Releases</category>
      <category>Data Management</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04</guid>
      <pubDate>Mon, 13 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04/images/thumb.jpg" medium="image" type="image/jpeg"/>
    </item>
  </channel>
</rss>
