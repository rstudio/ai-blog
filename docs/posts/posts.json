[
  {
    "path": "posts/2020-10-09-torch-optim/",
    "title": "Optimizers in torch",
    "description": "Today, we wrap up our mini-series on torch basics, adding to our toolset two abstractions: loss functions and optimizers.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-10-09",
    "categories": [
      "Torch",
      "R"
    ],
    "contents": "\n\nContents\nLosses and loss functions\nOptimizers\nSimple network: final version\n\n\n\n\nThis is the fourth and last installment in a series introducing torch basics. Initially, we focused on tensors. To illustrate their power, we coded a complete (if toy-size) neural network from scratch. We didn’t make use of any of torch’s higher-level capabilities – not even autograd, its automatic-differentiation feature.\nThis changed in the follow-up post. No more thinking about derivatives and the chain rule; a single call to backward() did it all.\nIn the third post, the code again saw a major simplification. Instead of tediously assembling a DAG1 by hand, we let modules take care of the logic.\nBased on that last state, there are just two more things to do. For one, we still compute the loss by hand. And secondly, even though we get the gradients all nicely computed from autograd, we still loop over the model’s parameters, updating them all ourselves. You won’t be surprised to hear that none of this is necessary.\nLosses and loss functions\ntorch comes with all the usual loss functions, such as mean squared error, cross entropy, Kullback-Leibler divergence, and the like. In general, there are two usage modes.\nTake the example of calculating mean squared error. One way is to call nnf_mse_loss() directly on the prediction and ground truth tensors. For example:\n\n\nx <- torch_randn(c(3, 2, 3))\ny <- torch_zeros(c(3, 2, 3))\n\nnnf_mse_loss(x, y)\n\n\n\ntorch_tensor \n0.682362\n[ CPUFloatType{} ]\nOther loss functions designed to be called directly start with nnf_ as well: nnf_binary_cross_entropy(), nnf_nll_loss(), nnf_kl_div() … and so on.2\nThe second way is to define the algorithm in advance and call it at some later time. Here, respective constructors all start with nn_ and end in _loss. For example: nn_bce_loss(), nn_nll_loss(), nn_kl_div_loss() …3\n\n\nloss <- nn_mse_loss()\n\nloss(x, y)\n\n\n\ntorch_tensor \n0.682362\n[ CPUFloatType{} ]\nThis method may be preferable when one and the same algorithm should be applied to more than one pair of tensors.\nOptimizers\nSo far, we’ve been updating model parameters following a simple strategy: The gradients told us which direction on the loss curve was downward; the learning rate told us how big of a step to take. What we did was a straightforward implementation of gradient descent.\nHowever, optimization algorithms used in deep learning get a lot more sophisticated than that. Below, we’ll see how to replace our manual updates using optim_adam(), torch’s implementation of the Adam algorithm (Kingma and Ba 2017). First though, let’s take a quick look at how torch optimizers work.\nHere is a very simple network, consisting of just one linear layer, to be called on a single data point.\n\n\ndata <- torch_randn(1, 3)\n\nmodel <- nn_linear(3, 1)\nmodel$parameters\n\n\n\n$weight\ntorch_tensor \n-0.0385  0.1412 -0.5436\n[ CPUFloatType{1,3} ]\n\n$bias\ntorch_tensor \n-0.1950\n[ CPUFloatType{1} ]\nWhen we create an optimizer, we tell it what parameters it is supposed to work on.\n\n\noptimizer <- optim_adam(model$parameters, lr = 0.01)\noptimizer\n\n\n\n<optim_adam>\n  Inherits from: <torch_Optimizer>\n  Public:\n    add_param_group: function (param_group) \n    clone: function (deep = FALSE) \n    defaults: list\n    initialize: function (params, lr = 0.001, betas = c(0.9, 0.999), eps = 1e-08, \n    param_groups: list\n    state: list\n    step: function (closure = NULL) \n    zero_grad: function () \nAt any time, we can inspect those parameters:\n\n\noptimizer$param_groups[[1]]$params\n\n\n\n$weight\ntorch_tensor \n-0.0385  0.1412 -0.5436\n[ CPUFloatType{1,3} ]\n\n$bias\ntorch_tensor \n-0.1950\n[ CPUFloatType{1} ]\nNow we perform the forward and backward passes. The backward pass calculates the gradients, but does not update the parameters, as we can see both from the model and the optimizer objects:\n\n\nout <- model(data)\nout$backward()\n\noptimizer$param_groups[[1]]$params\nmodel$parameters\n\n\n\n$weight\ntorch_tensor \n-0.0385  0.1412 -0.5436\n[ CPUFloatType{1,3} ]\n\n$bias\ntorch_tensor \n-0.1950\n[ CPUFloatType{1} ]\n\n$weight\ntorch_tensor \n-0.0385  0.1412 -0.5436\n[ CPUFloatType{1,3} ]\n\n$bias\ntorch_tensor \n-0.1950\n[ CPUFloatType{1} ]\nCalling step() on the optimizer actually performs the updates. Again, let’s check that both model and optimizer now hold the updated values:\n\n\noptimizer$step()\n\noptimizer$param_groups[[1]]$params\nmodel$parameters\n\n\n\nNULL\n$weight\ntorch_tensor \n-0.0285  0.1312 -0.5536\n[ CPUFloatType{1,3} ]\n\n$bias\ntorch_tensor \n-0.2050\n[ CPUFloatType{1} ]\n\n$weight\ntorch_tensor \n-0.0285  0.1312 -0.5536\n[ CPUFloatType{1,3} ]\n\n$bias\ntorch_tensor \n-0.2050\n[ CPUFloatType{1} ]\nIf we perform optimization in a loop, we need to make sure to call optimizer$zero_grad() on every step, as otherwise gradients would be accumulated. You can see this in our final version of the network.\nSimple network: final version\n\n\nlibrary(torch)\n\n### generate training data -----------------------------------------------------\n\n# input dimensionality (number of input features)\nd_in <- 3\n# output dimensionality (number of predicted features)\nd_out <- 1\n# number of observations in training set\nn <- 100\n\n\n# create random data\nx <- torch_randn(n, d_in)\ny <- x[, 1, NULL] * 0.2 - x[, 2, NULL] * 1.3 - x[, 3, NULL] * 0.5 + torch_randn(n, 1)\n\n\n\n### define the network ---------------------------------------------------------\n\n# dimensionality of hidden layer\nd_hidden <- 32\n\nmodel <- nn_sequential(\n  nn_linear(d_in, d_hidden),\n  nn_relu(),\n  nn_linear(d_hidden, d_out)\n)\n\n### network parameters ---------------------------------------------------------\n\n# for adam, need to choose a much higher learning rate in this problem\nlearning_rate <- 0.08\n\noptimizer <- optim_adam(model$parameters, lr = learning_rate)\n\n### training loop --------------------------------------------------------------\n\nfor (t in 1:200) {\n  \n  ### -------- Forward pass -------- \n  \n  y_pred <- model(x)\n  \n  ### -------- compute loss -------- \n  loss <- nnf_mse_loss(y_pred, y, reduction = \"sum\")\n  if (t %% 10 == 0)\n    cat(\"Epoch: \", t, \"   Loss: \", loss$item(), \"\\n\")\n  \n  ### -------- Backpropagation -------- \n  \n  # Still need to zero out the gradients before the backward pass, only this time,\n  # on the optimizer object\n  optimizer$zero_grad()\n  \n  # gradients are still computed on the loss tensor (no change here)\n  loss$backward()\n  \n  ### -------- Update weights -------- \n  \n  # use the optimizer to update model parameters\n  optimizer$step()\n}\n\n\n\nAnd that’s it! We’ve seen all the major actors on stage: tensors, autograd, modules, loss functions, and optimizers. In future posts, we’ll explore how to use torch for standard deep learning tasks involving images, text, tabular data, and more. Thanks for reading!\n\n\n\nKingma, Diederik P., and Jimmy Ba. 2017. “Adam: A Method for Stochastic Optimization.” http://arxiv.org/abs/1412.6980.\n\n\ndirected acyclic graph↩︎\nThe prefix nnf_ was chosen because in PyTorch, the corresponding functions live in torch.nn.functional.↩︎\nThis time, the corresponding PyTorch module is torch.nn.↩︎\n",
    "preview": "posts/2020-10-09-torch-optim/images/preview.jpg",
    "last_modified": "2020-10-09T08:39:48+02:00",
    "input_file": "torch_network_with_optim.utf8.md"
  },
  {
    "path": "posts/2020-10-07-torch-modules/",
    "title": "Using torch modules",
    "description": "In this third installment of our mini-series introducing torch basics, we replace hand-coded matrix operations by modules, considerably simplifying our toy network's code.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-10-07",
    "categories": [
      "Torch",
      "R"
    ],
    "contents": "\n\nContents\nModules\nBase modules (“layers”)\nContainer modules (“models”)\n\nSimple network using modules\n\nInitially, we started learning about torch basics by coding a simple neural network from scratch, making use of just a single of torch’s features: tensors. Then, we immensely simplified the task, replacing manual backpropagation with autograd. Today, we modularize the network - in both the habitual and a very literal sense: Low-level matrix operations are swapped out for torch modules.\nModules\nFrom other frameworks (Keras, say), you may be used to distinguishing between models and layers. In torch, both are instances of nn_Module(), and thus, have some methods in common. For those thinking in terms of “models” and “layers”, I’m artificially splitting up this section into two parts. In reality though, there is no dichotomy: New modules may be composed of existing ones up to arbitrary levels of recursion.\nBase modules (“layers”)\nInstead of writing out an affine operation by hand – x$mm(w1) + b1, say –, as we’ve been doing so far, we can create a linear module. The following snippet instantiates a linear layer that expects three-feature inputs and returns a single output per observation:\n\n\nlibrary(torch)\nl <- nn_linear(3, 1)\n\n\n\nThe module has two parameters, “weight” and “bias”. Both now come pre-initialized:\n\n\nl$parameters\n\n\n\n$weight\ntorch_tensor \n-0.0385  0.1412 -0.5436\n[ CPUFloatType{1,3} ]\n\n$bias\ntorch_tensor \n-0.1950\n[ CPUFloatType{1} ]\nModules are callable; calling a module executes its forward() method, which, for a linear layer, matrix-multiplies input and weights, and adds the bias.\nLet’s try this:\n\n\ndata  <- torch_randn(10, 3)\nout <- l(data)\n\n\n\nUnsurprisingly, out now holds some data:\n\n\nout$data()\n\n\n\ntorch_tensor \n 0.2711\n-1.8151\n-0.0073\n 0.1876\n-0.0930\n 0.7498\n-0.2332\n-0.0428\n 0.3849\n-0.2618\n[ CPUFloatType{10,1} ]\nIn addition though, this tensor knows what will need to be done, should ever it be asked to calculate gradients:\n\n\nout$grad_fn\n\n\n\nAddmmBackward\nNote the difference between tensors returned by modules and self-created ones. When creating tensors ourselves, we need to pass requires_grad = TRUE to trigger gradient calculation. With modules, torch correctly assumes that we’ll want to perform backpropagation at some point.\nBy now though, we haven’t called backward() yet. Thus, no gradients have yet been computed:\n\n\nl$weight$grad\nl$bias$grad\n\n\n\ntorch_tensor \n[ Tensor (undefined) ]\ntorch_tensor \n[ Tensor (undefined) ]\nLet’s change this:\n\n\nout$backward()\n\n\n\nError in (function (self, gradient, keep_graph, create_graph)  : \n  grad can be implicitly created only for scalar outputs (_make_grads at ../torch/csrc/autograd/autograd.cpp:47)\nWhy the error? Autograd expects the output tensor to be a scalar, while in our example, we have a tensor of size (10, 1). This error won’t often occur in practice, where we work with batches of inputs (sometimes, just a single batch). But still, it’s interesting to see how to resolve this.\nTo make the example work, we introduce a – virtual – final aggregation step – taking the mean, say. Let’s call it avg. If such a mean were taken, its gradient with respect to l$weight would be obtained via the chain rule:\n\\[\n\\begin{equation*} \n \\frac{\\partial \\ avg}{\\partial w} = \\frac{\\partial \\ avg}{\\partial \\ out}  \\ \\frac{\\partial \\ out}{\\partial w}\n\\end{equation*}\n\\]\nOf the quantities on the right side, we’re interested in the second. We need to provide the first one, the way it would look if really we were taking the mean:\n\n\nd_avg_d_out <- torch_tensor(10)$`repeat`(10)$unsqueeze(1)$t()\nout$backward(gradient = d_avg_d_out)\n\n\n\nNow, l$weight$grad and l$bias$grad do contain gradients:\n\n\nl$weight$grad\nl$bias$grad\n\n\n\ntorch_tensor \n 1.3410  6.4343 -30.7135\n[ CPUFloatType{1,3} ]\ntorch_tensor \n 100\n[ CPUFloatType{1} ]\nIn addition to nn_linear() , torch provides pretty much all the common layers you might hope for. But few tasks are solved by a single layer. How do you combine them? Or, in the usual lingo: How do you build models?\nContainer modules (“models”)\nNow, models are just modules that contain other modules. For example, if all inputs are supposed to flow through the same nodes and along the same edges, then nn_sequential() can be used to build a simple graph.\nFor example:\n\n\nmodel <- nn_sequential(\n    nn_linear(3, 16),\n    nn_relu(),\n    nn_linear(16, 1)\n)\n\n\n\nWe can use the same technique as above to get an overview of all model parameters (two weight matrices and two bias vectors):\n\n\nmodel$parameters\n\n\n\n$`0.weight`\ntorch_tensor \n-0.1968 -0.1127 -0.0504\n 0.0083  0.3125  0.0013\n 0.4784 -0.2757  0.2535\n-0.0898 -0.4706 -0.0733\n-0.0654  0.5016  0.0242\n 0.4855 -0.3980 -0.3434\n-0.3609  0.1859 -0.4039\n 0.2851  0.2809 -0.3114\n-0.0542 -0.0754 -0.2252\n-0.3175  0.2107 -0.2954\n-0.3733  0.3931  0.3466\n 0.5616 -0.3793 -0.4872\n 0.0062  0.4168 -0.5580\n 0.3174 -0.4867  0.0904\n-0.0981 -0.0084  0.3580\n 0.3187 -0.2954 -0.5181\n[ CPUFloatType{16,3} ]\n\n$`0.bias`\ntorch_tensor \n-0.3714\n 0.5603\n-0.3791\n 0.4372\n-0.1793\n-0.3329\n 0.5588\n 0.1370\n 0.4467\n 0.2937\n 0.1436\n 0.1986\n 0.4967\n 0.1554\n-0.3219\n-0.0266\n[ CPUFloatType{16} ]\n\n$`2.weight`\ntorch_tensor \nColumns 1 to 10-0.0908 -0.1786  0.0812 -0.0414 -0.0251 -0.1961  0.2326  0.0943 -0.0246  0.0748\n\nColumns 11 to 16 0.2111 -0.1801 -0.0102 -0.0244  0.1223 -0.1958\n[ CPUFloatType{1,16} ]\n\n$`2.bias`\ntorch_tensor \n 0.2470\n[ CPUFloatType{1} ]\nTo inspect an individual parameter, make use of its position in the sequential model. For example:\n\n\nmodel[[1]]$bias\n\n\n\ntorch_tensor \n-0.3714\n 0.5603\n-0.3791\n 0.4372\n-0.1793\n-0.3329\n 0.5588\n 0.1370\n 0.4467\n 0.2937\n 0.1436\n 0.1986\n 0.4967\n 0.1554\n-0.3219\n-0.0266\n[ CPUFloatType{16} ]\nAnd just like nn_linear() above, this module can be called directly on data:\n\n\nout <- model(data)\n\n\n\nOn a composite module like this one, calling backward() will backpropagate through all the layers:\n\n\nout$backward(gradient = torch_tensor(10)$`repeat`(10)$unsqueeze(1)$t())\n\n# e.g.\nmodel[[1]]$bias$grad\n\n\n\ntorch_tensor \n  0.0000\n-17.8578\n  1.6246\n -3.7258\n -0.2515\n -5.8825\n 23.2624\n  8.4903\n -2.4604\n  6.7286\n 14.7760\n-14.4064\n -1.0206\n -1.7058\n  0.0000\n -9.7897\n[ CPUFloatType{16} ]\nAnd placing the composite module on the GPU will move all tensors there:\n\n\nmodel$cuda()\nmodel[[1]]$bias$grad\n\n\n\ntorch_tensor \n  0.0000\n-17.8578\n  1.6246\n -3.7258\n -0.2515\n -5.8825\n 23.2624\n  8.4903\n -2.4604\n  6.7286\n 14.7760\n-14.4064\n -1.0206\n -1.7058\n  0.0000\n -9.7897\n[ CUDAFloatType{16} ]\nNow let’s see how using nn_sequential() can simplify our example network.\nSimple network using modules\n\n\n### generate training data -----------------------------------------------------\n\n# input dimensionality (number of input features)\nd_in <- 3\n# output dimensionality (number of predicted features)\nd_out <- 1\n# number of observations in training set\nn <- 100\n\n\n# create random data\nx <- torch_randn(n, d_in)\ny <- x[, 1, NULL] * 0.2 - x[, 2, NULL] * 1.3 - x[, 3, NULL] * 0.5 + torch_randn(n, 1)\n\n\n### define the network ---------------------------------------------------------\n\n# dimensionality of hidden layer\nd_hidden <- 32\n\nmodel <- nn_sequential(\n  nn_linear(d_in, d_hidden),\n  nn_relu(),\n  nn_linear(d_hidden, d_out)\n)\n\n### network parameters ---------------------------------------------------------\n\nlearning_rate <- 1e-4\n\n### training loop --------------------------------------------------------------\n\nfor (t in 1:200) {\n  \n  ### -------- Forward pass -------- \n  \n  y_pred <- model(x)\n  \n  ### -------- compute loss -------- \n  loss <- (y_pred - y)$pow(2)$sum()\n  if (t %% 10 == 0)\n    cat(\"Epoch: \", t, \"   Loss: \", loss$item(), \"\\n\")\n  \n  ### -------- Backpropagation -------- \n  \n  # Zero the gradients before running the backward pass.\n  model$zero_grad()\n  \n  # compute gradient of the loss w.r.t. all learnable parameters of the model\n  loss$backward()\n  \n  ### -------- Update weights -------- \n  \n  # Wrap in with_no_grad() because this is a part we DON'T want to record\n  # for automatic gradient computation\n  # Update each parameter by its `grad`\n  \n  with_no_grad({\n    model$parameters %>% purrr::walk(function(param) param$sub_(learning_rate * param$grad))\n  })\n  \n}\n\n\n\nThe forward pass looks a lot better now; however, we still loop through the model’s parameters and update each one by hand. Furthermore, you may be already be suspecting that torch provides abstractions for common loss functions. In the next and last installment of this series, we’ll address both points, making use of torch losses and optimizers. See you then!\n\n\n\n",
    "preview": "posts/2020-10-07-torch-modules/images/preview.jpg",
    "last_modified": "2020-10-07T08:43:43+02:00",
    "input_file": "torch_modules.utf8.md"
  },
  {
    "path": "posts/2020-10-05-torch-network-with-autograd/",
    "title": "Introducing torch autograd",
    "description": "With torch, there is hardly ever a reason to code backpropagation from scratch. Its automatic differentiation feature, called autograd, keeps track of operations that need their gradients computed, as well as how to compute them. In this second post of a four-part series, we update our simple, hand-coded network to make use of autograd.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-10-05",
    "categories": [
      "Torch",
      "R"
    ],
    "contents": "\n\nContents\nAutomatic differentiation with autograd\nThe simple network, now using autograd\nOutlook\n\nLast week, we saw how to code a simple network from scratch, using nothing but torch tensors. Predictions, loss, gradients, weight updates – all these things we’ve been computing ourselves. Today, we make a significant change: Namely, we spare ourselves the cumbersome calculation of gradients, and have torch do it for us.\nPrior to that though, let’s get some background.\nAutomatic differentiation with autograd\ntorch uses a module called autograd to\nrecord operations performed on tensors, and\nstore what will have to be done to obtain the corresponding gradients, once we’re entering the backward pass.\nThese prospective actions are stored internally as functions, and when it’s time to compute the gradients, these functions are applied in order: Application starts from the output node, and calculated gradients are successively propagated back through the network. This is a form of reverse mode automatic differentiation.\nAutograd basics\nAs users, we can see a bit of the implementation. As a prerequisite for this “recording” to happen, tensors have to be created with requires_grad = TRUE. For example:\n\n\nlibrary(torch)\n\nx <- torch_ones(2, 2, requires_grad = TRUE)\n\n\n\nTo be clear, x now is a tensor with respect to which gradients have to be calculated – normally, a tensor representing a weight or a bias, not the input data 1. If we subsequently perform some operation on that tensor, assigning the result to y,\n\n\ny <- x$mean()\n\n\n\nwe find that y now has a non-empty grad_fn that tells torch how to compute the gradient of y with respect to x:\n\n\ny$grad_fn\n\n\n\nMeanBackward0\nActual computation of gradients is triggered by calling backward() on the output tensor.\n\n\ny$backward()\n\n\n\nAfter backward() has been called, x has a non-null field termed grad that stores the gradient of y with respect to x:\n\n\nx$grad\n\n\n\ntorch_tensor \n 0.2500  0.2500\n 0.2500  0.2500\n[ CPUFloatType{2,2} ]\nWith longer chains of computations, we can take a glance at how torch builds up a graph of backward operations. Here is a slightly more complex example – feel free to skip if you’re not the type who just has to peek into things for them to make sense.\nDigging deeper\nWe build up a simple graph of tensors, with inputs x1 and x2 being connected to output out by intermediaries y and z.\n\n\nx1 <- torch_ones(2, 2, requires_grad = TRUE)\nx2 <- torch_tensor(1.1, requires_grad = TRUE)\n\ny <- x1 * (x2 + 2)\n\nz <- y$pow(2) * 3\n\nout <- z$mean()\n\n\n\nTo save memory, intermediate gradients are normally not being stored. Calling retain_grad() on a tensor allows one to deviate from this default. Let’s do this here, for the sake of demonstration:\n\n\ny$retain_grad()\n\nz$retain_grad()\n\n\n\nNow we can go backwards through the graph and inspect torch’s action plan for backprop, starting from out$grad_fn, like so:\n\n\n# how to compute the gradient for mean, the last operation executed\nout$grad_fn\n\n\n\nMeanBackward0\n\n\n# how to compute the gradient for the multiplication by 3 in z = y.pow(2) * 3\nout$grad_fn$next_functions\n\n\n\n[[1]]\nMulBackward1\n\n\n# how to compute the gradient for pow in z = y.pow(2) * 3\nout$grad_fn$next_functions[[1]]$next_functions\n\n\n\n[[1]]\nPowBackward0\n\n\n# how to compute the gradient for the multiplication in y = x * (x + 2)\nout$grad_fn$next_functions[[1]]$next_functions[[1]]$next_functions\n\n\n\n[[1]]\nMulBackward0\n\n\n# how to compute the gradient for the two branches of y = x * (x + 2),\n# where the left branch is a leaf node (AccumulateGrad for x1)\nout$grad_fn$next_functions[[1]]$next_functions[[1]]$next_functions[[1]]$next_functions\n\n\n\n[[1]]\ntorch::autograd::AccumulateGrad\n[[2]]\nAddBackward1\n\n\n# here we arrive at the other leaf node (AccumulateGrad for x2)\nout$grad_fn$next_functions[[1]]$next_functions[[1]]$next_functions[[1]]$next_functions[[2]]$next_functions\n\n\n\n[[1]]\ntorch::autograd::AccumulateGrad\nIf we now call out$backward(), all tensors in the graph will have their respective gradients calculated.\n\n\nout$backward()\n\nz$grad\ny$grad\nx2$grad\nx1$grad\n\n\n\ntorch_tensor \n 0.2500  0.2500\n 0.2500  0.2500\n[ CPUFloatType{2,2} ]\ntorch_tensor \n 4.6500  4.6500\n 4.6500  4.6500\n[ CPUFloatType{2,2} ]\ntorch_tensor \n 18.6000\n[ CPUFloatType{1} ]\ntorch_tensor \n 14.4150  14.4150\n 14.4150  14.4150\n[ CPUFloatType{2,2} ]\nAfter this nerdy excursion, let’s see how autograd makes our network simpler.\nThe simple network, now using autograd\nThanks to autograd, we say good-bye to the tedious, error-prone process of coding backpropagation ourselves. A single method call does it all: loss$backward().\nWith torch keeping track of operations as required, we don’t even have to explicitly name the intermediate tensors any more. We can code forward pass, loss calculation, and backward pass in just three lines:\n\n\ny_pred <- x$mm(w1)$add(b1)$clamp(min = 0)$mm(w2)$add(b2)\n  \nloss <- (y_pred - y)$pow(2)$sum()\n\nloss$backward()\n\n\n\nHere is the complete code. We’re at an intermediate stage: We still manually compute the forward pass and the loss, and we still manually update the weights. Due to the latter, there is something I need to explain. But I’ll let you check out the new version first:\n\n\nlibrary(torch)\n\n### generate training data -----------------------------------------------------\n\n# input dimensionality (number of input features)\nd_in <- 3\n# output dimensionality (number of predicted features)\nd_out <- 1\n# number of observations in training set\nn <- 100\n\n\n# create random data\nx <- torch_randn(n, d_in)\ny <- x[, 1, NULL] * 0.2 - x[, 2, NULL] * 1.3 - x[, 3, NULL] * 0.5 + torch_randn(n, 1)\n\n\n### initialize weights ---------------------------------------------------------\n\n# dimensionality of hidden layer\nd_hidden <- 32\n# weights connecting input to hidden layer\nw1 <- torch_randn(d_in, d_hidden, requires_grad = TRUE)\n# weights connecting hidden to output layer\nw2 <- torch_randn(d_hidden, d_out, requires_grad = TRUE)\n\n# hidden layer bias\nb1 <- torch_zeros(1, d_hidden, requires_grad = TRUE)\n# output layer bias\nb2 <- torch_zeros(1, d_out, requires_grad = TRUE)\n\n### network parameters ---------------------------------------------------------\n\nlearning_rate <- 1e-4\n\n### training loop --------------------------------------------------------------\n\nfor (t in 1:200) {\n  ### -------- Forward pass --------\n  \n  y_pred <- x$mm(w1)$add(b1)$clamp(min = 0)$mm(w2)$add(b2)\n  \n  ### -------- compute loss -------- \n  loss <- (y_pred - y)$pow(2)$sum()\n  if (t %% 10 == 0)\n    cat(\"Epoch: \", t, \"   Loss: \", loss$item(), \"\\n\")\n  \n  ### -------- Backpropagation --------\n  \n  # compute gradient of loss w.r.t. all tensors with requires_grad = TRUE\n  loss$backward()\n  \n  ### -------- Update weights -------- \n  \n  # Wrap in with_no_grad() because this is a part we DON'T \n  # want to record for automatic gradient computation\n   with_no_grad({\n     w1 <- w1$sub_(learning_rate * w1$grad)\n     w2 <- w2$sub_(learning_rate * w2$grad)\n     b1 <- b1$sub_(learning_rate * b1$grad)\n     b2 <- b2$sub_(learning_rate * b2$grad)  \n     \n     # Zero gradients after every pass, as they'd accumulate otherwise\n     w1$grad$zero_()\n     w2$grad$zero_()\n     b1$grad$zero_()\n     b2$grad$zero_()  \n   })\n\n}\n\n\n\nAs explained above, after some_tensor$backward(), all tensors preceding it in the graph2 will have their grad fields populated. We make use of these fields to update the weights. But now that autograd is “on”, whenever we execute an operation we don’t want recorded for backprop, we need to explicitly exempt it: This is why we wrap the weight updates in a call to with_no_grad().\nWhile this is something you may file under “nice to know” – after all, once we arrive at the last post in the series, this manual updating of weights will be gone – the idiom of zeroing gradients is here to stay: Values stored in grad fields accumulate; whenever we’re done using them, we need to zero them out before reuse.\nOutlook\nSo where do we stand? We started out coding a network completely from scratch, making use of nothing but torch tensors. Today, we got significant help from autograd.\nBut we’re still manually updating the weights, – and aren’t deep learning frameworks known to provide abstractions (“layers”, or: “modules”) on top of tensor computations …?\nWe address both issues in the follow-up installments. Thanks for reading!\n\nUnless we want to change the data, as when generating adversarial examples.↩︎\nAll that have requires_grad set to TRUE, to be precise.↩︎\n",
    "preview": "posts/2020-10-05-torch-network-with-autograd/images/preview.jpg",
    "last_modified": "2020-10-05T08:57:05+02:00",
    "input_file": "torch-network-with-autograd.utf8.md"
  },
  {
    "path": "posts/2020-10-01-torch-network-from-scratch/",
    "title": "Getting familiar with torch tensors",
    "description": "In this first installment of a four-part miniseries, we present the main things you will want to know about torch tensors. As an illustrative example, we'll code a simple neural network from scratch.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-10-01",
    "categories": [
      "Torch",
      "R"
    ],
    "contents": "\n\nContents\nTensors\nCreation\nConversion to built-in R data types\nIndexing and slicing tensors\nReshaping tensors\nOperations on tensors\n\nRunning on GPU\nBroadcasting\nA simple neural network using torch tensors\n\nTwo days ago, I introduced torch, an R package that provides the native functionality that is brought to Python users by PyTorch. In that post, I assumed basic familiarity with TensorFlow/Keras. Consequently, I portrayed torch in a way I figured would be helpful to someone who “grew up” with the Keras way of training a model: Aiming to focus on differences, yet not lose sight of the overall process.\nThis post now changes perspective. We code a simple neural network “from scratch”, making use of just one of torch’s building blocks: tensors. This network will be as “raw” (low-level) as can be. (For the less math-inclined people among us, it may serve as a refresher of what’s actually going on beneath all those convenience tools they built for us. But the real purpose is to illustrate what can be done with tensors alone.)\nSubsequently, three posts will progressively show how to reduce the effort – noticeably right from the start, enormously once we finish. At the end of this mini-series, you will have seen how automatic differentiation works in torch, how to use modules (layers, in keras speak, and compositions thereof), and optimizers. By then, you’ll have a lot of the background desirable when applying torch to real-world tasks.\nThis post will be the longest, since there is a lot to learn about tensors: How to create them; how to manipulate their contents and/or modify their shapes; how to convert them to R arrays, matrices or vectors; and of course, given the omnipresent need for speed: how to get all those operations executed on the GPU. Once we’ve cleared that agenda, we code the aforementioned little network, seeing all those aspects in action.\nTensors\nCreation\nTensors may be created by specifying individual values. Here we create two one-dimensional tensors (vectors), of types float and bool, respectively:\n\n\nlibrary(torch)\n# a 1d vector of length 2\nt <- torch_tensor(c(1, 2))\nt\n\n# also 1d, but of type boolean\nt <- torch_tensor(c(TRUE, FALSE))\nt\n\n\n\ntorch_tensor \n 1\n 2\n[ CPUFloatType{2} ]\n\ntorch_tensor \n 1\n 0\n[ CPUBoolType{2} ]\nAnd here are two ways to create two-dimensional tensors (matrices). Note how in the second approach, you need to specify byrow = TRUE in the call to matrix() to get values arranged in row-major order.\n\n\n# a 3x3 tensor (matrix)\nt <- torch_tensor(rbind(c(1,2,0), c(3,0,0), c(4,5,6)))\nt\n\n# also 3x3\nt <- torch_tensor(matrix(1:9, ncol = 3, byrow = TRUE))\nt\n\n\n\ntorch_tensor \n 1  2  0\n 3  0  0\n 4  5  6\n[ CPUFloatType{3,3} ]\n\ntorch_tensor \n 1  2  3\n 4  5  6\n 7  8  9\n[ CPULongType{3,3} ]\nIn higher dimensions especially, it can be easier to specify the type of tensor abstractly, as in: “give me a tensor of <…> of shape n1 x n2”, where <…> could be “zeros”; or “ones”; or, say, “values drawn from a standard normal distribution”:\n\n\n# a 3x3 tensor of standard-normally distributed values\nt <- torch_randn(3, 3)\nt\n\n# a 4x2x2 (3d) tensor of zeroes\nt <- torch_zeros(4, 2, 2)\nt\n\n\n\ntorch_tensor \n-2.1563  1.7085  0.5245\n 0.8955 -0.6854  0.2418\n 0.4193 -0.7742 -1.0399\n[ CPUFloatType{3,3} ]\n\ntorch_tensor \n(1,.,.) = \n  0  0\n  0  0\n\n(2,.,.) = \n  0  0\n  0  0\n\n(3,.,.) = \n  0  0\n  0  0\n\n(4,.,.) = \n  0  0\n  0  0\n[ CPUFloatType{4,2,2} ]\nMany similar functions exist, including, e.g., torch_arange() to create a tensor holding a sequence of evenly spaced values, torch_eye() which returns an identity matrix, and torch_logspace() which fills a specified range with a list of values spaced logarithmically.\nIf no dtype argument is specified, torch will infer the data type from the passed-in value(s). For example:\n\n\nt <- torch_tensor(c(3, 5, 7))\nt$dtype\n\nt <- torch_tensor(1L)\nt$dtype\n\n\n\ntorch_Float\ntorch_Long\nBut we can explicitly request a different dtype if we want:\n\n\nt <- torch_tensor(2, dtype = torch_double())\nt$dtype\n\n\n\ntorch_Double\ntorch tensors live on a device. By default, this will be the CPU:\n\n\nt$device\n\n\n\ntorch_device(type='cpu')\nBut we could also define a tensor to live on the GPU:\n\n\nt <- torch_tensor(2, device = \"cuda\")\nt$device\n\n\n\ntorch_device(type='cuda', index=0)\nWe’ll talk more about devices below.\nThere is another very important parameter to the tensor-creation functions: requires_grad. Here though, I need to ask for your patience: This one will prominently figure in the follow-up post.\nConversion to built-in R data types\nTo convert torch tensors to R, use as_array():\n\n\nt <- torch_tensor(matrix(1:9, ncol = 3, byrow = TRUE))\nas_array(t)\n\n\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\nDepending on whether the tensor is one-, two-, or three-dimensional, the resulting R object will be a vector, a matrix, or an array:\n\n\nt <- torch_tensor(c(1, 2, 3))\nas_array(t) %>% class()\n\nt <- torch_ones(c(2, 2))\nas_array(t) %>% class()\n\nt <- torch_ones(c(2, 2, 2))\nas_array(t) %>% class()\n\n\n\n[1] \"numeric\"\n\n[1] \"matrix\" \"array\" \n\n[1] \"array\"\nFor one-dimensional and two-dimensional tensors, it is also possible to use as.integer() / as.matrix(). (One reason you might want to do this is to have more self-documenting code.)\nIf a tensor currently lives on the GPU, you need to move it to the CPU first:\n\n\nt <- torch_tensor(2, device = \"cuda\")\nas.integer(t$cpu())\n\n\n\n[1] 2\nIndexing and slicing tensors\nOften, we want to retrieve not a complete tensor, but only some of the values it holds, or even just a single value. In these cases, we talk about slicing and indexing, respectively.\nIn R, these operations are 1-based, meaning that when we specify offsets, we assume for the very first element in an array to reside at offset 1. The same behavior was implemented for torch. Thus, a lot of the functionality described in this section should feel intuitive.\nThe way I’m organizing this section is the following. We’ll inspect the intuitive parts first, where by intuitive I mean: intuitive to the R user who has not yet worked with Python’s NumPy. Then come things which, to this user, may look more surprising, but will turn out to be pretty useful.\nIndexing and slicing: the R-like part\nNone of these should be overly surprising:\n\n\nt <- torch_tensor(rbind(c(1,2,3), c(4,5,6)))\nt\n\n# a single value\nt[1, 1]\n\n# first row, all columns\nt[1, ]\n\n# first row, a subset of columns\nt[1, 1:2]\n\n\n\ntorch_tensor \n 1  2  3\n 4  5  6\n[ CPUFloatType{2,3} ]\n\ntorch_tensor \n1\n[ CPUFloatType{} ]\n\ntorch_tensor \n 1\n 2\n 3\n[ CPUFloatType{3} ]\n\ntorch_tensor \n 1\n 2\n[ CPUFloatType{2} ]\nNote how, just as in R, singleton dimensions are dropped:\n\n\nt <- torch_tensor(rbind(c(1,2,3), c(4,5,6)))\n\n# 2x3\nt$size() \n\n# just a single row: will be returned as a vector\nt[1, 1:2]$size() \n\n# a single element\nt[1, 1]$size()\n\n\n\n[1] 2 3\n\n[1] 2\n\ninteger(0)\nAnd just like in R, you can specify drop = FALSE to keep those dimensions:\n\n\nt[1, 1:2, drop = FALSE]$size()\n\nt[1, 1, drop = FALSE]$size()\n\n\n\n[1] 1 2\n\n[1] 1 1\nIndexing and slicing: What to look out for\nWhereas R uses negative numbers to remove elements at specified positions, in torch negative values indicate that we start counting from the end of a tensor – with -1 pointing to its last element:\n\n\nt <- torch_tensor(rbind(c(1,2,3), c(4,5,6)))\n\nt[1, -1]\n\nt[ , -2:-1] \n\n\n\ntorch_tensor \n3\n[ CPUFloatType{} ]\n\ntorch_tensor \n 2  3\n 5  6\n[ CPUFloatType{2,2} ]\nThis is a feature you might know from NumPy. Same with the following.\nWhen the slicing expression m:n is augmented by another colon and a third number – m:n:o –, we will take every oth item from the range specified by m and n:\n\n\nt <- torch_tensor(1:10)\nt[2:10:2]\n\n\n\ntorch_tensor \n  2\n  4\n  6\n  8\n 10\n[ CPULongType{5} ]\nSometimes we don’t know how many dimensions a tensor has, but we do know what to do with the final dimension, or the first one. To subsume all others, we can use ..:\n\n\nt <- torch_randint(-7, 7, size = c(2, 2, 2))\nt\n\nt[.., 1]\n\nt[2, ..]\n\n\n\ntorch_tensor \n(1,.,.) = \n  2 -2\n -5  4\n\n(2,.,.) = \n  0  4\n -3 -1\n[ CPUFloatType{2,2,2} ]\n\ntorch_tensor \n 2 -5\n 0 -3\n[ CPUFloatType{2,2} ]\n\ntorch_tensor \n 0  4\n-3 -1\n[ CPUFloatType{2,2} ]\nNow we move on to a topic that, in practice, is just as indispensable as slicing: changing tensor shapes.\nReshaping tensors\nChanges in shape can occur in two fundamentally different ways. Seeing how “reshape” really means: keep the values but modify their layout, we could either alter how they’re arranged physically, or keep the physical structure as-is and just change the “mapping” (a semantic change, as it were).\nIn the first case, storage will have to be allocated for two tensors, source and target, and elements will be copied from the latter to the former. In the second, physically there will be just a single tensor, referenced by two logical entities with distinct metadata.\nNot surprisingly, for performance reasons, the second operation is preferred.\nZero-copy reshaping\nWe start with zero-copy methods, as we’ll want to use them whenever we can.\nA special case often seen in practice is adding or removing a singleton dimension.\nunsqueeze() adds a dimension of size 1 at a position specified by dim:\n\n\nt1 <- torch_randint(low = 3, high = 7, size = c(3, 3, 3))\nt1$size()\n\nt2 <- t1$unsqueeze(dim = 1)\nt2$size()\n\nt3 <- t1$unsqueeze(dim = 2)\nt3$size()\n\n\n\n[1] 3 3 3\n\n[1] 1 3 3 3\n\n[1] 3 1 3 3\nConversely, squeeze() removes singleton dimensions:\n\n\nt4 <- t3$squeeze()\nt4$size()\n\n\n\n[1] 3 3 3\nThe same could be accomplished with view(). view(), however, is much more general, in that it allows you to reshape the data to any valid dimensionality. (Valid meaning: The number of elements stays the same.)\nHere we have a 3x2 tensor that is reshaped to size 2x3:\n\n\nt1 <- torch_tensor(rbind(c(1, 2), c(3, 4), c(5, 6)))\nt1\n\nt2 <- t1$view(c(2, 3))\nt2\n\n\n\ntorch_tensor \n 1  2\n 3  4\n 5  6\n[ CPUFloatType{3,2} ]\n\ntorch_tensor \n 1  2  3\n 4  5  6\n[ CPUFloatType{2,3} ]\n(Note how this is different from matrix transposition.)\nInstead of going from two to three dimensions, we can flatten the matrix to a vector.\n\n\nt4 <- t1$view(c(-1, 6))\n\nt4$size()\n\nt4\n\n\n\n[1] 1 6\n\ntorch_tensor \n 1  2  3  4  5  6\n[ CPUFloatType{1,6} ]\nIn contrast to indexing operations, this does not drop dimensions.\nLike we said above, operations like squeeze() or view() do not make copies. Or, put differently: The output tensor shares storage with the input tensor. We can in fact verify this ourselves:\n\n\nt1$storage()$data_ptr()\n\nt2$storage()$data_ptr()\n\n\n\n[1] \"0x5648d02ac800\"\n\n[1] \"0x5648d02ac800\"\nWhat’s different is the storage metadata torch keeps about both tensors. Here, the relevant information is the stride:\nA tensor’s stride() method tracks, for every dimension, how many elements have to be traversed to arrive at its next element (row or column, in two dimensions). For t1 above, of shape 3x2, we have to skip over 2 items to arrive at the next row. To arrive at the next column though, in every row we just have to skip a single entry:\n\n\nt1$stride()\n\n\n\n[1] 2 1\nFor t2, of shape 3x2, the distance between column elements is the same, but the distance between rows is now 3:\n\n\nt2$stride()\n\n\n\n[1] 3 1\nWhile zero-copy operations are optimal, there are cases where they won’t work.\nWith view(), this can happen when a tensor was obtained via an operation – other than view() itself – that itself has already modified the stride. One example would be transpose():\n\n\nt1 <- torch_tensor(rbind(c(1, 2), c(3, 4), c(5, 6)))\nt1\nt1$stride()\n\nt2 <- t1$t()\nt2\nt2$stride()\n\n\n\ntorch_tensor \n 1  2\n 3  4\n 5  6\n[ CPUFloatType{3,2} ]\n\n[1] 2 1\n\ntorch_tensor \n 1  3  5\n 2  4  6\n[ CPUFloatType{2,3} ]\n\n[1] 1 2\nIn torch lingo, tensors – like t2 – that re-use existing storage (and just read it differently), are said not to be “contiguous”1. One way to reshape them is to use contiguous() on them before. We’ll see this in the next subsection.\nReshape with copy\nIn the following snippet, trying to reshape t2 using view() fails, as it already carries information indicating that the underlying data should not be read in physical order.\n\n\nt1 <- torch_tensor(rbind(c(1, 2), c(3, 4), c(5, 6)))\n\nt2 <- t1$t()\n\nt2$view(6) # error!\n\n\n\nError in (function (self, size)  : \n  view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces).\n  Use .reshape(...) instead. (view at ../aten/src/ATen/native/TensorShape.cpp:1364)\nHowever, if we first call contiguous() on it, a new tensor is created, which may then be (virtually) reshaped using view().2\n\n\nt3 <- t2$contiguous()\n\nt3$view(6)\n\n\n\ntorch_tensor \n 1\n 3\n 5\n 2\n 4\n 6\n[ CPUFloatType{6} ]\nAlternatively, we can use reshape(). reshape() defaults to view()-like behavior if possible; otherwise it will create a physical copy.\n\n\nt2$storage()$data_ptr()\n\nt4 <- t2$reshape(6)\n\nt4$storage()$data_ptr()\n\n\n\n[1] \"0x5648d49b4f40\"\n\n[1] \"0x5648d2752980\"\nOperations on tensors\nUnsurprisingly, torch provides a bunch of mathematical operations on tensors; we’ll see some of them in the network code below, and you’ll encounter lots more when you continue your torch journey. Here, we quickly take a look at the overall tensor method semantics.\nTensor methods normally return references to new objects. Here, we add to t1 a clone of itself:\n\n\nt1 <- torch_tensor(rbind(c(1, 2), c(3, 4), c(5, 6)))\nt2 <- t1$clone()\n\nt1$add(t2)\n\n\n\ntorch_tensor \n  2   4\n  6   8\n 10  12\n[ CPUFloatType{3,2} ]\nIn this process, t1 has not been modified:\n\n\nt1\n\n\n\ntorch_tensor \n 1  2\n 3  4\n 5  6\n[ CPUFloatType{3,2} ]\nMany tensor methods have variants for mutating operations. These all carry a trailing underscore:\n\n\nt1$add_(t1)\n\n# now t1 has been modified\nt1\n\n\n\ntorch_tensor \n  4   8\n 12  16\n 20  24\n[ CPUFloatType{3,2} ]\n\ntorch_tensor \n  4   8\n 12  16\n 20  24\n[ CPUFloatType{3,2} ]\nAlternatively, you can of course assign the new object to a new reference variable:\n\n\nt3 <- t1$add(t1)\n\nt3\n\n\n\ntorch_tensor \n  8  16\n 24  32\n 40  48\n[ CPUFloatType{3,2} ]\nThere is one thing we need to discuss before we wrap up our introduction to tensors: How can we have all those operations executed on the GPU?\nRunning on GPU\nTo check if your GPU(s) is/are visible to torch, run\n\n\ncuda_is_available()\n\ncuda_device_count()\n\n\n\n[1] TRUE\n\n[1] 1\nTensors may be requested to live on the GPU right at creation:\n\n\ndevice <- torch_device(\"cuda\")\n\nt <- torch_ones(c(2, 2), device = device) \n\n\n\nAlternatively, they can be moved between devices at any time:\n\n\nt2 <- t$cuda()\nt2$device\n\n\n\ntorch_device(type='cuda', index=0)\n\n\nt3 <- t2$cpu()\nt3$device\n\n\n\ntorch_device(type='cpu')\nThat’s it for our discussion on tensors — almost. There is one torch feature that, although related to tensor operations, deserves special mention. It is called broadcasting, and “bilingual” (R + Python) users will know it from NumPy.\nBroadcasting\nWe often have to perform operations on tensors with shapes that don’t match exactly.\nUnsurprisingly, we can add a scalar to a tensor:\n\n\nt1 <- torch_randn(c(3,5))\n\nt1 + 22\n\n\n\ntorch_tensor \n 23.1097  21.4425  22.7732  22.2973  21.4128\n 22.6936  21.8829  21.1463  21.6781  21.0827\n 22.5672  21.2210  21.2344  23.1154  20.5004\n[ CPUFloatType{3,5} ]\nThe same will work if we add tensor of size 1:\n\n\nt1 <- torch_randn(c(3,5))\n\nt1 + torch_tensor(c(22))\n\n\n\nAdding tensors of different sizes normally won’t work:\n\n\nt1 <- torch_randn(c(3,5))\nt2 <- torch_randn(c(5,5))\n\nt1$add(t2) # error\n\n\n\nError in (function (self, other, alpha)  : \n  The size of tensor a (2) must match the size of tensor b (5) at non-singleton dimension 1 (infer_size at ../aten/src/ATen/ExpandUtils.cpp:24)\nHowever, under certain conditions, one or both tensors may be virtually expanded so both tensors line up. This behavior is what is meant by broadcasting. The way it works in torch is not just inspired by, but actually identical to that of NumPy.\nThe rules are:\nWe align array shapes, starting from the right.\nSay we have two tensors, one of size 8x1x6x1, the other of size 7x1x5.\nHere they are, right-aligned:\n# t1, shape:     8  1  6  1\n# t2, shape:        7  1  5\nStarting to look from the right, the sizes along aligned axes either have to match exactly, or one of them has to be equal to 1: in which case the latter is broadcast to the larger one.\nIn the above example, this is the case for the second-from-last dimension. This now gives\n# t1, shape:     8  1  6  1\n# t2, shape:        7  6  5\n, with broadcasting happening in t2.\nIf on the left, one of the arrays has an additional axis (or more than one), the other is virtually expanded to have a size of 1 in that place, in which case broadcasting will happen as stated in (2).\nThis is the case with t1’s leftmost dimension. First, there is a virtual expansion\n# t1, shape:     8  1  6  1\n# t2, shape:     1  7  1  5\nand then, broadcasting happens:\n# t1, shape:     8  1  6  1\n# t2, shape:     8  7  1  5\nAccording to these rules, our above example\n\n\nt1 <- torch_randn(c(3,5))\nt2 <- torch_randn(c(5,5))\n\nt1$add(t2)\n\n\n\ncould be modified in various ways that would allow for adding two tensors.\nFor example, if t2 were 1x5, it would only need to get broadcast to size 3x5 before the addition operation:\n\n\nt1 <- torch_randn(c(3,5))\nt2 <- torch_randn(c(1,5))\n\nt1$add(t2)\n\n\n\ntorch_tensor \n-1.0505  1.5811  1.1956 -0.0445  0.5373\n 0.0779  2.4273  2.1518 -0.6136  2.6295\n 0.1386 -0.6107 -1.2527 -1.3256 -0.1009\n[ CPUFloatType{3,5} ]\nIf it were of size 5, a virtual leading dimension would be added, and then, the same broadcasting would take place as in the previous case.\n\n\nt1 <- torch_randn(c(3,5))\nt2 <- torch_randn(c(5))\n\nt1$add(t2)\n\n\n\ntorch_tensor \n-1.4123  2.1392 -0.9891  1.1636 -1.4960\n 0.8147  1.0368 -2.6144  0.6075 -2.0776\n-2.3502  1.4165  0.4651 -0.8816 -1.0685\n[ CPUFloatType{3,5} ]\nHere is a more complex example. Broadcasting how happens both in t1 and in t2:\n\n\nt1 <- torch_randn(c(1,5))\nt2 <- torch_randn(c(3,1))\n\nt1$add(t2)\n\n\n\ntorch_tensor \n 1.2274  1.1880  0.8531  1.8511 -0.0627\n 0.2639  0.2246 -0.1103  0.8877 -1.0262\n-1.5951 -1.6344 -1.9693 -0.9713 -2.8852\n[ CPUFloatType{3,5} ]\nAs a nice concluding example, through broadcasting an outer product can be computed like so:\n\n\nt1 <- torch_tensor(c(0, 10, 20, 30))\n\nt2 <- torch_tensor(c(1, 2, 3))\n\nt1$view(c(4,1)) * t2\n\n\n\ntorch_tensor \n  0   0   0\n 10  20  30\n 20  40  60\n 30  60  90\n[ CPUFloatType{4,3} ]\nAnd now, we really get to implementing that neural network!\nA simple neural network using torch tensors\nOur task, which we approach in a low-level way today but considerably simplify in upcoming installments, consists of regressing a single target datum based on three input variables.\nWe directly use torch to simulate some data.\nToy data\n\n\nlibrary(torch)\n\n# input dimensionality (number of input features)\nd_in <- 3\n# output dimensionality (number of predicted features)\nd_out <- 1\n# number of observations in training set\nn <- 100\n\n\n# create random data\n# input\nx <- torch_randn(n, d_in)\n# target\ny <- x[, 1, drop = FALSE] * 0.2 -\n  x[, 2, drop = FALSE] * 1.3 -\n  x[, 3, drop = FALSE] * 0.5 +\n  torch_randn(n, 1)\n\n\n\nNext, we need to initialize the network’s weights. We’ll have one hidden layer, with 32 units. The output layer’s size, being determined by the task, is equal to 1.\nInitialize weights\n\n\n# dimensionality of hidden layer\nd_hidden <- 32\n\n# weights connecting input to hidden layer\nw1 <- torch_randn(d_in, d_hidden)\n# weights connecting hidden to output layer\nw2 <- torch_randn(d_hidden, d_out)\n\n# hidden layer bias\nb1 <- torch_zeros(1, d_hidden)\n# output layer bias\nb2 <- torch_zeros(1, d_out)\n\n\n\nNow for the training loop proper. The training loop here really is the network.\nTraining loop\nIn each iteration (“epoch”), the training loop does four things:\nruns through the network, computing predictions (forward pass)\ncompares those predictions to the ground truth and quantify the loss\nruns backwards through the network, computing the gradients that indicate how the weights should be changed\nupdates the weights, making use of the requested learning rate.\nHere is the template we’re going to fill:\n\n\nfor (t in 1:200) {\n    \n    ### -------- Forward pass -------- \n    \n    # here we'll compute the prediction\n    \n    \n    ### -------- compute loss -------- \n    \n    # here we'll compute the sum of squared errors\n    \n\n    ### -------- Backpropagation -------- \n    \n    # here we'll pass through the network, calculating the required gradients\n    \n\n    ### -------- Update weights -------- \n    \n    # here we'll update the weights, subtracting portion of the gradients \n}\n\n\n\nThe forward pass effectuates two affine transformations, one each for the hidden and output layers. In-between, ReLU activation is applied:\n\n\n  # compute pre-activations of hidden layers (dim: 100 x 32)\n  # torch_mm does matrix multiplication\n  h <- x$mm(w1) + b1\n  \n  # apply activation function (dim: 100 x 32)\n  # torch_clamp cuts off values below/above given thresholds\n  h_relu <- h$clamp(min = 0)\n  \n  # compute output (dim: 100 x 1)\n  y_pred <- h_relu$mm(w2) + b2\n\n\n\nOur loss here is mean squared error:\n\n\n  loss <- as.numeric((y_pred - y)$pow(2)$sum())\n\n\n\nCalculating gradients the manual way is a bit tedious3, but it can be done:\n\n\n  # gradient of loss w.r.t. prediction (dim: 100 x 1)\n  grad_y_pred <- 2 * (y_pred - y)\n  # gradient of loss w.r.t. w2 (dim: 32 x 1)\n  grad_w2 <- h_relu$t()$mm(grad_y_pred)\n  # gradient of loss w.r.t. hidden activation (dim: 100 x 32)\n  grad_h_relu <- grad_y_pred$mm(w2$t())\n  # gradient of loss w.r.t. hidden pre-activation (dim: 100 x 32)\n  grad_h <- grad_h_relu$clone()\n  \n  grad_h[h < 0] <- 0\n  \n  # gradient of loss w.r.t. b2 (shape: ())\n  grad_b2 <- grad_y_pred$sum()\n  \n  # gradient of loss w.r.t. w1 (dim: 3 x 32)\n  grad_w1 <- x$t()$mm(grad_h)\n  # gradient of loss w.r.t. b1 (shape: (32, ))\n  grad_b1 <- grad_h$sum(dim = 1)\n\n\n\nThe final step then uses the calculated gradients to update the weights:\n\n\n  learning_rate <- 1e-4\n  \n  w2 <- w2 - learning_rate * grad_w2\n  b2 <- b2 - learning_rate * grad_b2\n  w1 <- w1 - learning_rate * grad_w1\n  b1 <- b1 - learning_rate * grad_b1\n\n\n\nLet’s use these snippets to fill in the gaps in the above template, and give it a try!\nPutting it all together\n\n\nlibrary(torch)\n\n### generate training data -----------------------------------------------------\n\n# input dimensionality (number of input features)\nd_in <- 3\n# output dimensionality (number of predicted features)\nd_out <- 1\n# number of observations in training set\nn <- 100\n\n\n# create random data\nx <- torch_randn(n, d_in)\ny <-\n  x[, 1, NULL] * 0.2 - x[, 2, NULL] * 1.3 - x[, 3, NULL] * 0.5 + torch_randn(n, 1)\n\n\n### initialize weights ---------------------------------------------------------\n\n# dimensionality of hidden layer\nd_hidden <- 32\n# weights connecting input to hidden layer\nw1 <- torch_randn(d_in, d_hidden)\n# weights connecting hidden to output layer\nw2 <- torch_randn(d_hidden, d_out)\n\n# hidden layer bias\nb1 <- torch_zeros(1, d_hidden)\n# output layer bias\nb2 <- torch_zeros(1, d_out)\n\n### network parameters ---------------------------------------------------------\n\nlearning_rate <- 1e-4\n\n### training loop --------------------------------------------------------------\n\nfor (t in 1:200) {\n  ### -------- Forward pass --------\n  \n  # compute pre-activations of hidden layers (dim: 100 x 32)\n  h <- x$mm(w1) + b1\n  # apply activation function (dim: 100 x 32)\n  h_relu <- h$clamp(min = 0)\n  # compute output (dim: 100 x 1)\n  y_pred <- h_relu$mm(w2) + b2\n  \n  ### -------- compute loss --------\n\n  loss <- as.numeric((y_pred - y)$pow(2)$sum())\n  \n  if (t %% 10 == 0)\n    cat(\"Epoch: \", t, \"   Loss: \", loss, \"\\n\")\n  \n  ### -------- Backpropagation --------\n  \n  # gradient of loss w.r.t. prediction (dim: 100 x 1)\n  grad_y_pred <- 2 * (y_pred - y)\n  # gradient of loss w.r.t. w2 (dim: 32 x 1)\n  grad_w2 <- h_relu$t()$mm(grad_y_pred)\n  # gradient of loss w.r.t. hidden activation (dim: 100 x 32)\n  grad_h_relu <- grad_y_pred$mm(\n    w2$t())\n  # gradient of loss w.r.t. hidden pre-activation (dim: 100 x 32)\n  grad_h <- grad_h_relu$clone()\n  \n  grad_h[h < 0] <- 0\n  \n  # gradient of loss w.r.t. b2 (shape: ())\n  grad_b2 <- grad_y_pred$sum()\n  \n  # gradient of loss w.r.t. w1 (dim: 3 x 32)\n  grad_w1 <- x$t()$mm(grad_h)\n  # gradient of loss w.r.t. b1 (shape: (32, ))\n  grad_b1 <- grad_h$sum(dim = 1)\n  \n  ### -------- Update weights --------\n  \n  w2 <- w2 - learning_rate * grad_w2\n  b2 <- b2 - learning_rate * grad_b2\n  w1 <- w1 - learning_rate * grad_w1\n  b1 <- b1 - learning_rate * grad_b1\n  \n}\n\n\n\nEpoch:  10     Loss:  352.3585 \nEpoch:  20     Loss:  219.3624 \nEpoch:  30     Loss:  155.2307 \nEpoch:  40     Loss:  124.5716 \nEpoch:  50     Loss:  109.2687 \nEpoch:  60     Loss:  100.1543 \nEpoch:  70     Loss:  94.77817 \nEpoch:  80     Loss:  91.57003 \nEpoch:  90     Loss:  89.37974 \nEpoch:  100    Loss:  87.64617 \nEpoch:  110    Loss:  86.3077 \nEpoch:  120    Loss:  85.25118 \nEpoch:  130    Loss:  84.37959 \nEpoch:  140    Loss:  83.44133 \nEpoch:  150    Loss:  82.60386 \nEpoch:  160    Loss:  81.85324 \nEpoch:  170    Loss:  81.23454 \nEpoch:  180    Loss:  80.68679 \nEpoch:  190    Loss:  80.16555 \nEpoch:  200    Loss:  79.67953 \nThis looks like it worked pretty well! It also should have fulfilled its purpose: Showing what you can achieve using torch tensors alone. In case you didn’t feel like going through the backprop logic with too much enthusiasm, don’t worry: In the next installment, this will get significantly less cumbersome. See you then!\n\nAlthough the assumption may be tempting, “contiguous” does not correspond to what we’d call “contiguous in memory” in casual language.↩︎\nFor correctness’ sake, contiguous() will only make a copy if the tensor it is called on is not contiguous already.↩︎\nJust to avoid any misunderstandings: In the next installment, this will be very first thing rendered obsolete by torch’s automatic differentiation capabilities.↩︎\n",
    "preview": "posts/2020-10-01-torch-network-from-scratch/images/pic.jpg",
    "last_modified": "2020-10-01T10:08:33+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-09-30-sparklyr-1.4.0-released/",
    "title": "sparklyr 1.4: Weighted Sampling, Tidyr Verbs, Robust Scaler, RAPIDS, and more",
    "description": "Sparklyr 1.4 is now available! This release comes with delightful new features such as weighted sampling and tidyr verbs support for Spark dataframes, robust scaler for standardizing data based on median and interquartile range, spark_connect interface for RAPIDS GPU acceleration plugin, as well as a number of dplyr-related improvements.",
    "author": [
      {
        "name": "Yitao Li",
        "url": "https://github.com/yitao-li"
      }
    ],
    "date": "2020-09-30",
    "categories": [
      "R",
      "Packages/Releases",
      "Distributed Computing"
    ],
    "contents": "\nsparklyr 1.4 is now available on CRAN! To install sparklyr 1.4 from CRAN, run\n\n\ninstall.packages(\"sparklyr\")\n\n\n\nIn this blog post, we will showcase the following much-anticipated new functionalities from the sparklyr 1.4 release:\nParallelized Weighted Sampling with Spark\nSupport for Tidyr Verbs on Spark Dataframes\nft_robust_scaler as the R interface for RobustScaler from Spark 3.0\nOption for enabling RAPIDS GPU acceleration plugin in spark_connect()\nHigher-order functions and dplyr-related improvements\nParallelized Weighted Sampling\nReaders familiar with dplyr::sample_n() and dplyr::sample_frac() functions may have noticed that both of them support weighted-sampling use cases on R dataframes, e.g.,\n\n\ndplyr::sample_n(mtcars, size = 3, weight = mpg, replace = FALSE)\n\n\n\n               mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nFiat 128      32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nMerc 280C     17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMazda RX4 Wag 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nand\n\n\ndplyr::sample_frac(mtcars, size = 0.1, weight = mpg, replace = FALSE)\n\n\n\n             mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nHonda Civic 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nMerc 450SE  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nFiat X1-9   27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nwill select some random subset of mtcars using the mpg attribute as the sampling weight for each row. If replace = FALSE is set, then a row is removed from the sampling population once it gets selected, whereas when setting replace = TRUE, each row will always stay in the sampling population and can be selected multiple times.\nNow the exact same use cases are supported for Spark dataframes in sparklyr 1.4! For example:\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\nmtcars_sdf <- copy_to(sc, mtcars, repartition = 4L)\n\ndplyr::sample_n(mtcars_sdf, size = 5, weight = mpg, replace = FALSE)\n\n\n\nwill return a random subset of size 5 from the Spark dataframe mtcars_sdf.\nMore importantly, the sampling algorithm implemented in sparklyr 1.4 is something that fits perfectly into the MapReduce paradigm: as we have split our mtcars data into 4 partitions of mtcars_sdf by specifying repartition = 4L, the algorithm will first process each partition independently and in parallel, selecting a sample set of size up to 5 from each, and then reduce all 4 sample sets into a final sample set of size 5 by choosing records having the top 5 highest sampling priorities among all.\nHow is such parallelization possible, especially for the sampling without replacement scenario, where the desired result is defined as the outcome of a sequential process? A detailed answer to this question is in this blog post, which includes a definition of the problem (in particular, the exact meaning of sampling weights in term of probabilities), a high-level explanation of the current solution and the motivation behind it, and also, some mathematical details all hidden in one link to a PDF file, so that non-math-oriented readers can get the gist of everything else without getting scared away, while math-oriented readers can enjoy working out all the integrals themselves before peeking at the answer.\nTidyr Verbs\nThe specialized implementations of the following tidyr verbs that work efficiently with Spark dataframes were included as part of sparklyr 1.4:\ntidyr::fill\ntidyr::nest\ntidyr::unnest\ntidyr::pivot_wider\ntidyr::pivot_longer\ntidyr::separate\ntidyr::unite\nWe can demonstrate how those verbs are useful for tidying data through some examples.\nLet’s say we are given mtcars_sdf, a Spark dataframe containing all rows from mtcars plus the name of each row:\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\nmtcars_sdf <- cbind(\n  data.frame(model = rownames(mtcars)),\n  data.frame(mtcars, row.names = NULL)\n) %>%\n  copy_to(sc, ., repartition = 4L)\n\nprint(mtcars_sdf, n = 5)\n\n\n\n# Source: spark<?> [?? x 12]\n  model          mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <chr>        <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 Mazda RX4     21       6   160   110  3.9   2.62  16.5     0     1     4     4\n2 Mazda RX4 W…  21       6   160   110  3.9   2.88  17.0     0     1     4     4\n3 Datsun 710    22.8     4   108    93  3.85  2.32  18.6     1     1     4     1\n4 Hornet 4 Dr…  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1\n5 Hornet Spor…  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2\n# … with more rows\nand we would like to turn all numeric attributes in mtcar_sdf (in other words, all columns other than the model column) into key-value pairs stored in 2 columns, with the key column storing the name of each attribute, and the value column storing each attribute’s numeric value. One way to accomplish that with tidyr is by utilizing the tidyr::pivot_longer functionality:\n\n\nmtcars_kv_sdf <- mtcars_sdf %>%\n  tidyr::pivot_longer(cols = -model, names_to = \"key\", values_to = \"value\")\nprint(mtcars_kv_sdf, n = 5)\n\n\n\n# Source: spark<?> [?? x 3]\n  model     key   value\n  <chr>     <chr> <dbl>\n1 Mazda RX4 am      1\n2 Mazda RX4 carb    4\n3 Mazda RX4 cyl     6\n4 Mazda RX4 disp  160\n5 Mazda RX4 drat    3.9\n# … with more rows\nTo undo the effect of tidyr::pivot_longer, we can apply tidyr::pivot_wider to our mtcars_kv_sdf Spark dataframe, and get back the original data that was present in mtcars_sdf:\n\n\ntbl <- mtcars_kv_sdf %>%\n  tidyr::pivot_wider(names_from = key, values_from = value)\nprint(tbl, n = 5)\n\n\n\n# Source: spark<?> [?? x 12]\n  model         carb   cyl  drat    hp   mpg    vs    wt    am  disp  gear  qsec\n  <chr>        <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 Mazda RX4        4     6  3.9    110  21       0  2.62     1  160      4  16.5\n2 Hornet 4 Dr…     1     6  3.08   110  21.4     1  3.22     0  258      3  19.4\n3 Hornet Spor…     2     8  3.15   175  18.7     0  3.44     0  360      3  17.0\n4 Merc 280C        4     6  3.92   123  17.8     1  3.44     0  168.     4  18.9\n5 Merc 450SLC      3     8  3.07   180  15.2     0  3.78     0  276.     3  18\n# … with more rows\nAnother way to reduce many columns into fewer ones is by using tidyr::nest to move some columns into nested tables. For instance, we can create a nested table perf encapsulating all performance-related attributes from mtcars (namely, hp, mpg, disp, and qsec). However, unlike R dataframes, Spark Dataframes do not have the concept of nested tables, and the closest to nested tables we can get is a perf column containing named structs with hp, mpg, disp, and qsec attributes:\n\n\nmtcars_nested_sdf <- mtcars_sdf %>%\n  tidyr::nest(perf = c(hp, mpg, disp, qsec))\n\n\n\nWe can then inspect the type of perf column in mtcars_nested_sdf:\n\n\nsdf_schema(mtcars_nested_sdf)$perf$type\n\n\n\n[1] \"ArrayType(StructType(StructField(hp,DoubleType,true), StructField(mpg,DoubleType,true), StructField(disp,DoubleType,true), StructField(qsec,DoubleType,true)),true)\"\nand inspect individual struct elements within perf:\n\n\nperf <- mtcars_nested_sdf %>% dplyr::pull(perf)\nunlist(perf[[1]])\n\n\n\n    hp    mpg   disp   qsec\n110.00  21.00 160.00  16.46\nFinally, we can also use tidyr::unnest to undo the effects of tidyr::nest:\n\n\nmtcars_unnested_sdf <- mtcars_nested_sdf %>%\n  tidyr::unnest(col = perf)\nprint(mtcars_unnested_sdf, n = 5)\n\n\n\n# Source: spark<?> [?? x 12]\n  model          cyl  drat    wt    vs    am  gear  carb    hp   mpg  disp  qsec\n  <chr>        <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 Mazda RX4        6  3.9   2.62     0     1     4     4   110  21    160   16.5\n2 Hornet 4 Dr…     6  3.08  3.22     1     0     3     1   110  21.4  258   19.4\n3 Duster 360       8  3.21  3.57     0     0     3     4   245  14.3  360   15.8\n4 Merc 280         6  3.92  3.44     1     0     4     4   123  19.2  168.  18.3\n5 Lincoln Con…     8  3     5.42     0     0     3     4   215  10.4  460   17.8\n# … with more rows\nRobust Scaler\nRobustScaler is a new functionality introduced in Spark 3.0 (SPARK-28399). Thanks to a pull request by @zero323, an R interface for RobustScaler, namely, the ft_robust_scaler() function, is now part of sparklyr.\nIt is often observed that many machine learning algorithms perform better on numeric inputs that are standardized. Many of us have learned in stats 101 that given a random variable \\(X\\), we can compute its mean \\(\\mu = E[X]\\), standard deviation \\(\\sigma = \\sqrt{E[X^2] - (E[X])^2}\\), and then obtain a standard score \\(z = \\frac{X - \\mu}{\\sigma}\\) which has mean of 0 and standard deviation of 1.\nHowever, notice both \\(E[X]\\) and \\(E[X^2]\\) from above are quantities that can be easily skewed by extreme outliers in \\(X\\), causing distortions in \\(z\\). A particular bad case of it would be if all non-outliers among \\(X\\) are very close to \\(0\\), hence making \\(E[X]\\) close to \\(0\\), while extreme outliers are all far in the negative direction, hence dragging down \\(E[X]\\) while skewing \\(E[X^2]\\) upwards.\nAn alternative way of standardizing \\(X\\) based on its median, 1st quartile, and 3rd quartile values, all of which are robust against outliers, would be the following:\n\\(\\displaystyle z = \\frac{X - \\text{Median}(X)}{\\text{P75}(X) - \\text{P25}(X)}\\)\nand this is precisely what RobustScaler offers.\nTo see ft_robust_scaler() in action and demonstrate its usefulness, we can go through a contrived example consisting of the following steps:\nDraw 500 random samples from the standard normal distribution\n\n\nsample_values <- rnorm(500)\nprint(sample_values)\n\n\n\n  [1] -0.626453811  0.183643324 -0.835628612  1.595280802  0.329507772\n  [6] -0.820468384  0.487429052  0.738324705  0.575781352 -0.305388387\n  ...\nInspect the minimal and maximal values among the \\(500\\) random samples:\n\n\nprint(min(sample_values))\n\n\n\n  [1] -3.008049\n\n\nprint(max(sample_values))\n\n\n\n  [1] 3.810277\nNow create \\(10\\) other values that are extreme outliers compared to the \\(500\\) random samples above. Given that we know all \\(500\\) samples are within the range of \\((-4, 4)\\), we can choose \\(-501, -502, \\ldots, -509, -510\\) as our \\(10\\) outliers:\n\n\noutliers <- -500L - seq(10)\n\n\n\nCopy all \\(510\\) values into a Spark dataframe named sdf\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\", version = \"3.0.0\")\nsdf <- copy_to(sc, data.frame(value = c(sample_values, outliers)))\n\n\n\nWe can then apply ft_robust_scaler() to obtain the standardized value for each input:\n\n\nscaled <- sdf %>%\n  ft_vector_assembler(\"value\", \"input\") %>%\n  ft_robust_scaler(\"input\", \"scaled\") %>%\n  dplyr::pull(scaled) %>%\n  unlist()\n\n\n\nPlotting the result shows the non-outlier data points being scaled to values that still more or less form a bell-shaped distribution centered around \\(0\\), as expected, so the scaling is robust against influence of the outliers:\n\n\nlibrary(ggplot2)\n\nggplot(data.frame(scaled = scaled), aes(x = scaled)) +\n  xlim(-7, 7) +\n  geom_histogram(binwidth = 0.2)\n\n\n\n\nFinally, we can compare the distribution of the scaled values above with the distribution of z-scores of all input values, and notice how scaling the input with only mean and standard deviation would have caused noticeable skewness – which the robust scaler has successfully avoided:\n\n\nall_values <- c(sample_values, outliers)\nz_scores <- (all_values - mean(all_values)) / sd(all_values)\nggplot(data.frame(scaled = z_scores), aes(x = scaled)) +\n  xlim(-0.05, 0.2) +\n  geom_histogram(binwidth = 0.005)\n\n\n\n\nFrom the 2 plots above, one can observe while both standardization processes produced some distributions that were still bell-shaped, the one produced by ft_robust_scaler() is centered around \\(0\\), correctly indicating the average among all non-outlier values, while the z-score distribution is clearly not centered around \\(0\\) as its center has been noticeably shifted by the \\(10\\) outlier values.\nRAPIDS\nReaders following Apache Spark releases closely probably have noticed the recent addition of RAPIDS GPU acceleration support in Spark 3.0. Catching up with this recent development, an option to enable RAPIDS in Spark connections was also created in sparklyr and shipped in sparklyr 1.4. On a host with RAPIDS-capable hardware (e.g., an Amazon EC2 instance of type ‘p3.2xlarge’), one can install sparklyr 1.4 and observe RAPIDS hardware acceleration being reflected in Spark SQL physical query plans:\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\", version = \"3.0.0\", packages = \"rapids\")\ndplyr::db_explain(sc, \"SELECT 4\")\n\n\n\n== Physical Plan ==\n*(2) GpuColumnarToRow false\n+- GpuProject [4 AS 4#45]\n   +- GpuRowToColumnar TargetSize(2147483647)\n      +- *(1) Scan OneRowRelation[]\nHigher-Order Functions and dplyr-Related Improvements\nAll newly introduced higher-order functions from Spark 3.0, such as array_sort() with custom comparator, transform_keys(), transform_values(), and map_zip_with(), are supported by sparklyr 1.4.\nIn addition, all higher-order functions can now be accessed directly through dplyr rather than their hof_* counterparts in sparklyr. This means, for example, that we can run the following dplyr queries to calculate the square of all array elements in column x of sdf, and then sort them in descending order:\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\", version = \"3.0.0\")\nsdf <- copy_to(sc, tibble::tibble(x = list(c(-3, -2, 1, 5), c(6, -7, 5, 8))))\n\nsq_desc <- sdf %>%\n  dplyr::mutate(x = transform(x, ~ .x * .x)) %>%\n  dplyr::mutate(x = array_sort(x, ~ as.integer(sign(.y - .x)))) %>%\n  dplyr::pull(x)\n\nprint(sq_desc)\n\n\n\n[[1]]\n[1] 25  9  4  1\n\n[[2]]\n[1] 64 49 36 25\nAcknowledgement\nIn chronological order, we would like to thank the following individuals for their contributions to sparklyr 1.4:\n@javierluraschi\n@nealrichardson\n@yitao-li\n@wkdavis\n@Loquats\n@zero323\nWe also appreciate bug reports, feature requests, and valuable other feedback about sparklyr from our awesome open-source community (e.g., the weighted sampling feature in sparklyr 1.4 was largely motivated by this Github issue filed by @ajing, and some dplyr-related bug fixes in this release were initiated in #2648 and completed with this pull request by @wkdavis).\nLast but not least, the author of this blog post is extremely grateful for fantastic editorial suggestions from @javierluraschi, @batpigandme, and @skeydan.\nIf you wish to learn more about sparklyr, we recommend checking out sparklyr.ai, spark.rstudio.com, and also some of the previous release posts such as sparklyr 1.3 and sparklyr 1.2.\nThanks for reading!\n\n\n\n",
    "preview": "posts/2020-09-30-sparklyr-1.4.0-released/images/sparklyr-1.4.jpg",
    "last_modified": "2020-10-01T10:08:33+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-09-29-introducing-torch-for-r/",
    "title": "Please allow me to introduce myself: Torch for R",
    "description": "Today, we are excited to introduce torch, an R package that allows you to use PyTorch-like functionality natively from R. No Python installation is required: torch is built directly on top of libtorch, a C++ library that provides the tensor-computation and automatic-differentiation capabilities essential to building neural networks.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-09-29",
    "categories": [
      "Packages/Releases",
      "Torch",
      "R"
    ],
    "contents": "\n\nContents\nInstallation\nData loading and pre-processing\nNetwork\nTraining\nEvaluation\nLearn\nWe need you\n\nLast January at rstudio::conf, in that distant past when conferences still used to take place at some physical location, my colleague Daniel gave a talk introducing new features and ongoing development in the tensorflow ecosystem. In the Q&A part, he was asked something unexpected: Were we going to build support for PyTorch? He hesitated; that was in fact the plan, and he had already played around with natively implementing torch tensors at a prior time, but he was not completely certain how well “it” would work.\n“It”, that is an implementation which does not bind to Python Torch, meaning, we don’t install the PyTorch wheel and import it via reticulate. Instead, we delegate to the underlying C++ library libtorch for tensor computations and automatic differentiation, while neural network features – layers, activations, optimizers – are implemented directly in R. Removing the intermediary has at least two benefits: For one, the leaner software stack means fewer possible problems in installation and fewer places to look when troubleshooting. Secondly, through its non-dependence on Python, torch does not require users to install and maintain a suitable Python environment. Depending on operating system and context, this can make an enormous difference: For example, in many organizations employees are not allowed to manipulate privileged software installations on their laptops.\nSo why did Daniel hesitate, and, if I recall correctly, give a not-too-conclusive answer? On the one hand, it was not clear whether compilation against libtorch would, on some operating systems, pose severe difficulties. (It did, but difficulties turned out to be surmountable.)1 On the other, the sheer amount of work involved in re-implementing – not all, but a big amount of – PyTorch in R seemed intimidating. Today, there is still lots of work to be done (we’ll pick up that thread at the end), but the main obstacles have been ovecome, and enough components are available that torch can be useful to the R community. Thus, without further ado, let’s train a neural network.\nYou’re not at your laptop now? Just follow along in the companion notebook on Colaboratory.\nInstallation\ntorch\nInstalling torch is as straightforward as typing\n\n\ninstall.packages(\"torch\")\n\n\n\nThis will detect whether you have CUDA installed, and either download the CPU or the GPU version of libtorch. Then, it will install the R package from CRAN. To make use of the very newest features, you can install the development version from GitHub:\n\n\ndevtools::install_github(\"mlverse/torch\")\n\n\n\nTo quickly check the installation, and whether GPU support works fine (assuming that there is a CUDA-capable NVidia GPU), create a tensor on the CUDA device:\n\n\ntorch_tensor(1, device = \"cuda\")\n\n\n\ntorch_tensor \n 1\n[ CUDAFloatType{1} ]\nIf all our hello torch example did was run a network on, say, simulated data, we could stop here. As we’ll do image classification, however, we need to install another package: torchvision.\ntorchvision\n\n\n\nWhereas torch is where tensors, network modules, and generic data loading functionality live, datatype-specific capabilities are – or will be – provided by dedicated packages. In general, these capabilities comprise three types of things: datasets, tools for pre-processing and data loading, and pre-trained models.\nAs of this writing, PyTorch has dedicated libraries for three domain areas: vision, text, and audio. In R, we plan to proceed analogously – “plan”, because torchtext and torchaudio are yet to be created. Right now, torchvision is all we need:\n\n\ndevtools::install_github(\"mlverse/torchvision\")\n\n\n\nAnd we’re ready to load the data.\nData loading and pre-processing\n\n\nlibrary(torch)\nlibrary(torchvision)\n\n\n\nThe list of vision datasets bundled with PyTorch is long, and they’re continually being added to torchvision.\nThe one we need right now is available already, and it’s – MNIST? … not quite: It’s my favorite “MNIST dropin”, Kuzushiji-MNIST (Clanuwat et al. 2018). Like other datasets explicitly created to replace MNIST, it has ten classes – characters, in this case, depicted as grayscale images of resolution 28x28.\nHere are the first 32 characters:\n\n\n\nFigure 1: Kuzushiji MNIST.\n\n\n\nDataset\nThe following code will download the data separately for training and test sets.\n\n\ntrain_ds <- kmnist_dataset(\n  \".\",\n  download = TRUE,\n  train = TRUE,\n  transform = transform_to_tensor\n)\n\ntest_ds <- kmnist_dataset(\n  \".\",\n  download = TRUE,\n  train = FALSE,\n  transform = transform_to_tensor\n)\n\n\n\nNote the transform argument. transform_to_tensor takes an image and applies two transformations: First, it normalizes the pixels to the range between 0 and 1. Then, it adds another dimension in front. Why?\nContrary to what you might expect – if until now, you’ve been using keras – the additional dimension is not the batch dimension. Batching will be taken care of by the dataloader, to be introduced next. Instead, this is the channels dimension that in torch, is found before the width and height dimensions by default.\nOne thing I’ve found to be extremely useful about torch is how easy it is to inspect objects. Even though we’re dealing with a dataset, a custom object, and not an R array or even a torch tensor, we can easily peek at what’s inside. Indexing in torch is 1-based, conforming to the R user’s intuitions. Consequently,\n\n\ntrain_ds[1]\n\n\n\ngives us the first element in the dataset, an R list of two tensors corresponding to input and target, respectively. (We don’t reproduce the output here, but you can see for yourself in the notebook.)\nLet’s inspect the shape of the input tensor:\n\n\ntrain_ds[1][[1]]$size()\n\n\n\n[1]  1 28 28\nNow that we have the data, we need someone to feed them to a deep learning model, nicely batched and all. In torch, this is the task of data loaders.\nData loader\nEach of the training and test sets gets their own data loader:\n\n\ntrain_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE)\ntest_dl <- dataloader(test_ds, batch_size = 32)\n\n\n\nAgain, torch makes it easy to verify we did the correct thing. To take a look at the content of the first batch, do\n\n\ntrain_iter <- train_dl$.iter()\ntrain_iter$.next()\n\n\n\nFunctionality like this may not seem indispensable when working with a well-known dataset, but it will turn out to be very useful when a lot of domain-specific pre-processing is required.\nNow that we’ve seen how to load data, all prerequisites are fulfilled for visualizing them. Here is the code that was used to display the first batch of characters, above:\n\n\npar(mfrow = c(4,8), mar = rep(0, 4))\nimages <- train_dl$.iter()$.next()[[1]][1:32, 1, , ] \nimages %>%\n  purrr::array_tree(1) %>%\n  purrr::map(as.raster) %>%\n  purrr::iwalk(~{plot(.x)})\n\n\n\nWe’re ready to define our network – a simple convnet.\nNetwork\nIf you’ve been using keras custom models (or have some experience with PyTorch), the following way of defining a network may not look too surprising.\nYou use nn_module() to define an R6 class that will hold the network’s components. Its layers are created in initialize(); forward() describes what happens during the network’s forward pass. One thing on terminology: In torch, layers are called modules, as are networks. This makes sense: The design is truly modular in that any module can be used as a component in a larger one.\n\n\nnet <- nn_module(\n  \n  \"KMNIST-CNN\",\n  \n  initialize = function() {\n    # in_channels, out_channels, kernel_size, stride = 1, padding = 0\n    self$conv1 <- nn_conv2d(1, 32, 3)\n    self$conv2 <- nn_conv2d(32, 64, 3)\n    self$dropout1 <- nn_dropout2d(0.25)\n    self$dropout2 <- nn_dropout2d(0.5)\n    self$fc1 <- nn_linear(9216, 128)\n    self$fc2 <- nn_linear(128, 10)\n  },\n  \n  forward = function(x) {\n    x %>% \n      self$conv1() %>%\n      nnf_relu() %>%\n      self$conv2() %>%\n      nnf_relu() %>%\n      nnf_max_pool2d(2) %>%\n      self$dropout1() %>%\n      torch_flatten(start_dim = 2) %>%\n      self$fc1() %>%\n      nnf_relu() %>%\n      self$dropout2() %>%\n      self$fc2()\n  }\n)\n\n\n\nThe layers – apologies: modules – themselves may look familiar. Unsurprisingly, nn_conv2d() performs two-dimensional convolution; nn_linear() multiplies by a weight matrix and adds a vector of biases. But what are those numbers: nn_linear(128, 10), say?\nIn torch, instead of the number of units in a layer, you specify input and output dimensionalities of the “data” that run through it. Thus, nn_linear(128, 10) has 128 input connections and outputs 10 values – one for every class. In some cases, such as this one, specifying dimensions is easy – we know how many input edges there are (namely, the same as the number of output edges from the previous layer), and we know how many output values we need. But how about the previous module? How do we arrive at 9216 input connections?\nHere, a bit of calculation is necessary. We go through all actions that happen in forward() – if they affect shapes, we keep track of the transformation; if they don’t, we ignore them.\nSo, we start with input tensors of shape batch_size x 1 x 28 x 28. Then,\nnn_conv2d(1, 32, 3) , or equivalently, nn_conv2d(in_channels = 1, out_channels = 32, kernel_size = 3),applies a convolution with kernel size 3, stride 1 (the default), and no padding (the default). We can consult the documentation to look up the resulting output size, or just intuitively reason that with a kernel of size 3 and no padding, the image will shrink by one pixel in each direction, resulting in a spatial resolution of 26 x 26. Per channel, that is. Thus, the actual output shape is batch_size x 32 x 26 x 26 . Next,\nnnf_relu() applies ReLU activation, in no way touching the shape. Next is\nnn_conv2d(32, 64, 3), another convolution with zero padding and kernel size 3. Output size now is batch_size x 64 x 24 x 24 . Now, the second\nnnf_relu() again does nothing to the output shape, but\nnnf_max_pool2d(2) (equivalently: nnf_max_pool2d(kernel_size = 2)) does: It applies max pooling over regions of extension 2 x 2, thus downsizing the output to a format of batch_size x 64 x 12 x 12 . Now,\nnn_dropout2d(0.25) is a no-op, shape-wise, but if we want to apply a linear layer later, we need to merge all of the channels, height and width axes into a single dimension. This is done in\ntorch_flatten(start_dim = 2). Output shape is now batch_size * 9216 , since 64 * 12 * 12 = 9216 . Thus here we have the 9216 input connections fed into the\nnn_linear(9216, 128) discussed above. Again,\nnnf_relu() and nn_dropout2d(0.5) leave dimensions as they are, and finally,\nnn_linear(128, 10) gives us the desired output scores, one for each of the ten classes.\nNow you’ll be thinking, – what if my network is more complicated? Calculations could become pretty cumbersome. Luckily, with torch’s flexibility, there is another way. Since every layer is callable in isolation, we can just … create some sample data and see what happens!\nHere is a sample “image” – or more precisely, a one-item batch containing it:\n\n\nx <- torch_randn(c(1, 1, 28, 28))\n\n\n\nWhat if we call the first conv2d module on it?\n\n\nconv1 <- nn_conv2d(1, 32, 3)\nconv1(x)$size()\n\n\n\n[1]  1 32 26 26\nOr both conv2d modules?\n\n\nconv2 <- nn_conv2d(32, 64, 3)\n(conv1(x) %>% conv2())$size()\n\n\n\n[1]  1 64 24 24\nAnd so on. This is just one example illustrating how torchs flexibility makes developing neural nets easier.\nBack to the main thread. We instantiate the model, and we ask torch to allocate its weights (parameters) on the GPU:\n\n\nmodel <- net()\nmodel$to(device = \"cuda\")\n\n\n\nWe’ll do the same for the input and output data – that is, we’ll move them to the GPU. This is done in the training loop, which we’ll inspect next.\nTraining\nIn torch, when creating an optimizer, we tell it what to operate on, namely, the model’s parameters:\n\n\noptimizer <- optim_adam(model$parameters)\n\n\n\nWhat about the loss function? For classification with more than two classes, we use cross entropy, in torch: nnf_cross_entropy(prediction, ground_truth):\n\n\n# this will be called for every batch, see training loop below\nloss <- nnf_cross_entropy(output, b[[2]]$to(device = \"cuda\"))\n\n\n\nUnlike categorical cross entropy in keras , which would expect prediction to contain probabilities, as obtained by applying a softmax activation, torch’s nnf_cross_entropy() works with the raw outputs (the logits). This is why the network’s last linear layer was not followed by any activation.\nThe training loop, in fact, is a double one: It loops over epochs and batches. For every batch, it calls the model on the input, calculates the loss, and has the optimizer update the weights:\n\n\nfor (epoch in 1:5) {\n\n  l <- c()\n\n  for (b in enumerate(train_dl)) {\n    # make sure each batch's gradient updates are calculated from a fresh start\n    optimizer$zero_grad()\n    # get model predictions\n    output <- model(b[[1]]$to(device = \"cuda\"))\n    # calculate loss\n    loss <- nnf_cross_entropy(output, b[[2]]$to(device = \"cuda\"))\n    # calculate gradient\n    loss$backward()\n    # apply weight updates\n    optimizer$step()\n    # track losses\n    l <- c(l, loss$item())\n  }\n\n  cat(sprintf(\"Loss at epoch %d: %3f\\n\", epoch, mean(l)))\n}\n\n\n\nLoss at epoch 1: 1.795564\nLoss at epoch 2: 1.540063\nLoss at epoch 3: 1.495343\nLoss at epoch 4: 1.461649\nLoss at epoch 5: 1.446628\nAlthough there is a lot more that could be done – calculate metrics or evaluate performance on a validation set, for example – the above is a typical (if simple) template for a torch training loop.\nThe optimizer-related idioms in particular\n\n\noptimizer$zero_grad()\n# ...\nloss$backward()\n# ...\noptimizer$step()\n\n\n\nyou’ll keep encountering over and over.\nFinally, let’s evaluate model performance on the test set.\nEvaluation\nPutting a model in eval mode tells torch not to calculate gradients and perform backprop during the operations that follow:\n\n\nmodel$eval()\n\n\n\nWe iterate over the test set, keeping track of losses and accuracies obtained on the batches.\n\n\ntest_losses <- c()\ntotal <- 0\ncorrect <- 0\n\nfor (b in enumerate(test_dl)) {\n  output <- model(b[[1]]$to(device = \"cuda\"))\n  labels <- b[[2]]$to(device = \"cuda\")\n  loss <- nnf_cross_entropy(output, labels)\n  test_losses <- c(test_losses, loss$item())\n  # torch_max returns a list, with position 1 containing the values \n  # and position 2 containing the respective indices\n  predicted <- torch_max(output$data(), dim = 2)[[2]]\n  total <- total + labels$size(1)\n  # add number of correct classifications in this batch to the aggregate\n  correct <- correct + (predicted == labels)$sum()$item()\n}\n\nmean(test_losses)\n\n\n\n[1] 1.53784480643349\nHere is mean accuracy, computed as proportion of correct classifications:\n\n\ntest_accuracy <-  correct/total\ntest_accuracy\n\n\n\n[1] 0.9449\nThat’s it for our first torch example. Where to from here?\nLearn\nTo learn more, check out our vignettes on the torch website. To begin, you may want to check out these in particular:\n“Getting started” series: Build a simple neural network from scratch, starting from low-level tensor manipulation and gradually adding in higher-level features like automatic differentiation and network modules.\nMore on tensors: Tensor creation and indexing\nBackpropagation in torch: autograd\nIf you have questions, or run into problems, please feel free to ask on GitHub or on the RStudio community forum.\nWe need you\nWe very much hope that the R community will find the new functionality useful. But that’s not all. We hope that you, many of you, will take part in the journey.\nThere is not just a whole framework to be built, including many specialized modules, activation functions, optimizers and schedulers, with more of each being added continuously, on the Python side.\nThere is not just that whole “bag of data types” to be taken care of (images, text, audio…), each of which demand their own pre-processing and data-loading functionality. As everyone knows from experience, ease of data preparation is a, perhaps the essential factor in how usable a framework is.\nThen, there is the ever-expanding ecosystem of libraries built on top of PyTorch: PySyft and CrypTen for privacy-preserving machine learning, PyTorch Geometric for deep learning on manifolds, and Pyro for probabilistic programming, to name just a few.\nAll this is much more than can be done by one or two people: We need your help! Contributions are greatly welcomed at absolutely any scale:\nAdd or improve documentation, add introductory examples\nImplement missing layers (modules), activations, helper functions…\nImplement model architectures\nPort some of the PyTorch ecosystem\nOne component that should be of special interest to the R community is Torch distributions, the basis for probabilistic computation. This package is built upon by e.g. the aforementioned Pyro; at the same time, the distributions that live there are used in probabilistic neural networks or normalizing flows.\nTo reiterate, participation from the R community is greatly encouraged (more than that – fervently hoped for!). Have fun with torch, and thanks for reading!\n\n\n\nClanuwat, Tarin, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. 2018. “Deep Learning for Classical Japanese Literature.” December 3, 2018. http://arxiv.org/abs/cs.CV/1812.01718.\n\n\nIn a nutshell, Javier had the idea of wrapping libtorch into lantern, a C interface to libtorch, thus avoiding cross-compiler issues between MinGW and Visual Studio.↩︎\n",
    "preview": "posts/2020-09-29-introducing-torch-for-r/images/pt.png",
    "last_modified": "2020-10-01T10:08:33+02:00",
    "input_file": {},
    "preview_width": 919,
    "preview_height": 264
  },
  {
    "path": "posts/2020-09-07-sparklyr-flint/",
    "title": "Introducing sparklyr.flint: A time-series extension for sparklyr",
    "description": "We are pleased to announce that sparklyr.flint, a sparklyr extension for analyzing time series at scale with Flint, is now available on CRAN. Flint is an open-source library for working with time-series in Apache Spark which supports aggregates and joins on time-series datasets.",
    "author": [
      {
        "name": "Yitao Li",
        "url": {}
      }
    ],
    "date": "2020-09-07",
    "categories": [
      "R",
      "Time Series"
    ],
    "contents": "\nIn this blog post, we will showcase sparklyr.flint, a brand new sparklyr extension providing a simple and intuitive R interface to the Flint time series library. sparklyr.flint is available on CRAN today and can be installed as follows:\n\ninstall.packages(\"sparklyr.flint\")\nThe first two sections of this post will be a quick bird’s eye view on sparklyr and Flint, which will ensure readers unfamiliar with sparklyr or Flint can see both of them as essential building blocks for sparklyr.flint. After that, we will feature sparklyr.flint’s design philosophy, current state, example usages, and last but not least, its future directions as an open-source project in the subsequent sections.\nQuick Intro to sparklyr\nsparklyr is an open-source R interface that integrates the power of distributed computing from Apache Spark with the familiar idioms, tools, and paradigms for data transformation and data modelling in R. It allows data pipelines working well with non-distributed data in R to be easily transformed into analogous ones that can process large-scale, distributed data in Apache Spark.\nInstead of summarizing everything sparklyr has to offer in a few sentences, which is impossible to do, this section will solely focus on a small subset of sparklyr functionalities that are relevant to connecting to Apache Spark from R, importing time series data from external data sources to Spark, and also simple transformations which are typically part of data pre-processing steps.\nConnecting to an Apache Spark cluster\nThe first step in using sparklyr is to connect to Apache Spark. Usually this means one of the following:\nRunning Apache Spark locally on your machine, and connecting to it to test, debug, or to execute quick demos that don’t require a multi-node Spark cluster:\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\", version = \"2.4.4\")\n\nConnecting to a multi-node Apache Spark cluster that is managed by a cluster manager such as YARN, e.g.,\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"yarn-client\", spark_home = \"/usr/lib/spark\")\n\nImporting external data to Spark\nMaking external data available in Spark is easy with sparklyr given the large number of data sources sparklyr supports. For example, given an R dataframe, such as\n\n\ndat <- data.frame(id = seq(10), value = rnorm(10))\n\nthe command to copy it to a Spark dataframe with 3 partitions is simply\n\n\nsdf <- copy_to(sc, dat, name = \"unique_name_of_my_spark_dataframe\", repartition = 3L)\n\nSimilarly, there are options for ingesting data in CSV, JSON, ORC, AVRO, and many other well-known formats into Spark as well:\n\n\nsdf_csv <- spark_read_csv(sc, name = \"another_spark_dataframe\", path = \"file:///tmp/file.csv\", repartition = 3L)\n# or\nsdf_json <- spark_read_json(sc, name = \"yet_another_one\", path = \"file:///tmp/file.json\", repartition = 3L)\n# or spark_read_orc, spark_read_avro, etc\n\nTransforming a Spark dataframe\nWith sparklyr, the simplest and most readable way to transformation a Spark dataframe is by using dplyr verbs and the pipe operator (%>%) from magrittr.\nSparklyr supports a large number of dplyr verbs. For example,\n\n\nsdf <- sdf %>%\n  dplyr::filter(!is.null(id)) %>%\n  dplyr::mutate(value = value ^ 2)\n\nEnsures sdf only contains rows with non-null IDs, and then squares the value column of each row.\nThat’s about it for a quick intro to sparklyr. You can learn more in sparklyr.ai, where you will find links to reference material, books, communities, sponsors, and much more.\nWhat is Flint?\nFlint is a powerful open-source library for working with time-series data in Apache Spark. First of all, it supports efficient computation of aggregate statistics on time-series data points having the same timestamp (a.k.a summarizeCycles in Flint nomenclature), within a given time window (a.k.a., summarizeWindows), or within some given time intervals (a.k.a summarizeIntervals). It can also join two or more time-series datasets based on inexact match of timestamps using asof join functions such as LeftJoin and FutureLeftJoin. The author of Flint has outlined many more of Flint’s major functionalities in this article, which I found to be extremely helpful when working out how to build sparklyr.flint as a simple and straightforward R interface for such functionalities.\nReaders wanting some direct hands-on experience with Flint and Apache Spark can go through the following steps to run a minimal example of using Flint to analyze time-series data:\nFirst, install Apache Spark locally, and then for convenience reasons, define the SPARK_HOME environment variable. In this example, we will run Flint with Apache Spark 2.4.4 installed at ~/spark, so:\n\n\nexport SPARK_HOME=~/spark/spark-2.4.4-bin-hadoop2.7\n\nLaunch Spark shell and instruct it to download Flint and its Maven dependencies:\n\n\n\"${SPARK_HOME}\"/bin/spark-shell --packages=com.twosigma:flint:0.6.0\n\nCreate a simple Spark dataframe containing some time-series data:\n\n\nimport spark.implicits._\n\nval ts_sdf = Seq((1L, 1), (2L, 4), (3L, 9), (4L, 16)).toDF(\"time\", \"value\")\n\nImport the dataframe along with additional metadata such as time unit and name of the timestamp column into a TimeSeriesRDD, so that Flint can interpret the time-series data unambiguously:\n\n\nimport com.twosigma.flint.timeseries.TimeSeriesRDD\n\nval ts_rdd = TimeSeriesRDD.fromDF(\n  ts_sdf\n)(\n  isSorted = true, // rows are already sorted by time\n  timeUnit = java.util.concurrent.TimeUnit.SECONDS,\n  timeColumn = \"time\"\n)\n\nFinally, after all the hard work above, we can leverage various time-series functionalities provided by Flint to analyze ts_rdd. For example, the following will produce a new column named value_sum. For each row, value_sum will contain the summation of values that occurred within the past 2 seconds from the timestamp of that row:\n\n\nimport com.twosigma.flint.timeseries.Windows\nimport com.twosigma.flint.timeseries.Summarizers\n\nval window = Windows.pastAbsoluteTime(\"2s\")\nval summarizer = Summarizers.sum(\"value\")\nval result = ts_rdd.summarizeWindows(window, summarizer)\n\nresult.toDF.show()\n\n\n    +-------------------+-----+---------+\n    |               time|value|value_sum|\n    +-------------------+-----+---------+\n    |1970-01-01 00:00:01|    1|      1.0|\n    |1970-01-01 00:00:02|    4|      5.0|\n    |1970-01-01 00:00:03|    9|     14.0|\n    |1970-01-01 00:00:04|   16|     29.0|\n    +-------------------+-----+---------+\n     In other words, given a timestamp t and a row in the result having time equal to t, one can notice the value_sum column of that row contains sum of values within the time window of [t - 2, t] from ts_rdd.\nIntro to sparklyr.flint\nThe purpose of sparklyr.flint is to make time-series functionalities of Flint easily accessible from sparklyr. To see sparklyr.flint in action, one can skim through the example in the previous section, go through the following to produce the exact R-equivalent of each step in that example, and then obtain the same summarization as the final result:\nFirst of all, install sparklyr and sparklyr.flint if you haven’t done so already.\n\n\ninstall.packages(\"sparklyr\")\ninstall.packages(\"sparklyr.flint\")\n\nConnect to Apache Spark that is running locally from sparklyr, but remember to attach sparklyr.flint before running sparklyr::spark_connect, and then import our example time-series data to Spark:\n\n\nlibrary(sparklyr)\nlibrary(sparklyr.flint)\n\nsc <- spark_connect(master = \"local\", version = \"2.4\")\nsdf <- copy_to(sc, data.frame(time = seq(4), value = seq(4)^2))\n\nConvert sdf above into a TimeSeriesRDD\n\n\nts_rdd <- fromSDF(sdf, is_sorted = TRUE, time_unit = \"SECONDS\", time_column = \"time\")\n\nAnd finally, run the ‘sum’ summarizer to obtain a summation of values in all past-2-second time windows:\n\n\nresult <- summarize_sum(ts_rdd, column = \"value\", window = in_past(\"2s\"))\n\nprint(result %>% collect())\n\n\n\n## # A tibble: 4 x 3\n##   time                value value_sum\n##   <dttm>              <dbl>     <dbl>\n## 1 1970-01-01 00:00:01     1         1\n## 2 1970-01-01 00:00:02     4         5\n## 3 1970-01-01 00:00:03     9        14\n## 4 1970-01-01 00:00:04    16        29\n\nWhy create a sparklyr extension?\nThe alternative to making sparklyr.flint a sparklyr extension is to bundle all time-series functionalities it provides with sparklyr itself. We decided that this would not be a good idea because of the following reasons:\nNot all sparklyr users will need those time-series functionalities\ncom.twosigma:flint:0.6.0 and all Maven packages it transitively relies on are quite heavy dependency-wise\nImplementing an intuitive R interface for Flint also takes a non-trivial number of R source files, and making all of that part of sparklyr itself would be too much\nSo, considering all of the above, building sparklyr.flint as an extension of sparklyr seems to be a much more reasonable choice.\nCurrent state of sparklyr.flint and its future directions\nRecently sparklyr.flint has had its first successful release on CRAN. At the moment, sparklyr.flint only supports the summarizeCycle and summarizeWindow functionalities of Flint, and does not yet support asof join and other useful time-series operations. While sparklyr.flint contains R interfaces to most of the summarizers in Flint (one can find the list of summarizers currently supported by sparklyr.flint in here), there are still a few of them missing (e.g., the support for OLSRegressionSummarizer, among others).\nIn general, the goal of building sparklyr.flint is for it to be a thin “translation layer” between sparklyr and Flint. It should be as simple and intuitive as possibly can be, while supporting a rich set of Flint time-series functionalities.\nWe cordially welcome any open-source contribution towards sparklyr.flint. Please visit https://github.com/r-spark/sparklyr.flint/issues if you would like to initiate discussions, report bugs, or propose new features related to sparklyr.flint, and https://github.com/r-spark/sparklyr.flint/pulls if you would like to send pull requests.\nAcknowledgement\nFirst and foremost, the author wishes to thank Javier (@javierluraschi) for proposing the idea of creating sparklyr.flint as the R interface for Flint, and for his guidance on how to build it as an extension to sparklyr.\nBoth Javier (@javierluraschi) and Daniel (@dfalbel) have offered numerous helpful tips on making the initial submission of sparklyr.flint to CRAN successful.\nWe really appreciate the enthusiasm from sparklyr users who were willing to give sparklyr.flint a try shortly after it was released on CRAN (and there were quite a few downloads of sparklyr.flint in the past week according to CRAN stats, which was quite encouraging for us to see). We hope you enjoy using sparklyr.flint.\nThe author is also grateful for valuable editorial suggestions from Mara (@batpigandme), Sigrid (@skeydan), and Javier (@javierluraschi) on this blog post.\nThanks for reading!\n\n\n",
    "preview": "posts/2020-09-07-sparklyr-flint/images/thumb.png",
    "last_modified": "2020-09-30T20:24:31+02:00",
    "input_file": {},
    "preview_width": 126,
    "preview_height": 77
  },
  {
    "path": "posts/2020-09-01-weather-prediction/",
    "title": "An introduction to weather forecasting with deep learning",
    "description": "A few weeks ago, we showed how to forecast chaotic dynamical systems with deep learning, augmented by a custom constraint derived from domain-specific insight. Global weather is a chaotic system, but of much higher complexity than many tasks commonly addressed with machine and/or deep learning. In this post, we provide a practical introduction featuring a simple deep learning baseline for atmospheric forecasting. While far away from being competitive, it serves to illustrate how more sophisticated and compute-intensive models may approach that formidable task by means of methods situated on the \"black-box end\" of the continuum.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-09-01",
    "categories": [
      "R",
      "TensorFlow/Keras",
      "Time Series"
    ],
    "contents": "\nWith all that is going on in the world these days, is it frivolous to talk about weather prediction? Asked in the 21st century, this is bound to be a rhetorical question. In the 1930s, when German poet Bertolt Brecht wrote the famous lines:\n\nWas sind das für Zeiten, wo\nEin Gespräch über Bäume fast ein Verbrechen ist\nWeil es ein Schweigen über so viele Untaten einschließt!1\n\n(“What kind of times are these, where a conversation about trees is almost a crime, for it means silence about so many atrocities!”),\nhe couldn’t have anticipated the responses he would get in the second half of that century, with trees symbolizing, as well as literally falling victim to, environmental pollution and climate change.\nToday, no lengthy justification is needed as to why prediction of atmospheric states is vital: Due to global warming, frequency and intensity of severe weather conditions – droughts, wildfires, hurricanes, heatwaves – have risen and will continue to rise. And while accurate forecasts don’t change those events per se, they constitute essential information in mitigating their consequences. This goes for atmospheric forecasts on all scales: from so-called “nowcasting” (operating on a range of about six hours), over medium-range (three to five days) and sub-seasonal (weekly/monthly), to climate forecasts (concerned with years and decades). Medium-range forecasts especially are extremely important in acute disaster prevention.\nThis post will show how deep learning (DL) methods can be used to generate atmospheric forecasts, using a newly published benchmark dataset(Rasp et al. 2020). Future posts may refine the model used here and/or discuss the role of DL (“AI”) in mitigating climate change – and its implications – more globally.\nThat said, let’s put the current endeavor in context. In a way, we have here the usual dejà vu of using DL as a black-box-like, magic instrument on a task where human knowledge used to be required. Of course, this characterization is overly dichotomizing; many choices are made in creating DL models, and performance is necessarily constrained by available algorithms – which may, or may not, fit the domain to be modeled to a sufficient degree.\nIf you’ve started learning about image recognition rather recently, you may well have been using DL methods from the outset, and not have heard much about the rich set of feature engineering methods developed in pre-DL image recognition. In the context of atmospheric prediction, then, let’s begin by asking: How in the world did they do that before?\nNumerical weather prediction in a nutshell\nIt is not like machine learning and/or statistics are not already used in numerical weather prediction – on the contrary. For example, every model has to start from somewhere; but raw observations are not suited to direct use as initial conditions. Instead, they have to be assimilated to the four-dimensional grid2 over which model computations are performed. At the other end, namely, model output, statistical post-processing is used to refine the predictions. And very importantly, ensemble forecasts are employed to determine uncertainty.\nThat said, the model core, the part that extrapolates into the future atmospheric conditions observed today, is based on a set of differential3 equations, the so-called primitive equations, that are due to the conservation laws of momentum, energy, and mass. These differential equations cannot be solved analytically; rather, they have to be solved numerically, and that on a grid of resolution as high as possible. In that light, even deep learning could appear as just “moderately resource-intensive” (dependent, though, on the model in question). So how, then, could a DL approach look?\nDeep learning models for weather prediction\nAccompanying the benchmark dataset they created, Rasp et al.(Rasp et al. 2020) provide a set of notebooks, including one demonstrating the use of a simple convolutional neural network to predict two of the available atmospheric variables, 500hPa geopotential and 850hPa temperature. Here 850hPa temperature is the (spatially varying) temperature at a fix atmospheric height of 850hPa (~ 1.5 kms)4 ; 500hPa geopotential is proportional to the (again, spatially varying) altitude associated with the pressure level in question (500hPa).\nFor this task, two-dimensional convnets, as usually employed in image processing, are a natural fit: Image width and height map to longitude and latitude of the spatial grid, respectively; target variables appear as channels. In this architecture, the time series character of the data is essentially lost: Every sample stands alone, without dependency on either past or present. In this respect, as well as given its size and simplicity, the convnet presented below is only a toy model, meant to introduce the approach as well as the application overall. It may also serve as a deep learning baseline, along with two other types of baseline commonly used in numerical weather prediction introduced below.\nDirections on how to improve on that baseline are given by recent publications. Weyn et al.(Weyn, Durran, and Caruana, n.d.), in addition to applying more geometrically-adequate spatial preprocessing, use a U-Net-based architecture instead of a plain convnet. Rasp and Thuerey (Rasp and Thuerey 2020), building on a fully convolutional, high-capacity ResNet architecture, add a key new procedural ingredient: pre-training on climate models. With their method, they are able to not just compete with physical models, but also, show evidence of the network learning about physical structure and dependencies. Unfortunately, compute facilities of this order are not available to the average individual, which is why we’ll content ourselves with demonstrating a simple toy model. Still, having seen a simple model in action, as well as the type of data it works on, should help a lot in understanding how DL can be used for weather prediction.\nDataset\nWeatherbench was explicitly created as a benchmark dataset and thus, as is common for this species, hides a lot of preprocessing and standardization effort from the user. Atmospheric data are available on an hourly basis, ranging from 1979 to 2018, at different spatial resolutions. Depending on resolution, there are about 15 to 20 measured variables, including temperature, geopotential, wind speed, and humidity. Of these variables, some are available at several pressure levels. Thus, our example makes use of a small subset of available “channels”. To save storage, network and computational resources, it also operates at the smallest available resolution.\nThis post is accompanied by executable code on Google Colaboratory, which should not just render unnecessary any copy-pasting of code snippets but also, allow for uncomplicated modification and experimentation.\nTo read in and extract the data, stored as NetCDF files, we use tidync, a high-level package built on top of ncdf4 and RNetCDF. Otherwise, availability of the usual “TensorFlow family” as well as a subset of tidyverse packages is assumed.\n\n\nlibrary(reticulate)\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(tfdatasets)\nlibrary(tfautograph)\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(lubridate)\n\nlibrary(tidync)\n\nAs already alluded to, our example makes use of two spatio-temporal series: 500hPa geopotential and 850hPa temperature. The following commands will download and unpack the respective sets of by-year files, for a spatial resolution of 5.625 degrees:\n\n\ndownload.file(\"https://dataserv.ub.tum.de/s/m1524895/download?path=%2F5.625deg%2Ftemperature_850&files=temperature_850_5.625deg.zip\",\n              \"temperature_850_5.625deg.zip\")\nunzip(\"temperature_850_5.625deg.zip\", exdir = \"temperature_850\")\n\ndownload.file(\"https://dataserv.ub.tum.de/s/m1524895/download?path=%2F5.625deg%2Fgeopotential_500&files=geopotential_500_5.625deg.zip\",\n              \"geopotential_500_5.625deg.zip\")\nunzip(\"geopotential_500_5.625deg.zip\", exdir = \"geopotential_500\")\n\nInspecting one of those files’ contents, we see that its data array is structured along three dimensions, longitude (64 different values), latitude (32) and time (8760). The data itself is z, the geopotential.\n\n\ntidync(\"geopotential_500/geopotential_500hPa_2015_5.625deg.nc\") %>% hyper_array()\n\n\nClass: tidync_data (list of tidync data arrays)\nVariables (1): 'z'\nDimension (3): lon,lat,time (64, 32, 8760)\nSource: /[...]/geopotential_500/geopotential_500hPa_2015_5.625deg.nc\nExtraction of the data array is as easy as telling tidync to read the first in the list of arrays:\n\n\nz500_2015 <- (tidync(\"geopotential_500/geopotential_500hPa_2015_5.625deg.nc\") %>%\n                hyper_array())[[1]]\n\ndim(z500_2015)\n\n\n[1] 64 32 8760\nWhile we delegate further introduction to tidync to a comprehensive blog post on the ROpenSci website, let’s at least look at a quick visualization, for which we pick the very first time point. (Extraction and visualization code is analogous for 850hPa temperature.)\n\n\nimage(z500_2015[ , , 1],\n      col = hcl.colors(20, \"viridis\"), # for temperature, the color scheme used is YlOrRd \n      xaxt = 'n',\n      yaxt = 'n',\n      main = \"500hPa geopotential\"\n)\n\nThe maps show how pressure5 and temperature strongly depend on latitude. Furthermore, it’s easy to spot the atmospheric waves:\n\n\n\nFigure 1: Spatial distribution of 500hPa geopotential and 850 hPa temperature for 2015/01/01 0:00h.\n\n\n\nFor training, validation and testing, we choose consecutive years: 2015, 2016, and 2017, respectively.\n\n\nz500_train <- (tidync(\"geopotential_500/geopotential_500hPa_2015_5.625deg.nc\") %>% hyper_array())[[1]]\n\nt850_train <- (tidync(\"temperature_850/temperature_850hPa_2015_5.625deg.nc\") %>% hyper_array())[[1]]\n\nz500_valid <- (tidync(\"geopotential_500/geopotential_500hPa_2016_5.625deg.nc\") %>% hyper_array())[[1]]\n\nt850_valid <- (tidync(\"temperature_850/temperature_850hPa_2016_5.625deg.nc\") %>% hyper_array())[[1]]\n\nz500_test <- (tidync(\"geopotential_500/geopotential_500hPa_2017_5.625deg.nc\") %>% hyper_array())[[1]]\n\nt850_test <- (tidync(\"temperature_850/temperature_850hPa_2017_5.625deg.nc\") %>% hyper_array())[[1]]\n\nSince geopotential and temperature will be treated as channels, we concatenate the corresponding arrays. To transform the data into the format needed for images, a permutation is necessary:\n\n\ntrain_all <- abind::abind(z500_train, t850_train, along = 4)\ntrain_all <- aperm(train_all, perm = c(3, 2, 1, 4))\ndim(train_all)\n\n\n[1] 8760 32 64 2\nAll data will be standardized according to mean and standard deviation as obtained from the training set:\n\n\nlevel_means <- apply(train_all, 4, mean)\nlevel_sds <- apply(train_all, 4, sd)\n\nround(level_means, 2)\n\n\n54124.91  274.8\nIn words, the mean geopotential height (see footnote 5 for more on this term), as measured at an isobaric surface of 500hPa, amounts to about 5400 metres6, while the mean temperature at the 850hPa level approximates 275 Kelvin (about 2 degrees Celsius).\n\n\ntrain <- train_all\ntrain[, , , 1] <- (train[, , , 1] - level_means[1]) / level_sds[1]\ntrain[, , , 2] <- (train[, , , 2] - level_means[2]) / level_sds[2]\n\nvalid_all <- abind::abind(z500_valid, t850_valid, along = 4)\nvalid_all <- aperm(valid_all, perm = c(3, 2, 1, 4))\n\nvalid <- valid_all\nvalid[, , , 1] <- (valid[, , , 1] - level_means[1]) / level_sds[1]\nvalid[, , , 2] <- (valid[, , , 2] - level_means[2]) / level_sds[2]\n\ntest_all <- abind::abind(z500_test, t850_test, along = 4)\ntest_all <- aperm(test_all, perm = c(3, 2, 1, 4))\n\ntest <- test_all\ntest[, , , 1] <- (test[, , , 1] - level_means[1]) / level_sds[1]\ntest[, , , 2] <- (test[, , , 2] - level_means[2]) / level_sds[2]\n\nWe’ll attempt to predict three days ahead.\n\n\nlead_time <- 3 * 24 # 3d\n\nNow all that remains to be done is construct the actual datasets.\n\n\nbatch_size <- 32\n\ntrain_x <- train %>%\n  tensor_slices_dataset() %>%\n  dataset_take(dim(train)[1] - lead_time)\n\ntrain_y <- train %>%\n  tensor_slices_dataset() %>%\n  dataset_skip(lead_time)\n\ntrain_ds <- zip_datasets(train_x, train_y) %>%\n  dataset_shuffle(buffer_size = dim(train)[1] - lead_time) %>%\n  dataset_batch(batch_size = batch_size, drop_remainder = TRUE)\n\nvalid_x <- valid %>%\n  tensor_slices_dataset() %>%\n  dataset_take(dim(valid)[1] - lead_time)\n\nvalid_y <- valid %>%\n  tensor_slices_dataset() %>%\n  dataset_skip(lead_time)\n\nvalid_ds <- zip_datasets(valid_x, valid_y) %>%\n  dataset_batch(batch_size = batch_size, drop_remainder = TRUE)\n\ntest_x <- test %>%\n  tensor_slices_dataset() %>%\n  dataset_take(dim(test)[1] - lead_time)\n\ntest_y <- test %>%\n  tensor_slices_dataset() %>%\n  dataset_skip(lead_time)\n\ntest_ds <- zip_datasets(test_x, test_y) %>%\n  dataset_batch(batch_size = batch_size, drop_remainder = TRUE)\n\nLet’s proceed to defining the model.\nBasic CNN with periodic convolutions\nThe model is a straightforward convnet, with one exception: Instead of plain convolutions, it uses slightly more sophisticated ones that “wrap around” longitudinally.\n\n\nperiodic_padding_2d <- function(pad_width,\n                                name = NULL) {\n  \n  keras_model_custom(name = name, function(self) {\n    self$pad_width <- pad_width\n    \n    function (x, mask = NULL) {\n      x <- if (self$pad_width == 0) {\n        x\n      } else {\n        lon_dim <- dim(x)[3]\n        pad_width <- tf$cast(self$pad_width, tf$int32)\n        # wrap around for longitude\n        tf$concat(list(x[, ,-pad_width:lon_dim,],\n                       x,\n                       x[, , 1:pad_width,]),\n                  axis = 2L) %>%\n          tf$pad(list(\n            list(0L, 0L),\n            # zero-pad for latitude\n            list(pad_width, pad_width),\n            list(0L, 0L),\n            list(0L, 0L)\n          ))\n      }\n    }\n  })\n}\n\nperiodic_conv_2d <- function(filters,\n                             kernel_size,\n                             name = NULL) {\n  \n  keras_model_custom(name = name, function(self) {\n    self$padding <- periodic_padding_2d(pad_width = (kernel_size - 1) / 2)\n    self$conv <-\n      layer_conv_2d(filters = filters,\n                    kernel_size = kernel_size,\n                    padding = 'valid')\n    \n    function (x, mask = NULL) {\n      x %>% self$padding() %>% self$conv()\n    }\n  })\n}\n\nFor our purposes of establishing a deep-learning baseline that is fast to train, CNN architecture and parameter defaults are chosen to be simple and moderate, respectively:7\n\n\nperiodic_cnn <- function(filters = c(64, 64, 64, 64, 2),\n                         kernel_size = c(5, 5, 5, 5, 5),\n                         dropout = rep(0.2, 5),\n                         name = NULL) {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$conv1 <-\n      periodic_conv_2d(filters = filters[1], kernel_size = kernel_size[1])\n    self$act1 <- layer_activation_leaky_relu()\n    self$drop1 <- layer_dropout(rate = dropout[1])\n    self$conv2 <-\n      periodic_conv_2d(filters = filters[2], kernel_size = kernel_size[2])\n    self$act2 <- layer_activation_leaky_relu()\n    self$drop2 <- layer_dropout(rate =dropout[2])\n    self$conv3 <-\n      periodic_conv_2d(filters = filters[3], kernel_size = kernel_size[3])\n    self$act3 <- layer_activation_leaky_relu()\n    self$drop3 <- layer_dropout(rate = dropout[3])\n    self$conv4 <-\n      periodic_conv_2d(filters = filters[4], kernel_size = kernel_size[4])\n    self$act4 <- layer_activation_leaky_relu()\n    self$drop4 <- layer_dropout(rate = dropout[4])\n    self$conv5 <-\n      periodic_conv_2d(filters = filters[5], kernel_size = kernel_size[5])\n    \n    function (x, mask = NULL) {\n      x %>%\n        self$conv1() %>%\n        self$act1() %>%\n        self$drop1() %>%\n        self$conv2() %>%\n        self$act2() %>%\n        self$drop2() %>%\n        self$conv3() %>%\n        self$act3() %>%\n        self$drop3() %>%\n        self$conv4() %>%\n        self$act4() %>%\n        self$drop4() %>%\n        self$conv5()\n    }\n  })\n}\n\nmodel <- periodic_cnn()\n\nTraining\nIn that same spirit of “default-ness”, we train with MSE loss and Adam optimizer.\n\n\nloss <- tf$keras$losses$MeanSquaredError(reduction = tf$keras$losses$Reduction$SUM)\noptimizer <- optimizer_adam()\n\ntrain_loss <- tf$keras$metrics$Mean(name='train_loss')\n\nvalid_loss <- tf$keras$metrics$Mean(name='test_loss')\n\ntrain_step <- function(train_batch) {\n\n  with (tf$GradientTape() %as% tape, {\n    predictions <- model(train_batch[[1]])\n    l <- loss(train_batch[[2]], predictions)\n  })\n\n  gradients <- tape$gradient(l, model$trainable_variables)\n  optimizer$apply_gradients(purrr::transpose(list(\n    gradients, model$trainable_variables\n  )))\n\n  train_loss(l)\n\n}\n\nvalid_step <- function(valid_batch) {\n  predictions <- model(valid_batch[[1]])\n  l <- loss(valid_batch[[2]], predictions)\n  \n  valid_loss(l)\n}\n\ntraining_loop <- tf_function(autograph(function(train_ds, valid_ds, epoch) {\n  \n  for (train_batch in train_ds) {\n    train_step(train_batch)\n  }\n  \n  for (valid_batch in valid_ds) {\n    valid_step(valid_batch)\n  }\n  \n  tf$print(\"MSE: train: \", train_loss$result(), \", validation: \", valid_loss$result()) \n    \n}))\n\nDepicted graphically, we see that the model trains well, but extrapolation does not surpass a certain threshold (which is reached early, after training for just two epochs).\n\n\n\nFigure 2: MSE per epoch on training and validation sets.\n\n\n\nThis is not too surprising though, given the model’s architectural simplicity and modest size.\nEvaluation\nHere, we first present two other baselines, which – given a highly complex and chaotic system like the atmosphere – may sound irritatingly simple and yet, be pretty hard to beat. The metric used for comparison is latitudinally weighted root-mean-square error. Latitudinal weighting up-weights the lower latitudes and down-weights the upper ones.\n\n\ndeg2rad <- function(d) {\n  (d / 180) * pi\n}\n\nlats <- tidync(\"geopotential_500/geopotential_500hPa_2015_5.625deg.nc\")$transforms$lat %>%\n  select(lat) %>%\n  pull()\n\nlat_weights <- cos(deg2rad(lats))\nlat_weights <- lat_weights / mean(lat_weights)\n\nweighted_rmse <- function(forecast, ground_truth) {\n  error <- (forecast - ground_truth) ^ 2\n  for (i in seq_along(lat_weights)) {\n    error[, i, ,] <- error[, i, ,] * lat_weights[i]\n  }\n  apply(error, 4, mean) %>% sqrt()\n}\n\nBaseline 1: Weekly climatology\nIn general, climatology refers to long-term averages computed over defined time ranges. Here, we first calculate weekly averages based on the training set. These averages are then used to forecast the variables in question for the time period used as test set.\nStep one makes use of tidync, ncmeta, RNetCDF and lubridate to compute weekly averages for 2015, following the ISO week date system.\n\n\ntrain_file <- \"geopotential_500/geopotential_500hPa_2015_5.625deg.nc\"\n\ntimes_train <- (tidync(train_file) %>% activate(\"D2\") %>% hyper_array())$time\n\ntime_unit_train <- ncmeta::nc_atts(train_file, \"time\") %>%\n  tidyr::unnest(cols = c(value)) %>%\n  dplyr::filter(name == \"units\")\n\ntime_parts_train <- RNetCDF::utcal.nc(time_unit_train$value, times_train)\n\niso_train <- ISOdate(\n  time_parts_train[, \"year\"],\n  time_parts_train[, \"month\"],\n  time_parts_train[, \"day\"],\n  time_parts_train[, \"hour\"],\n  time_parts_train[, \"minute\"],\n  time_parts_train[, \"second\"]\n)\n\nisoweeks_train <- map(iso_train, isoweek) %>% unlist()\n\ntrain_by_week <- apply(train_all, c(2, 3, 4), function(x) {\n  tapply(x, isoweeks_train, function(y) {\n    mean(y)\n  })\n})\n\ndim(train_by_week)\n\n\n53 32 64 2\nStep two then runs through the test set, mapping dates to corresponding ISO weeks and associating the weekly averages from the training set:\n\n\ntest_file <- \"geopotential_500/geopotential_500hPa_2017_5.625deg.nc\"\n\ntimes_test <- (tidync(test_file) %>% activate(\"D2\") %>% hyper_array())$time\n\ntime_unit_test <- ncmeta::nc_atts(test_file, \"time\") %>%\n  tidyr::unnest(cols = c(value)) %>%\n  dplyr::filter(name == \"units\")\n\ntime_parts_test <- RNetCDF::utcal.nc(time_unit_test$value, times_test)\n\niso_test <- ISOdate(\n  time_parts_test[, \"year\"],\n  time_parts_test[, \"month\"],\n  time_parts_test[, \"day\"],\n  time_parts_test[, \"hour\"],\n  time_parts_test[, \"minute\"],\n  time_parts_test[, \"second\"]\n)\n\nisoweeks_test <- map(iso_test, isoweek) %>% unlist()\n\nclimatology_forecast <- test_all\n\nfor (i in 1:dim(climatology_forecast)[1]) {\n  week <- isoweeks_test[i]\n  lookup <- train_by_week[week, , , ]\n  climatology_forecast[i, , ,] <- lookup\n}\n\nFor this baseline, the latitudinally-weighted RMSE amounts to roughly 975 for geopotential and 4 for temperature.\n\n\nwrmse <- weighted_rmse(climatology_forecast, test_all)\nround(wrmse, 2)\n\n\n974.50   4.09\nBaseline 2: Persistence forecast\nThe second baseline commonly used makes a straightforward assumption: Tomorrow’s weather is today’s weather, or, in our case: In three days, things will be just like they are now.\nComputation for this metric is almost a one-liner. And as it turns out, for the given lead time (three days), performance is not too dissimilar from obtained by means of weekly climatology:\n\n\npersistence_forecast <- test_all[1:(dim(test_all)[1] - lead_time), , ,]\n\ntest_period <- test_all[(lead_time + 1):dim(test_all)[1], , ,]\n\nwrmse <- weighted_rmse(persistence_forecast, test_period)\n\nround(wrmse, 2)\n\n\n937.55  4.31\nBaseline 3: Simple convnet\nHow does the simple deep learning model stack up against those two?\nTo answer that question, we first need to obtain predictions on the test set.\n\n\ntest_wrmses <- data.frame()\n\ntest_loss <- tf$keras$metrics$Mean(name = 'test_loss')\n\ntest_step <- function(test_batch, batch_index) {\n  predictions <- model(test_batch[[1]])\n  l <- loss(test_batch[[2]], predictions)\n  \n  predictions <- predictions %>% as.array()\n  predictions[, , , 1] <- predictions[, , , 1] * level_sds[1] + level_means[1]\n  predictions[, , , 2] <- predictions[, , , 2] * level_sds[2] + level_means[2]\n  \n  wrmse <- weighted_rmse(predictions, test_all[batch_index:(batch_index + 31), , ,])\n  test_wrmses <<- test_wrmses %>% bind_rows(c(z = wrmse[1], temp = wrmse[2]))\n\n  test_loss(l)\n}\n\ntest_iterator <- as_iterator(test_ds)\n\nbatch_index <- 0\nwhile (TRUE) {\n  test_batch <- test_iterator %>% iter_next()\n  if (is.null(test_batch))\n    break\n  batch_index <- batch_index + 1\n  test_step(test_batch, as.integer(batch_index))\n}\n\ntest_loss$result() %>% as.numeric()\n\n\n3821.016\nThus, average loss on the test set parallels that seen on the validation set. As to latitudinally weighted RMSE, it turns out to be higher for the DL baseline than for the other two:\n\n\napply(test_wrmses, 2, mean) %>% round(2)\n\n\n      z    temp \n1521.47    7.70 \nConclusion\nAt first glance, seeing the DL baseline perform worse than the others might feel anticlimactic. But if you think about it, there is no need to be disappointed.\nFor one, given the enormous complexity of the task, these heuristics are not as easy to outsmart. Take persistence: Depending on lead time - how far into the future we’re forecasting - the wisest guess may actually be that everything will stay the same. What would you guess the weather will look like in five minutes? — Same with weekly climatology: Looking back at how warm it was, at a given location, that same week two years ago, does not in general sound like a bad strategy.\nSecond, the DL baseline shown is as basic as it can get, architecture- as well as parameter-wise. More sophisticated and powerful architectures have been developed that not just by far surpass the baselines, but can even compete with physical models (cf. especially Rasp and Thuerey (Rasp and Thuerey 2020) already mentioned above). Unfortunately, models like that need to be trained on a lot of data.\nHowever, other weather-related applications (other than medium-range forecasting, that is) may be more in reach for individuals interested in the topic. For those, we hope we have given a useful introduction. Thanks for reading!\n\n\nRasp, Stephan, Peter D. Dueben, Sebastian Scher, Jonathan A. Weyn, Soukayna Mouatadid, and Nils Thuerey. 2020. “WeatherBench: A benchmark dataset for data-driven weather forecasting.” arXiv E-Prints, February, arXiv:2002.00469. http://arxiv.org/abs/2002.00469.\n\n\nRasp, Stephan, and Nils Thuerey. 2020. “Purely Data-Driven Medium-Range Weather Forecasting Achieves Comparable Skill to Physical Models at Similar Resolution.” http://arxiv.org/abs/2008.08626.\n\n\nWeyn, Jonathan A., Dale R. Durran, and Rich Caruana. n.d. “Improving Data-Driven Global Weather Prediction Using Deep Convolutional Neural Networks on a Cubed Sphere.” Journal of Advances in Modeling Earth Systems n/a (n/a): e2020MS002109. https://doi.org/10.1029/2020MS002109.\n\n\nAn die Nachgeborenen, 1934-38. The atrocities referred to are those of Nazi Germany.↩︎\nFour, because in addition to three spatial dimensions, there is the time dimension.↩︎\nmostly↩︎\nPressure and altitude are related by the barometric equation. On weather maps, pressure is often used to represent the vertical dimension.↩︎\nWhereas we normally might think of atmospheric pressure as varying at a fixed height (for example, at sea level), meteorologists like to display things the other way round, displaying variable heights at fixed constant-pressure (isobaric) surfaces. Still, intuitively these may be read in the same way: High pressure at a given location means that some predefined pressure level is attained at higher altitude than in some low-pressure location. To be precise, these kinds of “inverted pressure maps” normally display geopotential height, measured in metres, not geopotential, the variable we’re dealing with here. Geopotential (without the “height”) refers to gravitational potential energy per unit mass; it is obtained by multiplying by gravitational acceleration, and is measured in metres squared per second squared. The measures are not a hundred percent equivalent, because gravitational acceleration varies with latitude and longitude as well as elevation.↩︎\nAs explained in the previous footnote, geopotential height is geopotential divided by standard gravitational acceleration, roughly, 9.8 metres per seconds squared.↩︎\nThese are the same filter and kernel sizes as employed in Rasp et al.'s simple CNN example on github.↩︎\n",
    "preview": "posts/2020-09-01-weather-prediction/images/thumb.png",
    "last_modified": "2020-09-30T20:24:31+02:00",
    "input_file": {},
    "preview_width": 600,
    "preview_height": 332
  },
  {
    "path": "posts/2020-08-24-training-imagenet-with-r/",
    "title": "Training ImageNet with R",
    "description": "This post explores how to train large datasets with TensorFlow and R. Specifically, we present how to download and repartition ImageNet, followed by training ImageNet across multiple GPUs in distributed environments using TensorFlow and Apache Spark.",
    "author": [
      {
        "name": "Javier Luraschi",
        "url": {}
      }
    ],
    "date": "2020-08-24",
    "categories": [
      "R",
      "TensorFlow/Keras",
      "Distributed Computing",
      "Data Management"
    ],
    "contents": "\nImageNet (Deng et al. 2009) is an image database organized according to the WordNet (Miller 1995) hierarchy which, historically, has been used in computer vision benchmarks and research. However, it was not until AlexNet (Krizhevsky, Sutskever, and Hinton 2012) demonstrated the efficiency of deep learning using convolutional neural networks on GPUs that the computer-vision discipline turned to deep learning to achieve state-of-the-art models that revolutionized their field. Given the importance of ImageNet and AlexNet, this post introduces tools and techniques to consider when training ImageNet and other large-scale datasets with R.\nNow, in order to process ImageNet, we will first have to divide and conquer, partitioning the dataset into several manageable subsets. Afterwards, we will train ImageNet using AlexNet across multiple GPUs and compute instances. Preprocessing ImageNet and distributed training are the two topics that this post will present and discuss, starting with preprocessing ImageNet.\nPreprocessing ImageNet\nWhen dealing with large datasets, even simple tasks like downloading or reading a dataset can be much harder than what you would expect. For instance, since ImageNet is roughly 300GB in size, you will need to make sure to have at least 600GB of free space to leave some room for download and decompression. But no worries, you can always borrow computers with huge disk drives from your favorite cloud provider. While you are at it, you should also request compute instances with multiple GPUs, Solid State Drives (SSDs), and a reasonable amount of CPUs and memory. If you want to use the exact configuration we used, take a look at the mlverse/imagenet repo, which contains a Docker image and configuration commands required to provision reasonable computing resources for this task. In summary, make sure you have access to sufficient compute resources.\nNow that we have resources capable of working with ImageNet, we need to find a place to download ImageNet from. The easiest way is to use a variation of ImageNet used in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which contains a subset of about 250GB of data and can be easily downloaded from many Kaggle competitions, like the ImageNet Object Localization Challenge.\nIf you’ve read some of our previous posts, you might be already thinking of using the pins package, which you can use to: cache, discover and share resources from many services, including Kaggle. You can learn more about data retrieval from Kaggle in the Using Kaggle Boards article; in the meantime, let’s assume you are already familiar with this package.\nAll we need to do now is register the Kaggle board, retrieve ImageNet as a pin, and decompress this file. Warning, the following code requires you to stare at a progress bar for, potentially, over an hour.\n\n\nlibrary(pins)\nboard_register(\"kaggle\", token = \"kaggle.json\")\n\npin_get(\"c/imagenet-object-localization-challenge\", board = \"kaggle\")[1] %>%\n  untar(exdir = \"/localssd/imagenet/\")\n\nIf we are going to be training this model over and over using multiple GPUs and even multiple compute instances, we want to make sure we don’t waste too much time downloading ImageNet every single time.\nThe first improvement to consider is getting a faster hard drive. In our case, we locally-mounted an array of SSDs into the /localssd path. We then used /localssd to extract ImageNet and configured R’s temp path and pins cache to use the SSDs as well. Consult your cloud provider’s documentation to configure SSDs, or take a look at mlverse/imagenet.\nNext, a well-known approach we can follow is to partition ImageNet into chunks that can be individually downloaded to perform distributed training later on.\nIn addition, it is also faster to download ImageNet from a nearby location, ideally from a URL stored within the same data center where our cloud instance is located. For this, we can also use pins to register a board with our cloud provider and then re-upload each partition. Since ImageNet is already partitioned by category, we can easily split ImageNet into multiple zip files and re-upload to our closest data center as follows. Make sure the storage bucket is created in the same region as your computing instances.\n\n\nboard_register(\"<board>\", name = \"imagenet\", bucket = \"r-imagenet\")\n\ntrain_path <- \"/localssd/imagenet/ILSVRC/Data/CLS-LOC/train/\"\nfor (path in dir(train_path, full.names = TRUE)) {\n  dir(path, full.names = TRUE) %>%\n    pin(name = basename(path), board = \"imagenet\", zip = TRUE)\n}\n\nWe can now retrieve a subset of ImageNet quite efficiently. If you are motivated to do so and have about one gigabyte to spare, feel free to follow along executing this code. Notice that ImageNet contains lots of JPEG images for each WordNet category.\n\n\nboard_register(\"https://storage.googleapis.com/r-imagenet/\", \"imagenet\")\n\ncategories <- pin_get(\"categories\", board = \"imagenet\")\npin_get(categories$id[1], board = \"imagenet\", extract = TRUE) %>%\n  tibble::as_tibble()\n\n\n# A tibble: 1,300 x 1\n   value                                                           \n   <chr>                                                           \n 1 /localssd/pins/storage/n01440764/n01440764_10026.JPEG\n 2 /localssd/pins/storage/n01440764/n01440764_10027.JPEG\n 3 /localssd/pins/storage/n01440764/n01440764_10029.JPEG\n 4 /localssd/pins/storage/n01440764/n01440764_10040.JPEG\n 5 /localssd/pins/storage/n01440764/n01440764_10042.JPEG\n 6 /localssd/pins/storage/n01440764/n01440764_10043.JPEG\n 7 /localssd/pins/storage/n01440764/n01440764_10048.JPEG\n 8 /localssd/pins/storage/n01440764/n01440764_10066.JPEG\n 9 /localssd/pins/storage/n01440764/n01440764_10074.JPEG\n10 /localssd/pins/storage/n01440764/n01440764_1009.JPEG \n# … with 1,290 more rows\nWhen doing distributed training over ImageNet, we can now let a single compute instance process a partition of ImageNet with ease. Say, 1/16 of ImageNet can be retrieved and extracted, in under a minute, using parallel downloads with the callr package:\n\n\ncategories <- pin_get(\"categories\", board = \"imagenet\")\ncategories <- categories$id[1:(length(categories$id) / 16)]\n\nprocs <- lapply(categories, function(cat)\n  callr::r_bg(function(cat) {\n    library(pins)\n    board_register(\"https://storage.googleapis.com/r-imagenet/\", \"imagenet\")\n    \n    pin_get(cat, board = \"imagenet\", extract = TRUE)\n  }, args = list(cat))\n)\n  \nwhile (any(sapply(procs, function(p) p$is_alive()))) Sys.sleep(1)\n\nWe can wrap this up partition in a list containing a map of images and categories, which we will later use in our AlexNet model through tfdatasets.\n\n\ndata <- list(\n    image = unlist(lapply(categories, function(cat) {\n        pin_get(cat, board = \"imagenet\", download = FALSE)\n    })),\n    category = unlist(lapply(categories, function(cat) {\n        rep(cat, length(pin_get(cat, board = \"imagenet\", download = FALSE)))\n    })),\n    categories = categories\n)\n\nGreat! We are halfway there training ImageNet. The next section will focus on introducing distributed training using multiple GPUs.\nDistributed Training\nNow that we have broken down ImageNet into manageable parts, we can forget for a second about the size of ImageNet and focus on training a deep learning model for this dataset. However, any model we choose is likely to require a GPU, even for a 1/16 subset of ImageNet. So make sure your GPUs are properly configured by running is_gpu_available(). If you need help getting a GPU configured, the Using GPUs with TensorFlow and Docker video can help you get up to speed.\n\n\nlibrary(tensorflow)\ntf$test$is_gpu_available()\n\n\n[1] TRUE\nWe can now decide which deep learning model would best be suited for ImageNet classification tasks. Instead, for this post, we will go back in time to the glory days of AlexNet and use the r-tensorflow/alexnet repo instead. This repo contains a port of AlexNet to R, but please notice that this port has not been tested and is not ready for any real use cases. In fact, we would appreciate PRs to improve it if someone feels inclined to do so. Regardless, the focus of this post is on workflows and tools, not about achieving state-of-the-art image classification scores. So by all means, feel free to use more appropriate models.\nOnce we’ve chosen a model, we will want to me make sure that it properly trains on a subset of ImageNet:\n\n\nremotes::install_github(\"r-tensorflow/alexnet\")\nalexnet::alexnet_train(data = data)\n\n\nEpoch 1/2\n 103/2269 [>...............] - ETA: 5:52 - loss: 72306.4531 - accuracy: 0.9748\nSo far so good! However, this post is about enabling large-scale training across multiple GPUs, so we want to make sure we are using as many as we can. Unfortunately, running nvidia-smi will show that only one GPU currently being used:\n\n\nnvidia-smi\n\n\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 418.152.00   Driver Version: 418.152.00   CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   48C    P0    89W / 149W |  10935MiB / 11441MiB |     28%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           Off  | 00000000:00:06.0 Off |                    0 |\n| N/A   74C    P0    74W / 149W |     71MiB / 11441MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\nIn order to train across multiple GPUs, we need to define a distributed-processing strategy. If this is a new concept, it might be a good time to take a look at the Distributed Training with Keras tutorial and the distributed training with TensorFlow docs. Or, if you allow us to oversimplify the process, all you have to do is define and compile your model under the right scope. A step-by-step explanation is available in the Distributed Deep Learning with TensorFlow and R video. In this case, the alexnet model already supports a strategy parameter, so all we have to do is pass it along.\n\n\nlibrary(tensorflow)\nstrategy <- tf$distribute$MirroredStrategy(\n  cross_device_ops = tf$distribute$ReductionToOneDevice())\n\nalexnet::alexnet_train(data = data, strategy = strategy, parallel = 6)\n\nNotice also parallel = 6 which configures tfdatasets to make use of multiple CPUs when loading data into our GPUs, see Parallel Mapping for details.\nWe can now re-run nvidia-smi to validate all our GPUs are being used:\n\n\nnvidia-smi\n\n\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 418.152.00   Driver Version: 418.152.00   CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   49C    P0    94W / 149W |  10936MiB / 11441MiB |     53%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           Off  | 00000000:00:06.0 Off |                    0 |\n| N/A   76C    P0   114W / 149W |  10936MiB / 11441MiB |     26%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\nThe MirroredStrategy can help us scale up to about 8 GPUs per compute instance; however, we are likely to need 16 instances with 8 GPUs each to train ImageNet in a reasonable time (see Jeremy Howard’s post on Training Imagenet in 18 Minutes). So where do we go from here?\nWelcome to MultiWorkerMirroredStrategy: This strategy can use not only multiple GPUs, but also multiple GPUs across multiple computers. To configure them, all we have to do is define a TF_CONFIG environment variable with the right addresses and run the exact same code in each compute instance.\n\n\nlibrary(tensorflow)\n\npartition <- 0\nSys.setenv(TF_CONFIG = jsonlite::toJSON(list(\n    cluster = list(\n        worker = c(\"10.100.10.100:10090\", \"10.100.10.101:10090\")\n    ),\n    task = list(type = 'worker', index = partition)\n), auto_unbox = TRUE))\n\nstrategy <- tf$distribute$MultiWorkerMirroredStrategy(\n  cross_device_ops = tf$distribute$ReductionToOneDevice())\n\nalexnet::imagenet_partition(partition = partition) %>%\n  alexnet::alexnet_train(strategy = strategy, parallel = 6)\n\nPlease note that partition must change for each compute instance to uniquely identify it, and that the IP addresses also need to be adjusted. In addition, data should point to a different partition of ImageNet, which we can retrieve with pins; although, for convenience, alexnet contains similar code under alexnet::imagenet_partition(). Other than that, the code that you need to run in each compute instance is exactly the same.\nHowever, if we were to use 16 machines with 8 GPUs each to train ImageNet, it would be quite time-consuming and error-prone to manually run code in each R session. So instead, we should think of making use of cluster-computing frameworks, like Apache Spark with barrier execution. If you are new to Spark, there are many resources available at sparklyr.ai. To learn just about running Spark and TensorFlow together, watch our Deep Learning with Spark, TensorFlow and R video.\nPutting it all together, training ImageNet in R with TensorFlow and Spark looks as follows:\n\n\nlibrary(sparklyr)\nsc <- spark_connect(\"yarn|mesos|etc\", config = list(\"sparklyr.shell.num-executors\" = 16))\n\nsdf_len(sc, 16, repartition = 16) %>%\n  spark_apply(function(df, barrier) {\n      library(tensorflow)\n\n      Sys.setenv(TF_CONFIG = jsonlite::toJSON(list(\n        cluster = list(\n          worker = paste(\n            gsub(\":[0-9]+$\", \"\", barrier$address),\n            8000 + seq_along(barrier$address), sep = \":\")),\n        task = list(type = 'worker', index = barrier$partition)\n      ), auto_unbox = TRUE))\n      \n      if (is.null(tf_version())) install_tensorflow()\n      \n      strategy <- tf$distribute$MultiWorkerMirroredStrategy()\n    \n      result <- alexnet::imagenet_partition(partition = barrier$partition) %>%\n        alexnet::alexnet_train(strategy = strategy, epochs = 10, parallel = 6)\n      \n      result$metrics$accuracy\n  }, barrier = TRUE, columns = c(accuracy = \"numeric\"))\n\nWe hope this post gave you a reasonable overview of what training large-datasets in R looks like – thanks for reading along!\n\n\nDeng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. “Imagenet: A Large-Scale Hierarchical Image Database.” In 2009 Ieee Conference on Computer Vision and Pattern Recognition, 248–55. Ieee.\n\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. “Imagenet Classification with Deep Convolutional Neural Networks.” In Advances in Neural Information Processing Systems, 1097–1105.\n\n\nMiller, George A. 1995. “WordNet: A Lexical Database for English.” Communications of the ACM 38 (11): 39–41.\n\n\n\n\n",
    "preview": "posts/2020-08-24-training-imagenet-with-r/images/fishing-net.jpg",
    "last_modified": "2020-09-30T20:24:31+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-08-18-deepfake/",
    "title": "Deepfake detection challenge from R",
    "description": "A couple of months ago, Amazon, Facebook, Microsoft, and other contributors initiated a challenge consisting of telling apart real and AI-generated (\"fake\") videos. We show how to approach this challenge from R.",
    "author": [
      {
        "name": "Turgut Abdullayev",
        "url": "https://github.com/henry090"
      }
    ],
    "date": "2020-08-18",
    "categories": [
      "Image Recognition & Image Processing"
    ],
    "contents": "\nTable of Contents\nIntroduction\nData exploration\nFrame extraction\nFace detection\nFace extraction\nDeep learning model\nConclusion\n\n.colab-root {\n    display: inline-block;\n    background: rgba(255, 255, 255, 0.75);\n    padding: 4px 8px;\n    border-radius: 4px;\n    font-size: 11px!important;\n    text-decoration: none;\n    color: #aaa;\n    border: none;\n    font-weight: 300;\n    border: solid 1px rgba(0, 0, 0, 0.08);\n    border-bottom-color: rgba(0, 0, 0, 0.15);\n    text-transform: uppercase;\n    line-height: 16px;\n}\nspan.colab-span {\n    background-image: url(https://distill.pub/2020/growing-ca/images/colab.svg);\n    background-repeat: no-repeat;\n    background-size: 20px;\n    background-position-y: 2px;\n    display: inline-block;\n    padding-left: 24px;\n    border-radius: 4px;\n    text-decoration: none;\n}\nIntroduction\nWorking with video datasets, particularly with respect to detection of AI-based fake objects, is very challenging due to proper frame selection and face detection. To approach this challenge from R, one can make use of capabilities offered by OpenCV, magick, and keras.\nOur approach consists of the following consequent steps:\nread all the videos\ncapture and extract images from the videos\ndetect faces from the extracted images\ncrop the faces\nbuild an image classification model with Keras\nLet’s quickly introduce the non-deep-learning libraries we’re using. OpenCV is a computer vision library that includes:\nFacial recognition technology\nMotion tracking\nAugmented reality\nand more.\nOn the other hand, magick is the open-source image-processing library that will help to read and extract useful features from video datasets:\nRead video files\nExtract images per second from the video\nCrop the faces from the images\nBefore we go into a detailed explanation, readers should know that there is no need to copy-paste code chunks. Because at the end of the post one can find a link to Google Colab with GPU acceleration. This kernel allows everyone to run and reproduce the same results.\nData exploration\nThe dataset that we are going to analyze is provided by AWS, Facebook, Microsoft, the Partnership on AI’s Media Integrity Steering Committee, and various academics.\nIt contains both real and AI-generated fake videos. The total size is over 470 GB. However, the sample 4 GB dataset is separately available.\nFrame extraction\nThe videos in the folders are in the format of mp4 and have various lengths. Our task is to determine the number of images to capture per second of a video. We usually took 1-3 fps for every video.\n\nNote: Set fps to NULL if you want to extract all frames.\n\n\n\nvideo = magick::image_read_video(\"aagfhgtpmv.mp4\",fps = 2)\nvid_1 = video[[1]]\nvid_1 = magick::image_read(vid_1) %>% image_resize('1000x1000')\n\n\n\n\n\nFigure 1: Deepfake detection challenge\n\n\n\n\nWe saw just the first frame. What about the rest of them?\n\n\n\nFigure 2: Deepfake detection challenge\n\n\n\nLooking at the gif one can observe that some fakes are very easy to differentiate, but a small fraction looks pretty realistic. This is another challenge during data preparation.\nFace detection\nAt first, face locations need to be determined via bounding boxes, using OpenCV. Then, magick is used to automatically extract them from all images.\n\n\n# get face location and calculate bounding box\nlibrary(opencv)\nunconf <- ocv_read('frame_1.jpg')\nfaces <- ocv_face(unconf)\nfacemask <- ocv_facemask(unconf)\ndf = attr(facemask, 'faces')\nrectX = (df$x - df$radius) \nrectY = (df$y - df$radius)\nx = (df$x + df$radius) \ny = (df$y + df$radius)\n\n# draw with red dashed line the box\nimh  = image_draw(image_read('frame_1.jpg'))\nrect(rectX, rectY, x, y, border = \"red\", \n     lty = \"dashed\", lwd = 2)\ndev.off()\n\n\n\n\n\nFigure 3: Deepfake detection challenge\n\n\n\n\nFace extraction\nIf face locations are found, then it is very easy to extract them all.\n\n\nedited = image_crop(imh, \"49x49+66+34\")\nedited = image_crop(imh, paste(x-rectX+1,'x',x-rectX+1,'+',rectX, '+',rectY,sep = ''))\nedited\n\n\n\n\n\nFigure 4: Deepfake detection challenge\n\n\n\n\nDeep learning model\nAfter dataset preparation, it is time to build a deep learning model with Keras. We can quickly place all the images into folders and, using image generators, feed faces to a pre-trained Keras model.\n\n\ntrain_dir = 'fakes_reals'\nwidth = 150L\nheight = 150L\nepochs = 10\n\ntrain_datagen = image_data_generator(\n  rescale = 1/255,\n  rotation_range = 40,\n  width_shift_range = 0.2,\n  height_shift_range = 0.2,\n  shear_range = 0.2,\n  zoom_range = 0.2,\n  horizontal_flip = TRUE,\n  fill_mode = \"nearest\",\n  validation_split=0.2\n)\n\n\ntrain_generator <- flow_images_from_directory(\n  train_dir,                  \n  train_datagen,             \n  target_size = c(width,height), \n  batch_size = 10,\n  class_mode = \"binary\"\n)\n\n# Build the model ---------------------------------------------------------\n\nconv_base <- application_vgg16(\n  weights = \"imagenet\",\n  include_top = FALSE,\n  input_shape = c(width, height, 3)\n)\n\nmodel <- keras_model_sequential() %>% \n  conv_base %>% \n  layer_flatten() %>% \n  layer_dense(units = 256, activation = \"relu\") %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %>% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = optimizer_rmsprop(lr = 2e-5),\n  metrics = c(\"accuracy\")\n)\n\nhistory <- model %>% fit_generator(\n  train_generator,\n  steps_per_epoch = ceiling(train_generator$samples/train_generator$batch_size),\n  epochs = 10\n)\n\n\nReproduce in a Notebook\n\nConclusion\nThis post shows how to do video classification from R. The steps were:\nRead videos and extract images from the dataset\nApply OpenCV to detect faces\nExtract faces via bounding boxes\nBuild a deep learning model\nHowever, readers should know that the implementation of the following steps may drastically improve model performance:\nextract all of the frames from the video files\nload different pre-trained weights, or use different pre-trained models\nuse another technology to detect faces – e.g., “MTCNN face detector”\nFeel free to try these options on the Deepfake detection challenge and share your results in the comments section!\nThanks for reading!\n\n\n",
    "preview": "posts/2020-08-18-deepfake/files/frame_2.jpg",
    "last_modified": "2020-09-30T20:24:31+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-07-31-fnn-vae-for-noisy-timeseries/",
    "title": "FNN-VAE for noisy time series forecasting",
    "description": "In the last part of this mini-series on forecasting with false nearest neighbors (FNN) loss, we replace the LSTM autoencoder from the previous post by a convolutional VAE, resulting in equivalent prediction performance but significantly lower training time. In addition, we find that FNN regularization is of great help when an underlying deterministic process is obscured by substantial noise.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-07-31",
    "categories": [
      "R",
      "TensorFlow/Keras",
      "Time Series",
      "Unsupervised Learning"
    ],
    "contents": "\nThis post did not end up quite the way I’d imagined. A quick follow-up on the recent Time series prediction with FNN-LSTM, it was supposed to demonstrate how noisy time series (so common in practice) could profit from a change in architecture: Instead of FNN-LSTM, an LSTM autoencoder regularized by false nearest neighbors (FNN) loss, use FNN-VAE, a variational autoencoder constrained by the same. However, FNN-VAE did not seem to handle noise better than FNN-LSTM. No plot, no post, then?\nOn the other hand – this is not a scientific study, with hypothesis and experimental setup all preregistered; all that really matters is if there’s something useful to report. And it looks like there is.\nFirstly, FNN-VAE, while on par performance-wise with FNN-LSTM, is far superior in that other meaning of “performance”: Training goes a lot faster for FNN-VAE.\nSecondly, while we don’t see much difference between FNN-LSTM and FNN-VAE, we do see a clear impact of using FNN loss. Adding in FNN loss strongly reduces mean squared error with respect to the underlying (denoised) series – especially in the case of VAE, but for LSTM as well. This is of particular interest with VAE, as it comes with a regularizer out-of-the-box – namely, Kullback-Leibler (KL) divergence.\nOf course, we don’t claim that similar results will always be obtained on other noisy series; nor did we tune any of the models “to death”. For what could be the intent of such a post but to show our readers interesting (and promising) ideas to pursue in their own experimentation?\nThe context\nThis post is the third in a mini-series.\nIn Deep attractors: Where deep learning meets chaos, we explained, with a substantial detour into chaos theory, the idea of FNN loss, introduced in (Gilpin 2020). Please consult that first post for theoretical background and intuitions behind the technique.\nThe subsequent post, Time series prediction with FNN-LSTM, showed how to use an LSTM autoencoder, constrained by FNN loss, for forecasting (as opposed to reconstructing an attractor). The results were stunning: In multi-step prediction (12-120 steps, with that number varying by dataset), the short-term forecasts were drastically improved by adding in FNN regularization. See that second post for experimental setup and results on four very different, non-synthetic datasets.\nToday, we show how to replace the LSTM autoencoder by a – convolutional – VAE. In light of the experimentation results, already hinted at above, it is completely plausible that the “variational” part is not even so important here – that a convolutional autoencoder with just MSE loss would have performed just as well on those data. In fact, to find out, it’s enough to remove the call to reparameterize() and multiply the KL component of the loss by 0. (We leave this to the interested reader, to keep the post at reasonable length.)\nOne last piece of context, in case you haven’t read the two previous posts and would like to jump in here directly. We’re doing time series forecasting; so why this talk of autoencoders? Shouldn’t we just be comparing an LSTM (or some other type of RNN, for that matter) to a convnet? In fact, the necessity of a latent representation is due to the very idea of FNN: The latent code is supposed to reflect the true attractor of a dynamical system. That is, if the attractor of the underlying system is roughly two-dimensional, we hope to find that just two of the latent variables have considerable variance. (This reasoning is explained in a lot of detail in the previous posts.)\nFNN-VAE\nSo, let’s start with the code for our new model.\nThe encoder takes the time series, of format batch_size x num_timesteps x num_features just like in the LSTM case, and produces a flat, 10-dimensional output: the latent code, which FNN loss is computed on.\n\n\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(tfdatasets)\nlibrary(tfautograph)\nlibrary(reticulate)\nlibrary(purrr)\n\nvae_encoder_model <- function(n_timesteps,\n                               n_features,\n                               n_latent,\n                               name = NULL) {\n  keras_model_custom(name = name, function(self) {\n    self$conv1 <- layer_conv_1d(kernel_size = 3,\n                                filters = 16,\n                                strides = 2)\n    self$act1 <- layer_activation_leaky_relu()\n    self$batchnorm1 <- layer_batch_normalization()\n    self$conv2 <- layer_conv_1d(kernel_size = 7,\n                                filters = 32,\n                                strides = 2)\n    self$act2 <- layer_activation_leaky_relu()\n    self$batchnorm2 <- layer_batch_normalization()\n    self$conv3 <- layer_conv_1d(kernel_size = 9,\n                                filters = 64,\n                                strides = 2)\n    self$act3 <- layer_activation_leaky_relu()\n    self$batchnorm3 <- layer_batch_normalization()\n    self$conv4 <- layer_conv_1d(\n      kernel_size = 9,\n      filters = n_latent,\n      strides = 2,\n      activation = \"linear\" \n    )\n    self$batchnorm4 <- layer_batch_normalization()\n    self$flat <- layer_flatten()\n    \n    function (x, mask = NULL) {\n      x %>%\n        self$conv1() %>%\n        self$act1() %>%\n        self$batchnorm1() %>%\n        self$conv2() %>%\n        self$act2() %>%\n        self$batchnorm2() %>%\n        self$conv3() %>%\n        self$act3() %>%\n        self$batchnorm3() %>%\n        self$conv4() %>%\n        self$batchnorm4() %>%\n        self$flat()\n    }\n  })\n}\n\nThe decoder starts from this – flat – representation and decompresses it into a time sequence. In both encoder and decoder (de-)conv layers, parameters are chosen to handle a sequence length (num_timesteps) of 120, which is what we’ll use for prediction below.\n\n\nvae_decoder_model <- function(n_timesteps,\n                               n_features,\n                               n_latent,\n                               name = NULL) {\n  keras_model_custom(name = name, function(self) {\n    self$reshape <- layer_reshape(target_shape = c(1, n_latent))\n    self$conv1 <- layer_conv_1d_transpose(kernel_size = 15,\n                                          filters = 64,\n                                          strides = 3)\n    self$act1 <- layer_activation_leaky_relu()\n    self$batchnorm1 <- layer_batch_normalization()\n    self$conv2 <- layer_conv_1d_transpose(kernel_size = 11,\n                                          filters = 32,\n                                          strides = 3)\n    self$act2 <- layer_activation_leaky_relu()\n    self$batchnorm2 <- layer_batch_normalization()\n    self$conv3 <- layer_conv_1d_transpose(\n      kernel_size = 9,\n      filters = 16,\n      strides = 2,\n      output_padding = 1\n    )\n    self$act3 <- layer_activation_leaky_relu()\n    self$batchnorm3 <- layer_batch_normalization()\n    self$conv4 <- layer_conv_1d_transpose(\n      kernel_size = 7,\n      filters = 1,\n      strides = 1,\n      activation = \"linear\"\n    )\n    self$batchnorm4 <- layer_batch_normalization()\n    \n    function (x, mask = NULL) {\n      x %>%\n        self$reshape() %>%\n        self$conv1() %>%\n        self$act1() %>%\n        self$batchnorm1() %>%\n        self$conv2() %>%\n        self$act2() %>%\n        self$batchnorm2() %>%\n        self$conv3() %>%\n        self$act3() %>%\n        self$batchnorm3() %>%\n        self$conv4() %>%\n        self$batchnorm4()\n    }\n  })\n}\n\nNote that even though we called these constructors vae_encoder_model() and vae_decoder_model(), there is nothing variational to these models per se; they are really just an encoder and a decoder, respectively. Metamorphosis into a VAE will happen in the training procedure; in fact, the only two things that will make this a VAE are going to be the reparameterization of the latent layer and the added-in KL loss.\nSpeaking of training, these are the routines we’ll call. The function to compute FNN loss, loss_false_nn(), can be found in both of the abovementioned predecessor posts; we kindly ask the reader to copy it from one of these places.\n\n\n# to reparameterize encoder output before calling decoder\nreparameterize <- function(mean, logvar = 0) {\n  eps <- k_random_normal(shape = n_latent)\n  eps * k_exp(logvar * 0.5) + mean\n}\n\n# loss has 3 components: NLL, KL, and FNN\n# otherwise, this is just normal TF2-style training \ntrain_step_vae <- function(batch) {\n  with (tf$GradientTape(persistent = TRUE) %as% tape, {\n    code <- encoder(batch[[1]])\n    z <- reparameterize(code)\n    prediction <- decoder(z)\n    \n    l_mse <- mse_loss(batch[[2]], prediction)\n    # see loss_false_nn in 2 previous posts\n    l_fnn <- loss_false_nn(code)\n    # KL divergence to a standard normal\n    l_kl <- -0.5 * k_mean(1 - k_square(z))\n    # overall loss is a weighted sum of all 3 components\n    loss <- l_mse + fnn_weight * l_fnn + kl_weight * l_kl\n  })\n  \n  encoder_gradients <-\n    tape$gradient(loss, encoder$trainable_variables)\n  decoder_gradients <-\n    tape$gradient(loss, decoder$trainable_variables)\n  \n  optimizer$apply_gradients(purrr::transpose(list(\n    encoder_gradients, encoder$trainable_variables\n  )))\n  optimizer$apply_gradients(purrr::transpose(list(\n    decoder_gradients, decoder$trainable_variables\n  )))\n  \n  train_loss(loss)\n  train_mse(l_mse)\n  train_fnn(l_fnn)\n  train_kl(l_kl)\n}\n\n# wrap it all in autograph\ntraining_loop_vae <- tf_function(autograph(function(ds_train) {\n  \n  for (batch in ds_train) {\n    train_step_vae(batch) \n  }\n  \n  tf$print(\"Loss: \", train_loss$result())\n  tf$print(\"MSE: \", train_mse$result())\n  tf$print(\"FNN loss: \", train_fnn$result())\n  tf$print(\"KL loss: \", train_kl$result())\n  \n  train_loss$reset_states()\n  train_mse$reset_states()\n  train_fnn$reset_states()\n  train_kl$reset_states()\n  \n}))\n\nTo finish up the model section, here is the actual training code. This is nearly identical to what we did for FNN-LSTM before.\n\n\nn_latent <- 10L\nn_features <- 1\n\nencoder <- vae_encoder_model(n_timesteps,\n                         n_features,\n                         n_latent)\n\ndecoder <- vae_decoder_model(n_timesteps,\n                         n_features,\n                         n_latent)\nmse_loss <-\n  tf$keras$losses$MeanSquaredError(reduction = tf$keras$losses$Reduction$SUM)\n\ntrain_loss <- tf$keras$metrics$Mean(name = 'train_loss')\ntrain_fnn <- tf$keras$metrics$Mean(name = 'train_fnn')\ntrain_mse <-  tf$keras$metrics$Mean(name = 'train_mse')\ntrain_kl <-  tf$keras$metrics$Mean(name = 'train_kl')\n\nfnn_multiplier <- 1 # default value used in nearly all cases (see text)\nfnn_weight <- fnn_multiplier * nrow(x_train)/batch_size\n\nkl_weight <- 1\n\noptimizer <- optimizer_adam(lr = 1e-3)\n\nfor (epoch in 1:100) {\n  cat(\"Epoch: \", epoch, \" -----------\\n\")\n  training_loop_vae(ds_train)\n \n  test_batch <- as_iterator(ds_test) %>% iter_next()\n  encoded <- encoder(test_batch[[1]][1:1000])\n  test_var <- tf$math$reduce_variance(encoded, axis = 0L)\n  print(test_var %>% as.numeric() %>% round(5))\n}\n\nExperimental setup and data\nThe idea was to add white noise to a deterministic series. This time, the Roessler system was chosen, mainly for the prettiness of its attractor, apparent even in its two-dimensional projections:\n\n\n\nFigure 1: Roessler attractor, two-dimensional projections.\n\n\n\nLike we did for the Lorenz system in the first part of this series, we use deSolve to generate data from the Roessler equations.\n\n\nlibrary(deSolve)\n\nparameters <- c(a = .2,\n                b = .2,\n                c = 5.7)\n\ninitial_state <-\n  c(x = 1,\n    y = 1,\n    z = 1.05)\n\nroessler <- function(t, state, parameters) {\n  with(as.list(c(state, parameters)), {\n    dx <- -y - z\n    dy <- x + a * y\n    dz = b + z * (x - c)\n    \n    list(c(dx, dy, dz))\n  })\n}\n\ntimes <- seq(0, 2500, length.out = 20000)\n\nroessler_ts <-\n  ode(\n    y = initial_state,\n    times = times,\n    func = roessler,\n    parms = parameters,\n    method = \"lsoda\"\n  ) %>% unclass() %>% as_tibble()\n\nn <- 10000\nroessler <- roessler_ts$x[1:n]\n\nroessler <- scale(roessler)\n\nThen, noise is added, to the desired degree, by drawing from a normal distribution, centered at zero, with standard deviations varying between 1 and 2.5.\n\n\n# add noise\nnoise <- 1 # also used 1.5, 2, 2.5\nroessler <- roessler + rnorm(10000, mean = 0, sd = noise)\n\nHere you can compare effects of not adding any noise (left), standard deviation-1 (middle), and standard deviation-2.5 Gaussian noise:\n\n\n\nFigure 2: Roessler series with added noise. Top: none. Middle: SD = 1. Bottom: SD = 2.5.\n\n\n\nOtherwise, preprocessing proceeds as in the previous posts. In the upcoming results section, we’ll compare forecasts not just to the “real”, after noise addition, test split of the data, but also to the underlying Roessler system – that is, the thing we’re really interested in. (Just that in the real world, we can’t do that check.) This second test set is prepared for forecasting just like the other one; to avoid duplication we don’t reproduce the code.\n\n\nn_timesteps <- 120\nbatch_size <- 32\n\ngen_timesteps <- function(x, n_timesteps) {\n  do.call(rbind,\n          purrr::map(seq_along(x),\n                     function(i) {\n                       start <- i\n                       end <- i + n_timesteps - 1\n                       out <- x[start:end]\n                       out\n                     })\n  ) %>%\n    na.omit()\n}\n\ntrain <- gen_timesteps(roessler[1:(n/2)], 2 * n_timesteps)\ntest <- gen_timesteps(roessler[(n/2):n], 2 * n_timesteps) \n\ndim(train) <- c(dim(train), 1)\ndim(test) <- c(dim(test), 1)\n\nx_train <- train[ , 1:n_timesteps, , drop = FALSE]\ny_train <- train[ , (n_timesteps + 1):(2*n_timesteps), , drop = FALSE]\n\nds_train <- tensor_slices_dataset(list(x_train, y_train)) %>%\n  dataset_shuffle(nrow(x_train)) %>%\n  dataset_batch(batch_size)\n\nx_test <- test[ , 1:n_timesteps, , drop = FALSE]\ny_test <- test[ , (n_timesteps + 1):(2*n_timesteps), , drop = FALSE]\n\nds_test <- tensor_slices_dataset(list(x_test, y_test)) %>%\n  dataset_batch(nrow(x_test))\n\nResults\nThe LSTM used for comparison with the VAE described above is identical to the architecture employed in the previous post. While with the VAE, an fnn_multiplier of 1 yielded sufficient regularization for all noise levels, some more experimentation was needed for the LSTM: At noise levels 2 and 2.5, that multiplier was set to 5.\nAs a result, in all cases, there was one latent variable with high variance and a second one of minor importance. For all others, variance was close to 0.\nIn all cases here means: In all cases where FNN regularization was used. As already hinted at in the introduction, the main regularizing factor providing robustness to noise here seems to be FNN loss, not KL divergence. So for all noise levels, besides FNN-regularized LSTM and VAE models we also tested their non-constrained counterparts.\nLow noise\nSeeing how all models did superbly on the original deterministic series, a noise level of 1 can almost be treated as a baseline. Here you see sixteen 120-timestep predictions from both regularized models, FNN-VAE (dark blue), and FNN-LSTM (orange). The noisy test data, both input (x, 120 steps) and output (y, 120 steps) are displayed in (blue-ish) grey. In green, also spanning the whole sequence, we have the original Roessler data, the way they would look had no noise been added.\n\n\n\nFigure 3: Roessler series with added Gaussian noise of standard deviation 1. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from FNN-LSTM. Dark blue: Predictions from FNN-VAE.\n\n\n\nDespite the noise, forecasts from both models look excellent. Is this due to the FNN regularizer?\nLooking at forecasts from their unregularized counterparts, we have to admit these do not look any worse. (For better comparability, the sixteen sequences to forecast were initiallly picked at random, but used to test all models and conditions.)\n\n\n\nFigure 4: Roessler series with added Gaussian noise of standard deviation 1. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from unregularized LSTM. Dark blue: Predictions from unregularized VAE.\n\n\n\nWhat happens when we start to add noise?\nSubstantial noise\nBetween noise levels 1.5 and 2, something changed, or became noticeable from visual inspection. Let’s jump directly to the highest-used level though: 2.5.\nHere first are predictions obtained from the unregularized models.\n\n\n\nFigure 5: Roessler series with added Gaussian noise of standard deviation 2.5. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from unregularized LSTM. Dark blue: Predictions from unregularized VAE.\n\n\n\nBoth LSTM and VAE get “distracted” a bit too much by the noise, the latter to an even higher degree. This leads to cases where predictions strongly “overshoot” the underlying non-noisy rhythm. This is not surprising, of course: They were trained on the noisy version; predict fluctuations is what they learned.\nDo we see the same with the FNN models?\n\n\n\nFigure 6: Roessler series with added Gaussian noise of standard deviation 2.5. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from FNN-LSTM. Dark blue: Predictions from FNN-VAE.\n\n\n\nInterestingly, we see a much better fit to the underlying Roessler system now! Especially the VAE model, FNN-VAE, surprises with a whole new smoothness of predictions; but FNN-LSTM turns up much smoother forecasts as well.\n“Smooth, fitting the system…” – by now you may be wondering, when are we going to come up with more quantitative assertions? If quantitative implies “mean squared error” (MSE), and if MSE is taken to be some divergence between forecasts and the true target from the test set, the answer is that this MSE doesn’t differ much between any of the four architectures. Put differently, it is mostly a function of noise level.\nHowever, we could argue that what we’re really interested in is how well a model forecasts the underlying process. And there, we see differences.\nIn the following plot, we contrast MSEs obtained for the four model types (grey: VAE; orange: LSTM; dark blue: FNN-VAE; green: FNN-LSTM). The rows reflect noise levels (1, 1.5, 2, 2.5); the columns represent MSE in relation to the noisy(“real”) target (left) on the one hand, and in relation to the underlying system on the other (right). For better visibility of the effect, MSEs have been normalized as fractions of the maximum MSE in a category.\nSo, if we want to predict signal plus noise (left), it is not extremely critical whether we use FNN or not. But if we want to predict the signal only (right), with increasing noise in the data FNN loss becomes increasingly effective. This effect is far stronger for VAE vs. FNN-VAE than for LSTM vs. FNN-LSTM: The distance between the grey line (VAE) and the dark blue one (FNN-VAE) becomes larger and larger as we add more noise.\n\n\n\nFigure 7: Normalized MSEs obtained for the four model types (grey: VAE; orange: LSTM; dark blue: FNN-VAE; green: FNN-LSTM). Rows are noise levels (1, 1.5, 2, 2.5); columns are MSE as related to the real target (left) and the underlying system (right).\n\n\n\nSumming up\nOur experiments show that when noise is likely to obscure measurements from an underlying deterministic system, FNN regularization can strongly improve forecasts. This is the case especially for convolutional VAEs, and probably convolutional autoencoders in general. And if an FNN-constrained VAE performs as well, for time series prediction, as an LSTM, there is a strong incentive to use the convolutional model: It trains significantly faster.\nWith that, we conclude our mini-series on FNN-regularized models. As always, we’d love to hear from you if you were able to make use of this in your own work!\nThanks for reading!\n\n\nGilpin, William. 2020. “Deep Reconstruction of Strange Attractors from Time Series.” http://arxiv.org/abs/2002.05909.\n\n\n\n\n",
    "preview": "posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/kb.jpg",
    "last_modified": "2020-09-30T20:24:31+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-07-30-state-of-the-art-nlp-models-from-r/",
    "title": "State-of-the-art NLP models from R",
    "description": "Nowadays, Microsoft, Google, Facebook, and OpenAI are sharing lots of state-of-the-art models in the field of Natural Language Processing. However, fewer materials exist how to use these models from R. In this post, we will show how R users can access and benefit from these models as well.",
    "author": [
      {
        "name": "Turgut Abdullayev",
        "url": "https://github.com/henry090"
      }
    ],
    "date": "2020-07-30",
    "categories": [
      "Natural Language Processing"
    ],
    "contents": "\nTable of Contents\nIntroduction\nPrerequisites\nTemplate\nData preparation\nData input for Keras\nConclusion\n\n.colab-root {\n    display: inline-block;\n    background: rgba(255, 255, 255, 0.75);\n    padding: 4px 8px;\n    border-radius: 4px;\n    font-size: 11px!important;\n    text-decoration: none;\n    color: #aaa;\n    border: none;\n    font-weight: 300;\n    border: solid 1px rgba(0, 0, 0, 0.08);\n    border-bottom-color: rgba(0, 0, 0, 0.15);\n    text-transform: uppercase;\n    line-height: 16px;\n}\nspan.colab-span {\n    background-image: url(https://www.vectorlogo.zone/logos/kaggle/kaggle-ar21.svg);\n    background-repeat: no-repeat;\n    background-size: 51px;\n    background-position-y: -4px;\n    display: inline-block;\n    padding-left: 24px;\n    border-radius: 4px;\n    text-decoration: none;\n}\nIntroduction\nThe Transformers repository from “Hugging Face” contains a lot of ready to use, state-of-the-art models, which are straightforward to download and fine-tune with Tensorflow & Keras.\nFor this purpose the users usually need to get:\nThe model itself (e.g. Bert, Albert, RoBerta, GPT-2 and etc.)\nThe tokenizer object\nThe weights of the model\nIn this post, we will work on a classic binary classification task and train our dataset on 3 models:\nGPT-2 from Open AI\nRoBERTa from Facebook\nElectra from Google Research/Stanford University\nHowever, readers should know that one can work with transformers on a variety of down-stream tasks, such as:\nfeature extraction\nsentiment analysis\ntext classification\nquestion answering\nsummarization\ntranslation and many more.\nPrerequisites\nOur first job is to install the transformers package via reticulate.\n\n\nreticulate::py_install('transformers', pip = TRUE)\n\nThen, as usual, load standard ‘Keras’, ‘TensorFlow’ >= 2.0 and some classic libraries from R.\n\n\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(dplyr)\nlibrary(tfdatasets)\n\ntransformer = reticulate::import('transformers')\n\nNote that if running TensorFlow on GPU one could specify the following parameters in order to avoid memory issues.\n\n\nphysical_devices = tf$config$list_physical_devices('GPU')\ntf$config$experimental$set_memory_growth(physical_devices[[1]],TRUE)\n\ntf$keras$backend$set_floatx('float32')\n\nTemplate\nWe already mentioned that to train a data on the specific model, users should download the model, its tokenizer object and weights. For example, to get a RoBERTa model one has to do the following:\n\n\n# get Tokenizer\ntransformer$RobertaTokenizer$from_pretrained('roberta-base', do_lower_case=TRUE)\n\n# get Model with weights\ntransformer$TFRobertaModel$from_pretrained('roberta-base')\n\nData preparation\nA dataset for binary classification is provided in text2vec package. Let’s load the dataset and take a sample for fast model training.\n\n\nlibrary(text2vec)\ndata(\"movie_review\")\ndf = movie_review %>% rename(target = sentiment, comment_text = review) %>% \n  sample_n(2000) %>% \n  data.table::as.data.table()\n\nSplit our data into 2 parts:\n\n\nidx_train = sample.int(nrow(df)*0.8)\n\ntrain = df[idx_train,]\ntest = df[!idx_train,]\n\nData input for Keras\nUntil now, we’ve just covered data import and train-test split. To feed input to the network we have to turn our raw text into indices via the imported tokenizer. And then adapt the model to do binary classification by adding a dense layer with a single unit at the end.\nHowever, we want to train our data for 3 models GPT-2, RoBERTa, and Electra. We need to write a loop for that.\n\nNote: one model in general requires 500-700 MB\n\n\n\n# list of 3 models\nai_m = list(\n  c('TFGPT2Model',       'GPT2Tokenizer',       'gpt2'),\n   c('TFRobertaModel',    'RobertaTokenizer',    'roberta-base'),\n   c('TFElectraModel',    'ElectraTokenizer',    'google/electra-small-generator')\n)\n\n# parameters\nmax_len = 50L\nepochs = 2\nbatch_size = 10\n\n# create a list for model results\ngather_history = list()\n\nfor (i in 1:length(ai_m)) {\n  \n  # tokenizer\n  tokenizer = glue::glue(\"transformer${ai_m[[i]][2]}$from_pretrained('{ai_m[[i]][3]}',\n                         do_lower_case=TRUE)\") %>% \n    rlang::parse_expr() %>% eval()\n  \n  # model\n  model_ = glue::glue(\"transformer${ai_m[[i]][1]}$from_pretrained('{ai_m[[i]][3]}')\") %>% \n    rlang::parse_expr() %>% eval()\n  \n  # inputs\n  text = list()\n  # outputs\n  label = list()\n  \n  data_prep = function(data) {\n    for (i in 1:nrow(data)) {\n      \n      txt = tokenizer$encode(data[['comment_text']][i],max_length = max_len, \n                             truncation=T) %>% \n        t() %>% \n        as.matrix() %>% list()\n      lbl = data[['target']][i] %>% t()\n      \n      text = text %>% append(txt)\n      label = label %>% append(lbl)\n    }\n    list(do.call(plyr::rbind.fill.matrix,text), do.call(plyr::rbind.fill.matrix,label))\n  }\n  \n  train_ = data_prep(train)\n  test_ = data_prep(test)\n  \n  # slice dataset\n  tf_train = tensor_slices_dataset(list(train_[[1]],train_[[2]])) %>% \n    dataset_batch(batch_size = batch_size, drop_remainder = TRUE) %>% \n    dataset_shuffle(128) %>% dataset_repeat(epochs) %>% \n    dataset_prefetch(tf$data$experimental$AUTOTUNE)\n  \n  tf_test = tensor_slices_dataset(list(test_[[1]],test_[[2]])) %>% \n    dataset_batch(batch_size = batch_size)\n  \n  # create an input layer\n  input = layer_input(shape=c(max_len), dtype='int32')\n  hidden_mean = tf$reduce_mean(model_(input)[[1]], axis=1L) %>% \n    layer_dense(64,activation = 'relu')\n  # create an output layer for binary classification\n  output = hidden_mean %>% layer_dense(units=1, activation='sigmoid')\n  model = keras_model(inputs=input, outputs = output)\n  \n  # compile with AUC score\n  model %>% compile(optimizer= tf$keras$optimizers$Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0),\n                    loss = tf$losses$BinaryCrossentropy(from_logits=F),\n                    metrics = tf$metrics$AUC())\n  \n  print(glue::glue('{ai_m[[i]][1]}'))\n  # train the model\n  history = model %>% keras::fit(tf_train, epochs=epochs, #steps_per_epoch=len/batch_size,\n                validation_data=tf_test)\n  gather_history[[i]]<- history\n  names(gather_history)[i] = ai_m[[i]][1]\n}\n\n\nReproduce in a           Notebook\n\n\nExtract results to see the benchmarks:\n\n\nres = sapply(1:3, function(x) {\n  do.call(rbind,gather_history[[x]][[\"metrics\"]]) %>% \n    as.data.frame() %>% \n    tibble::rownames_to_column() %>% \n    mutate(model_names = names(gather_history[x])) \n}, simplify = F) %>% do.call(plyr::rbind.fill,.) %>% \n  mutate(rowname = stringr::str_extract(rowname, 'loss|val_loss|auc|val_auc')) %>% \n  rename(epoch_1 = V1, epoch_2 = V2)\n\n\n\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\"],[\"val_auc\",\"val_auc\",\"val_auc\"],[0.892,0.868,0.844],[0.893,0.855,0.845],[\"TFRobertaModel\",\"TFGPT2Model\",\"TFElectraModel\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>metric<\\/th>\\n      <th>epoch_1<\\/th>\\n      <th>epoch_2<\\/th>\\n      <th>model_names<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"dom\":\"t\",\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[2,3]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\nBoth the RoBERTa and Electra models show some additional improvements after 2 epochs of training, which cannot be said of GPT-2. In this case, it is clear that it can be enough to train a state-of-the-art model even for a single epoch.\nConclusion\nIn this post, we showed how to use state-of-the-art NLP models from R. To understand how to apply them to more complex tasks, it is highly recommended to review the transformers tutorial.\nWe encourage readers to try out these models and share their results below in the comments section!\n\n\n",
    "preview": "posts/2020-07-30-state-of-the-art-nlp-models-from-r/files/dino.jpg",
    "last_modified": "2020-09-30T20:24:31+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-07-29-parallelized-sampling/",
    "title": "Parallelized sampling using exponential variates",
    "description": "How can the seemingly iterative process of weighted sampling without replacement be transformed into something highly parallelizable? Turns out a well-known technique based on exponential variates accomplishes exactly that.",
    "author": [
      {
        "name": "Yitao Li",
        "url": {}
      }
    ],
    "date": "2020-07-29",
    "categories": [
      "Concepts",
      "Distributed Computing"
    ],
    "contents": "\nAs part of our recent work to support weighted sampling of Spark data frames in sparklyr, we embarked on a journey searching for algorithms that can perform weighted sampling, especially sampling without replacement, in efficient and scalable ways within a distributed cluster-computing framework, such as Apache Spark.\nIn the interest of brevity, “weighted sampling without replacement” shall be shortened into SWoR for the remainder of this blog post.\nIn the following sections, we will explain and illustrate what SWoR means probability-wise, briefly outline some alternative solutions we have considered but were not completely satisfied with, and then deep-dive into exponential variates, a simple mathematical construct that made the ideal solution for this problem possible.\nIf you cannot wait to jump into action, there is also a section in which we showcase example usages of sdf_weighted_sample() in sparklyr. In addition, you can examine the implementation detail of sparklyr::sdf_weighted_sample() in this pull request.\nHow it all started\nOur journey started from a Github issue inquiring about the possibility of supporting the equivalent of dplyr::sample_frac(..., weight = <weight_column>) for Spark data frames in sparklyr. For example,\n\n\ndplyr::sample_frac(mtcars, 0.25, weight = gear, replace = FALSE)\n\n\n##                    mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Merc 280C         17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\n## Chrysler Imperial 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\n## Fiat X1-9         27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n## Hornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\n## Porsche 914-2     26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n## Maserati Bora     15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n## Ferrari Dino      19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nwill randomly select one-fourth of all rows from a R data frame named “mtcars” without replacement, using mtcars$gear as weights. We were unable to find any function implementing the weighted versions of dplyr::sample_frac among Spark SQL built-in functions in Spark 3.0 or in earlier versions, which means a future version of sparklyr will need to run its own weighted sampling algorithm to support such use cases.\nWhat exactly is SWoR\nThe purpose of this section is to mathematically describe the probability distribution generated by SWoR in terms of \\(w_1, \\dotsc, w_N\\), so that readers can clearly see that the exponential-variate based algorithm presented in a subsequent section in fact samples from precisely the same probability distribution. Readers already having a crystal-clear mental picture of what SWoR entails should probably skip most of this section. The key take-away here is given \\(N\\) rows \\(r_1, \\dotsc, r_N\\) and their weights \\(w_1, \\dotsc, w_N\\) and a desired sample size \\(n\\), the probability of SWoR selecting \\((r_1, \\dotsc, r_n)\\) is \\(\\prod\\limits_{j = 1}^{n} \\left( {w_j} \\middle/ {\\sum\\limits_{k = j}^{N}{w_k}} \\right)\\).\nSWOR is conceptually equivalent to a \\(n\\)-step process of selecting 1 out of \\((n - j + 1)\\) remaining rows in the \\(j\\)-th step for \\(j \\in \\{1, \\dotsc, n\\}\\), with each remaining row’s likelihood of getting selected being linearly proportional to its weight in any of the steps, i.e.,\n\nsamples := {}\npopulation := {r[1], ..., r[N]}\n\nfor j = 1 to n\n  select r[x] from population with probability\n    (w[x] / TotalWeight(population))\n  samples := samples + {r[x]}\n  population := population - {r[x]}\nNotice the outcome of a SWoR process is in fact order-significant, which is why in this post it will always be represented as an ordered tuple of elements.\nIntuitively, SWoR is analogous to throwing darts at a bunch of tiles. For example, let’s say the size of our sample space is 5:\nImagine \\(r_1, r_2, \\dotsc, r_5\\) as 5 rectangular tiles laid out contiguously on a wall with widths \\(w_1, w_2, \\dotsc, w_5\\), with \\(r_1\\) covering \\([0, w_1)\\), \\(r_2\\) covering \\([w_1, w_1 + w_2)\\), …, and \\(r_5\\) covering \\(\\left[\\sum\\limits_{j = 1}^{4} w_j, \\sum\\limits_{j = 1}^{5} w_j\\right)\\)\nEquate drawing a random sample in each step to throwing a dart uniformly randomly within the interval covered by all tiles that are not hit yet\nAfter a tile is hit, it gets taken out and remaining tiles are re-arranged so that they continue to cover a contiguous interval without overlapping\nIf our sample size is 3, then we shall ask ourselves what is the probability of the dart hitting \\((r_1, r_2, r_3)\\) in that order?\nIn step \\(j = 1\\), the dart will hit \\(r_1\\) with probability \\(\\left. w_1 \\middle/ \\left(\\sum\\limits_{k = 1}^{N}w_k\\right) \\right.\\)\n .\nAfter deleting \\(r_1\\) from the sample space after it’s hit, step \\(j = 2\\) will look like this:\n ,\nand the probability of the dart hitting \\(r_2\\) in step 2 is \\(\\left. w_2 \\middle/ \\left(\\sum\\limits_{k = 2}^{N}w_k\\right) \\right.\\) .\nFinally, moving on to step \\(j = 3\\), we have:\n ,\nwith the probability of the dart hitting \\(r_3\\) being \\(\\left. w_3 \\middle/ \\left(\\sum\\limits_{k = 3}^{N}w_k\\right) \\right.\\).\nSo, combining all of the above, the overall probability of selecting \\((r_1, r_2, r_3)\\) is \\(\\prod\\limits_{j = 1}^{3} \\left( {w_j} \\middle/ {\\sum\\limits_{k = j}^{N}{w_k}} \\right)\\).\nNaive approaches for implementing SWoR\nThis section outlines some possible approaches that were briefly under consideration. Because none of these approaches scales well to a large number of rows or a non-trivial number of partitions in a Spark data frame, we decided to avoid all of them in sparklyr.\nA tree-base approach\nOne possible way to accomplish SWoR is to have a mutable data structure keeping track of the sample space at each step.\nContinuing with the dart-throwing analogy from the previous section, let us say initially, none of the tiles has been taken out yet, and a dart has landed at some point \\(x \\in \\left[0, \\sum\\limits_{k = 1}^{N} w_k\\right)\\). Which tile did it hit? This can be answered efficiently if we have a binary tree, pictured as the following (or in general, some \\(b\\)-ary tree for integer \\(b \\ge 2\\))\n.To find the tile that was hit given the dart’s position \\(x\\), we simply need to traverse down the tree, going through the box containing \\(x\\) in each level, incurring a \\(O(\\log(N))\\) cost in time complexity for each sample. To take a tile out of the picture, we update the width of the tile to \\(0\\) and propagate this change upwards from leaf level to root of the tree, again incurring a \\(O(\\log(N))\\) cost in time complexity, making the overall time complexity of selecting \\(n\\) samples \\(O(n \\cdot \\log(N))\\), which is not so great for large data sets, and also, not parallelizable across multiple partitions of a Spark data frame.\nRejection sampling\nAnother possible approach is to use rejection sampling. In term of the previously mentioned dart-throwing analogy, that means not removing any tile that is hit, hence avoiding the performance cost of keeping the sample space up-to-date, but then having to re-throw the dart in each of the subsequent rounds until the dart lands on a tile that was not hit previously. This approach, just like the previous one, would not be performant, and would not be parallelizable across multiple partitions of a Spark data frame either.\nExponential variates to the rescue\nA solution that has proven to be much better than any of the naive approaches turns out to be a numerical stable variant of the algorithm described in “Weighted Random Sampling” (Efraimidis and Spirakis 2016) by Pavlos S. Efraimidis and Paul G. Spirakis.\nA version of this sampling algorithm implemented by sparklyr does the following to sample \\(n\\) out of \\(N\\) rows from a Spark data frame \\(X\\):\nFor each row \\(r_j \\in X\\), draw a random number \\(u_j\\) independently and uniformly randomly from \\((0, 1)\\) and compute the key of \\(r_j\\) as \\(k_j = \\ln(u_j) / w_j\\), where \\(w_j\\) is the weight of \\(r_j\\). Perform this calulation in parallel across all partitions of \\(X\\).\nSelect \\(n\\) rows with largest keys and return them as the result. This step is also mostly parallelizable: for each partition of \\(X\\), one can select up to \\(n\\) rows having largest keys within that partition as candidates, and after selecting candidates from all partitions in parallel, simply extract the top \\(n\\) rows among all candidates, and return them as the \\(n\\) chosen samples.\nThere are at least 4 reasons why this solution is highly appealing and was chosen to be implemented in sparklyr:\nIt is a one-pass algorithm (i.e., only need to iterate through all rows of a data frame exactly once).\nIts computational overhead is quite low (as selecting top \\(n\\) rows at any stage only requires a bounded priority queue of max size \\(n\\), which costs \\(O(\\log(n))\\) per update in time complexity).\nMore importantly, most of its required computations can be performed in parallel. In fact, the only non-parallelizable step is the very last stage of combining top candidates from all partitions and choosing the top \\(n\\) rows among those candidates. So, it fits very well into the world of Spark / MapReduce, and has drastically better horizontal scalability compared to the naive approaches.\nBonus: It is also suitable for weighted reservoir sampling (i.e., can sample \\(n\\) out of a possibly infinite stream of rows according to their weights such that at any moment the \\(n\\) samples will be a weighted representation of all rows that have been processed so far).\nWhy does this algorithm work\nAs an interesting aside, some readers have probably seen this technique presented in a slightly different form under another name. It is in fact equivalent to a generalized version of the Gumbel-max trick which is commonly referred to as the Gumbel-top-k trick. Readers familiar with properties of the Gumbel distribution will no doubt have an easy time convincing themselves the algorithm above works as expected.\nIn this section, we will also present a proof of correctness for this algorithm based on elementary properties of probability density function (shortened as PDF from now on), cumulative distribution function (shortened as CDF from now on), and basic calculus.\nFirst of all, to make sense of all the \\(\\ln(u_j) / w_j\\) calculations in this algorithm, one has to understand inverse transform sampling. For each \\(j \\in \\{1, \\dotsc, N\\}\\), consider the probability distribution defined on \\((-\\infty, 0)\\) with CDF \\(F_j(x) = e^{w_j \\cdot x}\\). In order to pluck out a value \\(y\\) from this distribution, we first sample a value \\(u_j\\) uniformly randomly out of \\((0, 1)\\) that determines the percentile of \\(y\\) (i.e., how our \\(y\\) value ranks relative to all possible \\(y\\) values, a.k.a, the “overall population”, from this distribution), and then apply \\(F_j^{-1}\\) to \\(u_j\\) to find \\(y\\), so, \\(y = F_j^{-1}(u_j) = \\ln(u_j) / w_j\\).\nSecondly, after defining all the required CDF functions \\(F_j(x) = e^{w_j \\cdot x}\\) for \\(j \\in \\{1, \\dotsc, N\\}\\), we can also easily derive their corresponding PDF functions \\(f_j\\): \\[f_j(x) = \\frac{d F_j(x)}{dx} = w_j e^{w_j \\cdot x}\\].\nFinally, with a clear understanding of the family of probability distributions involved, one can prove the probability of this algorithm selecting a given sequence of rows \\((r_1, \\dotsc, r_n)\\) is equal to \\(\\prod\\limits_{j = 1}^{n} \\left( {w_j} \\middle/ {\\sum\\limits_{k = j}^{N}{w_k}} \\right)\\), identical to the probability previously mentioned in the “What exactly is SWoR” section, which implies the possible outcomes of this algorithm will follow exactly the same probability distribution as that of a \\(n\\)-step SWoR.\nIn order to not deprive our dear readers the pleasure of completing this proof by themselves, we have decided to not inline the rest of the proof (which boils down to a calculus exercise) within this blog post, but it is available in this file.\nWeighted sampling with replacement\nWhile all previous sections focused entirely on weighted sampling without replacement, this section will briefly discuss how the exponential-variate approach can also benefit the weighted-sampling-with-replacement use case (which will be shortened as SWR from now on).\nAlthough SWR with sample size \\(n\\) can be carried out by \\(n\\) independent processes each selecting \\(1\\) sample, parallelizing a SWR workload across all partitions of a Spark data frame (let’s call it \\(X\\)) will still be more performant if the number of partitions is much larger than \\(n\\) and more than \\(n\\) executors are available in a Spark cluster.\nAn initial solution we had in mind was to run SWR with sample size \\(n\\) in parallel on each partition of \\(X\\), and then re-sample the results based on relative total weights of each partition. Despite sounding deceptively simple when summarized in words, implementing such a solution in practice would be a moderately complicated task. First, one has to apply the alias method or similar in order to perform weighted sampling efficiently on each partition of \\(X\\), and on top of that, implementing the re-sampling logic across all partitions correctly and verifying the correctness of such procedure will also require considerable effort.\nIn comparison, with the help of exponential variates, a SWR carried out as \\(n\\) independent SWoR processes each selecting \\(1\\) sample is much simpler to implement, while still being comparable to our initial solution in terms of efficiency and scalability. An example implementation of it (which takes fewer than 60 lines of Scala) is presented in samplingutils.scala.\nVisualization\nHow do we know sparklyr::sdf_weighted_sample() is working as expected? While the rigorous answer to this question is presented in full in the testing section, we thought it would also be useful to first show some histograms that will help readers visualize what that test plan is. Therefore in this section, we will do the following:\nRun dplyr::slice_sample() multiple times on a small sample space, with each run using a different PRNG seed (sample size will be reduced to \\(2\\) here so that there will fewer than 100 possible outcomes and visualization will be easier)\nDo the same for sdf_weighted_sample()\nUse histograms to visualize the distribution of sampling outcomes\nThroughout this section, we will sample \\(2\\) elements out of \\(\\{0, \\dotsc, 7\\}\\) without replacement according to some weights, so, the first step is to set up the following in R:\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n\n# `octs` will be our sample space\nocts <- data.frame(\n  x = seq(0, 7),\n  weight = c(1, 4, 2, 8, 5, 7, 1, 4)\n)\n# `octs_sdf` will be our sample space copied into a Spark data frame\nocts_sdf <- copy_to(sc, octs)\n\nsample_size <- 2\n\nIn order to tally up and visualize the sampling outcomes efficiently, we shall map each possible outcome to an octal number (e.g., (6, 7) gets mapped to \\(6 \\cdot 8^0 + 7 \\cdot 8^1\\)) using a helper function to_oct in R:\n\n\nto_oct <- function(sample) sum(8 ^ seq(0, sample_sz - 1) * sample$x)\n\nWe also need to tally up sampling outcomes from dplyr::slice_sample() and sparklyr::sdf_weighted_sample() in 2 separate arrays:\n\n\nmax_possible_outcome <- to_oct(list(x = seq(8 - sample_sz, 7)))\n\nsdf_weighted_sample_outcomes <- rep(0, max_possible_outcome)\ndplyr_slice_sample_outcomes <- rep(0, max_possible_outcome)\n\nFinally, we can run both dplyr::slice_sample() and sparklyr::sdf_weighted_sample() for arbitrary number of iterations and compare tallied outcomes from both:\n\n\nnum_sampling_iters <- 1000  # actually we will vary this value from 500 to 5000\n\nfor (x in seq(num_sampling_iters)) {\n  sample1 <- octs_sdf %>%\n    sdf_weighted_sample(\n      k = sample_size, weight_col = \"weight\", replacement = FALSE, seed = seed\n    ) %>%\n    collect() %>%\n    to_oct()\n  sdf_weighted_sample_outcomes[[sample1]] <-\n      sdf_weighted_sample_outcomes[[sample1]] + 1\n\n  seed <- x * 97\n  set.seed(seed) # set random seed for dplyr::sample_slice()\n  sample2 <- octs %>%\n    dplyr::slice_sample(\n      n = sample_size, weight_by = weight, replace = FALSE\n    ) %>%\n    to_oct()\n  dplyr_slice_sample_outcomes[[sample2]] <-\n      dplyr_slice_sample_outcomes[[sample2]] + 1\n}\n\nAfter all the hard work above, we can now enjoy plotting the sampling outcomes from dplyr::slice_sample() and those from sparklyr::sdf_weighted_sample() after 500, 1000, and 5000 iterations and observe how the distributions of both start converging after a large number of iterations.\nSampling outcomes after 500, 1000, and 5000 iterations, shown in 3 histograms:\n (you will most probably need to view it in a separate tab to see everything clearly)\nTesting\nWhile parallelized sampling based on exponential variates looks fantastic on paper, there are still plenty of potential pitfalls when it comes to translating such idea into code, and as usual, a good testing plan is necessary to ensure implementation correctness.\nFor instance, numerical instability issues from floating point numbers arise if \\(\\ln(u_j) / w_j\\) were replaced by \\(u_j ^ {1 / w_j}\\) in the aforementioned computations.\nAnother more subtle source of error is the usage of PRNG seeds. For example, consider the following:\n\n  def sampleWithoutReplacement(\n    rdd: RDD[Row],\n    weightColumn: String,\n    sampleSize: Int,\n    seed: Long\n  ): RDD[Row] = {\n    val sc = rdd.context\n    if (0 == sampleSize) {\n      sc.emptyRDD\n    } else {\n      val random = new Random(seed)\n      val mapRDDs = rdd.mapPartitions { iter =>\n        for (row <- iter) {\n          val weight = row.getAs[Double](weightColumn)\n          val key = scala.math.log(random.nextDouble) / weight\n          <and then make sampling decision for `row` based on its `key`,\n           as described in the previous section>\n        }\n        ...\n      }\n      ...\n    }\n  }\nEven though it might look OK upon first glance, rdd.mapPartitions(...) from the above will cause the same sequence of pseudorandom numbers to be applied to multiple partitions of the input Spark data frame, which will cause undesired bias (i.e., sampling outcomes from one partition will have non-trivial correlation with those from another partition when such correlation should be negligible in a correct implementation).\nThe code snippet below is an example implementation in which each partition of the input Spark data frame is sampled using a different sequence of pseudorandom numbers:\n\n  def sampleWithoutReplacement(\n    rdd: RDD[Row],\n    weightColumn: String,\n    sampleSize: Int,\n    seed: Long\n  ): RDD[Row] = {\n    val sc = rdd.context\n    if (0 == sampleSize) {\n      sc.emptyRDD\n    } else {\n      val mapRDDs = rdd.mapPartitionsWithIndex { (index, iter) =>\n        val random = new Random(seed + index)\n\n        for (row <- iter) {\n          val weight = row.getAs[Double](weightColumn)\n          val key = scala.math.log(random.nextDouble) / weight\n          <and then make sampling decision for `row` based on its `key`,\n           as described in the previous section>\n        }\n\n        ...\n      }\n    ...\n  }\n}\nAn example test case in which a two-sided Kolmogorov-Smirnov test is used to compare distribution of sampling outcomes from dplyr::slice_sample() with that from sparklyr::sdf_weighted_sample() is shown in this file. Such tests have proven to be effective in surfacing non-obvious implementation errors such as the ones mentioned above.\nExample Usages\nPlease note the sparklyr::sdf_weighted_sample() functionality is not included in any official release of sparklyr yet. We are aiming to ship it as part of sparklyr 1.4 in about 2 to 3 months from now.\nIn the meanwhile, you can try it out with the following steps:\nFirst, make sure remotes is installed, and then run\n\n\nremotes::install_github(\"sparklyr/sparklyr\", ref = \"master\")\n\nto install sparklyr from source.\nNext, create a test data frame with numeric weight column consisting of non-negative weight for each row, and then copy it to Spark (see code snippet below as an example):\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n\nexample_df <- data.frame(\n  x = seq(100),\n  weight = c(\n    rep(1, 50),\n    rep(2, 25),\n    rep(4, 10),\n    rep(8, 10),\n    rep(16, 5)\n  )\n)\nexample_sdf <- copy_to(sc, example_df, repartition = 5, overwrite = TRUE)\n\nFinally, run sparklyr::sdf_weighted_sample() on example_sdf:\n\n\nsample_size <- 5\n\nsamples_without_replacement <- example_sdf %>%\n  sdf_weighted_sample(\n    weight_col = \"weight\",\n    k = sample_size,\n    replacement = FALSE\n  )\n\nsamples_without_replacement %>% print(n = sample_size)\n\n\n## # Source: spark<?> [?? x 2]\n##       x weight\n##   <int>  <dbl>\n## 1    48      1\n## 2    22      1\n## 3    78      4\n## 4    56      2\n## 5   100     16\n\n\nsamples_with_replacement <- example_sdf %>%\n  sdf_weighted_sample(\n    weight_col = \"weight\",\n    k = sample_size,\n    replacement = TRUE\n  )\n\nsamples_with_replacement %>% print(n = sample_size)\n\n\n## # Source: spark<?> [?? x 2]\n##       x weight\n##   <int>  <dbl>\n## 1    86      8\n## 2    97     16\n## 3    91      8\n## 4   100     16\n## 5    65      2\nAcknowledgement\nFirst and foremost, the author wishes to thank @ajing for reporting the weighted sampling use cases were not properly supported yet in sparklyr 1.3 and suggesting it should be part of some future version of sparklyr in this Github issue.\nSpecial thanks also goes to Javier (@javierluraschi) for reviewing the implementation of all exponential-variate based sampling algorithms in sparklyr, and to Mara (@batpigandme), Sigrid (@Sigrid), and Javier (@javierluraschi) for their valuable editorial suggestions.\nWe hope you have enjoyed reading this blog post! If you wish to learn more about sparklyr, we recommend visiting sparklyr.ai, spark.rstudio.com, and some of the previous release posts such as sparklyr 1.3 and sparklyr 1.2. Also, your contributions to sparklyr are more than welcome. Please send your pull requests through here and file any bug report or feature request in here.\nThanks for reading!\n\n\nEfraimidis, Pavlos, and Paul (Pavlos) Spirakis. 2016. “Weighted Random Sampling.” In Encyclopedia of Algorithms, edited by Ming-Yang Kao, 2365–7. New York, NY: Springer New York. https://doi.org/10.1007/978-1-4939-2864-4_478.\n\n\n\n\n",
    "preview": "posts/2020-07-29-parallelized-sampling/images/dice.jpg",
    "last_modified": "2020-09-30T20:24:31+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-07-20-fnn-lstm/",
    "title": "Time series prediction with FNN-LSTM",
    "description": "In a recent post, we showed how an LSTM autoencoder, regularized by false nearest neighbors (FNN) loss, can be used to reconstruct the attractor of a nonlinear, chaotic dynamical system. Here, we explore how that same technique assists in prediction. Matched up with a comparable, capacity-wise, \"vanilla LSTM\", FNN-LSTM improves performance on a set of very different, real-world datasets, especially for the initial steps in a multi-step forecast.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-07-20",
    "categories": [
      "R",
      "TensorFlow/Keras",
      "Time Series",
      "Unsupervised Learning"
    ],
    "contents": "\nToday, we pick up on the plan alluded to in the conclusion of the recent Deep attractors: Where deep learning meets chaos: employ that same technique to generate forecasts for empirical time series data.\n“That same technique”, which for conciseness, I’ll take the liberty of referring to as FNN-LSTM, is due to William Gilpin’s 2020 paper “Deep reconstruction of strange attractors from time series” (Gilpin 2020).\nIn a nutshell1, the problem addressed is as follows: A system, known or assumed to be nonlinear and highly dependent on initial conditions, is observed, resulting in a scalar series of measurements. The measurements are not just – inevitably – noisy, but in addition, they are – at best – a projection of a multidimensional state space onto a line.\nClassically in nonlinear time series analysis, such scalar series of observations are augmented by supplementing, at every point in time, delayed measurements of that same series – a technique called delay coordinate embedding (Sauer, Yorke, and Casdagli 1991). For example, instead of just a single vector X1, we could have a matrix of vectors X1, X2, and X3, with X2 containing the same values as X1, but starting from the third observation, and X3, from the fifth. In this case, the delay would be 2, and the embedding dimension, 3. Various theorems state that if these parameters are chosen adequately, it is possible to reconstruct the complete state space. There is a problem though: The theorems assume that the dimensionality of the true state space is known, which in many real-world applications, won’t be the case.\nThis is where Gilpin’s idea comes in: Train an autoencoder, whose intermediate representation encapsulates the system’s attractor. Not just any MSE-optimized autoencoder though. The latent representation is regularized by false nearest neighbors (FNN) loss, a technique commonly used with delay coordinate embedding to determine an adequate embedding dimension. False neighbors are those who are close in n-dimensional space, but significantly farther apart in n+1-dimensional space. In the aforementioned introductory post, we showed how this technique allowed to reconstruct the attractor of the (synthetic) Lorenz system. Now, we want to move on to prediction.\nWe first describe the setup, including model definitions, training procedures, and data preparation. Then, we tell you how it went.\nSetup\nFrom reconstruction to forecasting, and branching out into the real world\nIn the previous post, we trained an LSTM autoencoder to generate a compressed code, representing the attractor of the system. As usual with autoencoders, the target when training is the same as the input, meaning that overall loss consisted of two components: The FNN loss, computed on the latent representation only, and the mean-squared-error loss between input and output. Now for prediction, the target consists of future values, as many as we wish to predict. Put differently: The architecture stays the same, but instead of reconstruction we perform prediction, in the standard RNN way. Where the usual RNN setup would just directly chain the desired number of LSTMs, we have an LSTM encoder that outputs a (timestep-less) latent code, and an LSTM decoder that starting from that code, repeated as many times as required, forecasts the required number of future values.\nThis of course means that to evaluate forecast performance, we need to compare against an LSTM-only setup. This is exactly what we’ll do, and comparison will turn out to be interesting not just quantitatively, but qualitatively as well.\nWe perform these comparisons on the four datasets Gilpin chose to demonstrate attractor reconstruction on observational data. While all of these, as is evident from the images in that notebook, exhibit nice attractors, we’ll see that not all of them are equally suited to forecasting using simple RNN-based architectures – with or without FNN regularization. But even those that clearly demand a different approach allow for interesting observations as to the impact of FNN loss.\nModel definitions and training setup\nIn all four experiments, we use the same model definitions and training procedures, the only differing parameter being the number of timesteps used in the LSTMs (for reasons that will become evident when we introduce the individual datasets).\nBoth architectures were chosen to be straightforward, and about comparable in number of parameters – both basically consist of two LSTMs with 32 units (n_recurrent will be set to 32 for all experiments).2\nFNN-LSTM\nFNN-LSTM looks nearly like in the previous post, apart from the fact that we split up the encoder LSTM into two, to uncouple capacity (n_recurrent) from maximal latent state dimensionality (n_latent, kept at 10 just like before).\n\n\n# DL-related packages\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(tfdatasets)\nlibrary(tfautograph)\nlibrary(reticulate)\n\n# going to need those later\nlibrary(tidyverse)\nlibrary(cowplot)\n\nencoder_model <- function(n_timesteps,\n                          n_features,\n                          n_recurrent,\n                          n_latent,\n                          name = NULL) {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$noise <- layer_gaussian_noise(stddev = 0.5)\n    self$lstm1 <-  layer_lstm(\n      units = n_recurrent,\n      input_shape = c(n_timesteps, n_features),\n      return_sequences = TRUE\n    ) \n    self$batchnorm1 <- layer_batch_normalization()\n    self$lstm2 <-  layer_lstm(\n      units = n_latent,\n      return_sequences = FALSE\n    ) \n    self$batchnorm2 <- layer_batch_normalization()\n    \n    function (x, mask = NULL) {\n      x %>%\n        self$noise() %>%\n        self$lstm1() %>%\n        self$batchnorm1() %>%\n        self$lstm2() %>%\n        self$batchnorm2() \n    }\n  })\n}\n\ndecoder_model <- function(n_timesteps,\n                          n_features,\n                          n_recurrent,\n                          n_latent,\n                          name = NULL) {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$repeat_vector <- layer_repeat_vector(n = n_timesteps)\n    self$noise <- layer_gaussian_noise(stddev = 0.5)\n    self$lstm <- layer_lstm(\n      units = n_recurrent,\n      return_sequences = TRUE,\n      go_backwards = TRUE\n    ) \n    self$batchnorm <- layer_batch_normalization()\n    self$elu <- layer_activation_elu() \n    self$time_distributed <- time_distributed(layer = layer_dense(units = n_features))\n    \n    function (x, mask = NULL) {\n      x %>%\n        self$repeat_vector() %>%\n        self$noise() %>%\n        self$lstm() %>%\n        self$batchnorm() %>%\n        self$elu() %>%\n        self$time_distributed()\n    }\n  })\n}\n\nn_latent <- 10L\nn_features <- 1\nn_hidden <- 32\n\nencoder <- encoder_model(n_timesteps,\n                         n_features,\n                         n_hidden,\n                         n_latent)\n\ndecoder <- decoder_model(n_timesteps,\n                         n_features,\n                         n_hidden,\n                         n_latent)\n\nThe regularizer, FNN loss, is unchanged:\n\n\nloss_false_nn <- function(x) {\n  \n  # changing these parameters is equivalent to\n  # changing the strength of the regularizer, so we keep these fixed (these values\n  # correspond to the original values used in Kennel et al 1992).\n  rtol <- 10 \n  atol <- 2\n  k_frac <- 0.01\n  \n  k <- max(1, floor(k_frac * batch_size))\n  \n  ## Vectorized version of distance matrix calculation\n  tri_mask <-\n    tf$linalg$band_part(\n      tf$ones(\n        shape = c(tf$cast(n_latent, tf$int32), tf$cast(n_latent, tf$int32)),\n        dtype = tf$float32\n      ),\n      num_lower = -1L,\n      num_upper = 0L\n    )\n  \n  # latent x batch_size x latent\n  batch_masked <-\n    tf$multiply(tri_mask[, tf$newaxis,], x[tf$newaxis, reticulate::py_ellipsis()])\n  \n  # latent x batch_size x 1\n  x_squared <-\n    tf$reduce_sum(batch_masked * batch_masked,\n                  axis = 2L,\n                  keepdims = TRUE)\n  \n  # latent x batch_size x batch_size\n  pdist_vector <- x_squared + tf$transpose(x_squared, perm = c(0L, 2L, 1L)) -\n    2 * tf$matmul(batch_masked, tf$transpose(batch_masked, perm = c(0L, 2L, 1L)))\n  \n  #(latent, batch_size, batch_size)\n  all_dists <- pdist_vector\n  # latent\n  all_ra <-\n    tf$sqrt((1 / (\n      batch_size * tf$range(1, 1 + n_latent, dtype = tf$float32)\n    )) *\n      tf$reduce_sum(tf$square(\n        batch_masked - tf$reduce_mean(batch_masked, axis = 1L, keepdims = TRUE)\n      ), axis = c(1L, 2L)))\n  \n  # Avoid singularity in the case of zeros\n  #(latent, batch_size, batch_size)\n  all_dists <-\n    tf$clip_by_value(all_dists, 1e-14, tf$reduce_max(all_dists))\n  \n  #inds = tf.argsort(all_dists, axis=-1)\n  top_k <- tf$math$top_k(-all_dists, tf$cast(k + 1, tf$int32))\n  # (#(latent, batch_size, batch_size)\n  top_indices <- top_k[[1]]\n  \n  #(latent, batch_size, batch_size)\n  neighbor_dists_d <-\n    tf$gather(all_dists, top_indices, batch_dims = -1L)\n  #(latent - 1, batch_size, batch_size)\n  neighbor_new_dists <-\n    tf$gather(all_dists[2:-1, , ],\n              top_indices[1:-2, , ],\n              batch_dims = -1L)\n  \n  # Eq. 4 of Kennel et al.\n  #(latent - 1, batch_size, batch_size)\n  scaled_dist <- tf$sqrt((\n    tf$square(neighbor_new_dists) -\n      # (9, 8, 2)\n      tf$square(neighbor_dists_d[1:-2, , ])) /\n      # (9, 8, 2)\n      tf$square(neighbor_dists_d[1:-2, , ])\n  )\n  \n  # Kennel condition #1\n  #(latent - 1, batch_size, batch_size)\n  is_false_change <- (scaled_dist > rtol)\n  # Kennel condition 2\n  #(latent - 1, batch_size, batch_size)\n  is_large_jump <-\n    (neighbor_new_dists > atol * all_ra[1:-2, tf$newaxis, tf$newaxis])\n  \n  is_false_neighbor <-\n    tf$math$logical_or(is_false_change, is_large_jump)\n  #(latent - 1, batch_size, 1)\n  total_false_neighbors <-\n    tf$cast(is_false_neighbor, tf$int32)[reticulate::py_ellipsis(), 2:(k + 2)]\n  \n  # Pad zero to match dimensionality of latent space\n  # (latent - 1)\n  reg_weights <-\n    1 - tf$reduce_mean(tf$cast(total_false_neighbors, tf$float32), axis = c(1L, 2L))\n  # (latent,)\n  reg_weights <- tf$pad(reg_weights, list(list(1L, 0L)))\n  \n  # Find batch average activity\n  \n  # L2 Activity regularization\n  activations_batch_averaged <-\n    tf$sqrt(tf$reduce_mean(tf$square(x), axis = 0L))\n  \n  loss <- tf$reduce_sum(tf$multiply(reg_weights, activations_batch_averaged))\n  loss\n  \n}\n\nTraining is unchanged as well, apart from the fact that now, we continually output latent variable variances in addition to the losses. This is because with FNN-LSTM, we have to choose an adequate weight for the FNN loss component. An “adequate weight” is one where the variance drops sharply after the first n variables, with n thought to correspond to attractor dimensionality. For the Lorenz system discussed in the previous post, this is how these variances looked:\n\n     V1       V2        V3        V4        V5        V6        V7        V8        V9       V10\n 0.0739   0.0582   1.12e-6   3.13e-4   1.43e-5   1.52e-8   1.35e-6   1.86e-4   1.67e-4   4.39e-5\nIf we take variance as an indicator of importance, the first two variables are clearly more important than the rest. This finding nicely corresponds to “official” estimates of Lorenz attractor dimensionality. For example, the correlation dimension is estimated to lie around 2.05 (Grassberger and Procaccia 1983).\nThus, here we have the training routine:\n\n\ntrain_step <- function(batch) {\n  with (tf$GradientTape(persistent = TRUE) %as% tape, {\n    code <- encoder(batch[[1]])\n    prediction <- decoder(code)\n    \n    l_mse <- mse_loss(batch[[2]], prediction)\n    l_fnn <- loss_false_nn(code)\n    loss <- l_mse + fnn_weight * l_fnn\n  })\n  \n  encoder_gradients <-\n    tape$gradient(loss, encoder$trainable_variables)\n  decoder_gradients <-\n    tape$gradient(loss, decoder$trainable_variables)\n  \n  optimizer$apply_gradients(purrr::transpose(list(\n    encoder_gradients, encoder$trainable_variables\n  )))\n  optimizer$apply_gradients(purrr::transpose(list(\n    decoder_gradients, decoder$trainable_variables\n  )))\n  \n  train_loss(loss)\n  train_mse(l_mse)\n  train_fnn(l_fnn)\n  \n  \n}\n\ntraining_loop <- tf_function(autograph(function(ds_train) {\n  for (batch in ds_train) {\n    train_step(batch)\n  }\n  \n  tf$print(\"Loss: \", train_loss$result())\n  tf$print(\"MSE: \", train_mse$result())\n  tf$print(\"FNN loss: \", train_fnn$result())\n  \n  train_loss$reset_states()\n  train_mse$reset_states()\n  train_fnn$reset_states()\n  \n}))\n\n\nmse_loss <-\n  tf$keras$losses$MeanSquaredError(reduction = tf$keras$losses$Reduction$SUM)\n\ntrain_loss <- tf$keras$metrics$Mean(name = 'train_loss')\ntrain_fnn <- tf$keras$metrics$Mean(name = 'train_fnn')\ntrain_mse <-  tf$keras$metrics$Mean(name = 'train_mse')\n\n# fnn_multiplier should be chosen individually per dataset\n# this is the value we used on the geyser dataset\nfnn_multiplier <- 0.7\nfnn_weight <- fnn_multiplier * nrow(x_train)/batch_size\n\n# learning rate may also need adjustment\noptimizer <- optimizer_adam(lr = 1e-3)\n\nfor (epoch in 1:200) {\n cat(\"Epoch: \", epoch, \" -----------\\n\")\n training_loop(ds_train)\n \n test_batch <- as_iterator(ds_test) %>% iter_next()\n encoded <- encoder(test_batch[[1]]) \n test_var <- tf$math$reduce_variance(encoded, axis = 0L)\n print(test_var %>% as.numeric() %>% round(5))\n}\n\nOn to what we’ll use as a baseline for comparison.\nVanilla LSTM\nHere is the vanilla LSTM, stacking two layers, each, again, of size 32. Dropout and recurrent dropout were chosen individually per dataset, as was the learning rate.\n\n\nlstm <- function(n_latent, n_timesteps, n_features, n_recurrent, dropout, recurrent_dropout,\n                 optimizer = optimizer_adam(lr =  1e-3)) {\n  \n  model <- keras_model_sequential() %>%\n    layer_lstm(\n      units = n_recurrent,\n      input_shape = c(n_timesteps, n_features),\n      dropout = dropout, \n      recurrent_dropout = recurrent_dropout,\n      return_sequences = TRUE\n    ) %>% \n    layer_lstm(\n      units = n_recurrent,\n      dropout = dropout,\n      recurrent_dropout = recurrent_dropout,\n      return_sequences = TRUE\n    ) %>% \n    time_distributed(layer_dense(units = 1))\n  \n  model %>%\n    compile(\n      loss = \"mse\",\n      optimizer = optimizer\n    )\n  model\n  \n}\n\nmodel <- lstm(n_latent, n_timesteps, n_features, n_hidden, dropout = 0.2, recurrent_dropout = 0.2)\n\nData preparation\nFor all experiments, data were prepared in the same way.\nIn every case, we used the first 10000 measurements available in the respective .pkl files provided by Gilpin in his GitHub repository. To save on file size and not depend on an external data source, we extracted those first 10000 entries to .csv files downloadable directly from this blog’s repo:\n\n\ngeyser <- download.file(\n  \"https://raw.githubusercontent.com/rstudio/ai-blog/master/docs/posts/2020-07-20-fnn-lstm/data/geyser.csv\",\n  \"data/geyser.csv\")\n\nelectricity <- download.file(\n  \"https://raw.githubusercontent.com/rstudio/ai-blog/master/docs/posts/2020-07-20-fnn-lstm/data/electricity.csv\",\n  \"data/electricity.csv\")\n\necg <- download.file(\n  \"https://raw.githubusercontent.com/rstudio/ai-blog/master/docs/posts/2020-07-20-fnn-lstm/data/ecg.csv\",\n  \"data/ecg.csv\")\n\nmouse <- download.file(\n  \"https://raw.githubusercontent.com/rstudio/ai-blog/master/docs/posts/2020-07-20-fnn-lstm/data/mouse.csv\",\n  \"data/mouse.csv\")\n\nShould you want to access the complete time series (of considerably greater lengths), just download them from Gilpin’s repo and load them using reticulate:\n\n\n# e.g.\ngeyser <- reticulate::py_load_object(\"geyser_train_test.pkl\")\n\nHere is the data preparation code for the first dataset, geyser - all other datasets were treated the same way.\n\n\n# the first 10000 measurements from the compilation provided by Gilpin\ngeyser <- read_csv(\"geyser.csv\", col_names = FALSE) %>% select(X1) %>% pull() %>% unclass()\n\n# standardize\ngeyser <- scale(geyser)\n\n# varies per dataset; see below \nn_timesteps <- 60\nbatch_size <- 32\n\n# transform into [batch_size, timesteps, features] format required by RNNs\ngen_timesteps <- function(x, n_timesteps) {\n  do.call(rbind,\n          purrr::map(seq_along(x),\n                     function(i) {\n                       start <- i\n                       end <- i + n_timesteps - 1\n                       out <- x[start:end]\n                       out\n                     })\n  ) %>%\n    na.omit()\n}\n\nn <- 10000\ntrain <- gen_timesteps(geyser[1:(n/2)], 2 * n_timesteps)\ntest <- gen_timesteps(geyser[(n/2):n], 2 * n_timesteps) \n\ndim(train) <- c(dim(train), 1)\ndim(test) <- c(dim(test), 1)\n\n# split into input and target  \nx_train <- train[ , 1:n_timesteps, , drop = FALSE]\ny_train <- train[ , (n_timesteps + 1):(2*n_timesteps), , drop = FALSE]\n\nx_test <- test[ , 1:n_timesteps, , drop = FALSE]\ny_test <- test[ , (n_timesteps + 1):(2*n_timesteps), , drop = FALSE]\n\n# create tfdatasets\nds_train <- tensor_slices_dataset(list(x_train, y_train)) %>%\n  dataset_shuffle(nrow(x_train)) %>%\n  dataset_batch(batch_size)\n\nds_test <- tensor_slices_dataset(list(x_test, y_test)) %>%\n  dataset_batch(nrow(x_test))\n\nNow we’re ready to look at how forecasting goes on our four datasets.\nExperiments\nGeyser dataset\nPeople working with time series may have heard of Old Faithful, a geyser in Wyoming, US that has continually been erupting every 44 minutes to two hours since the year 2004. For the subset of data Gilpin extracted3,\n\ngeyser_train_test.pkl corresponds to detrended temperature readings from the main runoff pool of the Old Faithful geyser in Yellowstone National Park, downloaded from the GeyserTimes database. Temperature measurements start on April 13, 2015 and occur in one-minute increments.\n\nLike we said above, geyser.csv is a subset of these measurements, comprising the first 10000 data points. To choose an adequate timestep for the LSTMs, we inspect the series at various resolutions:\n\n\n\nFigure 1: Geyer dataset. Top: First 1000 observations. Bottom: Zooming in on the first 200.\n\n\n\nIt seems like the behavior is periodic with a period of about 40-50; a timestep of 60 thus seemed like a good try.\nHaving trained both FNN-LSTM and the vanilla LSTM for 200 epochs, we first inspect the variances of the latent variables on the test set. The value of fnn_multiplier corresponding to this run was 0.7.\n\n\ntest_batch <- as_iterator(ds_test) %>% iter_next()\nencoded <- encoder(test_batch[[1]]) %>%\n  as.array() %>%\n  as_tibble()\n\nencoded %>% summarise_all(var)\n\n\n   V1     V2        V3          V4       V5       V6       V7       V8       V9      V10\n0.258 0.0262 0.0000627 0.000000600 0.000533 0.000362 0.000238 0.000121 0.000518 0.000365\nThere is a drop in importance between the first two variables and the rest; however, unlike in the Lorenz system, V1 and V2 variances also differ by an order of magnitude.\nNow, it’s interesting to compare prediction errors for both models. We are going to make an observation that will carry through to all three datasets to come.\nKeeping up the suspense for a while, here is the code used to compute per-timestep prediction errors from both models. The same code will be used for all other datasets.\n\n\ncalc_mse <- function(df, y_true, y_pred) {\n  (sum((df[[y_true]] - df[[y_pred]])^2))/nrow(df)\n}\n\nget_mse <- function(test_batch, prediction) {\n  \n  comp_df <- \n    data.frame(\n      test_batch[[2]][, , 1] %>%\n        as.array()) %>%\n        rename_with(function(name) paste0(name, \"_true\")) %>%\n    bind_cols(\n      data.frame(\n        prediction[, , 1] %>%\n          as.array()) %>%\n          rename_with(function(name) paste0(name, \"_pred\")))\n  \n  mse <- purrr::map(1:dim(prediction)[2],\n                        function(varno)\n                          calc_mse(comp_df,\n                                   paste0(\"X\", varno, \"_true\"),\n                                   paste0(\"X\", varno, \"_pred\"))) %>%\n    unlist()\n  \n  mse\n}\n\nprediction_fnn <- decoder(encoder(test_batch[[1]]))\nmse_fnn <- get_mse(test_batch, prediction_fnn)\n\nprediction_lstm <- model %>% predict(ds_test)\nmse_lstm <- get_mse(test_batch, prediction_lstm)\n\nmses <- data.frame(timestep = 1:n_timesteps, fnn = mse_fnn, lstm = mse_lstm) %>%\n  gather(key = \"type\", value = \"mse\", -timestep)\n\nggplot(mses, aes(timestep, mse, color = type)) +\n  geom_point() +\n  scale_color_manual(values = c(\"#00008B\", \"#3CB371\")) +\n  theme_classic() +\n  theme(legend.position = \"none\") \n\nAnd here is the actual comparison. One thing especially jumps to the eye: FNN-LSTM forecast error is significantly lower for initial timesteps, first and foremost, for the very first prediction, which from this graph we expect to be pretty good!\n\n\n\nFigure 2: Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM.\n\n\n\nInterestingly, we see “jumps” in prediction error, for FNN-LSTM, between the very first forecast and the second, and then between the second and the ensuing ones, reminding of the similar jumps in variable importance for the latent code! After the first ten timesteps, vanilla LSTM has caught up with FNN-LSTM, and we won’t interpret further development of the losses based on just a single run’s output.\nInstead, let’s inspect actual predictions. We randomly pick sequences from the test set, and ask both FNN-LSTM and vanilla LSTM for a forecast. The same procedure will be followed for the other datasets.\n\n\ngiven <- data.frame(as.array(tf$concat(list(\n  test_batch[[1]][, , 1], test_batch[[2]][, , 1]\n),\naxis = 1L)) %>% t()) %>%\n  add_column(type = \"given\") %>%\n  add_column(num = 1:(2 * n_timesteps))\n\nfnn <- data.frame(as.array(prediction_fnn[, , 1]) %>%\n                    t()) %>%\n  add_column(type = \"fnn\") %>%\n  add_column(num = (n_timesteps  + 1):(2 * n_timesteps))\n\nlstm <- data.frame(as.array(prediction_lstm[, , 1]) %>%\n                     t()) %>%\n  add_column(type = \"lstm\") %>%\n  add_column(num = (n_timesteps + 1):(2 * n_timesteps))\n\ncompare_preds_df <- bind_rows(given, lstm, fnn)\n\nplots <- \n  purrr::map(sample(1:dim(compare_preds_df)[2], 16),\n             function(v) {\n               ggplot(compare_preds_df, aes(num, .data[[paste0(\"X\", v)]], color = type)) +\n                 geom_line() +\n                 theme_classic() +\n                 theme(legend.position = \"none\", axis.title = element_blank()) +\n                 scale_color_manual(values = c(\"#00008B\", \"#DB7093\", \"#3CB371\"))\n             })\n\nplot_grid(plotlist = plots, ncol = 4)\n\nHere are sixteen random picks of predictions on the test set. The ground truth is displayed in pink; blue forecasts are from FNN-LSTM, green ones from vanilla LSTM.\n\n\n\nFigure 3: 60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth.\n\n\n\nWhat we expect from the error inspection comes true: FNN-LSTM yields significantly better predictions for immediate continuations of a given sequence.\nLet’s move on to the second dataset on our list.\nElectricity dataset\nThis is a dataset on power consumption, aggregated over 321 different households and fifteen-minute-intervals.\n\nelectricity_train_test.pkl corresponds to average power consumption by 321 Portuguese households between 2012 and 2014, in units of kilowatts consumed in fifteen minute increments. This dataset is from the UCI machine learning database.4\n\nHere, we see a very regular pattern:\n\n\n\nFigure 4: Electricity dataset. Top: First 2000 observations. Bottom: Zooming in on 500 observations, skipping the very beginning of the series.\n\n\n\nWith such regular behavior, we immediately tried to predict a higher number of timesteps (120) – and didn’t have to retract behind that aspiration.\nFor an fnn_multiplier of 0.5, latent variable variances look like this:\n\nV1          V2            V3       V4       V5            V6       V7         V8      V9     V10\n0.390 0.000637 0.00000000288 1.48e-10 2.10e-11 0.00000000119 6.61e-11 0.00000115 1.11e-4 1.40e-4\nWe definitely see a sharp drop already after the first variable.\nHow do prediction errors compare on the two architectures?\n\n\n\nFigure 5: Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM.\n\n\n\nHere, FNN-LSTM performs better over a long range of timesteps, but again, the difference is most visible for immediate predictions. Will an inspection of actual predictions confirm this view?\n\n\n\nFigure 6: 60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth.\n\n\n\nIt does! In fact, forecasts from FNN-LSTM are very impressive on all time scales.\nNow that we’ve seen the easy and predictable, let’s approach the weird and difficult.\nECG dataset\nSays Gilpin,\n\necg_train.pkl and ecg_test.pkl correspond to ECG measurements for two different patients, taken from the PhysioNet QT database.5\n\nHow do these look?\n\n\n\nFigure 7: ECG dataset. Top: First 1000 observations. Bottom: Zooming in on the first 400 observations.\n\n\n\nTo the layperson that I am, these do not look nearly as regular as expected. First experiments showed that both architectures are not capable of dealing with a high number of timesteps. In every try, FNN-LSTM performed better for the very first timestep.\nThis is also the case for n_timesteps = 12, the final try (after 120, 60 and 30). With an fnn_multiplier of 1, the latent variances obtained amounted to the following:\n\n     V1        V2          V3        V4         V5       V6       V7         V8         V9       V10\n  0.110  1.16e-11     3.78e-9 0.0000992    9.63e-9  4.65e-5  1.21e-4    9.91e-9    3.81e-9   2.71e-8\nThere is a gap between the first variable and all other ones; but not much variance is explained by V1 either.\nApart from the very first prediction, vanilla LSTM shows lower forecast errors this time; however, we have to add that this was not consistently observed when experimenting with other timestep settings.\n\n\n\nFigure 8: Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM.\n\n\n\nLooking at actual predictions, both architectures perform best when a persistence forecast is adequate – in fact, they produce one even when it is not.\n\n\n\nFigure 9: 60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth.\n\n\n\nOn this dataset, we certainly would want to explore other architectures better able to capture the presence of high and low frequencies in the data, such as mixture models. But – were we forced to stay with one of these, and could do a one-step-ahead, rolling forecast, we’d go with FNN-LSTM.\nSpeaking of mixed frequencies – we haven’t seen the extremes yet …\nMouse dataset\n“Mouse”, that’s spike rates recorded from a mouse thalamus.\n\nmouse.pkl A time series of spiking rates for a neuron in a mouse thalamus. Raw spike data was obtained from CRCNS and processed with the authors' code in order to generate a spike rate time series.6\n\n\n\n\nFigure 10: Mouse dataset. Top: First 2000 observations. Bottom: Zooming in on the first 500 observations.\n\n\n\nObviously, this dataset will be very hard to predict. How, after “long” silence, do you know that a neuron is going to fire?\nAs usual, we inspect latent code variances (fnn_multiplier was set to 0.4):\n\n\n     V1       V2        V3         V4       V5       V6        V7      V8       V9        V10\n 0.0796  0.00246  0.000214    2.26e-7   .71e-9  4.22e-8  6.45e-10 1.61e-4 2.63e-10    2.05e-8\n>\n\nAgain, we don’t see the first variable explaining much variance. Still, interestingly, when inspecting forecast errors we get a picture very similar to the one obtained on our first, geyser, dataset:\n\n\n\nFigure 11: Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM.\n\n\n\nSo here, the latent code definitely seems to help! With every timestep “more” that we try to predict, prediction performance goes down continuously – or put the other way round, short-time predictions are expected to be pretty good!\nLet’s see:\n\n\n\nFigure 12: 60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth.\n\n\n\nIn fact on this dataset, the difference in behavior between both architectures is striking. When nothing is “supposed to happen”, vanilla LSTM produces “flat” curves at about the mean of the data, while FNN-LSTM takes the effort to “stay on track” as long as possible before also converging to the mean. Choosing FNN-LSTM – had we to choose one of these two – would be an obvious decision with this dataset.\nDiscussion\nWhen, in timeseries forecasting, would we consider FNN-LSTM? Judging by the above experiments, conducted on four very different datasets: Whenever we consider a deep learning approach. Of course, this has been a casual exploration – and it was meant to be, as – hopefully – was evident from the nonchalant and bloomy (sometimes) writing style.\nThroughout the text, we’ve emphasized utility – how could this technique be used to improve predictions? But, looking at the above results, a number of interesting questions come to mind. We already speculated (though in an indirect way) whether the number of high-variance variables in the latent code was relatable to how far we could sensibly forecast into the future. However, even more intriguing is the question of how characteristics of the dataset itself affect FNN efficiency.\nSuch characteristics could be:\nHow nonlinear is the dataset? (Put differently, how incompatible, as indicated by some form of test algorithm, is it with the hypothesis that the data generation mechanism was a linear one?)\nTo what degree does the system appear to be sensitively dependent on initial conditions? In other words, what is the value of its (estimated, from the observations) highest Lyapunov exponent?\nWhat is its (estimated) dimensionality, for example, in terms of correlation dimension?\nWhile it is easy to obtain those estimates, using, for instance, the nonlinearTseries package explicitly modeled after practices described in Kantz & Schreiber’s classic (Kantz and Schreiber 2004), we don’t want to extrapolate from our tiny sample of datasets, and leave such explorations and analyses to further posts, and/or the interested reader’s ventures :-). In any case, we hope you enjoyed the demonstration of practical usability of an approach that in the preceding post, was mainly introduced in terms of its conceptual attractivity.\nThanks for reading!\n\n\nGilpin, William. 2020. “Deep Reconstruction of Strange Attractors from Time Series.” http://arxiv.org/abs/2002.05909.\n\n\nGrassberger, Peter, and Itamar Procaccia. 1983. “Measuring the Strangeness of Strange Attractors.” Physica D: Nonlinear Phenomena 9 (1): 189–208. https://doi.org/https://doi.org/10.1016/0167-2789(83)90298-1.\n\n\nKantz, Holger, and Thomas Schreiber. 2004. Nonlinear Time Series Analysis. Cambridge University Press.\n\n\nSauer, Tim, James A. Yorke, and Martin Casdagli. 1991. “Embedology.” Journal of Statistical Physics 65 (3-4): 579–616. https://doi.org/10.1007/BF01053745.\n\n\nPlease refer to the aforementioned predecessor post for a detailed introduction.↩︎\n“Basically” because FNN-LSTM technically has three LSTMs – the third one, with n_latent = 10 units, being used to store the latent code.↩︎\nsee dataset descriptions in the repository's README↩︎\nagain, citing from Gilpin’s repository’s README.↩︎\nagain, citing from Gilpin’s repository’s README.↩︎\nagain, citing from Gilpin’s repository’s README.↩︎\n",
    "preview": "posts/2020-07-20-fnn-lstm/images/old_faithful.jpg",
    "last_modified": "2020-09-30T20:24:31+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-07-16-sparklyr-1.3.0-released/",
    "title": "sparklyr 1.3: Higher-order Functions, Avro and Custom Serializers",
    "description": "Sparklyr 1.3 is now available, featuring exciting new functionalities such as integration of Spark higher-order functions and data import/export in Avro and in user-defined serialization formats.",
    "author": [
      {
        "name": "Yitao Li",
        "url": "https://github.com/yl790"
      }
    ],
    "date": "2020-07-16",
    "categories": [
      "Packages/Releases",
      "Distributed Computing"
    ],
    "contents": "\nsparklyr 1.3 is now available on CRAN, with the following major new features:\nHigher-order Functions to easily manipulate arrays and structs\nSupport for Apache Avro, a row-oriented data serialization framework\nCustom Serialization using R functions to read and write any data format\nOther Improvements such as compatibility with EMR 6.0 & Spark 3.0, and initial support for Flint time series library\nTo install sparklyr 1.3 from CRAN, run\n\n\ninstall.packages(\"sparklyr\")\n\nIn this post, we shall highlight some major new features introduced in sparklyr 1.3, and showcase scenarios where such features come in handy. While a number of enhancements and bug fixes (especially those related to spark_apply(), Apache Arrow, and secondary Spark connections) were also an important part of this release, they will not be the topic of this post, and it will be an easy exercise for the reader to find out more about them from the sparklyr NEWS file.\nHigher-order Functions\nHigher-order functions are built-in Spark SQL constructs that allow user-defined lambda expressions to be applied efficiently to complex data types such as arrays and structs. As a quick demo to see why higher-order functions are useful, let’s say one day Scrooge McDuck dove into his huge vault of money and found large quantities of pennies, nickels, dimes, and quarters. Having an impeccable taste in data structures, he decided to store the quantities and face values of everything into two Spark SQL array columns:\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\", version = \"2.4.5\")\ncoins_tbl <- copy_to(\n  sc,\n  tibble::tibble(\n    quantities = list(c(4000, 3000, 2000, 1000)),\n    values = list(c(1, 5, 10, 25))\n  )\n)\n\nThus declaring his net worth of 4k pennies, 3k nickels, 2k dimes, and 1k quarters. To help Scrooge McDuck calculate the total value of each type of coin in sparklyr 1.3 or above, we can apply hof_zip_with(), the sparklyr equivalent of ZIP_WITH, to quantities column and values column, combining pairs of elements from arrays in both columns. As you might have guessed, we also need to specify how to combine those elements, and what better way to accomplish that than a concise one-sided formula   ~ .x * .y   in R, which says we want (quantity * value) for each type of coin? So, we have the following:\n\n\nresult_tbl <- coins_tbl %>%\n  hof_zip_with(~ .x * .y, dest_col = total_values) %>%\n  dplyr::select(total_values)\n\nresult_tbl %>% dplyr::pull(total_values)\n\n\n[1]  4000 15000 20000 25000\nWith the result 4000 15000 20000 25000 telling us there are in total $40 dollars worth of pennies, $150 dollars worth of nickels, $200 dollars worth of dimes, and $250 dollars worth of quarters, as expected.\nUsing another sparklyr function named hof_aggregate(), which performs an AGGREGATE operation in Spark, we can then compute the net worth of Scrooge McDuck based on result_tbl, storing the result in a new column named total. Notice for this aggregate operation to work, we need to ensure the starting value of aggregation has data type (namely, BIGINT) that is consistent with the data type of total_values (which is ARRAY<BIGINT>), as shown below:\n\n\nresult_tbl %>%\n  dplyr::mutate(zero = dplyr::sql(\"CAST (0 AS BIGINT)\")) %>%\n  hof_aggregate(start = zero, ~ .x + .y, expr = total_values, dest_col = total) %>%\n  dplyr::select(total) %>%\n  dplyr::pull(total)\n\n\n[1] 64000\nSo Scrooge McDuck’s net worth is $640 dollars.\nOther higher-order functions supported by Spark SQL so far include transform, filter, and exists, as documented in here, and similar to the example above, their counterparts (namely, hof_transform(), hof_filter(), and hof_exists()) all exist in sparklyr 1.3, so that they can be integrated with other dplyr verbs in an idiomatic manner in R.\nAvro\nAnother highlight of the sparklyr 1.3 release is its built-in support for Avro data sources. Apache Avro is a widely used data serialization protocol that combines the efficiency of a binary data format with the flexibility of JSON schema definitions. To make working with Avro data sources simpler, in sparklyr 1.3, as soon as a Spark connection is instantiated with spark_connect(..., package = \"avro\"), sparklyr will automatically figure out which version of spark-avro package to use with that connection, saving a lot of potential headaches for sparklyr users trying to determine the correct version of spark-avro by themselves. Similar to how spark_read_csv() and spark_write_csv() are in place to work with CSV data, spark_read_avro() and spark_write_avro() methods were implemented in sparklyr 1.3 to facilitate reading and writing Avro files through an Avro-capable Spark connection, as illustrated in the example below:\n\n\nlibrary(sparklyr)\n\n# The `package = \"avro\"` option is only supported in Spark 2.4 or higher\nsc <- spark_connect(master = \"local\", version = \"2.4.5\", package = \"avro\")\n\nsdf <- sdf_copy_to(\n  sc,\n  tibble::tibble(\n    a = c(1, NaN, 3, 4, NaN),\n    b = c(-2L, 0L, 1L, 3L, 2L),\n    c = c(\"a\", \"b\", \"c\", \"\", \"d\")\n  )\n)\n\n# This example Avro schema is a JSON string that essentially says all columns\n# (\"a\", \"b\", \"c\") of `sdf` are nullable.\navro_schema <- jsonlite::toJSON(list(\n  type = \"record\",\n  name = \"topLevelRecord\",\n  fields = list(\n    list(name = \"a\", type = list(\"double\", \"null\")),\n    list(name = \"b\", type = list(\"int\", \"null\")),\n    list(name = \"c\", type = list(\"string\", \"null\"))\n  )\n), auto_unbox = TRUE)\n\n# persist the Spark data frame from above in Avro format\nspark_write_avro(sdf, \"/tmp/data.avro\", as.character(avro_schema))\n\n# and then read the same data frame back\nspark_read_avro(sc, \"/tmp/data.avro\")\n\n\n# Source: spark<data> [?? x 3]\n      a     b c\n  <dbl> <int> <chr>\n  1     1    -2 \"a\"\n  2   NaN     0 \"b\"\n  3     3     1 \"c\"\n  4     4     3 \"\"\n  5   NaN     2 \"d\"\n\nCustom Serialization\nIn addition to commonly used data serialization formats such as CSV, JSON, Parquet, and Avro, starting from sparklyr 1.3, customized data frame serialization and deserialization procedures implemented in R can also be run on Spark workers via the newly implemented spark_read() and spark_write() methods. We can see both of them in action through a quick example below, where saveRDS() is called from a user-defined writer function to save all rows within a Spark data frame into 2 RDS files on disk, and readRDS() is called from a user-defined reader function to read the data from the RDS files back to Spark:\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\nsdf <- sdf_len(sc, 7)\npaths <- c(\"/tmp/file1.RDS\", \"/tmp/file2.RDS\")\n\nspark_write(sdf, writer = function(df, path) saveRDS(df, path), paths = paths)\nspark_read(sc, paths, reader = function(path) readRDS(path), columns = c(id = \"integer\"))\n\n\n# Source: spark<?> [?? x 1]\n     id\n  <int>\n1     1\n2     2\n3     3\n4     4\n5     5\n6     6\n7     7\nOther Improvements\nSparklyr.flint\nSparklyr.flint is a sparklyr extension that aims to make functionalities from the Flint time-series library easily accessible from R. It is currently under active development. One piece of good news is that, while the original Flint library was designed to work with Spark 2.x, a slightly modified fork of it will work well with Spark 3.0, and within the existing sparklyr extension framework. sparklyr.flint can automatically determine which version of the Flint library to load based on the version of Spark it’s connected to. Another bit of good news is, as previously mentioned, sparklyr.flint doesn’t know too much about its own destiny yet. Maybe you can play an active part in shaping its future!\nEMR 6.0\nThis release also features a small but important change that allows sparklyr to correctly connect to the version of Spark 2.4 that is included in Amazon EMR 6.0.\nPreviously, sparklyr automatically assumed any Spark 2.x it was connecting to was built with Scala 2.11 and attempted to load any required Scala artifacts built with Scala 2.11 as well. This became problematic when connecting to Spark 2.4 from Amazon EMR 6.0, which is built with Scala 2.12. Starting from sparklyr 1.3, such problem can be fixed by simply specifying scala_version = \"2.12\" when calling spark_connect() (e.g., spark_connect(master = \"yarn-client\", scala_version = \"2.12\")).\nSpark 3.0\nLast but not least, it is worthwhile to mention sparklyr 1.3.0 is known to be fully compatible with the recently released Spark 3.0. We highly recommend upgrading your copy of sparklyr to 1.3.0 if you plan to have Spark 3.0 as part of your data workflow in future.\nAcknowledgement\nIn chronological order, we want to thank the following individuals for submitting pull requests towards sparklyr 1.3:\nJozef Hajnala\nHossein Falaki\nSamuel Macêdo\nYitao Li\nAndy Zhang\nJavier Luraschi\nNeal Richardson\nWe are also grateful for valuable input on the sparklyr 1.3 roadmap, #2434, and #2551 from [@javierluraschi](https://github.com/javierluraschi), and great spiritual advice on #1773 and #2514 from @mattpollock and @benmwhite.\nPlease note if you believe you are missing from the acknowledgement above, it may be because your contribution has been considered part of the next sparklyr release rather than part of the current release. We do make every effort to ensure all contributors are mentioned in this section. In case you believe there is a mistake, please feel free to contact the author of this blog post via e-mail (yitao at rstudio dot com) and request a correction.\nIf you wish to learn more about sparklyr, we recommend visiting sparklyr.ai, spark.rstudio.com, and some of the previous release posts such as sparklyr 1.2 and sparklyr 1.1.\nThanks for reading!\n\n\n",
    "preview": "posts/2020-07-16-sparklyr-1.3.0-released/images/sparklyr-1.3.jpg",
    "last_modified": "2020-09-30T20:24:31+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-06-24-deep-attractors/",
    "title": "Deep attractors: Where deep learning meets chaos",
    "description": "In nonlinear dynamics, when the state space is thought to be multidimensional but all we have for data is just a univariate time series, one may attempt to reconstruct the true space via delay coordinate embeddings. However, it is not clear a priori how to choose dimensionality and time lag of the reconstruction space. In this post, we show how to use an autoencoder architecture to circumvent the problem: Given just a scalar series of observations, the autoencoder directly learns to represent attractors of chaotic systems in adequate dimensionality.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-06-24",
    "categories": [
      "R",
      "TensorFlow/Keras",
      "Time Series",
      "Unsupervised Learning"
    ],
    "contents": "\nFor us deep learning practitioners, the world is – not flat, but – linear, mostly. Or piecewise linear.1 Like other linear approximations, or maybe even more so, deep learning can be incredibly successful at making predictions. But let’s admit it – sometimes we just miss the thrill of the nonlinear, of good, old, deterministic-yet-unpredictable chaos. Can we have both? It looks like we can. In this post, we’ll see an application of deep learning (DL) to nonlinear time series prediction – or rather, the essential step that predates it: reconstructing the attractor underlying its dynamics. While this post is an introduction, presenting the topic from scratch, further posts will build on this and extrapolate to observational datasets.\nWhat to expect from this post\nIn his 2020 paper Deep reconstruction of strange attractors from time series (Gilpin 2020), William Gilpin uses an autoencoder architecture, combined with a regularizer implementing the false nearest neighbors statistic (Kennel, Brown, and Abarbanel 1992), to reconstruct attractors from univariate observations of multivariate, nonlinear dynamical systems. If you feel you completely understand the sentence you just read, you may as well directly jump to the paper – come back for the code though2. If, on the other hand, you’re more familiar with the chaos on your desk (extrapolating … apologies) than chaos theory chaos, read on. Here, we’ll first go into what it’s all about3, and then, show an example application, featuring Edward Lorenz’s famous butterfly attractor. While this initial post is primarily supposed to be a fun introduction to a fascinating topic, we hope to follow up with applications to real-world datasets in the future.\nRabbits, butterflies, and low-dimensional projections: Our problem statement in context\nIn curious misalignment with how we use “chaos” in day-to-day language, chaos, the technical concept, is very different from stochasticity, or randomness. Chaos may emerge from purely deterministic processes - very simplistic ones, even. Let’s see how; with rabbits.\nRabbits, or: Sensitive dependence on initial conditions\nYou may be familiar with the logistic equation, used as a toy model for population growth. Often it’s written like this – with \\(x\\) being the size of the population, expressed as a fraction of the maximal size (a fraction of possible rabbits, thus), and \\(r\\) being the growth rate (the rate at which rabbits reproduce):\n\\[\nx_{n + 1} = r \\ x_n \\ (1 - x_n)\n\\]\nThis equation describes an iterated map over discrete timesteps \\(n\\). Its repeated application results in a trajectory describing how the population of rabbits evolves. Maps can have fixed points, states where further function application goes on producing the same result forever. Example-wise, say the growth rate amounts to \\(2.1\\), and we start at two (pretty different!) initial values, \\(0.3\\) and \\(0.8\\). Both trajectories arrive at a fixed point – the same fixed point – in fewer than 10 iterations. Were we asked to predict the population size after a hundred iterations, we could make a very confident guess, whatever the of starting value. (If the initial value is \\(0\\), we stay at \\(0\\), but we can be pretty certain of that as well.)\n\n\n\nFigure 1: Trajectory of the logistic map for r = 2.1 and two different initial values.\n\n\n\nWhat if the growth rate were somewhat higher, at \\(3.3\\), say? Again, we immediately compare trajectories resulting from initial values \\(0.3\\) and \\(0.9\\):\n\n\n\nFigure 2: Trajectory of the logistic map for r = 3.3 and two different initial values.\n\n\n\nThis time, don’t see a single fixed point, but a two-cycle: As the trajectories stabilize, population size inevitably is at one of two possible values – either too many rabbits or too few, you could say. The two trajectories are phase-shifted, but again, the attracting values – the attractor – is shared by both initial conditions. So still, predictability is pretty high. But we haven’t seen everything yet.\nLet’s again enhance the growth rate some. Now this (literally) is chaos:\n\n\n\nFigure 3: Trajectory of the logistic map for r = 3.6 and two different initial values, 0.3 and 0.9.\n\n\n\nEven after a hundred iterations, there is no set of values the trajectories recur to. We can’t be confident about any prediction we might make.\nOr can we? After all, we have the governing equation, which is deterministic. So we should be able to calculate the size of the population at, say, time \\(150\\)? In principle, yes; but this presupposes we have an accurate measurement for the starting state.\nHow accurate? Let’s compare trajectories for initial values \\(0.3\\) and \\(0.301\\):\n\n\n\nFigure 4: Trajectory of the logistic map for r = 3.6 and two different initial values, 0.3 and 0.301.\n\n\n\nAt first, trajectories seem to jump around in unison; but during the second dozen iterations already, they dissociate more and more, and increasingly, all bets are off. What if initial values are really close, as in, \\(0.3\\) vs. \\(0.30000001\\)?\nIt just takes a bit longer for the disassociation to surface.\n\n\n\nFigure 5: Trajectory of the logistic map for r = 3.6 and two different initial values, 0.3 and 0.30000001.\n\n\n\nWhat we’re seeing here is sensitive dependence on initial conditions, an essential precondition for a system to be chaotic. In an nutshell: Chaos arises when a deterministic system shows sensitive dependence on initial conditions. Or as Edward Lorenz is said to have put it,\n\nWhen the present determines the future, but the approximate present does not approximately determine the future.\n\nNow if these unstructured, random-looking point clouds constitute chaos, what with the all-but-amorphous butterfly (to be displayed very soon)?\nButterflies, or: Attractors and strange attractors\nActually, in the context of chaos theory, the term butterfly may be encountered in different contexts.\nFirstly, as so-called “butterfly effect”, it is an instantiation of the templatic phrase “the flap of a butterfly’s wing in _________ profoundly affects the course of the weather in _________.”4 In this usage, it is mostly a metaphor for sensitive dependence on initial conditions.\nSecondly, the existence of this metaphor led to a Rorschach-test-like identification with two-dimensional visualizations of attractors of the Lorenz system. The Lorenz system is a set of three first-order differential equations designed to describe atmospheric convection:\n\\[\n\\begin{aligned}\n& \\frac{dx}{dt} = \\sigma (y - x)\\\\\n& \\frac{dy}{dt} = \\rho x - x z - y\\\\\n& \\frac{dz}{dt} = x y - \\beta z\n\\end{aligned}\n\\]\nThis set of equations is nonlinear, as required for chaotic behavior to appear. It also has the required dimensionality, which for smooth, continuous systems, is at least 35. Whether we actually see chaotic attractors – among which, the butterfly – depends on the settings of the parameters \\(\\sigma\\), \\(\\rho\\) and \\(\\beta\\). For the values conventionally chosen, \\(\\sigma=10\\), \\(\\rho=28\\), and \\(\\beta=8/3\\) , we see it when projecting the trajectory on the \\(x\\) and \\(z\\) axes:\n\n\n\nFigure 6: Two-dimensional projections of the Lorenz attractor for sigma = 10, rho = 28, beta = 8 / 3. On the right: the butterfly.\n\n\n\nThe butterfly is an attractor (as are the other two projections), but it is neither a point nor a cycle. It is an attractor in the sense that starting from a variety of different initial values, we end up in some sub-region of the state space, and we don’t get to escape no more. This is easier to see when watching evolution over time, as in this animation:\n\n\n\nFigure 7: How the Lorenz attractor traces out the famous “butterfly” shape.\n\n\n\nNow, to plot the attractor in two dimensions, we threw away the third. But in “real life”, we don’t usually have too much information (although it may sometimes seem like we had). We might have a lot of measurements, but these don’t usually reflect the actual state variables we’re interested in. In these cases, we may want to actually add information.\nEmbeddings (as a non-DL term), or: Undoing the projection\nAssume that instead of all three variables of the Lorenz system, we had measured just one: \\(x\\), the rate of convection. Often in nonlinear dynamics, the technique of delay coordinate embedding (Sauer, Yorke, and Casdagli 1991) is used to enhance a series of univariate measurements.\nIn this method – or family of methods – the univariate series is augmented by time-shifted copies of itself. There are two decisions to be made: How many copies to add, and how big the delay should be. To illustrate, if we had a scalar series,\n\n1 2 3 4 5 6 7 8 9 10 11 ...\na three-dimensional embedding with time delay 2 would look like this:\n\n1 3 5\n2 4 6\n3 5 7\n4 6 8\n5 7 9\n6 8 10\n7 9 11\n...\nOf the two decisions to be made – number of shifted series and time lag – the first is a decision on the dimensionality of the reconstruction space. Various theorems, such as Taken's theorem, indicate bounds on the number of dimensions required, provided the dimensionality of the true state space is known – which, in real-world applications, often is not the case.The second has been of little interest to mathematicians, but is important in practice. In fact, Kantz and Schreiber (Kantz and Schreiber 2004) argue that in practice, it is the product of both parameters that matters, as it indicates the time span represented by an embedding vector.\nHow are these parameters chosen? Regarding reconstruction dimensionality, the reasoning goes that even in chaotic systems, points that are close in state space at time \\(t\\) should still be close at time \\(t + \\Delta t\\), provided \\(\\Delta t\\) is very small. So say we have two points that are close, by some metric, when represented in two-dimensional space. But in three dimensions, that is, if we don’t “project away” the third dimension, they are a lot more distant. As illustrated in (Gilpin 2020):\n\n\n\nFigure 8: In the two-dimensional projection on axes x and y, the red points are close neighbors. In 3d, however, they are separate. Compare with the blue points, which stay close even in higher-dimensional space. Figure from Gilpin (2020).\n\n\n\nIf this happens, then projecting down has eliminated some essential information. In 2d, the points were false neighbors. The false nearest neighbors (FNN) statistic can be used to determine an adequate embedding size, like this:\nFor each point, take its closest neighbor in \\(m\\) dimensions, and compute the ratio of their distances in \\(m\\) and \\(m+1\\) dimensions. If the ratio is larger than some threshold \\(t\\), the neighbor was false. Sum the number of false neighbors over all points. Do this for different \\(m\\) and \\(t\\), and inspect the resulting curves.\nAt this point, let’s look ahead at the autoencoder approach. The autoencoder will use that same FNN statistic as a regularizer, in addition to the usual autoencoder reconstruction loss. This will result in a new heuristic regarding embedding dimensionality that involves fewer decisions.\nGoing back to the classic method for an instant, the second parameter, the time lag, is even more difficult to sort out (Kantz and Schreiber 2004). Usually, mutual information is plotted for different delays and then, the first delay where it falls below some threshold is chosen. We don’t further elaborate on this question as it is rendered obsolete in the neural network approach. Which we’ll see now.\nLearning the Lorenz attractor\nOur code closely follows the architecture, parameter settings, and data setup used in the reference implementation William provided. The loss function, especially, has been ported one-to-one.\nThe general idea is the following. An autoencoder – for example, an LSTM autoencoder as presented here – is used to compress the univariate time series into a latent representation of some dimensionality, which will constitute an upper bound on the dimensionality of the learned attractor. In addition to mean squared error between input and reconstructions, there will be a second loss term, applying the FNN regularizer. This results in the latent units being roughly ordered by importance, as measured by their variance. It is expected that somewhere in the listing of variances, a sharp drop will appear. The units before the drop are then assumed to encode the attractor of the system in question.\nIn this setup, there is still a choice to be made: how to weight the FNN loss. One would run training for different weights \\(\\lambda\\) and look for the drop. Surely, this could in principle be automated, but given the newness of the method – the paper was published this year – it makes sense to focus on thorough analysis first.\nData generation\nWe use the deSolve package to generate data from the Lorenz equations.\n\n\nlibrary(deSolve)\nlibrary(tidyverse)\n\nparameters <- c(sigma = 10,\n                rho = 28,\n                beta = 8/3)\n\ninitial_state <-\n  c(x = -8.60632853,\n    y = -14.85273055,\n    z = 15.53352487)\n\nlorenz <- function(t, state, parameters) {\n  with(as.list(c(state, parameters)), {\n    dx <- sigma * (y - x)\n    dy <- x * (rho - z) - y\n    dz <- x * y - beta * z\n    \n    list(c(dx, dy, dz))\n  })\n}\n\ntimes <- seq(0, 500, length.out = 125000)\n\nlorenz_ts <-\n  ode(\n    y = initial_state,\n    times = times,\n    func = lorenz,\n    parms = parameters,\n    method = \"lsoda\"\n  ) %>% as_tibble()\n\nlorenz_ts[1:10,]\n\n\n# A tibble: 10 x 4\n      time      x     y     z\n     <dbl>  <dbl> <dbl> <dbl>\n 1 0        -8.61 -14.9  15.5\n 2 0.00400  -8.86 -15.2  15.9\n 3 0.00800  -9.12 -15.6  16.3\n 4 0.0120   -9.38 -16.0  16.7\n 5 0.0160   -9.64 -16.3  17.1\n 6 0.0200   -9.91 -16.7  17.6\n 7 0.0240  -10.2  -17.0  18.1\n 8 0.0280  -10.5  -17.3  18.6\n 9 0.0320  -10.7  -17.7  19.1\n10 0.0360  -11.0  -18.0  19.7\nWe’ve already seen the attractor, or rather, its three two-dimensional projections, in figure 6 above. But now our scenario is different. We only have access to \\(x\\), a univariate time series. As the time interval used to numerically integrate the differential equations was rather tiny, we just use every tenth observation.\n\n\nobs <- lorenz_ts %>%\n  select(time, x) %>%\n  filter(row_number() %% 10 == 0)\n\nggplot(obs, aes(time, x)) +\n  geom_line() +\n  coord_cartesian(xlim = c(0, 100)) +\n  theme_classic()\n\n\n\n\nFigure 9: Convection rates as a univariate time series.\n\n\n\nPreprocessing\nThe first half of the series is used for training. The data is scaled and transformed into the three-dimensional form expected by recurrent layers.\n\n\nlibrary(keras)\nlibrary(tfdatasets)\nlibrary(tfautograph)\nlibrary(reticulate)\nlibrary(purrr)\n\n# scale observations\nobs <- obs %>% mutate(\n  x = scale(x)\n)\n\n# generate timesteps\nn <- nrow(obs)\nn_timesteps <- 10\n\ngen_timesteps <- function(x, n_timesteps) {\n  do.call(rbind,\n          purrr::map(seq_along(x),\n             function(i) {\n               start <- i\n               end <- i + n_timesteps - 1\n               out <- x[start:end]\n               out\n             })\n  ) %>%\n    na.omit()\n}\n\n# train with start of time series, test with end of time series \nx_train <- gen_timesteps(as.matrix(obs$x)[1:(n/2)], n_timesteps)\nx_test <- gen_timesteps(as.matrix(obs$x)[(n/2):n], n_timesteps) \n\n# add required dimension for features (we have one)\ndim(x_train) <- c(dim(x_train), 1)\ndim(x_test) <- c(dim(x_test), 1)\n\n# some batch size (value not crucial)\nbatch_size <- 100\n\n# transform to datasets so we can use custom training\nds_train <- tensor_slices_dataset(x_train) %>%\n  dataset_batch(batch_size)\n\nds_test <- tensor_slices_dataset(x_test) %>%\n  dataset_batch(nrow(x_test))\n\nAutoencoder\nWith newer versions of TensorFlow (>= 2.0, certainly if >= 2.2), autoencoder-like models are best coded as custom models, and trained in an “autographed” loop.6\nThe encoder is centered around a single LSTM layer, whose size determines the maximum dimensionality of the attractor. The decoder then undoes the compression – again, mainly using a single LSTM.\n\n\n# size of the latent code\nn_latent <- 10L\nn_features <- 1\n\nencoder_model <- function(n_timesteps,\n                          n_features,\n                          n_latent,\n                          name = NULL) {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$noise <- layer_gaussian_noise(stddev = 0.5)\n    self$lstm <-  layer_lstm(\n      units = n_latent,\n      input_shape = c(n_timesteps, n_features),\n      return_sequences = FALSE\n    ) \n    self$batchnorm <- layer_batch_normalization()\n    \n    function (x, mask = NULL) {\n      x %>%\n        self$noise() %>%\n        self$lstm() %>%\n        self$batchnorm() \n    }\n  })\n}\n\ndecoder_model <- function(n_timesteps,\n                          n_features,\n                          n_latent,\n                          name = NULL) {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$repeat_vector <- layer_repeat_vector(n = n_timesteps)\n    self$noise <- layer_gaussian_noise(stddev = 0.5)\n    self$lstm <- layer_lstm(\n        units = n_latent,\n        return_sequences = TRUE,\n        go_backwards = TRUE\n      ) \n    self$batchnorm <- layer_batch_normalization()\n    self$elu <- layer_activation_elu() \n    self$time_distributed <- time_distributed(layer = layer_dense(units = n_features))\n    \n    function (x, mask = NULL) {\n      x %>%\n        self$repeat_vector() %>%\n        self$noise() %>%\n        self$lstm() %>%\n        self$batchnorm() %>%\n        self$elu() %>%\n        self$time_distributed()\n    }\n  })\n}\n\n\nencoder <- encoder_model(n_timesteps, n_features, n_latent)\ndecoder <- decoder_model(n_timesteps, n_features, n_latent)\n\nLoss\nAs already explained above, the loss function we train with is twofold. On the one hand, we compare the original inputs with the decoder outputs (the reconstruction), using mean squared error:\n\n\nmse_loss <- tf$keras$losses$MeanSquaredError(\n  reduction = tf$keras$losses$Reduction$SUM)\n\nIn addition, we try to keep the number of false neighbors small, by means of the following regularizer.7\n\n\nloss_false_nn <- function(x) {\n \n  # original values used in Kennel et al. (1992)\n  rtol <- 10 \n  atol <- 2\n  k_frac <- 0.01\n  \n  k <- max(1, floor(k_frac * batch_size))\n  \n  tri_mask <-\n    tf$linalg$band_part(\n      tf$ones(\n        shape = c(n_latent, n_latent),\n        dtype = tf$float32\n      ),\n      num_lower = -1L,\n      num_upper = 0L\n    )\n  \n   batch_masked <- tf$multiply(\n     tri_mask[, tf$newaxis,], x[tf$newaxis, reticulate::py_ellipsis()]\n   )\n  \n  x_squared <- tf$reduce_sum(\n    batch_masked * batch_masked,\n    axis = 2L,\n    keepdims = TRUE\n  )\n\n  pdist_vector <- x_squared +\n  tf$transpose(\n    x_squared, perm = c(0L, 2L, 1L)\n  ) -\n  2 * tf$matmul(\n    batch_masked,\n    tf$transpose(batch_masked, perm = c(0L, 2L, 1L))\n  )\n\n  all_dists <- pdist_vector\n  all_ra <-\n    tf$sqrt((1 / (\n      batch_size * tf$range(1, 1 + n_latent, dtype = tf$float32)\n    )) *\n      tf$reduce_sum(tf$square(\n        batch_masked - tf$reduce_mean(batch_masked, axis = 1L, keepdims = TRUE)\n      ), axis = c(1L, 2L)))\n  \n  all_dists <- tf$clip_by_value(all_dists, 1e-14, tf$reduce_max(all_dists))\n\n  top_k <- tf$math$top_k(-all_dists, tf$cast(k + 1, tf$int32))\n  top_indices <- top_k[[1]]\n\n  neighbor_dists_d <- tf$gather(all_dists, top_indices, batch_dims = -1L)\n  \n  neighbor_new_dists <- tf$gather(\n    all_dists[2:-1, , ],\n    top_indices[1:-2, , ],\n    batch_dims = -1L\n  )\n  \n  # Eq. 4 of Kennel et al. (1992)\n  scaled_dist <- tf$sqrt((\n    tf$square(neighbor_new_dists) -\n      tf$square(neighbor_dists_d[1:-2, , ])) /\n      tf$square(neighbor_dists_d[1:-2, , ])\n  )\n  \n  # Kennel condition #1\n  is_false_change <- (scaled_dist > rtol)\n  # Kennel condition #2\n  is_large_jump <-\n    (neighbor_new_dists > atol * all_ra[1:-2, tf$newaxis, tf$newaxis])\n  \n  is_false_neighbor <-\n    tf$math$logical_or(is_false_change, is_large_jump)\n  \n  total_false_neighbors <-\n    tf$cast(is_false_neighbor, tf$int32)[reticulate::py_ellipsis(), 2:(k + 2)]\n  \n  reg_weights <- 1 -\n    tf$reduce_mean(tf$cast(total_false_neighbors, tf$float32), axis = c(1L, 2L))\n  reg_weights <- tf$pad(reg_weights, list(list(1L, 0L)))\n  \n  activations_batch_averaged <-\n    tf$sqrt(tf$reduce_mean(tf$square(x), axis = 0L))\n  \n  loss <- tf$reduce_sum(tf$multiply(reg_weights, activations_batch_averaged))\n  loss\n  \n}\n\nMSE and FNN are added , with FNN loss weighted according to the essential hyperparameter of this model:\n\n\nfnn_weight <- 10\n\nThis value was experimentally chosen as the one best conforming to our look-for-the-highest-drop heuristic.\nModel training\nThe training loop closely follows the aforementioned recipe on how to train with custom models and tfautograph.\n\n\ntrain_loss <- tf$keras$metrics$Mean(name='train_loss')\ntrain_fnn <- tf$keras$metrics$Mean(name='train_fnn')\ntrain_mse <-  tf$keras$metrics$Mean(name='train_mse')\n\ntrain_step <- function(batch) {\n  \n  with (tf$GradientTape(persistent = TRUE) %as% tape, {\n    \n    code <- encoder(batch)\n    reconstructed <- decoder(code)\n    \n    l_mse <- mse_loss(batch, reconstructed)\n    l_fnn <- loss_false_nn(code)\n    loss <- l_mse + fnn_weight * l_fnn\n    \n  })\n  \n  encoder_gradients <- tape$gradient(loss, encoder$trainable_variables)\n  decoder_gradients <- tape$gradient(loss, decoder$trainable_variables)\n  \n  optimizer$apply_gradients(\n    purrr::transpose(list(encoder_gradients, encoder$trainable_variables))\n  )\n  optimizer$apply_gradients(\n    purrr::transpose(list(decoder_gradients, decoder$trainable_variables))\n  )\n  \n  train_loss(loss)\n  train_mse(l_mse)\n  train_fnn(l_fnn)\n}\n\ntraining_loop <- tf_function(autograph(function(ds_train) {\n  \n  for (batch in ds_train) {\n    train_step(batch)\n  }\n  \n  tf$print(\"Loss: \", train_loss$result())\n  tf$print(\"MSE: \", train_mse$result())\n  tf$print(\"FNN loss: \", train_fnn$result())\n  \n  train_loss$reset_states()\n  train_mse$reset_states()\n  train_fnn$reset_states()\n  \n}))\n\noptimizer <- optimizer_adam(lr = 1e-3)\n\nfor (epoch in 1:200) {\n  cat(\"Epoch: \", epoch, \" -----------\\n\")\n  training_loop(ds_train)  \n}\n\nAfter two hundred epochs, overall loss is at 2.67, with the MSE component at 1.8 and FNN at 0.09.\nObtaining the attractor from the test set\nWe use the test set to inspect the latent code:\n\n\ntest_batch <- as_iterator(ds_test) %>% iter_next()\npredicted <- encoder(test_batch) %>%\n  as.array(predicted) %>%\n  as_tibble()\n\npredicted\n\n\n# A tibble: 6,242 x 10\n      V1    V2         V3        V4        V5         V6        V7        V8       V9       V10\n   <dbl> <dbl>      <dbl>     <dbl>     <dbl>      <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n 1 0.439 0.401 -0.000614  -0.0258   -0.00176  -0.0000276  0.000276  0.00677  -0.0239   0.00906 \n 2 0.415 0.504  0.0000481 -0.0279   -0.00435  -0.0000970  0.000921  0.00509  -0.0214   0.00921 \n 3 0.389 0.619  0.000848  -0.0240   -0.00661  -0.000171   0.00106   0.00454  -0.0150   0.00794 \n 4 0.363 0.729  0.00137   -0.0143   -0.00652  -0.000244   0.000523  0.00450  -0.00594  0.00476 \n 5 0.335 0.809  0.00128   -0.000450 -0.00338  -0.000307  -0.000561  0.00407   0.00394 -0.000127\n 6 0.304 0.828  0.000631   0.0126    0.000889 -0.000351  -0.00167   0.00250   0.0115  -0.00487 \n 7 0.274 0.769 -0.000202   0.0195    0.00403  -0.000367  -0.00220  -0.000308  0.0145  -0.00726 \n 8 0.246 0.657 -0.000865   0.0196    0.00558  -0.000359  -0.00208  -0.00376   0.0134  -0.00709 \n 9 0.224 0.535 -0.00121    0.0162    0.00608  -0.000335  -0.00169  -0.00697   0.0106  -0.00576 \n10 0.211 0.434 -0.00129    0.0129    0.00606  -0.000306  -0.00134  -0.00927   0.00820 -0.00447 \n# … with 6,232 more rows\nAs a result of the FNN regularizer, the latent code units should be ordered roughly by decreasing variance, with a sharp drop appearing some place (if the FNN weight has been chosen adequately).\nFor an fnn_weight of 10, we do see a drop after the first two units:\n\n\npredicted %>% summarise_all(var)\n\n\n# A tibble: 1 x 10\n      V1     V2      V3      V4      V5      V6      V7      V8      V9     V10\n   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 0.0739 0.0582 1.12e-6 3.13e-4 1.43e-5 1.52e-8 1.35e-6 1.86e-4 1.67e-4 4.39e-5\nSo the model indicates that the Lorenz attractor can be represented in two dimensions. If we nonetheless want to plot the complete (reconstructed) state space of three dimensions, we should reorder the remaining variables by magnitude of variance8. Here, this results in three projections of the set V1, V2 and V4:\n\n\n\nFigure 10: Attractors as predicted from the latent code (test set). The three highest-variance variables were chosen.\n\n\n\nWrapping up (for this time)\nAt this point, we’ve seen how to reconstruct the Lorenz attractor from data we did not train on (the test set), using an autoencoder regularized by a custom false nearest neighbors loss. It is important to stress that at no point was the network presented with the expected solution (attractor) – training was purely unsupervised.\nThis is a fascinating result. Of course, thinking practically, the next step is to obtain predictions on heldout data. Given how long this text has become already, we reserve that for a follow-up post. And again of course, we’re thinking about other datasets, especially ones where the true state space is not known beforehand. What about measurement noise? What about datasets that are not completely deterministic9? There is a lot to explore, stay tuned – and as always, thanks for reading!\n\n\nGilpin, William. 2020. “Deep Reconstruction of Strange Attractors from Time Series.” http://arxiv.org/abs/2002.05909.\n\n\nKantz, Holger, and Thomas Schreiber. 2004. Nonlinear Time Series Analysis. Cambridge University Press.\n\n\nKennel, Matthew B., Reggie Brown, and Henry D. I. Abarbanel. 1992. “Determining Embedding Dimension for Phase-Space Reconstruction Using a Geometrical Construction.” Phys. Rev. A 45 (6): 3403–11. https://doi.org/10.1103/PhysRevA.45.3403.\n\n\nSauer, Tim, James A. Yorke, and Martin Casdagli. 1991. “Embedology.” Journal of Statistical Physics 65 (3-4): 579–616. https://doi.org/10.1007/BF01053745.\n\n\nStrang, Gilbert. 2019. Linear Algebra and Learning from Data. Wellesley Cambridge Press.\n\n\nStrogatz, Steven. 2015. Nonlinear Dynamics and Chaos: With Applications to Physics, Biology, Chemistry, and Engineering. Westview Press.\n\n\nFor many popular activation functions at least (such as ReLU). See e.g. (Strang 2019).↩︎\nThe paper is also accompanied by a Python implementation.↩︎\nTo people who want to learn more about this topic, the usual recommendation is (Strogatz 2015). Personally I prefer another source, which I can’t recommend highly enough: Santa Fe Institute’s Nonlinear Dynamics: Mathematical and Computational Approaches, taught by Liz Bradley.↩︎\nSee e.g. Wikipedia for some history and links to sources.↩︎\nIn discrete systems, like the logistic map, a single dimension is enough.↩︎\nSee the custom training tutorial for a blueprint.↩︎\nSee the appendix of (Gilpin 2020) for a pseudocode-like documentation.↩︎\nAs per author recommendation (personal communication).↩︎\nSee (Kantz and Schreiber 2004) for detailed discussions on using methodology from nonlinear deterministic systems analysis for noisy and/or partly-stochastic data.↩︎\n",
    "preview": "posts/2020-06-24-deep-attractors/images/x_z.gif",
    "last_modified": "2020-09-30T20:24:31+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-05-29-pixelcnn/",
    "title": "Easy PixelCNN with tfprobability",
    "description": "PixelCNN is a deep learning architecture - or bundle of architectures - designed to generate highly realistic-looking images. To use it, no reverse-engineering of arXiv papers or search for reference implementations is required: TensorFlow Probability and its R wrapper, tfprobability, now include a PixelCNN distribution that can be used to train a straightforwardly-defined neural network in a parameterizable way.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-05-29",
    "categories": [
      "R",
      "Image Recognition & Image Processing",
      "TensorFlow/Keras",
      "Probabilistic ML/DL",
      "Unsupervised Learning"
    ],
    "contents": "\nWe’ve seen quite a few examples of unsupervised learning (or self-supervised learning, to choose the more correct but less popular term) on this blog.\nOften, these involved Variational Autoencoders (VAEs), whose appeal lies in them allowing to model a latent space of underlying, independent (preferably) factors that determine the visible features. A possible downside can be the inferior quality of generated samples. Generative Adversarial Networks (GANs) are another popular approach. Conceptually, these are highly attractive due to their game-theoretic framing. However, they can be difficult to train. PixelCNN variants, on the other hand – we’ll subsume them all here under PixelCNN – are generally known for their good results. They seem to involve some more alchemy1 though. Under those circumstances, what could be more welcome than an easy way of experimenting with them? Through TensorFlow Probability (TFP) and its R wrapper, tfprobability, we now have such a way.\nThis post first gives an introduction to PixelCNN, concentrating on high-level concepts (leaving the details for the curious to look them up in the respective papers). We’ll then show an example of using tfprobability to experiment with the TFP implementation.\nPixelCNN principles\nAutoregressivity, or: We need (some) order\nThe basic idea in PixelCNN is autoregressivity. Each pixel is modeled as depending on all prior pixels. Formally:\n\\[p(\\mathbf{x}) = \\prod_{i}p(x_i|x_0, x_1, ..., x_{i-1})\\]\nNow wait a second - what even are prior pixels? Last I saw one images were two-dimensional. So this means we have to impose an order on the pixels. Commonly this will be raster scan order: row after row, from left to right. But when dealing with color images, there’s something else: At each position, we actually have three intensity values, one for each of red, green, and blue. The original PixelCNN paper(Oord, Kalchbrenner, and Kavukcuoglu 2016) carried through autoregressivity here as well, with a pixel’s intensity for red depending on just prior pixels, those for green depending on these same prior pixels but additionally, the current value for red, and those for blue depending on the prior pixels as well as the current values for red and green.\n\\[p(x_i|\\mathbf{x}<i) = p(x_{i,R}|\\mathbf{x}<i)\\ p(x_{i,G}|\\mathbf{x}<i, x_{i,R})\\ p(x_{i,B}|\\mathbf{x}<i, x_{i,R}, x_{i,G})\\]\nHere, the variant implemented in TFP, PixelCNN++(Salimans et al. 2017) , introduces a simplification; it factorizes the joint distribution in a less compute-intensive way.2\nTechnically, then, we know how autoregressivity is realized; intuitively, it may still seem surprising that imposing a raster scan order “just works” (to me, at least, it is). Maybe this is one of those points where compute power successfully compensates for lack of an equivalent of a cognitive prior.\nMasking, or: Where not to look\nNow, PixelCNN ends in “CNN” for a reason – as usual in image processing, convolutional layers (or blocks thereof) are involved. But – is it not the very nature of a convolution that it computes an average of some sorts, looking, for each output pixel, not just at the corresponding input but also, at its spatial (or temporal) surroundings? How does that rhyme with the look-at-just-prior-pixels strategy?\nSurprisingly, this problem is easier to solve than it sounds. When applying the convolutional kernel, just multiply with a mask that zeroes out any “forbidden pixels” – like in this example for a 5x5 kernel, where we’re about to compute the convolved value for row 3, column 3:\n\\[\\left[\\begin{array}\n{rrr}\n1 & 1 & 1 & 1 & 1\\\\\n1 & 1 & 1 & 1 & 1\\\\\n1 & 1 & 1 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0\\\\\n\\end{array}\\right]\n\\]\nThis makes the algorithm honest, but introduces a different problem: With each successive convolutional layer consuming its predecessor’s output, there is a continuously growing blind spot (so-called in analogy to the blind spot on the retina, but located in the top right) of pixels that are never seen by the algorithm. Van den Oord et al. (2016)(Oord et al. 2016) fix this by using two different convolutional stacks, one proceeding from top to bottom, the other from left to right3.\nFig. 1: Left: Blind spot, growing over layers. Right: Using two different stacks (a vertical and a horizontal one) solves the problem. Source: van den Oord et al., 2016.Conditioning, or: Show me a kitten\nSo far, we’ve always talked about “generating images” in a purely generic way. But the real attraction lies in creating samples of some specified type – one of the classes we’ve been training on, or orthogonal information fed into the network. This is where PixelCNN becomes Conditional PixelCNN(Oord et al. 2016), and it is also where that feeling of magic resurfaces. Again, as “general math” it’s not hard to conceive. Here, \\(\\mathbf{h}\\) is the additional input we’re conditioning on:\n\\[p(\\mathbf{x}| \\mathbf{h}) = \\prod_{i}p(x_i|x_0, x_1, ..., x_{i-1}, \\mathbf{h})\\]\nBut how does this translate into neural network operations? It’s just another matrix multiplication (\\(V^T \\mathbf{h}\\)) added to the convolutional outputs (\\(W \\mathbf{x}\\)).\n\\[\\mathbf{y} = tanh(W_{k,f} \\mathbf{x} + V^T_{k,f} \\mathbf{h}) \\odot \\sigma(W_{k,g} \\mathbf{x} + V^T_{k,g} \\mathbf{h})\\]\n(If you’re wondering about the second part on the right, after the Hadamard product sign – we won’t go into details, but in a nutshell, it’s another modification introduced by (Oord et al. 2016), a transfer of the “gating” principle from recurrent neural networks, such as GRUs and LSTMs, to the convolutional setting.)\nSo we see what goes into the decision of a pixel value to sample. But how is that decision actually made?\nLogistic mixture likelihood , or: No pixel is an island\nAgain, this is where the TFP implementation does not follow the original paper, but the latter PixelCNN++ one. Originally, pixels were modeled as discrete values, decided on by a softmax over 256 (0-255) possible values. (That this actually worked seems like another instance of deep learning magic. Imagine: In this model, 254 is as far from 255 as it is from 0.)\nIn contrast, PixelCNN++ assumes an underlying continuous distribution of color intensity, and rounds to the nearest integer. That underlying distribution is a mixture of logistic distributions, thus allowing for multimodality:\n\\[\\nu \\sim \\sum_{i} \\pi_i \\ logistic(\\mu_i, \\sigma_i)\\]\nOverall architecture and the PixelCNN distribution\nOverall, PixelCNN++, as described in (Salimans et al. 2017), consists of six blocks. The blocks together make up a UNet-like structure, successively downsizing the input and then, upsampling again:\nFig. 2: Overall structure of PixelCNN++. From: Salimans et al., 2017.In TFP’s PixelCNN distribution, the number of blocks is configurable as num_hierarchies, the default being 3.\nEach block consists of a customizable number of layers, called ResNet layers due to the residual connection (visible on the right) complementing the convolutional operations in the horizontal stack:\nFig. 3: One so-called \"ResNet layer\", featuring both a vertical and a horizontal convolutional stack. Source: van den Oord et al., 2017.In TFP, the number of these layers per block is configurable as num_resnet.\nnum_resnet and num_hierarchies are the parameters you’re most likely to experiment with, but there are a few more you can check out in the documentation. The number of logistic distributions in the mixture is also configurable, but from my experiments it’s best to keep that number rather low to avoid producing NaNs during training.\nLet’s now see a complete example.\nEnd-to-end example\nOur playground will be QuickDraw, a dataset – still growing – obtained by asking people to draw some object in at most twenty seconds, using the mouse. (To see for yourself, just check out the website). As of today, there are more than a fifty million instances, from 345 different classes.\nFirst and foremost, these data were chosen to take a break from MNIST and its variants. But just like those (and many more!), QuickDraw can be obtained, in tfdatasets-ready form, via tfds, the R wrapper to TensorFlow datasets. In contrast to the MNIST “family” though, the “real samples” are themselves highly irregular, and often even missing essential parts. So to anchor judgment, when displaying generated samples we always show eight actual drawings with them.\nPreparing the data\nThe dataset being gigantic, we instruct tfds to load the first 500,000 drawings “only”.\n\n\nlibrary(reticulate)\n\n# >= 2.2 required\nlibrary(tensorflow)\nlibrary(keras)\n\n# make sure to use at least version 0.10\nlibrary(tfprobability)\n\nlibrary(tfdatasets)\n# currently to be installed from github\nlibrary(tfds)\n\n# load just the first 500,000 images\n# nonetheless, initially the complete dataset will be downloaded and unpacked\n# ... be prepared for this to take some time\ntrain_ds <- tfds_load(\"quickdraw_bitmap\", split='train[:500000]')\n\nTo speed up training further, we then zoom in on twenty classes. This effectively leaves us with ~ 1,100 - 1,500 drawings per class.\n\n\n# bee, bicycle, broccoli, butterfly, cactus,\n# frog, guitar, lightning, penguin, pizza,\n# rollerskates, sea turtle, sheep, snowflake, sun,\n# swan, The Eiffel Tower, tractor, train, tree\nclasses <- c(26, 29, 43, 49, 50,\n             125, 134, 172, 218, 225,\n             246, 255, 258, 271, 295,\n             296, 308, 320, 322, 323\n)\n\nclasses_tensor <- tf$cast(classes, tf$int64)\n\ntrain_ds <- train_ds %>%\n  dataset_filter(\n    function(record) tf$reduce_any(tf$equal(classes_tensor, record$label), -1L)\n  )\n\nThe PixelCNN distribution expects values in the range from 0 to 255 – no normalization required. Preprocessing then consists of just casting pixels and labels each to float:\n\n\npreprocess <- function(record) {\n  record$image <- tf$cast(record$image, tf$float32) \n  record$label <- tf$cast(record$label, tf$float32)\n  list(tuple(record$image, record$label))\n}\n\nbatch_size <- 32\n\ntrain <- train_ds %>%\n  dataset_map(preprocess) %>%\n  dataset_shuffle(10000) %>%\n  dataset_batch(batch_size)\n\nCreating the model\nWe now use tfd_pixel_cnn to define what will be the loglikelihood used by the model.\n\n\ndist <- tfd_pixel_cnn(\n  image_shape = c(28, 28, 1),\n  conditional_shape = list(),\n  num_resnet = 5,\n  num_hierarchies = 3,\n  num_filters = 128,\n  num_logistic_mix = 5,\n  dropout_p =.5\n)\n\nimage_input <- layer_input(shape = c(28, 28, 1))\nlabel_input <- layer_input(shape = list())\nlog_prob <- dist %>% tfd_log_prob(image_input, conditional_input = label_input)\n\nThis custom loglikelihood is added as a loss to the model, and then, the model is compiled with just an optimizer specification only. During training, loss first decreased quickly, but improvements from later epochs were smaller.\n\n\nmodel <- keras_model(inputs = list(image_input, label_input), outputs = log_prob)\nmodel$add_loss(-tf$reduce_mean(log_prob))\nmodel$compile(optimizer = optimizer_adam(lr = .001))\n\nmodel %>% fit(train, epochs = 10)\n\nTo jointly display real and fake images:\n\n\nfor (i in classes) {\n  \n  real_images <- train_ds %>%\n    dataset_filter(\n      function(record) record$label == tf$cast(i, tf$int64)\n    ) %>% \n    dataset_take(8) %>%\n    dataset_batch(8)\n  it <- as_iterator(real_images)\n  real_images <- iter_next(it)\n  real_images <- real_images$image %>% as.array()\n  real_images <- real_images[ , , , 1]/255\n  \n  generated_images <- dist %>% tfd_sample(8, conditional_input = i)\n  generated_images <- generated_images %>% as.array()\n  generated_images <- generated_images[ , , , 1]/255\n  \n  images <- abind::abind(real_images, generated_images, along = 1)\n  png(paste0(\"draw_\", i, \".png\"), width = 8 * 28 * 10, height = 2 * 28 * 10)\n  par(mfrow = c(2, 8), mar = c(0, 0, 0, 0))\n  images %>%\n    purrr::array_tree(1) %>%\n    purrr::map(as.raster) %>%\n    purrr::iwalk(plot)\n  dev.off()\n}\n\nFrom our twenty classes, here’s a choice of six, each showing real drawings in the top row, and fake ones below.\nFig. 4: Bicycles, drawn by people (top row) and the network (bottom row).Fig. 5: Broccoli, drawn by people (top row) and the network (bottom row).Fig. 6: Butterflies, drawn by people (top row) and the network (bottom row).Fig. 7: Guitars, drawn by people (top row) and the network (bottom row).Fig. 8: Penguins, drawn by people (top row) and the network (bottom row).Fig. 9: Roller skates, drawn by people (top row) and the network (bottom row).We probably wouldn’t confuse the first and second rows, but then, the actual human drawings exhibit enormous variation, too. And no one ever said PixelCNN was an architecture for concept learning. Feel free to play around with other datasets of your choice – TFP’s PixelCNN distribution makes it easy.\nWrapping up\nIn this post, we had tfprobability / TFP do all the heavy lifting for us, and so, could focus on the underlying concepts. Depending on your inclinations, this can be an ideal situation – you don’t lose sight of the forest for the trees. On the other hand: Should you find that changing the provided parameters doesn’t achieve what you want, you have a reference implementation to start from. So whatever the outcome, the addition of such higher-level functionality to TFP is a win for the users. (If you’re a TFP developer reading this: Yes, we’d like more :-)).\nTo everyone though, thanks for reading!\n\n\nOord, Aaron van den, Nal Kalchbrenner, and Koray Kavukcuoglu. 2016. “Pixel Recurrent Neural Networks.” CoRR abs/1601.06759. http://arxiv.org/abs/1601.06759.\n\n\nOord, Aaron van den, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. 2016. “Conditional Image Generation with Pixelcnn Decoders.” CoRR abs/1606.05328. http://arxiv.org/abs/1606.05328.\n\n\nSalimans, Tim, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. 2017. “PixelCNN++: A Pixelcnn Implementation with Discretized Logistic Mixture Likelihood and Other Modifications.” In ICLR.\n\n\nAlluding to Ali Rahimi’s (in)famous “deep learning is alchemy” talk at NeurIPS 2017. I would suspect that to some degree, that statement resonates with many DL practitioners – although one need not agree that more mathematical rigor is the solution.↩︎\nFor details, see (Salimans et al. 2017).↩︎\n.For details, see (Oord et al. 2016).↩︎\n",
    "preview": "posts/2020-05-29-pixelcnn/images/thumb.png",
    "last_modified": "2020-09-30T20:24:31+02:00",
    "input_file": {},
    "preview_width": 400,
    "preview_height": 203
  },
  {
    "path": "posts/2020-05-15-model-inversion-attacks/",
    "title": "Hacking deep learning: model inversion attack by example",
    "description": "Compared to other applications, deep learning models might not seem too likely as victims of privacy attacks. However, methods exist to determine whether an entity was used in the training set (an adversarial attack called member inference), and techniques subsumed under \"model inversion\" allow to reconstruct raw data input given just model output (and sometimes, context information). This post shows an end-to-end example of model inversion, and explores mitigation strategies using TensorFlow Privacy.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-05-15",
    "categories": [
      "R",
      "Privacy & Security",
      "TensorFlow/Keras"
    ],
    "contents": "\nHow private are individual data in the context of machine learning models? The data used to train the model, say. There are types of models where the answer is simple. Take k-nearest-neighbors, for example. There is not even a model without the complete dataset. Or support vector machines. There is no model without the support vectors. But neural networks? They’re just some composition of functions, – no data included.\nThe same is true for data fed to a deployed deep-learning model. It’s pretty unlikely one could invert the final softmax output from a big ResNet and get back the raw input data.\nIn theory, then, “hacking” a standard neural net to spy on input data sounds illusory. In practice, however, there is always some real-world context. The context may be other datasets, publicly available, that can be linked to the “private” data in question. This is a popular showcase used in advocating for differential privacy(Dwork et al. 2006): Take an “anonymized” dataset, dig up complementary information from public sources, and de-anonymize records ad libitum. Some context in that sense will often be used in “black-box” attacks, ones that presuppose no insider information about the model to be hacked.\nBut context can also be structural, such as in the scenario demonstrated in this post. For example, assume a distributed model, where sets of layers run on different devices – embedded devices or mobile phones, for example. (A scenario like that is sometimes seen as “white-box”(Wu et al. 2016), but in common understanding, white-box attacks probably presuppose some more insider knowledge, such as access to model architecture or even, weights. I’d therefore prefer calling this white-ish at most.) — Now assume that in this context, it is possible to intercept, and interact with, a system that executes the deeper layers of the model. Based on that system’s intermediate-level output, it is possible to perform model inversion(Fredrikson et al. 2014), that is, to reconstruct the input data fed into the system.\nIn this post, we’ll demonstrate such a model inversion attack, basically porting the approach given in a notebook found in the PySyft repository. We then experiment with different levels of \\(\\epsilon\\)-privacy, exploring impact on reconstruction success. This second part will make use of TensorFlow Privacy, introduced in a previous blog post.\nPart 1: Model inversion in action\nExample dataset: All the world’s letters1\nThe overall process of model inversion used here is the following. With no, or scarcely any, insider knowledge about a model, – but given opportunities to repeatedly query it –, I want to learn how to reconstruct unknown inputs based on just model outputs . Independently of original model training, this, too, is a training process; however, in general it will not involve the original data, as those won’t be publicly available. Still, for best success, the attacker model is trained with data as similar as possible to the original training data assumed. Thinking of images, for example, and presupposing the popular view of successive layers representing successively coarse-grained features, we want that the surrogate data to share as many representation spaces with the real data as possible – up to the very highest layers before final classification, ideally.\nIf we wanted to use classical MNIST as an example, one thing we could do is to only use some of the digits for training the “real” model; and the rest, for training the adversary. Let’s try something different though, something that might make the undertaking harder as well as easier at the same time. Harder, because the dataset features exemplars more complex than MNIST digits; easier because of the same reason: More could possibly be learned, by the adversary, from a complex task.\nOriginally designed to develop a machine model of concept learning and generalization (Lake, Salakhutdinov, and Tenenbaum 2015), the OmniGlot dataset incorporates characters from fifty alphabets, split into two disjoint groups of thirty and twenty alphabets each. We’ll use the group of twenty to train our target model. Here is a sample:\n\n\n\nFigure 1: Sample from the twenty-alphabet set used to train the target model (originally: ‘evaluation set’)\n\n\n\nThe group of thirty we don’t use; instead, we’ll employ two small five-alphabet collections to train the adversary and to test reconstruction, respectively. (These small subsets of the original “big” thirty-alphabet set are again disjoint.)\nHere first is a sample from the set used to train the adversary.\n\n\n\nFigure 2: Sample from the five-alphabet set used to train the adversary (originally: ‘background small 1’)\n\n\n\nThe other small subset will be used to test the adversary’s spying capabilities after training. Let’s peek at this one, too:\n\n\n\nFigure 3: Sample from the five-alphabet set used to test the adversary after training(originally: ‘background small 2’)\n\n\n\nConveniently, we can use tfds, the R wrapper to TensorFlow Datasets, to load those subsets:\n\n\nlibrary(reticulate)\nlibrary(keras)\nlibrary(tfdatasets)\nlibrary(tfautograph)\nlibrary(tfds)\n\nlibrary(purrr)\n\n# we'll use this to train the target model\n# n = 13180\nomni_train <- tfds$load(\"omniglot\", split = \"test\")\n\n# this is used to train the adversary\n# n = 2720\nomni_spy <- tfds$load(\"omniglot\", split = \"small1\")\n\n# this we'll use for testing\n# n = 3120\nomni_test <- tfds$load(\"omniglot\", split = \"small2\")\n\nNow first, we train the target model.\nTrain target model\nThe dataset originally has four columns: the image, of size 105 x 105; an alphabet id and a within-dataset character id; and a label. For our use case, we’re not really interested in the task the target model was/is used for; we just want to get at the data. Basically, whatever task we choose, it is not much more than a dummy task. So, let’s just say we train the target to classify characters by alphabet.\nWe thus throw out all unneeded features, keeping just the alphabet id and the image itself:\n\n\n# normalize and work with a single channel (images are black-and-white anyway)\npreprocess_image <- function(image) {\n  image %>%\n    tf$cast(dtype = tf$float32) %>%\n    tf$truediv(y = 255) %>%\n    tf$image$rgb_to_grayscale()\n}\n\n# use the first 11000 images for training\ntrain_ds <- omni_train %>% \n  dataset_take(11000) %>%\n  dataset_map(function(record) {\n    record$image <- preprocess_image(record$image)\n    list(record$image, record$alphabet)}) %>%\n  dataset_shuffle(1000) %>% \n  dataset_batch(32)\n\n# use the remaining 2180 records for validation\nval_ds <- omni_train %>% \n  dataset_skip(11000) %>%\n  dataset_map(function(record) {\n    record$image <- preprocess_image(record$image)\n    list(record$image, record$alphabet)}) %>%\n  dataset_batch(32)\n\nThe model consists of two parts. The first is imagined to run in a distributed fashion; for example, on mobile devices (stage one). These devices then send model outputs to a central server, where final results are computed (stage two). Sure, you will be thinking, this is a convenient setup for our scenario: If we intercept stage one results, we – most probably – gain access to richer information than what is contained in a model’s final output layer. — That is correct, but the scenario is less contrived than one might assume. Just like federated learning (McMahan et al. 2016), it fulfills important desiderata: Actual training data never leaves the devices, thus staying (in theory!) private; at the same time, ingoing traffic to the server is significantly reduced.\nIn our example setup, the on-device model is a convnet, while the server model is a simple feedforward network.\nWe link both together as a TargetModel that when called normally, will run both steps in succession. However, we’ll be able to call target_model$mobile_step() separately, thereby intercepting intermediate results.\n\n\non_device_model <- keras_model_sequential() %>%\n  layer_conv_2d(filters = 32, kernel_size = c(7, 7),\n                input_shape = c(105, 105, 1), activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_max_pooling_2d(pool_size = c(3, 3), strides = 3) %>%\n  layer_dropout(0.2) %>%\n  layer_conv_2d(filters = 32, kernel_size = c(7, 7), activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_max_pooling_2d(pool_size = c(3, 3), strides = 2) %>%\n  layer_dropout(0.2) %>%\n  layer_conv_2d(filters = 32, kernel_size = c(5, 5), activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_max_pooling_2d(pool_size = c(2, 2), strides = 2) %>%\n  layer_dropout(0.2) %>%\n  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_max_pooling_2d(pool_size = c(2, 2), strides = 2) %>%\n  layer_dropout(0.2) \n\nserver_model <- keras_model_sequential() %>%\n  layer_dense(units = 256, activation = \"relu\") %>%\n  layer_flatten() %>%\n  layer_dropout(0.2) %>% \n  # we have just 20 different ids, but they are not in lexicographic order\n  layer_dense(units = 50, activation = \"softmax\")\n\ntarget_model <- function() {\n  keras_model_custom(name = \"TargetModel\", function(self) {\n    \n    self$on_device_model <-on_device_model\n    self$server_model <- server_model\n    self$mobile_step <- function(inputs) \n      self$on_device_model(inputs)\n    self$server_step <- function(inputs)\n      self$server_model(inputs)\n\n    function(inputs, mask = NULL) {\n      inputs %>% \n        self$mobile_step() %>%\n        self$server_step()\n    }\n  })\n  \n}\n\nmodel <- target_model()\n\nThe overall model is a Keras custom model, so we train it TensorFlow 2.x - style. After ten epochs, training and validation accuracy are at ~0.84 and ~0.73, respectively – not bad at all for a 20-class discrimination task.\n\n\nloss <- loss_sparse_categorical_crossentropy\noptimizer <- optimizer_adam()\n\ntrain_loss <- tf$keras$metrics$Mean(name='train_loss')\ntrain_accuracy <-  tf$keras$metrics$SparseCategoricalAccuracy(name='train_accuracy')\n\nval_loss <- tf$keras$metrics$Mean(name='val_loss')\nval_accuracy <-  tf$keras$metrics$SparseCategoricalAccuracy(name='val_accuracy')\n\ntrain_step <- function(images, labels) {\n  with (tf$GradientTape() %as% tape, {\n    predictions <- model(images)\n    l <- loss(labels, predictions)\n  })\n  gradients <- tape$gradient(l, model$trainable_variables)\n  optimizer$apply_gradients(purrr::transpose(list(\n    gradients, model$trainable_variables\n  )))\n  train_loss(l)\n  train_accuracy(labels, predictions)\n}\n\nval_step <- function(images, labels) {\n  predictions <- model(images)\n  l <- loss(labels, predictions)\n  val_loss(l)\n  val_accuracy(labels, predictions)\n}\n\n\ntraining_loop <- tf_function(autograph(function(train_ds, val_ds) {\n  for (b1 in train_ds) {\n    train_step(b1[[1]], b1[[2]])\n  }\n  for (b2 in val_ds) {\n    val_step(b2[[1]], b2[[2]])\n  }\n  \n  tf$print(\"Train accuracy\", train_accuracy$result(),\n           \"    Validation Accuracy\", val_accuracy$result())\n  \n  train_loss$reset_states()\n  train_accuracy$reset_states()\n  val_loss$reset_states()\n  val_accuracy$reset_states()\n}))\n\n\nfor (epoch in 1:10) {\n  cat(\"Epoch: \", epoch, \" -----------\\n\")\n  training_loop(train_ds, val_ds)  \n}\n\n\nEpoch:  1  -----------\nTrain accuracy 0.195090905     Validation Accuracy 0.376605511\nEpoch:  2  -----------\nTrain accuracy 0.472272724     Validation Accuracy 0.5243119\n...\n...\nEpoch:  9  -----------\nTrain accuracy 0.821454525     Validation Accuracy 0.720183492\nEpoch:  10  -----------\nTrain accuracy 0.840454519     Validation Accuracy 0.726605475\nNow, we train the adversary.\nTrain adversary\nThe adversary’s general strategy will be:\nFeed its small, surrogate dataset to the on-device model. The output received can be regarded as a (highly) compressed version of the original images.\nPass that “compressed” version as input to its own model, which tries to reconstruct the original images from the sparse code.\nCompare original images (those from the surrogate dataset) to the reconstruction pixel-wise. The goal is to minimize the mean (squared, say) error.\nDoesn’t this sound a lot like the decoding side of an autoencoder? No wonder the attacker model is a deconvolutional network. Its input – equivalently, the on-device model’s output – is of size batch_size x 1 x 1 x 32. That is, the information is encoded in 32 channels, but the spatial resolution is 1. Just like in an autoencoder operating on images, we need to upsample until we arrive at the original resolution of 105 x 105.\nThis is exactly what’s happening in the attacker model:\n\n\nattack_model <- function() {\n  \n  keras_model_custom(name = \"AttackModel\", function(self) {\n    \n    self$conv1 <-layer_conv_2d_transpose(filters = 32, kernel_size = 9,\n                                         padding = \"valid\",\n                                         strides = 1, activation = \"relu\")\n    self$conv2 <- layer_conv_2d_transpose(filters = 32, kernel_size = 7,\n                                          padding = \"valid\",\n                                          strides = 2, activation = \"relu\") \n    self$conv3 <- layer_conv_2d_transpose(filters = 1, kernel_size = 7,\n                                          padding = \"valid\",\n                                          strides = 2, activation = \"relu\")  \n    self$conv4 <- layer_conv_2d_transpose(filters = 1, kernel_size = 5,\n                                          padding = \"valid\",\n                                          strides = 2, activation = \"relu\")\n    \n    function(inputs, mask = NULL) {\n      inputs %>% \n        # bs * 9 * 9 * 32\n        # output = strides * (input - 1) + kernel_size - 2 * padding\n        self$conv1() %>%\n        # bs * 23 * 23 * 32\n        self$conv2() %>%\n        # bs * 51 * 51 * 1\n        self$conv3() %>%\n        # bs * 105 * 105 * 1\n        self$conv4()\n    }\n  })\n  \n}\n\nattacker = attack_model()\n\nTo train the adversary, we use one of the small (five-alphabet) subsets. To reiterate what was said above, there is no overlap with the data used to train the target model.\n\n\nattacker_ds <- omni_spy %>% \ndataset_map(function(record) {\n    record$image <- preprocess_image(record$image)\n    list(record$image, record$alphabet)}) %>%\n  dataset_batch(32)\n\nHere, then, is the attacker training loop, striving to refine the decoding process over a hundred – short – epochs:\n\n\nattacker_criterion <- loss_mean_squared_error\nattacker_optimizer <- optimizer_adam()\nattacker_loss <- tf$keras$metrics$Mean(name='attacker_loss')\nattacker_mse <-  tf$keras$metrics$MeanSquaredError(name='attacker_mse')\n\nattacker_step <- function(images) {\n  \n  attack_input <- model$mobile_step(images)\n  \n  with (tf$GradientTape() %as% tape, {\n    generated <- attacker(attack_input)\n    l <- attacker_criterion(images, generated)\n  })\n  gradients <- tape$gradient(l, attacker$trainable_variables)\n  attacker_optimizer$apply_gradients(purrr::transpose(list(\n    gradients, attacker$trainable_variables\n  )))\n  attacker_loss(l)\n  attacker_mse(images, generated)\n}\n\n\nattacker_training_loop <- tf_function(autograph(function(attacker_ds) {\n  for (b in attacker_ds) {\n    attacker_step(b[[1]])\n  }\n  \n  tf$print(\"mse: \", attacker_mse$result())\n  \n  attacker_loss$reset_states()\n  attacker_mse$reset_states()\n}))\n\nfor (epoch in 1:100) {\n  cat(\"Epoch: \", epoch, \" -----------\\n\")\n  attacker_training_loop(attacker_ds)  \n}\n\n\nEpoch:  1  -----------\n  mse:  0.530902684\nEpoch:  2  -----------\n  mse:  0.201351956\n...\n...\nEpoch:  99  -----------\n  mse:  0.0413453057\nEpoch:  100  -----------\n  mse:  0.0413028933\nThe question now is, – does it work? Has the attacker really learned to infer actual data from (stage one) model output?\nTest adversary\nTo test the adversary, we use the third dataset we downloaded, containing images from five yet-unseen alphabets. For display, we select just the first sixteen records – a completely arbitrary decision, of course.\n\n\ntest_ds <- omni_test %>% \n  dataset_map(function(record) {\n    record$image <- preprocess_image(record$image)\n    list(record$image, record$alphabet)}) %>%\n  dataset_take(16) %>%\n  dataset_batch(16)\n\nbatch <- as_iterator(test_ds) %>% iterator_get_next()\nimages <- batch[[1]]\n\nattack_input <- model$mobile_step(images)\ngenerated <- attacker(attack_input) %>% as.array()\n\ngenerated[generated > 1] <- 1\ngenerated <- generated[ , , , 1]\ngenerated %>%\n  purrr::array_tree(1) %>%\n  purrr::map(as.raster) %>%\n  purrr::iwalk(~{plot(.x)})\n\nJust like during the training process, the adversary queries the target model (stage one), obtains the compressed representation, and attempts to reconstruct the original image. (Of course, in the real world, the setup would be different in that the attacker would not be able to simply inspect the images, as is the case here. There would thus have to be some way to intercept, and make sense of, network traffic.)\n\n\nattack_input <- model$mobile_step(images)\ngenerated <- attacker(attack_input) %>% as.array()\n\ngenerated[generated > 1] <- 1\ngenerated <- generated[ , , , 1]\ngenerated %>%\n  purrr::array_tree(1) %>%\n  purrr::map(as.raster) %>%\n  purrr::iwalk(~{plot(.x)})\n\nTo allow for easier comparison (and increase suspense …!), here again are the actual images, which we displayed already when introducing the dataset:\n\n\n\nFigure 4: First images from the test set, the way they really look.\n\n\n\nAnd here is the reconstruction:\n\n\n\nFigure 5: First images from the test set, as reconstructed by the adversary.\n\n\n\nOf course, it is hard to say how revealing these “guesses” are. There definitely seems to be a connection to character complexity; overall, it seems like the Greek and Roman letters, which are the least complex, are also the ones most easily reconstructed. Still, in the end, how much privacy is lost will very much depend on contextual factors.\nFirst and foremost, do the exemplars in the dataset represent individuals or classes of individuals? If – as in reality – the character X represents a class, it might not be so grave if we were able to reconstruct “some X” here: There are many Xs in the dataset, all pretty similar to each other; we’re unlikely to exactly to have reconstructed one special, individual X. If, however, this was a dataset of individual people, with all Xs being photographs of Alex, then in reconstructing an X we have effectively reconstructed Alex.\nSecond, in less obvious scenarios, evaluating the degree of privacy breach will likely surpass computation of quantitative metrics, and involve the judgment of domain experts.\nSpeaking of quantitative metrics though – our example seems like a perfect use case to experiment with differential privacy. Differential privacy is measured by \\(\\epsilon\\) (lower is better), the main idea being that answers to queries to a system should depend as little as possible on the presence or absence of a single (any single) datapoint.\nSo, we will repeat the above experiment, using TensorFlow Privacy (TFP) to add noise, as well as clip gradients, during optimization of the target model. We’ll try three different conditions, resulting in three different values for \\(\\epsilon\\)s, and for each condition, inspect the images reconstructed by the adversary.\nPart 2: Differential privacy to the rescue\nUnfortunately, the setup for this part of the experiment requires a little workaround. Making use of the flexibility afforded by TensorFlow 2.x, our target model has been a custom model, joining two distinct stages (“mobile” and “server”) that could be called independently.\nTFP, however, does still not work with TensorFlow 2.x, meaning we have to use old-style, non-eager model definitions and training. Luckily, the workaround will be easy.\nFirst, load (and possibly, install) libraries, taking care to disable TensorFlow V2 behavior.\n\n\nlibrary(keras)\nlibrary(tensorflow)\n# still necessary when working with TensorFlow Privacy, as of this writing\ntf$compat$v1$disable_v2_behavior()\n\n# if you don't have it installed:\n# reticulate::py_install(\"tensorflow_privacy\")\ntfp <- import(\"tensorflow_privacy\")\n\nlibrary(tfdatasets)\nlibrary(tfds)\n\nlibrary(purrr)\n\nThe training set is loaded, preprocessed and batched (nearly) as before.\n\n\nomni_train <- tfds$load(\"omniglot\", split = \"test\")\n\nbatch_size <- 32\n\ntrain_ds <- omni_train %>%\n  dataset_take(11000) %>%\n  dataset_map(function(record) {\n    record$image <- preprocess_image(record$image)\n    list(record$image, record$alphabet)}) %>%\n  dataset_shuffle(1000) %>%\n  # need dataset_repeat() when not eager\n  dataset_repeat() %>%\n  dataset_batch(batch_size)\n\nTrain target model – with TensorFlow Privacy\nTo train the target, we put the layers from both stages – “mobile” and “server” – into one sequential model. Note how we remove the dropout. This is because noise will be added during optimization anyway.\n\n\ncomplete_model <- keras_model_sequential() %>%\n  layer_conv_2d(filters = 32, kernel_size = c(7, 7),\n                input_shape = c(105, 105, 1),\n                activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_max_pooling_2d(pool_size = c(3, 3), strides = 3) %>%\n  #layer_dropout(0.2) %>%\n  layer_conv_2d(filters = 32, kernel_size = c(7, 7), activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_max_pooling_2d(pool_size = c(3, 3), strides = 2) %>%\n  #layer_dropout(0.2) %>%\n  layer_conv_2d(filters = 32, kernel_size = c(5, 5), activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_max_pooling_2d(pool_size = c(2, 2), strides = 2) %>%\n  #layer_dropout(0.2) %>%\n  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_max_pooling_2d(pool_size = c(2, 2), strides = 2, name = \"mobile_output\") %>%\n  #layer_dropout(0.2) %>%\n  layer_dense(units = 256, activation = \"relu\") %>%\n  layer_flatten() %>%\n  #layer_dropout(0.2) %>%\n  layer_dense(units = 50, activation = \"softmax\")\n\nUsing TFP mainly means using a TFP optimizer, one that clips gradients according to some defined magnitude and adds noise of defined size. noise_multiplier is the parameter we are going to vary to arrive at different \\(\\epsilon\\)s:\n\n\nl2_norm_clip <- 1\n\n# ratio of the standard deviation to the clipping norm\n# we run training for each of the three values\nnoise_multiplier <- 0.7\nnoise_multiplier <- 0.5\nnoise_multiplier <- 0.3\n\n# same as batch size\nnum_microbatches <- k_cast(batch_size, \"int32\")\nlearning_rate <- 0.005\n\noptimizer <- tfp$DPAdamGaussianOptimizer(\n  l2_norm_clip = l2_norm_clip,\n  noise_multiplier = noise_multiplier,\n  num_microbatches = num_microbatches,\n  learning_rate = learning_rate\n)\n\nIn training the model, the second important change for TFP we need to make is to have loss and gradients computed on the individual level.\n\n\n# need to add noise to every individual contribution\nloss <- tf$keras$losses$SparseCategoricalCrossentropy(reduction =   tf$keras$losses$Reduction$NONE)\n\ncomplete_model %>% compile(loss = loss, optimizer = optimizer, metrics = \"sparse_categorical_accuracy\")\n\nnum_epochs <- 20\n\nn_train <- 13180\n\nhistory <- complete_model %>% fit(\n  train_ds,\n  # need steps_per_epoch when not in eager mode\n  steps_per_epoch = n_train/batch_size,\n  epochs = num_epochs)\n\nTo test three different \\(\\epsilon\\)s, we run this thrice, each time with a different noise_multiplier. Each time we arrive at a different final accuracy.\nHere is a synopsis, where \\(\\epsilon\\) was computed like so:\n\n\ncompute_priv <- tfp$privacy$analysis$compute_dp_sgd_privacy\n\ncompute_priv$compute_dp_sgd_privacy(\n  # number of records in training set\n  n_train,\n  batch_size,\n  # noise_multiplier\n  0.7, # or 0.5, or 0.3\n  # number of epochs\n  20,\n  # delta - should not exceed 1/number of examples in training set\n  1e-5)\n\nnoise multiplier\nepsilon\nfinal acc. (training set)\n0.7\n4.0\n0.37\n0.5\n12.5\n0.45\n0.3\n84.7\n0.56\nNow, as the adversary won’t call the complete model, we need to “cut off” the second-stage layers. This leaves us with a model that executes stage-one logic only. We save its weights, so we can later call it from the adversary:\n\n\nintercepted <- keras_model(\n  complete_model$input,\n  complete_model$get_layer(\"mobile_output\")$output\n)\n\nintercepted %>% save_model_hdf5(\"./intercepted.hdf5\")\n\nTrain adversary (against differentially private target)\nIn training the adversary, we can keep most of the original code – meaning, we’re back to TF-2 style. Even the definition of the target model is the same as before:\n\n\non_device_model <- keras_model_sequential() %>%\n  [...]\n\nserver_model <- keras_model_sequential() %>%\n  [...]\n\ntarget_model <- function() {\n  keras_model_custom(name = \"TargetModel\", function(self) {\n    \n    self$on_device_model <-on_device_model\n    self$server_model <- server_model\n    self$mobile_step <- function(inputs) \n      self$on_device_model(inputs)\n    self$server_step <- function(inputs)\n      self$server_model(inputs)\n    \n    function(inputs, mask = NULL) {\n      inputs %>% \n        self$mobile_step() %>%\n        self$server_step()\n    }\n  })\n}\n\nintercepted <- target_model()\n\nBut now, we load the trained target’s weights into the freshly defined model’s “mobile stage”:\n\n\nintercepted$on_device_model$load_weights(\"intercepted.hdf5\")\n\nAnd now, we’re back to the old training routine. Testing setup is the same as before, as well.\nSo how well does the adversary perform with differential privacy added to the picture?\nTest adversary (against differentially private target)\nHere, ordered by decreasing \\(\\epsilon\\), are the reconstructions. Again, we refrain from judging the results, for the same reasons as before: In real-world applications, whether privacy is preserved “well enough” will depend on the context.\nHere, first, are reconstructions from the run where the least noise was added.\n\n\n\nFigure 6: Reconstruction attempts from a setup where the target model was trained with an epsilon of 84.7.\n\n\n\nOn to the next level of privacy protection:\n\n\n\nFigure 7: Reconstruction attempts from a setup where the target model was trained with an epsilon of 12.5.\n\n\n\nAnd the highest-\\(\\epsilon\\) one:\n\n\n\nFigure 8: Reconstruction attempts from a setup where the target model was trained with an epsilon of 4.0.\n\n\n\nConclusion\nThroughout this post, we’ve refrained from “over-commenting” on results, and focused on the why-and-how instead. This is because in an artificial setup, chosen to facilitate exposition of concepts and methods, there really is no objective frame of reference. What is a good reconstruction? What is a good \\(\\epsilon\\)? What constitutes a data breach? No-one knows.\nIn the real world, there is a context to everything – there are people involved, the people whose data we’re talking about. There are organizations, regulations, laws. There are abstract principles, and there are implementations; different implementations of the same “idea” can differ.\nAs in machine learning overall, research papers on privacy-, ethics- or otherwise society-related topics are full of LaTeX formulae. Amid the math, let’s not forget the people.\nThanks for reading!\n\n\nDwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. “Calibrating Noise to Sensitivity in Private Data Analysis.” In Proceedings of the Third Conference on Theory of Cryptography, 265–84. TCC’06. Berlin, Heidelberg: Springer-Verlag. https://doi.org/10.1007/11681878_14.\n\n\nFredrikson, Matthew, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas Ristenpart. 2014. “Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing.” In Proceedings of the 23rd Usenix Conference on Security Symposium, 17–32. SEC’14. USA: USENIX Association.\n\n\nLake, Brenden M., Ruslan Salakhutdinov, and Joshua B. Tenenbaum. 2015. “Human-Level Concept Learning Through Probabilistic Program Induction.” Science 350 (6266): 1332–8. https://doi.org/10.1126/science.aab3050.\n\n\nMcMahan, H. Brendan, Eider Moore, Daniel Ramage, and Blaise Agüera y Arcas. 2016. “Federated Learning of Deep Networks Using Model Averaging.” CoRR abs/1602.05629. http://arxiv.org/abs/1602.05629.\n\n\nWu, X., M. Fredrikson, S. Jha, and J. F. Naughton. 2016. “A Methodology for Formalizing Model-Inversion Attacks.” In 2016 Ieee 29th Computer Security Foundations Symposium (Csf), 355–70.\n\n\nDon’t take all literally please; it’s just a nice phrase.↩︎\n",
    "preview": "posts/2020-05-15-model-inversion-attacks/images/results.png",
    "last_modified": "2020-09-30T20:24:31+02:00",
    "input_file": {},
    "preview_width": 600,
    "preview_height": 394
  },
  {
    "path": "posts/2020-04-29-encrypted_keras_with_syft/",
    "title": "Towards privacy: Encrypted deep learning with Syft and Keras",
    "description": "Deep learning need not be irreconcilable with privacy protection. Federated learning enables on-device, distributed model training; encryption keeps model and gradient updates private; differential privacy prevents the training data from leaking. As of today, private and secure deep learning is an emerging technology. In this post, we introduce Syft, an open-source framework that integrates with PyTorch as well as TensorFlow. In an example use case, we obtain private predictions from a Keras model.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-04-29",
    "categories": [
      "R",
      "Privacy & Security",
      "TensorFlow/Keras"
    ],
    "contents": "\nThe word privacy, in the context of deep learning (or machine learning, or “AI”), and especially when combined with things like security, sounds like it could be part of a catch phrase: privacy, safety, security – like liberté, fraternité, égalité. In fact, there should probably be a mantra like that. But that’s another topic, and like with the other catch phrase just cited, not everyone interprets these terms in the same way.\nSo let’s think about privacy, narrowed down to its role in training or using deep learning models, in a more technical way. Since privacy – or rather, its violations – may appear in various ways, different violations will demand different countermeasures. Of course, in the end, we’d like to see them all integrated – but re privacy-related technologies, the field is really just starting out on a journey. The most important thing we can do, then, is to learn about the concepts, investigate the landscape of implementations under development, and – perhaps – decide to join the effort.\nThis post tries to do a tiny little bit of all of those.\nAspects of privacy in deep learning\nSay you work at a hospital, and would be interested in training a deep learning model to help diagnose some disease from brain scans. Where you work, you don’t have many patients with this disease; moreover, they tend to mostly be affected by the same subtypes: Your training set, were you to create one, would not reflect the overall distribution very well. It would, thus, make sense to cooperate with other hospitals; but that isn’t so easy, as the data collected is protected by privacy regulations. So, the first requirement is: The data has to stay where it is; e.g., it may not be sent to a central server.\nFederated learning\nThis first sine qua non is addressed by federated learning (McMahan et al. 2016). Federated learning is not “just” desirable for privacy reasons. On the contrary, in many use cases, it may be the only viable way (like with smartphones or sensors, which collect gigantic amounts of data). In federated learning, each participant receives a copy of the model, trains on their own data, and sends back the gradients obtained to the central server, where gradients are averaged and applied to the model.\nThis is good insofar as the data never leaves the individual devices; however, a lot of information can still be extracted from plain-text gradients. Imagine a smartphone app that provides trainable auto-completion for text messages. Even if gradient updates from many iterations are averaged, their distributions will greatly vary between individuals. Some form of encryption is needed. But then how is the server going to make sense of the encrypted gradients?\nOne way to accomplish this relies on secure multi-party computation (SMPC).\nSecure multi-party computation\nIn SMPC, we need a system of several agents who collaborate to provide a result no single agent could provide alone: “normal” computations (like addition, multiplication …) on “secret” (encrypted) data. The assumption is that these agents are “honest but curious” – honest, because they won’t tamper with their share of data; curious in the sense that if they were (curious, that is), they wouldn’t be able to inspect the data because it’s encrypted.\nThe principle behind this is secret sharing. A single piece of data – a salary, say – is “split up” into meaningless (hence, encrypted) parts which, when put together again, yield the original data. Here is an example.\nSay the parties involved are Julia, Greg, and me. The below function encrypts a single value, assigning to each of us their “meaningless” share:\n\n\n# a big prime number\n# all computations are performed in a finite field, for example, the integers modulo that prime\nQ <- 78090573363827\n \nencrypt <- function(x) {\n  # all but the very last share are random \n  julias <- runif(1, min = -Q, max = Q)\n  gregs <- runif(1, min = -Q, max = Q)\n  mine <- (x - julias - gregs) %% Q\n  list (julias, gregs, mine)\n}\n\n# some top secret value no-one may get to see\nvalue <- 77777\n\nencrypted <- encrypt(value)\nencrypted\n\n\n[[1]]\n[1] 7467283737857\n\n[[2]]\n[1] 36307804406429\n\n[[3]]\n[1] 34315485297318\nOnce the three of us put our shares together, getting back the plain value is straightforward:\n\n\ndecrypt <- function(shares) {\n  Reduce(sum, shares) %% Q  \n}\n\ndecrypt(encrypted)\n\n\n77777\nAs an example of how to compute on encrypted data, here’s addition. (Other operations will be a lot less straightforward.) To add two numbers, just have everyone add their respective shares:\n\n\nadd <- function(x, y) {\n  list(\n    # julia\n    (x[[1]] + y[[1]]) %% Q,\n    # greg\n    (x[[2]] + y[[2]]) %% Q,\n    # me\n    (x[[3]] + y[[3]]) %% Q\n  )\n}\n  \nx <- encrypt(11)\ny <- encrypt(122)\n\ndecrypt(add(x, y))\n\n\n133\nBack to the setting of deep learning and the current task to be solved: Have the server apply gradient updates without ever seeing them. With secret sharing, it would work like this:\nJulia, Greg and me each want to train on our own private data. Together, we will be responsible for gradient averaging, that is, we’ll form a cluster of workers united in that task. Now, the model owner secret shares the model, and we start training, each on their own data. After some number of iterations, we use secure averaging to combine our respective gradients. Then, all the server gets to see is the mean gradient, and there is no way to determine our respective contributions.\nBeyond private gradients\nAmazingly, it is even possible to train on encrypted data – amongst others, using that same technique of secret sharing. Of course, this has to negatively affect training speed. But it’s good to know that if one’s use case were to demand it, it would be feasible. (One possible use case is when training on one party’s data alone doesn’t make any sense, but data is sensitive, so others won’t let you access their data unless encrypted.)\nSo with encryption available on an all-you-need basis, are we completely safe, privacy-wise? The answer is no. The model can still leak information. For example, in some cases it is possible to perform model inversion [@abs-1805-04049], that is, with just black-box access to a model, train an attack model that allows reconstructing some of the original training data. Needless to say, this kind of leakage has to be avoided. Differential privacy (Dwork et al. 2006), (Dwork 2006) demands that results obtained from querying a model be independent from the presence or absence, in the dataset employed for training, of a single individual. In general, this is ensured by adding noise to the answer to every query. In training deep learning models, we add noise to the gradients, as well as clip them according to some chosen norm.\nAt some point, then, we will want all of those in combination: federated learning, encryption, and differential privacy.\nSyft is a very promising, very actively developed framework that aims for providing all of them. Instead of “aims for”, I should perhaps have written “provides” – it depends. We need some more context.\nIntroducing Syft\nSyft – also known as PySyft, since as of today, its most mature implementation is written in and for Python – is maintained by OpenMined, an open source community dedicated to enabling privacy-preserving AI. It’s worth it reproducing their mission statement here:\n\nIndustry standard tools for artificial intelligence have been designed with several assumptions: data is centralized into a single compute cluster, the cluster exists in a secure cloud, and the resulting models will be owned by a central authority. We envision a world in which we are not restricted to this scenario - a world in which AI tools treat privacy, security, and multi-owner governance as first class citizens. […] The mission of the OpenMined community is to create an accessible ecosystem of tools for private, secure, multi-owner governed AI.\n\nWhile far from being the only one, PySyft is their most maturely developed framework. Its role is to provide secure federated learning, including encryption and differential privacy. For deep learning, it relies on existing frameworks.\nPyTorch integration seems the most mature, as of today; with PyTorch, encrypted and differentially private training are already available. Integration with TensorFlow is a bit more involved; it does not yet include TensorFlow Federated and TensorFlow Privacy. For encryption, it relies on TensorFlow Encrypted (TFE), which as of this writing is not an official TensorFlow subproject.\nHowever, even now it is already possible to secret share Keras models and administer private predictions. Let’s see how.\nPrivate predictions with Syft, TensorFlow Encrypted and Keras\nOur introductory example will show how to use an externally-provided model to classify private data – without the model owner ever seeing that data, and without the user ever getting hold of (e.g., downloading) the model. (Think about the model owner wanting to keep the fruits of their labour hidden, as well.)\nPut differently: The model is encrypted, and the data is, too. As you might imagine, this involves a cluster of agents, together performing secure multi-party computation.\nThis use case presupposing an already trained model, we start by quickly creating one. There is nothing special going on here.\nPrelude: Train a simple model on MNIST\n\n\n# create_model.R\n\nlibrary(tensorflow)\nlibrary(keras)\n\nmnist <- dataset_mnist()\nmnist$train$x <- mnist$train$x/255\nmnist$test$x <- mnist$test$x/255\n\ndim(mnist$train$x) <- c(dim(mnist$train$x), 1)\ndim(mnist$test$x) <- c(dim(mnist$test$x), 1)\n\ninput_shape <- c(28, 28, 1)\n\nmodel <- keras_model_sequential() %>%\n  layer_conv_2d(filters = 16, kernel_size = c(3, 3), input_shape = input_shape) %>%\n  layer_average_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_activation(\"relu\") %>%\n  layer_conv_2d(filters = 32, kernel_size = c(3, 3)) %>%\n  layer_average_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_activation(\"relu\") %>%\n  layer_conv_2d(filters = 64, kernel_size = c(3, 3)) %>%\n  layer_average_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_activation(\"relu\") %>%\n  layer_flatten() %>%\n  layer_dense(units = 10, activation = \"linear\")\n  \n\nmodel %>% compile(\n  loss = \"sparse_categorical_crossentropy\",\n  optimizer = \"adam\",\n  metrics = \"accuracy\"\n)\n\nmodel %>% fit(\n    x = mnist$train$x,\n    y = mnist$train$y,\n    epochs = 1,\n    validation_split = 0.3,\n    verbose = 2\n)\n\nmodel$save(filepath = \"model.hdf5\")\n\nSet up cluster and serve model\nThe easiest way to get all required packages is to install the ensemble OpenMined put together for their Udacity Course that introduces federated learning and differential privacy with PySyft. This will install TensorFlow 1.15 and TensorFlow Encrypted, amongst others.\nThe following lines of code should all be put together in a single file. I found it practical to “source” this script from an R process running in a console tab.\nTo begin, we again define the model, two things being different now. First, for technical reasons, we need to pass in batch_input_shape instead of input_shape. Second, the final layer is “missing” the softmax activation. This is not an oversight – SMPC softmax has not been implemented yet. (Depending on when you read this, that statement may no longer be true.) Were we training this model in secret sharing mode, this would of course be a problem; for classification though, all we care about is the maximum score.\nAfter model definition, we load the actual weights from the model we trained in the previous step. Then, the action begins. We create an ensemble of TFE workers that together run a distributed TensorFlow cluster. The model is secret shared with the workers, that is, model weights are split up into shares that, each inspected alone, are unusable. Finally, the model is served, i.e., made available to clients requesting predictions.\nHow can a Keras model be shared and served? These are not methods provided by Keras itself. The magic comes from Syft hooking into Keras, extending the model object: cf. hook <- sy$KerasHook(tf$keras) right after we import Syft.\n\n\n# serve.R\n# you could start R on the console and \"source\" this file\n\n# do this just once\nreticulate::py_install(\"syft[udacity]\")\n\nlibrary(tensorflow)\nlibrary(keras)\n\nsy <- reticulate::import((\"syft\"))\nhook <- sy$KerasHook(tf$keras)\n\nbatch_input_shape <- c(1, 28, 28, 1)\n\nmodel <- keras_model_sequential() %>%\n layer_conv_2d(filters = 16, kernel_size = c(3, 3), batch_input_shape = batch_input_shape) %>%\n layer_average_pooling_2d(pool_size = c(2, 2)) %>%\n layer_activation(\"relu\") %>%\n layer_conv_2d(filters = 32, kernel_size = c(3, 3)) %>%\n layer_average_pooling_2d(pool_size = c(2, 2)) %>%\n layer_activation(\"relu\") %>%\n layer_conv_2d(filters = 64, kernel_size = c(3, 3)) %>%\n layer_average_pooling_2d(pool_size = c(2, 2)) %>%\n layer_activation(\"relu\") %>%\n layer_flatten() %>%\n layer_dense(units = 10) \n \npre_trained_weights <- \"model.hdf5\"\nmodel$load_weights(pre_trained_weights)\n\n# create and start TFE cluster\nAUTO <- TRUE\njulia <- sy$TFEWorker(host = 'localhost:4000', auto_managed = AUTO)\ngreg <- sy$TFEWorker(host = 'localhost:4001', auto_managed = AUTO)\nme <- sy$TFEWorker(host = 'localhost:4002', auto_managed = AUTO)\ncluster <- sy$TFECluster(julia, greg, me)\ncluster$start()\n\n# split up model weights into shares \nmodel$share(cluster)\n\n# serve model (limiting number of requests)\nmodel$serve(num_requests = 3L)\n\nOnce the desired number of requests have been served, we can go to this R process, stop model sharing, and shut down the cluster:\n\n\n# stop model sharing\nmodel$stop()\n\n# stop cluster\ncluster$stop()\n\nNow, on to the client(s).\nRequest predictions on private data\nIn our example, we have one client. The client is a TFE worker, just like the agents that make up the cluster.\nWe define the cluster here, client-side, as well; create the client; and connect the client to the model. This will set up a queueing server that takes care of secret sharing all input data before submitting them for prediction.\nFinally, we have the client asking for classification of the first three MNIST images.\nWith the server running in some different R process, we can conveniently run this in RStudio:\n\n\n# client.R\n\nlibrary(tensorflow)\nlibrary(keras)\n\nsy <- reticulate::import((\"syft\"))\nhook <- sy$KerasHook(tf$keras)\n\nmnist <- dataset_mnist()\nmnist$train$x <- mnist$train$x/255\nmnist$test$x <- mnist$test$x/255\n\ndim(mnist$train$x) <- c(dim(mnist$train$x), 1)\ndim(mnist$test$x) <- c(dim(mnist$test$x), 1)\n\nbatch_input_shape <- c(1, 28, 28, 1)\nbatch_output_shape <- c(1, 10)\n\n# define the same TFE cluster\nAUTO <- TRUE\njulia <- sy$TFEWorker(host = 'localhost:4000', auto_managed = AUTO)\ngreg <- sy$TFEWorker(host = 'localhost:4001', auto_managed = AUTO)\nme <- sy$TFEWorker(host = 'localhost:4002', auto_managed = AUTO)\ncluster <- sy$TFECluster(julia, greg, me)\n\n# create the client\nclient <- sy$TFEWorker()\n\n# create a queueing server on the client that secret shares the data \n# before submitting a prediction request\nclient$connect_to_model(batch_input_shape, batch_output_shape, cluster)\n\nnum_tests <- 3\nimages <- mnist$test$x[1: num_tests, , , , drop = FALSE]\nexpected_labels <- mnist$test$y[1: num_tests]\n\nfor (i in 1:num_tests) {\n  res <- client$query_model(images[i, , , , drop = FALSE])\n  predicted_label <- which.max(res) - 1\n  cat(\"Actual: \", expected_labels[i], \", predicted: \", predicted_label)\n}\n\n\nActual:  7 , predicted:  7 \nActual:  2 , predicted:  2 \nActual:  1 , predicted:  1 \nThere we go. Both model and data did remain secret, yet we were able to classify our data.\nLet’s wrap up.\nConclusion\nOur example use case has not been too ambitious – we started with a trained model, thus leaving aside federated learning. Keeping the setup simple, we were able to focus on underlying principles: Secret sharing as a means of encryption, and setting up a Syft/TFE cluster of workers that together, provide the infrastructure for encrypting model weights as well as client data.\nIn case you’ve read our previous post on TensorFlow Federated – that, too, a framework under development – you may have gotten an impression similar to the one I got: Setting up Syft was a lot more straightforward, concepts were easy to grasp, and surprisingly little code was required. As we may gather from a recent blog post, integration of Syft with TensorFlow Federated and TensorFlow Privacy are on the roadmap. I am looking forward a lot for this to happen.\nThanks for reading!\n\n\n\nDwork, Cynthia. 2006. “Differential Privacy.” In 33rd International Colloquium on Automata, Languages and Programming, Part Ii (Icalp 2006), 33rd International Colloquium on Automata, Languages and Programming, part II (ICALP 2006), 4052:1–12. Lecture Notes in Computer Science. Springer Verlag. https://www.microsoft.com/en-us/research/publication/differential-privacy/.\n\n\nDwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. “Calibrating Noise to Sensitivity in Private Data Analysis.” In Proceedings of the Third Conference on Theory of Cryptography, 265–84. TCC’06. Berlin, Heidelberg: Springer-Verlag. https://doi.org/10.1007/11681878_14.\n\n\nMcMahan, H. Brendan, Eider Moore, Daniel Ramage, and Blaise Agüera y Arcas. 2016. “Federated Learning of Deep Networks Using Model Averaging.” CoRR abs/1602.05629. http://arxiv.org/abs/1602.05629.\n\n\n\n\n",
    "preview": "posts/2020-04-29-encrypted_keras_with_syft/images/thumb.jpg",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-04-21-sparklyr-1.2.0-released/",
    "title": "sparklyr 1.2: Foreach, Spark 3.0 and Databricks Connect",
    "description": "A new sparklyr release is now available. This sparklyr 1.2 release features new functionalities such as support for Databricks Connect, a Spark backend for the 'foreach' package, inter-op improvements for working with Spark 3.0 preview, as well as a number of bug fixes and improvements addressing user-visible pain points.",
    "author": [
      {
        "name": "Yitao Li",
        "url": "https://github.com/yl790"
      }
    ],
    "date": "2020-04-21",
    "categories": [
      "R",
      "Packages/Releases",
      "Distributed Computing"
    ],
    "contents": "\nBehold the glory that is sparklyr 1.2! In this release, the following new hotnesses have emerged into spotlight:\nA registerDoSpark method to create a foreach parallel backend powered by Spark that enables hundreds of existing R packages to run in Spark.\nSupport for Databricks Connect, allowing sparklyr to connect to remote Databricks clusters.\nImproved support for Spark structures when collecting and querying their nested attributes with dplyr.\nA number of inter-op issues observed with sparklyr and Spark 3.0 preview were also addressed recently, in hope that by the time Spark 3.0 officially graces us with its presence, sparklyr will be fully ready to work with it. Most notably, key features such as spark_submit, sdf_bind_rows, and standalone connections are now finally working with Spark 3.0 preview.\nTo install sparklyr 1.2 from CRAN run,\n\n\ninstall.packages(\"sparklyr\")\n\nThe full list of changes are available in the sparklyr NEWS file.\nForeach\nThe foreach package provides the %dopar% operator to iterate over elements in a collection in parallel. Using sparklyr 1.2, you can now register Spark as a backend using registerDoSpark() and then easily iterate over R objects using Spark:\n\n\nlibrary(sparklyr)\nlibrary(foreach)\n\nsc <- spark_connect(master = \"local\", version = \"2.4\")\n\nregisterDoSpark(sc)\nforeach(i = 1:3, .combine = 'c') %dopar% {\n  sqrt(i)\n}\n\n\n[1] 1.000000 1.414214 1.732051\nSince many R packages are based on foreach to perform parallel computation, we can now make use of all those great packages in Spark as well!\nFor instance, we can use parsnip and the tune package with data from mlbench to perform hyperparameter tuning in Spark with ease:\n\n\nlibrary(tune)\nlibrary(parsnip)\nlibrary(mlbench)\n\ndata(Ionosphere)\nsvm_rbf(cost = tune(), rbf_sigma = tune()) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kernlab\") %>%\n  tune_grid(Class ~ .,\n    resamples = rsample::bootstraps(dplyr::select(Ionosphere, -V2), times = 30),\n    control = control_grid(verbose = FALSE))\n\n\n# Bootstrap sampling\n# A tibble: 30 x 4\n   splits            id          .metrics          .notes\n * <list>            <chr>       <list>            <list>\n 1 <split [351/124]> Bootstrap01 <tibble [10 × 5]> <tibble [0 × 1]>\n 2 <split [351/126]> Bootstrap02 <tibble [10 × 5]> <tibble [0 × 1]>\n 3 <split [351/125]> Bootstrap03 <tibble [10 × 5]> <tibble [0 × 1]>\n 4 <split [351/135]> Bootstrap04 <tibble [10 × 5]> <tibble [0 × 1]>\n 5 <split [351/127]> Bootstrap05 <tibble [10 × 5]> <tibble [0 × 1]>\n 6 <split [351/131]> Bootstrap06 <tibble [10 × 5]> <tibble [0 × 1]>\n 7 <split [351/141]> Bootstrap07 <tibble [10 × 5]> <tibble [0 × 1]>\n 8 <split [351/123]> Bootstrap08 <tibble [10 × 5]> <tibble [0 × 1]>\n 9 <split [351/118]> Bootstrap09 <tibble [10 × 5]> <tibble [0 × 1]>\n10 <split [351/136]> Bootstrap10 <tibble [10 × 5]> <tibble [0 × 1]>\n# … with 20 more rows\nThe Spark connection was already registered, so the code ran in Spark without any additional changes. We can verify this was the case by navigating to the Spark web interface:\n\nDatabricks Connect\nDatabricks Connect allows you to connect your favorite IDE (like RStudio!) to a Spark Databricks cluster.\nYou will first have to install the databricks-connect package as described in our README and start a Databricks cluster, but once that’s ready, connecting to the remote cluster is as easy as running:\n\n\nsc <- spark_connect(\n  method = \"databricks\",\n  spark_home = system2(\"databricks-connect\", \"get-spark-home\", stdout = TRUE))\n\n\nThat’s about it, you are now remotely connected to a Databricks cluster from your local R session.\nStructures\nIf you previously used collect to deserialize structurally complex Spark dataframes into their equivalents in R, you likely have noticed Spark SQL struct columns were only mapped into JSON strings in R, which was non-ideal. You might also have run into a much dreaded java.lang.IllegalArgumentException: Invalid type list error when using dplyr to query nested attributes from any struct column of a Spark dataframe in sparklyr.\nUnfortunately, often times in real-world Spark use cases, data describing entities comprising of sub-entities (e.g., a product catalog of all hardware components of some computers) needs to be denormalized / shaped in an object-oriented manner in the form of Spark SQL structs to allow efficient read queries. When sparklyr had the limitations mentioned above, users often had to invent their own workarounds when querying Spark struct columns, which explained why there was a mass popular demand for sparklyr to have better support for such use cases.\nThe good news is with sparklyr 1.2, those limitations no longer exist any more when working running with Spark 2.4 or above.\nAs a concrete example, consider the following catalog of computers:\n\n\nlibrary(dplyr)\n\ncomputers <- tibble::tibble(\n  id = seq(1, 2),\n  attributes = list(\n    list(\n      processor = list(freq = 2.4, num_cores = 256),\n      price = 100\n   ),\n   list(\n     processor = list(freq = 1.6, num_cores = 512),\n     price = 133\n   )\n  )\n)\n\ncomputers <- copy_to(sc, computers, overwrite = TRUE)\n\nA typical dplyr use case involving computers would be the following:\n\n\nhigh_freq_computers <- computers %>%\n                       filter(attributes.processor.freq >= 2) %>%\n                       collect()\n\nAs previously mentioned, before sparklyr 1.2, such query would fail with Error: java.lang.IllegalArgumentException: Invalid type list.\nWhereas with sparklyr 1.2, the expected result is returned in the following form:\n\n# A tibble: 1 x 2\n     id attributes\n  <int> <list>\n1     1 <named list [2]>\nwhere high_freq_computers$attributes is what we would expect:\n\n[[1]]\n[[1]]$price\n[1] 100\n\n[[1]]$processor\n[[1]]$processor$freq\n[1] 2.4\n\n[[1]]$processor$num_cores\n[1] 256\nAnd More!\nLast but not least, we heard about a number of pain points sparklyr users have run into, and have addressed many of them in this release as well. For example:\nDate type in R is now correctly serialized into Spark SQL date type by copy_to\n<spark dataframe> %>% print(n = 20) now actually prints 20 rows as expected instead of 10\nspark_connect(master = \"local\") will emit a more informative error message if it’s failing because the loopback interface is not up\n… to just name a few. We want to thank the open source community for their continuous feedback on sparklyr, and are looking forward to incorporating more of that feedback to make sparklyr even better in the future.\nFinally, in chronological order, we wish to thank the following individuals for contributing to sparklyr 1.2: zero323, Andy Zhang, Yitao Li, Javier Luraschi, Hossein Falaki, Lu Wang, Samuel Macedo and Jozef Hajnala. Great job everyone!\nIf you need to catch up on sparklyr, please visit sparklyr.ai, spark.rstudio.com, or some of the previous release posts: sparklyr 1.1 and sparklyr 1.0.\nThank you for reading this post.\n\n\n",
    "preview": "posts/2020-04-21-sparklyr-1.2.0-released/images/sparklyr.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 1241,
    "preview_height": 307
  },
  {
    "path": "posts/2020-04-13-pins-04/",
    "title": "pins 0.4: Versioning",
    "description": "A new release of pins is available on CRAN today. This release adds support to time travel across dataset versions, which improves collaboration and protects your code from breaking when remote resources change unexpectedly.",
    "author": [
      {
        "name": "Javier Luraschi",
        "url": {}
      }
    ],
    "date": "2020-04-13",
    "categories": [
      "R",
      "Packages/Releases",
      "Data Management"
    ],
    "contents": "\nA new version of pins is available on CRAN today, which adds support for versioning your datasets and DigitalOcean Spaces boards!\nAs a quick recap, the pins package allows you to cache, discover and share resources. You can use pins in a wide range of situations, from downloading a dataset from a URL to creating complex automation workflows (learn more at pins.rstudio.com). You can also use pins in combination with TensorFlow and Keras; for instance, use cloudml to train models in cloud GPUs, but rather than manually copying files into the GPU instance, you can store them as pins directly from R.\nTo install this new version of pins from CRAN, simply run:\n\ninstall.packages(\"pins\")\nYou can find a detailed list of improvements in the pins NEWS file.\nVersioning\nTo illustrate the new versioning functionality, let’s start by downloading and caching a remote dataset with pins. For this example, we will download the weather in London, this happens to be in JSON format and requires jsonlite to be parsed:\n\nlibrary(pins)\n\nweather_url <- \"https://samples.openweathermap.org/data/2.5/weather?q=London,uk&appid=b6907d289e10d714a6e88b30761fae22\"\n\npin(weather_url, \"weather\") %>%\n  jsonlite::read_json() %>%\n  as.data.frame()\n\n  coord.lon coord.lat weather.id weather.main     weather.description weather.icon\n1     -0.13     51.51        300      Drizzle light intensity drizzle          09d\nOne advantage of using pins is that, even if the URL or your internet connection becomes unavailable, the above code will still work.\nBut back to pins 0.4! The new signature parameter in pin_info() allows you to retrieve the “version” of this dataset:\n\npin_info(\"weather\", signature = TRUE)\n\n# Source: local<weather> [files]\n# Signature: 624cca260666c6f090b93c37fd76878e3a12a79b\n# Properties:\n#   - path: weather\nYou can then validate the remote dataset has not changed by specifying its signature:\n\npin(weather_url, \"weather\", signature = \"624cca260666c6f090b93c37fd76878e3a12a79b\") %>%\n  jsonlite::read_json()\nIf the remote dataset changes, pin() will fail and you can take the appropriate steps to accept the changes by updating the signature or properly updating your code. The previous example is useful as a way of detecting version changes, but we might also want to retrieve specific versions even when the dataset changes.\npins 0.4 allows you to display and retrieve versions from services like GitHub, Kaggle and RStudio Connect. Even in boards that don’t support versioning natively, you can opt-in by registering a board with versions = TRUE.\nTo keep this simple, let’s focus on GitHub first. We will register a GitHub board and pin a dataset to it. Notice that you can also specify the commit parameter in GitHub boards as the commit message for this change.\n\nboard_register_github(repo = \"javierluraschi/datasets\", branch = \"datasets\")\n\npin(iris, name = \"versioned\", board = \"github\", commit = \"use iris as the main dataset\")\nNow suppose that a colleague comes along and updates this dataset as well:\n\npin(mtcars, name = \"versioned\", board = \"github\", commit = \"slight preference to mtcars\")\nFrom now on, your code could be broken or, even worse, produce incorrect results!\nHowever, since GitHub was designed as a version control system and pins 0.4 adds support for pin_versions(), we can now explore particular versions of this dataset:\n\npin_versions(\"versioned\", board = \"github\")\n\n# A tibble: 2 x 4\n  version created              author         message                     \n  <chr>   <chr>                <chr>          <chr>                       \n1 6e6c320 2020-04-02T21:28:07Z javierluraschi slight preference to mtcars \n2 01f8ddf 2020-04-02T21:27:59Z javierluraschi use iris as the main dataset\nYou can then retrieve the version you are interested in as follows:\n\npin_get(\"versioned\", version = \"01f8ddf\", board = \"github\")\n\n# A tibble: 150 x 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 140 more rows\nYou can follow similar steps for RStudio Connect and Kaggle boards, even for existing pins! Other boards like Amazon S3, Google Cloud, Digital Ocean and Microsoft Azure require you explicitly enable versioning when registering your boards.\nDigitalOcean\nTo try out the new DigitalOcean Spaces board, first you will have to register this board and enable versioning by setting versions to TRUE:\n\nlibrary(pins)\nboard_register_dospace(space = \"pinstest\",\n                       key = \"AAAAAAAAAAAAAAAAAAAA\",\n                       secret = \"ABCABCABCABCABCABCABCABCABCABCABCABCABCA==\",\n                       datacenter = \"sfo2\",\n                       versions = TRUE)\nYou can then use all the functionality pins provides, including versioning:\n\n# create pin and replace content in digitalocean\npin(iris, name = \"versioned\", board = \"pinstest\")\npin(mtcars, name = \"versioned\", board = \"pinstest\")\n\n# retrieve versions from digitalocean\npin_versions(name = \"versioned\", board = \"pinstest\")\n\n# A tibble: 2 x 1\n  version\n  <chr>  \n1 c35da04\n2 d9034cd\nNotice that enabling versions in cloud services requires additional storage space for each version of the dataset being stored:\n\nTo learn more visit the Versioning and DigitalOcean articles. To catch up with previous releases:\npins 0.3: Azure, GCloud and S3\npins 0.2: Pin, Discover and Share Resources\nThanks for reading along!\n\n\n",
    "preview": "posts/2020-04-13-pins-04/images/thumb.jpg",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-04-08-tf-federated-intro/",
    "title": "A first look at federated learning with TensorFlow",
    "description": "The term \"federated learning\" was coined to describe a form of distributed model training where the data remains on client devices, i.e., is never shipped to the coordinating server. In this post, we introduce central concepts and run first experiments with TensorFlow Federated, using R.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-04-08",
    "categories": [
      "Privacy & Security",
      "TensorFlow/Keras"
    ],
    "contents": "\nHere, stereotypically, is the process of applied deep learning: Gather/get data; iteratively train and evaluate; deploy. Repeat (or have it all automated as a continuous workflow). We often discuss training and evaluation; deployment matters to varying degrees, depending on the circumstances. But the data often is just assumed to be there: All together, in one place (on your laptop; on a central server; in some cluster in the cloud.) In real life though, data could be all over the world: on smartphones for example, or on IoT devices. There are a lot of reasons why we don’t want to ship all that data to some central location: Privacy, of course (why should some third party get to know about what you texted your friend?); but also, sheer mass (and this latter aspect is bound to become more influential all the time).\nA solution is that data on client devices stays on client devices, yet participates in training a global model. How? In so-called federated learning(McMahan et al. 2016), there is a central coordinator (“server”), as well as a potentially huge number of clients (e.g., phones) who participate in learning on an “as-fits” basis: e.g., if plugged in and on a high-speed connection. Whenever they’re ready to train, clients are passed the current model weights, and perform some number of training iterations on their own data. They then send back gradient information to the server (more on that soon), whose job is to update the weights accordingly. Federated learning is not the only conceivable protocol to jointly train a deep learning model while keeping the data private: A fully decentralized alternative could be gossip learning (Blot et al. 2016), following the gossip protocol . As of today, however, I am not aware of existing implementations in any of the major deep learning frameworks.\nIn fact, even TensorFlow Federated (TFF), the library used in this post, was officially introduced just about a year ago. Meaning, all this is pretty new technology, somewhere inbetween proof-of-concept state and production readiness. So, let’s set expectations as to what you might get out of this post.\nWhat to expect from this post\nWe start with quick glance at federated learning in the context of privacy overall. Subsequently, we introduce, by example, some of TFF’s basic building blocks. Finally, we show a complete image classification example using Keras – from R.\nWhile this sounds like “business as usual”, it’s not – or not quite. With no R package existing, as of this writing, that would wrap TFF, we’re accessing its functionality using $-syntax – not in itself a big problem. But there’s something else.\nTFF, while providing a Python API, itself is not written in Python. Instead, it is an internal language designed specifically for serializability and distributed computation. One of the consequences is that TensorFlow (that is: TF as opposed to TFF) code has to be wrapped in calls to tf.function, triggering static-graph construction. However, as I write this, the TFF documentation cautions: “Currently, TensorFlow does not fully support serializing and deserializing eager-mode TensorFlow.” Now when we call TFF from R, we add another layer of complexity, and are more likely to run into corner cases.\nTherefore, at the current stage, when using TFF from R it’s advisable to play around with high-level functionality – using Keras models – instead of, e.g., translating to R the low-level functionality shown in the second TFF Core tutorial.\nOne final remark before we get started: As of this writing, there is no documentation on how to actually run federated training on “real clients”. There is, however, a document that describes how to run TFF on Google Kubernetes Engine, and deployment-related documentation is visibly and steadily growing.)\nThat said, now how does federated learning relate to privacy, and how does it look in TFF?\nFederated learning in context\nIn federated learning, client data never leaves the device. So in an immediate sense, computations are private. However, gradient updates are sent to a central server, and this is where privacy guarantees may be violated. In some cases, it may be easy to reconstruct the actual data from the gradients – in an NLP task, for example, when the vocabulary is known on the server, and gradient updates are sent for small pieces of text.\nThis may sound like a special case, but general methods have been demonstrated that work regardless of circumstances. For example, Zhu et al. (Zhu, Liu, and Han 2019) use a “generative” approach, with the server starting from randomly generated fake data (resulting in fake gradients) and then, iteratively updating that data to obtain gradients more and more like the real ones – at which point the real data has been reconstructed.\nComparable attacks would not be feasible were gradients not sent in clear text. However, the server needs to actually use them to update the model – so it must be able to “see” them, right? As hopeless as this sounds, there are ways out of the dilemma. For example, homomorphic encryption, a technique that enables computation on encrypted data. Or secure multi-party aggregation, often achieved through secret sharing, where individual pieces of data (e.g.: individual salaries) are split up into “shares”, exchanged and combined with random data in various ways, until finally the desired global result (e.g.: mean salary) is computed. (These are extremely fascinating topics that unfortunately, by far surpass the scope of this post.)\nNow, with the server prevented from actually “seeing” the gradients, a problem still remains. The model – especially a high-capacity one, with many parameters – could still memorize individual training data. Here is where differential privacy comes into play. In differential privacy, noise is added to the gradients to decouple them from actual training examples. (This post gives an introduction to differential privacy with TensorFlow, from R.)\nAs of this writing, TFF’s federal averaging mechanism (McMahan et al. 2016) does not yet include these additional privacy-preserving techniques. But research papers exist that outline algorithms for integrating both secure aggregation (Bonawitz et al. 2016) and differential privacy (McMahan et al. 2017) .\nClient-side and server-side computations\nLike we said above, at this point it is advisable to mainly stick with high-level computations using TFF from R. (Presumably that is what we’d be interested in in many cases, anyway.) But it’s instructive to look at a few building blocks from a high-level, functional point of view.\nIn federated learning, model training happens on the clients. Clients each compute their local gradients, as well as local metrics. The server, on the other hand, calculates global gradient updates, as well as global metrics.\nLet’s say the metric is accuracy. Then clients and server both compute averages: local averages and a global average, respectively. All the server will need to know to determine the global averages are the local ones and the respective sample sizes.\nLet’s see how TFF would calculate a simple average.\nThe code in this post was run with the current TensorFlow release 2.1 and TFF version 0.13.1. We use reticulate to install and import TFF.\n\n\nlibrary(tensorflow)\nlibrary(reticulate)\nlibrary(tfdatasets)\n\npy_install(\"tensorflow-federated\")\n\ntff <- import(\"tensorflow_federated\")\n\nFirst, we need every client to be able to compute their own local averages.\nHere is a function that reduces a list of values to their sum and count, both at the same time, and then returns their quotient.\nThe function contains only TensorFlow operations, not computations described in R directly; if there were any, they would have to be wrapped in calls to tf_function, calling for construction of a static graph. (The same would apply to raw (non-TF) Python code.)\nNow, this function will still have to be wrapped (we’re getting to that in an instant), as TFF expects functions that make use of TF operations to be decorated by calls to tff$tf_computation. Before we do that, one comment on the use of dataset_reduce: Inside tff$tf_computation, the data that is passed in behaves like a dataset, so we can perform tfdatasets operations like dataset_map, dataset_filter etc. on it.\n\n\nget_local_temperature_average <- function(local_temperatures) {\n  sum_and_count <- local_temperatures %>% \n    dataset_reduce(tuple(0, 0), function(x, y) tuple(x[[1]] + y, x[[2]] + 1))\n  sum_and_count[[1]] / tf$cast(sum_and_count[[2]], tf$float32)\n}\n\nNext is the call to tff$tf_computation we already alluded to, wrapping get_local_temperature_average. We also need to indicate the argument’s TFF-level type. (In the context of this post, TFF datatypes are definitely out-of-scope, but the TFF documentation has lots of detailed information in that regard. All we need to know right now is that we will be able to pass the data as a list.)\n\n\nget_local_temperature_average <- tff$tf_computation(get_local_temperature_average, tff$SequenceType(tf$float32))\n\nLet’s test this function:\n\n\nget_local_temperature_average(list(1, 2, 3))\n\n\n[1] 2\nSo that’s a local average, but we originally set out to compute a global one. Time to move on to server side (code-wise).\nNon-local computations are called federated (not too surprisingly). Individual operations start with federated_; and these have to be wrapped in tff$federated_computation:\n\n\nget_global_temperature_average <- function(sensor_readings) {\n  tff$federated_mean(tff$federated_map(get_local_temperature_average, sensor_readings))\n}\n\nget_global_temperature_average <- tff$federated_computation(\n  get_global_temperature_average, tff$FederatedType(tff$SequenceType(tf$float32), tff$CLIENTS))\n\nCalling this on a list of lists – each sub-list presumedly representing client data – will display the global (non-weighted) average:\n\n\nget_global_temperature_average(list(list(1, 1, 1), list(13)))\n\n\n[1] 7\nNow that we’ve gotten a bit of a feeling for “low-level TFF”, let’s train a Keras model the federated way.\nFederated Keras\nThe setup for this example looks a bit more Pythonian1 than usual. We need the collections module from Python to make use of OrderedDicts, and we want them to be passed to Python without intermediate conversion to R – that’s why we import the module with convert set to FALSE.\n\n\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(tfds)\nlibrary(reticulate)\nlibrary(tfdatasets)\nlibrary(dplyr)\n\ntff <- import(\"tensorflow_federated\")\ncollections <- import(\"collections\", convert = FALSE)\nnp <- import(\"numpy\")\n\nFor this example, we use Kuzushiji-MNIST (Clanuwat et al. 2018), which may conveniently be obtained through tfds, the R wrapper for TensorFlow Datasets.\nThe 10 classes of Kuzushiji-MNIST, with the first column showing each character's modern hiragana counterpart. From: https://github.com/rois-codh/kmnistTensorFlow datasets come as – well – datasets, which normally would be just fine; here however, we want to simulate different clients each with their own data. The following code splits up the dataset into ten arbitrary – sequential, for convenience – ranges and, for each range (that is: client), creates a list of OrderedDicts that have the images as their x, and the labels as their y component:\n\n\nn_train <- 60000\nn_test <- 10000\n\ns <- seq(0, 90, by = 10)\ntrain_ranges <- paste0(\"train[\", s, \"%:\", s + 10, \"%]\") %>% as.list()\ntrain_splits <- purrr::map(train_ranges, function(r) tfds_load(\"kmnist\", split = r))\n\ntest_ranges <- paste0(\"test[\", s, \"%:\", s + 10, \"%]\") %>% as.list()\ntest_splits <- purrr::map(test_ranges, function(r) tfds_load(\"kmnist\", split = r))\n\nbatch_size <- 100\n\ncreate_client_dataset <- function(source, n_total, batch_size) {\n  iter <- as_iterator(source %>% dataset_batch(batch_size))\n  output_sequence <- vector(mode = \"list\", length = n_total/10/batch_size)\n  i <- 1\n  while (TRUE) {\n    item <- iter_next(iter)\n    if (is.null(item)) break\n    x <- tf$reshape(tf$cast(item$image, tf$float32), list(100L,784L))/255\n    y <- item$label\n    output_sequence[[i]] <-\n      collections$OrderedDict(\"x\" = np_array(x$numpy(), np$float32), \"y\" = y$numpy())\n     i <- i + 1\n  }\n  output_sequence\n}\n\nfederated_train_data <- purrr::map(\n  train_splits, function(split) create_client_dataset(split, n_train, batch_size))\n\nAs a quick check, the following are the labels for the first batch of images for client 5:\n\n\nfederated_train_data[[5]][[1]][['y']]\n\n\n> [0. 9. 8. 3. 1. 6. 2. 8. 8. 2. 5. 7. 1. 6. 1. 0. 3. 8. 5. 0. 5. 6. 6. 5.\n 2. 9. 5. 0. 3. 1. 0. 0. 6. 3. 6. 8. 2. 8. 9. 8. 5. 2. 9. 0. 2. 8. 7. 9.\n 2. 5. 1. 7. 1. 9. 1. 6. 0. 8. 6. 0. 5. 1. 3. 5. 4. 5. 3. 1. 3. 5. 3. 1.\n 0. 2. 7. 9. 6. 2. 8. 8. 4. 9. 4. 2. 9. 5. 7. 6. 5. 2. 0. 3. 4. 7. 8. 1.\n 8. 2. 7. 9.]\nThe model is a simple, one-layer sequential Keras model. For TFF to have full control over graph construction, it has to be defined inside a function. The blueprint for creation is passed to tff$learning$from_keras_model, together with a “dummy” batch that exemplifies how the training data will look:\n\n\nsample_batch = federated_train_data[[5]][[1]]\n\ncreate_keras_model <- function() {\n  keras_model_sequential() %>%\n    layer_dense(input_shape = 784,\n                units = 10,\n                kernel_initializer = \"zeros\",\n                activation = \"softmax\") \n}\n\nmodel_fn <- function() {\n  keras_model <- create_keras_model()\n  tff$learning$from_keras_model(\n    keras_model,\n    dummy_batch = sample_batch,\n    loss = tf$keras$losses$SparseCategoricalCrossentropy(),\n    metrics = list(tf$keras$metrics$SparseCategoricalAccuracy()))\n}\n\nTraining is a stateful process that keeps updating model weights (and if applicable, optimizer states). It is created via tff$learning$build_federated_averaging_process …\n\n\niterative_process <- tff$learning$build_federated_averaging_process(\n  model_fn,\n  client_optimizer_fn = function() tf$keras$optimizers$SGD(learning_rate = 0.02),\n  server_optimizer_fn = function() tf$keras$optimizers$SGD(learning_rate = 1.0))\n\n… and on initialization, produces a starting state:\n\n\nstate <- iterative_process$initialize()\nstate\n\n\n<model=<trainable=<[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]],[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]>,non_trainable=<>>,optimizer_state=<0>,delta_aggregate_state=<>,model_broadcast_state=<>>\nThus before training, all the state does is reflect our zero-initialized model weights.\nNow, state transitions are accomplished via calls to next(). After one round of training, the state then comprises the “state proper” (weights, optimizer parameters …) as well as the current training metrics:\n\n\nstate_and_metrics <- iterative_process$`next`(state, federated_train_data)\n\nstate <- state_and_metrics[0]\nstate\n\n\n<model=<trainable=<[[ 9.9695253e-06 -8.5083229e-05 -8.9266898e-05 ... -7.7834651e-05\n  -9.4819807e-05  3.4227365e-04]\n [-5.4778640e-05 -1.5390900e-04 -1.7912561e-04 ... -1.4122366e-04\n  -2.4614178e-04  7.7663612e-04]\n [-1.9177950e-04 -9.0706220e-05 -2.9841764e-04 ... -2.2249141e-04\n  -4.1685964e-04  1.1348884e-03]\n ...\n [-1.3832574e-03 -5.3664664e-04 -3.6622395e-04 ... -9.0854493e-04\n   4.9618416e-04  2.6899918e-03]\n [-7.7253254e-04 -2.4583895e-04 -8.3220737e-05 ... -4.5274393e-04\n   2.6396243e-04  1.7454443e-03]\n [-2.4157032e-04 -1.3836231e-05  5.0371520e-05 ... -1.0652864e-04\n   1.5947431e-04  4.5250656e-04]],[-0.01264258  0.00974309  0.00814162  0.00846065 -0.0162328   0.01627758\n -0.00445857 -0.01607843  0.00563046  0.00115899]>,non_trainable=<>>,optimizer_state=<1>,delta_aggregate_state=<>,model_broadcast_state=<>>\n\n\nmetrics <- state_and_metrics[1]\nmetrics\n\n\n<sparse_categorical_accuracy=0.5710999965667725,loss=1.8662642240524292,keras_training_time_client_sum_sec=0.0>\nLet’s train for a few more epochs, keeping track of accuracy:\n\n\nnum_rounds <- 20\n\nfor (round_num in (2:num_rounds)) {\n  state_and_metrics <- iterative_process$`next`(state, federated_train_data)\n  state <- state_and_metrics[0]\n  metrics <- state_and_metrics[1]\n  cat(\"round: \", round_num, \"  accuracy: \", round(metrics$sparse_categorical_accuracy, 4), \"\\n\")\n}\n\n\nround:  2    accuracy:  0.6949 \nround:  3    accuracy:  0.7132 \nround:  4    accuracy:  0.7231 \nround:  5    accuracy:  0.7319 \nround:  6    accuracy:  0.7404 \nround:  7    accuracy:  0.7484 \nround:  8    accuracy:  0.7557 \nround:  9    accuracy:  0.7617 \nround:  10   accuracy:  0.7661 \nround:  11   accuracy:  0.7695 \nround:  12   accuracy:  0.7728 \nround:  13   accuracy:  0.7764 \nround:  14   accuracy:  0.7788 \nround:  15   accuracy:  0.7814 \nround:  16   accuracy:  0.7836 \nround:  17   accuracy:  0.7855 \nround:  18   accuracy:  0.7872 \nround:  19   accuracy:  0.7885 \nround:  20   accuracy:  0.7902 \nTraining accuracy is increasing continuously. These values represent averages of local accuracy measurements, so in the real world, they might well be overly optimistic (with each client overfitting on their respective data). So supplementing federated training, a federated evaluation process would need to be built in order to get a realistic view on performance. This is a topic to come back to when more related TFF documentation is available.\nConclusion\nWe hope you’ve enjoyed this first introduction to TFF using R. Certainly at this time, it is too early for use in production; and for application in research (e.g., adversarial attacks on federated learning) familiarity with “lowish”-level implementation code is required – regardless whether you use R or Python.\nHowever, judging from activity on GitHub, TFF is under very active development right now (including new documentation being added!), so we’re looking forward to what’s to come. In the meantime, it’s never too early to start learning the concepts…\nThanks for reading!\n\n\nBlot, Michael, David Picard, Matthieu Cord, and Nicolas Thome. 2016. “Gossip Training for Deep Learning.” CoRR abs/1611.09726. http://arxiv.org/abs/1611.09726.\n\n\nBonawitz, Keith, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. 2016. “Practical Secure Aggregation for Federated Learning on User-Held Data.” CoRR abs/1611.04482. http://arxiv.org/abs/1611.04482.\n\n\nClanuwat, Tarin, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. 2018. “Deep Learning for Classical Japanese Literature.” December 3, 2018. http://arxiv.org/abs/cs.CV/1812.01718.\n\n\nMcMahan, H. Brendan, Eider Moore, Daniel Ramage, and Blaise Agüera y Arcas. 2016. “Federated Learning of Deep Networks Using Model Averaging.” CoRR abs/1602.05629. http://arxiv.org/abs/1602.05629.\n\n\nMcMahan, H. Brendan, Daniel Ramage, Kunal Talwar, and Li Zhang. 2017. “Learning Differentially Private Language Models Without Losing Accuracy.” CoRR abs/1710.06963. http://arxiv.org/abs/1710.06963.\n\n\nZhu, Ligeng, Zhijian Liu, and Song Han. 2019. “Deep Leakage from Gradients.” CoRR abs/1906.08935. http://arxiv.org/abs/1906.08935.\n\n\nnot Pythonic :-)↩︎\n",
    "preview": "posts/2020-04-08-tf-federated-intro/images/federated_learning.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 1122,
    "preview_height": 570
  },
  {
    "path": "posts/2020-04-01-rstudio-ai-blog/",
    "title": "Introducing: The RStudio AI Blog",
    "description": "This blog just got a new title: RStudio AI Blog. We explain why.",
    "author": [
      {
        "name": "The Multiverse Team",
        "url": {}
      }
    ],
    "date": "2020-03-30",
    "categories": [
      "Meta"
    ],
    "contents": "\nWhy the new name, RStudio AI Blog? There is a straightforward reason. The previous title, “TensorFlow for R Blog”, was a good match for the content we covered so far: technical or applied aspects of performing deep learning with TensorFlow and Keras. Yet, our team (the Multiverse Team) is not working exclusively in those areas; instead, enabling distributed computing from R (sparklyr), integrating automated machine learning workflows (mlflow), and optimizing data ingestion (pins) are substantial aspects of what we do. We would like to have a platform we can use to tell you about our work in these areas as well. Furthermore, regarding the hitherto dominant topic on this blog, deep learning, we might also want to reflect about it in a less technical way, focussing on impacts on society, ethics, or even “just” epistemic questions.\nConsequently, we needed a new name, but why “AI”? Maybe “data science” would work as well – however, the science in data science brings up connotations of formality and theoretic ambitions which we would rather avoid. Instead, AI appeared to be a more rigorous definition, understood as outlined in an article by Michael Jordan. Jordan envisions AI as a new engineering discipline that builds on existing knowledge about inference, optimization, computation, and data processing the way that chemical engineering and civil engineering built upon chemistry and physics, respectively. Supplementing those building blocks (from mathematics, statistics, computer science), the goal of this new discipline is to include guidance from the social sciences and the humanities.\nBy the way, as of this writing, the Multiverse Team consists of Daniel Falbel, Sigrid Keydana, Yitao Li, and Javier Luraschi. You can find us on Twitter under the #mlverse tag, or pass by our new mlverse channel on YouTube. Thank you for your support!\n\n\n",
    "preview": "posts/2020-04-01-rstudio-ai-blog/images/thumb.jpg",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-02-19-kl-divergence/",
    "title": "Infinite surprise - the iridescent personality of Kullback-Leibler divergence",
    "description": "Kullback-Leibler divergence is not just used to train variational autoencoders or Bayesian networks (and not just a hard-to-pronounce thing). It is a fundamental concept in information theory, put to use in a vast range of applications. Most interestingly, it's not always about constraint, regularization or compression. Quite on the contrary, sometimes it is about novelty, discovery and surprise.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-02-19",
    "categories": [
      "Probabilistic ML/DL",
      "Concepts"
    ],
    "contents": "\nAmong deep learning practitioners, Kullback-Leibler divergence (KL divergence) is perhaps best known for its role in training variational autoencoders (VAEs).1 To learn an informative latent space, we don’t just optimize for good reconstruction. Rather, we also impose a prior on the latent distribution, and aim to keep them close – often, by minimizing KL divergence.\nIn this role, KL divergence acts like a watchdog; it is a constraining, regularizing factor, and if anthropomorphized, would seem stern and severe. If we leave it at that, however, we’ve seen just one side of its character, and are missing out on its complement, a picture of playfulness, adventure, and curiosity. In this post, we’ll take a look at that other side.\nWhile being inspired by a series of tweets by Simon de Deo, enumerating applications of KL divergence in a vast number of disciplines,\n\nwe don’t aspire to provide a comprehensive write-up here – as mentioned in the initial tweet, the topic could easily fill a whole semester of study.\nThe much more modest goals of this post, then, are\nto quickly recap the role of KL divergence in training VAEs, and mention similar-in-character applications;\nto illustrate that more playful, adventurous “other side” of its character;2 and\nin a not-so-entertaining, but – hopefully – useful manner, differentiate KL divergence from related concepts such as cross entropy, mutual information, or free energy.\nBefore though, we start with a definition and some terminology.\nKL divergence in a nutshell\nKL divergence is the expected value of the logarithmic difference in probabilities according to two distributions, \\(p\\) and \\(q\\). Here it is in its discrete-probabilities variant:\n\\[\\begin{equation}\nD_{KL}(p||q) = \\sum\\limits_{x} p(x) log(\\frac{p(x)}{q(x)})\n \\tag{1}\n\\end{equation}\\]\nNotably, it is asymmetric; that is, \\(D_{KL}(p||q)\\) is not the same as \\(D_{KL}(q||p)\\). (Which is why it is a divergence, not a distance.) This aspect will play an important role in section 2 dedicated to the “other side”.\nTo stress this asymmetry, KL divergence is sometimes called relative information (as in “information of \\(p\\) relative to \\(q\\)”), or information gain. We agree with one of our sources3 that because of its universality and importance, KL divergence would probably have deserved a more informative name; such as, precisely, information gain. (Which is less ambiguous pronunciation-wise, as well.)\nKL divergence, “villain”\nIn many machine learning algorithms, KL divergence appears in the context of variational inference. Often, for realistic data, exact computation of the posterior distribution is infeasible. Thus, some form of approximation is required. In variational inference, the true posterior \\(p^*\\) is approximated by a simpler distribution, \\(q\\), from some tractable family. To ensure we have a good approximation, we minimize – in theory, at least – the KL divergence of \\(q\\) relative to \\(p^*\\), thus replacing inference by optimization.\nIn practice, again for reasons of intractability, the KL divergence minimized is that of \\(q\\) relative to an unnormalized distribution \\(\\widetilde{p}\\)\n\\[\\begin{equation}\nJ(q)\\ = D_{KL}(q||\\widetilde{p})\n \\tag{2}\n\\end{equation}\\]\nwhere \\(\\widetilde{p}\\) is the joint distribution of parameters and data:\n\\[\\begin{equation}\n\\widetilde{p}(\\mathbf{x}) = p(\\mathbf{x}, \\mathcal{D}) = p^*(\\mathbf{x}) \\ p(\\mathcal{D})\n \\tag{3}\n\\end{equation}\\]\nand \\(p^*\\) is the true posterior:\n\\[\\begin{equation}\np^*(\\mathbf{x}) = p(\\mathbf{x}|\\mathcal{D})\n \\tag{4}\n\\end{equation}\\]\nEquivalent to that formulation (eq. (2)) – for a derivation see (Murphy 2012) – is this, which shows the optimization objective to be an upper bound on the negative log-likelihood (NLL):\n\\[\\begin{equation}\nJ(q)\\ =  D_{KL}(q||p^*) - log \\ p(D)\n \\tag{5}\n\\end{equation}\\]\nYet another formulation – again, see (Murphy 2012) for details – is the one we actually use when training (e.g.) VAEs. This one corresponds to the expected NLL plus the KL divergence between the approximation \\(q\\) and the imposed prior \\(p\\):\n\\[\\begin{equation}\nJ(q)\\ =  D_{KL}(q||p) - E_q[- log \\ p(\\mathcal{D}|\\mathbf{x})]\n \\tag{6}\n\\end{equation}\\]\nNegated, this formulation is also called the ELBO, for evidence lower bound. In the VAE post cited above, the ELBO was written\n\\[\\begin{equation}\nELBO\\ = \\ E[log\\ p(x|z)]\\ -\\ KL(q(z)||p(z))\n \\tag{7}\n\\end{equation}\\]\nwith \\(z\\) denoting the latent variables (\\(q(z)\\) being the approximation, \\(p(z)\\) the prior, often a multivariate normal).\nBeyond VAEs\nGeneralizing this “conservative” action pattern of KL divergence beyond VAEs, we can say that it expresses the quality of approximations. An important area where approximation takes place is (lossy) compression. KL divergence provides a way to quantify how much information is lost when we compress data.\nSumming up, in these and similar applications, KL divergence is “bad” – although we don’t want it to be zero (or else, why bother using the algorithm?), we certainly want to keep it low. So now, let’s see the other side.\nKL divergence, good guy\nIn a second category of applications, KL divergence is not something to be minimized.4 In these domains, KL divergence is indicative of surprise, disagreement, exploratory behavior, or learning: This truly is the perspective of information gain.\nSurprise\nOne domain where surprise, not information per se, governs behavior is perception. For example, eyetracking studies (e.g., (Itti and Baldi 2005)) showed that surprise, as measured by KL divergence, was a better predictor of visual attention than information, measured by entropy.5 While these studies seem to have popularized the expression “Bayesian surprise”, this compound is – I think – not the most informative one, as neither part adds much information to the other. In Bayesian updating, the magnitude of the difference between prior and posterior reflects the degree of surprise brought about by the data – surprise is an integral part of the concept.\nThus, with KL divergence linked to surprise, and surprise rooted in the fundamental process of Bayesian updating, a process that could be used to describe the course of life itself, KL divergence itself becomes fundamental. We could get tempted to see it everywhere. Accordingly, it has been used in many fields to quantify unidirectional divergence.\nFor example, (Zanardo 2017) have applied it in trading, measuring how much a person disagrees with the market belief. Higher disagreement then corresponds to higher expected gains from betting against the market.\nCloser to the area of deep learning, it is used in intrinsically motivated reinforcement learning (e.g., (Sun, Gomez, and Schmidhuber 2011)), where an optimal policy should maximize the long-term information gain. This is possible because like entropy, KL divergence is additive.\nAlthough its asymmetry is relevant whether you use KL divergence for regularization (section 1) or surprise (this section), it becomes especially evident when used for learning and surprise.\nAsymmetry in action\nLooking again at the KL formula\n\\[\\begin{equation}\nD_{KL}(p||q) = \\sum\\limits_{x} p(x) log(\\frac{p(x)}{q(x)})\n \\tag{1}\n\\end{equation}\\]\nthe roles of \\(p\\) and \\(q\\) are fundamentally different. For one, the expectation is computed over the first distribution (\\(p\\) in (1)). This aspect is important because the “order” (the respective roles) of \\(p\\) and \\(q\\) may have to be chosen according to tractability (which distribution are we able to average over).\nSecondly, the fraction inside the \\(log\\) means that if \\(q\\) is ever zero at a point where \\(p\\) isn’t, the KL divergence will “blow up”. What this means for distribution estimation in general is nicely detailed in Murphy (2012). In the context of surprise, it means that if I learn something I used to think had probability zero, I will be “infinitely surprised”.\nTo avoid infinite surprise, we can make sure our prior probability is never zero. But even then, the interesting thing is that how much information we gain in any one event depends on how much information I had before. Let’s see a simple example.\nAssume that in my current understanding of the world, black swans probably don’t exist, but they could … maybe 1 percent of them is black. Put differently, my prior belief of a swan, should I encounter one, being black is \\(q = 0.01\\).\nNow in fact I do encounter one, and it’s black. The information I’ve gained is:\n\\[\\begin{equation}\nl(p,q) = 0 * log(\\frac{0}{0.99}) + 1 * log(\\frac{1}{0.01}) = 6.6 \\ bits\n \\tag{8}\n\\end{equation}\\]\nConversely, suppose I’d been much more undecided before; say I’d have thought the odds were 50:50. On seeing a black swan, I get a lot less information:\n\\[\\begin{equation}\nl(p,q) = 0 * log(\\frac{0}{0.5}) + 1 * log(\\frac{1}{0.5}) = 1 \\ bit\n \\tag{9}\n\\end{equation}\\]\nThis view of KL divergence, in terms of surprise and learning, is inspiring – it could lead one to seeing it in action everywhere. However, we still have the third and final task to handle: quickly compare KL divergence to other concepts in the area.\nRelated concepts\nEntropy\nIt all starts with entropy, or uncertainty, or information, as formulated by Claude Shannon. Entropy is the average log probability of a distribution:\n\\[\\begin{equation}\nH(X) = - \\sum\\limits_{x=1}^n p(x_i) log(p(x_i))\n \\tag{10}\n\\end{equation}\\]\nAs nicely described in (DeDeo 2016), this formulation was chosen to satisfy four criteria, one of which is what we commonly picture as its “essence”, and one of which is especially interesting.\nAs to the former, if there are \\(n\\) possible states, entropy is maximal when all states are equiprobable. E.g., for a coin flip uncertainty is highest when coin bias is 0.5.\nThe latter has to do with coarse-graining, a change in “resolution” of the state space. Say we have 16 possible states, but we don’t really care at that level of detail. We do care about 3 individual states, but all the rest are basically the same to us. Then entropy decomposes additively; total (fine-grained) entropy is the entropy of the coarse-grained space, plus the entropy of the “lumped-together” group, weighted by their probabilities.6\nSubjectively, entropy reflects our uncertainty whether an event will happen. Interestingly though, it exists in the physical world as well: For example, when ice melts, it becomes more uncertain where individual particles are. As reported by (DeDeo 2016), the number of bits released when one gram of ice melts is about 100 billion terabytes!\nAs fascinating as it is, information per se may, in many cases, not be the best means of characterizing human behavior. Going back to the eyetracking example, it is completely intuitive that people look at surprising parts of images, not at white noise areas, which are the maximum you could get in terms of entropy.\nAs a deep learning practitioner, you’ve probably been waiting for the point at which we’d mention cross entropy – the most commonly used loss function in categorization.\nCross entropy\nThe cross entropy between distributions \\(p\\) and \\(q\\) is the entropy of \\(p\\) plus the KL divergence of \\(p\\) relative to \\(q\\). If you’ve ever implemented your own classification network, you probably recognize the sum on the very right:\n\\[\\begin{equation}\nH(p,q) = H(p) + D_{KL}(p||q) = - \\sum p \\ log(q)\n \\tag{11}\n\\end{equation}\\]\nIn information theory-speak, \\(H(p,q)\\) is the expected message length per datum when \\(q\\) is assumed but \\(p\\) is true. Closer to the world of machine learning, for fixed \\(p\\), minimizing cross entropy is equivalent to minimizing KL divergence.\nMutual information\nAnother extremely important quantity, used in many contexts and applications, is mutual information. Again citing DeDeo, “you can think of it as the most general form of correlation coefficient that you can measure”.\nWith two variables \\(X\\) and \\(Y\\), we can ask: How much do we learn about \\(X\\) when we learn about an individual \\(y\\), \\(Y=y\\)? Averaged over all \\(y\\), this is the conditional entropy:\n\\[\\begin{equation}\nH(X|Y) = - \\sum\\limits_{i} P(y_i) log(H(X|y_i))\n \\tag{12}\n\\end{equation}\\]\nNow mutual information is entropy minus conditional entropy:\n\\[\\begin{equation}\nI(X, Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)\n \\tag{13}\n\\end{equation}\\]\nThis quantity – as required for a measure representing something like correlation – is symmetric: If two variables \\(X\\) and \\(Y\\) are related, the amount of information \\(X\\) gives you about \\(Y\\) is equal to that \\(Y\\) gives you about \\(X\\).\nKL divergence is part of a family of divergences, called f-divergences, used to measure directed difference between probability distributions. Let’s also quickly look another information-theoretic measure that unlike those, is a distance.\nJensen-Shannon distance\nIn math, a distance, or metric, besides being non-negative has to satisfy two other criteria: It must be symmetric, and it must obey the triangle inequality.\nBoth criteria are met by the Jensen-Shannon distance. With \\(m\\) a mixture distribution:\n\\[\\begin{equation}\nm_i = \\frac{1}{2}(p_i + q_i)\n \\tag{14}\n\\end{equation}\\]\nthe Jensen-Shannon distance is an average of KL divergences, one of \\(m\\) relative to \\(p\\), the other of \\(m\\) relative to \\(q\\):\n\\[\\begin{equation}\nJSD = \\frac{1}{2}(KL(m||p) + KL(m||q))\n \\tag{15}\n\\end{equation}\\]\nThis would be an ideal candidate to use were we interested in (undirected) distance between, not directed surprise caused by, distributions.\nFinally, let’s wrap up with a last term, restricting ourselves to a quick glimpse at something whole books could be written about.\n(Variational) Free Energy\nReading papers on variational inference, you’re pretty likely to hear people talking not “just” about KL divergence and/or the ELBO (which as soon as you know what it stands for, is just what it is), but also, something mysteriously called free energy (or: variational free energy, in that context).\nFor practical purposes, it suffices to know that variational free energy is negative the ELBO, that is, corresponds to equation (2). But for those interested, there is free energy as a central concept in thermodynamics.\nIn this post, we’re mainly interested in how concepts are related to KL divergence, and for this, we follow the characterization John Baez gives in his aforementioned talk.\nFree energy, that is, energy in useful form, is the expected energy minus temperature times entropy:\n\\[\\begin{equation}\nF = [E] -T \\ H\n \\tag{16}\n\\end{equation}\\]\nThen, the extra free energy of a system \\(Q\\) – compared to a system in equilibrium \\(P\\) – is proportional to their KL divergence, that is, the information of \\(Q\\) relative to \\(P\\):7\n\\[\\begin{equation}\nF(Q) - F(P) = k \\ T \\ KL(q||p)\n \\tag{17}\n\\end{equation}\\]\nSpeaking of free energy, there’s also the – not uncontroversial – free energy principle posited in neuroscience..8 But at some point, we have to stop, and we do it here.\nConclusion\nWrapping up, this post has tried to do three things: Having in mind a reader with background mainly in deep learning, start with the “habitual” use in training variational autoencoders; then show the – probably less familiar – “other side”; and finally, provide a synopsis of related terms and their applications.\nIf you’re interested in digging deeper into the many various applications, in a range of different fields, no better place to start than from the Twitter thread, mentioned above, that gave rise to this post. Thanks for reading!\n\n\nDeDeo, Simon. 2016. “Information Theory for Intelligent People.”\n\n\nFriston, Karl. 2010. “Friston, K.j.: the Free-Energy Principle: A Unified Brain Theory? Nat. Rev. Neurosci. 11, 127-138.” Nature Reviews. Neuroscience 11 (February): 127–38. https://doi.org/10.1038/nrn2787.\n\n\nItti, Laurent, and Pierre Baldi. 2005. “Bayesian Surprise Attracts Human Attention.” In Advances in Neural Information Processing Systems 18 [Neural Information Processing Systems, NIPS 2005, December 5-8, 2005, Vancouver, British Columbia, Canada], 547–54. http://papers.nips.cc/paper/2822-bayesian-surprise-attracts-human-attention.\n\n\nMurphy, Kevin. 2012. Machine Learning: A Probabilistic Perspective. MIT Press.\n\n\nSun, Yi, Faustino J. Gomez, and Juergen Schmidhuber. 2011. “Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments.” CoRR abs/1103.5708. http://arxiv.org/abs/1103.5708.\n\n\nZanardo, Enrico. 2017. “HOW to Measure Disagreement ?” In.\n\n\nSee Representation learning with MMD-VAE for an introduction.↩︎\nAs you probably guessed, these epitheta are not to be taken entirely seriously…↩︎\nJohn Baez, whom we cite below when discussing free energy.↩︎\nNot, by contrast, something to be maximized either. Rather, depending on the domain, there will probably be an “optimal” amount of KL divergence for the related behavior to ensue.↩︎\nWe discuss entropy in section 3.↩︎\nSee DeDeo (2016) for details.↩︎\nHere k is the Boltzmann constant.↩︎\nSee, e.g., (Friston 2010)↩︎\n",
    "preview": "posts/2020-02-19-kl-divergence/images/ultimatemachine.jpg",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-01-24-numpy-broadcasting/",
    "title": "NumPy-style broadcasting for R TensorFlow users",
    "description": "Broadcasting, as done by Python's scientific computing library NumPy, involves dynamically extending shapes so that arrays of different sizes may be passed to operations that expect conformity - such as adding or multiplying elementwise. In NumPy, the way broadcasting works is specified exactly; the same rules apply to TensorFlow operations. For anyone who finds herself, occasionally, consulting Python code, this post strives to explain.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-01-24",
    "categories": [
      "TensorFlow/Keras",
      "Concepts"
    ],
    "contents": "\nWe develop, train, and deploy TensorFlow models from R. But that doesn’t mean we don’t make use of documentation, blog posts, and examples written in Python. We look up specific functionality in the official TensorFlow API docs; we get inspiration from other people’s code.\nDepending on how comfortable you are with Python, there’s a problem. For example: You’re supposed to know how broadcasting works. And perhaps, you’d say you’re vaguely familiar with it: So when arrays have different shapes, some elements get duplicated until their shapes match and … and isn’t R vectorized anyway?\nWhile such a global notion may work in general, like when skimming a blog post, it’s not enough to understand, say, examples in the TensorFlow API docs. In this post, we’ll try to arrive at a more exact understanding, and check it on concrete examples.\nSpeaking of examples, here are two motivating ones.\nBroadcasting in action\nThe first uses TensorFlow’s matmul to multiply two tensors. Would you like to guess the result – not the numbers, but how it comes about in general? Does this even run without error – shouldn’t matrices be two-dimensional (rank-2 tensors, in TensorFlow speak)?\n\n\na <- tf$constant(keras::array_reshape(1:12, dim = c(2, 2, 3)))\na \n# tf.Tensor(\n# [[[ 1.  2.  3.]\n#   [ 4.  5.  6.]]\n# \n#  [[ 7.  8.  9.]\n#   [10. 11. 12.]]], shape=(2, 2, 3), dtype=float64)\n\nb <- tf$constant(keras::array_reshape(101:106, dim = c(1, 3, 2)))\nb  \n# tf.Tensor(\n# [[[101. 102.]\n#   [103. 104.]\n#   [105. 106.]]], shape=(1, 3, 2), dtype=float64)\n\nc <- tf$matmul(a, b)\n\nSecond, here is a “real example” from a TensorFlow Probability (TFP) github issue. (Translated to R, but keeping the semantics). In TFP, we can have batches of distributions. That, per se, is not surprising. But look at this:\n\n\nlibrary(tfprobability)\nd <- tfd_normal(loc = c(0, 1), scale = matrix(1.5:4.5, ncol = 2, byrow = TRUE))\nd\n# tfp.distributions.Normal(\"Normal\", batch_shape=[2, 2], event_shape=[], dtype=float64)\n\nWe create a batch of four normal distributions: each with a different scale (1.5, 2.5, 3.5, 4.5). But wait: there are only two location parameters given. So what are their scales, respectively? Thankfully, TFP developers Brian Patton and Chris Suter explained how it works: TFP actually does broadcasting – with distributions – just like with tensors!\nWe get back to both examples at the end of this post. Our main focus will be to explain broadcasting as done in NumPy, as NumPy-style broadcasting is what numerous other frameworks have adopted (e.g., TensorFlow).\nBefore though, let’s quickly review a few basics about NumPy arrays: How to index or slice them (indexing normally referring to single-element extraction, while slicing would yield – well – slices containing several elements); how to parse their shapes; some terminology and related background. Though not complicated per se, these are the kinds of things that can be confusing to infrequent Python users; yet they’re often a prerequisite to successfully making use of Python documentation.\nStated upfront, we’ll really restrict ourselves to the basics here; for example, we won’t touch advanced indexing which – just like lots more –, can be looked up in detail in the NumPy documentation.\nFew facts about NumPy\nBasic slicing\nFor simplicity, we’ll use the terms indexing and slicing more or less synonymously from now on. The basic device here is a slice, namely, a start:stop 1 structure indicating, for a single dimension, which range of elements to include in the selection.\nIn contrast to R, Python indexing is zero-based, and the end index is exclusive:\n\n\nimport numpy as np\nx = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\nx[1:7] \n# array([1, 2, 3, 4, 5, 6])\n\nMinus, to R users, is a false friend; it means we start counting from the end (the last element being -1):\n\n\nx[-2:10] \n# array([8, 9])\n\nLeaving out start (stop, resp.) selects all elements from the start (till the end). This may feel so convenient that Python users might miss it in R:\n\n\nx[5:] \n# array([5, 6, 7, 8, 9])\n\nx[:7]\n# array([0, 1, 2, 3, 4, 5, 6])\n\nJust to make a point about the syntax, we could leave out both the start and the stop indices, in this one-dimensional case effectively resulting in a no-op:\n\n\nx[:] \narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\nGoing on to two dimensions – without commenting on array creation just yet –, we can immediately apply the “semicolon trick” here too. This will select the second row with all its columns:\n\n\nx = np.array([[1, 2], [3, 4], [5, 6]])\nx\n# array([[1, 2],\n#        [3, 4],\n#        [5, 6]])\n\nx[1, :] \n# array([3, 4])\n\nWhile this, arguably, makes for the easiest way to achieve that result and thus, would be the way you’d write it yourself, it’s good to know that these are two alternative ways that do the same:\n\n\nx[1] \n# array([3, 4])\n\nx[1, ] \n# array([3, 4])\n\nWhile the second one sure looks a bit like R, the mechanism is different. Technically, these start:stop things are parts of a Python tuple – that list-like, but immutable data structure that can be written with or without parentheses, e.g., 1,2 or (1,2) –, and whenever we have more dimensions in the array than elements in the tuple NumPy will assume we meant : for that dimension: Just select everything.\nWe can see that moving on to three dimensions. Here is a 2 x 3 x 1-dimensional array:\n\n\nx = np.array([[[1],[2],[3]], [[4],[5],[6]]])\nx\n# array([[[1],\n#         [2],\n#         [3]],\n# \n#        [[4],\n#         [5],\n#         [6]]])\n\nx.shape\n# (2, 3, 1)\n\nIn R, this would throw an error, while in Python it works:\n\n\nx[0,]\n#array([[1],\n#       [2],\n#       [3]])\n\nIn such a case, for enhanced readability we could instead use the so-called Ellipsis, explicitly asking Python to “use up all dimensions required to make this work”:\n\n\nx[0, ...]\n#array([[1],\n#       [2],\n#       [3]])\n\nWe stop here with our selection of essential (yet confusing, possibly, to infrequent Python users) Numpy indexing features; re. “possibly confusing” though, here are a few remarks about array creation.\nSyntax for array creation\nCreating a more-dimensional NumPy array is not that hard – depending on how you do it. The trick is to use reshape to tell NumPy exactly what shape you want. For example, to create an array of all zeros, of dimensions 3 x 4 x 2:\n\n\nnp.zeros(24).reshape(4, 3, 2)\n\nBut we also want to understand what others might write. And then, you might see things like these:\n\n\nc1 = np.array([[[0, 0, 0]]])\nc2 = np.array([[[0], [0], [0]]]) \nc3 = np.array([[[0]], [[0]], [[0]]])\n\nThese are all 3-dimensional, and all have three elements, so their shapes must be 1 x 1 x 3, 1 x 3 x 1, and 3 x 1 x 1, in some order. Of course, shape is there to tell us:\n\n\nc1.shape # (1, 1, 3)\nc2.shape # (1, 3, 1)\nc3.shape # (3, 1, 1) \n\nbut we’d like to be able to “parse” internally without executing the code. One way to think about it would be processing the brackets like a state machine, every opening bracket moving one axis to the right and every closing bracket moving back left by one axis. Let us know if you can think of other – possibly more helpful – mnemonics!\nIn the very last sentence, we on purpose used “left” and “right” referring to the array axes; “out there” though, you’ll also hear “outmost” and “innermost”. Which, then, is which?\nA bit of terminology\nIn common Python (TensorFlow, for example) usage, when talking of an array shape like (2, 6, 7), outmost is left and innermost is right. Why? Let’s take a simpler, two-dimensional example of shape (2, 3).\n\n\na = np.array([[1, 2, 3], [4, 5, 6]])\na\n# array([[1, 2, 3],\n#        [4, 5, 6]])\n\nComputer memory is conceptually one-dimensional, a sequence of locations; so when we create arrays in a high-level programming language, their contents are effectively “flattened” into a vector. That flattening could occur “by row” (row-major, C-style, the default in NumPy), resulting in the above array ending up like this\n\n1 2 3 4 5 6\nor “by column” (column-major, Fortran-style, the ordering used in R), yielding\n\n1 4 2 5 3 6\n\nfor the above example.\nNow if we see “outmost” as the axis whose index varies the least often, and “innermost” as the one that changes most quickly, in row-major ordering the left axis is “outer”, and the right one is “inner”.\nJust as a (cool!) aside, NumPy arrays have an attribute called strides that stores how many bytes have to be traversed, for each axis, to arrive at its next element. For our above example:\n\n\nc1 = np.array([[[0, 0, 0]]])\nc1.shape   # (1, 1, 3)\nc1.strides # (24, 24, 8)\n\nc2 = np.array([[[0], [0], [0]]]) \nc2.shape   # (1, 3, 1)\nc2.strides # (24, 8, 8)\n\nc3 = np.array([[[0]], [[0]], [[0]]])\nc3.shape   # (3, 1, 1) \nc3.strides # (8, 8, 8)\n\nFor array c3, every element is on its own on the outmost level; so for axis 0, to jump from one element to the next, it’s just 8 bytes. For c2 and c1 though, everything is “squished” in the first element of axis 0 (there is just a single element there). So if we wanted to jump to another, nonexisting-as-yet, outmost item, it’d take us 3 * 8 = 24 bytes.\nAt this point, we’re ready to talk about broadcasting. We first stay with NumPy and then, examine some TensorFlow examples.\nNumPy Broadcasting\nWhat happens if we add a scalar to an array? This won’t be surprising for R users:\n\n\na = np.array([1,2,3])\nb = 1\na + b\n\n\narray([2, 3, 4])\nTechnically, this is already broadcasting in action; b is virtually (not physically!) expanded to shape (3,) in order to match the shape of a.\nHow about two arrays, one of shape (2, 3) – two rows, three columns –, the other one-dimensional, of shape (3,)?\n\n\na = np.array([1,2,3])\nb = np.array([[1,2,3], [4,5,6]])\na + b\n\n\narray([[2, 4, 6],\n       [5, 7, 9]])\nThe one-dimensional array gets added to both rows. If a were length-two instead, would it get added to every column?\n\n\na = np.array([1,2,3])\nb = np.array([[1,2,3], [4,5,6]])\na + b\n\n\nValueError: operands could not be broadcast together with shapes (2,) (2,3) \nSo now it is time for the broadcasting rule. For broadcasting (virtual expansion) to happen, the following is required.\nWe align array shapes, starting from the right.\n\n   # array 1, shape:     8  1  6  1\n   # array 2, shape:        7  1  5\nStarting to look from the right, the sizes along aligned axes either have to match exactly, or one of them has to be 1: In which case the latter is broadcast to the one not equal to 1.\nIf on the left, one of the arrays has an additional axis (or more than one), the other is virtually expanded to have a 1 in that place, in which case broadcasting will happen as stated in (2).\nStated like this, it probably sounds incredibly simple. Maybe it is, and it only seems complicated because it presupposes correct parsing of array shapes (which as shown above, can be confusing)?\nHere again is a quick example to test our understanding:\n\n\na = np.zeros([2, 3]) # shape (2, 3)\nb = np.zeros([2])    # shape (2,)\nc = np.zeros([3])    # shape (3,)\n\na + b # error\n\na + c\n# array([[0., 0., 0.],\n#        [0., 0., 0.]])\n\nAll in accord with the rules. Maybe there’s something else that makes it confusing? From linear algebra, we are used to thinking in terms of column vectors (often seen as the default) and row vectors (accordingly, seen as their transposes). What now is\n\n\nnp.array([0, 0])\n\n, of shape – as we’ve seen a few times by now – (2,)? Really it’s neither, it’s just some one-dimensional array structure. We can create row vectors and column vectors though, in the sense of 1 x n and n x 1 matrices, by explicitly adding a second axis. Any of these would create a column vector:\n\n\n# start with the above \"non-vector\"\nc = np.array([0, 0])\nc.shape\n# (2,)\n\n# way 1: reshape\nc.reshape(2, 1).shape\n# (2, 1)\n\n# np.newaxis inserts new axis\nc[ :, np.newaxis].shape\n# (2, 1)\n\n# None does the same\nc[ :, None].shape\n# (2, 1)\n\n# or construct directly as (2, 1), paying attention to the parentheses...\nc = np.array([[0], [0]])\nc.shape\n# (2, 1)\n\nAnd analogously for row vectors. Now these “more explicit”, to a human reader, shapes should make it easier to assess where broadcasting will work, and where it won’t.\n\n\nc = np.array([[0], [0]])\nc.shape\n# (2, 1)\n\na = np.zeros([2, 3])\na.shape\n# (2, 3)\na + c\n# array([[0., 0., 0.],\n#       [0., 0., 0.]])\n\na = np.zeros([3, 2])\na.shape\n# (3, 2)\na + c\n# ValueError: operands could not be broadcast together with shapes (3,2) (2,1) \n\nBefore we jump to TensorFlow, let’s see a simple practical application: computing an outer product.\n\n\na = np.array([0.0, 10.0, 20.0, 30.0])\na.shape\n# (4,)\n\nb = np.array([1.0, 2.0, 3.0])\nb.shape\n# (3,)\n\na[:, np.newaxis] * b\n# array([[ 0.,  0.,  0.],\n#        [10., 20., 30.],\n#        [20., 40., 60.],\n#        [30., 60., 90.]])\n\nTensorFlow\nIf by now, you’re feeling less than enthusiastic about hearing a detailed exposition of how TensorFlow broadcasting differs from NumPy’s, there is good news: Basically, the rules are the same. However, when matrix operations work on batches – as in the case of matmul and friends – , things may still get complicated; the best advice here probably is to carefully read the documentation (and as always, try things out).\nBefore revisiting our introductory matmul example, we quickly check that really, things work just like in NumPy. Thanks to the tensorflow R package, there is no reason to do this in Python; so at this point, we switch to R – attention, it’s 1-based indexing from here.\nFirst check – (4, 1) added to (4,) should yield (4, 4):\n\n\na <- tf$ones(shape = c(4L, 1L))\na\n# tf.Tensor(\n# [[1.]\n#  [1.]\n#  [1.]\n#  [1.]], shape=(4, 1), dtype=float32)\n\nb <- tf$constant(c(1, 2, 3, 4))\nb\n# tf.Tensor([1. 2. 3. 4.], shape=(4,), dtype=float32)\n\na + b\n# tf.Tensor(\n# [[2. 3. 4. 5.]\n# [2. 3. 4. 5.]\n# [2. 3. 4. 5.]\n# [2. 3. 4. 5.]], shape=(4, 4), dtype=float32)\n\nAnd second, when we add tensors with shapes (3, 3) and (3,), the 1-d tensor should get added to every row (not every column):\n\n\na <- tf$constant(matrix(1:9, ncol = 3, byrow = TRUE), dtype = tf$float32)\na\n# tf.Tensor(\n# [[1. 2. 3.]\n#  [4. 5. 6.]\n#  [7. 8. 9.]], shape=(3, 3), dtype=float32)\n\nb <- tf$constant(c(100, 200, 300))\nb\n# tf.Tensor([100. 200. 300.], shape=(3,), dtype=float32)\n\na + b\n# tf.Tensor(\n# [[101. 202. 303.]\n#  [104. 205. 306.]\n#  [107. 208. 309.]], shape=(3, 3), dtype=float32)\n\nNow back to the initial matmul example.\nBack to the puzzles\nThe documentation for matmul says,\n\nThe inputs must, following any transpositions, be tensors of rank >= 2 where the inner 2 dimensions specify valid matrix multiplication dimensions, and any further outer dimensions specify matching batch size.\n\nSo here (see code just below), the inner two dimensions look good – (2, 3) and (3, 2) – while the one (one and only, in this case) batch dimension shows mismatching values 2 and 1, respectively. A case for broadcasting thus: Both “batches” of a get matrix-multiplied with b.\n\n\na <- tf$constant(keras::array_reshape(1:12, dim = c(2, 2, 3)))\na \n# tf.Tensor(\n# [[[ 1.  2.  3.]\n#   [ 4.  5.  6.]]\n# \n#  [[ 7.  8.  9.]\n#   [10. 11. 12.]]], shape=(2, 2, 3), dtype=float64)\n\nb <- tf$constant(keras::array_reshape(101:106, dim = c(1, 3, 2)))\nb  \n# tf.Tensor(\n# [[[101. 102.]\n#   [103. 104.]\n#   [105. 106.]]], shape=(1, 3, 2), dtype=float64)\n\nc <- tf$matmul(a, b)\nc\n# tf.Tensor(\n# [[[ 622.  628.]\n#   [1549. 1564.]]\n# \n#  [[2476. 2500.]\n#   [3403. 3436.]]], shape=(2, 2, 2), dtype=float64) \n\nLet’s quickly check this really is what happens, by multiplying both batches separately:\n\n\ntf$matmul(a[1, , ], b)\n# tf.Tensor(\n# [[[ 622.  628.]\n#   [1549. 1564.]]], shape=(1, 2, 2), dtype=float64)\n\ntf$matmul(a[2, , ], b)\n# tf.Tensor(\n# [[[2476. 2500.]\n#   [3403. 3436.]]], shape=(1, 2, 2), dtype=float64)\n\nIs it too weird to be wondering if broadcasting would also happen for matrix dimensions? E.g., could we try matmuling tensors of shapes (2, 4, 1) and (2, 3, 1), where the 4 x 1 matrix would be broadcast to 4 x 3? – A quick test shows that no.\nTo see how really, when dealing with TensorFlow operations, it pays off overcoming one’s initial reluctance and actually consult the documentation, let’s try another one.\nIn the documentation for matvec, we are told:\n\nMultiplies matrix a by vector b, producing a * b. The matrix a must, following any transpositions, be a tensor of rank >= 2, with shape(a)[-1] == shape(b)[-1], and shape(a)[:-2] able to broadcast with shape(b)[:-1].\n\nIn our understanding, given input tensors of shapes (2, 2, 3) and (2, 3), matvec should perform two matrix-vector multiplications: once for each batch, as indexed by each input’s leftmost dimension. Let’s check this – so far, there is no broadcasting involved:\n\n\n# two matrices\na <- tf$constant(keras::array_reshape(1:12, dim = c(2, 2, 3)))\na\n# tf.Tensor(\n# [[[ 1.  2.  3.]\n#   [ 4.  5.  6.]]\n# \n#  [[ 7.  8.  9.]\n#   [10. 11. 12.]]], shape=(2, 2, 3), dtype=float64)\n\nb = tf$constant(keras::array_reshape(101:106, dim = c(2, 3)))\nb\n# tf.Tensor(\n# [[101. 102. 103.]\n#  [104. 105. 106.]], shape=(2, 3), dtype=float64)\n\nc <- tf$linalg$matvec(a, b)\nc\n# tf.Tensor(\n# [[ 614. 1532.]\n#  [2522. 3467.]], shape=(2, 2), dtype=float64)\n\nDoublechecking, we manually multiply the corresponding matrices and vectors, and get:\n\n\ntf$linalg$matvec(a[1,  , ], b[1, ])\n# tf.Tensor([ 614. 1532.], shape=(2,), dtype=float64)\n\ntf$linalg$matvec(a[2,  , ], b[2, ])\n# tf.Tensor([2522. 3467.], shape=(2,), dtype=float64)\n\nThe same. Now, will we see broadcasting if b has just a single batch?\n\n\nb = tf$constant(keras::array_reshape(101:103, dim = c(1, 3)))\nb\n# tf.Tensor([[101. 102. 103.]], shape=(1, 3), dtype=float64)\n\nc <- tf$linalg$matvec(a, b)\nc\n# tf.Tensor(\n# [[ 614. 1532.]\n#  [2450. 3368.]], shape=(2, 2), dtype=float64)\n\nMultiplying every batch of a with b, for comparison:\n\n\ntf$linalg$matvec(a[1,  , ], b)\n# tf.Tensor([ 614. 1532.], shape=(2,), dtype=float64)\n\ntf$linalg$matvec(a[2,  , ], b)\n# tf.Tensor([[2450. 3368.]], shape=(1, 2), dtype=float64)\n\nIt worked!\nNow, on to the other motivating example, using tfprobability.\nBroadcasting everywhere\nHere again is the setup:\n\n\nlibrary(tfprobability)\nd <- tfd_normal(loc = c(0, 1), scale = matrix(1.5:4.5, ncol = 2, byrow = TRUE))\nd\n# tfp.distributions.Normal(\"Normal\", batch_shape=[2, 2], event_shape=[], dtype=float64)\n\nWhat is going on? Let’s inspect location and scale separately:\n\n\nd$loc\n# tf.Tensor([0. 1.], shape=(2,), dtype=float64)\n\nd$scale\n# tf.Tensor(\n# [[1.5 2.5]\n#  [3.5 4.5]], shape=(2, 2), dtype=float64)\n\nJust focusing on these tensors and their shapes, and having been told that there’s broadcasting going on, we can reason like this: Aligning both shapes on the right and extending loc’s shape by 1 (on the left), we have (1, 2) which may be broadcast with (2,2) - in matrix-speak, loc is treated as a row and duplicated.\nMeaning: We have two distributions with mean \\(0\\) (one of scale \\(1.5\\), the other of scale \\(3.5\\)), and also two with mean \\(1\\) (corresponding scales being \\(2.5\\) and \\(4.5\\)).\nHere’s a more direct way to see this:\n\n\nd$mean()\n# tf.Tensor(\n# [[0. 1.]\n#  [0. 1.]], shape=(2, 2), dtype=float64)\n\nd$stddev()\n# tf.Tensor(\n# [[1.5 2.5]\n#  [3.5 4.5]], shape=(2, 2), dtype=float64)\n\nPuzzle solved!\nSumming up, broadcasting is simple “in theory” (its rules are), but may need some practicing to get it right. Especially in conjunction with the fact that functions / operators do have their own views on which parts of its inputs should broadcast, and which shouldn’t. Really, there is no way around looking up the actual behaviors in the documentation.\nHopefully though, you’ve found this post to be a good start into the topic. Maybe, like the author, you feel like you might see broadcasting going on anywhere in the world now. Thanks for reading!\nor start:stop:step, if applicable↩︎\n",
    "preview": "posts/2020-01-24-numpy-broadcasting/images/thumb.jpg",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-01-13-mixed-precision-training/",
    "title": "First experiments with TensorFlow mixed-precision training",
    "description": "TensorFlow 2.1, released last week, allows for mixed-precision training, making use of the Tensor Cores available in the most recent NVidia GPUs. In this post, we report first experimental results and provide some background on what this is all about.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-01-13",
    "categories": [
      "TensorFlow/Keras"
    ],
    "contents": "\nStarting from its - very - recent 2.1 release, TensorFlow supports what is called mixed-precision training (in the following: MPT) for Keras. In this post, we experiment with MPT and provide some background. Stated upfront: On a Tesla V100 GPU, our CNN-based experiment did not reveal substantial reductions in execution time. In a case like this, it is hard to decide whether to actually write a post or not. You could argue that just like in science, null results are results. Or, more practically: They open up a discussion that may lead to bug discovery, clarification of usage instructions, and further experimentation, among others.\nIn addition, the topic itself is interesting enough to deserve some background explanations – even if the results are not quite there yet.\nSo to start, let’s hear some context on MPT.\nThis is not just about saving memory\nOne way to describe MPT in TensorFlow could go like this: MPT lets you train models where the weights are of type float32 or float64, as usual (for reasons of numeric stability), but the data – the tensors pushed between operations – have lower precision, namely, 16bit (float16).\nThis sentence would probably do fine as a TLDR; 1 for the new-ish MPT documentation page, also available for R on the TensorFlow for R website. And based on this sentence, you might be lead to think “oh sure, so this is about saving memory”. Less memory usage would then imply you could run larger batch sizes without getting out-of-memory errors.\nThis is of course correct, and you’ll see it happening in the experimentation results. But it’s only part of the story. The other part is related to GPU architecture and parallel (not just parallel on-GPU, as we’ll see) computing.\nAVX & co.\nGPUs are all about parallelization. But for CPUs as well, the last ten years have seen important developments in architecture and instruction sets. SIMD (Single Instruction Multiple Data) operations perform one instruction over a bunch of data at once. For example, two 128-bit operands could hold two 64-bit integers each, and these could be added pairwise. Conceptually, this reminds of vector addition in R (it’s just an analogue though!):\n\n\n# picture these as 64-bit integers\nc(1, 2) + c(3, 4)\n\nOr, those operands could contain four 32-bit integers each, in which case we could symbolically write\n\n\n# picture these as 32-bit integers\nc(1, 2, 3, 4) + c(5, 6, 7, 8)\n\nWith 16-bit integers, we could again double the number of elements operated upon:\n\n\n# picture these as 16-bit integers\nc(1, 2, 3, 4, 5, 6, 7, 8) + c(9, 10, 11, 12, 13, 14, 15, 16)\n\nOver the last decade, the major SIMD-related X-86 assembly language extensions have been AVX (Advanced Vector Extensions), AVX2, AVX-512, and FMA (more on FMA soon). Do any of these ring a bell?\n\nYour CPU supports instructions that this TensorFlow binary was not compiled to use:\nAVX2 FMA\nThis is a line you are likely to see if you are using a pre-built TensorFlow binary, as opposed to compiling from source. (Later, when reporting experimentation results, we will also indicate on-CPU execution times, to provide some context for the GPU execution times we’re interested in – and just for fun, we’ll also do a – very superficial – comparison between a TensorFlow binary installed from PyPi and one that was compiled manually.)\nWhile all those AVXes are (basically) about an extension of vector processing to larger and larger data types, FMA is different, and it’s an interesting thing to know about in itself – for anyone doing signal processing or using neural networks.\nFused Multiply-Add (FMA)\nFused Multiply-Add is a type of multiply-accumulate operation. In multiply-accumulate, operands are multiplied and then added to accumulator keeping track of the running sum. If “fused”, the whole multiply-then-add operation is performed with a single rounding at the end (as opposed to rounding once after the multiplication, and then again after the addition). Usually, this results in higher accuracy.\nFor CPUs, FMA was introduced concurrently with AVX2. FMA can be performed on scalars or on vectors, “packed” in the way described in the previous paragraph.\nWhy did we say this was so interesting to data scientists? Well, a lot of operations – dot products, matrix multiplications, convolutions – involve multiplications followed by additions. “Matrix multiplication” here actually has us leave the realm of CPUs and jump to GPUs instead, because what MPT does is make use of the new-ish NVidia Tensor Cores that extend FMA from scalars/vectors to matrices.\nTensor Cores\nAs documented, MPT requires GPUs with compute capability >= 7.0. The respective GPUs, in addition to the usual Cuda Cores, have so called “Tensor Cores” that perform FMA on matrices:\n\n\n\nFigure 1: Source: https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf.\n\n\n\nThe operation takes place on 4x4 matrices; multiplications happen on 16-bit operands while the final result could be 16-bit or 32-bit.\n\n\n\nFigure 2: Source: https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf\n\n\n\nWe can see how this is immediately relevant to the operations involved in deep learning; the details, however, are not necessarily clear.\nLeaving those internals to the experts, we now proceed to the actual experiment.\nExperiments\nDataset\nWith their 28x28px / 32x32px sized images, neither MNIST nor CIFAR seemed particularly suited to challenge the GPU. Instead, we chose Imagenette, the “little ImageNet” created by the fast.ai folks, consisting of 10 classes: tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, and parachute. 2 Here are a few examples, taken from the 320px version:\n\n\n\nFigure 3: Examples of the 10 classes of Imagenette.\n\n\n\nThese images have been resized - keeping the aspect ratio - such that the larger dimension has length 320px. As part of preprocessing, we’ll further resize to 256x256px, to work with a nice power of 2. 3\nThe dataset may conveniently be obtained via using tfds, the R interface to TensorFlow Datasets.\n\n\nlibrary(keras)\n# needs version 2.1\nlibrary(tensorflow)\nlibrary(tfdatasets)\n# available from github: devtools::install_github(\"rstudio/tfds\")\nlibrary(tfds)\n\n# to use TensorFlow Datasets, we need the Python backend\n# normally, just use tfds::install_tfds for this\n# as of this writing though, we need a nightly build of TensorFlow Datasets\n# envname should refer to whatever environment you run TensorFlow in\nreticulate::py_install(\"tfds-nightly\", envname = \"r-reticulate\") \n\n# on first execution, this downloads the dataset\nimagenette <- tfds_load(\"imagenette/320px\")\n\n# extract train and test parts\ntrain <- imagenette$train\ntest <- imagenette$validation\n\n# batch size for the initial run\nbatch_size <- 32\n# 12895 is the number of items in the training set\nbuffer_size <- 12895/batch_size\n\n# training dataset is resized, scaled to between 0 and 1,\n# cached, shuffled, and divided into batches\ntrain_dataset <- train %>%\n  dataset_map(function(record) {\n    record$image <- record$image %>%\n      tf$image$resize(size = c(256L, 256L)) %>%\n      tf$truediv(255)\n    record\n  }) %>%\n  dataset_cache() %>%\n  dataset_shuffle(buffer_size) %>%\n  dataset_batch(batch_size) %>%\n  dataset_map(unname)\n\n# test dataset is resized, scaled to between 0 and 1, and divided into batches\ntest_dataset <- test %>% \n  dataset_map(function(record) {\n    record$image <- record$image %>% \n      tf$image$resize(size = c(256L, 256L)) %>%\n      tf$truediv(255)\n    record}) %>%\n  dataset_batch(batch_size) %>% \n  dataset_map(unname)\n\nIn the above code, we cache the dataset after the resize and scale operations, as we want to minimize preprocessing time spent on the CPU.\nConfiguring MPT\nOur experiment uses Keras fit – as opposed to a custom training loop –, and given these preconditions, running MPT is mostly a matter of adding three lines of code. (There is a small change to the model, as we’ll see in a moment.)4\nWe tell Keras to use the mixed_float16 Policy, and verify that the tensors have type float16 while the Variables (weights) still are of type float32:\n\n\n# if you read this at a later time and get an error here,\n# check out whether the location in the codebase has changed\nmixed_precision <- tf$keras$mixed_precision$experimental\n\npolicy <- mixed_precision$Policy('mixed_float16')\nmixed_precision$set_policy(policy)\n\n# float16\npolicy$compute_dtype\n# float32\npolicy$variable_dtype\n\nThe model is a straightforward convnet, with numbers of filters being multiples of 8, as specified in the documentation. There is one thing to note though: For reasons of numerical stability, the actual output tensor of the model should be of type float32.\n\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 32, kernel_size = 5, strides = 2, padding = \"same\", input_shape = c(256, 256, 3), activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_conv_2d(filters = 64, kernel_size = 7, strides = 2, padding = \"same\", activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_conv_2d(filters = 128, kernel_size = 11, strides = 2, padding = \"same\", activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_global_average_pooling_2d() %>%\n  # separate logits from activations so actual outputs can be float32\n  layer_dense(units = 10) %>%\n  layer_activation(\"softmax\", dtype = \"float32\")\n\nmodel %>% compile(\n  loss = \"sparse_categorical_crossentropy\",\n  optimizer = \"adam\",\n  metrics = \"accuracy\")\n\nmodel %>% \n  fit(train_dataset, validation_data = test_dataset, epochs = 20)\n\nResults\nThe main experiment was done on a Tesla V100 with 16G of memory. Just for curiosity, we ran that same model under four other conditions, none of which fulfill the prerequisite of having a compute capability equal to at least 7.0. We’ll quickly mention those after the main results.\nWith the above model, final accuracy (final as in: after 20 epochs) fluctuated about 0.78: 5\n\nEpoch 16/20\n403/403 [==============================] - 12s 29ms/step - loss: 0.3365 -\naccuracy: 0.8982 - val_loss: 0.7325 - val_accuracy: 0.8060\nEpoch 17/20\n403/403 [==============================] - 12s 29ms/step - loss: 0.3051 -\naccuracy: 0.9084 - val_loss: 0.6683 - val_accuracy: 0.7820\nEpoch 18/20\n403/403 [==============================] - 11s 28ms/step - loss: 0.2693 -\naccuracy: 0.9208 - val_loss: 0.8588 - val_accuracy: 0.7840\nEpoch 19/20\n403/403 [==============================] - 11s 28ms/step - loss: 0.2274 -\naccuracy: 0.9358 - val_loss: 0.8692 - val_accuracy: 0.7700\nEpoch 20/20\n403/403 [==============================] - 11s 28ms/step - loss: 0.2082 -\naccuracy: 0.9410 - val_loss: 0.8473 - val_accuracy: 0.7460\nThe numbers reported below are milliseconds per step, step being a pass over a single batch. Thus in general, doubling the batch size we would expect execution time to double as well.\nHere are execution times, taken from epoch 20, for five different batch sizes, comparing MPT with a default Policy that uses float32 throughout. (We should add that apart from the very first epoch, execution times per step fluctuated by at most one millisecond in every condition.)\nBatch size\nms/step, MPT\nms/step, f32\n32\n28\n30\n64\n52\n56\n128\n97\n106\n256\n188\n206\n512\n377\n415\nConsistently, MPT was faster, indicating that the intended code path was used. But the speedup is not that big.\nWe also watched GPU utilization during the runs. These ranged from around 72% for batch_size 32 over ~ 78% for batch_size 128 to hightly fluctuating values, repeatedly reaching 100%, for batch_size 512.\nAs alluded to above, just to anchor these values we ran the same model in four other conditions, where no speedup was to be expected. Even though these execution times are not strictly part of the experiments, we report them, in case the reader is as curious about some context as we were.\nFirstly, here is the equivalent table for a Titan XP with 12G of memory and compute capability 6.1.\nBatch size\nms/step, MPT\nms/step, f32\n32\n44\n38\n64\n70\n70\n128\n142\n136\n256\n270\n270\n512\n518\n539\nAs expected, there is no consistent superiority of MPT; as an aside, looking at the values overall (especially as compared to CPU execution times to come!) you might conclude that luckily, one doesn’t always need the latest and greatest GPU to train neural networks!\nNext, we take one further step down the hardware ladder. Here are execution times from a Quadro M2200 (4G, compute capability 5.2). (The three runs that don’t have a number crashed with out of memory.)\nBatch size\nms/step, MPT\nms/step, f32\n32\n186\n197\n64\n352\n375\n128\n687\n746\n256\n1000\n-\n512\n-\n-\nThis time, we actually see how the pure memory-usage aspect plays a role: With MPT, we can run batches of size 256; without, we get an out-of-memory error.\nNow, we also compared with runtime on CPU (Intel Core I7, clock speed 2.9Ghz). To be honest, we stopped after a single epoch though. With a batch_size of 32 and running a standard pre-built installation of TensorFlow, a single step now took 321 - not milliseconds, but seconds. Just for fun, we compared to a manually built TensorFlow that can make use of AVX2 and FMA instructions (this topic might in fact deserve a dedicated experiment): Execution time per step was reduced to 304 seconds/step.\nConclusion\nSumming up, our experiment did not show important reductions in execution times – for reasons as yet unclear. We’d be happy to encourage a discussion in the comments!\nExperimental results notwithstanding, we hope you’ve enjoyed getting some background information on a not-too-frequently discussed topic. Thanks for reading!\nEvidently, TLDR; has been inserted inside some chunk of text here in order to confuse any future GPT-2s.↩︎\nWe do hope usage is allowed even in case we can’t produce the required “corny inauthentic French accent”.↩︎\nAs per the documentation, the number of filters in a convolutional layer should be a multiple of 8; however, taking this additional measure couldn’t possibly hurt.↩︎\nWith custom training loops, losses should be scaled (multiplied by a large number) before being passed into the gradient calculation, to avoid numerical underflow/overflow. For detailed instructions, see the documentation.↩︎\nExample output from run with batch_size 32 and MPT.↩︎\n",
    "preview": "posts/2020-01-13-mixed-precision-training/images/tc.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 589,
    "preview_height": 399
  },
  {
    "path": "posts/2019-12-20-differential-privacy/",
    "title": "Differential Privacy with TensorFlow",
    "description": "Differential Privacy guarantees that results of a database query are basically independent of the presence in the data of a single individual. Applied to machine learning, we expect that no single training example influences the parameters of the trained model in a substantial way. This post introduces TensorFlow Privacy, a library built on top of TensorFlow, that can be used to train differentially private deep learning models from R.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-12-20",
    "categories": [
      "Privacy & Security",
      "TensorFlow/Keras",
      "Time Series"
    ],
    "contents": "\nWhat could be treacherous about summary statistics?\nThe famous cat overweight study (X. et al., 2019) showed that as of May 1st, 2019, 32 of 101 domestic cats held in Y., a cozy Bavarian village, were overweight. Even though I’d be curious to know if my aunt G.’s cat (a happy resident of that village) has been fed too many treats and has accumulated some excess pounds, the study results don’t tell.\nThen, six months later, out comes a new study, ambitious to earn scientific fame. The authors report that of 100 cats living in Y., 50 are striped, 31 are black, and the rest are white; the 31 black ones are all overweight. Now, I happen to know that, with one exception, no new cats joined the community, and no cats left. But, my aunt moved away to a retirement home, chosen of course for the possibility to bring one’s cat.\nWhat have I just learned? My aunt’s cat is overweight. (Or was, at least, before they moved to the retirement home.)\nEven though none of the studies reported anything but summary statistics, I was able to infer individual-level facts by connecting both studies and adding in another piece of information I had access to.\nIn reality, mechanisms like the above – technically called linkage – have been shown to lead to privacy breaches many times, thus defeating the purpose of database anonymization seen as a panacea in many organizations. A more promising alternative is offered by the concept of differential privacy.\nDifferential Privacy\nIn differential privacy (DP)1(Dwork et al. 2006), privacy is not a property of what’s in the database; it’s a property of how query results are delivered.\nIntuitively paraphrasing results from a domain where results are communicated as theorems and proofs (Dwork 2006)(Dwork and Roth 2014), the only achievable (in a lossy but quantifiable way) objective is that from queries to a database, nothing more should be learned about an individual in that database than if they hadn’t been in there at all.(Wood et al. 2018)\nWhat this statement does is caution against overly high expectations: Even if query results are reported in a DP way (we’ll see how that goes in a second), they enable some probabilistic inferences about individuals in the respective population. (Otherwise, why conduct studies at all.)\nSo how is DP being achieved? The main ingredient is noise added to the results of a query. In the above cat example, instead of exact numbers we’d report approximate ones: “Of ~ 100 cats living in Y, about 30 are overweight…”. If this is done for both of the above studies, no inference will be possible about aunt G.’s cat.\nEven with random noise added to query results though, answers to repeated queries will leak information. So in reality, there is a privacy budget that can be tracked, and may be used up in the course of consecutive queries.\nThis is reflected in the formal definition of DP. The idea is that queries to two databases differing in at most one element should give basically the same result. Put formally (Dwork 2006):\n\nA randomized function \\(\\mathcal{K}\\) gives \\(\\epsilon\\) -differential privacy if for all data sets D1 and D2 differing on at most one element, and all \\(S \\subseteq Range(K)\\),\n\n\n\\(Pr[\\mathcal{K}(D1)\\in S] \\leq exp(\\epsilon) × Pr[K(D2) \\in S]\\)\n\nThis \\(\\epsilon\\) -differential privacy is additive: If one query is \\(\\epsilon\\)-DP at a value of 0.01, and another one at 0.03, together they will be 0.04 \\(\\epsilon\\)-differentially private.\nIf \\(\\epsilon\\)-DP is to be achieved via adding noise, how exactly should this be done? Here, several mechanisms exist; the basic, intuitively plausible principle though is that the amount of noise should be calibrated to the target function’s sensitivity, defined as the maximum \\(\\ell 1\\) norm of the difference of function values computed on all pairs of datasets differing in a single example (Dwork 2006):\n\n\\(\\Delta f = \\max_{D1,D2} {\\| f(D1)−f(D2) \\|}_1\\)\n\nSo far, we’ve been talking about databases and datasets. How does this apply to machine and/or deep learning?\nTensorFlow Privacy\nApplying DP to deep learning, we want a model’s parameters to wind up “essentially the same” whether trained on a dataset including that cute little kitty or not. TensorFlow (TF) Privacy (Abadi et al. 2016), a library built on top of TF, makes it easy on users to add privacy guarantees to their models – easy, that is, from a technical point of view. (As with life overall, the hard decisions on how much of an asset we should be reaching for, and how to trade off one asset (here: privacy) with another (here: model performance), remain to be taken by each of us ourselves.)\nConcretely, about all we have to do is exchange the optimizer we were using against one provided by TF Privacy.2 TF Privacy optimizers wrap the original TF ones, adding two actions:\nTo honor the principle that each individual training example should have just moderate influence on optimization, gradients are clipped (to a degree specifiable by the user). In contrast to the familiar gradient clipping sometimes used to prevent exploding gradients, what is clipped here is gradient contribution per user.\nBefore updating the parameters, noise is added to the gradients, thus implementing the main idea of \\(\\epsilon\\)-DP algorithms.\nIn addition to \\(\\epsilon\\)-DP optimization, TF Privacy provides privacy accounting. We’ll see all this applied after an introduction to our example dataset.\nDataset\nThe dataset we’ll be working with(Reiss et al. 2019), downloadable from the UCI Machine Learning Repository, is dedicated to heart rate estimation via photoplethysmography. Photoplethysmography (PPG) is an optical method of measuring blood volume changes in the microvascular bed of tissue, which are indicative of cardiovascular activity. More precisely,\n\nThe PPG waveform comprises a pulsatile (‘AC’) physiological waveform attributed to cardiac synchronous changes in the blood volume with each heart beat, and is superimposed on a slowly varying (‘DC’) baseline with various lower frequency components attributed to respiration, sympathetic nervous system activity and thermoregulation. (Allen 2007)\n\nIn this dataset, heart rate determined from EKG provides the ground truth; predictors were obtained from two commercial devices, comprising PPG, electrodermal activity, body temperature as well as accelerometer data. Additionally, a wealth of contextual data is available, ranging from age, height, and weight to fitness level and type of activity performed.\nWith this data, it’s easy to imagine a bunch of interesting data-analysis questions; however here our focus is on differential privacy, so we’ll keep the setup simple. We will try to predict heart rate given the physiological measurements from one of the two devices, Empatica E4. Also, we’ll zoom in on a single subject, S1, who will provide us with 4603 instances of two-second heart rate values.3\nAs usual, we start with the required libraries; unusually though, as of this writing we need to disable version 2 behavior in TensorFlow, as TensorFlow Privacy does not yet fully work with TF 2. (Hopefully, for many future readers, this won’t be the case anymore.) Note how TF Privacy – a Python library – is imported via reticulate.\n\n\nlibrary(tensorflow)\ntf$compat$v1$disable_v2_behavior()\n\nlibrary(keras)\nlibrary(tfdatasets)\nlibrary(tfautograph)\n\nlibrary(purrr)\n\nlibrary(reticulate)\n# if you haven't yet, install TF Privacy, e.g. using reticulate:\n# py_install(\"tensorflow_privacy\")\npriv <- import(\"tensorflow_privacy\")\n\nFrom the downloaded archive, we just need S1.pkl, saved in a native Python serialization format, yet nicely loadable using reticulate:\n\n\ns1 <- py_load_object(\"PPG_FieldStudy/S1/S1.pkl\", encoding = \"latin1\")\n\ns1 points to an R list comprising elements of different length – the various physical/physiological signals have been sampled with different frequencies:\n\n\n### predictors ###\n\n# accelerometer data - sampling freq. 32 Hz\n# also note that these are 3 \"columns\", for each of x, y, and z axes\ns1$signal$wrist$ACC %>% nrow() # 294784\n# PPG data - sampling freq. 64 Hz\ns1$signal$wrist$BVP %>% nrow() # 589568\n# electrodermal activity data - sampling freq. 4 Hz\ns1$signal$wrist$EDA %>% nrow() # 36848\n# body temperature data - sampling freq. 4 Hz\ns1$signal$wrist$TEMP %>% nrow() # 36848\n\n### target ###\n\n# EKG data - provided in already averaged form, at frequency 0.5 Hz\ns1$label %>% nrow() # 4603\n\nIn light of the different sampling frequencies, our tfdatasets pipeline will have do some moving averaging, paralleling that applied to construct the ground truth data.\nPreprocessing pipeline\nAs every “column”4 is of different length and resolution, we build up the final dataset piece-by-piece. The following function serves two purposes:\ncompute running averages over differently sized windows, thus downsampling to 0.5Hz for every modality\ntransform the data to the (num_timesteps, num_features) format that will be required by the 1d-convnet we’re going to use soon\n\n\naverage_and_make_sequences <-\n  function(data, window_size_avg, num_timesteps) {\n    data %>% k_cast(\"float32\") %>%\n      # create an initial tf.data dataset to work with\n      tensor_slices_dataset() %>%\n      # use dataset_window to compute the running average of size window_size_avg\n      dataset_window(window_size_avg) %>%\n      dataset_flat_map(function (x)\n        x$batch(as.integer(window_size_avg), drop_remainder = TRUE)) %>%\n      dataset_map(function(x)\n        tf$reduce_mean(x, axis = 0L)) %>%\n      # use dataset_window to create a \"timesteps\" dimension with length num_timesteps)\n      dataset_window(num_timesteps, shift = 1) %>%\n      dataset_flat_map(function(x)\n        x$batch(as.integer(num_timesteps), drop_remainder = TRUE))\n  }\n\nWe’ll call this function for every column separately. Not all columns are exactly the same length (in terms of time), thus it’s safest to cut off individual observations that surpass a common length (dictated by the target variable):\n\n\nlabel <- s1$label %>% matrix() # 4603 observations, each spanning 2 secs\nn_total <- 4603 # keep track of this\n\n# keep matching numbers of observations of predictors\nacc <- s1$signal$wrist$ACC[1:(n_total * 64), ] # 32 Hz, 3 columns\nbvp <- s1$signal$wrist$BVP[1:(n_total * 128)] %>% matrix() # 64 Hz\neda <- s1$signal$wrist$EDA[1:(n_total * 8)] %>% matrix() # 4 Hz\ntemp <- s1$signal$wrist$TEMP[1:(n_total * 8)] %>% matrix() # 4 Hz\n\nSome more housekeeping. Both training and the test set need to have a timesteps dimension, as usual with architectures that work on sequential data (1-d convnets and RNNs). To make sure there is no overlap between respective timesteps, we split the data “up front” and assemble both sets separately. We’ll use the first 4000 observations for training.\nHousekeeping-wise, we also keep track of actual training and test set cardinalities. The target variable will be matched to the last of any twelve timesteps, so we end up throwing away the first eleven ground truth measurements for each of the training and test datasets. (We don’t have complete sequences building up to them.)\n\n\n# number of timesteps used in the second dimension\nnum_timesteps <- 12\n\n# number of observations to be used for the training set\n# a round number for easier checking!\ntrain_max <- 4000\n\n# also keep track of actual number of training and test observations\nn_train <- train_max - num_timesteps + 1\nn_test <- n_total - train_max - num_timesteps + 1\n\nHere, then, are the basic building blocks that will go into the final training and test datasets.\n\n\nacc_train <-\n  average_and_make_sequences(acc[1:(train_max * 64), ], 64, num_timesteps)\nbvp_train <-\n  average_and_make_sequences(bvp[1:(train_max * 128), , drop = FALSE], 128, num_timesteps)\neda_train <-\n  average_and_make_sequences(eda[1:(train_max * 8), , drop = FALSE], 8, num_timesteps)\ntemp_train <-\n  average_and_make_sequences(temp[1:(train_max * 8), , drop = FALSE], 8, num_timesteps)\n\n\nacc_test <-\n  average_and_make_sequences(acc[(train_max * 64 + 1):nrow(acc), ], 64, num_timesteps)\nbvp_test <-\n  average_and_make_sequences(bvp[(train_max * 128 + 1):nrow(bvp), , drop = FALSE], 128, num_timesteps)\neda_test <-\n  average_and_make_sequences(eda[(train_max * 8 + 1):nrow(eda), , drop = FALSE], 8, num_timesteps)\ntemp_test <-\n  average_and_make_sequences(temp[(train_max * 8 + 1):nrow(temp), , drop = FALSE], 8, num_timesteps)\n\nNow put all predictors together:\n\n\n# all predictors\nx_train <- zip_datasets(acc_train, bvp_train, eda_train, temp_train) %>%\n  dataset_map(function(...)\n    tf$concat(list(...), axis = 1L))\n\nx_test <- zip_datasets(acc_test, bvp_test, eda_test, temp_test) %>%\n  dataset_map(function(...)\n    tf$concat(list(...), axis = 1L))\n\nOn the ground truth side, as alluded to before, we leave out the first eleven values in each case:\n\n\ny_train <- tensor_slices_dataset(label[num_timesteps:train_max] %>% k_cast(\"float32\"))\n\ny_test <- tensor_slices_dataset(label[(train_max + num_timesteps):nrow(label)] %>% k_cast(\"float32\")\n\nZip predictors and targets together, configure shuffling/batching, and the datasets are complete:\n\n\nds_train <- zip_datasets(x_train, y_train)\nds_test <- zip_datasets(x_test, y_test)\n\nbatch_size <- 32\n\nds_train <- ds_train %>% \n  dataset_shuffle(n_train) %>%\n  # dataset_repeat is needed because of pre-TF 2 style\n  # hopefully at a later time, the code can run eagerly and this is no longer needed\n  dataset_repeat() %>%\n  dataset_batch(batch_size, drop_remainder = TRUE)\n\nds_test <- ds_test %>%\n  # see above reg. dataset_repeat\n  dataset_repeat() %>%\n  dataset_batch(batch_size)\n\nWith data manipulations as complicated as the above, it’s always worthwhile checking some pipeline outputs. We can do that using the usual reticulate::as_iterator magic, provided that for this test run, we don’t disable V2 behavior. (Just restart the R session between a “pipeline checking” and the later modeling runs.)\nHere, in any case, would be the relevant code:\n\n\n# this piece needs TF 2 behavior enabled\n# run after restarting R and commenting the tf$compat$v1$disable_v2_behavior() line\n# then to fit the DP model, undo comment, restart R and rerun\niter <- as_iterator(ds_test) # or any other dataset you want to check\nwhile (TRUE) {\n item <- iter_next(iter)\n if (is.null(item)) break\n print(item)\n}\n\nWith that we’re ready to create the model.\nModel\nThe model will be a rather simple convnet. The main difference between standard and DP training lies in the optimization procedure; thus, it’s straightforward to first establish a non-DP baseline. Later, when switching to DP, we’ll be able to reuse almost everything.\nHere, then, is the model definition valid for both cases:\n\n\nmodel <- keras_model_sequential() %>%\n  layer_conv_1d(\n      filters = 32,\n      kernel_size = 3,\n      activation = \"relu\"\n    ) %>%\n  layer_batch_normalization() %>%\n  layer_conv_1d(\n      filters = 64,\n      kernel_size = 5,\n      activation = \"relu\"\n    ) %>%\n  layer_batch_normalization() %>%\n  layer_conv_1d(\n      filters = 128,\n      kernel_size = 5,\n      activation = \"relu\"\n    ) %>%\n  layer_batch_normalization() %>%\n  layer_global_average_pooling_1d() %>%\n  layer_dense(units = 128, activation = \"relu\") %>%\n  layer_dense(units = 1)\n\nWe train the model with mean squared error loss.\n\n\noptimizer <- optimizer_adam()\nmodel %>% compile(loss = \"mse\", optimizer = optimizer, metrics = metric_mean_absolute_error)\n\nnum_epochs <- 20\nhistory <- model %>% fit(\n  ds_train, \n  steps_per_epoch = n_train/batch_size,\n  validation_data = ds_test,\n  epochs = num_epochs,\n  validation_steps = n_test/batch_size)\n\nBaseline results\nAfter 20 epochs, mean absolute error is around 6 bpm:\n\n\n\nFigure 1: Training history without differential privacy.\n\n\n\nJust to put this in context, the MAE reported for subject S1 in the paper(Reiss et al. 2019) – based on a higher-capacity network, extensive hyperparameter tuning, and naturally, training on the complete dataset – amounts to 8.45 bpm on average; so our setup seems to be sound.\nNow we’ll make this differentially private.\nDP training\nInstead of the plain Adam optimizer, we use the corresponding TF Privacy wrapper, DPAdamGaussianOptimizer.\nWe need to tell it how aggressive gradient clipping should be (l2_norm_clip) and how much noise to add (noise_multiplier). Furthermore, we define the learning rate (there is no default), going for 10 times the default 0.001 based on initial experiments.\nThere is an additional parameter, num_microbatches, that could be used to speed up training (McMahan and Andrew 2018), but, as training duration is not an issue here, we just set it equal to batch_size.\nThe values for l2_norm_clip and noise_multiplier chosen here follow those used in the tutorials in the TF Privacy repo.\nNicely, TF Privacy comes with a script that allows one to compute the attained \\(\\epsilon\\) beforehand, based on number of training examples, batch_size, noise_multiplier and number of training epochs.5\nCalling that script, and assuming we train for 20 epochs here as well,\n\n\npython compute_dp_sgd_privacy.py --N=3989 --batch_size=32 --noise_multiplier=1.1 --epochs=20\n\nthis is what we get back:\n\nDP-SGD with sampling rate = 0.802% and noise_multiplier = 1.1 iterated over\n2494 steps satisfies differential privacy with eps = 2.73 and delta = 1e-06.\nHow good is a value of 2.73? Citing the TF Privacy authors:\n\n\\(\\epsilon\\) gives a ceiling on how much the probability of a particular output can increase by including (or removing) a single training example. We usually want it to be a small constant (less than 10, or, for more stringent privacy guarantees, less than 1). However, this is only an upper bound, and a large value of epsilon may still mean good practical privacy.\n\nObviously, choice of \\(\\epsilon\\) is a (challenging) topic unto itself, and not something we can elaborate on in a post dedicated to the technical aspects of DP with TensorFlow.\nHow would \\(\\epsilon\\) change if we trained for 50 epochs instead? (This is actually what we’ll do, seeing that training results on the test set tend to jump around quite a bit.)\n\n\npython compute_dp_sgd_privacy.py --N=3989 --batch_size=32 --noise_multiplier=1.1 --epochs=60\n\n\nDP-SGD with sampling rate = 0.802% and noise_multiplier = 1.1 iterated over\n6233 steps satisfies differential privacy with eps = 4.25 and delta = 1e-06.\nHaving talked about its parameters, now let’s define the DP optimizer:\n\n\nl2_norm_clip <- 1\nnoise_multiplier <- 1.1\nnum_microbatches <- k_cast(batch_size, \"int32\")\nlearning_rate <- 0.01\n\noptimizer <- priv$DPAdamGaussianOptimizer(\n  l2_norm_clip = l2_norm_clip,\n  noise_multiplier = noise_multiplier,\n  num_microbatches = num_microbatches,\n  learning_rate = learning_rate\n)\n\nThere is one other change to make for DP. As gradients are clipped on a per-sample basis, the optimizer needs to work with per-sample losses as well:\n\n\nloss <- tf$keras$losses$MeanSquaredError(reduction =  tf$keras$losses$Reduction$NONE)\n\nEverything else stays the same. Training history (like we said above, lasting for 50 epochs now) looks a lot more turbulent, with MAEs on the test set fluctuating between 8 and 20 over the last 10 training epochs:\n\n\n\nFigure 2: Training history with differential privacy.\n\n\n\nIn addition to the above-mentioned command line script, we can also compute \\(\\epsilon\\) as part of the training code. Let’s double check:\n\n\n# probability of an individual training point being included in a minibatch\nsampling_probability <- batch_size / n_train\n\n# number of steps the optimizer takes over the training data\nsteps <- num_epochs * n_train / batch_size\n\n# required for reasons related to how TF Privacy computes privacy\n# this actually is Renyi Differential Privacy: https://arxiv.org/abs/1702.07476\n# we don't go into details here and use same values as the command line script\norders <- c((1 + (1:99)/10), 12:63)\n\nrdp <- priv$privacy$analysis$rdp_accountant$compute_rdp(\n  q = sampling_probability,\n  noise_multiplier = noise_multiplier,\n  steps = steps,\n  orders = orders)\n\npriv$privacy$analysis$rdp_accountant$get_privacy_spent(\n  orders, rdp, target_delta = 1e-6)[[1]]\n\n\n[1] 4.249645\nSo, we do get the same result.\nConclusion\nThis post showed how to convert a normal deep learning procedure into an \\(\\epsilon\\)-differentially private one. Necessarily, a blog post has to leave open questions. In the present case, some possible questions could be answered by straightforward experimentation:\nHow well do other optimizers work in this setting?\nHow does the learning rate affect privacy and performance?\nWhat happens if we train for a lot longer?\nOthers sound more like they could lead to a research project:\nWhen model performance – and thus, model parameters – fluctuate that much, how do we decide on when to stop training? Is stopping at high model performance cheating? Is model averaging a sound solution?\nHow good really is any one \\(\\epsilon\\)?\nFinally, yet others transcend the realms of experimentation as well as mathematics:\nHow do we trade off \\(\\epsilon\\)-DP against model performance – for different applications, with different types of data, in different societal contexts?\nAssuming we “have” \\(\\epsilon\\)-DP, what might we still be missing?\nWith questions like these – and more, probably – to ponder: Thanks for reading and a happy new year!\n\n\nAbadi, Martin, Andy Chu, Ian Goodfellow, Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. 2016. “Deep Learning with Differential Privacy.” In 23rd Acm Conference on Computer and Communications Security (Acm Ccs), 308–18. https://arxiv.org/abs/1607.00133.\n\n\nAllen, John. 2007. “Photoplethysmography and Its Application in Clinical Physiological Measurement.” Physiological Measurement 28 (3): R1–R39. https://doi.org/10.1088/0967-3334/28/3/r01.\n\n\nDwork, Cynthia. 2006. “Differential Privacy.” In 33rd International Colloquium on Automata, Languages and Programming, Part Ii (Icalp 2006), 33rd International Colloquium on Automata, Languages and Programming, part II (ICALP 2006), 4052:1–12. Lecture Notes in Computer Science. Springer Verlag. https://www.microsoft.com/en-us/research/publication/differential-privacy/.\n\n\nDwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. “Calibrating Noise to Sensitivity in Private Data Analysis.” In Proceedings of the Third Conference on Theory of Cryptography, 265–84. TCC’06. Berlin, Heidelberg: Springer-Verlag. https://doi.org/10.1007/11681878_14.\n\n\nDwork, Cynthia, and Aaron Roth. 2014. “The Algorithmic Foundations of Differential Privacy.” Found. Trends Theor. Comput. Sci. 9 (3&#8211;4): 211–407. https://doi.org/10.1561/0400000042.\n\n\nMcMahan, H. Brendan, and Galen Andrew. 2018. “A General Approach to Adding Differential Privacy to Iterative Training Procedures.” CoRR abs/1812.06210. http://arxiv.org/abs/1812.06210.\n\n\nReiss, Attila, Ina Indlekofer, Philip Schmidt, and Kristof Van Laerhoven. 2019. “Deep Ppg: Large-Scale Heart Rate Estimation with Convolutional Neural Networks.” Sensors 19 (14): 3079. https://doi.org/10.3390/s19143079.\n\n\nWood, Alexandra, Micah Altman, Aaron Bembenek, Mark Bun, Marco Gaboardi, James Honaker, Kobbi Nissim, David O’Brien, Thomas Steinke, and Salil Vadhan. 2018. “Differential Privacy: A Primer for a Non-Technical Audience.” SSRN Electronic Journal, January. https://doi.org/10.2139/ssrn.3338027.\n\n\nWe’ll be using DP as an acronym for both the noun phrase “differential privacy” and the adjective phrase “differentially private”.↩︎\nThis is not exactly everything, as we’ll see when we get to the code, but “just about”.↩︎\nRelative files per subject are 1.4G in size.↩︎\nFor convenience, we take the liberty to talk as if we had the usual rectangular data here.↩︎\nThere is an additional parameter, \\(\\delta\\), that allows for bounding the risk that the privacy guarantee does not hold. The recommendation is to set this to at most the inverse of the number of training examples, which in out case would mean <= ~ 1e-04; the default setting is 1e-06 so we should be fine here.↩︎\n",
    "preview": "posts/2019-12-20-differential-privacy/images/cat.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 400,
    "preview_height": 251
  },
  {
    "path": "posts/2019-12-18-tfhub-0.7.0/",
    "title": "tfhub: R interface to TensorFlow Hub",
    "description": "TensorFlow Hub is a library for the publication, discovery, and consumption of reusable parts of machine learning models. A module is a self-contained piece of a TensorFlow graph, along with its weights and assets, that can be reused across different tasks in a process known as transfer learning.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": {}
      }
    ],
    "date": "2019-12-18",
    "categories": [
      "TensorFlow/Keras",
      "Packages/Releases"
    ],
    "contents": "\nWe are pleased to announce that the first version of tfhub is now on CRAN. tfhub is an R interface to TensorFlow Hub - a library for the publication, discovery, and consumption of reusable parts of machine learning models. A module is a self-contained piece of a TensorFlow graph, along with its weights and assets, that can be reused across different tasks in a process known as transfer learning.\nThe CRAN version of tfhub can be installed with:\n\n\ninstall.packages(\"tfhub\")\n\nAfter installing the R package you need to install the TensorFlow Hub python package. You can do it by running:\n\n\ntfhub::install_tfhub()\n\nGetting started\nThe essential function of tfhub is layer_hub which works just like a keras layer but allows you to load a complete pre-trained deep learning model.\nFor example you can:\n\n\nlibrary(tfhub)\nlayer_mobilenet <- layer_hub(\n  handle = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4\"\n)\n\nThis will download the MobileNet model pre-trained on the ImageNet dataset. tfhub models are cached locally and don’t need to be downloaded the next time you use the same model.\nYou can now use layer_mobilenet as a usual Keras layer. For example you can define a model:\n\n\nlibrary(keras)\ninput <- layer_input(shape = c(224, 224, 3))\noutput <- layer_mobilenet(input)\nmodel <- keras_model(input, output)\nsummary(model)\n\n\nModel: \"model\"\n____________________________________________________________________\nLayer (type)                  Output Shape               Param #    \n====================================================================\ninput_2 (InputLayer)          [(None, 224, 224, 3)]      0          \n____________________________________________________________________\nkeras_layer_1 (KerasLayer)    (None, 1001)               3540265    \n====================================================================\nTotal params: 3,540,265\nTrainable params: 0\nNon-trainable params: 3,540,265\n____________________________________________________________________\nThis model can now be used to predict Imagenet labels for an image. For example, let’s see the results for the famous Grace Hopper’s photo:\nGrace Hopper\n\nimg <- image_load(\"images/grace-hopper.jpg\", target_size = c(224,224)) %>% \n  image_to_array()\nimg <- img/255\ndim(img) <- c(1, dim(img))\npred <- predict(model, img)\nimagenet_decode_predictions(pred[,-1,drop=FALSE])[[1]]\n\n\n  class_name class_description    score\n1  n03763968  military_uniform 9.760404\n2  n02817516          bearskin 5.922512\n3  n04350905              suit 5.729345\n4  n03787032       mortarboard 5.400651\n5  n03929855       pickelhaube 5.008665\nTensorFlow Hub also offers many other pre-trained image, text and video models. All possible models can be found on the TensorFlow hub website.\nTensorFlow HubYou can find more examples of layer_hub usage in the following articles on the TensorFlow for R website:\nTransfer Learning with tfhub\nUsing tfhub with Keras\ntfhub Basics\nText classification example\nUsage with Recipes and the Feature Spec API\ntfhub also offers recipes steps to make it easier to use pre-trained deep learning models in your machine learning workflow.\nFor example, you can define a recipe that uses a pre-trained text embedding model with:\n\n\nrec <- recipe(obscene ~ comment_text, data = train) %>%\n  step_pretrained_text_embedding(\n    comment_text,\n    handle = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim-with-oov/1\"\n  ) %>%\n  step_bin2factor(obscene)\n\nYou can see a complete running example here.\nYou can also use tfhub with the new Feature Spec API implemented in tfdatasets. You can see a complete example here.\nWe hope our readers have fun experimenting with Hub models and/or can put them to good use. If you run into any problems, let us know by creating an issue in the tfhub repository\n\n\n",
    "preview": "posts/2019-12-18-tfhub-0.7.0/images/tfhub.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 1365,
    "preview_height": 909
  },
  {
    "path": "posts/2019-12-10-variational-gaussian-process/",
    "title": "Gaussian Process Regression with tfprobability",
    "description": "Continuing our tour of applications of TensorFlow Probability (TFP), after Bayesian Neural Networks, Hamiltonian Monte Carlo and State Space Models, here we show an example of Gaussian Process Regression. In fact, what we see is a rather \"normal\" Keras network, defined and trained in pretty much the usual way, with TFP's Variational Gaussian Process layer pulling off all the magic.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-12-10",
    "categories": [
      "Probabilistic ML/DL",
      "TensorFlow/Keras"
    ],
    "contents": "\nHow do you motivate, or come up with a story around Gaussian Process Regression on a blog primarily dedicated to deep learning?\nEasy. As demonstrated by seemingly unavoidable, reliably recurring Twitter “wars” surrounding AI, nothing attracts attention like controversy and antagonism. So, let’s go back twenty years and find citations of people saying, “here come Gaussian Processes, we don’t need to bother with those finicky, hard to tune neural networks anymore!”. And today, here we are; everyone knows something about deep learning but who’s heard of Gaussian Processes?\nWhile similar tales tell a lot about history of science and development of opinions, we prefer a different angle here. In the preface to their 2006 book on Gaussian Processes for Machine Learning (Rasmussen and Williams 2005), Rasmussen and Williams say, referring to the “two cultures” – the disciplines of statistics and machine learning, respectively:1\n\nGaussian process models in some sense bring together work in the two communities.\n\nIn this post, that “in some sense” gets very concrete. We’ll see a Keras network, defined and trained the usual way, that has a Gaussian Process layer for its main constituent. The task will be “simple” multivariate regression.\nAs an aside, this “bringing together communities” – or ways of thinking, or solution strategies – makes for a good overall characterization of TensorFlow Probability as well.2\nGaussian Processes\nA Gaussian Process is a distribution over functions, where the function values you sample are jointly Gaussian - roughly speaking, a generalization to infinity of the multivariate Gaussian. Besides the reference book we already mentioned (Rasmussen and Williams 2005), there are a number of nice introductions on the net: see e.g. https://distill.pub/2019/visual-exploration-gaussian-processes/ or https://peterroelants.github.io/posts/gaussian-process-tutorial/. And like on everything cool, there is a chapter on Gaussian Processes in the late David MacKay’s (MacKay 2002) book.\nIn this post, we’ll use TensorFlow Probability’s Variational Gaussian Process (VGP) layer, designed to efficiently work with “big data”. As Gaussian Process Regression (GPR, from now on) involves the inversion of a – possibly big – covariance matrix, attempts have been made to design approximate versions, often based on variational principles. The TFP implementation is based on papers by Titsias (2009) (Titsias 2009) and Hensman et al. (2013) (Hensman, Fusi, and Lawrence 2013). Instead of \\(p(\\mathbf{y}|\\mathbf{X})\\), the actual probability of the target data given the actual input, we work with a variational distribution \\(q(\\mathbf{u})\\) that acts as a lower bound.\nHere \\(\\mathbf{u}\\) are the function values at a set of so-called inducing index points specified by the user, chosen to well cover the range of the actual data. This algorithm is a lot faster than “normal” GPR, as only the covariance matrix of \\(\\mathbf{u}\\) has to be inverted. As we’ll see below, at least in this example (as well as in others not described here) it seems to be pretty robust as to the number of inducing points passed.\nLet’s start.\nThe dataset\nThe Concrete Compressive Strength Data Set is part of the UCI Machine Learning Repository. Its web page says:\n\nConcrete is the most important material in civil engineering. The concrete compressive strength is a highly nonlinear function of age and ingredients.\n\nHighly nonlinear function - doesn’t that sound intriguing? In any case, it should constitute an interesting test case for GPR.\nHere is a first look.\n\n\nlibrary(tidyverse)\nlibrary(GGally)\nlibrary(visreg)\nlibrary(readxl)\nlibrary(rsample)\nlibrary(reticulate)\nlibrary(tfdatasets)\nlibrary(keras)\nlibrary(tfprobability)\n\nconcrete <- read_xls(\n  \"Concrete_Data.xls\",\n  col_names = c(\n    \"cement\",\n    \"blast_furnace_slag\",\n    \"fly_ash\",\n    \"water\",\n    \"superplasticizer\",\n    \"coarse_aggregate\",\n    \"fine_aggregate\",\n    \"age\",\n    \"strength\"\n  ),\n  skip = 1\n)\n\nconcrete %>% glimpse()\n\n\nObservations: 1,030\nVariables: 9\n$ cement             <dbl> 540.0, 540.0, 332.5, 332.5, 198.6, 266.0, 380.0, 380.0, …\n$ blast_furnace_slag <dbl> 0.0, 0.0, 142.5, 142.5, 132.4, 114.0, 95.0, 95.0, 114.0,…\n$ fly_ash            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ water              <dbl> 162, 162, 228, 228, 192, 228, 228, 228, 228, 228, 192, 1…\n$ superplasticizer   <dbl> 2.5, 2.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0…\n$ coarse_aggregate   <dbl> 1040.0, 1055.0, 932.0, 932.0, 978.4, 932.0, 932.0, 932.0…\n$ fine_aggregate     <dbl> 676.0, 676.0, 594.0, 594.0, 825.5, 670.0, 594.0, 594.0, …\n$ age                <dbl> 28, 28, 270, 365, 360, 90, 365, 28, 28, 28, 90, 28, 270,…\n$ strength           <dbl> 79.986111, 61.887366, 40.269535, 41.052780, 44.296075, 4…\nIt is not that big – just a little more than 1000 rows –, but still, we will have room to experiment with different numbers of inducing points.\nWe have eight predictors, all numeric. With the exception of age (in days), these represent masses (in kg) in one cubic metre of concrete. The target variable, strength, is measured in megapascals.\nLet’s get a quick overview of mutual relationships.\n\n\nggpairs(concrete)\n\n\n\n\nChecking for a possible interaction (one that a layperson could easily think of), does cement concentration act differently on concrete strength depending on how much water there is in the mixture?\n\n\ncement_ <- cut(concrete$cement, 3, labels = c(\"low\", \"medium\", \"high\"))\nfit <- lm(strength ~ (.) ^ 2, data = cbind(concrete[, 2:9], cement_))\nsummary(fit)\n\nvisreg(fit, \"cement_\", \"water\", gg = TRUE) + theme_minimal()\n\n\n\n\nTo anchor our future perception of how well VGP does for this example, we fit a simple linear model, as well as one involving two-way interactions.\n\n\n# scale predictors here already, so data are the same for all models\nconcrete[, 1:8] <- scale(concrete[, 1:8])\n\n# train-test split \nset.seed(777)\nsplit <- initial_split(concrete, prop = 0.8)\ntrain <- training(split)\ntest <- testing(split)\n\n# simple linear model with no interactions\nfit1 <- lm(strength ~ ., data = train)\nfit1 %>% summary()\n\n\nCall:\nlm(formula = strength ~ ., data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-30.594  -6.075   0.612   6.694  33.032 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         35.6773     0.3596  99.204  < 2e-16 ***\ncement              13.0352     0.9702  13.435  < 2e-16 ***\nblast_furnace_slag   9.1532     0.9582   9.552  < 2e-16 ***\nfly_ash              5.9592     0.8878   6.712 3.58e-11 ***\nwater               -2.5681     0.9503  -2.702  0.00703 ** \nsuperplasticizer     1.9660     0.6138   3.203  0.00141 ** \ncoarse_aggregate     1.4780     0.8126   1.819  0.06929 .  \nfine_aggregate       2.2213     0.9470   2.346  0.01923 *  \nage                  7.7032     0.3901  19.748  < 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 10.32 on 816 degrees of freedom\nMultiple R-squared:  0.627, Adjusted R-squared:  0.6234 \nF-statistic: 171.5 on 8 and 816 DF,  p-value: < 2.2e-16\n\n\n# two-way interactions\nfit2 <- lm(strength ~ (.) ^ 2, data = train)\nfit2 %>% summary()\n\n\nCall:\nlm(formula = strength ~ (.)^2, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-24.4000  -5.6093  -0.0233   5.7754  27.8489 \n\nCoefficients:\n                                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                          40.7908     0.8385  48.647  < 2e-16 ***\ncement                               13.2352     1.0036  13.188  < 2e-16 ***\nblast_furnace_slag                    9.5418     1.0591   9.009  < 2e-16 ***\nfly_ash                               6.0550     0.9557   6.336 3.98e-10 ***\nwater                                -2.0091     0.9771  -2.056 0.040090 *  \nsuperplasticizer                      3.8336     0.8190   4.681 3.37e-06 ***\ncoarse_aggregate                      0.3019     0.8068   0.374 0.708333    \nfine_aggregate                        1.9617     0.9872   1.987 0.047256 *  \nage                                  14.3906     0.5557  25.896  < 2e-16 ***\ncement:blast_furnace_slag             0.9863     0.5818   1.695 0.090402 .  \ncement:fly_ash                        1.6434     0.6088   2.700 0.007093 ** \ncement:water                         -4.2152     0.9532  -4.422 1.11e-05 ***\ncement:superplasticizer              -2.1874     1.3094  -1.670 0.095218 .  \ncement:coarse_aggregate               0.2472     0.5967   0.414 0.678788    \ncement:fine_aggregate                 0.7944     0.5588   1.422 0.155560    \ncement:age                            4.6034     1.3811   3.333 0.000899 ***\nblast_furnace_slag:fly_ash            2.1216     0.7229   2.935 0.003434 ** \nblast_furnace_slag:water             -2.6362     1.0611  -2.484 0.013184 *  \nblast_furnace_slag:superplasticizer  -0.6838     1.2812  -0.534 0.593676    \nblast_furnace_slag:coarse_aggregate  -1.0592     0.6416  -1.651 0.099154 .  \nblast_furnace_slag:fine_aggregate     2.0579     0.5538   3.716 0.000217 ***\nblast_furnace_slag:age                4.7563     1.1148   4.266 2.23e-05 ***\nfly_ash:water                        -2.7131     0.9858  -2.752 0.006054 ** \nfly_ash:superplasticizer             -2.6528     1.2553  -2.113 0.034891 *  \nfly_ash:coarse_aggregate              0.3323     0.7004   0.474 0.635305    \nfly_ash:fine_aggregate                2.6764     0.7817   3.424 0.000649 ***\nfly_ash:age                           7.5851     1.3570   5.589 3.14e-08 ***\nwater:superplasticizer                1.3686     0.8704   1.572 0.116289    \nwater:coarse_aggregate               -1.3399     0.5203  -2.575 0.010194 *  \nwater:fine_aggregate                 -0.7061     0.5184  -1.362 0.173533    \nwater:age                             0.3207     1.2991   0.247 0.805068    \nsuperplasticizer:coarse_aggregate     1.4526     0.9310   1.560 0.119125    \nsuperplasticizer:fine_aggregate       0.1022     1.1342   0.090 0.928239    \nsuperplasticizer:age                  1.9107     0.9491   2.013 0.044444 *  \ncoarse_aggregate:fine_aggregate       1.3014     0.4750   2.740 0.006286 ** \ncoarse_aggregate:age                  0.7557     0.9342   0.809 0.418815    \nfine_aggregate:age                    3.4524     1.2165   2.838 0.004657 ** \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 8.327 on 788 degrees of freedom\nMultiple R-squared:  0.7656,    Adjusted R-squared:  0.7549 \nF-statistic: 71.48 on 36 and 788 DF,  p-value: < 2.2e-16\nWe also store the predictions on the test set, for later comparison.\n\n\nlinreg_preds1 <- fit1 %>% predict(test[, 1:8])\nlinreg_preds2 <- fit2 %>% predict(test[, 1:8])\n\ncompare <-\n  data.frame(\n    y_true = test$strength,\n    linreg_preds1 = linreg_preds1,\n    linreg_preds2 = linreg_preds2\n  )\n\nWith no further preprocessing required, the tfdatasets input pipeline ends up nice and short:\n\n\ncreate_dataset <- function(df, batch_size, shuffle = TRUE) {\n  \n  df <- as.matrix(df)\n  ds <-\n    tensor_slices_dataset(list(df[, 1:8], df[, 9, drop = FALSE]))\n  if (shuffle)\n    ds <- ds %>% dataset_shuffle(buffer_size = nrow(df))\n  ds %>%\n    dataset_batch(batch_size = batch_size)\n  \n}\n\n# just one possible choice for batch size ...\nbatch_size <- 64\ntrain_ds <- create_dataset(train, batch_size = batch_size)\ntest_ds <- create_dataset(test, batch_size = nrow(test), shuffle = FALSE)\n\nAnd on to model creation.\nThe model\nModel definition is short as well, although there are a few things to expand on. Don’t execute this yet:\n\n\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 8,\n              input_shape = 8,\n              use_bias = FALSE) %>%\n  layer_variational_gaussian_process(\n    # number of inducing points\n    num_inducing_points = num_inducing_points,\n    # kernel to be used by the wrapped Gaussian Process distribution\n    kernel_provider = RBFKernelFn(),\n    # output shape \n    event_shape = 1, \n    # initial values for the inducing points\n    inducing_index_points_initializer = initializer_constant(as.matrix(sampled_points)),\n    unconstrained_observation_noise_variance_initializer =\n      initializer_constant(array(0.1))\n  )\n\nTwo arguments to layer_variational_gaussian_process() need some preparation before we can actually run this. First, as the documentation tells us, kernel_provider should be\n\na layer instance equipped with an @property, which yields a PositiveSemidefiniteKernel instance\".\n\nIn other words, the VGP layer wraps another Keras layer that, itself, wraps or bundles together the TensorFlow Variables containing the kernel parameters.\nWe can make use of reticulate’s new PyClass constructor to fulfill the above requirements. Using PyClass, we can directly inherit from a Python object, adding and/or overriding methods or fields as we like - and yes, even create a Python property.\n\n\nbt <- import(\"builtins\")\nRBFKernelFn <- reticulate::PyClass(\n  \"KernelFn\",\n  inherit = tensorflow::tf$keras$layers$Layer,\n  list(\n    `__init__` = function(self, ...) {\n      kwargs <- list(...)\n      super()$`__init__`(kwargs)\n      dtype <- kwargs[[\"dtype\"]]\n      self$`_amplitude` = self$add_variable(initializer = initializer_zeros(),\n                                            dtype = dtype,\n                                            name = 'amplitude')\n      self$`_length_scale` = self$add_variable(initializer = initializer_zeros(),\n                                               dtype = dtype,\n                                               name = 'length_scale')\n      NULL\n    },\n    \n    call = function(self, x, ...) {\n      x\n    },\n    \n    kernel = bt$property(\n      reticulate::py_func(\n        function(self)\n          tfp$math$psd_kernels$ExponentiatedQuadratic(\n            amplitude = tf$nn$softplus(array(0.1) * self$`_amplitude`),\n            length_scale = tf$nn$softplus(array(2) * self$`_length_scale`)\n          )\n      )\n    )\n  )\n)\n\nThe Gaussian Process kernel used is one of several available in tfp.math.psd_kernels (psd standing for positive semidefinite), and probably the one that comes to mind first when thinking of GPR: the squared exponential, or exponentiated quadratic.3 The version used in TFP, with hyperparameters amplitude \\(a\\) and length scale \\(\\lambda\\), is\n\\[k(x,x') = 2 \\ a \\ exp (\\frac{- 0.5 (x−x')^2}{\\lambda^2}) \\]\nHere the interesting parameter is the length scale \\(\\lambda\\). When we have several features, their length scales – as induced by the learning algorithm – reflect their importance: If, for some feature, \\(\\lambda\\) is large, the respective squared deviations from the mean don’t matter that much. The inverse length scale can thus be used for automatic relevance determination (Neal 1996).\nThe second thing to take care of is choosing the initial index points. From experiments, the exact choices don’t matter that much, as long as the data are sensibly covered. For instance, an alternative way we tried was to construct an empirical distribution (tfd_empirical) from the data, and then sample from it. Here instead, we just use an – unnecessary, admittedly, given the availability of sample in R – fancy way to select random observations from the training data:\n\n\nnum_inducing_points <- 50\n\nsample_dist <- tfd_uniform(low = 1, high = nrow(train) + 1)\nsample_ids <- sample_dist %>%\n  tfd_sample(num_inducing_points) %>%\n  tf$cast(tf$int32) %>%\n  as.numeric()\nsampled_points <- train[sample_ids, 1:8]\n\nOne interesting point to note before we start training: Computation of the posterior predictive parameters involves a Cholesky decomposition, which could fail if, due to numerical issues, the covariance matrix is no longer positive definite. A sufficient action to take in our case is to do all computations using tf$float64:\n\n\nk_set_floatx(\"float64\")\n\nNow we define (for real, this time) and run the model.4\n\n\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 8,\n              input_shape = 8,\n              use_bias = FALSE) %>%\n  layer_variational_gaussian_process(\n    num_inducing_points = num_inducing_points,\n    kernel_provider = RBFKernelFn(),\n    event_shape = 1,\n    inducing_index_points_initializer = initializer_constant(as.matrix(sampled_points)),\n    unconstrained_observation_noise_variance_initializer =\n      initializer_constant(array(0.1))\n  )\n\n# KL weight sums to one for one epoch\nkl_weight <- batch_size / nrow(train)\n\n# loss that implements the VGP algorithm\nloss <- function(y, rv_y)\n  rv_y$variational_loss(y, kl_weight = kl_weight)\n\nmodel %>% compile(optimizer = optimizer_adam(lr = 0.008),\n                  loss = loss,\n                  metrics = \"mse\")\n\nhistory <- model %>% fit(train_ds,\n                         epochs = 100,\n                         validation_data = test_ds)\n\nplot(history)\n\n\n\n\nInterestingly, higher numbers of inducing points (we tried 100 and 200) did not have much impact on regression performance.5 Nor does the exact choice of multiplication constants (0.1 and 2) applied to the trained kernel Variables (_amplitude and _length_scale)\n\n\ntfp$math$psd_kernels$ExponentiatedQuadratic(\n  amplitude = tf$nn$softplus(array(0.1) * self$`_amplitude`),\n  length_scale = tf$nn$softplus(array(2) * self$`_length_scale`)\n)\n\nmake much of a difference to the end result.6\nPredictions\nWe generate predictions on the test set and add them to the data.frame containing the linear models’ predictions. As with other probabilistic output layers, “the predictions” are in fact distributions; to obtain actual tensors we sample from them. Here, we average over 10 samples:\n\n\nyhats <- model(tf$convert_to_tensor(as.matrix(test[, 1:8])))\n\nyhat_samples <-  yhats %>%\n  tfd_sample(10) %>%\n  tf$squeeze() %>%\n  tf$transpose()\n\nsample_means <- yhat_samples %>% apply(1, mean)\n\ncompare <- compare %>%\n  cbind(vgp_preds = sample_means)\n\nWe plot the average VGP predictions against the ground truth, together with the predictions from the simple linear model (cyan) and the model including two-way interactions (violet):\n\n\nggplot(compare, aes(x = y_true)) +\n  geom_abline(slope = 1, intercept = 0) +\n  geom_point(aes(y = vgp_preds, color = \"VGP\")) +\n  geom_point(aes(y = linreg_preds1, color = \"simple lm\"), alpha = 0.4) +\n  geom_point(aes(y = linreg_preds2, color = \"lm w/ interactions\"), alpha = 0.4) +\n  scale_colour_manual(\"\", \n                      values = c(\"VGP\" = \"black\", \"simple lm\" = \"cyan\", \"lm w/ interactions\" = \"violet\")) +\n  coord_cartesian(xlim = c(min(compare$y_true), max(compare$y_true)), ylim = c(min(compare$y_true), max(compare$y_true))) +\n  ylab(\"predictions\") +\n  theme(aspect.ratio = 1) \n\n\n\n\nFigure 1: Predictions vs. ground truth for linear regression (no interactions; cyan), linear regression with 2-way interactions (violet), and VGP (black).\n\n\n\nAdditionally, comparing MSEs for the three sets of predictions, we see\n\n\nmse <- function(y_true, y_pred) {\n  sum((y_true - y_pred) ^ 2) / length(y_true)\n}\n\nlm_mse1 <- mse(compare$y_true, compare$linreg_preds1) # 117.3111\nlm_mse2 <- mse(compare$y_true, compare$linreg_preds2) # 80.79726\nvgp_mse <- mse(compare$y_true, compare$vgp_preds)     # 58.49689\n\nSo, the VGP does in fact outperform both baselines. Something else we might be interested in: How do its predictions vary? Not as much as we might want, were we to construct uncertainty estimates from them alone. Here we plot the 10 samples we drew before:\n\n\nsamples_df <-\n  data.frame(cbind(compare$y_true, as.matrix(yhat_samples))) %>%\n  gather(key = run, value = prediction, -X1) %>% \n  rename(y_true = \"X1\")\n\nggplot(samples_df, aes(y_true, prediction)) +\n  geom_point(aes(color = run),\n             alpha = 0.2,\n             size = 2) +\n  geom_abline(slope = 1, intercept = 0) +\n  theme(legend.position = \"none\") +\n  ylab(\"repeated predictions\") +\n  theme(aspect.ratio = 1)\n\n\n\n\nFigure 2: Predictions from 10 consecutive samples from the VGP distribution.\n\n\n\nDiscussion: Feature Relevance\nAs mentioned above, the inverse length scale can be used as an indicator of feature importance. When using the ExponentiatedQuadratic kernel alone, there will only be a single length scale; in our example, the initial dense layer takes of scaling (and additionally, recombining) the features.\nAlternatively, we could wrap the ExponentiatedQuadratic in a FeatureScaled kernel. FeatureScaled has an additional scale_diag parameter related to exactly that: feature scaling. Experiments with FeatureScaled (and initial dense layer removed, to be “fair”) showed slightly worse performance, and the learned scale_diag values varied quite a bit from run to run. For that reason, we chose to present the other approach; however, we include the code for a wrapping FeatureScaled in case readers would like to experiment with this:\n\n\nScaledRBFKernelFn <- reticulate::PyClass(\n  \"KernelFn\",\n  inherit = tensorflow::tf$keras$layers$Layer,\n  list(\n    `__init__` = function(self, ...) {\n      kwargs <- list(...)\n      super()$`__init__`(kwargs)\n      dtype <- kwargs[[\"dtype\"]]\n      self$`_amplitude` = self$add_variable(initializer = initializer_zeros(),\n                                            dtype = dtype,\n                                            name = 'amplitude')\n      self$`_length_scale` = self$add_variable(initializer = initializer_zeros(),\n                                               dtype = dtype,\n                                               name = 'length_scale')\n      self$`_scale_diag` = self$add_variable(\n        initializer = initializer_ones(),\n        dtype = dtype,\n        shape = 8L,\n        name = 'scale_diag'\n      )\n      NULL\n    },\n    \n    call = function(self, x, ...) {\n      x\n    },\n    \n    kernel = bt$property(\n      reticulate::py_func(\n        function(self)\n          tfp$math$psd_kernels$FeatureScaled(\n            kernel = tfp$math$psd_kernels$ExponentiatedQuadratic(\n              amplitude = tf$nn$softplus(array(1) * self$`_amplitude`),\n              length_scale = tf$nn$softplus(array(2) * self$`_length_scale`)\n            ),\n            scale_diag = tf$nn$softplus(array(1) * self$`_scale_diag`)\n          )\n      )\n    )\n  )\n)\n\nFinally, if all you cared about was prediction performance, you could use FeatureScaled and keep the initial dense layer all the same. But in that case, you’d probably use a neural network – not a Gaussian Process – anyway …\nThanks for reading!\n\n\nBreiman, Leo. 2001. “Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).” Statist. Sci. 16 (3): 199–231. https://doi.org/10.1214/ss/1009213726.\n\n\nHensman, James, Nicolo Fusi, and Neil D. Lawrence. 2013. “Gaussian Processes for Big Data.” CoRR abs/1309.6835. http://arxiv.org/abs/1309.6835.\n\n\nMacKay, David J. C. 2002. Information Theory, Inference & Learning Algorithms. New York, NY, USA: Cambridge University Press.\n\n\nNeal, Radford M. 1996. Bayesian Learning for Neural Networks. Berlin, Heidelberg: Springer-Verlag.\n\n\nRasmussen, Carl Edward, and Christopher K. I. Williams. 2005. Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning). The MIT Press.\n\n\nTitsias, Michalis. 2009. “Variational Learning of Inducing Variables in Sparse Gaussian Processes.” In Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics, edited by David van Dyk and Max Welling, 5:567–74. Proceedings of Machine Learning Research. Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA: PMLR. http://proceedings.mlr.press/v5/titsias09a.html.\n\n\nIn the book, it’s “two communities”, not “two cultures”; we choose L. Breiman’s (Breiman 2001) more punchy expression just … because.↩︎\nSo far, we have seen uses of TensorFlow Probability for such different applications as adding uncertainty estimates to neural networks, Bayesian model estimation using Hamiltonian Monte Carlo, or linear-Gaussian state space models.↩︎\nSee David Duvenaud’s https://www.cs.toronto.edu/~duvenaud/cookbook/ for an excellent synopsis of kernels and kernel composition.↩︎\nIn case you’re wondering about the dense layer up front: This will be discussed in the last section on feature relevance.↩︎\nPerformance being used in the sense of “lower regression error”, not running speed. As to the latter, there definitely is a (negative) relationship between number of inducing points and training speed.↩︎\nThis may sound like a matter of course; it isn’t necessarily, as shown by prior experiments with variational layers e.g. in Adding uncertainty estimates to Keras models with tfprobability.↩︎\n",
    "preview": "posts/2019-12-10-variational-gaussian-process/images/kernel_cookbook.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 818,
    "preview_height": 352
  },
  {
    "path": "posts/2019-11-27-gettingstarted-2020/",
    "title": "Getting started with Keras from R - the 2020 edition",
    "description": "Looking for materials to get started with deep learning from R? This post presents useful tutorials, guides, and background documentation on the new TensorFlow for R website.  Advanced users will find pointers to applications of new release 2.0 (or upcoming 2.1!) features alluded to in the recent TensorFlow 2.0 post.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-11-27",
    "categories": [
      "Packages/Releases",
      "TensorFlow/Keras"
    ],
    "contents": "\nIf you’ve been thinking about diving into deep learning for a while – using R, preferentially –, now is a good time. For TensorFlow / Keras, one of the predominant deep learning frameworks on the market, last year was a year of substantial changes; for users, this sometimes would mean ambiguity and confusion about the “right” (or: recommended) way to do things. By now, TensorFlow 2.0 has been the current stable release for about two months; the mists have cleared away, and patterns have emerged, enabling leaner, more modular code that accomplishes a lot in just a few lines.\nTo give the new features the space they deserve, and assemble central contributions from related packages all in one place, we have significantly remodeled the TensorFlow for R website. So this post really has two objectives.\nFirst, it would like to do exactly what is suggested by the title: Point new users to resources that make for an effective start into the subject.\nSecond, it could be read as a “best of new website content”. Thus, as an existing user, you might still be interested in giving it a quick skim, checking for pointers to new features that appear in familiar contexts. To make this easier, we’ll add side notes to highlight new features.\nOverall, the structure of what follows is this. We start from the core question: How do you build a model?, then frame it from both sides; i.e.: What comes before? (data loading / preprocessing) and What comes after? (model saving / deployment).\nAfter that, we quickly go into creating models for different types of data: images, text, tabular.\nThen, we touch on where to find background information, such as: How do I add a custom callback? How do I create a custom layer? How can I define my own training loop?\nFinally, we round up with something that looks like a tiny technical addition but has far greater impact: integrating modules from TensorFlow (TF) Hub.\nGetting started\nHow to build a model?\nIf linear regression is the Hello World of machine learning, non-linear regression has to be the Hello World of neural networks. The Basic Regression tutorial shows how to train a dense network on the Boston Housing dataset. This example uses the Keras Functional API, one of the two “classical” model-building approaches – the one that tends to be used when some sort of flexibility is required. In this case, the desire for flexibility comes from the use of feature columns - a nice new addition to TensorFlow that allows for convenient integration of e.g. feature normalization (more about this in the next section).\n\nThe regression tutorial now uses feature columns for convenient data preprocessing.\nThis introduction to regression is complemented by a tutorial on multi-class classification using “Fashion MNIST”. It is equally suited for a first encounter with Keras.\nA third tutorial in this section is dedicated to text classification. Here too, there is a hidden gem in the current version that makes text preprocessing a lot easier: layer_text_vectorization, one of the brand new Keras preprocessing layers.1 If you’ve used Keras for NLP before: No more messing with text_tokenizer!\n\nCheck out the new text vectorization layer in the text classification tutorial.\nThese tutorials are nice introductions explaining code as well as concepts. What if you’re familiar with the basic procedure and just need a quick reminder (or: something to quickly copy-paste from)? The ideal document to consult for those purposes is the Overview.\nNow – knowledge how to build models is fine, but as in data science overall, there is no modeling without data.\nData ingestion and preprocessing\nTwo detailed, end-to-end tutorials show how to load csv data and images, respectively.\nIn current Keras, two mechanisms are central to data preparation. One is the use of tfdatasets pipelines. tfdatasets lets you load data in a streaming fashion (batch-by-batch), optionally applying transformations as you go. The other handy device here is feature specs andfeature columns. Together with a matching Keras layer, these allow for transforming the input data without having to think about what the new format will mean to Keras.\nWhile there are other types of data not discussed in the docs, the principles – pre-processing pipelines and feature extraction – generalize.\nModel saving\nThe best-performing model is of little use if ephemeral. Straightforward ways of saving Keras models are explained in a dedicated tutorial.\n\nAdvanced users: Additional options exist; see the tutorial on checkpoints.\nAnd unless one’s just tinkering around, the question will often be: How can I deploy my model? There is a complete new section on deployment, featuring options like plumber, Shiny, TensorFlow Serving and RStudio Connect.\n\nCheck out the new section on deployment options.\nAfter this workflow-oriented run-through, let’s see about different types of data you might want to model.\nNeural networks for different kinds of data\nNo introduction to deep learning is complete without image classification. The “Fashion MNIST” classification tutorial mentioned in the beginning is a good introduction, but it uses a fully connected neural network to make it easy to remain focused on the overall approach. Standard models for image recognition, however, are commonly based on a convolutional architecture. Here is a nice introductory tutorial.\nFor text data, the concept of embeddings – distributed representations endowed with a measure of similarity – is central. As in the aforementioned text classification tutorial, embeddings can be learned using the respective Keras layer (layer_embedding); in fact, the more idiosyncratic the dataset, the more recommendable this approach. Often though, it makes a lot of sense to use pre-trained embeddings, obtained from large language models trained on enormous amounts of data. With TensorFlow Hub, discussed in more detail in the last section, pre-trained embeddings can be made use of simply by integrating an adequate hub layer, as shown in one of the Hub tutorials.\n\nModels from TF Hub can now conveniently be integrated into a model as Keras layers.\nAs opposed to images and text, “normal”, a.k.a. tabular, a.k.a. structured data often seems like less of a candidate for deep learning. Historically, the mix of data types – numeric, binary, categorical –, together with different handling in the network (“leave alone” or embed) used to require a fair amount of manual fiddling. In contrast, the Structured data tutorial shows the, quote-unquote, modern way, again using feature columns and feature specs. The consequence: If you’re not sure that in the area of tabular data, deep learning will lead to improved performance – if it’s as easy as that, why not give it a try?\n\nIf you’re working with structured data, definitely check out the feature spec way to do it.\nBefore rounding up with a special on TensorFlow Hub, let’s quickly see where to get more information on immediate and background-level technical questions.\nGuides: topic-related and background information\nThe Guide section has lots of additional information, covering specific questions that will come up when coding Keras models\nHow can I define a custom layer?\nA custom model?\nWhat are training callbacks?\nas well as background knowledge and terminology: What are tensors, Variables, how does automatic differentiation work in TensorFlow?\nLike for the basics, above we pointed out a document called “Quickstart”, for advanced topics here too is a Quickstart that in one end-to-end example, shows how to define and train a custom model. One especially nice aspect is the use of tfautograph, a package developed by T. Kalinowski that – among others – allows for concisely iterating over a dataset in a for loop.\n\nPower users: Check out the custom training Quickstart featuring custom models, GradientTapes and tfautograph.\nFinally, let’s talk about TF Hub.\nA special highlight: Hub layers\nOne of the most interesting aspects of contemporary neural network architectures is the use of transfer learning. Not everyone has the data, or computing facilities, to train big networks on big data from scratch. Through transfer learning, existing pre-trained models can be used for similar (but not identical) applications and in similar (but not identical) domains.\nDepending on one’s requirements, building on an existing model could be more or less cumbersome. Some time ago, TensorFlow Hub was created as a mechanism to publicly share models, or modules, that is, reusable building blocks that could be made use of by others. Until recently, there was no convenient way to incorporate these modules, though.\nStarting from TensorFlow 2.0, Hub modules can now seemlessly be integrated in Keras models, using layer_hub. This is demonstrated in two tutorials, for text and images, respectively. But really, these two documents are just starting points: Starting points into a journey of experimentation, with other modules, combination of modules, areas of applications…\n\nDon’t miss out on the new TensorFlow Hub layer available in Keras… potentially, an extremely powerful way to enhance your models.\nIn sum, we hope you have fun with the “new” (TF 2.0) Keras and find the documentation useful. Thanks for reading!\nIn fact, it is so new that as of this writing, you will have to install the nightly build of TensorFlow – as well as tensorflow from github – to use it.↩︎\n",
    "preview": "posts/2019-11-27-gettingstarted-2020/images/website.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 1591,
    "preview_height": 725
  },
  {
    "path": "posts/2019-11-13-variational-convnet/",
    "title": "Variational convnets with tfprobability",
    "description": "In a Bayesian neural network, layer weights are distributions, not tensors. Using tfprobability, the R wrapper to TensorFlow Probability, we can build regular Keras models that have probabilistic layers, and thus get uncertainty estimates \"for free\". In this post, we show how to define, train and obtain predictions from a probabilistic convolutional neural network.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-11-13",
    "categories": [
      "Probabilistic ML/DL",
      "Time Series",
      "TensorFlow/Keras"
    ],
    "contents": "\nA bit more than a year ago, in his beautiful guest post, Nick Strayer showed how to classify a set of everyday activities using smartphone-recorded gyroscope and accelerometer data. Accuracy was very good, but Nick went on to inspect classification results more closely. Were there activities more prone to misclassification than others? And how about those erroneous results: Did the network report them with equal, or less confidence than those that were correct?\nTechnically, when we speak of confidence in that manner, we’re referring to the score obtained for the “winning” class after softmax activation.1 If that winning score is 0.9, we might say “the network is sure that’s a gentoo penguin”; if it’s 0.2, we’d instead conclude “to the network, neither option seemed fitting, but cheetah looked best”.\nThis use of “confidence” is convincing, but it has nothing to do with confidence – or credibility, or prediction, what have you – intervals. What we’d really like to be able to do is put distributions over the network’s weights and make it Bayesian. Using tfprobability’s variational Keras-compatible layers, this is something we actually can do.\nAdding uncertainty estimates to Keras models with tfprobability shows how to use a variational dense layer to obtain estimates of epistemic uncertainty. In this post, we modify the convnet used in Nick’s post to be variational throughout. Before we start, let’s quickly summarize the task.\nThe task\nTo create the Smartphone-Based Recognition of Human Activities and Postural Transitions Data Set (Reyes-Ortiz et al. 2016), the researchers had subjects walk, sit, stand, and transition from one of those activities to another. Meanwhile, two types of smartphone sensors were used to record motion data: Accelerometers measure linear acceleration in three2 dimensions, while gyroscopes are used to track angular velocity around the coordinate axes. Here are the respective raw sensor data for six types of activities from Nick’s original post:\n\n\n\nFigure 1: Source: https://blogs.rstudio.com/tensorflow/posts/2018-07-17-activity-detection/\n\n\n\nJust like Nick, we’re going to zoom in on those six types of activity, and try to infer them from the sensor data. Some data wrangling is needed to get the dataset into a form we can work with; here we’ll build on Nick’s post, and effectively start from the data nicely pre-processed and split up into training and test sets:\n\n\ntrainData %>% glimpse()\n\n\nObservations: 289\nVariables: 6\n$ experiment    <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 17, 18, 19, 2…\n$ userId        <int> 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 7, 7, 9, 9, 10, 10, 11…\n$ activity      <int> 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7…\n$ data          <list> [<data.frame[160 x 6]>, <data.frame[206 x 6]>, <dat…\n$ activityName  <fct> STAND_TO_SIT, STAND_TO_SIT, STAND_TO_SIT, STAND_TO_S…\n$ observationId <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 17, 18, 19, 2…\n\n\ntestData %>% glimpse()\n\n\nObservations: 69\nVariables: 6\n$ experiment    <int> 11, 12, 15, 16, 32, 33, 42, 43, 52, 53, 56, 57, 11, …\n$ userId        <int> 6, 6, 8, 8, 16, 16, 21, 21, 26, 26, 28, 28, 6, 6, 8,…\n$ activity      <int> 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8…\n$ data          <list> [<data.frame[185 x 6]>, <data.frame[151 x 6]>, <dat…\n$ activityName  <fct> STAND_TO_SIT, STAND_TO_SIT, STAND_TO_SIT, STAND_TO_S…\n$ observationId <int> 11, 12, 15, 16, 31, 32, 41, 42, 51, 52, 55, 56, 71, …\nThe code required to arrive at this stage (copied from Nick’s post) may be found in the appendix at the bottom of this page.\nTraining pipeline\nThe dataset in question is small enough to fit in memory – but yours might not be, so it can’t hurt to see some streaming in action. Besides, it’s probably safe to say that with TensorFlow 2.0, tfdatasets pipelines are the way to feed data to a model.\nOnce the code listed in the appendix has run, the sensor data is to be found in trainData$data, a list column containing data.frames where each row corresponds to a point in time and each column holds one of the measurements. However, not all time series (recordings) are of the same length; we thus follow the original post to pad all series to length pad_size (= 338). The expected shape of training batches will then be (batch_size, pad_size, 6).\nWe initially create our training dataset:\n\n\ntrain_x <- train_data$data %>% \n  map(as.matrix) %>%\n  pad_sequences(maxlen = pad_size, dtype = \"float32\") %>%\n  tensor_slices_dataset() \n\ntrain_y <- train_data$activity %>% \n  one_hot_classes() %>% \n  tensor_slices_dataset()\n\ntrain_dataset <- zip_datasets(train_x, train_y)\ntrain_dataset\n\n\n<ZipDataset shapes: ((338, 6), (6,)), types: (tf.float64, tf.float64)>\nThen shuffle and batch it:\n\n\nn_train <- nrow(train_data)\n# the highest possible batch size for this dataset\n# chosen because it yielded the best performance\n# alternatively, experiment with e.g. different learning rates, ...\nbatch_size <- n_train\n\ntrain_dataset <- train_dataset %>% \n  dataset_shuffle(n_train) %>%\n  dataset_batch(batch_size)\ntrain_dataset\n\n\n<BatchDataset shapes: ((None, 338, 6), (None, 6)), types: (tf.float64, tf.float64)>\nSame for the test data.\n\n\ntest_x <- test_data$data %>% \n  map(as.matrix) %>%\n  pad_sequences(maxlen = pad_size, dtype = \"float32\") %>%\n  tensor_slices_dataset() \n\ntest_y <- test_data$activity %>% \n  one_hot_classes() %>% \n  tensor_slices_dataset()\n\nn_test <- nrow(test_data)\ntest_dataset <- zip_datasets(test_x, test_y) %>%\n  dataset_batch(n_test)\n\nUsing tfdatasets does not mean we cannot run a quick sanity check on our data:\n\n\nfirst <- test_dataset %>% \n  reticulate::as_iterator() %>% \n  # get first batch (= whole test set, in our case)\n  reticulate::iter_next() %>%\n  # predictors only\n  .[[1]] %>% \n  # first item in batch\n  .[1,,]\nfirst\n\n\ntf.Tensor(\n[[ 0.          0.          0.          0.          0.          0.        ]\n [ 0.          0.          0.          0.          0.          0.        ]\n [ 0.          0.          0.          0.          0.          0.        ]\n ...\n [ 1.00416672  0.2375      0.12916666 -0.40225476 -0.20463985 -0.14782938]\n [ 1.04166663  0.26944447  0.12777779 -0.26755899 -0.02779437 -0.1441642 ]\n [ 1.0250001   0.27083334  0.15277778 -0.19639318  0.35094208 -0.16249016]],\n shape=(338, 6), dtype=float64)\nNow let’s build the network.\nA variational convnet\nWe build on the straightforward convolutional architecture from Nick’s post, just making minor modifications to kernel sizes and numbers of filters. We also throw out all dropout layers; no additional regularization is needed on top of the priors applied to the weights.\nNote the following about the “Bayesified” network.\nEach layer is variational in nature, the convolutional ones (layer_conv_1d_flipout) as well as the dense layers (layer_dense_flipout).\nWith variational layers, we can specify the prior weight distribution as well as the form of the posterior; here the defaults are used, resulting in a standard normal prior and a default mean-field posterior.\nLikewise, the user may influence the divergence function used to assess the mismatch between prior and posterior; in this case, we actually take some action: We scale the (default) KL divergence by the number of samples in the training set.\nOne last thing to note is the output layer. It is a distribution layer, that is, a layer wrapping a distribution – where wrapping means: Training the network is business as usual, but predictions are distributions, one for each data point.\n\n\nlibrary(tfprobability)\n\nnum_classes <- 6\n\n# scale the KL divergence by number of training examples\nn <- n_train %>% tf$cast(tf$float32)\nkl_div <- function(q, p, unused)\n  tfd_kl_divergence(q, p) / n\n\nmodel <- keras_model_sequential()\nmodel %>% \n  layer_conv_1d_flipout(\n    filters = 12,\n    kernel_size = 3, \n    activation = \"relu\",\n    kernel_divergence_fn = kl_div\n  ) %>%\n  layer_conv_1d_flipout(\n    filters = 24,\n    kernel_size = 5, \n    activation = \"relu\",\n    kernel_divergence_fn = kl_div\n  ) %>%\n  layer_conv_1d_flipout(\n    filters = 48,\n    kernel_size = 7, \n    activation = \"relu\",\n    kernel_divergence_fn = kl_div\n  ) %>%\n  layer_global_average_pooling_1d() %>% \n  layer_dense_flipout(\n    units = 48,\n    activation = \"relu\",\n    kernel_divergence_fn = kl_div\n  ) %>% \n  layer_dense_flipout(\n    num_classes, \n    kernel_divergence_fn = kl_div,\n    name = \"dense_output\"\n  ) %>%\n  layer_one_hot_categorical(event_size = num_classes)\n\nWe tell the network to minimize the negative log likelihood.\n\n\nnll <- function(y, model) - (model %>% tfd_log_prob(y))\n\nThis will become part of the loss. The way we set up this example, this is not its most substantial part though. Here, what dominates the loss is the sum of the KL divergences, added (automatically) to model$losses.\nIn a setup like this, it’s interesting to monitor both parts of the loss separately. We can do this by means of two metrics:\n\n\n# the KL part of the loss\nkl_part <-  function(y_true, y_pred) {\n    kl <- tf$reduce_sum(model$losses)\n    kl\n}\n\n# the NLL part\nnll_part <- function(y_true, y_pred) {\n    cat_dist <- tfd_one_hot_categorical(logits = y_pred)\n    nll <- - (cat_dist %>% tfd_log_prob(y_true) %>% tf$reduce_mean())\n    nll\n}\n\nWe train somewhat longer than Nick did in the original post, allowing for early stopping though.\n\n\nmodel %>% compile(\n  optimizer = \"rmsprop\",\n  loss = nll,\n  metrics = c(\"accuracy\", \n              custom_metric(\"kl_part\", kl_part),\n              custom_metric(\"nll_part\", nll_part)),\n  experimental_run_tf_function = FALSE\n)\n\ntrain_history <- model %>% fit(\n  train_dataset,\n  epochs = 1000,\n  validation_data = test_dataset,\n  callbacks = list(\n    callback_early_stopping(patience = 10)\n  )\n)\n\nWhile the overall loss declines linearly (and probably would for many more epochs), this is not the case for classification accuracy or the NLL part of the loss:\n\n\n\nFinal accuracy is not as high as in the non-variational setup, though still not bad for a six-class problem. We see that without any additional regularization, there is very little overfitting to the training data.\nNow how do we obtain predictions from this model?\nProbabilistic predictions\nThough we won’t go into this here, it’s good to know that we access more than just the output distributions; through their kernel_posterior attribute, we can access the hidden layers’ posterior weight distributions as well.\nGiven the small size of the test set, we compute all predictions at once. The predictions are now categorical distributions, one for each sample in the batch:\n\n\ntest_data_all <- dataset_collect(test_dataset) %>% { .[[1]][[1]]}\n\none_shot_preds <- model(test_data_all) \n\none_shot_preds\n\n\ntfp.distributions.OneHotCategorical(\n \"sequential_one_hot_categorical_OneHotCategorical_OneHotCategorical\",\n batch_shape=[69], event_shape=[6], dtype=float32)\nWe prefixed those predictions with one_shot to indicate their noisy nature: These are predictions obtained on a single pass through the network, all layer weights being sampled from their respective posteriors.\nFrom the predicted distributions, we calculate mean and standard deviation per (test) sample.\n\n\none_shot_means <- tfd_mean(one_shot_preds) %>% \n  as.matrix() %>%\n  as_tibble() %>% \n  mutate(obs = 1:n()) %>% \n  gather(class, mean, -obs) \n\none_shot_sds <- tfd_stddev(one_shot_preds) %>% \n  as.matrix() %>%\n  as_tibble() %>% \n  mutate(obs = 1:n()) %>% \n  gather(class, sd, -obs) \n\nThe standard deviations thus obtained could be said to reflect the overall predictive uncertainty. We can estimate another kind of uncertainty, called epistemic,3 by making a number of passes through the network and then, calculating – again, per test sample – the standard deviations of the predicted means.\n\n\nmc_preds <- purrr::map(1:100, function(x) {\n  preds <- model(test_data_all)\n  tfd_mean(preds) %>% as.matrix()\n})\n\nmc_sds <- abind::abind(mc_preds, along = 3) %>% \n  apply(c(1,2), sd) %>% \n  as_tibble() %>%\n  mutate(obs = 1:n()) %>% \n  gather(class, mc_sd, -obs) \n\nPutting it all together, we have\n\n\npred_data <- one_shot_means %>%\n  inner_join(one_shot_sds, by = c(\"obs\", \"class\")) %>% \n  inner_join(mc_sds, by = c(\"obs\", \"class\")) %>% \n  right_join(one_hot_to_label, by = \"class\") %>% \n  arrange(obs)\n\npred_data\n\n\n# A tibble: 414 x 6\n     obs class       mean      sd    mc_sd label       \n   <int> <chr>      <dbl>   <dbl>    <dbl> <fct>       \n 1     1 V1    0.945      0.227   0.0743   STAND_TO_SIT\n 2     1 V2    0.0534     0.225   0.0675   SIT_TO_STAND\n 3     1 V3    0.00114    0.0338  0.0346   SIT_TO_LIE  \n 4     1 V4    0.00000238 0.00154 0.000336 LIE_TO_SIT  \n 5     1 V5    0.0000132  0.00363 0.00164  STAND_TO_LIE\n 6     1 V6    0.0000305  0.00553 0.00398  LIE_TO_STAND\n 7     2 V1    0.993      0.0813  0.149    STAND_TO_SIT\n 8     2 V2    0.00153    0.0390  0.102    SIT_TO_STAND\n 9     2 V3    0.00476    0.0688  0.108    SIT_TO_LIE  \n10     2 V4    0.00000172 0.00131 0.000613 LIE_TO_SIT  \n# … with 404 more rows\nComparing predictions to the ground truth:\n\n\neval_table <- pred_data %>% \n  group_by(obs) %>% \n  summarise(\n    maxprob = max(mean),\n    maxprob_sd = sd[mean == maxprob],\n    maxprob_mc_sd = mc_sd[mean == maxprob],\n    predicted = label[mean == maxprob]\n  ) %>% \n  mutate(\n    truth = test_data$activityName,\n    correct = truth == predicted\n  ) \n\neval_table %>% print(n = 20)\n\n\n# A tibble: 69 x 7\n     obs maxprob maxprob_sd maxprob_mc_sd predicted    truth        correct\n   <int>   <dbl>      <dbl>         <dbl> <fct>        <fct>        <lgl>  \n 1     1   0.945     0.227         0.0743 STAND_TO_SIT STAND_TO_SIT TRUE   \n 2     2   0.993     0.0813        0.149  STAND_TO_SIT STAND_TO_SIT TRUE   \n 3     3   0.733     0.443         0.131  STAND_TO_SIT STAND_TO_SIT TRUE   \n 4     4   0.796     0.403         0.138  STAND_TO_SIT STAND_TO_SIT TRUE   \n 5     5   0.843     0.364         0.358  SIT_TO_STAND STAND_TO_SIT FALSE  \n 6     6   0.816     0.387         0.176  SIT_TO_STAND STAND_TO_SIT FALSE  \n 7     7   0.600     0.490         0.370  STAND_TO_SIT STAND_TO_SIT TRUE   \n 8     8   0.941     0.236         0.0851 STAND_TO_SIT STAND_TO_SIT TRUE   \n 9     9   0.853     0.355         0.274  SIT_TO_STAND STAND_TO_SIT FALSE  \n10    10   0.961     0.195         0.195  STAND_TO_SIT STAND_TO_SIT TRUE   \n11    11   0.918     0.275         0.168  STAND_TO_SIT STAND_TO_SIT TRUE   \n12    12   0.957     0.203         0.150  STAND_TO_SIT STAND_TO_SIT TRUE   \n13    13   0.987     0.114         0.188  SIT_TO_STAND SIT_TO_STAND TRUE   \n14    14   0.974     0.160         0.248  SIT_TO_STAND SIT_TO_STAND TRUE   \n15    15   0.996     0.0657        0.0534 SIT_TO_STAND SIT_TO_STAND TRUE   \n16    16   0.886     0.318         0.0868 SIT_TO_STAND SIT_TO_STAND TRUE   \n17    17   0.773     0.419         0.173  SIT_TO_STAND SIT_TO_STAND TRUE   \n18    18   0.998     0.0444        0.222  SIT_TO_STAND SIT_TO_STAND TRUE   \n19    19   0.885     0.319         0.161  SIT_TO_STAND SIT_TO_STAND TRUE   \n20    20   0.930     0.255         0.271  SIT_TO_STAND SIT_TO_STAND TRUE   \n# … with 49 more rows\nAre standard deviations higher for misclassifications?\n\n\neval_table %>% \n  group_by(truth, predicted) %>% \n  summarise(avg_mean = mean(maxprob),\n            avg_sd = mean(maxprob_sd),\n            avg_mc_sd = mean(maxprob_mc_sd)) %>% \n  mutate(correct = truth == predicted) %>%\n  arrange(avg_mc_sd) \n\n\n# A tibble: 2 x 5\n  correct count avg_mean avg_sd avg_mc_sd\n  <lgl>   <int>    <dbl>  <dbl>     <dbl>\n1 FALSE      19    0.775  0.380     0.237\n2 TRUE       50    0.879  0.264     0.183\nThey are; though perhaps not to the extent we might desire.\nWith just six classes, we can also inspect standard deviations on the individual prediction-target pairings level.\n\n\neval_table %>% \n  group_by(truth, predicted) %>% \n  summarise(cnt = n(),\n            avg_mean = mean(maxprob),\n            avg_sd = mean(maxprob_sd),\n            avg_mc_sd = mean(maxprob_mc_sd)) %>% \n  mutate(correct = truth == predicted) %>%\n  arrange(desc(cnt), avg_mc_sd) \n\n\n# A tibble: 14 x 7\n# Groups:   truth [6]\n   truth        predicted      cnt avg_mean avg_sd avg_mc_sd correct\n   <fct>        <fct>        <int>    <dbl>  <dbl>     <dbl> <lgl>  \n 1 SIT_TO_STAND SIT_TO_STAND    12    0.935  0.205    0.184  TRUE   \n 2 STAND_TO_SIT STAND_TO_SIT     9    0.871  0.284    0.162  TRUE   \n 3 LIE_TO_SIT   LIE_TO_SIT       9    0.765  0.377    0.216  TRUE   \n 4 SIT_TO_LIE   SIT_TO_LIE       8    0.908  0.254    0.187  TRUE   \n 5 STAND_TO_LIE STAND_TO_LIE     7    0.956  0.144    0.132  TRUE   \n 6 LIE_TO_STAND LIE_TO_STAND     5    0.809  0.353    0.227  TRUE   \n 7 SIT_TO_LIE   STAND_TO_LIE     4    0.685  0.436    0.233  FALSE  \n 8 LIE_TO_STAND SIT_TO_STAND     4    0.909  0.271    0.282  FALSE  \n 9 STAND_TO_LIE SIT_TO_LIE       3    0.852  0.337    0.238  FALSE  \n10 STAND_TO_SIT SIT_TO_STAND     3    0.837  0.368    0.269  FALSE  \n11 LIE_TO_STAND LIE_TO_SIT       2    0.689  0.454    0.233  FALSE  \n12 LIE_TO_SIT   STAND_TO_SIT     1    0.548  0.498    0.0805 FALSE  \n13 SIT_TO_STAND LIE_TO_STAND     1    0.530  0.499    0.134  FALSE  \n14 LIE_TO_SIT   LIE_TO_STAND     1    0.824  0.381    0.231  FALSE  \nAgain, we see higher standard deviations for wrong predictions, but not to a high degree.\nConclusion\nWe’ve shown how to build, train, and obtain predictions from a fully variational convnet. Evidently, there is room for experimentation: Alternative layer implementations exist4; a different prior could be specified; the divergence could be calculated differently; and the usual neural network hyperparameter tuning options apply.\nThen, there’s the question of consequences (or: decision making). What is going to happen in high-uncertainty cases, what even is a high-uncertainty case? Naturally, questions like these are out-of-scope for this post, yet of essential importance in real-world applications. Thanks for reading!\nAppendix\nTo be executed before running this post’s code. Copied from Classifying physical activity from smartphone data.\n\n\nlibrary(keras)     \nlibrary(tidyverse) \n\nactivity_labels <- read.table(\"data/activity_labels.txt\", \n                             col.names = c(\"number\", \"label\")) \n\none_hot_to_label <- activity_labels %>% \n  mutate(number = number - 7) %>% \n  filter(number >= 0) %>% \n  mutate(class = paste0(\"V\",number + 1)) %>% \n  select(-number)\n\nlabels <- read.table(\n  \"data/RawData/labels.txt\",\n  col.names = c(\"experiment\", \"userId\", \"activity\", \"startPos\", \"endPos\")\n)\n\ndataFiles <- list.files(\"data/RawData\")\ndataFiles %>% head()\n\nfileInfo <- data_frame(\n  filePath = dataFiles\n) %>%\n  filter(filePath != \"labels.txt\") %>%\n  separate(filePath, sep = '_',\n           into = c(\"type\", \"experiment\", \"userId\"),\n           remove = FALSE) %>%\n  mutate(\n    experiment = str_remove(experiment, \"exp\"),\n    userId = str_remove_all(userId, \"user|\\\\.txt\")\n  ) %>%\n  spread(type, filePath)\n\n# Read contents of single file to a dataframe with accelerometer and gyro data.\nreadInData <- function(experiment, userId){\n  genFilePath = function(type) {\n    paste0(\"data/RawData/\", type, \"_exp\",experiment, \"_user\", userId, \".txt\")\n  }\n  bind_cols(\n    read.table(genFilePath(\"acc\"), col.names = c(\"a_x\", \"a_y\", \"a_z\")),\n    read.table(genFilePath(\"gyro\"), col.names = c(\"g_x\", \"g_y\", \"g_z\"))\n  )\n}\n\n# Function to read a given file and get the observations contained along\n# with their classes.\nloadFileData <- function(curExperiment, curUserId) {\n\n  # load sensor data from file into dataframe\n  allData <- readInData(curExperiment, curUserId)\n  extractObservation <- function(startPos, endPos){\n    allData[startPos:endPos,]\n  }\n\n  # get observation locations in this file from labels dataframe\n  dataLabels <- labels %>%\n    filter(userId == as.integer(curUserId),\n           experiment == as.integer(curExperiment))\n\n  # extract observations as dataframes and save as a column in dataframe.\n  dataLabels %>%\n    mutate(\n      data = map2(startPos, endPos, extractObservation)\n    ) %>%\n    select(-startPos, -endPos)\n}\n\n# scan through all experiment and userId combos and gather data into a dataframe.\nallObservations <- map2_df(fileInfo$experiment, fileInfo$userId, loadFileData) %>%\n  right_join(activityLabels, by = c(\"activity\" = \"number\")) %>%\n  rename(activityName = label)\n\nwrite_rds(allObservations, \"allObservations.rds\")\n\nallObservations <- readRDS(\"allObservations.rds\")\n\ndesiredActivities <- c(\n  \"STAND_TO_SIT\", \"SIT_TO_STAND\", \"SIT_TO_LIE\", \n  \"LIE_TO_SIT\", \"STAND_TO_LIE\", \"LIE_TO_STAND\"  \n)\n\nfilteredObservations <- allObservations %>% \n  filter(activityName %in% desiredActivities) %>% \n  mutate(observationId = 1:n())\n\n# get all users\nuserIds <- allObservations$userId %>% unique()\n\n# randomly choose 24 (80% of 30 individuals) for training\nset.seed(42) # seed for reproducibility\ntrainIds <- sample(userIds, size = 24)\n\n# set the rest of the users to the testing set\ntestIds <- setdiff(userIds,trainIds)\n\n# filter data. \n# note S.K.: renamed to train_data for consistency with \n# variable naming used in this post\ntrain_data <- filteredObservations %>% \n  filter(userId %in% trainIds)\n\n# note S.K.: renamed to test_data for consistency with \n# variable naming used in this post\ntest_data <- filteredObservations %>% \n  filter(userId %in% testIds)\n\n# note S.K.: renamed to pad_size for consistency with \n# variable naming used in this post\npad_size <- trainData$data %>% \n  map_int(nrow) %>% \n  quantile(p = 0.98) %>% \n  ceiling()\n\n# note S.K.: renamed to one_hot_classes for consistency with \n# variable naming used in this post\none_hot_classes <- . %>% \n  {. - 7} %>%        # bring integers down to 0-6 from 7-12\n  to_categorical()   # One-hot encode\n\n\n\nReyes-Ortiz, Jorge-L., Luca Oneto, Albert Samà, Xavier Parra, and Davide Anguita. 2016. “Transition-Aware Human Activity Recognition Using Smartphones.” Neurocomput. 171 (C): 754–67. https://doi.org/10.1016/j.neucom.2015.07.085.\n\n\nsee Winner takes all: A look at activations and cost functions↩︎\nor two, depending on the application↩︎\nsee Adding uncertainty estimates to Keras models with tfprobability↩︎\ne.g., layer_conv_1d_reparameterization, layer_dense_local_reparameterization↩︎\n",
    "preview": "posts/2019-11-13-variational-convnet/images/bbb.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 796,
    "preview_height": 378
  },
  {
    "path": "posts/2019-11-07-tfp-cran/",
    "title": "tfprobability 0.8 on CRAN: Now how can you use it?",
    "description": "Part of the r-tensorflow ecosystem, tfprobability is an R wrapper to TensorFlow Probability, the Python probabilistic programming framework developed by Google. We take the occasion of tfprobability's acceptance on CRAN to give a high-level introduction, highlighting interesting use cases and applications.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-11-07",
    "categories": [
      "Probabilistic ML/DL",
      "Packages/Releases",
      "TensorFlow/Keras"
    ],
    "contents": "\nAbout a week ago, tfprobability 0.8 was accepted on CRAN. While we’ve been using this package quite frequently already on this blog, in this post we’d like to (re-)introduce it on a high level, especially addressing new users.\ntfprobability, what is it?\ntfprobability is an R wrapper for TensorFlow Probability, a Python library built on top of the TensorFlow framework. So now the question is, what is TensorFlow Probability?\nIf – let’s call it “probabilistic programming” – is not something you do every day, an enumeration of features, or even a hierarchical listing of modules as given on the TensorFlow Probability website might leave you a bit helpless, informative though it may be.\nLet’s start from use cases instead. We’ll look at three high-level example scenarios, before rounding up with a quick tour of more basic building blocks provided by TFP. (Short note aside: We’ll use TFP as an acronym for the Python library as well as the R package, unless we’re referring to the R wrapper specifically, in which case we’ll say tfprobability).\nUse case 1: Extending deep learning\nWe start with the type of use case that might be the most interesting to the majority of our readers: extending deep learning.\nDistribution layers\nIn deep learning, usually output layers are deterministic. True, in classification we are used to talking about “class probabilities”. Take the multi-class case: We may attribute to the network the conclusion, “with 80% probability this is a Bernese mountain dog” – but we can only do this because the last layer’s output has been squished, by a softmax activation, to values between \\(0\\) and \\(1\\). Nonetheless, the actual output is a tensor (a number).\nTFP, however, augments TensorFlow by means of distribution layers: a hybrid species that can be used just like a normal TensorFlow/Keras layer but that, internally, contains the defining characteristics of some probability distribution.\nConcretely, for multi-class classification, we could use a categorical layer (layer_one_hot_categorical), replacing something like\n\n\nlayer_dense(\n  num_classes, \n  activation = \"softmax\"\n)\n\nby\n\n\nlayer_one_hot_categorical(event_size = num_classes)\n\nThe model, thus modified, now outputs a distribution, not a tensor. However, it can still be trained passing “normal” target tensors. This is what was meant by use like a normal layer, above: TFP will take the distribution, obtain a tensor from it 1, and compare that to the target. The other side of the layer’s personality is seen when generating predictions: Calling the model on fresh data will result in a bunch of distributions, one for every data point. You then call tfd_mean to elicit actual predictions:\n\n\npred_dists <- model(x_test)\npred_means <- pred_dists %>% tfd_mean()\n\nYou may be wondering, what good is this? One way or the other, we’ll decide on picking the class with the highest probability, right?\nRight, but there are a number of interesting things you can do with these layers. We’ll quickly introduce three well-known ones here, but wouldn’t be surprised if we saw a lot more emerging in the near future.\nVariational autoencoders, the elegant way\nVariational autoencoders are a prime example of something that got way easier to code when TF-2 style custom models and custom training appeared (TF-2 style, not TF-2, as these techniques actually became available more than a year before TF 2 was finally released). 2\nEvolutionarily, the next step was to use TFP distributions 3, but back in the time some fiddling was required, as distributions could not yet alias as layers.\nDue to those hybrids though, we now can do something like this 4:\n\n\n# encoder\nencoder_model <- keras_model_sequential() %>%\n  [...] %>%\n  layer_multivariate_normal_tri_l(event_size = encoded_size) %>%\n  layer_kl_divergence_add_loss([...])\n\n# decoder\ndecoder_model <- keras_model_sequential() %>%\n  [...] %>%\n layer_independent_bernoulli([...])\n\n# complete VAE\nvae_model <- keras_model(inputs = encoder_model$inputs,\n                         outputs = decoder_model(encoder_model$outputs[1]))\n\n# loss function\nvae_loss <- function (x, rv_x) - (rv_x %>% tfd_log_prob(x))\n\nBoth the encoder and the decoder are “just” sequential models, joined through the functional API(keras_model). The loss is just the negative log-probability of the data given the model. So where is the other part of the (negative) ELBO, the KL divergence? It is implicit in the encoder’s output layer, layer_kl_divergence_add_loss.\nOur two other examples involve quantifying uncertainty.\nLearning the spread in the data\nIf a model’s last layer wraps a distribution parameterized by location and scale, like the normal distribution, we can train the network to learn not just the mean, but also the spread in the data:\n\n\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 8, activation = \"relu\") %>%\n  layer_dense(units = 2, activation = \"linear\") %>%\n  layer_distribution_lambda(function(x)\n    tfd_normal(# use unit 1 of previous layer\n               loc = x[, 1, drop = FALSE],\n               # use unit 2 of previous layer\n               scale = 1e-3 + tf$math$softplus(x[, 2, drop = FALSE])\n               )\n  )\n\nIn essence, this means the network’s predictions will reflect any existing heteroscedasticity in the data. Here is an example: Given simulated training data of shape\n\n\n\nFigure 1: Simulated data (from https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/)\n\n\n\nthe network’s predictions show the same spread:\n\n\n\nFigure 2: Aleatoric uncertainty on simulated data (from https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/)\n\n\n\nPlease see Adding uncertainty estimates to Keras models with tfprobability for a detailed explanation.\nUsing a normal distribution layer as the output, we can capture irreducible variability in the data, also known as aleatoric uncertainty. A different type of probabilistic layer allows to model what is called epistemic uncertainty.\nPutting distributions over network weights\nUsing variational layers, we can make neural networks probabilistic. A simple example could look like so:\n\n\nmodel <- keras_model_sequential() %>%\n  layer_dense_variational(\n    units = 1,\n    make_posterior_fn = posterior_mean_field,\n    make_prior_fn = prior_trainable,\n    kl_weight = 1 / n\n  ) %>%\n  layer_distribution_lambda(function(x)\n    tfd_normal(loc = x, scale = 1))\n\nThis defines a network with a single dense layer, containing a single neuron, that has a prior distribution put over its weights. The network will be trained to minimize the KL divergence between that prior and an approximate posterior weight distribution, as well as maximize the probability of the data under the posterior weights. (For details, please again see the aforementioned post.)\nAs a consequence of this setup, each test run will now yield different predictions. For the above simulated data, we might get an ensemble of predictions, like so:\n\n\n\nFigure 3: Epistemic uncertainty on simulated data (from https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/)\n\n\n\nVariational layers for non-dense layers exist, and we’ll see an example next week. Now let’s move on to the next type of use cases, from big data to small data, in a way.\nUse case 2: Fitting Bayesian models with Monte Carlo methods\nIn sciences where data aren’t abound, Markov Chain Monte Carlo (MCMC) methods are common. We’ve shown some examples how to this with TFP (Tadpoles on TensorFlow: Hierarchical partial pooling with tfprobability, Hierarchical partial pooling, continued: Varying slopes models with TensorFlow Probability, Modeling censored data with tfprobability, best read in this order), as well as tried to explain, in an accessible way, some of the background (On leapfrogs, crashing satellites, and going nuts: A very first conceptual introduction to Hamiltonian Monte Carlo).\nMCMC software may roughly be divided into two flavors: “low-level” and “high-level”. Low-level software, like Stan – or TFP, for that matter – requires you to write code in either some programming language, or in a DSL that is pretty close in syntax and semantics to an existing programming language. High-level tools, on the other hand, are DSLs that resemble the way you’d express a model using mathematical notation. (Put differently, the former read like C or Python; the latter read like LaTeX.)\nIn general, low-level software tends to offer more flexibility, while high-level interfaces may be more convenient to use and easier to learn. To start with MCMC in TFP, we recommend checking out the first of the posts listed above, Tadpoles on TensorFlow: Hierarchical partial pooling with tfprobability. If you prefer a higher-level interface, you might be interested in greta, which is built on TFP.\nOur last use case is of a Bayesian nature as well.\nUse case 3: State space models\nState space models are a perhaps lesser used, but highly conceptually attractive way of performing inference and prediction on signals evolving in time. Dynamic linear models with tfprobability is an introduction to dynamic linear models with TFP, showcasing two of their (many) great strengths: ease of performing dynamic regression and additive (de)composition.\nIn dynamic regression, coefficients are allowed to vary over time. Here is an example from the above-mentioned post showing, for both a single predictor and the regression intercept, the filtered (in the sense of Kálmán filtering) estimates over time:\n\n\n\nFigure 4: Filtering estimates from the Kálmán filter (from: https://blogs.rstudio.com/tensorflow/posts/2019-06-25-dynamic_linear_models_tfprobability/).\n\n\n\nAnd here is the ubiquitous AirPassengers dataset, decomposed into a trend and a seasonal component:\n\n\n\nFigure 5: AirPassengers, decomposition into a linear trend and a seasonal component (from: https://blogs.rstudio.com/tensorflow/posts/2019-06-25-dynamic_linear_models_tfprobability/).\n\n\n\nIf this interests you, you might want to take a look at available state space models. Given how rapidly TFP is evolving, plus the model-inherent composability, we expect the number of options in this area to grow quite a bit.\nThat is it for our tour of use cases. To wrap up, let’s talk about the basic building blocks of TFP.\nThe basics: Distributions and bijectors\nNo probabilistic framework without probability distributions – that’s for sure. In release 0.8, TFP has about 80 distributions. But what are bijectors?\nBijectors are invertible, differentiable maps. Getting into the flow: Bijectors in TensorFlow Probability introduces the main ideas and shows how to chain such bijective transformations into a flow. Bijectors are being used by TFP internally all the time, and as a user too you’ll likely encounter situations where you need them.\nOne example is doing MCMC (for example, Hamiltonian Monte Carlo) with TFP. In your model, you might have a prior on a standard deviation. Standard deviations are positive, so you’d like to specify, for example, an exponential distribution for it, resulting in exclusively positive values. But Hamiltonian Monte Carlo has to run in unconstrained space to work. This is where a bijector comes in, mapping between the two spaces used for model specification and sampling.\nFor bijectors too, there are many of them - about 40, ranging from straightforward affine maps to more complex operations on Cholesky factors or the discrete cosine transform.\nTo see both building blocks in action, let’s end with a “Hello World” of TFP, or two, rather. Here first is the direct way to obtain samples from a standard normal distribution.\n\n\nlibrary(tfprobability)\n\n# create a normal distribution\nd <- tfd_normal(loc = 0, scale = 1)\n# sample from it\nd %>% tfd_sample(3)\n\n\ntf.Tensor([-1.0863057  -0.61655647  1.8151687 ], shape=(3,), dtype=float32)\nIn case you thought that was too easy, here’s how to do the same using a bijector instead of a distribution.\n\n\n# generate 3 values uniformly distributed between 0 and 1\nu <- runif(3)\n\n# a bijector that in the inverse direction, maps values between 0 and 1\n# to a normal distribution\nb <- tfb_normal_cdf()\n\n# call bijector's inverse transform op\nb %>% tfb_inverse(u) \n\n\ntf.Tensor([ 0.96157753  1.0103974  -1.4986734 ], shape=(3,), dtype=float32)\nWith this we conclude our introduction. If you run into problems using tfprobability, or have questions about it, please open an issue in the github repo. Thanks for reading!\nBy default, just sampling from the distribution – but this is something the user can influence if desired, making use of the convert_to_tensor_fn argument.↩︎\nSee Representation learning with MMD-VAE for an example.↩︎\nDone so, for example, in Getting started with TensorFlow Probability from R.↩︎\nFor a complete running example, see the tfprobability README.↩︎\n",
    "preview": "posts/2019-11-07-tfp-cran/images/tfprobability.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 518,
    "preview_height": 600
  },
  {
    "path": "posts/2019-10-23-gpt-2/",
    "title": "Innocent unicorns considered harmful? How to experiment with GPT-2 from R",
    "description": "Is society ready to deal with challenges brought about by artificially-generated  information - fake images, fake videos, fake text? While this post won't answer that question, it should help form an opinion on the threat exerted by fake text as of this writing, autumn 2019.  We introduce gpt2, an R package that wraps OpenAI's public implementation of GPT-2, the language model that early this year surprised the NLP community with the unprecedented quality of its creations.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      },
      {
        "name": "Javier Luraschi",
        "url": {}
      }
    ],
    "date": "2019-10-23",
    "categories": [
      "Natural Language Processing",
      "Packages/Releases"
    ],
    "contents": "\nWhen this year in February, OpenAI presented GPT-2(Radford et al. 2019), a large Transformer-based language model trained on an enormous amount of web-scraped text, their announcement caught great attention, not just in the NLP community. This was primarily due to two facts. First, the samples of generated text were stunning.\nPresented with the following input\n\nIn a shocking finding, scientist [sic] discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\nthis was how the model continued:\n\nThe scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science. Now, after almost two centuries, the mystery of what sparked this odd phenomenon is finally solved. Dr. Jorge Pérez, an evolutionary biologist from the University of La Paz, and several companions, were exploring the Andes Mountains when they found a small valley, with no other animals or humans. Pérez noticed that the valley had what appeared to be a natural fountain, surrounded by two peaks of rock and silver snow. […]\n\nSecond, “due to our concerns about malicious applications” (quote) they didn’t release the full model, but a smaller one that has less than one tenth the number of parameters. Neither did they make public the dataset, nor the training code.\nWhile at first glance, this may look like a marketing move (we created something so powerful that it’s too dangerous to be released to the public!), let’s not make things that easy on ourselves.\nWith great power …\nWhatever your take on the “innate priors in deep learning” discussion – how much knowledge needs to be hardwired into neural networks for them to solve tasks that involve more than pattern matching? – there is no doubt that in many areas, systems driven by “AI”1 will impact our lives in an essential, and ever more powerful, way. Although there may be some awareness of the ethical, legal, and political problems this poses, it is probably fair to say that by and large, society is closing its eyes and holding its hands over its ears.\nIf you were a deep learning researcher working in an area susceptible to abuse, generative ML say, what options would you have? As always in the history of science, what can be done will be done; all that remains is the search for antidotes. You may doubt that on a political level, constructive responses could evolve. But you can encourage other researchers to scrutinize the artifacts your algorithm created and develop other algorithms designed to spot the fakes – essentially like in malware detection. Of course this is a feedback system: Like with GANs, impostor algorithms will happily take the feedback and go on working on their shortcomings. But still, deliberately entering this circle might be the only viable action to take.\nAlthough it may be the first thing that comes to mind, the question of veracity here isn’t the only one. With ML systems, it’s always: garbage in - garbage out. What is fed as training data determines the quality of the output, and any biases in its upbringing will carry through to an algorithm’s grown-up behavior. Without interventions, software designed to do translation, autocompletion and the like will be biased.2\nIn this light, all we can sensibly do is – constantly – point out the biases, analyze the artifacts, and conduct adversarial attacks. These are the kinds of responses OpenAI was asking for. In appropriate modesty, they called their approach an experiment. Put plainly, no-one today knows how to deal with the threats emerging from powerful AI appearing in our lives. But there is no way around exploring our options.\nThe story unwinding\nThree months later, OpenAI published an update to the initial post, stating that they had decided on a staged-release strategy. In addition to making public the next-in-size, 355M-parameters version of the model, they also released a dataset of generated outputs from all model sizes, to facilitate research. Last not least, they announced partnerships with academic and non-academic institutions, to increase “societal preparedness” (quote).\nAgain after three months, in a new post OpenAI announced the release of a yet larger – 774M-parameter – version of the model. At the same time, they reported evidence demonstrating insufficiencies in current statistical fake detection, as well as study results suggesting that indeed, text generators exist that can trick humans.\nDue to those results, they said, no decision had yet been taken as to the release of the biggest, the “real” model, of size 1.5 billion parameters.\nGPT-2\nSo what is GPT-2? Among state-of-the-art NLP models, GPT-2 stands out due to the gigantic (40G) dataset it was trained on, as well as its enormous number of weights. The architecture, in contrast, wasn’t new when it appeared. GPT-2, as well as its predecessor GPT (Radford 2018), is based on a transformer architecture.\nThe original Transformer (Vaswani et al. 2017) is an encoder-decoder architecture designed for sequence-to-sequence tasks, like machine translation. The paper introducing it was called “Attention is all you need”, emphasizing – by absence – what you don’t need: RNNs.\nBefore its publication, the prototypical model for e.g. machine translation would use some form of RNN as an encoder, some form of RNN as a decoder, and an attention mechanism that at each time step of output generation, told the decoder where in the encoded input to look. Now the transformer was disposing with RNNs, essentially replacing them by a mechanism called self-attention where already during encoding, the encoder stack would encode each token not independently, but as a weighted sum of tokens encountered before (including itself).3\nMany subsequent NLP models built on the Transformer, but – depending on purpose – either picked up the encoder stack only, or just the decoder stack. GPT-2 was trained to predict consecutive words in a sequence. It is thus a language model, a term resounding the conception that an algorithm which can predict future words and sentences somehow has to understand language (and a lot more, we might add). As there is no input to be encoded (apart from an optional one-time prompt), all that is needed is the stack of decoders.\nIn our experiments, we’ll be using the biggest as-yet released pretrained model, but this being a pretrained model our degrees of freedom are limited. We can, of course, condition on different input prompts. In addition, we can influence the sampling algorithm used.\nSampling options with GPT-2\nWhenever a new token is to be predicted, a softmax is taken over the vocabulary.4 Directly taking the softmax output amounts to maximum likelihood estimation. In reality, however, always choosing the maximum likelihood estimate results in highly repetitive output.\nA natural option seems to be using the softmax outputs as probabilities: Instead of just taking the argmax, we sample from the output distribution. Unfortunately, this procedure has negative ramifications of its own. In a big vocabulary, very improbable words together make up a substantial part of the probability mass; at every step of generation, there is thus a non-negligible probability that an improbable word may be chosen. This word will now exert great influence on what is chosen next. In that manner, highly improbable sequences can build up.\nThe task thus is to navigate between the Scylla of determinism and the Charybdis of weirdness. With the GPT-2 model presented below, we have three options:\nvary the temperature (parameter temperature);\nvary top_k, the number of tokens considered; or\nvary top_p, the probability mass considered.\nThe temperature concept is rooted in statistical mechanics. Looking at the Boltzmann distribution used to model state probabilities \\(p_i\\)dependent on energy \\(\\epsilon_i\\):\n\\[p_i \\sim e^{-\\frac{\\epsilon_i}{kT}}\\]\nwe see there is a moderating variable temperature \\(T\\)5 that dependent on whether it’s below or above 1, will exert an either amplifying or attenuating influence on differences between probabilities.\nAnalogously, in the context of predicting the next token, the individual logits are scaled by the temperature, and only then is the softmax taken. Temperatures below zero would make the model even more rigorous in choosing the maximum likelihood candidate; instead, we’d be interested in experimenting with temperatures above 1 to give higher chances to less likely candidates – hopefully, resulting in more human-like text.\nIn top-\\(k\\) sampling, the softmax outputs are sorted, and only the top-\\(k\\) tokens are considered for sampling. The difficulty here is how to choose \\(k\\). Sometimes a few words make up for almost all probability mass, in which case we’d like to choose a low number; in other cases the distribution is flat, and a higher number would be adequate.\nThis sounds like rather than the number of candidates, a target probability mass should be specified. This is the approach suggested by (Holtzman et al. 2019). Their method, called top-\\(p\\), or Nucleus sampling, computes the cumulative distribution of softmax outputs and picks a cut-off point \\(p\\). Only the tokens constituting the top-\\(p\\) portion of probability mass is retained for sampling.\nNow all you need to experiment with GPT-2 is the model.\nSetup\nInstall gpt2 from github:\n\n\nremotes::install_github(\"r-tensorflow/gpt2\")\n\nThe R package being a wrapper to the implementation provided by OpenAI, we then need to install the Python runtime.\n\n\ngpt2::install_gpt2(envname = \"r-gpt2\")\n\nThis command will also install TensorFlow into the designated environment. All TensorFlow-related installation options (resp. recommendations) apply. Python 3 is required.\nWhile OpenAI indicates a dependency on TensorFlow 1.12, the R package was adapted to work with more current versions. The following versions have been found to be working fine:\nif running on GPU: TF 1.15\nCPU-only: TF 2.0\nUnsurprisingly, with GPT-2, running on GPU vs. CPU makes a huge difference.\nAs a quick test if installation was successful, just run gpt2() with the default parameters:\n\n\n# equivalent to:\n# gpt2(prompt = \"Hello my name is\", model = \"124M\", seed = NULL, batch_size = 1, total_tokens = NULL,\n#      temperature = 1, top_k = 0, top_p = 1)\n# see ?gpt2 for an explanation of the parameters\n#\n# available models as of this writing: 124M, 355M, 774M\n#\n# on first run of a given model, allow time for download\ngpt2()\n\nThings to try out\nSo how dangerous exactly is GPT-2? We can’t say, as we don’t have access to the “real” model. But we can compare outputs, given the same prompt, obtained from all available models. The number of parameters has approximately doubled at every release – 124M, 355M, 774M. The biggest, yet unreleased, model, again has twice the number of weights: about 1.5B. In light of the evolution we observe, what do we expect to get from the 1.5B version?\nIn performing these kinds of experiments, don’t forget about the different sampling strategies explained above. Non-default parameters might yield more real-looking results.\nNeedless to say, the prompt we specify will make a difference. The models have been trained on a web-scraped dataset, subject to the quality criterion “3 stars on reddit”. We expect more fluency in certain areas than in others, to put it in a cautious way.\nMost definitely, we expect various biases in the outputs.\nUndoubtedly, by now the reader will have her own ideas about what to test. But there is more.\n“Language Models are Unsupervised Multitask Learners”\nHere we are citing the title of the official GPT-2 paper (Radford et al. 2019). What is that supposed to mean? It means that a model like GPT-2, trained to predict the next token in naturally occurring text, can be used to “solve” standard NLP tasks that, in the majority of cases, are approached via supervised training (translation, for example).\nThe clever idea is to present the model with cues about the task at hand. Some information on how to do this is given in the paper; more (unofficial; conflicting or confirming) hints can be found on the net. From what we found, here are some things you could try.\nSummarization\nThe clue to induce summarization is “TL;DR:”, written on a line by itself. The authors report that this worked best setting top_k = 2 and asking for 100 tokens. Of the generated output, they took the first three sentences as a summary.\nTo try this out, we chose a sequence of content-wise standalone paragraphs from a NASA website dedicated to climate change, the idea being that with a clearly structured text like this, it should be easier to establish relationships between input and output.\n\n# put this in a variable called text\n\nThe planet's average surface temperature has risen about 1.62 degrees Fahrenheit\n(0.9 degrees Celsius) since the late 19th century, a change driven largely by\nincreased carbon dioxide and other human-made emissions into the atmosphere.4 Most\nof the warming occurred in the past 35 years, with the five warmest years on record\ntaking place since 2010. Not only was 2016 the warmest year on record, but eight of\nthe 12 months that make up the year — from January through September, with the\nexception of June — were the warmest on record for those respective months.\n\nThe oceans have absorbed much of this increased heat, with the top 700 meters\n(about 2,300 feet) of ocean showing warming of more than 0.4 degrees Fahrenheit\nsince 1969.\n\nThe Greenland and Antarctic ice sheets have decreased in mass. Data from NASA's\nGravity Recovery and Climate Experiment show Greenland lost an average of 286\nbillion tons of ice per year between 1993 and 2016, while Antarctica lost about 127\nbillion tons of ice per year during the same time period. The rate of Antarctica\nice mass loss has tripled in the last decade.\n\nGlaciers are retreating almost everywhere around the world — including in the Alps,\nHimalayas, Andes, Rockies, Alaska and Africa.\n\nSatellite observations reveal that the amount of spring snow cover in the Northern\nHemisphere has decreased over the past five decades and that the snow is melting\nearlier.\n\nGlobal sea level rose about 8 inches in the last century. The rate in the last two\ndecades, however, is nearly double that of the last century and is accelerating\nslightly every year.\n\nBoth the extent and thickness of Arctic sea ice has declined rapidly over the last\nseveral decades.\n\nThe number of record high temperature events in the United States has been\nincreasing, while the number of record low temperature events has been decreasing,\nsince 1950. The U.S. has also witnessed increasing numbers of intense rainfall events.\n\nSince the beginning of the Industrial Revolution, the acidity of surface ocean\nwaters has increased by about 30 percent.13,14 This increase is the result of humans\nemitting more carbon dioxide into the atmosphere and hence more being absorbed into\nthe oceans. The amount of carbon dioxide absorbed by the upper layer of the oceans\nis increasing by about 2 billion tons per year.\n\nTL;DR:\n\n\ngpt2(prompt = text,\n     model = \"774M\",\n     total_tokens = 100,\n     top_k = 2)\n\nHere is the generated result, whose quality on purpose we don’t comment on. (Of course one can’t help having “gut reactions”; but to actually present an evaluation we’d want to conduct a systematic experiment, varying not only input prompts but also function parameters. All we want to show in this post is how you can set up such experiments yourself.)\n\n\"\\nGlobal temperatures are rising, but the rate of warming has been accelerating.\n\\n\\nThe oceans have absorbed much of the increased heat, with the top 700 meters of\nocean showing warming of more than 0.4 degrees Fahrenheit since 1969.\n\\n\\nGlaciers are retreating almost everywhere around the world, including in the\nAlps, Himalayas, Andes, Rockies, Alaska and Africa.\n\\n\\nSatellite observations reveal that the amount of spring snow cover in the\nNorthern Hemisphere has decreased over the past\"\nSpeaking of parameters to vary, – they fall into two classes, in a way. It is unproblematic to vary the sampling strategy, let alone the prompt. But for tasks like summarization, or the ones we’ll see below, it doesn’t feel right to have to tell the model how many tokens to generate. Finding the right length of the answer seems to be part of the task.6 Breaking our “we don’t judge” rule just a single time, we can’t help but remark that even in less clear-cut tasks, language generation models that are meant to approach human-level competence would have to fulfill a criterion of relevance (Grice 1975).\nQuestion answering\nTo trick GPT-2 into question answering, the common approach seems to be presenting it with a number of Q: / A: pairs, followed by a final question and a final A: on its own line.\nWe tried like this, asking questions on the above climate change - related text:\n\n\nq <- str_c(str_replace(text, \"\\nTL;DR:\\n\", \"\"), \" \\n\", \"\nQ: What time period has seen the greatest increase in global temperature? \nA: The last 35 years. \nQ: What is happening to the Greenland and Antarctic ice sheets? \nA: They are rapidly decreasing in mass. \nQ: What is happening to glaciers? \nA: \")\n\ngpt2(prompt = q,\n     model = \"774M\",\n     total_tokens = 10,\n     top_p = 0.9)\n\nThis did not turn out so well.\n\n\"\\nQ: What is happening to the Arctic sea\"\nBut maybe, more successful tricks exist.\nTranslation\nFor translation, the strategy presented in the paper is juxtaposing sentences in two languages, joined by \" = “, followed by a single sentence on its own and a” =\". Thinking that English <-> French might be the combination best represented in the training corpus, we tried the following:\n\n# save this as eng_fr\n\nThe issue of climate change concerns all of us. = La question du changement\nclimatique nous affecte tous. \\n\nThe problems of climate change and global warming affect all of humanity, as well as\nthe entire ecosystem. = Les problèmes créés par les changements climatiques et le\nréchauffement de la planète touchent toute l'humanité, de même que l'écosystème tout\nentier.\\n\nClimate Change Central is a not-for-profit corporation in Alberta, and its mandate\nis to reduce Alberta's greenhouse gas emissions. = Climate Change Central est une\nsociété sans but lucratif de l'Alberta ayant pour mission de réduire les émissions\nde gaz. \\n\nClimate change will affect all four dimensions of food security: food availability,\nfood accessibility, food utilization and food systems stability. = \"\n\ngpt2(prompt = eng_fr,\n     model = \"774M\",\n     total_tokens = 25,\n     top_p = 0.9)\nResults varied a lot between different runs. Here are three examples:\n\n\"ét durant les pages relevantes du Centre d'Action des Sciences Humaines et dans sa\nspecies situé,\"\n\n\"études des loi d'affaires, des reasons de demande, des loi d'abord and de\"\n\n\"étiquettes par les changements changements changements et les bois d'escalier,\nainsi que des\"\nConclusion\nWith that, we conclude our tour of “what to explore with GPT-2”. Keep in mind that the yet-unreleased model has double the number of parameters; essentially, what we see is not what we get.\nThis post’s goal was to show how you can experiment with GPT-2 from R. But it also reflects the decision to, from time to time, widen the narrow focus on technology and allow ourselves to think about ethical and societal implications of ML/DL.\nThanks for reading!\n\n\nGrice, H. P. 1975. “Logic and Conversation.” In Syntax and Semantics: Vol. 3: Speech Acts, 41–58. Academic Press. http://www.ucl.ac.uk/ls/studypacks/Grice-Logic.pdf.\n\n\nHoltzman, Ari, Jan Buys, Maxwell Forbes, and Yejin Choi. 2019. “The Curious Case of Neural Text Degeneration.” arXiv E-Prints, April, arXiv:1904.09751. http://arxiv.org/abs/1904.09751.\n\n\nRadford, Alec. 2018. “Improving Language Understanding by Generative Pre-Training.” In.\n\n\nRadford, Alec, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. “Language Models Are Unsupervised Multitask Learners.”\n\n\nSun, Tony, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth M. Belding, Kai-Wei Chang, and William Yang Wang. 2019. “Mitigating Gender Bias in Natural Language Processing: Literature Review.” CoRR abs/1906.08976. http://arxiv.org/abs/1906.08976.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In Advances in Neural Information Processing Systems 30, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 5998–6008. Curran Associates, Inc. http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\n\n\nThe acronym here is used for convenience only, not to imply any specific view on what is, or is not, “artificial intelligence”.↩︎\nFor an overview of bias detection and mitigation specific to gender bias, see e.g. (Sun et al. 2019)↩︎\nFor a detailed, and exceptionally visual, explanation of the Transformer, the place to go is Jay Alammar’s post. Also check out The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning, the article that might be held mainly responsible for the pervasive sesame-streetification of NLP.↩︎\nFor an introduction to how softmax activation behaves, see Winner takes all: A look at activations and cost functions.↩︎\n\\(k\\) is the Boltzmann constant↩︎\nFormally, total_tokens isn’t a required parameter. If not passed, a default based on model size will be applied, resulting in lengthy output that definitely will have to be processed by some human-made rule.↩︎\n",
    "preview": "posts/2019-10-23-gpt-2/images/thumb.jpg",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-10-08-tf2-whatchanges/",
    "title": "TensorFlow 2.0 is here - what changes for R users?",
    "description": "TensorFlow 2.0 was finally released last week. As R users we have two kinds of questions. First, will my keras code still run? And second, what is it that changes? In this post, we answer both and, then, give a tour of exciting new developments in the r-tensorflow ecosystem.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-10-08",
    "categories": [
      "TensorFlow/Keras",
      "Packages/Releases"
    ],
    "contents": "\nThe wait is over – TensorFlow 2.0 (TF 2) is now officially here! What does this mean for us, users of R packages keras and/or tensorflow, which, as we know, rely on the Python TensorFlow backend?\nBefore we go into details and explanations, here is an all-clear, for the concerned user who fears their keras code might become obsolete (it won’t).\nDon’t panic\nIf you are using keras in standard ways, such as those depicted in most code examples and tutorials seen on the web, and things have been working fine for you in recent keras releases (>= 2.2.4.1), don’t worry. Most everything should work without major changes.\nIf you are using an older release of keras (< 2.2.4.1), syntactically things should work fine as well, but you will want to check for changes in behavior/performance.\nAnd now for some news and background. This post aims to do three things:\nExplain the above all-clear statement. Is it really that simple – what exactly is going on?\nCharacterize the changes brought about by TF 2, from the point of view of the R user.\nAnd, perhaps most interestingly: Take a look at what is going on, in the r-tensorflow ecosystem, around new functionality related to the advent of TF 2.\nSome background\nSo if all still works fine (assuming standard usage), why so much ado about TF 2 in Python land?\nThe difference is that on the R side, for the vast majority of users, the framework you used to do deep learning was keras. tensorflow was needed just occasionally, or not at all.\nBetween keras and tensorflow, there was a clear separation of responsibilities: keras was the frontend, depending on TensorFlow as a low-level backend, just like the original Python Keras it was wrapping did. 1. In some cases, this lead to people using the words keras and tensorflow almost synonymously: Maybe they said tensorflow, but the code they wrote was keras.\nThings were different in Python land. There was original Python Keras, but TensorFlow had its own layers API, and there were a number of third-party high-level APIs built on TensorFlow. Keras, in contrast, was a separate library that just happened to rely on TensorFlow.\nSo in Python land, now we have a big change: With TF 2, Keras (as incorporated in the TensorFlow codebase) is now the official high-level API for TensorFlow. To bring this across has been a major point of Google’s TF 2 information campaign since the early stages.\nAs R users, who have been focusing on keras all the time, we are essentially less affected. Like we said above, syntactically most everything stays the way it was. So why differentiate between different keras versions?\nWhen keras was written, there was original Python Keras, and that was the library we were binding to. However, Google started to incorporate original Keras code into their TensorFlow codebase as a fork, to continue development independently. For a while there were two “Kerases”: Original Keras and tf.keras. Our R keras offered to switch between implementations 2, the default being original Keras.\nIn keras release 2.2.4.1, anticipating discontinuation of original Keras and wanting to get ready for TF 2, we switched to using tf.keras as the default. While in the beginning, the tf.keras fork and original Keras developed more or less in sync, the latest developments for TF 2 brought with them bigger changes in the tf.keras codebase, especially as regards optimizers. This is why, if you are using a keras version < 2.2.4.1, upgrading to TF 2 you will want to check for changes in behavior and/or performance. 3\nThat’s it for some background. In sum, we’re happy most existing code will run just fine. But for us R users, something must be changing as well, right?\nTF 2 in a nutshell, from an R perspective\nIn fact, the most evident-on-user-level change is something we wrote several posts about, more than a year ago 4. By then, eager execution was a brand-new option that had to be turned on explicitly; TF 2 now makes it the default. Along with it came custom models (a.k.a. subclassed models, in Python land) and custom training, making use of tf$GradientTape. Let’s talk about what those termini refer to, and how they are relevant to R users.\nEager Execution\nIn TF 1, it was all about the graph you built when defining your model. The graph, that was – and is – an Abstract Syntax Tree (AST), with operations as nodes and tensors “flowing” along the edges. Defining a graph and running it (on actual data) were different steps.\nIn contrast, with eager execution, operations are run directly when defined.\nWhile this is a more-than-substantial change that must have required lots of resources to implement, if you use keras you won’t notice. Just as previously, the typical keras workflow of create model -> compile model -> train model never made you think about there being two distinct phases (define and run), now again you don’t have to do anything. Even though the overall execution mode is eager, Keras models are trained in graph mode, to maximize performance. We will talk about how this is done in part 3 when introducing the tfautograph package.\nIf keras runs in graph mode, how can you even see that eager execution is “on”? Well, in TF 1, when you ran a TensorFlow operation on a tensor 5, like so\n\n\nlibrary(tensorflow)\ntf$math$cumprod(1:5)\n\nthis is what you saw:\n\nTensor(\"Cumprod:0\", shape=(5,), dtype=int32)\nTo extract the actual values, you had to create a TensorFlow Session and run the tensor, or alternatively, use keras::k_eval that did this under the hood:\n\n\nlibrary(keras)\ntf$math$cumprod(1:5) %>% k_eval()\n\n\n[1]   1   2   6  24 120\nWith TF 2’s execution mode defaulting to eager, we now automatically see the values contained in the tensor: 6\n\n\ntf$math$cumprod(1:5)\n\n\ntf.Tensor([  1   2   6  24 120], shape=(5,), dtype=int32)\nSo that’s eager execution. In our last year’s Eager-category blog posts, it was always accompanied by custom models, so let’s turn there next.\nCustom models\nAs a keras user, probably you’re familiar with the sequential and functional styles of building a model. Custom models allow for even greater flexibility than functional-style ones. Check out the documentation for how to create one.\nLast year’s series on eager execution has plenty of examples using custom models, featuring not just their flexibility, but another important aspect as well: the way they allow for modular, easily-intelligible code. 7\nEncoder-decoder scenarios are a natural match. If you have seen, or written, “old-style” code for a Generative Adversarial Network (GAN), imagine something like this instead:\n\n\n# define the generator (simplified)\ngenerator <-\n  function(name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      \n      # define layers for the generator \n      self$fc1 <- layer_dense(units = 7 * 7 * 64, use_bias = FALSE)\n      self$batchnorm1 <- layer_batch_normalization()\n      # more layers ...\n      \n      # define what should happen in the forward pass\n      function(inputs, mask = NULL, training = TRUE) {\n        self$fc1(inputs) %>%\n          self$batchnorm1(training = training) %>%\n          # call remaining layers ...\n      }\n    })\n  }\n\n# define the discriminator\ndiscriminator <-\n  function(name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      \n      self$conv1 <- layer_conv_2d(filters = 64, #...)\n      self$leaky_relu1 <- layer_activation_leaky_relu()\n      # more layers ...\n    \n      function(inputs, mask = NULL, training = TRUE) {\n        inputs %>% self$conv1() %>%\n          self$leaky_relu1() %>%\n          # call remaining layers ...\n      }\n    })\n  }\n\nCoded like this, picture the generator and the discriminator as agents, ready to engage in what is actually the opposite of a zero-sum game.\nThe game, then, can be nicely coded using custom training.\nCustom training\nCustom training, as opposed to using keras fit, allows to interleave the training of several models. Models are called on data, and all calls have to happen inside the context of a GradientTape. In eager mode, GradientTapes are used to keep track of operations such that during backprop, their gradients can be calculated.\nThe following code example shows how using GradientTape-style training, we can see our actors play against each other:\n\n\n# zooming in on a single batch of a single epoch\nwith(tf$GradientTape() %as% gen_tape, { with(tf$GradientTape() %as% disc_tape, {\n  \n  # first, it's the generator's call (yep pun intended)\n  generated_images <- generator(noise)\n  # now the discriminator gives its verdict on the real images \n  disc_real_output <- discriminator(batch, training = TRUE)\n  # as well as the fake ones\n  disc_generated_output <- discriminator(generated_images, training = TRUE)\n  \n  # depending on the discriminator's verdict we just got,\n  # what's the generator's loss?\n  gen_loss <- generator_loss(disc_generated_output)\n  # and what's the loss for the discriminator?\n  disc_loss <- discriminator_loss(disc_real_output, disc_generated_output)\n}) })\n\n# now outside the tape's context compute the respective gradients\ngradients_of_generator <- gen_tape$gradient(gen_loss, generator$variables)\ngradients_of_discriminator <- disc_tape$gradient(disc_loss, discriminator$variables)\n \n# and apply them!\ngenerator_optimizer$apply_gradients(\n  purrr::transpose(list(gradients_of_generator, generator$variables)))\ndiscriminator_optimizer$apply_gradients(\n  purrr::transpose(list(gradients_of_discriminator, discriminator$variables)))\n\nAgain, compare this with pre-TF 2 GAN training – it makes for a lot more readable code.\nAs an aside, last year’s post series may have created the impression that with eager execution, you have to use custom (GradientTape) training instead of Keras-style fit. In fact, that was the case at the time those posts were written. Today, Keras-style code works just fine with eager execution.\nSo now with TF 2, we are in an optimal position. We can use custom training when we want to, but we don’t have to if declarative fit is all we need.\nThat’s it for a flashlight on what TF 2 means to R users. We now take a look around in the r-tensorflow ecosystem to see new developments – recent-past, present and future – in areas like data loading, preprocessing, and more.\nNew developments in the r-tensorflow ecosystem\nThese are what we’ll cover:\ntfdatasets: Over the recent past, tfdatasets pipelines have become the preferred way for data loading and preprocessing.\nfeature columns and feature specs: Specify your features recipes-style and have keras generate the adequate layers for them.\nKeras preprocessing layers: Keras preprocessing pipelines integrating functionality such as data augmentation (currently in planning).\ntfhub: Use pretrained models as keras layers, and/or as feature columns in a keras model.\ntf_function and tfautograph: Speed up training by running parts of your code in graph mode.\ntfdatasets input pipelines\nFor 2 years now, the tfdatasets package has been available to load data for training Keras models in a streaming way.\nLogically, there are three steps involved:\nFirst, data has to be loaded from some place. This could be a csv file, a directory containing images, or other sources. In this recent example from Image segmentation with U-Net, information about file names was first stored into an R tibble, and then tensor_slices_dataset was used to create a dataset from it:\n\n\ndata <- tibble(\n  img = list.files(here::here(\"data-raw/train\"), full.names = TRUE),\n  mask = list.files(here::here(\"data-raw/train_masks\"), full.names = TRUE)\n)\n\ndata <- initial_split(data, prop = 0.8)\n\ndataset <- training(data) %>%  \n  tensor_slices_dataset() \n\nOnce we have a dataset, we perform any required transformations, mapping over the batch dimension. Continuing with the example from the U-Net post, here we use functions from the tf.image module to (1) load images according to their file type, (2) scale them to values between 0 and 1 (converting to float32 at the same time), and (3) resize them to the desired format:\n\n\ndataset <- dataset %>%\n  dataset_map(~.x %>% list_modify(\n    img = tf$image$decode_jpeg(tf$io$read_file(.x$img)),\n    mask = tf$image$decode_gif(tf$io$read_file(.x$mask))[1,,,][,,1,drop=FALSE]\n  )) %>% \n  dataset_map(~.x %>% list_modify(\n    img = tf$image$convert_image_dtype(.x$img, dtype = tf$float32),\n    mask = tf$image$convert_image_dtype(.x$mask, dtype = tf$float32)\n  )) %>% \n  dataset_map(~.x %>% list_modify(\n    img = tf$image$resize(.x$img, size = shape(128, 128)),\n    mask = tf$image$resize(.x$mask, size = shape(128, 128))\n  ))\n\nNote how once you know what these functions do, they free you of a lot of thinking (remember how in the “old” Keras approach to image preprocessing, you were doing things like dividing pixel values by 255 “by hand”?)\nAfter transformation, a third conceptual step relates to item arrangement. You will often want to shuffle, and you certainly will want to batch the data:\n\n\n if (train) {\n    dataset <- dataset %>% \n      dataset_shuffle(buffer_size = batch_size*128)\n  }\n\ndataset <- dataset %>%  dataset_batch(batch_size)\n\nSumming up, using tfdatasets you build a pipeline, from loading over transformations to batching, that can then be fed directly to a Keras model. From preprocessing, let’s go a step further and look at a new, extremely convenient way to do feature engineering.\nFeature columns and feature specs\nFeature columns as such are a Python-TensorFlow feature, while feature specs are an R-only idiom modeled after the popular recipes package.\nIt all starts off with creating a feature spec object, using formula syntax to indicate what’s predictor and what’s target:\n\n\nlibrary(tfdatasets)\nhearts_dataset <- tensor_slices_dataset(hearts)\nspec <- feature_spec(hearts_dataset, target ~ .)\n\nThat specification is then refined by successive information about how we want to make use of the raw predictors. This is where feature columns come into play. Different column types exist, of which you can see a few in the following code snippet:\n\n\nspec <- feature_spec(hearts, target ~ .) %>% \n  step_numeric_column(\n    all_numeric(), -cp, -restecg, -exang, -sex, -fbs,\n    normalizer_fn = scaler_standard()\n  ) %>% \n  step_categorical_column_with_vocabulary_list(thal) %>% \n  step_bucketized_column(age, boundaries = c(18, 25, 30, 35, 40, 45, 50, 55, 60, 65)) %>% \n  step_indicator_column(thal) %>% \n  step_embedding_column(thal, dimension = 2) %>% \n  step_crossed_column(c(thal, bucketized_age), hash_bucket_size = 10) %>%\n  step_indicator_column(crossed_thal_bucketized_age)\n\nspec %>% fit()\n\nWhat happened here is that we told TensorFlow, please take all numeric columns (besides a few ones listed exprès) and scale them; take column thal, treat it as categorical and create an embedding for it; discretize age according to the given ranges; and finally, create a crossed column to capture interaction between thal and that discretized age-range column. 8\nThis is nice, but when creating the model, we’ll still have to define all those layers, right? (Which would be pretty cumbersome, having to figure out all the right dimensions…) Luckily, we don’t have to. In sync with tfdatasets, keras now provides layer_dense_features to create a layer tailor-made to accommodate the specification.\nAnd we don’t need to create separate input layers either, due to layer_input_from_dataset. Here we see both in action:\n\n\ninput <- layer_input_from_dataset(hearts %>% select(-target))\n\noutput <- input %>% \n  layer_dense_features(feature_columns = dense_features(spec)) %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nFrom then on, it’s just normal keras compile and fit. See the vignette for the complete example. There also is a post on feature columns explaining more of how this works, and illustrating the time-and-nerve-saving effect by comparing with the pre-feature-spec way of working with heterogeneous datasets.\nAs a last item on the topics of preprocessing and feature engineering, let’s look at a promising thing to come in what we hope is the near future.\nKeras preprocessing layers\nReading what we wrote above about using tfdatasets for building a input pipeline, and seeing how we gave an image loading example, you may have been wondering: What about data augmentation functionality available, historically, through keras? Like image_data_generator?\nThis functionality does not seem to fit. But a nice-looking solution is in preparation. In the Keras community, the recent RFC on preprocessing layers for Keras addresses this topic. The RFC is still under discussion, but as soon as it gets implemented in Python we’ll follow up on the R side.\nThe idea is to provide (chainable) preprocessing layers to be used for data transformation and/or augmentation in areas such as image classification, image segmentation, object detection, text processing, and more. 9 The envisioned, in the RFC, pipeline of preprocessing layers should return a dataset, for compatibility with tf.data (our tfdatasets). We’re definitely looking forward to having available this sort of workflow!\nLet’s move on to the next topic, the common denominator being convenience. But now convenience means not having to build billion-parameter models yourself!\nTensorflow Hub and the tfhub package\nTensorflow Hub is a library for publishing and using pretrained models. Existing models can be browsed on tfhub.dev.\nAs of this writing, the original Python library is still under development, so complete stability is not guaranteed. That notwithstanding, the tfhub R package already allows for some instructive experimentation.\nThe traditional Keras idea of using pretrained models typically involved either (1) applying a model like MobileNet as a whole, including its output layer, or (2) chaining a “custom head” to its penultimate layer 10. In contrast, the TF Hub idea is to use a pretrained model as a module in a larger setting.\nThere are two main ways to accomplish this, namely, integrating a module as a keras layer and using it as a feature column. The tfhub README shows the first option:\n\n\nlibrary(tfhub)\nlibrary(keras)\n\ninput <- layer_input(shape = c(32, 32, 3))\n\noutput <- input %>%\n  # we are using a pre-trained MobileNet model!\n  layer_hub(handle = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2\") %>%\n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel <- keras_model(input, output)\n\nWhile the tfhub feature columns vignette illustrates the second one:\n\n\nspec <- dataset_train %>%\n  feature_spec(AdoptionSpeed ~ .) %>%\n  step_text_embedding_column(\n    Description,\n    module_spec = \"https://tfhub.dev/google/universal-sentence-encoder/2\"\n    ) %>%\n  step_image_embedding_column(\n    img,\n    module_spec = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/3\"\n  ) %>%\n  step_numeric_column(Age, Fee, Quantity, normalizer_fn = scaler_standard()) %>%\n  step_categorical_column_with_vocabulary_list(\n    has_type(\"string\"), -Description, -RescuerID, -img_path, -PetID, -Name\n  ) %>%\n  step_embedding_column(Breed1:Health, State)\n\nBoth usage modes illustrate the high potential of working with Hub modules. Just be cautioned that, as of today, not every model published will work with TF 2.\ntf_function, TF autograph and the R package tfautograph\nAs explained above, the default execution mode in TF 2 is eager. For performance reasons however, in many cases it will be desirable to compile parts of your code into a graph. Calls to Keras layers, for example, are run in graph mode.\nTo compile a function into a graph, wrap it in a call to tf_function, as done e.g. in the post Modeling censored data with tfprobability:\n\n\nrun_mcmc <- function(kernel) {\n  kernel %>% mcmc_sample_chain(\n    num_results = n_steps,\n    num_burnin_steps = n_burnin,\n    current_state = tf$ones_like(initial_betas),\n    trace_fn = trace_fn\n  )\n}\n\n# important for performance: run HMC in graph mode\nrun_mcmc <- tf_function(run_mcmc)\n\nOn the Python side, the tf.autograph module automatically translates Python control flow statements into appropriate graph operations.\nIndependently of tf.autograph, the R package tfautograph, developed by Tomasz Kalinowski, implements control flow conversion directly from R to TensorFlow. This lets you use R’s if, while, for, break, and next when writing custom training flows. Check out the package’s extensive documentation for instructive examples!\nConclusion\nWith that, we end our introduction of TF 2 and the new developments that surround it.\nIf you have been using keras in traditional ways, how much changes for you is mainly up to you: Most everything will still work, but new options exist to write more performant, more modular, more elegant code. In particular, check out tfdatasets pipelines for efficient data loading.\nIf you’re an advanced user requiring non-standard setup, have a look into custom training and custom models, and consult the tfautograph documentation to see how the package can help.\nIn any case, stay tuned for upcoming posts showing some of the above-mentioned functionality in action. Thanks for reading!\nOriginal Python Keras, and thus, R keras, supported additional backends: Theano and CNTK. But the default backend in R keras always was TensorFlow.↩︎\nNote the terminology: in R keras, implementation referred to the Python library (Keras or TensorFlow, with its module tf.keras) bound to, while backend referred to the framework providing low-level operations, which could be one of Theano, TensorFlow and CNTK.)↩︎\nE.g., parameters like learning_rate may have to be adapted.↩︎\nSee More flexible models with TensorFlow eager execution and Keras for an overview and annotated links.↩︎\nHere the nominal input is an R vector that gets converted to a Python list by reticulate, and to a tensor by TensorFlow.↩︎\nThis is still a tensor though. To continue working with its values in R, we need to convert it to R using as.numeric, as.matrix, as.array etc.↩︎\nFor example, see Generating images with Keras and TensorFlow eager execution on GANs, Neural style transfer with eager execution and Keras on neural style transfer, or Representation learning with MMD-VAE on Variational Autoencoders.↩︎\nstep_indicator_column is there (twice) for technical reasons. Our post on feature columns explains.↩︎\nAs readers working in e.g. image segmentation will know, data augmentation is not as easy as just using image_data_generator on the input images, as analogous distortions have to be applied to the masks.↩︎\nor block of layers↩︎\n",
    "preview": "posts/2019-10-08-tf2-whatchanges/images/thumb.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 400,
    "preview_height": 400
  },
  {
    "path": "posts/2019-10-03-intro-to-hmc/",
    "title": "On leapfrogs, crashing satellites, and going nuts: A very first conceptual introduction to Hamiltonian Monte Carlo",
    "description": "TensorFlow Probability, and its R wrapper tfprobability, provide Markov Chain Monte Carlo (MCMC) methods that were used in a number of recent posts on this blog. These posts were directed to users already comfortable with the method, and terminology, per se, which readers mainly interested in deep learning won't necessarily be. Here we try to make up leeway, introducing Hamitonian Monte Carlo (HMC) as well as a few often-heard \"buzzwords\" accompanying it, always striving to keep in mind what it is all \"for\".",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-10-03",
    "categories": [
      "Bayesian Modeling",
      "Concepts"
    ],
    "contents": "\nWhy a very (meaning: VERY!) first conceptual introduction to Hamiltonian Monte Carlo (HMC) on this blog?\nWell, in our endeavor to feature the various capabilities of TensorFlow Probability (TFP) / tfprobability, we started showing examples1 of how to fit hierarchical models, using one of TFP’s joint distribution classes2 and HMC. The technical aspects being complex enough in themselves, we never gave an introduction to the “math side of things”. Here we are trying to make up for this.\nSeeing how it is impossible, in a short blog post, to provide an introduction to Bayesian modeling and Markov Chain Monte Carlo in general, and how there are so many excellent texts doing this already, we will presuppose some prior knowledge. Our specific focus then is on the latest and greatest, the magic buzzwords, the famous incantations: Hamiltonian Monte Carlo, leapfrog steps, NUTS – as always, trying to demystify, to make things as understandable as possible. In that spirit, welcome to a “glossary with a narrative”.\nSo what is it for?\nSampling, or Monte Carlo, techniques in general are used when we want to produce samples from, or statistically describe a distribution we don’t have a closed-form formulation of. Sometimes, we might really be interested in the samples; sometimes we just want them so we can compute, for example, the mean and variance of the distribution.\nWhat distribution? In the type of applications we’re talking about, we have a model, a joint distribution, which is supposed to describe some reality. Starting from the most basic scenario, it might look like this:\n\\[\nx \\sim \\mathcal{Poisson}(\\lambda)\n\\]\nThis “joint distribution” only has a single member, a Poisson distribution, that is supposed to model, say, the number of comments in a code review. We also have data on actual code reviews, like this, say:3\n\n\n\n\n\n\nWe now want to determine the parameter, \\(\\lambda\\), of the Poisson that make these data most likely. So far, we’re not even being Bayesian yet: There is no prior on this parameter. But of course, we want to be Bayesian, so we add one – imagine fixed priors4 on its parameters:\n\\[\nx \\sim \\mathcal{Poisson}(\\lambda)\\\\\n\\lambda \\sim \\gamma(\\alpha, \\beta)\\\\\n\\alpha \\sim [...]\\\\  \n\\beta \\sim [...]\n\\]\nThis being a joint distribution, we have three parameters to determine: \\(\\lambda\\), \\(\\alpha\\) and \\(\\beta\\). And what we’re interested in is the posterior distribution of the parameters given the data.\nNow, depending on the distributions involved, we usually cannot calculate the posterior distributions in closed form. Instead, we have to use sampling techniques to determine those parameters.5 What we’d like to point out instead is the following: In the upcoming discussions of sampling, HMC & co., it is really easy to forget what is it that we are sampling. Try to always keep in mind that what we’re sampling isn’t the data, it’s parameters: the parameters of the posterior distributions we’re interested in.\nSampling\nSampling methods in general consist of two steps: generating a sample (“proposal”) and deciding whether to keep it or to throw it away (“acceptance”). Intuitively, in our given scenario – where we have measured something and are now looking for a mechanism that explains those measurements – the latter should be easier: We “just” need to determine the likelihood of the data under those hypothetical model parameters. But how do we come up with suggestions to start with?\nIn theory, straightforward(-ish) methods exist that could be used to generate samples from an unknown (in closed form) distribution – as long as their unnormalized probabilities can be evaluated, and the problem is (very) low-dimensional. (For concise portraits of those methods, such as uniform sampling, importance sampling, and rejection sampling, see(MacKay 2002).) Those are not used in MCMC software though, for lack of efficiency and non-suitability in high dimensions. Before HMC became the dominant algorithm in such software, the Metropolis and Gibbs methods were the algorithms of choice. Both are nicely and understandably explained – in the case of Metropolis, often exemplified by nice stories –, and we refer the interested reader to the go-to references, such as (McElreath 2016) and (Kruschke 2010). Both were shown to be less efficient than HMC, the main topic of this post, due to their random-walk behavior: Every proposal is based on the current position in state space, meaning that samples may be highly correlated and state space exploration proceeds slowly.\nHMC\nSo HMC is popular because compared to random-walk-based algorithms, it is a lot more efficient. Unfortunately, it is also a lot more difficult to “get”.6 As discussed in Math, code, concepts: A third road to deep learning, there seem to be (at least) three languages to express an algorithm: Math; code (including pseudo-code, which may or may not be on the verge to math notation); and one I call conceptual which spans the whole range from very abstract to very concrete, even visual. To me personally, HMC is different from most other cases in that even though I find the conceptual explanations fascinating, they result in less “perceived understanding” than either the equations or the code. For people with backgrounds in physics, statistical mechanics and/or differential geometry this will probably be different!\nIn any case, physical analogies make for the best start.\nPhysical analogies\nThe classic physical analogy is given in the reference article, Radford Neal’s “MCMC using Hamiltonian dynamics” (Neal 2012), and nicely explained in a video by Ben Lambert.\nSo there’s this “thing” we want to maximize, the loglikelihood of the data under the model parameters. Alternatively we can say, we want to minimize the negative loglikelihood (like loss in a neural network). This “thing” to be optimized can then be visualized as an object sliding over a landscape with hills and valleys, and like with gradient descent in deep learning, we want it to end up deep down in some valley.\nIn Neal’s own words\n\nIn two dimensions, we can visualize the dynamics as that of a frictionless puck that slides over a surface of varying height. The state of this system consists of the position of the puck, given by a 2D vector q, and the momentum of the puck (its mass times its velocity), given by a 2D vector p.\n\nNow when you hear “momentum” (and given that I’ve primed you to think of deep learning) you may feel that sounds familiar, but even though the respective analogies are related the association does not help that much. In deep learning, momentum is commonly praised for its avoidance of ineffective oscillations in imbalanced optimization landscapes.7 With HMC however, the focus is on the concept of energy.\nIn statistical mechanics, the probability of being in some state \\(i\\) is inverse-exponentially related to its energy. (Here \\(T\\) is the temperature; we won’t focus on this so just imagine it being set to 1 in this and subsequent equations.)\n\\[P(E_i) \\sim e^{\\frac{-E_i}{T}} \\]\nAs you might or might not remember from school physics, energy comes in two forms: potential energy and kinetic energy. In the sliding-object scenario, the object’s potential energy corresponds to its height (position), while its kinetic energy is related to its momentum, \\(m\\), by the formula8\n\\[K(m) = \\frac{m^2}{2 * mass} \\]\nNow without kinetic energy, the object would slide downhill always, and as soon as the landscape slopes up again, would come to a halt. Through its momentum though, it is able to continue uphill for a while, just as if, going downhill on your bike, you pick up speed you may make it over the next (short) hill without pedaling.\nSo that’s kinetic energy. The other part, potential energy, corresponds to the thing we really want to know - the negative log posterior of the parameters we’re really after:\n\\[U(\\theta) \\sim - log (P(x | \\theta) P(\\theta))\\]\nSo the “trick” of HMC is augmenting the state space of interest - the vector of posterior parameters - by a momentum vector, to improve optimization efficiency. When we’re finished, the momentum part is just thrown away. (This aspect is especially nicely explained in Ben Lambert’s video.)\nFollowing his exposition and notation, here we have the energy of a state of parameter and momentum vectors, equaling a sum of potential and kinetic energies:\n\\[E(\\theta, m) = U(\\theta) + K(m)\\]\nThe corresponding probability, as per the relationship given above, then is\n\\[P(E) \\sim e^{\\frac{-E}{T}} = e^{\\frac{- U(\\theta)}{T}} e^{\\frac{- K(m)}{T}}\\]\nWe now substitute into this equation, assuming a temperature (T) of 1 and a mass of 1:\n\\[P(E) \\sim P(x | \\theta) P(\\theta) e^{\\frac{- m^2}{2}}\\]\nNow in this formulation, the distribution of momentum is just a standard normal (\\(e^{\\frac{- m^2}{2}}\\))! Thus, we can just integrate out the momentum and take \\(P(\\theta)\\) as samples from the posterior distribution:9\n\\[\n\\begin{aligned}\n& P(\\theta) = \n\\int \\! P(\\theta, m) \\mathrm{d}m = \\frac{1}{Z} \\int \\! P(x | \\theta) P(\\theta) \\mathcal{N}(m|0,1) \\mathrm{d}m\\\\\n& P(\\theta) = \\frac{1}{Z} \\int \\! P(x | \\theta) P(\\theta)\n\\end{aligned}\n\\]\nHow does this work in practice? At every step, we\nsample a new momentum value from its marginal distribution (which is the same as the conditional distribution given \\(U\\), as they are independent), and\nsolve for the path of the particle. This is where Hamilton’s equations come into play.\nHamilton’s equations (equations of motion)\nFor the sake of less confusion, should you decide to read the paper, here we switch to Radford Neal’s notation.\nHamiltonian dynamics operates on a d-dimensional position vector, \\(q\\), and a d-dimensional momentum vector, \\(p\\). The state space is described by the Hamiltonian, a function of \\(p\\) and \\(q\\):\n\\[H(q, p) =U(q) +K(p)\\]\nHere \\(U(q)\\) is the potential energy (called \\(U(\\theta)\\) above), and \\(K(p)\\) is the kinetic energy as a function of momentum (called \\(K(m)\\) above).\nThe partial derivatives of the Hamiltonian determine how \\(p\\) and \\(q\\) change over time, \\(t\\), according to Hamilton’s equations:\n\\[\n\\begin{aligned}\n& \\frac{dq}{dt} = \\frac{\\partial H}{\\partial p}\\\\\n& \\frac{dp}{dt} = - \\frac{\\partial H}{\\partial q}\n\\end{aligned}\n\\]\nHow can we solve this system of partial differential equations? The basic workhorse in numerical integration is Euler’s method, where time (or the independent variable, in general) is advanced by a step of size \\(\\epsilon\\), and a new value of the dependent variable is computed by taking the (partial) derivative and adding it to its current value. For the Hamiltonian system, doing this one equation after the other looks like this:\n\\[\n\\begin{aligned}\n& p(t+\\epsilon) = p(t) + \\epsilon \\frac{dp}{dt}(t) = p(t) − \\epsilon \\frac{\\partial U}{\\partial q}(q(t))\\\\\n& q(t+\\epsilon) = q(t) + \\epsilon \\frac{dq}{dt}(t) = q(t) + \\epsilon \\frac{p(t)}{m})\n\\end{aligned}\n\\]\nHere first a new position is computed for time \\(t + 1\\), making use of the current momentum at time \\(t\\); then a new momentum is computed, also for time \\(t + 1\\), making use of the current position at time \\(t\\).\nThis process can be improved if in step 2, we make use of the new position we just freshly computed in step 1; but let’s directly go to what is actually used in contemporary software, the leapfrog method.\nLeapfrog algorithm\nSo after Hamiltonian, we’ve hit the second magic word: leapfrog. Unlike Hamiltonian however, there is less mystery here. The leapfrog method is “just” a more efficient way to perform the numerical integration.\nIt consists of three steps, basically splitting up the Euler step 1 into two parts, before and after the momentum update:\n\\[\n\\begin{aligned}\n& p(t+\\frac{\\epsilon}{2}) = p(t) − \\frac{\\epsilon}{2} \\frac{\\partial U}{\\partial q}(q(t))\\\\\n& q(t+\\epsilon) = q(t) + \\epsilon \\frac{p(t + \\frac{\\epsilon}{2})}{m}\\\\\n& p(t+ \\epsilon) = p(t+\\frac{\\epsilon}{2}) − \\frac{\\epsilon}{2} \\frac{\\partial U}{\\partial q}(q(t + \\epsilon))\n\\end{aligned}\n\\]\nAs you can see, each step makes use of the corresponding variable-to-differentiate’s value computed in the preceding step. In practice, several leapfrog steps are executed before a proposal is made; so steps 3 and 1 (of the subsequent iteration) are combined.\nProposal – this keyword brings us back to the higher-level “plan”. All this – Hamiltonian equations, leapfrog integration – served to generate a proposal for a new value of the parameters, which can be accepted or not. The way that decision is taken is not particular to HMC and explained in detail in the above-mentioned expositions on the Metropolis algorithm, so we just cover it briefly.\nAcceptance: Metropolis algorithm\nUnder the Metropolis algorithm, proposed new vectors \\(q*\\) and \\(p*\\) are accepted with probability\n\\[\nmin(1, exp(−H(q∗, p∗) +H(q, p)))\n\\]\nThat is, if the proposed parameters yield a higher likelihood, they are accepted; if not, they are accepted only with a certain probability that depends on the ratio between old and new likelihoods. In theory, energy staying constant in a Hamiltonian system, proposals should always be accepted; in practice, loss of precision due to numerical integration may yield an acceptance rate less than 1.\nHMC in a few lines of code\nWe’ve talked about concepts, and we’ve seen the math, but between analogies and equations, it’s easy to lose track of the overall algorithm. Nicely, Radford Neal’s paper (Neal 2012) has some code, too! Here it is reproduced, with just a few additional comments added (many comments were preexisting):\n\n\n# U is a function that returns the potential energy given q\n# grad_U returns the respective partial derivatives\n# epsilon stepsize\n# L number of leapfrog steps\n# current_q current position\n\n# kinetic energy is assumed to be sum(p^2/2) (mass == 1)\nHMC <- function (U, grad_U, epsilon, L, current_q) {\n  q <- current_q\n  # independent standard normal variates\n  p <- rnorm(length(q), 0, 1)  \n  # Make a half step for momentum at the beginning\n  current_p <- p \n  # Alternate full steps for position and momentum\n  p <- p - epsilon * grad_U(q) / 2 \n  for (i in 1:L) {\n    # Make a full step for the position\n    q <- q + epsilon * p\n    # Make a full step for the momentum, except at end of trajectory\n    if (i != L) p <- p - epsilon * grad_U(q)\n    }\n  # Make a half step for momentum at the end\n  p <- p - epsilon * grad_U(q) / 2\n  # Negate momentum at end of trajectory to make the proposal symmetric\n  p <- -p\n  # Evaluate potential and kinetic energies at start and end of trajectory \n  current_U <- U(current_q)\n  current_K <- sum(current_p^2) / 2\n  proposed_U <- U(q)\n  proposed_K <- sum(p^2) / 2\n  # Accept or reject the state at end of trajectory, returning either\n  # the position at the end of the trajectory or the initial position\n  if (runif(1) < exp(current_U-proposed_U+current_K-proposed_K)) {\n    return (q)  # accept\n  } else {\n    return (current_q)  # reject\n  }\n}\n\nHopefully, you find this piece of code as helpful as I do. Are we through yet? Well, so far we haven’t encountered the last magic word: NUTS. What, or who, is NUTS?\nNUTS\nNUTS, added to Stan in 201110 and about a month ago, to TensorFlow Probability’s master branch, is an algorithm that aims to circumvent one of the practical difficulties in using HMC: The choice of number of leapfrog steps to perform before making a proposal. The acronym stands for No-U-Turn Sampler, alluding to the avoidance of U-turn-shaped curves in the optimization landscape when the number of leapfrog steps is chosen too high.\nThe reference paper by Hoffman & Gelman (Hoffman and Gelman 2011) also describes a solution to a related difficulty: choosing the step size \\(\\epsilon\\). The respective algorithm, dual averaging, was also recently added to TFP.\nNUTS being more of algorithm in the computer science usage of the word than a thing to explain conceptually, we’ll leave it at that, and ask the interested reader to read the paper – or even, consult the TFP documentation to see how NUTS is implemented there. Instead, we’ll round up with another conceptual analogy, Michael Bétancourts crashing (or not!) satellite (Betancourt 2017).\nHow to avoid crashes\nBétancourt’s article is an awesome read, and a paragraph focusing on a single point made in the paper can be nothing than a “teaser” (which is why we’ll have a picture, too!).\nTo introduce the upcoming analogy, the problem starts with high dimensionality, which is a given in most real-world problems. In high dimensions, as usual, the density function has a mode (the place where it is maximal), but necessarily, there cannot be much volume around it – just like with k-nearest neighbors, the more dimensions you add, the farther your nearest neighbor will be. A product of volume and density, the only significant probability mass resides in the so-called typical set,11 which becomes more and more narrow in high dimensions.\nSo, the typical set is what we want to explore, but it gets more and more difficult to find it (and stay there). Now as we saw above, HMC uses gradient information to get near the mode, but if it just followed the gradient of the log probability (the position) it would leave the typical set and stop at the mode.\nThis is where momentum comes in – it counteracts the gradient, and both together ensure that the Markov chain stays on the typical set. Now here’s the satellite analogy, in Bétancourt’s own words:\n\nFor example, instead of trying to reason about a mode, a gradient, and a typical set, we can equivalently reason about a planet, a gravitational field, and an orbit (Figure 14). The probabilistic endeavor of exploring the typical set then becomes a physical endeavor of placing a satellite in a stable orbit around the hypothetical planet. Because these are just two different perspectives of the same mathematical system, they will suffer from the same pathologies. Indeed, if we place a satellite at rest out in space it will fall in the gravitational field and crash into the surface of the planet, just as naive gradient-driven trajectories crash into the mode (Figure 15). From either the probabilistic or physical perspective we are left with a catastrophic outcome.\n\n\nThe physical picture, however, provides an immediate solution: although objects at rest will crash into the planet, we can maintain a stable orbit by endowing our satellite with enough momentum to counteract the gravitational attraction. We have to be careful, however, in how exactly we add momentum to our satellite. If we add too little momentum transverse to the gravitational field, for example, then the gravitational attraction will be too strong and the satellite will still crash into the planet (Figure 16a). On the other hand, if we add too much momentum then the gravitational attraction will be too weak to capture the satellite at all and it will instead fly out into the depths of space (Figure 16b).\n\nAnd here’s the picture I promised (Figure 16 from the paper):\n\n\n\nFigure 1: Figure 16 from (Betancourt 2017)\n\n\n\nAnd with this, we conclude. Hopefully, you’ll have found this helpful – unless you knew it all (or more) beforehand, in which case you probably wouldn’t have read this post :-)\nThanks for reading!\n\n\nBetancourt, Michael. 2017. “A Conceptual Introduction to Hamiltonian Monte Carlo.” arXiv E-Prints, January, arXiv:1701.02434. http://arxiv.org/abs/1701.02434.\n\n\nBlei, David M., Alp Kucukelbir, and Jon D. McAuliffe. 2017. “Variational Inference: A Review for Statisticians.” Journal of the American Statistical Association 112 (518): 859–77. https://doi.org/10.1080/01621459.2017.1285773.\n\n\nHoffman, Matthew D., and Andrew Gelman. 2011. “The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.” http://arxiv.org/abs/1111.4246.\n\n\nKruschke, John K. 2010. Doing Bayesian Data Analysis: A Tutorial with R and Bugs. 1st ed. Orlando, FL, USA: Academic Press, Inc.\n\n\nMacKay, David J. C. 2002. Information Theory, Inference & Learning Algorithms. New York, NY, USA: Cambridge University Press.\n\n\nMcElreath, Richard. 2016. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. CRC Press. http://xcelab.net/rm/statistical-rethinking/.\n\n\nNeal, Radford M. 2012. “MCMC using Hamiltonian dynamics.” arXiv E-Prints, June, arXiv:1206.1901. http://arxiv.org/abs/1206.1901.\n\n\nSee e.g. Tadpoles on TensorFlow: Hierarchical partial pooling with tfprobability and Hierarchical partial pooling, continued: Varying slopes models with TensorFlow Probability↩︎\ntfd_joint_distribution_sequential↩︎\ndata are purely made up↩︎\nelided↩︎\nIn some cases, variational inference is an alternative. See (Blei, Kucukelbir, and McAuliffe 2017) for a nice introduction.↩︎\nBy “get [it]” I mean the subjective feeling of understanding what’s going on; not more and not less.↩︎\nThough see this great distill.pub post for a different view.↩︎\n\\(m\\) here stands for momentum.↩︎\nHere \\(Z\\) is the normalizer required to have the integral sum to 1.↩︎\nAs of today, Stan uses a modified version, see appendix A.5 of (Betancourt 2017), to be cited soon.↩︎\nfor a detailed discussion of typical sets, see the book by McKay (MacKay 2002).↩︎\n",
    "preview": "posts/2019-10-03-intro-to-hmc/images/mb.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 548,
    "preview_height": 345
  },
  {
    "path": "posts/2019-09-30-bert-r/",
    "title": "BERT from R",
    "description": "A deep learning model - BERT from Google AI Research - has yielded state-of-the-art results in a wide variety of Natural Language Processing (NLP) tasks. In this tutorial, we will show how to load and train the BERT model from R, using Keras.",
    "author": [
      {
        "name": "Turgut Abdullayev",
        "url": "https://github.com/henry090"
      }
    ],
    "date": "2019-09-30",
    "categories": [
      "Natural Language Processing",
      "TensorFlow/Keras"
    ],
    "contents": "\nToday, we’re happy to feature a guest post written by Turgut Abdullayev, showing how to use BERT from R. Turgut is a data scientist at AccessBank Azerbaijan. Currently, he is pursuing a Ph.D. in economics at Baku State University, Azerbaijan.\nIn the previous post, Sigrid Keydana explained the logic behind the reticulate package and how it enables interoperability between Python and R. So, this time we will build a classification model with BERT, taking into account one of the powerful capabilities of the reticulate package – calling Python from R via importing Python modules.\nBefore we start, make sure that the Python version used is 3, as Python 2 can introduce lots of difficulties while working with BERT, such as Unicode issues related to the input text.\n\nNote: The R implementation presupposes TF Keras while by default, keras-bert does not use it. So, adding that environment variable makes it work.\n\n\n\nSys.setenv(TF_KERAS=1) \n# make sure we use python 3\nreticulate::use_python('C:/Users/turgut.abdullayev/AppData/Local/Continuum/anaconda3/python.exe',\n                       required=T)\n# to see python version\nreticulate::py_config()\n\n\n\npython:         C:/Users/turgut.abdullayev/AppData/Local/Continuum/anaconda3/python.exe\nlibpython:      C:/Users/turgut.abdullayev/AppData/Local/Continuum/anaconda3/python37.dll\npythonhome:     C:\\Users\\TURGUT~1.ABD\\AppData\\Local\\CONTIN~1\\ANACON~1\nversion:        3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)]\nArchitecture:   64bit\nnumpy:          C:\\Users\\TURGUT~1.ABD\\AppData\\Local\\CONTIN~1\\ANACON~1\\lib\\site-packages\\numpy\nnumpy_version:  1.16.4\n\nNOTE: Python version was forced by use_python function\n\nLuckily for us, a convenient way of importing BERT with Keras was created by Zhao HG. It is called Keras-bert. For us, this means that importing that same python library with reticulate will allow us to build a popular state-of-the-art model within R.\nThere are several methods to install keras-bert in Python.\nin Jupyter Notebook, run:\n\n\n!pip install keras-bert\n\nin Terminal (Linux, Mac OS), run:\n\n\npython3 -m pip install keras-bert\n\nin Anaconda prompt (Windows), run:\n\n\nconda install keras-bert\n\nAfter this procedure, you can check whether keras-bert is installed or not.\n\n\nreticulate::py_module_available('keras_bert')\n\n\n\n[1] TRUE\n\nFinally, the TensorFlow version used should be 1.14/1.15. You can check it in the following form:\n\n\ntensorflow::tf_version()\n\n\n\n[1] ‘1.14’\n\nIn a nutshell:\n\n\npip install keras-bert\ntensorflow::install_tensorflow(version = \"1.15\")\n\nWhat is BERT?\nBERT1 is a pre-trained deep learning model introduced by Google AI Research which has been trained on Wikipedia and BooksCorpus. It has a unique way to understand the structure of a given text. Instead of reading the text from left to right or from right to left, BERT, using an attention mechanism which is called Transformer encoder2, reads the entire word sequences at once. So, it allows to understanding a word based on its surroundings. There are different kind of pre-trained BERT models but the main difference between them is trained parameters. In our case, BERT with 12 encoder layers (Transformer Blocks), 768-hidden hidden units, 12-heads3, and 110M parameters will be used to create a text classification model.\nModel structure\nLoading a pre-trained BERT model is straightforward. The downloaded zip file contains:\nbert_model.ckpt, which is for loading the weights from the TensorFlow checkpoint\nbert_config.json, which is a configuration file\nvocab.txt, which is for text tokenization\n\n\npretrained_path = '/Users/turgutabdullayev/Downloads/uncased_L-12_H-768_A-12'\nconfig_path = file.path(pretrained_path, 'bert_config.json')\ncheckpoint_path = file.path(pretrained_path, 'bert_model.ckpt')\nvocab_path = file.path(pretrained_path, 'vocab.txt')\n\nImport Keras-Bert module via reticulate\nLet’s load keras-bert via reticulate and prepare a tokenizer object. The BERT tokenizer will help us to turn words into indices.\n\n\nlibrary(reticulate)\nk_bert = import('keras_bert')\ntoken_dict = k_bert$load_vocabulary(vocab_path)\ntokenizer = k_bert$Tokenizer(token_dict)\n\nHow does the tokenizer work?\nBERT uses a WordPiece tokenization strategy. If a word is Out-of-vocabulary (OOV), then BERT will break it down into subwords. (eating => eat, ##ing).\n\n\n\nFigure 1: BERT input representation. The input embeddings are the sum of the token embeddings, the segmentation embeddings, and the position embeddings\n\n\n\nEmbedding Layers in BERT\nThere are 3 types of embedding layers in BERT:\nToken Embeddings help to transform words into vector representations. In our model dimension size is 768.\nSegment Embeddings help to understand the semantic similarity of different pieces of the text.\nPosition Embeddings mean that identical words at different positions will not have the same output representation.\nDefine model parameters and column names\nAs usual with keras, the batch size, number of epochs and the learning rate should be defined for training BERT. Additionally, the sequence length is needed.\n\n\nseq_length = 50L\nbch_size = 70\nepochs = 1\nlearning_rate = 1e-4\n\nDATA_COLUMN = 'comment_text'\nLABEL_COLUMN = 'target'\n\n\nNote: the max input length is 512, and the model is extremely compute intensive even on GPU.\n\nLoad BERT model into R\nWe can load the BERT model and automatically pad sequences with seq_len function. Keras-bert4 makes the loading process very easy and comfortable.\n\n\nmodel = k_bert$load_trained_model_from_checkpoint(\n  config_path,\n  checkpoint_path,\n  training=T,\n  trainable=T,\n  seq_len=seq_length)\n\nData structure, reading, preparation\nThe dataset for this post is taken from the Kaggle Jigsaw Unintended Bias in Toxicity Classification competition.\nIn order to prepare the dataset, we write a preprocessing function which will read and tokenize data simultaneously. Then, we feed the outputs of the function as input for BERT model.\n\n\n# tokenize text\ntokenize_fun = function(dataset) {\n  c(indices, target, segments) %<-% list(list(),list(),list())\n  for ( i in 1:nrow(dataset)) {\n    c(indices_tok, segments_tok) %<-% tokenizer$encode(dataset[[DATA_COLUMN]][i], \n                                                       max_len=seq_length)\n    indices = indices %>% append(list(as.matrix(indices_tok)))\n    target = target %>% append(dataset[[LABEL_COLUMN]][i])\n    segments = segments %>% append(list(as.matrix(segments_tok)))\n  }\n  return(list(indices,segments, target))\n}\n\n\n\n# read data\ndt_data = function(dir, rows_to_read){\n  data = data.table::fread(dir, nrows=rows_to_read)\n  c(x_train, x_segment, y_train) %<-% tokenize_fun(data)\n  return(list(x_train, x_segment, y_train))\n}\n\nLoad dataset\nThe way we have written the preprocess function, at first, it will read data, then add zeros and encode words into indices. Hence, we will have 3 output files:\nx_train is input matrix for BERT\nx_segment contains zeros for segment embeddings\ny_train is the output target which we should predict\n\n\nc(x_train,x_segment, y_train) %<-% \ndt_data('~/Downloads/jigsaw-unintended-bias-in-toxicity-classification/train.csv',2000)\n\nMatrix format for Keras-Bert\nThe input data are in list format. They need to be extracted and transposed. Then, the train and segment matrices should be placed into the list.\n\n\ntrain = do.call(cbind,x_train) %>% t()\nsegments = do.call(cbind,x_segment) %>% t()\ntargets = do.call(cbind,y_train) %>% t()\n\nconcat = c(list(train ),list(segments))\n\nCalculate decay and warmup steps\nUsing the Adam optimizer with warmup helps to lower the learning rate at the beginning of the training process. After certain training steps, the learning rate will gradually be increased, because learning new data without warmup can negatively affect a BERT model.\n\n\nc(decay_steps, warmup_steps) %<-% k_bert$calc_train_steps(\n  targets %>% length(),\n  batch_size=bch_size,\n  epochs=epochs\n)\n\nDetermine inputs and outputs, then concatenate them\nIn order to build a binary classification model, the output of the BERT model should contain 1 unit. Therefore, first of all, we should get input and output layers. Then, adding an additional dense layer to the output can perfectly meet our needs.\n\n\nlibrary(keras)\n\ninput_1 = get_layer(model,name = 'Input-Token')$input\ninput_2 = get_layer(model,name = 'Input-Segment')$input\ninputs = list(input_1,input_2)\n\ndense = get_layer(model,name = 'NSP-Dense')$output\n\noutputs = dense %>% layer_dense(units=1L, activation='sigmoid',\n                         kernel_initializer=initializer_truncated_normal(stddev = 0.02),\n                         name = 'output')\n\nmodel = keras_model(inputs = inputs,outputs = outputs)\n\nThis is how the model architecture looks like after adding a dense layer and padding input sequences.\n\n\nModel\n__________________________________________________________________________________________\nLayer (type)                 Output Shape        Param #    Connected to                  \n==========================================================================================\nInput-Token (InputLayer)     (None, 50)          0                                        \n__________________________________________________________________________________________\nInput-Segment (InputLayer)   (None, 50)          0                                        \n__________________________________________________________________________________________\nEmbedding-Token (TokenEmbedd [(None, 50, 768), ( 23440896   Input-Token[0][0]             \n__________________________________________________________________________________________\nEmbedding-Segment (Embedding (None, 50, 768)     1536       Input-Segment[0][0]           \n__________________________________________________________________________________________\nEmbedding-Token-Segment (Add (None, 50, 768)     0          Embedding-Token[0][0]         \n                                                            Embedding-Segment[0][0]       \n__________________________________________________________________________________________\nEmbedding-Position (Position (None, 50, 768)     38400      Embedding-Token-Segment[0][0] \n__________________________________________________________________________________________\nEmbedding-Dropout (Dropout)  (None, 50, 768)     0          Embedding-Position[0][0]      \n__________________________________________________________________________________________\nEmbedding-Norm (LayerNormali (None, 50, 768)     1536       Embedding-Dropout[0][0]       \n__________________________________________________________________________________________\nEncoder-1-MultiHeadSelfAtten (None, 50, 768)     2362368    Embedding-Norm[0][0]          \n__________________________________________________________________________________________\nEncoder-1-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-1-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-1-MultiHeadSelfAtten (None, 50, 768)     0          Embedding-Norm[0][0]          \n                                                            Encoder-1-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-1-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-1-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-1-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-1-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-1-FeedForward-Dropou (None, 50, 768)     0          Encoder-1-FeedForward[0][0]   \n__________________________________________________________________________________________\nEncoder-1-FeedForward-Add (A (None, 50, 768)     0          Encoder-1-MultiHeadSelfAttenti\n                                                            Encoder-1-FeedForward-Dropout[\n__________________________________________________________________________________________\nEncoder-1-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-1-FeedForward-Add[0][0\n__________________________________________________________________________________________\nEncoder-2-MultiHeadSelfAtten (None, 50, 768)     2362368    Encoder-1-FeedForward-Norm[0][\n__________________________________________________________________________________________\nEncoder-2-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-2-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-2-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-1-FeedForward-Norm[0][\n                                                            Encoder-2-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-2-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-2-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-2-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-2-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-2-FeedForward-Dropou (None, 50, 768)     0          Encoder-2-FeedForward[0][0]   \n__________________________________________________________________________________________\nEncoder-2-FeedForward-Add (A (None, 50, 768)     0          Encoder-2-MultiHeadSelfAttenti\n                                                            Encoder-2-FeedForward-Dropout[\n__________________________________________________________________________________________\nEncoder-2-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-2-FeedForward-Add[0][0\n__________________________________________________________________________________________\nEncoder-3-MultiHeadSelfAtten (None, 50, 768)     2362368    Encoder-2-FeedForward-Norm[0][\n__________________________________________________________________________________________\nEncoder-3-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-3-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-3-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-2-FeedForward-Norm[0][\n                                                            Encoder-3-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-3-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-3-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-3-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-3-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-3-FeedForward-Dropou (None, 50, 768)     0          Encoder-3-FeedForward[0][0]   \n__________________________________________________________________________________________\nEncoder-3-FeedForward-Add (A (None, 50, 768)     0          Encoder-3-MultiHeadSelfAttenti\n                                                            Encoder-3-FeedForward-Dropout[\n__________________________________________________________________________________________\nEncoder-3-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-3-FeedForward-Add[0][0\n__________________________________________________________________________________________\nEncoder-4-MultiHeadSelfAtten (None, 50, 768)     2362368    Encoder-3-FeedForward-Norm[0][\n__________________________________________________________________________________________\nEncoder-4-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-4-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-4-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-3-FeedForward-Norm[0][\n                                                            Encoder-4-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-4-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-4-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-4-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-4-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-4-FeedForward-Dropou (None, 50, 768)     0          Encoder-4-FeedForward[0][0]   \n__________________________________________________________________________________________\nEncoder-4-FeedForward-Add (A (None, 50, 768)     0          Encoder-4-MultiHeadSelfAttenti\n                                                            Encoder-4-FeedForward-Dropout[\n__________________________________________________________________________________________\nEncoder-4-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-4-FeedForward-Add[0][0\n__________________________________________________________________________________________\nEncoder-5-MultiHeadSelfAtten (None, 50, 768)     2362368    Encoder-4-FeedForward-Norm[0][\n__________________________________________________________________________________________\nEncoder-5-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-5-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-5-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-4-FeedForward-Norm[0][\n                                                            Encoder-5-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-5-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-5-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-5-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-5-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-5-FeedForward-Dropou (None, 50, 768)     0          Encoder-5-FeedForward[0][0]   \n__________________________________________________________________________________________\nEncoder-5-FeedForward-Add (A (None, 50, 768)     0          Encoder-5-MultiHeadSelfAttenti\n                                                            Encoder-5-FeedForward-Dropout[\n__________________________________________________________________________________________\nEncoder-5-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-5-FeedForward-Add[0][0\n__________________________________________________________________________________________\nEncoder-6-MultiHeadSelfAtten (None, 50, 768)     2362368    Encoder-5-FeedForward-Norm[0][\n__________________________________________________________________________________________\nEncoder-6-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-6-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-6-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-5-FeedForward-Norm[0][\n                                                            Encoder-6-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-6-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-6-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-6-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-6-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-6-FeedForward-Dropou (None, 50, 768)     0          Encoder-6-FeedForward[0][0]   \n__________________________________________________________________________________________\nEncoder-6-FeedForward-Add (A (None, 50, 768)     0          Encoder-6-MultiHeadSelfAttenti\n                                                            Encoder-6-FeedForward-Dropout[\n__________________________________________________________________________________________\nEncoder-6-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-6-FeedForward-Add[0][0\n__________________________________________________________________________________________\nEncoder-7-MultiHeadSelfAtten (None, 50, 768)     2362368    Encoder-6-FeedForward-Norm[0][\n__________________________________________________________________________________________\nEncoder-7-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-7-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-7-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-6-FeedForward-Norm[0][\n                                                            Encoder-7-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-7-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-7-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-7-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-7-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-7-FeedForward-Dropou (None, 50, 768)     0          Encoder-7-FeedForward[0][0]   \n__________________________________________________________________________________________\nEncoder-7-FeedForward-Add (A (None, 50, 768)     0          Encoder-7-MultiHeadSelfAttenti\n                                                            Encoder-7-FeedForward-Dropout[\n__________________________________________________________________________________________\nEncoder-7-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-7-FeedForward-Add[0][0\n__________________________________________________________________________________________\nEncoder-8-MultiHeadSelfAtten (None, 50, 768)     2362368    Encoder-7-FeedForward-Norm[0][\n__________________________________________________________________________________________\nEncoder-8-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-8-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-8-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-7-FeedForward-Norm[0][\n                                                            Encoder-8-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-8-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-8-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-8-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-8-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-8-FeedForward-Dropou (None, 50, 768)     0          Encoder-8-FeedForward[0][0]   \n__________________________________________________________________________________________\nEncoder-8-FeedForward-Add (A (None, 50, 768)     0          Encoder-8-MultiHeadSelfAttenti\n                                                            Encoder-8-FeedForward-Dropout[\n__________________________________________________________________________________________\nEncoder-8-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-8-FeedForward-Add[0][0\n__________________________________________________________________________________________\nEncoder-9-MultiHeadSelfAtten (None, 50, 768)     2362368    Encoder-8-FeedForward-Norm[0][\n__________________________________________________________________________________________\nEncoder-9-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-9-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-9-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-8-FeedForward-Norm[0][\n                                                            Encoder-9-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-9-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-9-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-9-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-9-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-9-FeedForward-Dropou (None, 50, 768)     0          Encoder-9-FeedForward[0][0]   \n__________________________________________________________________________________________\nEncoder-9-FeedForward-Add (A (None, 50, 768)     0          Encoder-9-MultiHeadSelfAttenti\n                                                            Encoder-9-FeedForward-Dropout[\n__________________________________________________________________________________________\nEncoder-9-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-9-FeedForward-Add[0][0\n__________________________________________________________________________________________\nEncoder-10-MultiHeadSelfAtte (None, 50, 768)     2362368    Encoder-9-FeedForward-Norm[0][\n__________________________________________________________________________________________\nEncoder-10-MultiHeadSelfAtte (None, 50, 768)     0          Encoder-10-MultiHeadSelfAttent\n__________________________________________________________________________________________\nEncoder-10-MultiHeadSelfAtte (None, 50, 768)     0          Encoder-9-FeedForward-Norm[0][\n                                                            Encoder-10-MultiHeadSelfAttent\n__________________________________________________________________________________________\nEncoder-10-MultiHeadSelfAtte (None, 50, 768)     1536       Encoder-10-MultiHeadSelfAttent\n__________________________________________________________________________________________\nEncoder-10-FeedForward (Feed (None, 50, 768)     4722432    Encoder-10-MultiHeadSelfAttent\n__________________________________________________________________________________________\nEncoder-10-FeedForward-Dropo (None, 50, 768)     0          Encoder-10-FeedForward[0][0]  \n__________________________________________________________________________________________\nEncoder-10-FeedForward-Add ( (None, 50, 768)     0          Encoder-10-MultiHeadSelfAttent\n                                                            Encoder-10-FeedForward-Dropout\n__________________________________________________________________________________________\nEncoder-10-FeedForward-Norm  (None, 50, 768)     1536       Encoder-10-FeedForward-Add[0][\n__________________________________________________________________________________________\nEncoder-11-MultiHeadSelfAtte (None, 50, 768)     2362368    Encoder-10-FeedForward-Norm[0]\n__________________________________________________________________________________________\nEncoder-11-MultiHeadSelfAtte (None, 50, 768)     0          Encoder-11-MultiHeadSelfAttent\n__________________________________________________________________________________________\nEncoder-11-MultiHeadSelfAtte (None, 50, 768)     0          Encoder-10-FeedForward-Norm[0]\n                                                            Encoder-11-MultiHeadSelfAttent\n__________________________________________________________________________________________\nEncoder-11-MultiHeadSelfAtte (None, 50, 768)     1536       Encoder-11-MultiHeadSelfAttent\n__________________________________________________________________________________________\nEncoder-11-FeedForward (Feed (None, 50, 768)     4722432    Encoder-11-MultiHeadSelfAttent\n__________________________________________________________________________________________\nEncoder-11-FeedForward-Dropo (None, 50, 768)     0          Encoder-11-FeedForward[0][0]  \n__________________________________________________________________________________________\nEncoder-11-FeedForward-Add ( (None, 50, 768)     0          Encoder-11-MultiHeadSelfAttent\n                                                            Encoder-11-FeedForward-Dropout\n__________________________________________________________________________________________\nEncoder-11-FeedForward-Norm  (None, 50, 768)     1536       Encoder-11-FeedForward-Add[0][\n__________________________________________________________________________________________\nEncoder-12-MultiHeadSelfAtte (None, 50, 768)     2362368    Encoder-11-FeedForward-Norm[0]\n__________________________________________________________________________________________\nEncoder-12-MultiHeadSelfAtte (None, 50, 768)     0          Encoder-12-MultiHeadSelfAttent\n__________________________________________________________________________________________\nEncoder-12-MultiHeadSelfAtte (None, 50, 768)     0          Encoder-11-FeedForward-Norm[0]\n                                                            Encoder-12-MultiHeadSelfAttent\n__________________________________________________________________________________________\nEncoder-12-MultiHeadSelfAtte (None, 50, 768)     1536       Encoder-12-MultiHeadSelfAttent\n__________________________________________________________________________________________\nEncoder-12-FeedForward (Feed (None, 50, 768)     4722432    Encoder-12-MultiHeadSelfAttent\n__________________________________________________________________________________________\nEncoder-12-FeedForward-Dropo (None, 50, 768)     0          Encoder-12-FeedForward[0][0]  \n__________________________________________________________________________________________\nEncoder-12-FeedForward-Add ( (None, 50, 768)     0          Encoder-12-MultiHeadSelfAttent\n                                                            Encoder-12-FeedForward-Dropout\n__________________________________________________________________________________________\nEncoder-12-FeedForward-Norm  (None, 50, 768)     1536       Encoder-12-FeedForward-Add[0][\n__________________________________________________________________________________________\nExtract (Extract)            (None, 768)         0          Encoder-12-FeedForward-Norm[0]\n__________________________________________________________________________________________\nNSP-Dense (Dense)            (None, 768)         590592     Extract[0][0]                 \n__________________________________________________________________________________________\noutput (Dense)               (None, 1)           769        NSP-Dense[0][0]               \n==========================================================================================\nTotal params: 109,128,193\nTrainable params: 109,128,193\nNon-trainable params: 0\n__________________________________________________________________________________________\n\nCompile model and begin training\nAus usual with Keras, before training a model, we need to compile the model. And using fit(), we feed it the R arrays.\n\n\nmodel %>% compile(\n  k_bert$AdamWarmup(decay_steps=decay_steps, \n                    warmup_steps=warmup_steps, lr=learning_rate),\n  loss = 'binary_crossentropy',\n  metrics = 'accuracy'\n)\n\nmodel %>% fit(\n  concat,\n  targets,\n  epochs=epochs,\n  batch_size=bch_size, validation_split=0.2)\n\nConclusion\nIn this post, we’ve shown how we can use Keras to conveniently load, configure, and train a BERT model.\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding↩︎\nAttention Is All You Need↩︎\nAttention — focuses on salient parts of input by taking a weighted average of them. 768 hidden units divided by 12 chunks and each chunk will have 64 output dimensions, afterward, the result from each chunk will be concatenated and forwarded to the next layer↩︎\nImplementation of the BERT. Official pre-trained models could be loaded for feature extraction and prediction↩︎\n",
    "preview": "posts/2019-09-30-bert-r/images/bert.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 437,
    "preview_height": 367
  },
  {
    "path": "posts/2019-08-29-using-tf-from-r/",
    "title": "So, how come we can use TensorFlow from R?",
    "description": "Have you ever wondered why you can call TensorFlow - mostly known as a Python framework - from R? If not - that's how it should be, as the R packages keras and tensorflow aim to make this process as transparent as possible to the user. But for them to be those helpful genies, someone else first has to tame the Python.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-08-29",
    "categories": [
      "TensorFlow/Keras",
      "Meta",
      "Concepts"
    ],
    "contents": "\nWhich computer language is most closely associated with TensorFlow? While on the TensorFlow for R blog, we would of course like the answer to be R, chances are it is Python (though TensorFlow has official 1 bindings for C++, Swift, Javascript, Java, and Go as well).\nSo why is it you can define a Keras model as\n\n\nlibrary(keras)\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 32, activation = \"relu\") %>%\n  layer_dense(units = 1)\n\n(nice with %>%s and all!) – then train and evaluate it, get predictions and plot them, all that without ever leaving R?\nThe short answer is, you have keras, tensorflow and reticulate installed. reticulate embeds a Python session within the R process. A single process means a single address space: The same objects exist, and can be operated upon, regardless of whether they’re seen by R or by Python. On that basis, tensorflow and keras then wrap the respective Python libraries 2 and let you write R code that, in fact, looks like R.\nThis post first elaborates a bit on the short answer. We then go deeper into what happens in the background.\nOne note on terminology before we jump in: On the R side, we’re making a clear distinction between the packages keras and tensorflow. For Python we are going to use TensorFlow and Keras interchangeably. Historically, these have been different, and TensorFlow was commonly thought of as one possible backend to run Keras on, besides the pioneering, now discontinued Theano, and CNTK. Standalone Keras does still exist, but recent work has been, and is being, done in tf.keras. Of course, this makes Python Keras a subset of Python TensorFlow, but all examples in this post will use that subset so we can use both to refer to the same thing.\nSo keras, tensorflow, reticulate, what are they for?\nFirstly, nothing of this would be possible without reticulate. 3 reticulate is an R package designed to allow seemless interoperability between R and Python. If we absolutely wanted, we could construct a Keras model like this:\n\n\nlibrary(reticulate)\ntf <- import(\"tensorflow\")\nm <- tf$keras$models$Sequential()\nm$`__class__`\n\n\n<class 'tensorflow.python.keras.engine.sequential.Sequential'>\nWe could go on adding layers …\n\n\nm$add(tf$keras$layers$Dense(32, \"relu\"))\nm$add(tf$keras$layers$Dense(1))\nm$layers\n\n\n[[1]]\n<tensorflow.python.keras.layers.core.Dense>\n\n[[2]]\n<tensorflow.python.keras.layers.core.Dense>\n\nBut who would want to? If this were the only way, it’d be less cumbersome to directly write Python instead. Plus, as a user you’d have to know the complete Python-side module structure (now where do optimizers live, currently: tf.keras.optimizers, tf.optimizers …?), and keep up with all path and name changes in the Python API. 4\nThis is where keras comes into play. keras is where the TensorFlow-specific usability, re-usability, and convenience features live. 5 Functionality provided by keras spans the whole range between boilerplate-avoidance over enabling elegant, R-like idioms to providing means of advanced feature usage. As an example for the first two, consider layer_dense which, among others, converts its units argument to an integer, and takes arguments in an order that allow it to be “pipe-added” to a model: Instead of\n\n\nmodel <- keras_model_sequential()\nmodel$add(layer_dense(units = 32L))\n\nwe can just say\n\n\nmodel <- keras_model_sequential()\nmodel %>% layer_dense(units = 32)\n\nWhile these are nice to have, there is more. Advanced functionality in (Python) Keras mostly depends on the ability to subclass objects. One example is custom callbacks. If you were using Python, you’d have to subclass tf.keras.callbacks.Callback. From R, you can create an R6 class inheriting from KerasCallback, like so\n\n\nCustomCallback <- R6::R6Class(\"CustomCallback\",\n    inherit = KerasCallback,\n    public = list(\n      on_train_begin = function(logs) {\n        # do something\n      },\n      on_train_end = function(logs) {\n        # do something\n      }\n    )\n  )\n\nThis is because keras defines an actual Python class, RCallback, and maps your R6 class’ methods to it. Another example is custom models, introduced on this blog about a year ago. These models can be trained with custom training loops. In R, you use keras_model_custom to create one, for example, like this:\n\n\nm <- keras_model_custom(name = \"mymodel\", function(self) {\n  self$dense1 <- layer_dense(units = 32, activation = \"relu\")\n  self$dense2 <- layer_dense(units = 10, activation = \"softmax\")\n  \n  function(inputs, mask = NULL) {\n    self$dense1(inputs) %>%\n      self$dense2()\n  }\n})\n\nHere, keras will make sure an actual Python object is created which subclasses tf.keras.Model and when called, runs the above anonymous function().\nSo that’s keras. What about the tensorflow package? As a user you only need it when you have to do advanced stuff, like configure TensorFlow device usage or (in TF 1.x) access elements of the Graph or the Session. Internally, it is used by keras heavily. Essential internal functionality includes, e.g., implementations of S3 methods, like print, [ or +, on Tensors, so you can operate on them like on R vectors.\nNow that we know what each of the packages is “for”, let’s dig deeper into what makes this possible.\nShow me the magic: reticulate\nInstead of exposing the topic top-down, we follow a by-example approach, building up complexity as we go. We’ll have three scenarios.\nFirst, we assume we already have a Python object (that has been constructed in whatever way) and need to convert that to R. Then, we’ll investigate how we can create a Python object, calling its constructor. Finally, we go the other way round: We ask how we can pass an R function to Python for later usage.\nScenario 1: R-to-Python conversion\nLet’s assume we have created a Python object in the global namespace, like this:\n\n\npy_run_string(\"x = 1\")\n\nSo: There is a variable, called x, with value 1, living in Python world. Now how do we bring this thing into R?\nWe know the main entry point to conversion is py_to_r, defined as a generic in conversion.R:\n\n\npy_to_r <- function(x) {\n  ensure_python_initialized()\n  UseMethod(\"py_to_r\")\n}\n\n… with the default implementation calling a function named py_ref_to_r:\n\n\n#' @export\npy_to_r.default <- function(x) {\n  [...]\n  x <- py_ref_to_r(x)\n  [...]\n}\n\nTo find out more about what is going on, debugging on the R level won’t get us far. We start gdb so we can set breakpoints in C++ functions: 6\n\n$ R -d gdb\n\nGNU gdb (GDB) Fedora 8.3-6.fc30\n[... some more gdb saying hello ...]\nReading symbols from /usr/lib64/R/bin/exec/R...\nReading symbols from /usr/lib/debug/usr/lib64/R/bin/exec/R-3.6.0-1.fc30.x86_64.debug...\n\nNow start R, load reticulate, and execute the assignment we’re going to presuppose:\n\n(gdb) run\nStarting program: /usr/lib64/R/bin/exec/R \n[...]\nR version 3.6.0 (2019-04-26) -- \"Planting of a Tree\"\nCopyright (C) 2019 The R Foundation for Statistical Computing\n[...]\n> library(reticulate)\n> py_run_string(\"x = 1\")\nSo that set up our scenario, the Python object (named x) we want to convert to R. Now, use Ctrl-C to “escape” to gdb, set a breakpoint in py_to_r and type c to get back to R:\n\n(gdb) b py_to_r\nBreakpoint 1 at 0x7fffe48315d0 (2 locations)\n(gdb) c\nNow what are we going to see when we access that x?\n\n> py$x\n\nThread 1 \"R\" hit Breakpoint 1, 0x00007fffe48315d0 in py_to_r(libpython::_object*, bool)@plt () from /home/key/R/x86_64-redhat-linux-gnu-library/3.6/reticulate/libs/reticulate.so\nHere are the relevant (for our investigation) frames of the backtrace:\n\nThread 1 \"R\" hit Breakpoint 3, 0x00007fffe48315d0 in py_to_r(libpython::_object*, bool)@plt () from /home/key/R/x86_64-redhat-linux-gnu-library/3.6/reticulate/libs/reticulate.so\n(gdb) bt\n#0  0x00007fffe48315d0 in py_to_r(libpython::_object*, bool)@plt () from /home/key/R/x86_64-redhat-linux-gnu-library/3.6/reticulate/libs/reticulate.so\n#1  0x00007fffe48588a0 in py_ref_to_r_with_convert (x=..., convert=true) at reticulate_types.h:32\n#2  0x00007fffe4858963 in py_ref_to_r (x=...) at /home/key/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include/RcppCommon.h:120\n#3  0x00007fffe483d7a9 in _reticulate_py_ref_to_r (xSEXP=0x55555daa7e50) at /home/key/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include/Rcpp/as.h:151\n...\n...\n#14 0x00007ffff7cc5fc7 in Rf_usemethod (generic=0x55555757ce70 \"py_to_r\", obj=obj@entry=0x55555daa7e50, call=call@entry=0x55555a0fe198, args=args@entry=0x55555557c4e0, \n    rho=rho@entry=0x55555dab2ed0, callrho=0x55555dab48d8, defrho=0x5555575a4068, ans=0x7fffffff69e8) at objects.c:486\nWe’ve removed a few intermediate frames related to (R-level) method dispatch.\nAs we already saw in the source code, py_to_r.default will delegate to a method called py_ref_to_r, which we see appears in #2. But what is _reticulate_py_ref_to_r in #3, the frame just below? Here is where the magic, unseen by the user, begins.\nLet’s look at this from a bird’s eye’s view. To translate an object from one language to another, we need to find a common ground, that is, a third language “spoken” by both of them. In the case of R and Python (as well as in a lot of other cases) this will be C / C++. So assuming we are going to write a C function to talk to Python, how can we use this function in R?\nWhile R users have the ability to call into C directly, using .Call or .External 7, this is made much more convenient by Rcpp 8: You just write your C++ function, and Rcpp takes care of compilation and provides the glue code necessary to call this function from R.\nSo py_ref_to_r really is written in C++:\n\n\n// [[Rcpp::export]]\nSEXP py_ref_to_r(PyObjectRef x) {\n  return py_ref_to_r_with_convert(x, x.convert());\n}\n\nbut the comment // [[Rcpp::export]] tells Rcpp to generate an R wrapper, py_ref_to_R, that itself calls a C++ wrapper, _reticulate_py_ref_to_r …\n\n\npy_ref_to_r <- function(x) {\n  .Call(`_reticulate_py_ref_to_r`, x)\n}\n\nwhich finally wraps the “real” thing, the C++ function py_ref_to_R we saw above.\nVia py_ref_to_r_with_convert in #1, a one-liner that extracts an object’s “convert” feature (see below)\n\n\n// [[Rcpp::export]]\nSEXP py_ref_to_r_with_convert(PyObjectRef x, bool convert) {\n  return py_to_r(x, convert);\n}\n\nwe finally arrive at py_to_r in #0.\nBefore we look at that, let’s contemplate that C/C++ “bridge” from the other side - Python. While strictly, Python is a language specification, its reference implementation is CPython, with a core written in C and much more functionality built on top in Python. In CPython, every Python object (including integers or other numeric types) is a PyObject. PyObjects are allocated through and operated on using pointers; most C API functions return a pointer to one, PyObject *.\nSo this is what we expect to work with, from R. What then is PyObjectRef doing in py_ref_to_r? PyObjectRef is not part of the C API, it is part of the functionality introduced by reticulate to manage Python objects. Its main purpose is to make sure the Python object is automatically cleaned up when the R object (an Rcpp::Environment) goes out of scope. Why use an R environment to wrap the Python-level pointer? This is because R environments can have finalizers: functions that are called before objects are garbage collected. We use this R-level finalizer to ensure the Python-side object gets finalized as well:\n\n\nRcpp::RObject xptr = R_MakeExternalPtr((void*) object, R_NilValue, R_NilValue);\nR_RegisterCFinalizer(xptr, python_object_finalize);\n\npython_object_finalize is interesting, as it tells us something crucial about Python – about CPython, to be precise: To find out if an object is still needed, or could be garbage collected, it uses reference counting, thus placing on the user the burden of correctly incrementing and decrementing references according to language semantics.\n\n\ninline void python_object_finalize(SEXP object) {\n  PyObject* pyObject = (PyObject*)R_ExternalPtrAddr(object);\n  if (pyObject != NULL)\n    Py_DecRef(pyObject);\n}\n\nResuming on PyObjectRef, note that it also stores the “convert” feature of the Python object, used to determine whether that object should be converted to R automatically.\nBack to py_to_r. This one now really gets to work with (a pointer to the) Python object,\n\n\nSEXP py_to_r(PyObject* x, bool convert) {\n  //...\n}\n\nand – but wait. Didn’t py_ref_to_r_with_convert pass it a PyObjectRef? So how come it receives a PyObject instead? This is because PyObjectRef inherits from Rcpp::Environment, and its implicit conversion operator is used to extract the Python object from the Environment. Concretely, that operator tells the compiler that a PyObjectRef can be used as though it were a PyObject* in some concepts, and the associated code specifies how to convert from PyObjectRef to PyObject*:\n\n\noperator PyObject*() const {\n  return get();\n}\n\nPyObject* get() const {\n  SEXP pyObject = getFromEnvironment(\"pyobj\");\n  if (pyObject != R_NilValue) {\n    PyObject* obj = (PyObject*)R_ExternalPtrAddr(pyObject);\n    if (obj != NULL)\n      return obj;\n  }\n  Rcpp::stop(\"Unable to access object (object is from previous session and is now invalid)\");\n}\n\n\nSo py_to_r works with a pointer to a Python object and returns what we want, an R object (a SEXP). The function checks for the type of the object, and then uses Rcpp to construct the adequate R object, in our case, an integer:\n\n\nelse if (scalarType == INTSXP)\n  return IntegerVector::create(PyInt_AsLong(x));\n\nFor other objects, typically there’s more action required; but essentially, the function is “just” a big if-else tree.\nSo this was scenario 1: converting a Python object to R. Now in scenario 2, we assume we still need to create that Python object.\nScenario 2:\nAs this scenario is considerably more complex than the previous one, we will explicitly concentrate on some aspects and leave out others. Importantly, we’ll not go into module loading, which would deserve separate treatment of its own. Instead, we try to shed a light on what’s involved using a concrete example: the ubiquitous, in keras code, keras_model_sequential(). All this R function does is\n\n\nfunction(layers = NULL, name = NULL) {\n  keras$models$Sequential(layers = layers, name = name)\n}\n\nHow can keras$models$Sequential() give us an object? When in Python, you run the equivalent\n\n\ntf.keras.models.Sequential()\n\nthis calls the constructor, that is, the __init__ method of the class:\n\n\nclass Sequential(training.Model):\n  def __init__(self, layers=None, name=None):\n    # ...\n  # ...\n\nSo this time, before – as always, in the end – getting an R object back from Python, we need to call that constructor, that is, a Python callable. (Python callables subsume functions, constructors, and objects created from a class that has a call method.)\nSo when py_to_r, inspecting its argument’s type, sees it is a Python callable (wrapped in a PyObjectRef, the reticulate-specific subclass of Rcpp::Environment we talked about above), it wraps it (the PyObjectRef) in an R function, using Rcpp:\n\n\nRcpp::Function f = py_callable_as_function(pyFunc, convert);\n\nThe cpython-side action starts when py_callable_as_function then calls py_call_impl. py_call_impl executes the actual call and returns an R object, a SEXP. Now you may be asking, how does the Python runtime know it shouldn’t deallocate that object, now that its work is done? This is taken of by the same PyObjectRef class used to wrap instances of PyObject *: It can wrap SEXPs as well.\nWhile a lot more could be said about what happens before we finally get to work with that Sequential model from R, let’s stop here and look at our third scenario.\nScenario 3: Calling R from Python\nNot surprisingly, sometimes we need to pass R callbacks to Python. An example are R data generators that can be used with keras models 9.\nIn general, for R objects to be passed to Python, the process is somewhat opposite to what we described in example 1. Say we type:\n\n\npy$a <- 1\n\nThis assigns 1 to a variable a in the python main module. To enable assignment, reticulate provides an implementation of the S3 generic $<-, $<-.python.builtin.object, which delegates to py_set_attr, which then calls py_set_attr_impl – yet another C++ function exported via Rcpp.\nLet’s focus on a different aspect here, though. A prerequisite for the assignment to happen is getting that 1 converted to Python. (We’re using the simplest possible example, obviously; but you can imagine this getting a lot more complex if the object isn’t a simple number).\nFor our “minimal example”, we see a stacktrace like the following\n\n#0 0x00007fffe4832010 in r_to_py_cpp(Rcpp::RObject_Impl<Rcpp::PreserveStorage>, bool)@plt () from /home/key/R/x86_64-redhat-linux-gnu-library/3.6/reticulate/libs/reticulate.so\n#1  0x00007fffe4854f38 in r_to_py_impl (object=..., convert=convert@entry=true) at /home/key/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include/RcppCommon.h:120\n#2  0x00007fffe48418f3 in _reticulate_r_to_py_impl (objectSEXP=0x55555ec88fa8, convertSEXP=<optimized out>) at /home/key/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include/Rcpp/as.h:151\n...\n#12 0x00007ffff7cc5c03 in dispatchMethod (sxp=0x55555d0cf1a0, dotClass=<optimized out>, cptr=cptr@entry=0x7ffffffeaae0, method=method@entry=0x55555bfe06c0, \n    generic=0x555557634458 \"r_to_py\", rho=0x55555d1d98a8, callrho=0x5555555af2d0, defrho=0x555557947430, op=<optimized out>, op=<optimized out>) at objects.c:436\n#13 0x00007ffff7cc5fc7 in Rf_usemethod (generic=0x555557634458 \"r_to_py\", obj=obj@entry=0x55555ec88fa8, call=call@entry=0x55555c0317b8, args=args@entry=0x55555557cc60, \n    rho=rho@entry=0x55555d1d98a8, callrho=0x5555555af2d0, defrho=0x555557947430, ans=0x7ffffffe9928) at objects.c:486\nWhereas r_to_py is a generic (like py_to_r above), r_to_py_impl is wrapped by Rcpp and r_to_py_cpp is a C++ function that branches on the type of the object – basically the counterpart of the C++ r_to_py.\nIn addition to that general process, there is more going on when we call an R function from Python. As Python doesn’t “speak” R, we need to wrap the R function in CPython - basically, we are extending Python here! How to do this is described in the official Extending Python Guide.\nIn official terms, what reticulate does it embed and extend Python. Embed, because it lets you use Python from inside R. Extend, because to enable Python to call back into R it needs to wrap R functions in C, so Python can understand them.\nAs part of the former, the desired Python is loaded (Py_Initialize()); as part of the latter, two functions are defined in a new module named rpycall, that will be loaded when Python itself is loaded.\n\n\nPyImport_AppendInittab(\"rpycall\", &initializeRPYCall);\n\nThese methods are call_r_function, used by default, and call_python_function_on_main_thread, used in cases where we need to make sure the R function is called on the main thread:\n\n\nPyMethodDef RPYCallMethods[] = {\n  { \"call_r_function\", (PyCFunction)call_r_function,\n    METH_VARARGS | METH_KEYWORDS, \"Call an R function\" },\n  { \"call_python_function_on_main_thread\", (PyCFunction)call_python_function_on_main_thread,\n    METH_VARARGS | METH_KEYWORDS, \"Call a Python function on the main thread\" },\n  { NULL, NULL, 0, NULL }\n};\n\ncall_python_function_on_main_thread is especially interesting. The R runtime is single-threaded; while the CPython implementation of Python effectively is as well, due to the Global Interpreter Lock, this is not automatically the case when other implementations are used, or C is used directly. So call_python_function_on_main_thread makes sure that unless we can execute on the main thread, we wait.\nThat’s it for our three “spotlights on reticulate”.\nWrapup\nIt goes without saying that there’s a lot about reticulate we didn’t cover in this article, such as memory management, initialization, or specifics of data conversion. Nonetheless, we hope we were able to shed a bit of light on the magic involved in calling TensorFlow from R.\nR is a concise and elegant language, but to a high degree its power comes from its packages, including those that allow you to call into, and interact with, the outside world, such as deep learning frameworks or distributed processing engines. In this post, it was a special pleasure to focus on a central building block that makes much of this possible: reticulate.\nThanks for reading!\nor semi-official, dependent on the language; see the TensorFlow website to track status↩︎\nbut see the “note on terminology” below↩︎\nNot without Rcpp either, but we’ll save that for the “Digging deeper” section.↩︎\nof which there are many, currently, accompanying the substantial changes related to the introduction of TF 2.0.↩︎\nIt goes without saying that as a generic mediator between R and Python, reticulate can not provide convenience features for all R packages that use it.↩︎\nFor a very nice introduction to debugging R with a debugger like gdb, see Kevin Ushey’s “Debugging with LLDB” That post uses lldb which is the standard debugger on Macintosh, while here we’re using gdb on linux; but mostly the behaviors are very similar.↩︎\nFor a nice introduction, see version 1 of Advanced R.↩︎\nNot a copy-paste error: For a nice introduction, see version 2 of Advanced R.↩︎\nFor performance reasons, it is often advisable to use tfdatasets instead.↩︎\n",
    "preview": "posts/2019-08-29-using-tf-from-r/images/thumb.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 739,
    "preview_height": 516
  },
  {
    "path": "posts/2019-08-23-unet/",
    "title": "Image segmentation with U-Net",
    "description": "In image segmentation, every pixel of an image is assigned a class. Depending on the application, classes could be different cell types; or the task could be binary, as in \"cancer cell yes or no?\". Area of application notwithstanding, the established neural network architecture of choice is U-Net. In this post, we show how to preprocess data and train a U-Net model on the Kaggle Carvana image segmentation data.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": {}
      },
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-08-23",
    "categories": [
      "Image Recognition & Image Processing",
      "TensorFlow/Keras"
    ],
    "contents": "\nSure, it is nice when I have a picture of some object, and a neural network can tell me what kind of object that is. More realistically, there might be several salient objects in that picture, and it tells me what they are, and where they are. The latter task (known as object detection) seems especially prototypical of contemporary AI applications that at the same time are intellectually fascinating and ethically questionable. It’s different with the subject of this post: Successful image segmentation has a lot of undeniably useful applications. For example, it is a sine qua non in medicine, neuroscience, biology and other life sciences.\nSo what, technically, is image segmentation, and how can we train a neural network to do it?\nImage segmentation in a nutshell\nSay we have an image with a bunch of cats in it. In classification, the question is “what’s that?”, and the answer we want to hear is: “cat”. In object detection, we again ask “what’s that”, but now that “what” is implicitly plural, and we expect an answer like “there’s a cat, a cat, and a cat, and they’re here, here, and here” (imagine the network pointing, by means of drawing bounding boxes, i.e., rectangles around the detected objects). In segmentation, we want more: We want the whole image covered by “boxes” – which aren’t boxes anymore, but unions of pixel-size “boxlets” – or put differently: We want the network to label every single pixel in the image.\nHere’s an example from the paper we’re going to talk about in a second. On the left is the input image (HeLa cells), next up is the ground truth, and third is the learned segmentation mask.\n\n\n\nFigure 1: Example segmentation from Ronneberger et al. 2015.\n\n\n\nTechnically, a distinction is made between class segmentation and instance segmentation. In class segmentation, referring to the “bunch of cats” example, there are two possible labels: Every pixel is either “cat” or “not cat”. Instance segmentation is more difficult: Here every cat gets their own label. (As an aside, why should that be more difficult? Presupposing human-like cognition, it wouldn’t be – if I have the concept of a cat, instead of just “cattiness”, I “see” there are two cats, not one. But depending on what a specific neural network relies on most – texture, color, isolated parts – those tasks may differ a lot in difficulty.)\nThe network architecture used in this post is adequate for class segmentation tasks and should be applicable to a vast number of practical, scientific as well as non-scientific applications. Speaking of network architecture, how should it look?\nIntroducing U-Net\nGiven their success in image classification, can’t we just use a classic architecture like Inception V[n], ResNet, ResNext … , whatever? The problem is, our task at hand – labeling every pixel – does not fit so well with the classic idea of a CNN. With convnets, the idea is to apply successive layers of convolution and pooling to build up feature maps of decreasing granularity, to finally arrive at an abstract level where we just say: “yep, a cat”. The counterpart being, we lose detail information: To the final classification, it does not matter whether the five pixels in the top-left area are black or white.\nIn practice, the classic architectures use (max) pooling or convolutions with stride > 1 to achieve those successive abstractions – necessarily resulting in decreased spatial resolution. So how can we use a convnet and still preserve detail information? In their 2015 paper U-Net: Convolutional Networks for Biomedical Image Segmentation (Ronneberger, Fischer, and Brox 2015), Olaf Ronneberger et al. came up with what four years later, in 2019, is still the most popular approach. (Which is to say something, four years being a long time, in deep learning.)\nThe idea is stunningly simple. While successive encoding (convolution / max pooling) steps, as usual, reduce resolution, the subsequent decoding – we have to arrive at an output of size same as the input, as we want to label every pixel! – does not simply upsample from the most compressed layer. Instead, during upsampling, at every step we feed in information from the corresponding, in resolution, layer in the downsizing chain.\nFor U-Net, really a picture says more than many words:\n\n\n\nFigure 2: U-Net architecture from Ronneberger et al. 2015.\n\n\n\nAt each upsampling stage we concatenate the output from the previous layer with that from its counterpart in the compression stage. The final output is a mask of size the original image, obtained via 1x1-convolution; no final dense layer is required, instead the output layer is just a convolutional layer with a single filter.\nNow let’s actually train a U-Net. We’re going to use the unet package that lets you create a well-performing model in a single line:\n\n\nremotes::install_github(\"r-tensorflow/unet\")\nlibrary(unet)\n\n# takes additional parameters, including number of downsizing blocks, \n# number of filters to start with, and number of classes to identify\n# see ?unet for more info\nmodel <- unet(input_shape = c(128, 128, 3))\n\nSo we have a model, and it looks like we’ll be wanting to feed it 128x128 RGB images. Now how do we get these images?\nThe data\nTo illustrate how applications arise even outside the area of medical research, we’ll use as an example the Kaggle Carvana Image Masking Challenge. The task is to create a segmentation mask separating cars from background. For our current purpose, we only need train.zip and train_mask.zip from the archive provided for download. In the following, we assume those have been extracted to a subdirectory called data-raw.\nLet’s first take a look at some images and their associated segmentation masks.\n\n\n# libraries we're going to need later\nlibrary(keras)\nlibrary(tfdatasets)\nlibrary(tidyverse)\nlibrary(rsample)\nlibrary(reticulate)\n\nimages <- tibble(\n  img = list.files(here::here(\"data-raw/train\"), full.names = TRUE),\n  mask = list.files(here::here(\"data-raw/train_masks\"), full.names = TRUE)\n  ) %>% \n  sample_n(2) %>% \n  map(. %>% magick::image_read() %>% magick::image_resize(\"128x128\"))\n\nout <- magick::image_append(c(\n  magick::image_append(images$img, stack = TRUE), \n  magick::image_append(images$mask, stack = TRUE)\n  )\n)\n\n\n\n\nThe photos are RGB-space JPEGs, while the masks are black-and-white GIFs.\nWe split the data into a training and a validation set. We’ll use the latter to monitor generalization performance during training.\n\n\ndata <- tibble(\n  img = list.files(here::here(\"data-raw/train\"), full.names = TRUE),\n  mask = list.files(here::here(\"data-raw/train_masks\"), full.names = TRUE)\n)\n\ndata <- initial_split(data, prop = 0.8)\n\nTo feed the data to the network, we’ll use tfdatasets. All preprocessing will end up in a simple pipeline, but we’ll first go over the required actions step-by-step.\nPreprocessing pipeline\nThe first step is to read in the images, making use of the appropriate functions in tf$image.\n\n\ntraining_dataset <- training(data) %>%  \n  tensor_slices_dataset() %>% \n  dataset_map(~.x %>% list_modify(\n    # decode_jpeg yields a 3d tensor of shape (1280, 1918, 3)\n    img = tf$image$decode_jpeg(tf$io$read_file(.x$img)),\n    # decode_gif yields a 4d tensor of shape (1, 1280, 1918, 3),\n    # so we remove the unneeded batch dimension and all but one \n    # of the 3 (identical) channels\n    mask = tf$image$decode_gif(tf$io$read_file(.x$mask))[1,,,][,,1,drop=FALSE]\n  ))\n\nWhile constructing a preprocessing pipeline, it’s very useful to check intermediate results. It’s easy to do using reticulate::as_iterator on the dataset:\n\n\nexample <- training_dataset %>% as_iterator() %>% iter_next()\nexample\n\n\n$img\ntf.Tensor(\n[[[243 244 239]\n  [243 244 239]\n  [243 244 239]\n  ...\n ...\n  ...\n  [175 179 178]\n  [175 179 178]\n  [175 179 178]]], shape=(1280, 1918, 3), dtype=uint8)\n\n$mask\ntf.Tensor(\n[[[0]\n  [0]\n  [0]\n  ...\n ...\n  ...\n  [0]\n  [0]\n  [0]]], shape=(1280, 1918, 1), dtype=uint8)\n\nWhile the uint8 datatype makes RGB values easy to read for humans, the network is going to expect floating point numbers. The following code converts its input and additionally, scales values to the interval [0,1):\n\n\ntraining_dataset <- training_dataset %>% \n  dataset_map(~.x %>% list_modify(\n    img = tf$image$convert_image_dtype(.x$img, dtype = tf$float32),\n    mask = tf$image$convert_image_dtype(.x$mask, dtype = tf$float32)\n  ))\n\nTo reduce computational cost, we resize the images to size 128x128. This will change the aspect ratio and thus, distort the images, but is not a problem with the given dataset.\n\n\ntraining_dataset <- training_dataset %>% \n  dataset_map(~.x %>% list_modify(\n    img = tf$image$resize(.x$img, size = shape(128, 128)),\n    mask = tf$image$resize(.x$mask, size = shape(128, 128))\n  ))\n\nNow, it’s well known that in deep learning, data augmentation is paramount. For segmentation, there’s one thing to consider, which is whether a transformation needs to be applied to the mask as well – this would be the case for e.g. rotations, or flipping. Here, results will be good enough applying just transformations that preserve positions:\n\n\nrandom_bsh <- function(img) {\n  img %>% \n    tf$image$random_brightness(max_delta = 0.3) %>% \n    tf$image$random_contrast(lower = 0.5, upper = 0.7) %>% \n    tf$image$random_saturation(lower = 0.5, upper = 0.7) %>% \n    # make sure we still are between 0 and 1\n    tf$clip_by_value(0, 1) \n}\n\ntraining_dataset <- training_dataset %>% \n  dataset_map(~.x %>% list_modify(\n    img = random_bsh(.x$img)\n  ))\n\nAgain, we can use as_iterator to see what these transformations do to our images:\n\n\nexample <- training_dataset %>% as_iterator() %>% iter_next()\nexample$img %>% as.array() %>% as.raster() %>% plot()\n\n\n\n\nHere’s the complete preprocessing pipeline.\n\n\ncreate_dataset <- function(data, train, batch_size = 32L) {\n  \n  dataset <- data %>% \n    tensor_slices_dataset() %>% \n    dataset_map(~.x %>% list_modify(\n      img = tf$image$decode_jpeg(tf$io$read_file(.x$img)),\n      mask = tf$image$decode_gif(tf$io$read_file(.x$mask))[1,,,][,,1,drop=FALSE]\n    )) %>% \n    dataset_map(~.x %>% list_modify(\n      img = tf$image$convert_image_dtype(.x$img, dtype = tf$float32),\n      mask = tf$image$convert_image_dtype(.x$mask, dtype = tf$float32)\n    )) %>% \n    dataset_map(~.x %>% list_modify(\n      img = tf$image$resize(.x$img, size = shape(128, 128)),\n      mask = tf$image$resize(.x$mask, size = shape(128, 128))\n    ))\n  \n  # data augmentation performed on training set only\n  if (train) {\n    dataset <- dataset %>% \n      dataset_map(~.x %>% list_modify(\n        img = random_bsh(.x$img)\n      )) \n  }\n  \n  # shuffling on training set only\n  if (train) {\n    dataset <- dataset %>% \n      dataset_shuffle(buffer_size = batch_size*128)\n  }\n  \n  # train in batches; batch size might need to be adapted depending on\n  # available memory\n  dataset <- dataset %>% \n    dataset_batch(batch_size)\n  \n  dataset %>% \n    # output needs to be unnamed\n    dataset_map(unname) \n}\n\nTraining and test set creation now is just a matter of two function calls.\n\n\ntraining_dataset <- create_dataset(training(data), train = TRUE)\nvalidation_dataset <- create_dataset(testing(data), train = FALSE)\n\nAnd we’re ready to train the model.\nTraining the model\nWe already showed how to create the model, but let’s repeat it here, and check model architecture:\n\n\nmodel <- unet(input_shape = c(128, 128, 3))\nsummary(model)\n\n\nModel: \"model\"\n______________________________________________________________________________________________\nLayer (type)                   Output Shape        Param #    Connected to                    \n==============================================================================================\ninput_1 (InputLayer)           [(None, 128, 128, 3 0                                          \n______________________________________________________________________________________________\nconv2d (Conv2D)                (None, 128, 128, 64 1792       input_1[0][0]                   \n______________________________________________________________________________________________\nconv2d_1 (Conv2D)              (None, 128, 128, 64 36928      conv2d[0][0]                    \n______________________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)   (None, 64, 64, 64)  0          conv2d_1[0][0]                  \n______________________________________________________________________________________________\nconv2d_2 (Conv2D)              (None, 64, 64, 128) 73856      max_pooling2d[0][0]             \n______________________________________________________________________________________________\nconv2d_3 (Conv2D)              (None, 64, 64, 128) 147584     conv2d_2[0][0]                  \n______________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D) (None, 32, 32, 128) 0          conv2d_3[0][0]                  \n______________________________________________________________________________________________\nconv2d_4 (Conv2D)              (None, 32, 32, 256) 295168     max_pooling2d_1[0][0]           \n______________________________________________________________________________________________\nconv2d_5 (Conv2D)              (None, 32, 32, 256) 590080     conv2d_4[0][0]                  \n______________________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D) (None, 16, 16, 256) 0          conv2d_5[0][0]                  \n______________________________________________________________________________________________\nconv2d_6 (Conv2D)              (None, 16, 16, 512) 1180160    max_pooling2d_2[0][0]           \n______________________________________________________________________________________________\nconv2d_7 (Conv2D)              (None, 16, 16, 512) 2359808    conv2d_6[0][0]                  \n______________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D) (None, 8, 8, 512)   0          conv2d_7[0][0]                  \n______________________________________________________________________________________________\ndropout (Dropout)              (None, 8, 8, 512)   0          max_pooling2d_3[0][0]           \n______________________________________________________________________________________________\nconv2d_8 (Conv2D)              (None, 8, 8, 1024)  4719616    dropout[0][0]                   \n______________________________________________________________________________________________\nconv2d_9 (Conv2D)              (None, 8, 8, 1024)  9438208    conv2d_8[0][0]                  \n______________________________________________________________________________________________\nconv2d_transpose (Conv2DTransp (None, 16, 16, 512) 2097664    conv2d_9[0][0]                  \n______________________________________________________________________________________________\nconcatenate (Concatenate)      (None, 16, 16, 1024 0          conv2d_7[0][0]                  \n                                                              conv2d_transpose[0][0]          \n______________________________________________________________________________________________\nconv2d_10 (Conv2D)             (None, 16, 16, 512) 4719104    concatenate[0][0]               \n______________________________________________________________________________________________\nconv2d_11 (Conv2D)             (None, 16, 16, 512) 2359808    conv2d_10[0][0]                 \n______________________________________________________________________________________________\nconv2d_transpose_1 (Conv2DTran (None, 32, 32, 256) 524544     conv2d_11[0][0]                 \n______________________________________________________________________________________________\nconcatenate_1 (Concatenate)    (None, 32, 32, 512) 0          conv2d_5[0][0]                  \n                                                              conv2d_transpose_1[0][0]        \n______________________________________________________________________________________________\nconv2d_12 (Conv2D)             (None, 32, 32, 256) 1179904    concatenate_1[0][0]             \n______________________________________________________________________________________________\nconv2d_13 (Conv2D)             (None, 32, 32, 256) 590080     conv2d_12[0][0]                 \n______________________________________________________________________________________________\nconv2d_transpose_2 (Conv2DTran (None, 64, 64, 128) 131200     conv2d_13[0][0]                 \n______________________________________________________________________________________________\nconcatenate_2 (Concatenate)    (None, 64, 64, 256) 0          conv2d_3[0][0]                  \n                                                              conv2d_transpose_2[0][0]        \n______________________________________________________________________________________________\nconv2d_14 (Conv2D)             (None, 64, 64, 128) 295040     concatenate_2[0][0]             \n______________________________________________________________________________________________\nconv2d_15 (Conv2D)             (None, 64, 64, 128) 147584     conv2d_14[0][0]                 \n______________________________________________________________________________________________\nconv2d_transpose_3 (Conv2DTran (None, 128, 128, 64 32832      conv2d_15[0][0]                 \n______________________________________________________________________________________________\nconcatenate_3 (Concatenate)    (None, 128, 128, 12 0          conv2d_1[0][0]                  \n                                                              conv2d_transpose_3[0][0]        \n______________________________________________________________________________________________\nconv2d_16 (Conv2D)             (None, 128, 128, 64 73792      concatenate_3[0][0]             \n______________________________________________________________________________________________\nconv2d_17 (Conv2D)             (None, 128, 128, 64 36928      conv2d_16[0][0]                 \n______________________________________________________________________________________________\nconv2d_18 (Conv2D)             (None, 128, 128, 1) 65         conv2d_17[0][0]                 \n==============================================================================================\nTotal params: 31,031,745\nTrainable params: 31,031,745\nNon-trainable params: 0\n______________________________________________________________________________________________\nThe “output shape” column shows the expected U-shape numerically: Width and height first go down, until we reach a minimum resolution of 8x8; they then go up again, until we’ve reached the original resolution. At the same time, the number of filters first goes up, then goes down again, until in the output layer we have a single filter. You can also see the concatenate layers appending information that comes from “below” to information that comes “laterally.”\nWhat should be the loss function here? We’re labeling each pixel, so each pixel contributes to the loss. We have a binary problem – each pixel may be “car” or “background” – so we want each output to be close to either 0 or 1. This makes binary_crossentropy the adequate loss function.\nDuring training, we keep track of classification accuracy as well as the dice coefficient, the evaluation metric used in the competition. The dice coefficient is a way to measure the proportion of correct classifications:\n\n\ndice <- custom_metric(\"dice\", function(y_true, y_pred, smooth = 1.0) {\n  y_true_f <- k_flatten(y_true)\n  y_pred_f <- k_flatten(y_pred)\n  intersection <- k_sum(y_true_f * y_pred_f)\n  (2 * intersection + smooth) / (k_sum(y_true_f) + k_sum(y_pred_f) + smooth)\n})\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(lr = 1e-5),\n  loss = \"binary_crossentropy\",\n  metrics = list(dice, metric_binary_accuracy)\n)\n\nFitting the model takes some time – how much, of course, will depend on your hardware.1 But the wait pays off: After five epochs, we saw a dice coefficient of ~ 0.87 on the validation set, and an accuracy of ~ 0.95.\nPredictions\nOf course, what we’re ultimately interested in are predictions. Let’s see a few masks generated for items from the validation set:\n\n\nbatch <- validation_dataset %>% as_iterator() %>% iter_next()\npredictions <- predict(model, batch)\n\nimages <- tibble(\n  image = batch[[1]] %>% array_branch(1),\n  predicted_mask = predictions[,,,1] %>% array_branch(1),\n  mask = batch[[2]][,,,1]  %>% array_branch(1)\n) %>% \n  sample_n(2) %>% \n  map_depth(2, function(x) {\n    as.raster(x) %>% magick::image_read()\n  }) %>% \n  map(~do.call(c, .x))\n\n\nout <- magick::image_append(c(\n  magick::image_append(images$mask, stack = TRUE),\n  magick::image_append(images$image, stack = TRUE), \n  magick::image_append(images$predicted_mask, stack = TRUE)\n  )\n)\n\nplot(out)\n\n\n\n\nFigure 3: From left to right: ground truth, input image, and predicted mask from U-Net.\n\n\n\nConclusion\nIf there were a competition for the highest sum of usefulness and architectural transparency, U-Net would certainly be a contender. Without much tuning, it’s possible to obtain decent results. If you’re able to put this model to use in your work, or if you have problems using it, let us know! Thanks for reading!\n\n\nRonneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” CoRR abs/1505.04597. http://arxiv.org/abs/1505.04597.\n\n\nExpect up to half an hour on a laptop CPU.↩︎\n",
    "preview": "posts/2019-08-23-unet/images/unet.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 1400,
    "preview_height": 932
  },
  {
    "path": "posts/2019-07-31-censored-data/",
    "title": "Modeling censored data with tfprobability",
    "description": "In this post we use tfprobability, the R interface to TensorFlow Probability, to model censored data. Again, the exposition is inspired by the treatment of this topic in Richard McElreath's Statistical Rethinking. Instead of cute cats though, we model immaterial entities from the cold world of technology: This post explores durations of CRAN package checks, a dataset that comes with Max Kuhn's parsnip.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-07-31",
    "categories": [
      "Bayesian Modeling",
      "TensorFlow/Keras"
    ],
    "contents": "\nNothing’s ever perfect, and data isn’t either. One type of “imperfection” is missing data, where some features are unobserved for some subjects. (A topic for another post.) Another is censored data, where an event whose characteristics we want to measure does not occur in the observation interval. The example in Richard McElreath’s Statistical Rethinking is time to adoption of cats in an animal shelter. If we fix an interval and observe wait times for those cats that actually did get adopted, our estimate will end up too optimistic: We don’t take into account those cats who weren’t adopted during this interval and thus, would have contributed wait times of length longer than the complete interval.\nIn this post, we use a slightly less emotional example which nonetheless may be of interest, especially to R package developers: time to completion of R CMD check, collected from CRAN and provided by the parsnip package as check_times. Here, the censored portion are those checks that errored out for whatever reason, i.e., for which the check did not complete.\nWhy do we care about the censored portion? In the cat adoption scenario, this is pretty obvious: We want to be able to get a realistic estimate for any unknown cat, not just those cats that will turn out to be “lucky”. How about check_times? Well, if your submission is one of those that errored out, you still care about how long you wait, so even though their percentage is low (< 1%) we don’t want to simply exclude them. Also, there is the possibility that the failing ones would have taken longer, had they run to completion, due to some intrinsic difference between both groups. Conversely, if failures were random, the longer-running checks would have a greater chance to get hit by an error. So here too, exluding the censored data may result in bias.\nHow can we model durations for that censored portion, where the “true duration” is unknown? Taking one step back, how can we model durations in general? Making as few assumptions as possible, the maximum entropy distribution for displacements (in space or time) is the exponential. Thus, for the checks that actually did complete, durations are assumed to be exponentially distributed.\nFor the others, all we know is that in a virtual world where the check completed, it would take at least as long as the given duration. This quantity can be modeled by the exponential complementary cumulative distribution function (CCDF). Why? A cumulative distribution function (CDF) indicates the probability that a value lower or equal to some reference point was reached; e.g., “the probability of durations <= 255 is 0.9”. Its complement, 1 - CDF, then gives the probability that a value will exceed than that reference point.\nLet’s see this in action.\nThe data\nThe following code works with the current stable releases of TensorFlow and TensorFlow Probability, which are 1.14 and 0.7, respectively. If you don’t have tfprobability installed, get it from Github:\n\n\nremotes::install_github(\"rstudio/tfprobability\")\n\nThese are the libraries we need. As of TensorFlow 1.14, we call tf$compat$v2$enable_v2_behavior() to run with eager execution.\n\n\nlibrary(tensorflow)\nlibrary(tfprobability)\nlibrary(parsnip)\nlibrary(tidyverse)\nlibrary(zeallot)\nlibrary(gridExtra)\nlibrary(HDInterval)\nlibrary(tidymodels)\nlibrary(survival)\n\ntf$compat$v2$enable_v2_behavior()\n\nBesides the check durations we want to model, check_times reports various features of the package in question, such as number of imported packages, number of dependencies, size of code and documentation files, etc. The status variable indicates whether the check completed or errored out.\n\n\ndf <- check_times %>% select(-package)\nglimpse(df)\n\n\nObservations: 13,626\nVariables: 24\n$ authors        <int> 1, 1, 1, 1, 5, 3, 2, 1, 4, 6, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ imports        <dbl> 0, 6, 0, 0, 3, 1, 0, 4, 0, 7, 0, 0, 0, 0, 3, 2, 14, 2, 2, 0…\n$ suggests       <dbl> 2, 4, 0, 0, 2, 0, 2, 2, 0, 0, 2, 8, 0, 0, 2, 0, 1, 3, 0, 0,…\n$ depends        <dbl> 3, 1, 6, 1, 1, 1, 5, 0, 1, 1, 6, 5, 0, 0, 0, 1, 1, 5, 0, 2,…\n$ Roxygen        <dbl> 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,…\n$ gh             <dbl> 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,…\n$ rforge         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ descr          <int> 217, 313, 269, 63, 223, 1031, 135, 344, 204, 335, 104, 163,…\n$ r_count        <int> 2, 20, 8, 0, 10, 10, 16, 3, 6, 14, 16, 4, 1, 1, 11, 5, 7, 1…\n$ r_size         <dbl> 0.029053, 0.046336, 0.078374, 0.000000, 0.019080, 0.032607,…\n$ ns_import      <dbl> 3, 15, 6, 0, 4, 5, 0, 4, 2, 10, 5, 6, 1, 0, 2, 2, 1, 11, 0,…\n$ ns_export      <dbl> 0, 19, 0, 0, 10, 0, 0, 2, 0, 9, 3, 4, 0, 1, 10, 0, 16, 0, 2…\n$ s3_methods     <dbl> 3, 0, 11, 0, 0, 0, 0, 2, 0, 23, 0, 0, 2, 5, 0, 4, 0, 0, 0, …\n$ s4_methods     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ doc_count      <int> 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ doc_size       <dbl> 0.000000, 0.019757, 0.038281, 0.000000, 0.007874, 0.000000,…\n$ src_count      <int> 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 3, 0, 0, 0, 0, 0, 0, 54, 0, 0…\n$ src_size       <dbl> 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000,…\n$ data_count     <int> 2, 0, 0, 3, 3, 1, 10, 0, 4, 2, 2, 146, 0, 0, 0, 0, 0, 10, 0…\n$ data_size      <dbl> 0.025292, 0.000000, 0.000000, 4.885864, 4.595504, 0.006500,…\n$ testthat_count <int> 0, 8, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0,…\n$ testthat_size  <dbl> 0.000000, 0.002496, 0.000000, 0.000000, 0.000000, 0.000000,…\n$ check_time     <dbl> 49, 101, 292, 21, 103, 46, 78, 91, 47, 196, 200, 169, 45, 2…\n$ status         <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\nOf these 13,626 observations, just 103 are censored:\n\n\ntable(df$status)\n\n\n0     1 \n103 13523 \nFor better readability, we’ll work with a subset of the columns. We use surv_reg to help us find a useful and interesting subset of predictors:\n\n\nsurvreg_fit <-\n  surv_reg(dist = \"exponential\") %>% \n  set_engine(\"survreg\") %>% \n  fit(Surv(check_time, status) ~ ., \n      data = df)\ntidy(survreg_fit) \n\n\n# A tibble: 23 x 7\n   term             estimate std.error statistic  p.value conf.low conf.high\n   <chr>               <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n 1 (Intercept)     3.86      0.0219     176.     0.             NA        NA\n 2 authors         0.0139    0.00580      2.40   1.65e- 2       NA        NA\n 3 imports         0.0606    0.00290     20.9    7.49e-97       NA        NA\n 4 suggests        0.0332    0.00358      9.28   1.73e-20       NA        NA\n 5 depends         0.118     0.00617     19.1    5.66e-81       NA        NA\n 6 Roxygen         0.0702    0.0209       3.36   7.87e- 4       NA        NA\n 7 gh              0.00898   0.0217       0.414  6.79e- 1       NA        NA\n 8 rforge          0.0232    0.0662       0.351  7.26e- 1       NA        NA\n 9 descr           0.000138  0.0000337    4.10   4.18e- 5       NA        NA\n10 r_count         0.00209   0.000525     3.98   7.03e- 5       NA        NA\n11 r_size          0.481     0.0819       5.87   4.28e- 9       NA        NA\n12 ns_import       0.00352   0.000896     3.93   8.48e- 5       NA        NA\n13 ns_export      -0.00161   0.000308    -5.24   1.57e- 7       NA        NA\n14 s3_methods      0.000449  0.000421     1.06   2.87e- 1       NA        NA\n15 s4_methods     -0.00154   0.00206     -0.745  4.56e- 1       NA        NA\n16 doc_count       0.0739    0.0117       6.33   2.44e-10       NA        NA\n17 doc_size        2.86      0.517        5.54   3.08e- 8       NA        NA\n18 src_count       0.0122    0.00127      9.58   9.96e-22       NA        NA\n19 src_size       -0.0242    0.0181      -1.34   1.82e- 1       NA        NA\n20 data_count      0.0000415 0.000980     0.0423 9.66e- 1       NA        NA\n21 data_size       0.0217    0.0135       1.61   1.08e- 1       NA        NA\n22 testthat_count -0.000128  0.00127     -0.101  9.20e- 1       NA        NA\n23 testthat_size   0.0108    0.0139       0.774  4.39e- 1       NA        NA\n\nIt seems that if we choose imports, depends, r_size, doc_size, ns_import and ns_export we end up with a mix of (comparatively) powerful predictors from different semantic spaces and of different scales.\nBefore pruning the dataframe, we save away the target variable. In our model and training setup, it is convenient to have censored and uncensored data stored separately, so here we create two target matrices instead of one:\n\n\n# check times for failed checks\n# _c stands for censored\ncheck_time_c <- df %>%\n  filter(status == 0) %>%\n  select(check_time) %>%\n  as.matrix()\n\n# check times for successful checks \ncheck_time_nc <- df %>%\n  filter(status == 1) %>%\n  select(check_time) %>%\n  as.matrix()\n\nNow we can zoom in on the variables of interest, setting up one dataframe for the censored data and one for the uncensored data each. All predictors are normalized to avoid overflow during sampling. 1 We add a column of 1s for use as an intercept.\n\n\ndf <- df %>% select(status,\n                    depends,\n                    imports,\n                    doc_size,\n                    r_size,\n                    ns_import,\n                    ns_export) %>%\n  mutate_at(.vars = 2:7, .funs = function(x) (x - min(x))/(max(x)-min(x))) %>%\n  add_column(intercept = rep(1, nrow(df)), .before = 1)\n\n# dataframe of predictors for censored data  \ndf_c <- df %>% filter(status == 0) %>% select(-status)\n# dataframe of predictors for non-censored data \ndf_nc <- df %>% filter(status == 1) %>% select(-status)\n\nThat’s it for preparations. But of course we’re curious. Do check times look different? Do predictors – the ones we chose – look different?\nComparing a few meaningful percentiles for both classes, we see that durations for uncompleted checks are higher than those for completed checks throughout, apart from the 100% percentile. It’s not surprising that given the enormous difference in sample size, maximum duration is higher for completed checks. Otherwise though, doesn’t it look like the errored-out package checks “were going to take longer”?\n\npercentiles: check time\n10%\n30%\n50%\n70%\n90%\n100%\ncompleted\n36\n54\n79\n115\n211\n1343\nnot completed\n42\n71\n97\n143\n293\n696\n\nHow about the predictors? We don’t see any differences for depends, the number of package dependencies (apart from, again, the higher maximum reached for packages whose check completed): 2\n\npercentiles: depends\n10%\n30%\n50%\n70%\n90%\n100%\ncompleted\n0\n1\n1\n2\n4\n12\nnot completed\n0\n1\n1\n2\n4\n7\n\nBut for all others, we see the same pattern as reported above for check_time. Number of packages imported is higher for censored data at all percentiles besides the maximum:\n\npercentiles: imports\n10%\n30%\n50%\n70%\n90%\n100%\ncompleted\n0\n0\n2\n4\n9\n43\nnot completed\n0\n1\n5\n8\n12\n22\n\nSame for ns_export, the estimated number of exported functions or methods:\n\npercentiles: ns_export\n10%\n30%\n50%\n70%\n90%\n100%\ncompleted\n0\n1\n2\n8\n26\n2547\nnot completed\n0\n1\n5\n13\n34\n336\n\nAs well as for ns_import, the estimated number of imported functions or methods:\n\npercentiles: ns_import\n10%\n30%\n50%\n70%\n90%\n100%\ncompleted\n0\n1\n3\n6\n19\n312\nnot completed\n0\n2\n5\n11\n23\n297\n\nSame pattern for r_size, the size on disk of files in the R directory:\n\npercentiles: r_size\n10%\n30%\n50%\n70%\n90%\n100%\ncompleted\n0.005\n0.015\n0.031\n0.063\n0.176\n3.746\nnot completed\n0.008\n0.019\n0.041\n0.097\n0.217\n2.148\n\nAnd finally, we see it for doc_size too, where doc_size is the size of .Rmd and .Rnw files:\n\npercentiles: doc_size\n10%\n30%\n50%\n70%\n90%\n100%\ncompleted\n0.000\n0.000\n0.000\n0.000\n0.023\n0.988\nnot completed\n0.000\n0.000\n0.000\n0.011\n0.042\n0.114\n\nGiven our task at hand – model check durations taking into account uncensored as well as censored data – we won’t dwell on differences between both groups any longer; nonetheless we thought it interesting to relate these numbers.\nSo now, back to work. We need to create a model.\nThe model\nAs explained in the introduction, for completed checks duration is modeled using an exponential PDF. This is as straightforward as adding tfd_exponential() to the model function, tfd_joint_distribution_sequential(). 3For the censored portion, we need the exponential CCDF. This one is not, as of today, easily added to the model. What we can do though is calculate its value ourselves and add it to the “main” model likelihood. We’ll see this below when discussing sampling; for now it means the model definition ends up straightforward as it only covers the non-censored data. It is made of just the said exponential PDF and priors for the regression parameters.\nAs for the latter, we use 0-centered, Gaussian priors for all parameters. Standard deviations of 1 turned out to work well. As the priors are all the same, instead of listing a bunch of tfd_normals, we can create them all at once as\n\n\ntfd_sample_distribution(tfd_normal(0, 1), sample_shape = 7)\n\nMean check time is modeled as an affine combination of the six predictors and the intercept. Here then is the complete model, instantiated using the uncensored data only:\n\n\nmodel <- function(data) {\n  tfd_joint_distribution_sequential(\n    list(\n      tfd_sample_distribution(tfd_normal(0, 1), sample_shape = 7),\n      function(betas)\n        tfd_independent(\n          tfd_exponential(\n            rate = 1 / tf$math$exp(tf$transpose(\n              tf$matmul(tf$cast(data, betas$dtype), tf$transpose(betas))))),\n          reinterpreted_batch_ndims = 1)))\n}\n\nm <- model(df_nc %>% as.matrix())\n\nAlways, we test if samples from that model have the expected shapes:\n\n\nsamples <- m %>% tfd_sample(2)\nsamples\n\n\n[[1]]\ntf.Tensor(\n[[ 1.4184642   0.17583323 -0.06547955 -0.2512014   0.1862184  -1.2662812\n   1.0231884 ]\n [-0.52142304 -1.0036682   2.2664437   1.29737     1.1123234   0.3810004\n   0.1663677 ]], shape=(2, 7), dtype=float32)\n\n[[2]]\ntf.Tensor(\n[[4.4954767  7.865639   1.8388556  ... 7.914391   2.8485563  3.859719  ]\n [1.549662   0.77833986 0.10015647 ... 0.40323067 3.42171    0.69368565]], shape=(2, 13523), dtype=float32)\nThis looks fine: We have a list of length two, one element for each distribution in the model. For both tensors, dimension 1 reflects the batch size (which we arbitrarily set to 2 in this test), while dimension 2 is 7 for the number of normal priors and 13523 for the number of durations predicted.\nHow likely are these samples?\n\n\nm %>% tfd_log_prob(samples)\n\n\ntf.Tensor([-32464.521   -7693.4023], shape=(2,), dtype=float32)\nHere too, the shape is correct, and the values look reasonable.\nThe next thing to do is define the target we want to optimize.\nOptimization target\nAbstractly, the thing to maximize is the log probility of the data – that is, the measured durations – under the model. Now here the data comes in two parts, and the target does as well. First, we have the non-censored data, for which\n\n\nm %>% tfd_log_prob(list(betas, tf$cast(target_nc, betas$dtype)))\n\nwill calculate the log probability. Second, to obtain log probability for the censored data we write a custom function that calculates the log of the exponential CCDF:\n\n\nget_exponential_lccdf <- function(betas, data, target) {\n  e <-  tfd_independent(tfd_exponential(rate = 1 / tf$math$exp(tf$transpose(tf$matmul(\n    tf$cast(data, betas$dtype), tf$transpose(betas)\n  )))),\n  reinterpreted_batch_ndims = 1)\n  cum_prob <- e %>% tfd_cdf(tf$cast(target, betas$dtype))\n  tf$math$log(1 - cum_prob)\n}\n\nBoth parts are combined in a little wrapper function that allows us to compare training including and excluding the censored data. We won’t do that in this post, but you might be interested to do it with your own data, especially if the ratio of censored and uncensored parts is a little less imbalanced.\n\n\nget_log_prob <-\n  function(target_nc,\n           censored_data = NULL,\n           target_c = NULL) {\n    log_prob <- function(betas) {\n      log_prob <-\n        m %>% tfd_log_prob(list(betas, tf$cast(target_nc, betas$dtype)))\n      potential <-\n        if (!is.null(censored_data) && !is.null(target_c))\n          get_exponential_lccdf(betas, censored_data, target_c)\n      else\n        0\n      log_prob + potential\n    }\n    log_prob\n  }\n\nlog_prob <-\n  get_log_prob(\n    check_time_nc %>% tf$transpose(),\n    df_c %>% as.matrix(),\n    check_time_c %>% tf$transpose()\n  )\n\nSampling\nWith model and target defined, we’re ready to do sampling.\n\n\nn_chains <- 4\nn_burnin <- 1000\nn_steps <- 1000\n\n# keep track of some diagnostic output, acceptance and step size\ntrace_fn <- function(state, pkr) {\n  list(\n    pkr$inner_results$is_accepted,\n    pkr$inner_results$accepted_results$step_size\n  )\n}\n\n# get shape of initial values \n# to start sampling without producing NaNs, we will feed the algorithm\n# tf$zeros_like(initial_betas)\n# instead \ninitial_betas <- (m %>% tfd_sample(n_chains))[[1]]\n\nFor the number of leapfrog steps and the step size, experimentation showed that a combination of 64 / 0.1 yielded reasonable results:\n\n\nhmc <- mcmc_hamiltonian_monte_carlo(\n  target_log_prob_fn = log_prob,\n  num_leapfrog_steps = 64,\n  step_size = 0.1\n) %>%\n  mcmc_simple_step_size_adaptation(target_accept_prob = 0.8,\n                                   num_adaptation_steps = n_burnin)\n\nrun_mcmc <- function(kernel) {\n  kernel %>% mcmc_sample_chain(\n    num_results = n_steps,\n    num_burnin_steps = n_burnin,\n    current_state = tf$ones_like(initial_betas),\n    trace_fn = trace_fn\n  )\n}\n\n# important for performance: run HMC in graph mode\nrun_mcmc <- tf_function(run_mcmc)\n\nres <- hmc %>% run_mcmc()\nsamples <- res$all_states\n\nResults\nBefore we inspect the chains, here is a quick look at the proportion of accepted steps and the per-parameter mean step size:\n\n\naccepted <- res$trace[[1]]\nas.numeric(accepted) %>% mean()\n\n\n0.995\n\n\nstep_size <- res$trace[[2]]\nas.numeric(step_size) %>% mean()\n\n\n0.004953894\nWe also store away effective sample sizes and the rhat metrics for later addition to the synopsis.\n\n\neffective_sample_size <- mcmc_effective_sample_size(samples) %>%\n  as.matrix() %>%\n  apply(2, mean)\npotential_scale_reduction <- mcmc_potential_scale_reduction(samples) %>%\n  as.numeric()\n\nWe then convert the samples tensor to an R array for use in postprocessing.\n\n\n# 2-item list, where each item has dim (1000, 4)\nsamples <- as.array(samples) %>% array_branch(margin = 3)\n\nHow well did the sampling work? The chains mix well, but for some parameters, autocorrelation is still pretty high.\n\n\nprep_tibble <- function(samples) {\n  as_tibble(samples,\n            .name_repair = ~ c(\"chain_1\", \"chain_2\", \"chain_3\", \"chain_4\")) %>%\n    add_column(sample = 1:n_steps) %>%\n    gather(key = \"chain\", value = \"value\",-sample)\n}\n\nplot_trace <- function(samples) {\n  prep_tibble(samples) %>%\n    ggplot(aes(x = sample, y = value, color = chain)) +\n    geom_line() +\n    theme_light() +\n    theme(\n      legend.position = \"none\",\n      axis.title = element_blank(),\n      axis.text = element_blank(),\n      axis.ticks = element_blank()\n    )\n}\n\nplot_traces <- function(samples) {\n  plots <- purrr::map(samples, plot_trace)\n  do.call(grid.arrange, plots)\n}\n\nplot_traces(samples)\n\n\n\n\nFigure 1: Trace plots for the 7 parameters.\n\n\n\nNow for a synopsis of posterior parameter statistics, including the usual per-parameter sampling indicators effective sample size and rhat.\n\n\nall_samples <- map(samples, as.vector)\n\nmeans <- map_dbl(all_samples, mean)\n\nsds <- map_dbl(all_samples, sd)\n\nhpdis <- map(all_samples, ~ hdi(.x) %>% t() %>% as_tibble())\n\nsummary <- tibble(\n  mean = means,\n  sd = sds,\n  hpdi = hpdis\n) %>% unnest() %>%\n  add_column(param = colnames(df_c), .after = FALSE) %>%\n  add_column(\n    n_effective = effective_sample_size,\n    rhat = potential_scale_reduction\n  )\n\nsummary\n\n\n# A tibble: 7 x 7\n  param       mean     sd  lower upper n_effective  rhat\n  <chr>      <dbl>  <dbl>  <dbl> <dbl>       <dbl> <dbl>\n1 intercept  4.05  0.0158  4.02   4.08       508.   1.17\n2 depends    1.34  0.0732  1.18   1.47      1000    1.00\n3 imports    2.89  0.121   2.65   3.12      1000    1.00\n4 doc_size   6.18  0.394   5.40   6.94       177.   1.01\n5 r_size     2.93  0.266   2.42   3.46       289.   1.00\n6 ns_import  1.54  0.274   0.987  2.06       387.   1.00\n7 ns_export -0.237 0.675  -1.53   1.10        66.8  1.01\n\n\n\nFigure 2: Posterior means and HPDIs.\n\n\n\nFrom the diagnostics and trace plots, the model seems to work reasonably well, but as there is no straightforward error metric involved, it’s hard to know if actual predictions would even land in an appropriate range.\nTo make sure they do, we inspect predictions from our model as well as from surv_reg. This time, we also split the data into training and test sets. Here first are the predictions from surv_reg:\n\n\ntrain_test_split <- initial_split(check_times, strata = \"status\")\ncheck_time_train <- training(train_test_split)\ncheck_time_test <- testing(train_test_split)\n\nsurvreg_fit <-\n  surv_reg(dist = \"exponential\") %>% \n  set_engine(\"survreg\") %>% \n  fit(Surv(check_time, status) ~ depends + imports + doc_size + r_size + \n        ns_import + ns_export, \n      data = check_time_train)\nsurvreg_fit(sr_fit)\n\n\n# A tibble: 7 x 7\n  term         estimate std.error statistic  p.value conf.low conf.high\n  <chr>           <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)  4.05      0.0174     234.    0.             NA        NA\n2 depends      0.108     0.00701     15.4   3.40e-53       NA        NA\n3 imports      0.0660    0.00327     20.2   1.09e-90       NA        NA\n4 doc_size     7.76      0.543       14.3   2.24e-46       NA        NA\n5 r_size       0.812     0.0889       9.13  6.94e-20       NA        NA\n6 ns_import    0.00501   0.00103      4.85  1.22e- 6       NA        NA\n7 ns_export   -0.000212  0.000375    -0.566 5.71e- 1       NA        NA\n\n\nsurvreg_pred <- \n  predict(survreg_fit, check_time_test) %>% \n  bind_cols(check_time_test %>% select(check_time, status))  \n\nggplot(survreg_pred, aes(x = check_time, y = .pred, color = factor(status))) +\n  geom_point() + \n  coord_cartesian(ylim = c(0, 1400))\n\n\n\n\nFigure 3: Test set predictions from surv_reg. One outlier (of value 160421) is excluded via coord_cartesian() to avoid distorting the plot.\n\n\n\nFor the MCMC model, we re-train on just the training set and obtain the parameter summary. The code is analogous to the above and not shown here.\nWe can now predict on the test set, for simplicity just using the posterior means:\n\n\ndf <- check_time_test %>% select(\n                    depends,\n                    imports,\n                    doc_size,\n                    r_size,\n                    ns_import,\n                    ns_export) %>%\n  add_column(intercept = rep(1, nrow(check_time_test)), .before = 1)\n\nmcmc_pred <- df %>% as.matrix() %*% summary$mean %>% exp() %>% as.numeric()\nmcmc_pred <- check_time_test %>% select(check_time, status) %>%\n  add_column(.pred = mcmc_pred)\n\nggplot(mcmc_pred, aes(x = check_time, y = .pred, color = factor(status))) +\n  geom_point() + \n  coord_cartesian(ylim = c(0, 1400)) \n\n\n\n\nFigure 4: Test set predictions from the mcmc model. No outliers, just using same scale as above for comparison.\n\n\n\nThis looks good!\nWrapup\nWe’ve shown how to model censored data – or rather, a frequent subtype thereof involving durations – using tfprobability. The check_times data from parsnip were a fun choice, but this modeling technique may be even more useful when censoring is more substantial. Hopefully his post has provided some guidance on how to handle censored data in your own work. Thanks for reading!\nBy itself, the predictors being on different scales isn’t a problem. Here we need to normalize due to the log link used in the model above.↩︎\nHere and in the following tables, we report the unnormalized, original values as contained in check_times.↩︎\nFor a first introduction to MCMC sampling with tfprobability, see Tadpoles on TensorFlow: Hierarchical partial pooling with tfprobability↩︎\n",
    "preview": "posts/2019-07-31-censored-data/images/thumb_cropped.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 955,
    "preview_height": 396
  },
  {
    "path": "posts/2019-07-09-feature-columns/",
    "title": "TensorFlow feature columns: Transforming your data recipes-style",
    "description": "TensorFlow feature columns provide useful functionality for preprocessing categorical data and chaining transformations, like bucketization or feature crossing. From R, we use them in popular \"recipes\" style, creating and subsequently refining a feature specification. In this post, we show how using feature specs frees cognitive resources and lets you focus on what you really want to accomplish. What's more, because of its elegance, feature-spec code reads nice and is fun to write as well.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": {}
      },
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-07-09",
    "categories": [
      "TensorFlow/Keras",
      "Tabular Data"
    ],
    "contents": "\nIt’s 2019; no one doubts the effectiveness of deep learning in computer vision. Or natural language processing. With “normal”, Excel-style, a.k.a. tabular data however, the situation is different.\nBasically there are two cases: One, you have numeric data only. Then, creating the network is straightforward, and all will be about optimization and hyperparameter search. Two, you have a mix of numeric and categorical data, where categorical could be anything from ordered-numeric to symbolic (e.g., text). In this latter case, with categorical data entering the picture, there is an extremely nice idea you can make use of: embed what are equidistant symbols into a high-dimensional, numeric representation. In that new representation, we can define a distance metric that allows us to make statements like “cycling is closer to running than to baseball”, or “😃 is closer to 😂 than to 😠”. When not dealing with language data, this technique is referred to as entity embeddings.1\nNice as this sounds, why don’t we see entity embeddings used all the time? Well, creating a Keras network that processes a mix of numeric and categorical data used to require a bit of an effort. With TensorFlow’s new feature columns, usable from R through a combination of tfdatasets and keras, there is a much easier way to achieve this. What’s more, tfdatasets follows the popular recipes idiom to initialize, refine, and apply a feature specification %>%-style. And finally, there are ready-made steps for bucketizing a numeric column, or hashing it, or creating crossed columns to capture interactions.\nThis post introduces feature specs starting from a scenario where they don’t exist: basically, the status quo until very recently. Imagine you have a dataset like that from the Porto Seguro car insurance competition where some of the columns are numeric, and some are categorical. You want to train a fully connected network on it, with all categorical columns fed into embedding layers. How can you do that? We then contrast this with the feature spec way, which makes things a lot easier – especially when there’s a lot of categorical columns. In a second applied example, we demonstrate the use of crossed columns on the rugged dataset from Richard McElreath’s rethinking package. Here, we also direct attention to a few technical details that are worth knowing about.\nMixing numeric data and embeddings, the pre-feature-spec way\nOur first example dataset is taken from Kaggle. Two years ago, Brazilian car insurance company Porto Seguro asked participants to predict how likely it is a car owner will file a claim based on a mix of characteristics collected during the previous year. The dataset is comparatively large – there are ~ 600,000 rows in the training set, with 57 predictors. Among others, features are named so as to indicate the type of the data – binary, categorical, or continuous/ordinal. While it’s common in competitions to try to reverse-engineer column meanings, here we just make use of the type of the data, and see how far that gets us.\nConcretely, this means we want to\nuse binary features just the way they are, as zeroes and ones,\nscale the remaining numeric features to mean 0 and variance 1, and\nembed the categorical variables (each one by itself).\nWe’ll then define a dense network to predict target, the binary outcome. So first, let’s see how we could get our data into shape, as well as build up the network, in a “manual”, pre-feature-columns way.\nWhen loading libraries, we already use the versions we’ll need very soon: Tensorflow 2 (>= beta 1), and the development (= Github) versions of tfdatasets and keras:\n\n\ntensorflow::install_tensorflow(version = \"2.0.0-beta1\")\n\nremotes::install_github(\"rstudio/tfdatasets\")\nremotes::install_github(\"rstudio/keras\")\n\nlibrary(keras)\nlibrary(tfdatasets)\nlibrary(readr)\nlibrary(dplyr)\n\nIn this first version of preparing the data, we make our lives easier by assigning different R types, based on what the features represent (categorical, binary, or numeric qualities):2\n\n\n# downloaded from https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data\npath <- \"train.csv\"\n\nporto <- read_csv(path) %>%\n  select(-id) %>%\n  # to obtain number of unique levels, later\n  mutate_at(vars(ends_with(\"cat\")), factor) %>%\n  # to easily keep them apart from the non-binary numeric data\n  mutate_at(vars(ends_with(\"bin\")), as.integer)\n\nporto %>% glimpse()\n\n\nObservations: 595,212\nVariables: 58\n$ target         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,…\n$ ps_ind_01      <dbl> 2, 1, 5, 0, 0, 5, 2, 5, 5, 1, 5, 2, 2, 1, 5, 5,…\n$ ps_ind_02_cat  <fct> 2, 1, 4, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,…\n$ ps_ind_03      <dbl> 5, 7, 9, 2, 0, 4, 3, 4, 3, 2, 2, 3, 1, 3, 11, 3…\n$ ps_ind_04_cat  <fct> 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,…\n$ ps_ind_05_cat  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ps_ind_06_bin  <int> 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ps_ind_07_bin  <int> 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,…\n$ ps_ind_08_bin  <int> 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,…\n$ ps_ind_09_bin  <int> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…\n$ ps_ind_10_bin  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ps_ind_11_bin  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ps_ind_12_bin  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ps_ind_13_bin  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ps_ind_14      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ps_ind_15      <dbl> 11, 3, 12, 8, 9, 6, 8, 13, 6, 4, 3, 9, 10, 12, …\n$ ps_ind_16_bin  <int> 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,…\n$ ps_ind_17_bin  <int> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ps_ind_18_bin  <int> 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,…\n$ ps_reg_01      <dbl> 0.7, 0.8, 0.0, 0.9, 0.7, 0.9, 0.6, 0.7, 0.9, 0.…\n$ ps_reg_02      <dbl> 0.2, 0.4, 0.0, 0.2, 0.6, 1.8, 0.1, 0.4, 0.7, 1.…\n$ ps_reg_03      <dbl> 0.7180703, 0.7660777, -1.0000000, 0.5809475, 0.…\n$ ps_car_01_cat  <fct> 10, 11, 7, 7, 11, 10, 6, 11, 10, 11, 11, 11, 6,…\n$ ps_car_02_cat  <fct> 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,…\n$ ps_car_03_cat  <fct> -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1…\n$ ps_car_04_cat  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 0, 0, 0, 0, 9,…\n$ ps_car_05_cat  <fct> 1, -1, -1, 1, -1, 0, 1, 0, 1, 0, -1, -1, -1, 1,…\n$ ps_car_06_cat  <fct> 4, 11, 14, 11, 14, 14, 11, 11, 14, 14, 13, 11, …\n$ ps_car_07_cat  <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ ps_car_08_cat  <fct> 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,…\n$ ps_car_09_cat  <fct> 0, 2, 2, 3, 2, 0, 0, 2, 0, 2, 2, 0, 2, 2, 2, 0,…\n$ ps_car_10_cat  <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ ps_car_11_cat  <fct> 12, 19, 60, 104, 82, 104, 99, 30, 68, 104, 20, …\n$ ps_car_11      <dbl> 2, 3, 1, 1, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 1, 2,…\n$ ps_car_12      <dbl> 0.4000000, 0.3162278, 0.3162278, 0.3741657, 0.3…\n$ ps_car_13      <dbl> 0.8836789, 0.6188165, 0.6415857, 0.5429488, 0.5…\n$ ps_car_14      <dbl> 0.3708099, 0.3887158, 0.3472751, 0.2949576, 0.3…\n$ ps_car_15      <dbl> 3.605551, 2.449490, 3.316625, 2.000000, 2.00000…\n$ ps_calc_01     <dbl> 0.6, 0.3, 0.5, 0.6, 0.4, 0.7, 0.2, 0.1, 0.9, 0.…\n$ ps_calc_02     <dbl> 0.5, 0.1, 0.7, 0.9, 0.6, 0.8, 0.6, 0.5, 0.8, 0.…\n$ ps_calc_03     <dbl> 0.2, 0.3, 0.1, 0.1, 0.0, 0.4, 0.5, 0.1, 0.6, 0.…\n$ ps_calc_04     <dbl> 3, 2, 2, 2, 2, 3, 2, 1, 3, 2, 2, 2, 4, 2, 3, 2,…\n$ ps_calc_05     <dbl> 1, 1, 2, 4, 2, 1, 2, 2, 1, 2, 3, 2, 1, 1, 1, 1,…\n$ ps_calc_06     <dbl> 10, 9, 9, 7, 6, 8, 8, 7, 7, 8, 8, 8, 8, 10, 8, …\n$ ps_calc_07     <dbl> 1, 5, 1, 1, 3, 2, 1, 1, 3, 2, 2, 2, 4, 1, 2, 5,…\n$ ps_calc_08     <dbl> 10, 8, 8, 8, 10, 11, 8, 6, 9, 9, 9, 10, 11, 8, …\n$ ps_calc_09     <dbl> 1, 1, 2, 4, 2, 3, 3, 1, 4, 1, 4, 1, 1, 3, 3, 2,…\n$ ps_calc_10     <dbl> 5, 7, 7, 2, 12, 8, 10, 13, 11, 11, 7, 8, 9, 8, …\n$ ps_calc_11     <dbl> 9, 3, 4, 2, 3, 4, 3, 7, 4, 3, 6, 9, 6, 2, 4, 5,…\n$ ps_calc_12     <dbl> 1, 1, 2, 2, 1, 2, 0, 1, 2, 5, 3, 2, 3, 0, 1, 2,…\n$ ps_calc_13     <dbl> 5, 1, 7, 4, 1, 0, 0, 3, 1, 0, 3, 1, 3, 4, 3, 6,…\n$ ps_calc_14     <dbl> 8, 9, 7, 9, 3, 9, 10, 6, 5, 6, 6, 10, 8, 3, 9, …\n$ ps_calc_15_bin <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ps_calc_16_bin <int> 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,…\n$ ps_calc_17_bin <int> 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,…\n$ ps_calc_18_bin <int> 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,…\n$ ps_calc_19_bin <int> 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,…\n$ ps_calc_20_bin <int> 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,…\nWe split off 25% for validation.\n\n\n# train-test split\nid_training <- sample.int(nrow(porto), size = 0.75*nrow(porto))\n\nx_train <- porto[id_training,] %>% select(-target)\nx_test <- porto[-id_training,] %>% select(-target)\ny_train <- porto[id_training, \"target\"]\ny_test <- porto[-id_training, \"target\"] \n\nThe only thing we want to do to the data before defining the network is scaling the numeric features. Binary and categorical features can stay as is, with the minor correction that for the categorical ones, we’ll actually pass the network the numeric representation of the factor data.\nHere is the scaling.\n\n\ntrain_means <- colMeans(x_train[sapply(x_train, is.double)]) %>% unname()\ntrain_sds <- apply(x_train[sapply(x_train, is.double)], 2, sd)  %>% unname()\ntrain_sds[train_sds == 0] <- 0.000001\n\nx_train[sapply(x_train, is.double)] <- sweep(\n  x_train[sapply(x_train, is.double)],\n  2,\n  train_means\n  ) %>%\n  sweep(2, train_sds, \"/\")\nx_test[sapply(x_test, is.double)] <- sweep(\n  x_test[sapply(x_test, is.double)],\n  2,\n  train_means\n  ) %>%\n  sweep(2, train_sds, \"/\")\n\nWhen building the network, we need to specify the input and output dimensionalities for the embedding layers. Input dimensionality refers to the number of different symbols that “come in”; in NLP tasks this would be the vocabulary size while here, it’s simply the number of values a variable can take.3 Output dimensionality, the capacity of the internal representation, can then be calculated based on some heuristic. Below, we’ll follow a popular rule of thumb that takes the square root of the dimensionality of the input.\nSo as part one of the network, here we build up the embedding layers in a loop, each wired to the input layer that feeds it:\n\n\n# number of levels per factor, required to specify input dimensionality for\n# the embedding layers\nn_levels_in <- map(x_train %>% select_if(is.factor), compose(length, levels)) %>%\n  unlist() \n\n# output dimensionality for the embedding layers, need +1 because Python is 0-based\nn_levels_out <- n_levels_in %>% sqrt() %>% trunc() %>% `+`(1)\n\n# each embedding layer gets its own input layer\ncat_inputs <- map(n_levels_in, function(l) layer_input(shape = 1)) %>%\n  unname()\n\n# construct the embedding layers, connecting each to its input\nembedding_layers <- vector(mode = \"list\", length = length(cat_inputs))\nfor (i in 1:length(cat_inputs)) {\n  embedding_layer <-  cat_inputs[[i]] %>% \n    layer_embedding(input_dim = n_levels_in[[i]] + 1, output_dim = n_levels_out[[i]]) %>%\n    layer_flatten()\n  embedding_layers[[i]] <- embedding_layer\n}\n\nIn case you were wondering about the flatten layer following each embedding: We need to squeeze out the third dimension (introduced by the embedding layers) from the tensors, effectively rendering them rank-2. That is because we want to combine them with the rank-2 tensor coming out of the dense layer processing the numeric features.\nIn order to be able to combine it with anything, we have to actually construct that dense layer first. It will be connected to a single input layer, of shape 43, that takes in the numeric features we scaled as well as the binary features we left untouched:\n\n\n# create a single input and a dense layer for the numeric data\nquant_input <- layer_input(shape = 43)\n  \nquant_dense <- quant_input %>% layer_dense(units = 64)\n\nAre parts assembled, we wire them together using layer_concatenate, and we’re good to call keras_model to create the final graph.\n\n\nintermediate_layers <- list(embedding_layers, list(quant_dense)) %>% flatten()\ninputs <- list(cat_inputs, list(quant_input)) %>% flatten()\n\nl <- 0.25\n\noutput <- layer_concatenate(intermediate_layers) %>%\n  layer_dense(units = 30, activation = \"relu\", kernel_regularizer = regularizer_l2(l)) %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_dense(units = 10, activation = \"relu\", kernel_regularizer = regularizer_l2(l)) %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_dense(units = 5, activation = \"relu\", kernel_regularizer = regularizer_l2(l)) %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_dense(units = 1, activation = \"sigmoid\", kernel_regularizer = regularizer_l2(l))\n\nmodel <- keras_model(inputs, output)\n\nNow, if you’ve actually read through the whole of this part, you may wish for an easier way to get to this point. So let’s switch to feature specs for the rest of this post.\nFeature specs to the rescue\nIn spirit, the way feature specs are defined follows the example of the recipes package. (It won’t make you hungry, though.) You initialize a feature spec with the prediction target – feature_spec(target ~ .), and then use the %>% to tell it what to do with individual columns. “What to do” here signifies two things:\nFirst, how to “read in” the data. Are they numeric or categorical, and if categorical, what am I supposed to do with them? For example, should I treat all distinct symbols as distinct, resulting in, potentially, an enormous count of categories – or should I constrain myself to a fixed number of entities? Or hash them, even?\nSecond, optional subsequent transformations. Numeric columns may be bucketized; categorical columns may be embedded. Or features could be combined to capture interaction.\nIn this post, we demonstrate the use of a subset of step_ functions. The vignettes on Feature columns and Feature specs illustrate additional functions and their application.\nStarting from the beginning again, here is the complete code for data read-in and train-test split in the feature spec version.\n\n\nlibrary(keras)\nlibrary(tfdatasets)\nlibrary(readr)\nlibrary(dplyr)\n\npath <- \"train.csv\"\n\nporto <- read_csv(path)\n\nporto <- porto %>%\n  mutate_at(vars(ends_with(\"cat\")), as.character)\n\nid_training <- sample.int(nrow(porto), size = 0.75*nrow(porto))\ntraining <- porto[id_training,]\ntesting <- porto[-id_training,]\n\nData-prep-wise, recall what our goals are: leave alone if binary; scale if numeric; embed if categorical. Specifying all of this does not need more than a few lines of code:\n\n\nft_spec <- training %>%\n  select(-id) %>%\n  feature_spec(target ~ .) %>%\n  step_numeric_column(ends_with(\"bin\")) %>%\n  step_numeric_column(-ends_with(\"bin\"),\n                      -ends_with(\"cat\"),\n                      normalizer_fn = scaler_standard()\n                      ) %>%\n  step_categorical_column_with_vocabulary_list(ends_with(\"cat\")) %>%\n  step_embedding_column(ends_with(\"cat\"),\n                        dimension = function(vocab_size) as.integer(sqrt(vocab_size) + 1)\n                        ) %>%\n  fit()\n\nNote how here we are passing in the training set, and just like with recipes, we won’t need to repeat any of the steps for the validation set. Scaling is taken care of by scaler_standard(), an optional transformation function passed in to step_numeric_column. Categorical columns are meant to make use of the complete vocabulary4 and pipe their outputs into embedding layers.\nNow, what actually happened when we called fit()? A lot – for us, as we got rid of a ton of manual preparation. For TensorFlow, nothing really – it just came to know about a few pieces in the graph we’ll ask it to construct.\nBut wait, – don’t we still have to build up that graph ourselves, connecting and concatenating layers? Concretely, above, we had to:\ncreate the correct number of input layers, of correct shape; and\nwire them to their matching embedding layers, of correct dimensionality.\nSo here comes the real magic, and it has two steps.\nFirst, we easily create the input layers by calling layer_input_from_dataset:\n`\n\n\ninputs <- layer_input_from_dataset(porto %>% select(-target))\n\nAnd second, we can extract the features from the feature spec and have layer_dense_features create the necessary layers based on that information:\n\n\nlayer_dense_features(ft_spec$dense_features())\n\nWithout further ado, we add a few dense layers, and there is our model. Magic!\n\n\noutput <- inputs %>%\n  layer_dense_features(ft_spec$dense_features()) %>%\n  layer_dense(units = 30, activation = \"relu\", kernel_regularizer = regularizer_l2(l)) %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_dense(units = 10, activation = \"relu\", kernel_regularizer = regularizer_l2(l)) %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_dense(units = 5, activation = \"relu\", kernel_regularizer = regularizer_l2(l)) %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_dense(units = 1, activation = \"sigmoid\", kernel_regularizer = regularizer_l2(l))\n\nmodel <- keras_model(inputs, output)\n\nHow do we feed this model? In the non-feature-columns example, we would have had to feed each input separately, passing a list of tensors. Now we can just pass it the complete training set all at once:\n\n\nmodel %>% fit(x = training, y = training$target)\n\nIn the Kaggle competition, submissions are evaluated using the normalized Gini coefficient, which we can calculate with the help of a new metric available in Keras, tf$keras$metrics$AUC(). For training, we can use an approximation to the AUC due to Yan et al. (2003) (Yan et al. 2003). Then training is as straightforward as:\n\n\nauc <- tf$keras$metrics$AUC()\n\ngini <- custom_metric(name = \"gini\", function(y_true, y_pred) {\n  2*auc(y_true, y_pred) - 1\n})\n\n# Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003). \n# Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\nroc_auc_score <- function(y_true, y_pred) {\n\n  pos = tf$boolean_mask(y_pred, tf$cast(y_true, tf$bool))\n  neg = tf$boolean_mask(y_pred, !tf$cast(y_true, tf$bool))\n\n  pos = tf$expand_dims(pos, 0L)\n  neg = tf$expand_dims(neg, 1L)\n\n  # original paper suggests performance is robust to exact parameter choice\n  gamma = 0.2\n  p     = 3\n\n  difference = tf$zeros_like(pos * neg) + pos - neg - gamma\n\n  masked = tf$boolean_mask(difference, difference < 0.0)\n\n  tf$reduce_sum(tf$pow(-masked, p))\n}\n\nmodel %>%\n  compile(\n    loss = roc_auc_score,\n    optimizer = optimizer_adam(),\n    metrics = list(auc, gini)\n  )\n\nmodel %>%\n  fit(\n    x = training,\n    y = training$target,\n    epochs = 50,\n    validation_data = list(testing, testing$target),\n    batch_size = 512\n  )\n\npredictions <- predict(model, testing)\nMetrics::auc(testing$target, predictions)\n\nAfter 50 epochs, we achieve an AUC of 0.64 on the validation set, or equivalently, a Gini coefficient of 0.27. Not a bad result for a simple fully connected network!\nWe’ve seen how using feature columns automates away a number of steps in setting up the network, so we can spend more time on actually tuning it. This is most impressively demonstrated on a dataset like this, with more than a handful categorical columns. However, to explain a bit more what to pay attention to when using feature columns, it’s better to choose a smaller example where we can easily do some peeking around.\nLet’s move on to the second application.\nInteractions, and what to look out for\nTo demonstrate the use of step_crossed_column to capture interactions, we make use of the rugged dataset from Richard McElreath’s rethinking package.\nWe want to predict log GDP based on terrain ruggedness, for a number of countries (170, to be precise). However, the effect of ruggedness is different in Africa as opposed to other continents. Citing from Statistical Rethinking\n\nIt makes sense that ruggedness is associated with poorer countries, in most of the world. Rugged terrain means transport is difficult. Which means market access is hampered. Which means reduced gross domestic product. So the reversed relationship within Africa is puzzling. Why should difficult terrain be associated with higher GDP per capita?\n\n\nIf this relationship is at all causal, it may be because rugged regions of Africa were protected against the Atlantic and Indian Ocean slave trades. Slavers preferred to raid easily accessed settlements, with easy routes to the sea. Those regions that suffered under the slave trade understandably continue to suffer economically, long after the decline of slave-trading markets. However, an outcome like GDP has many influences, and is furthermore a strange measure of economic activity. So it is hard to be sure what’s going on here.\n\nWhile the causal situation is difficult, the purely technical one is easily described: We want to learn an interaction. We could rely on the network finding out by itself (in this case it probably will, if we just give it enough parameters). But it’s an excellent occasion to showcase the new step_crossed_column.\nLoading the dataset, zooming in on the variables of interest, and normalizing them the way it is done in Rethinking, we have:\n\n\nlibrary(rethinking)\nlibrary(keras)\nlibrary(tfdatasets)\nlibrary(tibble)\n\ndata(rugged)\n\nd <- rugged\nd <- d[complete.cases(d$rgdppc_2000), ]\n\ndf <- tibble(\n  log_gdp = log(d$rgdppc_2000)/mean(log(d$rgdppc_2000)),\n  rugged = d$rugged/max(d$rugged),\n  africa = d$cont_africa\n)\n\ndf %>% glimpse()\n\n\nObservations: 170\nVariables: 3\n$ log_gdp <dbl> 0.8797119, 0.9647547, 1.1662705, 1.1044854, 0.9149038,…\n$ rugged  <dbl> 0.1383424702, 0.5525636891, 0.1239922606, 0.1249596904…\n$ africa  <int> 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, …\nNow, let’s first forget about the interaction and do the very minimal thing required to work with this data. rugged should be a numeric column, while africa is categorical in nature, which means we use one of the step_categorical_[...] functions on it. (In this case we happen to know there are just two categories, Africa and not-Africa, so we could as well treat the column as numeric like in the previous example; but in other applications that won’t be the case, so here we show a method that generalizes to categorical features in general.)\nSo we start out creating a feature spec and adding the two predictor columns. We check the result using feature_spec’s dense_features() method:\n\n\nft_spec <- training %>%\n  feature_spec(log_gdp ~ .) %>%\n  step_numeric_column(rugged) %>%\n  step_categorical_column_with_identity(africa, num_buckets = 2) \n  fit()\n\nft_spec$dense_features()\n\n\n$rugged\nNumericColumn(key='rugged', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)\nHm, that doesn’t look too good. Where’d africa go? In fact, there is one more thing we should have done: convert the categorical column to an indicator column. Why?\nThe rule of thumb is, whenever you have something categorical, including crossed, you need to then transform it into something numeric, which includes indicator and embedding.\nBeing a heuristic, this rule works overall, and it matches our intuition. There’s one exception though, step_bucketized_column, which although it “feels” categorical actually does not need that conversion.\nTherefore, it is best to supplement that intuition with a simple lookup diagram, which is also part of the feature columns vignette.\nWith this diagram, the simple rule is: We always need to end up with something that inherits from DenseColumn. So:\nstep_numeric_column, step_indicator_column, and step_embedding_column are standalone;\nstep_bucketized_column is, too, however categorical it “feels”; and\nall step_categorical_column_[...], as well as step_crossed_column, need to be transformed using one the dense column types.\n\n\n\nFigure 1: For use with Keras, all features need to end up inheriting from DenseColumn somehow.\n\n\n\nThus, we can fix the situation like so:\n\n\nft_spec <- training %>%\n  feature_spec(log_gdp ~ .) %>%\n  step_numeric_column(rugged) %>%\n  step_categorical_column_with_identity(africa, num_buckets = 2) %>%\n  step_indicator_column(africa) %>%\n  fit()\n\nand now ft_spec$dense_features() will show us\n\n$rugged\nNumericColumn(key='rugged', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)\n\n$indicator_africa\nIndicatorColumn(categorical_column=IdentityCategoricalColumn(key='africa', number_buckets=2.0, default_value=None))\n\nWhat we really wanted to do is capture the interaction between ruggedness and continent. To this end, we first bucketize rugged, and then cross it with – already binary – africa. As per the rules, we finally transform into an indicator column:\n\n\nft_spec <- training %>%\n  feature_spec(log_gdp ~ .) %>%\n  step_numeric_column(rugged) %>%\n  step_categorical_column_with_identity(africa, num_buckets = 2) %>%\n  step_indicator_column(africa) %>%\n  step_bucketized_column(rugged,\n                         boundaries = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.8)) %>%\n  step_crossed_column(africa_rugged_interact = c(africa, bucketized_rugged),\n                      hash_bucket_size = 16) %>%\n  step_indicator_column(africa_rugged_interact) %>%\n  fit()\n\nLooking at this code you may be asking yourself, now how many features do I have in the model? Let’s check.\n\n\nft_spec$dense_features()\n\n\n$rugged\nNumericColumn(key='rugged', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)\n\n$indicator_africa\nIndicatorColumn(categorical_column=IdentityCategoricalColumn(key='africa', number_buckets=2.0, default_value=None))\n\n$bucketized_rugged\nBucketizedColumn(source_column=NumericColumn(key='rugged', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.8))\n\n$indicator_africa_rugged_interact\nIndicatorColumn(categorical_column=CrossedColumn(keys=(IdentityCategoricalColumn(key='africa', number_buckets=2.0, default_value=None), BucketizedColumn(source_column=NumericColumn(key='rugged', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.8))), hash_bucket_size=16.0, hash_key=None))\nWe see that all features, original or transformed, are kept, as long as they inherit from DenseColumn. This means that, for example, the non-bucketized, continuous values of rugged are used as well.\nNow setting up the training goes as expected.\n\n\ninputs <- layer_input_from_dataset(df %>% select(-log_gdp))\n\noutput <- inputs %>%\n  layer_dense_features(ft_spec$dense_features()) %>%\n  layer_dense(units = 8, activation = \"relu\") %>%\n  layer_dense(units = 8, activation = \"relu\") %>%\n  layer_dense(units = 1)\n\nmodel <- keras_model(inputs, output)\n\nmodel %>% compile(loss = \"mse\", optimizer = \"adam\", metrics = \"mse\")\n\nhistory <- model %>% fit(\n  x = training,\n  y = training$log_gdp,\n  validation_data = list(testing, testing$log_gdp),\n  epochs = 100)\n\nJust as a sanity check, the final loss on the validation set for this code was ~ 0.014. But really this example did serve different purposes.\nIn a nutshell\nFeature specs are a convenient, elegant way of making categorical data available to Keras, as well as to chain useful transformations like bucketizing and creating crossed columns. The time you save data wrangling may go into tuning and experimentation. Enjoy, and thanks for reading!\n\n\nYan, Lian, Robert H Dodier, Michael Mozer, and Richard H Wolniewicz. 2003. “Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.” In Proceedings of the 20th International Conference on Machine Learning (Icml-03), 848–55.\n\n\nsee e.g. Entity embeddings for fun and profit↩︎\nHaving mentioned, above, that these really may be continuous or ordinal, from now on we’ll just call them numeric as we won’t make further use of the distinction. However, fitting ordinal data with neural networks is definitely a topic that would deserve its own post.↩︎\nIn this dataset, every categorical variable is a column of singleton integers.↩︎\nas indicated by leaving out the optional vocabulary_list↩︎\n",
    "preview": "posts/2019-07-09-feature-columns/images/feature_cols_hier.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 1172,
    "preview_height": 678
  },
  {
    "path": "posts/2019-06-25-dynamic_linear_models_tfprobability/",
    "title": "Dynamic linear models with tfprobability",
    "description": "Previous posts featuring tfprobability - the R interface to TensorFlow Probability - have focused on enhancements to deep neural networks (e.g., introducing Bayesian uncertainty estimates) and fitting hierarchical models with Hamiltonian Monte Carlo. This time, we show how to fit time series using dynamic linear models (DLMs), yielding posterior predictive forecasts as well as the smoothed and filtered estimates from the Kálmán filter.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-06-24",
    "categories": [
      "Probabilistic ML/DL",
      "Time Series"
    ],
    "contents": "\nWelcome to the world of state space models.1 In this world, there is a latent process, hidden from our eyes; and there are observations we make about the things it produces. The process evolves due to some hidden logic (transition model); and the way it produces the observations follows some hidden logic (observation model). There is noise in process evolution, and there is noise in observation. If the transition and observation models both are linear, and the process as well as observation noise are Gaussian, we have a linear-Gaussian state space model (SSM). The task is to infer the latent state from the observations. The most famous technique is the Kálmán filter.\nIn practical applications, two characteristics of linear-Gaussian SSMs are especially attractive.\nFor one, they let us estimate dynamically changing parameters. In regression, the parameters can be viewed as a hidden state; we may thus have a slope and an intercept that vary over time. When parameters can vary, we speak of dynamic linear models (DLMs). This is the term we’ll use throughout this post when referring to this class of models.\nSecond, linear-Gaussian SSMs are useful in time-series forecasting because Gaussian processes can be added. A time series can thus be framed as, e.g. the sum of a linear trend and a process that varies seasonally.\nUsing tfprobability, the R wrapper to TensorFlow Probability, we illustrate both aspects here. Our first example will be on dynamic linear regression. In a detailed walkthrough, we show on how to fit such a model, how to obtain filtered, as well as smoothed, estimates of the coefficients, and how to obtain forecasts. Our second example then illustrates process additivity. This example will build on the first, and may also serve as a quick recap of the overall procedure.\nLet’s jump in.\nDynamic linear regression example: Capital Asset Pricing Model (CAPM)\nOur code builds on the recently released versions of TensorFlow and TensorFlow Probability: 1.14 and 0.7, respectively.\n\n\ndevtools::install_github(\"rstudio/tfprobability\")\n\n# also installs TensorFlow Probability v. 0.7\ntensorflow::install_tensorflow(version = \"1.14\")\n\nlibrary(tensorflow)\nlibrary(tfprobability)\n\nlibrary(tidyverse)\nlibrary(zeallot)\n\nNote how there’s one thing we used to do lately that we’re not doing here: We’re not enabling eager execution. We say why in a minute.\nOur example is taken from Petris et al.(2009)(Petris, Petrone, and Campagnoli 2009), chapter 3.2.7.2 Besides introducing the dlm package, this book provides a nice introduction to the ideas behind DLMs in general.\nTo illustrate dynamic linear regression, the authors feature a dataset, originally from Berndt(1991)(Berndt 1991) that has monthly returns, collected from January 1978 to December 1987, for four different stocks, the 30-day Treasury Bill – standing in for a risk-free asset –, and the value-weighted average returns for all stocks listed at the New York and American Stock Exchanges, representing the overall market returns.\nLet’s take a look.\n\n\n# As the data does not seem to be available at the address given in Petris et al. any more,\n# we put it on the blog for download\n# download from: \n# https://github.com/rstudio/tensorflow-blog/blob/master/docs/posts/2019-06-25-dynamic_linear_models_tfprobability/data/capm.txt\"\ndf <- read_table(\n  \"capm.txt\",\n  col_types = list(X1 = col_date(format = \"%Y.%m\"))) %>%\n  rename(month = X1)\ndf %>% glimpse()\n\n\nObservations: 120\nVariables: 7\n$ month  <date> 1978-01-01, 1978-02-01, 1978-03-01, 1978-04-01, 1978-05-01, 19…\n$ MOBIL  <dbl> -0.046, -0.017, 0.049, 0.077, -0.011, -0.043, 0.028, 0.056, 0.0…\n$ IBM    <dbl> -0.029, -0.043, -0.063, 0.130, -0.018, -0.004, 0.092, 0.049, -0…\n$ WEYER  <dbl> -0.116, -0.135, 0.084, 0.144, -0.031, 0.005, 0.164, 0.039, -0.0…\n$ CITCRP <dbl> -0.115, -0.019, 0.059, 0.127, 0.005, 0.007, 0.032, 0.088, 0.011…\n$ MARKET <dbl> -0.045, 0.010, 0.050, 0.063, 0.067, 0.007, 0.071, 0.079, 0.002,…\n$ RKFREE <dbl> 0.00487, 0.00494, 0.00526, 0.00491, 0.00513, 0.00527, 0.00528, …\n\n\ndf %>% gather(key = \"symbol\", value = \"return\", -month) %>%\n  ggplot(aes(x = month, y = return, color = symbol)) +\n  geom_line() +\n  facet_grid(rows = vars(symbol), scales = \"free\")\n\n\n\n\nFigure 1: Monthly returns for selected stocks; data from Berndt (1991).\n\n\n\nThe Capital Asset Pricing Model then assumes a linear relationship between the excess returns of an asset under study and the excess returns of the market. For both, excess returns are obtained by subtracting the returns of the chosen risk-free asset; then, the scaling coefficient between them reveals the asset to either be an “aggressive” investment (slope > 1: changes in the market are amplified), or a conservative one (slope < 1: changes are damped).\nAssuming this relationship does not change over time, we can easily use lm to illustrate this. Following Petris et al. in zooming in on IBM as the asset under study, we have\n\n\n# excess returns of the asset under study\nibm <- df$IBM - df$RKFREE\n# market excess returns\nx <- df$MARKET - df$RKFREE\n\nfit <- lm(ibm ~ x)\nsummary(fit)\n\n\nCall:\nlm(formula = ibm ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.11850 -0.03327 -0.00263  0.03332  0.15042 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.0004896  0.0046400  -0.106    0.916    \nx            0.4568208  0.0675477   6.763 5.49e-10 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.05055 on 118 degrees of freedom\nMultiple R-squared:  0.2793,    Adjusted R-squared:  0.2732 \nF-statistic: 45.74 on 1 and 118 DF,  p-value: 5.489e-10\nSo IBM is found to be a conservative investment, the slope being ~ 0.5. But is this relationship stable over time?\nLet’s turn to tfprobability to investigate.\nWe want to use this example to demonstrate two essential applications of DLMs: obtaining smoothing and/or filtering estimates of the coefficients, as well as forecasting future values. So unlike Petris et al., we divide the dataset into a training and a testing part:3.\n\n\n# zoom in on ibm\nts <- ibm %>% matrix()\n# forecast 12 months\nn_forecast_steps <- 12\nts_train <- ts[1:(length(ts) - n_forecast_steps), 1, drop = FALSE]\n\n# make sure we work with float32 here\nts_train <- tf$cast(ts_train, tf$float32)\nts <- tf$cast(ts, tf$float32)\n\nWe now construct the model. sts_dynamic_linear_regression() does what we want:\n\n\n# define the model on the complete series\nlinreg <- ts %>%\n  sts_dynamic_linear_regression(\n    design_matrix = cbind(rep(1, length(x)), x) %>% tf$cast(tf$float32)\n  )\n\nWe pass it the column of excess market returns, plus a column of ones, following Petris et al.. Alternatively, we could center the single predictor – this would work just as well.\nHow are we going to train this model? Method-wise, we have a choice between variational inference (VI) and Hamiltonian Monte Carlo (HMC). We will see both. The second question is: Are we going to use graph mode or eager mode? As of today, for both VI and HMC, it is safest – and fastest – to run in graph mode, so this is the only technique we show. In a few weeks, or months, we should be able to prune a lot of sess$run()s from the code!\nNormally in posts, when presenting code we optimize for easy experimentation (meaning: line-by-line executability), not modularity. This time though, with an important number of evaluation statements involved, it’s easiest to pack not just the fitting, but the smoothing and forecasting as well into a function (which you could still step through if you wanted). For VI, we’ll have a fit _with_vi function that does it all. So when we now start explaining what it does, don’t type in the code just yet – it’ll all reappear nicely packed into that function, for you to copy and execute as a whole.\nFitting a time series with variational inference\nFitting with VI pretty much looks like training traditionally used to look in graph-mode TensorFlow. You define a loss – here it’s done using sts_build_factored_variational_loss() –, an optimizer, and an operation for the optimizer to reduce that loss:\n\n\noptimizer <- tf$compat$v1$train$AdamOptimizer(0.1)\n\n# only train on the training set!    \nloss_and_dists <- ts_train %>% sts_build_factored_variational_loss(model = model)\nvariational_loss <- loss_and_dists[[1]]\n\ntrain_op <- optimizer$minimize(variational_loss)\n\nNote how the loss is defined on the training set only, not the complete series.\nNow to actually train the model, we create a session and run that operation:\n\n\n with (tf$Session() %as% sess,  {\n      \n      sess$run(tf$compat$v1$global_variables_initializer())\n   \n      for (step in 1:n_iterations) {\n        res <- sess$run(train_op)\n        loss <- sess$run(variational_loss)\n        if (step %% 10 == 0)\n          cat(\"Loss: \", as.numeric(loss), \"\\n\")\n      }\n })\n\nGiven we have that session, let’s make use of it and compute all the estimates we desire. Again, – the following snippets will end up in the fit_with_vi function, so don’t run them in isolation just yet.\nObtaining forecasts\nThe first thing we want for the model to give us are forecasts. In order to create them, it needs samples from the posterior. Luckily we already have the posterior distributions, returned from sts_build_factored_variational_loss, so let’s sample from them and pass them to sts_forecast:\n\n\nvariational_distributions <- loss_and_dists[[2]]\nposterior_samples <-\n  Map(\n    function(d) d %>% tfd_sample(n_param_samples),\n    variational_distributions %>% reticulate::py_to_r() %>% unname()\n  )\nforecast_dists <- ts_train %>% sts_forecast(model, posterior_samples, n_forecast_steps)\n\nsts_forecast() returns distributions, so we call tfd_mean() to get the posterior predictions and tfd_stddev() for the corresponding standard deviations:\n\n\nfc_means <- forecast_dists %>% tfd_mean()\nfc_sds <- forecast_dists %>% tfd_stddev()\n\nBy the way – as we have the full posterior distributions, we are by no means restricted to summary statistics! We could easily use tfd_sample() to obtain individual forecasts.\nSmoothing and filtering (Kálmán filter)\nNow, the second (and last, for this example) thing we will want are the smoothed and filtered regression coefficients. The famous Kálmán Filter is a Bayesian-in-spirit method where at each time step, predictions are corrected by how much they differ from an incoming observation. Filtering estimates are based on observations we’ve seen so far; smoothing estimates are computed “in hindsight”, making use of the complete time series.\nWe first create a state space model from our time series definition:\n\n\n# only do this on the training set\n# returns an instance of tfd_linear_gaussian_state_space_model()\nssm <- model$make_state_space_model(length(ts_train), param_vals = posterior_samples)\n\ntfd_linear_gaussian_state_space_model(), technically a distribution, provides the Kálmán filter functionalities of smoothing and filtering.\nTo obtain the smoothed estimates:\n\n\nc(smoothed_means, smoothed_covs) %<-% ssm$posterior_marginals(ts_train)\n\nAnd the filtered ones:\n\n\nc(., filtered_means, filtered_covs, ., ., ., .) %<-% ssm$forward_filter(ts_train)\n\nFinally, we need to evaluate all those.\n\n\nc(posterior_samples, fc_means, fc_sds, smoothed_means, smoothed_covs, filtered_means, filtered_covs) %<-%\n  sess$run(list(posterior_samples, fc_means, fc_sds, smoothed_means, smoothed_covs, filtered_means, filtered_covs))\n\nPutting it all together (the VI edition)\nSo here’s the complete function, fit_with_vi, ready for us to call.\n\n\nfit_with_vi <-\n  function(ts,\n           ts_train,\n           model,\n           n_iterations,\n           n_param_samples,\n           n_forecast_steps,\n           n_forecast_samples) {\n    \n    optimizer <- tf$compat$v1$train$AdamOptimizer(0.1)\n    \n    loss_and_dists <-\n      ts_train %>% sts_build_factored_variational_loss(model = model)\n    variational_loss <- loss_and_dists[[1]]\n    train_op <- optimizer$minimize(variational_loss)\n    \n    with (tf$Session() %as% sess,  {\n      \n      sess$run(tf$compat$v1$global_variables_initializer())\n      for (step in 1:n_iterations) {\n        sess$run(train_op)\n        loss <- sess$run(variational_loss)\n        if (step %% 1 == 0)\n          cat(\"Loss: \", as.numeric(loss), \"\\n\")\n      }\n      variational_distributions <- loss_and_dists[[2]]\n      posterior_samples <-\n        Map(\n          function(d)\n            d %>% tfd_sample(n_param_samples),\n          variational_distributions %>% reticulate::py_to_r() %>% unname()\n        )\n      forecast_dists <-\n        ts_train %>% sts_forecast(model, posterior_samples, n_forecast_steps)\n      fc_means <- forecast_dists %>% tfd_mean()\n      fc_sds <- forecast_dists %>% tfd_stddev()\n      \n      ssm <- model$make_state_space_model(length(ts_train), param_vals = posterior_samples)\n      c(smoothed_means, smoothed_covs) %<-% ssm$posterior_marginals(ts_train)\n      c(., filtered_means, filtered_covs, ., ., ., .) %<-% ssm$forward_filter(ts_train)\n   \n      c(posterior_samples, fc_means, fc_sds, smoothed_means, smoothed_covs, filtered_means, filtered_covs) %<-%\n        sess$run(list(posterior_samples, fc_means, fc_sds, smoothed_means, smoothed_covs, filtered_means, filtered_covs))\n      \n    })\n    \n    list(\n      variational_distributions,\n      posterior_samples,\n      fc_means[, 1],\n      fc_sds[, 1],\n      smoothed_means,\n      smoothed_covs,\n      filtered_means,\n      filtered_covs\n    )\n  }\n\nAnd this is how we call it.\n\n\n# number of VI steps\nn_iterations <- 300\n# sample size for posterior samples\nn_param_samples <- 50\n# sample size to draw from the forecast distribution\nn_forecast_samples <- 50\n\n# here's the model again\nmodel <- ts %>%\n  sts_dynamic_linear_regression(design_matrix = cbind(rep(1, length(x)), x) %>% tf$cast(tf$float32))\n\n# call fit_vi defined above\nc(\n  param_distributions,\n  param_samples,\n  fc_means,\n  fc_sds,\n  smoothed_means,\n  smoothed_covs,\n  filtered_means,\n  filtered_covs\n) %<-% fit_vi(\n  ts,\n  ts_train,\n  model,\n  n_iterations,\n  n_param_samples,\n  n_forecast_steps,\n  n_forecast_samples\n)\n\nCurious about the results? We’ll see them in a second, but before let’s just quickly glance at the alternative training method: HMC.\nPutting it all together (the HMC edition)\ntfprobability provides sts_fit_with_hmc to fit a DLM using Hamiltonian Monte Carlo. Recent posts (e.g., Hierarchical partial pooling, continued: Varying slopes models with TensorFlow Probability) showed how to set up HMC to fit hierarchical models; here a single function does it all.\nHere is fit_with_hmc, wrapping sts_fit_with_hmc as well as the (unchanged) techniques for obtaining forecasts and smoothed/filtered parameters:\n\n\nnum_results <- 200\nnum_warmup_steps <- 100\n\nfit_hmc <- function(ts,\n                    ts_train,\n                    model,\n                    num_results,\n                    num_warmup_steps,\n                    n_forecast,\n                    n_forecast_samples) {\n  \n  states_and_results <-\n    ts_train %>% sts_fit_with_hmc(\n      model,\n      num_results = num_results,\n      num_warmup_steps = num_warmup_steps,\n      num_variational_steps = num_results + num_warmup_steps\n    )\n  \n  posterior_samples <- states_and_results[[1]]\n  forecast_dists <-\n    ts_train %>% sts_forecast(model, posterior_samples, n_forecast_steps)\n  fc_means <- forecast_dists %>% tfd_mean()\n  fc_sds <- forecast_dists %>% tfd_stddev()\n  \n  ssm <-\n    model$make_state_space_model(length(ts_train), param_vals = posterior_samples)\n  c(smoothed_means, smoothed_covs) %<-% ssm$posterior_marginals(ts_train)\n  c(., filtered_means, filtered_covs, ., ., ., .) %<-% ssm$forward_filter(ts_train)\n  \n  with (tf$Session() %as% sess,  {\n    sess$run(tf$compat$v1$global_variables_initializer())\n    c(\n      posterior_samples,\n      fc_means,\n      fc_sds,\n      smoothed_means,\n      smoothed_covs,\n      filtered_means,\n      filtered_covs\n    ) %<-%\n      sess$run(\n        list(\n          posterior_samples,\n          fc_means,\n          fc_sds,\n          smoothed_means,\n          smoothed_covs,\n          filtered_means,\n          filtered_covs\n        )\n      )\n  })\n  \n  list(\n    posterior_samples,\n    fc_means[, 1],\n    fc_sds[, 1],\n    smoothed_means,\n    smoothed_covs,\n    filtered_means,\n    filtered_covs\n  )\n}\n\nc(\n  param_samples,\n  fc_means,\n  fc_sds,\n  smoothed_means,\n  smoothed_covs,\n  filtered_means,\n  filtered_covs\n) %<-% fit_hmc(ts,\n               ts_train,\n               model,\n               num_results,\n               num_warmup_steps,\n               n_forecast,\n               n_forecast_samples)\n\nNow finally, let’s take a look at the forecasts and filtering resp. smoothing estimates.\nForecasts\nPutting all we need into one dataframe, we have\n\n\nsmoothed_means_intercept <- smoothed_means[, , 1] %>% colMeans()\nsmoothed_means_slope <- smoothed_means[, , 2] %>% colMeans()\n\nsmoothed_sds_intercept <- smoothed_covs[, , 1, 1] %>% colMeans() %>% sqrt()\nsmoothed_sds_slope <- smoothed_covs[, , 2, 2] %>% colMeans() %>% sqrt()\n\nfiltered_means_intercept <- filtered_means[, , 1] %>% colMeans()\nfiltered_means_slope <- filtered_means[, , 2] %>% colMeans()\n\nfiltered_sds_intercept <- filtered_covs[, , 1, 1] %>% colMeans() %>% sqrt()\nfiltered_sds_slope <- filtered_covs[, , 2, 2] %>% colMeans() %>% sqrt()\n\nforecast_df <- df %>%\n  select(month, IBM) %>%\n  add_column(pred_mean = c(rep(NA, length(ts_train)), fc_means)) %>%\n  add_column(pred_sd = c(rep(NA, length(ts_train)), fc_sds)) %>%\n  add_column(smoothed_means_intercept = c(smoothed_means_intercept, rep(NA, n_forecast_steps))) %>%\n  add_column(smoothed_means_slope = c(smoothed_means_slope, rep(NA, n_forecast_steps))) %>%\n  add_column(smoothed_sds_intercept = c(smoothed_sds_intercept, rep(NA, n_forecast_steps))) %>%\n  add_column(smoothed_sds_slope = c(smoothed_sds_slope, rep(NA, n_forecast_steps))) %>%\n  add_column(filtered_means_intercept = c(filtered_means_intercept, rep(NA, n_forecast_steps))) %>%\n  add_column(filtered_means_slope = c(filtered_means_slope, rep(NA, n_forecast_steps))) %>%\n  add_column(filtered_sds_intercept = c(filtered_sds_intercept, rep(NA, n_forecast_steps))) %>%\n  add_column(filtered_sds_slope = c(filtered_sds_slope, rep(NA, n_forecast_steps)))\n\nSo here first are the forecasts. We’re using the estimates returned from VI, but we could just as well have used those from HMC – they’re nearly indistinguishable. The same goes for the filtering and smoothing estimates displayed below.\n\n\nggplot(forecast_df, aes(x = month, y = IBM)) +\n  geom_line(color = \"grey\") +\n  geom_line(aes(y = pred_mean), color = \"cyan\") +\n  geom_ribbon(\n    aes(ymin = pred_mean - 2 * pred_sd, ymax = pred_mean + 2 * pred_sd),\n    alpha = 0.2,\n    fill = \"cyan\"\n  ) +\n  theme(axis.title = element_blank())\n\n\n\n\nFigure 2: 12-point-ahead forecasts for IBM; posterior means +/- 2 standard deviations.\n\n\n\nSmoothing estimates\nHere are the smoothing estimates. The intercept (shown in orange) remains pretty stable over time, but we do see a trend in the slope (displayed in green).4\n\n\nggplot(forecast_df, aes(x = month, y = smoothed_means_intercept)) +\n  geom_line(color = \"orange\") +\n  geom_line(aes(y = smoothed_means_slope),\n            color = \"green\") +\n  geom_ribbon(\n    aes(\n      ymin = smoothed_means_intercept - 2 * smoothed_sds_intercept,\n      ymax = smoothed_means_intercept + 2 * smoothed_sds_intercept\n    ),\n    alpha = 0.3,\n    fill = \"orange\"\n  ) +\n  geom_ribbon(\n    aes(\n      ymin = smoothed_means_slope - 2 * smoothed_sds_slope,\n      ymax = smoothed_means_slope + 2 * smoothed_sds_slope\n    ),\n    alpha = 0.1,\n    fill = \"green\"\n  ) +\n  coord_cartesian(xlim = c(forecast_df$month[1], forecast_df$month[length(ts) - n_forecast_steps]))  +\n  theme(axis.title = element_blank())\n\n\n\n\nFigure 3: Smoothing estimates from the Kálmán filter. Green: coefficient for dependence on excess market returns (slope), orange: vector of ones (intercept).\n\n\n\nFiltering estimates\nFor comparison, here are the filtering estimates. Note that the y-axis extends further up and down, so we can capture uncertainty better:5\n\n\nggplot(forecast_df, aes(x = month, y = filtered_means_intercept)) +\n  geom_line(color = \"orange\") +\n  geom_line(aes(y = filtered_means_slope),\n            color = \"green\") +\n  geom_ribbon(\n    aes(\n      ymin = filtered_means_intercept - 2 * filtered_sds_intercept,\n      ymax = filtered_means_intercept + 2 * filtered_sds_intercept\n    ),\n    alpha = 0.3,\n    fill = \"orange\"\n  ) +\n  geom_ribbon(\n    aes(\n      ymin = filtered_means_slope - 2 * filtered_sds_slope,\n      ymax = filtered_means_slope + 2 * filtered_sds_slope\n    ),\n    alpha = 0.1,\n    fill = \"green\"\n  ) +\n  coord_cartesian(ylim = c(-2, 2),\n                  xlim = c(forecast_df$month[1], forecast_df$month[length(ts) - n_forecast_steps])) +\n  theme(axis.title = element_blank())\n\n\n\n\nFigure 4: Filtering estimates from the Kálmán filter. Green: coefficient for dependence on excess market returns (slope), orange: vector of ones (intercept).\n\n\n\nSo far, we’ve seen a full example of time-series fitting, forecasting, and smoothing/filtering, in an exciting setting one doesn’t encounter too often: dynamic linear regression. What we haven’t seen as yet is the additivity feature of DLMs, and how it allows us to decompose a time series into its (theorized) constituents. Let’s do this next, in our second example, anti-climactically making use of the iris of time series, AirPassengers. Any guesses what components the model might presuppose?\n\n\n\nFigure 5: AirPassengers.\n\n\n\nComposition example: AirPassengers\nLibraries loaded, we prepare the data for tfprobability:\n\n\nlibrary(tensorflow)\nlibrary(tfprobability)\n\nlibrary(tidyverse)\nlibrary(zeallot)\nlibrary(lubridate)\n\nmonth <- seq(ymd(\"1949/1/1\"), ymd(\"1960/12/31\"), by = \"month\")\ndf <- data.frame(month) %>% add_column(n = matrix(AirPassengers))\n\n# train-test split\nts <- df$n %>% matrix()\nn_forecast_steps <- 12\nts_train <- ts[1:(length(ts) - n_forecast_steps), 1, drop = FALSE]\n\n# cast to float32\nts_train <- tf$cast(ts_train, tf$float32)\nts <- tf$cast(ts, tf$float32)\n\nThe model is a sum – cf. sts_sum – of a linear trend and a seasonal component:\n\n\nlinear_trend <- ts %>% sts_local_linear_trend()\nmonthly <- ts %>% sts_seasonal(num_seasons = 12)\nmodel <- ts %>% sts_sum(components = list(monthly, linear_trend))\n\nAgain, we could use VI as well as MCMC to train the model. Here’s the VI way:6\n\n\nn_iterations <- 100\nn_param_samples <- 50\nn_forecast_samples <- 50\n\noptimizer <- tf$compat$v1$train$AdamOptimizer(0.1)\n\nfit_vi <-\n  function(ts,\n           ts_train,\n           model,\n           n_iterations,\n           n_param_samples,\n           n_forecast_steps,\n           n_forecast_samples) {\n    loss_and_dists <-\n      ts_train %>% sts_build_factored_variational_loss(model = model)\n    variational_loss <- loss_and_dists[[1]]\n    train_op <- optimizer$minimize(variational_loss)\n    \n    with (tf$Session() %as% sess,  {\n      sess$run(tf$compat$v1$global_variables_initializer())\n      for (step in 1:n_iterations) {\n        res <- sess$run(train_op)\n        loss <- sess$run(variational_loss)\n        if (step %% 1 == 0)\n          cat(\"Loss: \", as.numeric(loss), \"\\n\")\n      }\n      variational_distributions <- loss_and_dists[[2]]\n      posterior_samples <-\n        Map(\n          function(d)\n            d %>% tfd_sample(n_param_samples),\n          variational_distributions %>% reticulate::py_to_r() %>% unname()\n        )\n      forecast_dists <-\n        ts_train %>% sts_forecast(model, posterior_samples, n_forecast_steps)\n      fc_means <- forecast_dists %>% tfd_mean()\n      fc_sds <- forecast_dists %>% tfd_stddev()\n      \n      c(posterior_samples,\n        fc_means,\n        fc_sds) %<-%\n        sess$run(list(posterior_samples,\n                      fc_means,\n                      fc_sds))\n    })\n    \n    list(variational_distributions,\n         posterior_samples,\n         fc_means[, 1],\n         fc_sds[, 1])\n  }\n\nc(param_distributions,\n  param_samples,\n  fc_means,\n  fc_sds) %<-% fit_vi(\n    ts,\n    ts_train,\n    model,\n    n_iterations,\n    n_param_samples,\n    n_forecast_steps,\n    n_forecast_samples\n  )\n\nFor brevity, we haven’t computed smoothed and/or filtered estimates for the overall model. In this example, this being a sum model, we want to show something else instead: the way it decomposes into components.\nBut first, the forecasts:\n\n\nforecast_df <- df %>%\n  add_column(pred_mean = c(rep(NA, length(ts_train)), fc_means)) %>%\n  add_column(pred_sd = c(rep(NA, length(ts_train)), fc_sds))\n\n\nggplot(forecast_df, aes(x = month, y = n)) +\n  geom_line(color = \"grey\") +\n  geom_line(aes(y = pred_mean), color = \"cyan\") +\n  geom_ribbon(\n    aes(ymin = pred_mean - 2 * pred_sd, ymax = pred_mean + 2 * pred_sd),\n    alpha = 0.2,\n    fill = \"cyan\"\n  ) +\n  theme(axis.title = element_blank())\n\n\n\n\nFigure 6: AirPassengers, 12-months-ahead forecast.\n\n\n\nA call to sts_decompose_by_component yields the (centered) components, a linear trend and a seasonal factor:\n\n\ncomponent_dists <-\n  ts_train %>% sts_decompose_by_component(model = model, parameter_samples = param_samples)\n\nseasonal_effect_means <- component_dists[[1]] %>% tfd_mean()\nseasonal_effect_sds <- component_dists[[1]] %>% tfd_stddev()\nlinear_effect_means <- component_dists[[2]] %>% tfd_mean()\nlinear_effect_sds <- component_dists[[2]] %>% tfd_stddev()\n\nwith(tf$Session() %as% sess, {\n  c(\n    seasonal_effect_means,\n    seasonal_effect_sds,\n    linear_effect_means,\n    linear_effect_sds\n  ) %<-% sess$run(\n    list(\n      seasonal_effect_means,\n      seasonal_effect_sds,\n      linear_effect_means,\n      linear_effect_sds\n    )\n  )\n})\n\ncomponents_df <- forecast_df %>%\n  add_column(seasonal_effect_means = c(seasonal_effect_means, rep(NA, n_forecast_steps))) %>%\n  add_column(seasonal_effect_sds = c(seasonal_effect_sds, rep(NA, n_forecast_steps))) %>%\n  add_column(linear_effect_means = c(linear_effect_means, rep(NA, n_forecast_steps))) %>%\n  add_column(linear_effect_sds = c(linear_effect_sds, rep(NA, n_forecast_steps)))\n\nggplot(components_df, aes(x = month, y = n)) +\n  geom_line(aes(y = seasonal_effect_means), color = \"orange\") +\n  geom_ribbon(\n    aes(\n      ymin = seasonal_effect_means - 2 * seasonal_effect_sds,\n      ymax = seasonal_effect_means + 2 * seasonal_effect_sds\n    ),\n    alpha = 0.2,\n    fill = \"orange\"\n  ) +\n  theme(axis.title = element_blank()) +\n  geom_line(aes(y = linear_effect_means), color = \"green\") +\n  geom_ribbon(\n    aes(\n      ymin = linear_effect_means - 2 * linear_effect_sds,\n      ymax = linear_effect_means + 2 * linear_effect_sds\n    ),\n    alpha = 0.2,\n    fill = \"green\"\n  ) +\n  theme(axis.title = element_blank())\n\n\n\n\nFigure 7: AirPassengers, decomposition into a linear trend and a seasonal component (both centered).\n\n\n\nWrapping up\nWe’ve seen how with DLMs, there is a bunch of interesting stuff you can do – apart from obtaining forecasts, which probably will be the ultimate goal in most applications – : You can inspect the smoothed and the filtered estimates from the Kálmán filter, and you can decompose a model into its posterior components. A particularly attractive model is dynamic linear regression, featured in our first example, which allows us to obtain regression coefficients that vary over time.\nThis post showed how to accomplish this with tfprobability. As of today, TensorFlow (and thus, TensorFlow Probability) is in a state of substantial internal changes, with eager to become the default execution mode very soon. Concurrently, the awesome TensorFlow Probability development team are adding new and exciting features every day. Consequently, this post is snapshot capturing how to best accomplish those goals now: If you’re reading this a few months from now, chances are that what’s work in progress now will have become a mature method by then, and there may be faster ways to attain the same goals. At the rate TFP is evolving, we’re excited for the things to come!\n\n\nBerndt, R. 1991. The Practice of Econometrics. Addison-Wesley.\n\n\nMurphy, Kevin. 2012. Machine Learning: A Probabilistic Perspective. MIT Press.\n\n\nPetris, Giovanni, sonia Petrone, and Patrizia Campagnoli. 2009. Dynamic Linear Models with R. Springer.\n\n\nFor an excellent yet concise introduction to state space models, see chapter 18 of Murphy (2012) (Murphy 2012).↩︎\nA draft version is available online at http://people.bordeaux.inria.fr/pierre.delmoral/dynamics-linear-models.petris_et_al.pdf. However, the draft does not yet fully elaborate on this example.↩︎\nForecasting functionality is not provided by dlm for dynamic linear models, see ?dlmForecast↩︎\nWe do not think it instructive to display the results obtained by Petris et al. here, as this post does not aim to evaluate or compare implementations.↩︎\nIt is still clipped up and below though. We’ve been trying to find a compromise between local visibility of the filtered means on the one hand, and global visibility of uncertainty in the estimates on the other.↩︎\nFor MCMC, you can straightforwardly adapt the above example.↩︎\n",
    "preview": "posts/2019-06-25-dynamic_linear_models_tfprobability/images/thumb.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 2012,
    "preview_height": 1065
  },
  {
    "path": "posts/2019-06-05-uncertainty-estimates-tfprobability/",
    "title": "Adding uncertainty estimates to Keras models with tfprobability",
    "description": "As of today, there is no mainstream road to obtaining uncertainty estimates from neural networks. All that can be said is that, normally, approaches tend to be Bayesian in spirit, involving some way of putting a prior over model weights. This holds true as well for the method presented in this post: We show how to use tfprobability, the R interface to TensorFlow Probability, to add uncertainty estimates to a Keras model in an elegant and conceptually plausible way.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-06-05",
    "categories": [
      "Probabilistic ML/DL",
      "TensorFlow/Keras",
      "Concepts"
    ],
    "contents": "\nAbout six months ago, we showed how to create a custom wrapper to obtain uncertainty estimates from a Keras network. Today we present a less laborious, as well faster-running way using tfprobability, the R wrapper to TensorFlow Probability. Like most posts on this blog, this one won’t be short, so let’s quickly state what you can expect in return of reading time.\nWhat to expect from this post\nStarting from what not to expect: There won’t be a recipe that tells you how exactly to set all parameters involved in order to report the “right” uncertainty measures. But then, what are the “right” uncertainty measures? Unless you happen to work with a method that has no (hyper-)parameters to tweak, there will always be questions about how to report uncertainty.\nWhat you can expect, though, is an introduction to obtaining uncertainty estimates for Keras networks, as well as an empirical report of how tweaking (hyper-)parameters may affect the results. As in the aforementioned post, we perform our tests on both a simulated and a real dataset, the Combined Cycle Power Plant Data Set. At the end, in place of strict rules, you should have acquired some intuition that will transfer to other real-world datasets.\nDid you notice our talking about Keras networks above? Indeed this post has an additional goal: So far, we haven’t really discussed yet how tfprobability goes together with keras. Now we finally do (in short: they work together seemlessly).\nFinally, the notions of aleatoric and epistemic uncertainty, which may have stayed a bit abstract in the prior post, should get much more concrete here.\nAleatoric vs. epistemic uncertainty\nReminiscent somehow of the classic decomposition of generalization error into bias and variance, splitting uncertainty into its epistemic and aleatoric constituents separates an irreducible from a reducible part.\nThe reducible part relates to imperfection in the model: In theory, if our model were perfect, epistemic uncertainty would vanish. Put differently, if the training data were unlimited – or if they comprised the whole population – we could just add capacity to the model until we’ve obtained a perfect fit.\nIn contrast, normally there is variation in our measurements. There may be one true process that determines my resting heart rate; nonetheless, actual measurements will vary over time. There is nothing to be done about this: This is the aleatoric part that just remains, to be factored into our expectations.\nNow reading this, you might be thinking: “Wouldn’t a model that actually were perfect capture those pseudo-random fluctuations?”. We’ll leave that phisosophical question be; instead, we’ll try to illustrate the usefulness of this distinction by example, in a practical way. In a nutshell, viewing a model’s aleatoric uncertainty output should caution us to factor in appropriate deviations when making our predictions, while inspecting epistemic uncertainty should help us re-think the appropriateness of the chosen model.\nNow let’s dive in and see how we may accomplish our goal with tfprobability. We start with the simulated dataset.\nUncertainty estimates on simulated data\nDataset\nWe re-use the dataset from the Google TensorFlow Probability team’s blog post on the same subject 1, with one exception: We extend the range of the independent variable a bit on the negative side, to better demonstrate the different methods’ behaviors.\nHere is the data-generating process. We also get library loading out of the way. Like the preceding posts on tfprobability, this one too features recently added functionality, so please use the development versions of tensorflow and tfprobability as well as keras. Call install_tensorflow(version = \"nightly\") to obtain a current nightly build of TensorFlow and TensorFlow Probability:\n\n\n# make sure we use the development versions of tensorflow, tfprobability and keras\ndevtools::install_github(\"rstudio/tensorflow\")\ndevtools::install_github(\"rstudio/tfprobability\")\ndevtools::install_github(\"rstudio/keras\")\n\n# and that we use a nightly build of TensorFlow and TensorFlow Probability\ntensorflow::install_tensorflow(version = \"nightly\")\n\nlibrary(tensorflow)\nlibrary(tfprobability)\nlibrary(keras)\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\n# make sure this code is compatible with TensorFlow 2.0\ntf$compat$v1$enable_v2_behavior()\n\n# generate the data\nx_min <- -40\nx_max <- 60\nn <- 150\nw0 <- 0.125\nb0 <- 5\n\nnormalize <- function(x) (x - x_min) / (x_max - x_min)\n\n# training data; predictor \nx <- x_min + (x_max - x_min) * runif(n) %>% as.matrix()\n\n# training data; target\neps <- rnorm(n) * (3 * (0.25 + (normalize(x)) ^ 2))\ny <- (w0 * x * (1 + sin(x)) + b0) + eps\n\n# test data (predictor)\nx_test <- seq(x_min, x_max, length.out = n) %>% as.matrix()\n\nHow does the data look?\n\n\nggplot(data.frame(x = x, y = y), aes(x, y)) + geom_point()\n\n\n\n\nFigure 1: Simulated data\n\n\n\nThe task here is single-predictor regression, which in principle we can achieve use Keras dense layers. Let’s see how to enhance this by indicating uncertainty, starting from the aleatoric type.\nAleatoric uncertainty\nAleatoric uncertainty, by definition, is not a statement about the model. So why not have the model learn the uncertainty inherent in the data?\nThis is exactly how aleatoric uncertainty is operationalized in this approach. Instead of a single output per input – the predicted mean of the regression – here we have two outputs: one for the mean, and one for the standard deviation.\nHow will we use these? Until shortly, we would have had to roll our own logic. Now with tfprobability, we make the network output not tensors, but distributions – put differently, we make the last layer a distribution layer.\nDistribution layers are Keras layers, but contributed by tfprobability. The awesome thing is that we can train them with just tensors as targets, as usual: No need to compute probabilities ourselves.\nSeveral specialized distribution layers exist, such as layer_kl_divergence_add_loss, layer_independent_bernoulli, or layer_mixture_same_family, but the most general is layer_distribution_lambda. layer_distribution_lambda takes as inputs the preceding layer and outputs a distribution. In order to be able to do this, we need to tell it how to make use of the preceding layer’s activations.\nIn our case, at some point we will want to have a dense layer with two units.\n\n\n... %>% layer_dense(units = 2, activation = \"linear\") %>%\n\nThen layer_distribution_lambda will use the first unit as the mean of a normal distribution, and the second as its standard deviation.\n\n\nlayer_distribution_lambda(function(x)\n    tfd_normal(loc = x[, 1, drop = FALSE],\n               scale = 1e-3 + tf$math$softplus(x[, 2, drop = FALSE])\n               )\n)\n\nHere is the complete model we use. We insert an additional dense layer in front, with a relu activation, to give the model a bit more freedom and capacity. We discuss this, as well as that scale = ... foo, as soon as we’ve finished our walkthrough of model training.\n\n\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 8, activation = \"relu\") %>%\n  layer_dense(units = 2, activation = \"linear\") %>%\n  layer_distribution_lambda(function(x)\n    tfd_normal(loc = x[, 1, drop = FALSE],\n               # ignore on first read, we'll come back to this\n               # scale = 1e-3 + 0.05 * tf$math$softplus(x[, 2, drop = FALSE])\n               scale = 1e-3 + tf$math$softplus(x[, 2, drop = FALSE])\n               )\n  )\n\nFor a model that outputs a distribution, the loss is the negative log likelihood given the target data.\n\n\nnegloglik <- function(y, model) - (model %>% tfd_log_prob(y))\n\nWe can now compile and fit the model.\n\n\nlearning_rate <- 0.01\nmodel %>% compile(optimizer = optimizer_adam(lr = learning_rate), loss = negloglik)\n\nmodel %>% fit(x, y, epochs = 1000)\n\nWe now call the model on the test data to obtain the predictions. The predictions now actually are distributions, and we have 150 of them, one for each datapoint:\n\n\nyhat <- model(tf$constant(x_test))\n\n\ntfp.distributions.Normal(\"sequential/distribution_lambda/Normal/\",\nbatch_shape=[150, 1], event_shape=[], dtype=float32)\nTo obtain the means and standard deviations – the latter being that measure of aleatoric uncertainty we’re interested in – we just call tfd_mean and tfd_stddev on these distributions. That will give us the predicted mean, as well as the predicted variance, per datapoint.\n\n\nmean <- yhat %>% tfd_mean()\nsd <- yhat %>% tfd_stddev()\n\nLet’s visualize this. Here are the actual test data points, the predicted means, as well as confidence bands indicating the mean estimate plus/minus two standard deviations.\n\n\nggplot(data.frame(\n  x = x,\n  y = y,\n  mean = as.numeric(mean),\n  sd = as.numeric(sd)\n),\naes(x, y)) +\n  geom_point() +\n  geom_line(aes(x = x_test, y = mean), color = \"violet\", size = 1.5) +\n  geom_ribbon(aes(\n    x = x_test,\n    ymin = mean - 2 * sd,\n    ymax = mean + 2 * sd\n  ),\n  alpha = 0.2,\n  fill = \"grey\")\n\n\n\n\nFigure 2: Aleatoric uncertainty on simulated data, using relu activation in the first dense layer.\n\n\n\nThis looks pretty reasonable. What if we had used linear activation in the first layer? Meaning, what if the model had looked like this2:\n\n\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 8, activation = \"linear\") %>%\n  layer_dense(units = 2, activation = \"linear\") %>%\n  layer_distribution_lambda(function(x)\n    tfd_normal(loc = x[, 1, drop = FALSE],\n               scale = 1e-3 + 0.05 * tf$math$softplus(x[, 2, drop = FALSE])\n               )\n  )\n\nThis time, the model does not capture the “form” of the data that well, as we’ve disallowed any nonlinearities.\n\n\n\nFigure 3: Aleatoric uncertainty on simulated data, using linear activation in the first dense layer.\n\n\n\nUsing linear activations only, we also need to do more experimenting with the scale = ... line to get the result look “right”. With relu, on the other hand, results are pretty robust to changes in how scale is computed. Which activation do we choose? If our goal is to adequately model variation in the data, we can just choose relu – and leave assessing uncertainty in the model to a different technique (the epistemic uncertainty that is up next).\nOverall, it seems like aleatoric uncertainty is the straightforward part. We want the network to learn the variation inherent in the data, which it does. What do we gain? Instead of obtaining just point estimates, which in this example might turn out pretty bad in the two fan-like areas of the data on the left and right sides, we learn about the spread as well. We’ll thus be appropriately cautious depending on what input range we’re making predictions for.\nEpistemic uncertainty\nNow our focus is on the model. Given a speficic model (e.g., one from the linear family), what kind of data does it say conforms to its expectations?\nTo answer this question, we make use of a variational-dense layer. This is again a Keras layer provided by tfprobability. Internally, it works by minimizing the evidence lower bound (ELBO), thus striving to find an approximative posterior that does two things:\nfit the actual data well (put differently: achieve high log likelihood), and\nstay close to a prior (as measured by KL divergence).\nAs users, we actually specify the form of the posterior as well as that of the prior. Here is how a prior could look.\n\n\nprior_trainable <-\n  function(kernel_size,\n           bias_size = 0,\n           dtype = NULL) {\n    n <- kernel_size + bias_size\n    keras_model_sequential() %>%\n      # we'll comment on this soon\n      # layer_variable(n, dtype = dtype, trainable = FALSE) %>%\n      layer_variable(n, dtype = dtype, trainable = TRUE) %>%\n      layer_distribution_lambda(function(t) {\n        tfd_independent(tfd_normal(loc = t, scale = 1),\n                        reinterpreted_batch_ndims = 1)\n      })\n  }\n\nThis prior is itself a Keras model, containing a layer that wraps a variable and a layer_distribution_lambda, that type of distribution-yielding layer we’ve just encountered above. The variable layer could be fixed (non-trainable) or non-trainable, corresponding to a genuine prior or a prior learnt from the data in an empirical Bayes-like way. The distribution layer outputs a normal distribution since we’re in a regression setting.\nThe posterior too is a Keras model – definitely trainable this time. It too outputs a normal distribution:\n\n\nposterior_mean_field <-\n  function(kernel_size,\n           bias_size = 0,\n           dtype = NULL) {\n    n <- kernel_size + bias_size\n    c <- log(expm1(1))\n    keras_model_sequential(list(\n      layer_variable(shape = 2 * n, dtype = dtype),\n      layer_distribution_lambda(\n        make_distribution_fn = function(t) {\n          tfd_independent(tfd_normal(\n            loc = t[1:n],\n            scale = 1e-5 + tf$nn$softplus(c + t[(n + 1):(2 * n)])\n            ), reinterpreted_batch_ndims = 1)\n        }\n      )\n    ))\n  }\n\nNow that we’ve defined both, we can set up the model’s layers. The first one, a variational-dense layer, has a single unit. The ensuing distribution layer then takes that unit’s output and uses it for the mean of a normal distribution – while the scale of that Normal is fixed at 1:\n\n\nmodel <- keras_model_sequential() %>%\n  layer_dense_variational(\n    units = 1,\n    make_posterior_fn = posterior_mean_field,\n    make_prior_fn = prior_trainable,\n    kl_weight = 1 / n\n  ) %>%\n  layer_distribution_lambda(function(x)\n    tfd_normal(loc = x, scale = 1))\n\nYou may have noticed one argument to layer_dense_variational we haven’t discussed yet, kl_weight. This is used to scale the contribution to the total loss of the KL divergence, and normally should equal one over the number of data points.\nTraining the model is straightforward. As users, we only specify the negative log likelihood part of the loss; the KL divergence part is taken care of transparently by the framework.\n\n\nnegloglik <- function(y, model) - (model %>% tfd_log_prob(y))\nmodel %>% compile(optimizer = optimizer_adam(lr = learning_rate), loss = negloglik)\nmodel %>% fit(x, y, epochs = 1000)\n\nBecause of the stochasticity inherent in a variational-dense layer, each time we call this model, we obtain different results: different normal distributions, in this case. To obtain the uncertainty estimates we’re looking for, we therefore call the model a bunch of times – 100, say:\n\n\nyhats <- purrr::map(1:100, function(x) model(tf$constant(x_test)))\n\nWe can now plot those 100 predictions – lines, in this case, as there are no nonlinearities:\n\n\nmeans <-\n  purrr::map(yhats, purrr::compose(as.matrix, tfd_mean)) %>% abind::abind()\n\nlines <- data.frame(cbind(x_test, means)) %>%\n  gather(key = run, value = value,-X1)\n\nmean <- apply(means, 1, mean)\n\nggplot(data.frame(x = x, y = y, mean = as.numeric(mean)), aes(x, y)) +\n  geom_point() +\n  geom_line(aes(x = x_test, y = mean), color = \"violet\", size = 1.5) +\n  geom_line(\n    data = lines,\n    aes(x = X1, y = value, color = run),\n    alpha = 0.3,\n    size = 0.5\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nFigure 4: Epistemic uncertainty on simulated data, using linear activation in the variational-dense layer.\n\n\n\nWhat we see here are essentially different models, consistent with the assumptions built into the architecture. What we’re not accounting for is the spread in the data. Can we do both? We can; but first let’s comment on a few choices that were made and see how they affect the results.\nTo prevent this post from growing to infinite size, we’ve refrained from performing a systematic experiment; please take what follows not as generalizable statements, but as pointers to things you will want to keep in mind in your own ventures. Especially, each (hyper-)parameter is not an island; they could interact in unforeseen ways.\nAfter those words of caution, here are some things we noticed.\nOne question you might ask: Before, in the aleatoric uncertainty setup, we added an additional dense layer to the model, with relu activation. What if we did this here? Firstly, we’re not adding any additional, non-variational layers in order to keep the setup “fully Bayesian” – we want priors at every level. As to using relu in layer_dense_variational, we did try that, and the results look pretty similar:\n\n\n\nFigure 5: Epistemic uncertainty on simulated data, using relu activation in the variational-dense layer.\n\n\n\nHowever, things look pretty different if we drastically reduce training time… which brings us to the next observation.\nUnlike in the aleatoric setup, the number of training epochs matter a lot. If we train, quote unquote, too long, the posterior estimates will get closer and closer to the posterior mean: we lose uncertainty. What happens if we train “too short” is even more notable. Here are the results for the linear-activation as well as the relu-activation cases:\n\n\n\nFigure 6: Epistemic uncertainty on simulated data if we train for 100 epochs only. Left: linear activation. Right: relu activation.\n\n\n\nInterestingly, both model families look very different now, and while the linear-activation family looks more reasonable at first, it still considers an overall negative slope consistent with the data.\nSo how many epochs are “long enough”? From observation, we’d say that a working heuristic should probably be based on the rate of loss reduction. But certainly, it’ll make sense to try different numbers of epochs and check the effect on model behavior. As an aside, monitoring estimates over training time may even yield important insights into the assumptions built into a model (e.g., the effect of different activation functions).\nAs important as the number of epochs trained, and similar in effect, is the learning rate. If we replace the learning rate in this setup by 0.001, results will look similar to what we saw above for the epochs = 100 case. Again, we will want to try different learning rates and make sure we train the model “to completion” in some reasonable sense.\nTo conclude this section, let’s quickly look at what happens if we vary two other parameters. What if the prior were non-trainable (see the commented line above)? And what if we scaled the importance of the KL divergence (kl_weight in layer_dense_variational’s argument list) differently, replacing kl_weight = 1/n by kl_weight = 1 (or equivalently, removing it)? Here are the respective results for an otherwise-default setup. They don’t lend themselves to generalization – on different (e.g., bigger!) datasets the outcomes will most certainly look different – but definitely interesting to observe.\n\n\n\nFigure 7: Epistemic uncertainty on simulated data. Left: kl_weight = 1. Right: prior non-trainable.\n\n\n\nNow let’s come back to the question: We’ve modeled spread in the data, we’ve peeked into the heart of the model, – can we do both at the same time?\nWe can, if we combine both approaches. We add an additional unit to the variational-dense layer and use this to learn the variance: once for each “sub-model” contained in the model.\nCombining both aleatoric and epistemic uncertainty\nReusing the prior and posterior from above, this is how the final model looks:\n\n\nmodel <- keras_model_sequential() %>%\n  layer_dense_variational(\n    units = 2,\n    make_posterior_fn = posterior_mean_field,\n    make_prior_fn = prior_trainable,\n    kl_weight = 1 / n\n  ) %>%\n  layer_distribution_lambda(function(x)\n    tfd_normal(loc = x[, 1, drop = FALSE],\n               scale = 1e-3 + tf$math$softplus(0.01 * x[, 2, drop = FALSE])\n               )\n    )\n\nWe train this model just like the epistemic-uncertainty only one. We then obtain a measure of uncertainty per predicted line. Or in the words we used above, we now have an ensemble of models each with its own indication of spread in the data. Here is a way we could display this – each colored line is the mean of a distribution, surrounded by a confidence band indicating +/- two standard deviations.\n\n\nyhats <- purrr::map(1:100, function(x) model(tf$constant(x_test)))\nmeans <-\n  purrr::map(yhats, purrr::compose(as.matrix, tfd_mean)) %>% abind::abind()\nsds <-\n  purrr::map(yhats, purrr::compose(as.matrix, tfd_stddev)) %>% abind::abind()\n\nmeans_gathered <- data.frame(cbind(x_test, means)) %>%\n  gather(key = run, value = mean_val,-X1)\nsds_gathered <- data.frame(cbind(x_test, sds)) %>%\n  gather(key = run, value = sd_val,-X1)\n\nlines <-\n  means_gathered %>% inner_join(sds_gathered, by = c(\"X1\", \"run\"))\nmean <- apply(means, 1, mean)\n\nggplot(data.frame(x = x, y = y, mean = as.numeric(mean)), aes(x, y)) +\n  geom_point() +\n  theme(legend.position = \"none\") +\n  geom_line(aes(x = x_test, y = mean), color = \"violet\", size = 1.5) +\n  geom_line(\n    data = lines,\n    aes(x = X1, y = mean_val, color = run),\n    alpha = 0.6,\n    size = 0.5\n  ) +\n  geom_ribbon(\n    data = lines,\n    aes(\n      x = X1,\n      ymin = mean_val - 2 * sd_val,\n      ymax = mean_val + 2 * sd_val,\n      group = run\n    ),\n    alpha = 0.05,\n    fill = \"grey\",\n    inherit.aes = FALSE\n  )\n\n\n\n\nFigure 8: Displaying both epistemic and aleatoric uncertainty on the simulated dataset.\n\n\n\nNice! This looks like something we could report.\nAs you might imagine, this model, too, is sensitive to how long (think: number of epochs) or how fast (think: learning rate) we train it. And compared to the epistemic-uncertainty only model, there is an additional choice to be made here: the scaling of the previous layer’s activation – the 0.01 in the scale argument to tfd_normal:\n\n\nscale = 1e-3 + tf$math$softplus(0.01 * x[, 2, drop = FALSE])\n\nKeeping everything else constant, here we vary that parameter between 0.01 and 0.05:\n\n\n\nFigure 9: Epistemic plus aleatoric uncertainty on the simulated dataset: Varying the scale argument.\n\n\n\nEvidently, this is another parameter we should be prepared to experiment with.\nNow that we’ve introduced all three types of presenting uncertainty – aleatoric only, epistemic only, or both – let’s see them on the aforementioned Combined Cycle Power Plant Data Set. Please see our previous post on uncertainty for a quick characterization, as well as visualization, of the dataset.\nCombined Cycle Power Plant Data Set\nTo keep this post at a digestible length, we’ll refrain from trying as many alternatives as with the simulated data and mainly stay with what worked well there. This should also give us an idea of how well these “defaults” generalize. We separately inspect two scenarios: The single-predictor setup (using each of the four available predictors alone), and the complete one (using all four predictors at once).\nThe dataset is loaded just as in the previous post.\n\n\nlibrary(tensorflow)\nlibrary(tfprobability)\nlibrary(keras)\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readxl)\n\n# make sure this code is compatible with TensorFlow 2.0\ntf$compat$v1$enable_v2_behavior()\n\ndf <- read_xlsx(\"CCPP/Folds5x2_pp.xlsx\")\n\ndf_scaled <- scale(df)\ncenters <- attr(df_scaled, \"scaled:center\")\nscales <- attr(df_scaled, \"scaled:scale\")\n\nX <- df_scaled[, 1:4]\ntrain_samples <- sample(1:nrow(df_scaled), 0.8 * nrow(X))\nX_train <- X[train_samples, ]\nX_val <- X[-train_samples, ]\n\ny <- df_scaled[, 5] \ny_train <- y[train_samples] %>% as.matrix()\ny_val <- y[-train_samples] %>% as.matrix()\n\nFirst we look at the single-predictor case, starting from aleatoric uncertainty.\nSingle predictor: Aleatoric uncertainty\nHere is the “default” aleatoric model again. We also duplicate the plotting code here for the reader’s convenience.\n\n\nn <- nrow(X_train) # 7654\nn_epochs <- 10 # we need fewer epochs because the dataset is so much bigger\n\nbatch_size <- 100\n\nlearning_rate <- 0.01\n\n# variable to fit - change to 2,3,4 to get the other predictors\ni <- 1\n\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 16, activation = \"relu\") %>%\n  layer_dense(units = 2, activation = \"linear\") %>%\n  layer_distribution_lambda(function(x)\n    tfd_normal(loc = x[, 1, drop = FALSE],\n               scale = tf$math$softplus(x[, 2, drop = FALSE])\n               )\n    )\n\nnegloglik <- function(y, model) - (model %>% tfd_log_prob(y))\n\nmodel %>% compile(optimizer = optimizer_adam(lr = learning_rate), loss = negloglik)\n\nhist <-\n  model %>% fit(\n    X_train[, i, drop = FALSE],\n    y_train,\n    validation_data = list(X_val[, i, drop = FALSE], y_val),\n    epochs = n_epochs,\n    batch_size = batch_size\n  )\n\nyhat <- model(tf$constant(X_val[, i, drop = FALSE]))\n\nmean <- yhat %>% tfd_mean()\nsd <- yhat %>% tfd_stddev()\n\nggplot(data.frame(\n  x = X_val[, i],\n  y = y_val,\n  mean = as.numeric(mean),\n  sd = as.numeric(sd)\n),\naes(x, y)) +\n  geom_point() +\n  geom_line(aes(x = x, y = mean), color = \"violet\", size = 1.5) +\n  geom_ribbon(aes(\n    x = x,\n    ymin = mean - 2 * sd,\n    ymax = mean + 2 * sd\n  ),\n  alpha = 0.4,\n  fill = \"grey\")\n\nHow well does this work?\n\n\n\nFigure 10: Aleatoric uncertainty on the Combined Cycle Power Plant Data Set; single predictors.\n\n\n\nThis looks pretty good we’d say! How about epistemic uncertainty?\nSingle predictor: Epistemic uncertainty\nHere’s the code:\n\n\nposterior_mean_field <-\n  function(kernel_size,\n           bias_size = 0,\n           dtype = NULL) {\n    n <- kernel_size + bias_size\n    c <- log(expm1(1))\n    keras_model_sequential(list(\n      layer_variable(shape = 2 * n, dtype = dtype),\n      layer_distribution_lambda(\n        make_distribution_fn = function(t) {\n          tfd_independent(tfd_normal(\n            loc = t[1:n],\n            scale = 1e-5 + tf$nn$softplus(c + t[(n + 1):(2 * n)])\n          ), reinterpreted_batch_ndims = 1)\n        }\n      )\n    ))\n  }\n\nprior_trainable <-\n  function(kernel_size,\n           bias_size = 0,\n           dtype = NULL) {\n    n <- kernel_size + bias_size\n    keras_model_sequential() %>%\n      layer_variable(n, dtype = dtype, trainable = TRUE) %>%\n      layer_distribution_lambda(function(t) {\n        tfd_independent(tfd_normal(loc = t, scale = 1),\n                        reinterpreted_batch_ndims = 1)\n      })\n  }\n\nmodel <- keras_model_sequential() %>%\n  layer_dense_variational(\n    units = 1,\n    make_posterior_fn = posterior_mean_field,\n    make_prior_fn = prior_trainable,\n    kl_weight = 1 / n,\n    activation = \"linear\",\n  ) %>%\n  layer_distribution_lambda(function(x)\n    tfd_normal(loc = x, scale = 1))\n\nnegloglik <- function(y, model) - (model %>% tfd_log_prob(y))\nmodel %>% compile(optimizer = optimizer_adam(lr = learning_rate), loss = negloglik)\nhist <-\n  model %>% fit(\n    X_train[, i, drop = FALSE],\n    y_train,\n    validation_data = list(X_val[, i, drop = FALSE], y_val),\n    epochs = n_epochs,\n    batch_size = batch_size\n  )\n\nyhats <- purrr::map(1:100, function(x)\n  yhat <- model(tf$constant(X_val[, i, drop = FALSE])))\n  \nmeans <-\n  purrr::map(yhats, purrr::compose(as.matrix, tfd_mean)) %>% abind::abind()\n\nlines <- data.frame(cbind(X_val[, i], means)) %>%\n  gather(key = run, value = value,-X1)\n\nmean <- apply(means, 1, mean)\nggplot(data.frame(x = X_val[, i], y = y_val, mean = as.numeric(mean)), aes(x, y)) +\n  geom_point() +\n  geom_line(aes(x = X_val[, i], y = mean), color = \"violet\", size = 1.5) +\n  geom_line(\n    data = lines,\n    aes(x = X1, y = value, color = run),\n    alpha = 0.3,\n    size = 0.5\n  ) +\n  theme(legend.position = \"none\")\n\nAnd this is the result.\n\n\n\nFigure 11: Epistemic uncertainty on the Combined Cycle Power Plant Data Set; single predictors.\n\n\n\nAs with the simulated data, the linear models seems to “do the right thing”. And here too, we think we will want to augment this with the spread in the data: Thus, on to way three.\nSingle predictor: Combining both types\nHere we go. Again, posterior_mean_field and prior_trainable look just like in the epistemic-only case.\n\n\nmodel <- keras_model_sequential() %>%\n  layer_dense_variational(\n    units = 2,\n    make_posterior_fn = posterior_mean_field,\n    make_prior_fn = prior_trainable,\n    kl_weight = 1 / n,\n    activation = \"linear\"\n  ) %>%\n  layer_distribution_lambda(function(x)\n    tfd_normal(loc = x[, 1, drop = FALSE],\n               scale = 1e-3 + tf$math$softplus(0.01 * x[, 2, drop = FALSE])))\n\n\nnegloglik <- function(y, model)\n  - (model %>% tfd_log_prob(y))\nmodel %>% compile(optimizer = optimizer_adam(lr = learning_rate), loss = negloglik)\nhist <-\n  model %>% fit(\n    X_train[, i, drop = FALSE],\n    y_train,\n    validation_data = list(X_val[, i, drop = FALSE], y_val),\n    epochs = n_epochs,\n    batch_size = batch_size\n  )\n\nyhats <- purrr::map(1:100, function(x)\n  model(tf$constant(X_val[, i, drop = FALSE])))\nmeans <-\n  purrr::map(yhats, purrr::compose(as.matrix, tfd_mean)) %>% abind::abind()\nsds <-\n  purrr::map(yhats, purrr::compose(as.matrix, tfd_stddev)) %>% abind::abind()\n\nmeans_gathered <- data.frame(cbind(X_val[, i], means)) %>%\n  gather(key = run, value = mean_val,-X1)\nsds_gathered <- data.frame(cbind(X_val[, i], sds)) %>%\n  gather(key = run, value = sd_val,-X1)\n\nlines <-\n  means_gathered %>% inner_join(sds_gathered, by = c(\"X1\", \"run\"))\n\nmean <- apply(means, 1, mean)\n\n#lines <- lines %>% filter(run==\"X3\" | run ==\"X4\")\n\nggplot(data.frame(x = X_val[, i], y = y_val, mean = as.numeric(mean)), aes(x, y)) +\n  geom_point() +\n  theme(legend.position = \"none\") +\n  geom_line(aes(x = X_val[, i], y = mean), color = \"violet\", size = 1.5) +\n  geom_line(\n    data = lines,\n    aes(x = X1, y = mean_val, color = run),\n    alpha = 0.2,\n    size = 0.5\n  ) +\ngeom_ribbon(\n  data = lines,\n  aes(\n    x = X1,\n    ymin = mean_val - 2 * sd_val,\n    ymax = mean_val + 2 * sd_val,\n    group = run\n  ),\n  alpha = 0.01,\n  fill = \"grey\",\n  inherit.aes = FALSE\n)\n\nAnd the output?\n\n\n\nFigure 12: Combined uncertainty on the Combined Cycle Power Plant Data Set; single predictors.\n\n\n\nThis looks useful! Let’s wrap up with our final test case: Using all four predictors together.\nAll predictors\nThe training code used in this scenario looks just like before, apart from our feeding all predictors to the model. For plotting, we resort to displaying the first principal component on the x-axis – this makes the plots look noisier than before. We also display fewer lines for the epistemic and epistemic-plus-aleatoric cases (20 instead of 100). Here are the results:\n\n\n\nFigure 13: Uncertainty (aleatoric, epistemic, both) on the Combined Cycle Power Plant Data Set; all predictors.\n\n\n\nConclusion\nWhere does this leave us? Compared to the learnable-dropout approach described in the prior post, the way presented here is a lot easier, faster, and more intuitively understandable. The methods per se are that easy to use that in this first introductory post, we could afford to explore alternatives already: something we had no time to do in that previous exposition.\nIn fact, we hope this post leaves you in a position to do your own experiments, on your own data. Obviously, you will have to make decisions, but isn’t that the way it is in data science? There’s no way around making decisions; we just should be prepared to justify them … Thanks for reading!\nsee also the corresponding notebook↩︎\nyes, we also use that other line for scale that was commented before; more on that in a second↩︎\n",
    "preview": "posts/2019-06-05-uncertainty-estimates-tfprobability/images/uci_both.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 2020,
    "preview_height": 1020
  },
  {
    "path": "posts/2019-05-24-varying-slopes/",
    "title": "Hierarchical partial pooling, continued: Varying slopes models with TensorFlow Probability",
    "description": "This post builds on our recent introduction to multi-level modeling with tfprobability, the R wrapper to TensorFlow Probability. We show how to pool not just mean values (\"intercepts\"), but also relationships (\"slopes\"), thus enabling models to learn from data in an even broader way. Again, we use an example from Richard McElreath's \"Statistical Rethinking\"; the terminology as well as the way we present this topic are largely owed to this book.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-05-24",
    "categories": [
      "Bayesian Modeling",
      "TensorFlow/Keras"
    ],
    "contents": "\nIn a previous post, we showed how to use tfprobability – the R interface to TensorFlow Probability – to build a multilevel, or partial pooling model of tadpole survival in differently sized (and thus, differing in inhabitant number) tanks.\nA completely pooled model would have resulted in a global estimate of survival count, irrespective of tank, while an unpooled model would have learned to predict survival count for each tank separately. The former approach does not take into account different circumstances; the latter does not make use of common information. (Also, it clearly has no predictive use unless we want to make predictions for the very same entities we used to train the model.)\nIn contrast, a partially pooled model lets you make predictions for the familiar, as well as new entities: Just use the appropriate prior.\nAssuming we are in fact interested in the same entities – why would we want to apply partial pooling? For the same reasons so much effort in machine learning goes into devising regularization mechanisms. We don’t want to overfit too much to actual measurements, be they related to the same entity or a class of entities. If I want to predict my heart rate as I wake up next morning, based on a single measurement I’m taking now (let’s say it’s evening and I’m frantically typing a blog post), I better take into account some facts about heart rate behavior in general (instead of just projecting into the future the exact value measured right now).\nIn the tadpole example, this means we expect generalization to work better for tanks with many inhabitants, compared to more solitary environments. For the latter ones, we better take a peek at survival rates from other tanks, to supplement the sparse, idiosyncratic information available. Or using the technical term, in the latter case we hope for the model to shrink its estimates toward the overall mean more noticeably than in the former.\nThis type of information sharing is already very useful, but it gets better. The tadpole model is a varying intercepts model, as McElreath calls it (or random intercepts, as it is sometimes – confusingly – called 1) – intercepts referring to the way we make predictions for entities (here: tanks), with no predictor variables present. So if we can pool information about intercepts, why not pool information about slopes as well? This will allow us to, in addition, make use of relationships between variables learnt on different entities in the training set.\nSo as you might have guessed by now, varying slopes (or random slopes, if you will) is the topic of today’s post. Again, we take up an example from McElreath’s book, and show how to accomplish the same thing with tfprobability.\nCoffee, please\nUnlike the tadpole case, this time we work with simulated data. This is the data McElreath uses to introduce the varying slopes modeling technique; he then goes on and applies it to one of the book’s most featured datasets, the pro-social (or indifferent, rather!) chimpanzees. For today, we stay with the simulated data for two reasons: First, the subject matter per se is non-trivial enough; and second, we want to keep careful track of what our model does, and whether its output is sufficiently close to the results McElreath obtained from Stan 2.\nSo, the scenario is this. 3 Cafés vary in how popular they are. In a popular café, when you order coffee, you’re likely to wait. In a less popular café, you’ll likely be served much faster. That’s one thing. Second, all cafés tend to be more crowded in the mornings than in the afternoons. Thus in the morning, you’ll wait longer than in the afternoon – this goes for the popular as well as the less popular cafés.\nIn terms of intercepts and slopes, we can picture the morning waits as intercepts, and the resultant afternoon waits as arising due to the slopes of the lines joining each morning and afternoon wait, respectively.\nSo when we partially-pool intercepts, we have one “intercept prior” (itself constrained by a prior, of course), and a set of café-specific intercepts that will vary around it. When we partially-pool slopes, we have a “slope prior” reflecting the overall relationship between morning and afternoon waits, and a set of café-specific slopes reflecting the individual relationships. Cognitively, that means that if you have never been to the Café Gerbeaud in Budapest but have been to cafés before, you might have a less-than-uninformed idea about how long you are going to wait; it also means that if you normally get your coffee in your favorite corner café in the mornings, and now you pass by there in the afternoon, you have an approximate idea how long it’s going to take (namely, fewer minutes than in the mornings).\nSo is that all? Actually, no. In our scenario, intercepts and slopes are related. If, at a less popular café, I always get my coffee before two minutes have passed, there is little room for improvement. At a highly popular café though, if it could easily take ten minutes in the mornings, then there is quite some potential for decrease in waiting time in the afternoon. So in my prediction for this afternoon’s waiting time, I should factor in this interaction effect.\nSo, now that we have an idea of what this is all about, let’s see how we can model these effects with tfprobability. But first, we actually have to generate the data.\nSimulate the data\nWe directly follow McElreath in the way the data are generated.\n\n\n##### Inputs needed to generate the covariance matrix between intercepts and slopes #####\n\n# average morning wait time\na <- 3.5\n# average difference afternoon wait time\n# we wait less in the afternoons\nb <- -1\n# standard deviation in the (café-specific) intercepts\nsigma_a <- 1\n# standard deviation in the (café-specific) slopes\nsigma_b <- 0.5\n# correlation between intercepts and slopes\n# the higher the intercept, the more the wait goes down\nrho <- -0.7\n\n\n##### Generate the covariance matrix #####\n\n# means of intercepts and slopes\nmu <- c(a, b)\n# standard deviations of means and slopes\nsigmas <- c(sigma_a, sigma_b) \n# correlation matrix\n# a correlation matrix has ones on the diagonal and the correlation in the off-diagonals\nrho <- matrix(c(1, rho, rho, 1), nrow = 2) \n# now matrix multiply to get covariance matrix\ncov_matrix <- diag(sigmas) %*% rho %*% diag(sigmas)\n\n\n##### Generate the café-specific intercepts and slopes #####\n\n# 20 cafés overall\nn_cafes <- 20\n\nlibrary(MASS)\nset.seed(5) # used to replicate example\n# multivariate distribution of intercepts and slopes\nvary_effects <- mvrnorm(n_cafes , mu ,cov_matrix)\n# intercepts are in the first column\na_cafe <- vary_effects[ ,1]\n# slopes are in the second\nb_cafe <- vary_effects[ ,2]\n\n\n##### Generate the actual wait times #####\n\nset.seed(22)\n# 10 visits per café\nn_visits <- 10\n\n# alternate values for mornings and afternoons in the data frame\nafternoon <- rep(0:1, n_visits * n_cafes/2)\n# data for each café are consecutive rows in the data frame\ncafe_id <- rep(1:n_cafes, each = n_visits)\n\n# the regression equation for the mean waiting time\nmu <- a_cafe[cafe_id] + b_cafe[cafe_id] * afternoon\n# standard deviation of waiting time within cafés\nsigma <- 0.5 # std dev within cafes\n# generate instances of waiting times\nwait <- rnorm(n_visits * n_cafes, mu, sigma)\n\nd <- data.frame(cafe = cafe_id, afternoon = afternoon, wait = wait)\n\nTake a glimpse at the data:\n\n\nd %>% glimpse()\n\n\nObservations: 200\nVariables: 3\n$ cafe      <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3,...\n$ afternoon <int> 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,...\n$ wait      <dbl> 3.9678929, 3.8571978, 4.7278755, 2.7610133, 4.1194827, 3.54365,...\nOn to building the model.\nThe model\nAs in the previous post on multi-level modeling, we use tfd_joint_distribution_sequential to define the model and Hamiltonian Monte Carlo for sampling. Consider taking a look at the first section of that post for a quick reminder of the overall procedure.\nBefore we code the model, let’s quickly get library loading out of the way. Importantly, again just like in the previous post, we need to install a master build of TensorFlow Probability, as we’re making use of very new features not yet available in the current release version. The same goes for the R packages tensorflow and tfprobability: Please install the respective development versions from github.\n\n\ndevtools::install_github(\"rstudio/tensorflow\")\ndevtools::install_github(\"rstudio/tfprobability\")\n\n# this will install the latest nightlies of TensorFlow as well as TensorFlow Probability\ntensorflow::install_tensorflow(version = \"nightly\")\n\nlibrary(tensorflow)\ntf$compat$v1$enable_v2_behavior()\n\nlibrary(tfprobability)\n\nlibrary(tidyverse)\nlibrary(zeallot)\nlibrary(abind)\nlibrary(gridExtra)\nlibrary(HDInterval)\nlibrary(ellipse)\n\nNow here is the model definition. We’ll go through it step by step in an instant.\n\n\nmodel <- function(cafe_id) {\n  tfd_joint_distribution_sequential(\n      list(\n        # rho, the prior for the correlation matrix between intercepts and slopes\n        tfd_cholesky_lkj(2, 2), \n        # sigma, prior variance for the waiting time\n        tfd_sample_distribution(tfd_exponential(rate = 1), sample_shape = 1),\n        # sigma_cafe, prior of variances for intercepts and slopes (vector of 2)\n        tfd_sample_distribution(tfd_exponential(rate = 1), sample_shape = 2), \n        # b, the prior mean for the slopes\n        tfd_sample_distribution(tfd_normal(loc = -1, scale = 0.5), sample_shape = 1),\n        # a, the prior mean for the intercepts\n        tfd_sample_distribution(tfd_normal(loc = 5, scale = 2), sample_shape = 1), \n        # mvn, multivariate distribution of intercepts and slopes\n        # shape: batch size, 20, 2\n        function(a,b,sigma_cafe,sigma,chol_rho) \n          tfd_sample_distribution(\n            tfd_multivariate_normal_tri_l(\n              loc = tf$concat(list(a,b), axis = -1L),\n              scale_tril = tf$linalg$LinearOperatorDiag(sigma_cafe)$matmul(chol_rho)),\n            sample_shape = n_cafes),\n        # waiting time\n        # shape should be batch size, 200\n        function(mvn, a, b, sigma_cafe, sigma)\n          tfd_independent(\n            # need to pull out the correct cafe_id in the middle column\n            tfd_normal(\n              loc = (tf$gather(mvn[ , , 1], cafe_id, axis = -1L) +\n                       tf$gather(mvn[ , , 2], cafe_id, axis = -1L) * afternoon), \n              scale=sigma),  # Shape [batch,  1]\n        reinterpreted_batch_ndims=1\n        )\n    )\n  )\n}\n\nThe first five distributions are priors. First, we have the prior for the correlation matrix. Basically, this would be an LKJ distribution of shape 2x2 and with concentration parameter equal to 2.\nFor performance reasons, we work with a version that inputs and outputs Cholesky factors instead:\n\n\n# rho, the prior correlation matrix between intercepts and slopes\ntfd_cholesky_lkj(2, 2)\n\nWhat kind of prior is this? As McElreath keeps reminding us, nothing is more instructive than sampling from the prior. For us to see what’s going on, we use the base LKJ distribution, not the Cholesky one:\n\n\ncorr_prior <- tfd_lkj(2, 2)\ncorrelation <- (corr_prior %>% tfd_sample(100))[ , 1, 2] %>% as.numeric()\nlibrary(ggplot2)\ndata.frame(correlation) %>% ggplot(aes(x = correlation)) + geom_density()\n\n\n\n\nSo this prior is moderately skeptical about strong correlations, but pretty open to learning from data.\nThe next distribution in line\n\n\n# sigma, prior variance for the waiting time\ntfd_sample_distribution(tfd_exponential(rate = 1), sample_shape = 1)\n\nis the prior for the variance of the waiting time, the very last distribution in the list.\nNext is the prior distribution of variances for the intercepts and slopes. This prior is the same for both cases, but we specify a sample_shape of 2 to get two individual samples.\n\n\n# sigma_cafe, prior of variances for intercepts and slopes (vector of 2)\ntfd_sample_distribution(tfd_exponential(rate = 1), sample_shape = 2)\n\nNow that we have the respective prior variances, we move on to the prior means. Both are normal distributions.\n\n\n# b, the prior mean for the slopes\ntfd_sample_distribution(tfd_normal(loc = -1, scale = 0.5), sample_shape = 1)\n\n\n\n# a, the prior mean for the intercepts\ntfd_sample_distribution(tfd_normal(loc = 5, scale = 2), sample_shape = 1)\n\nOn to the heart of the model, where the partial pooling happens. We are going to construct partially-pooled intercepts and slopes for all of the cafés. Like we said above, intercepts and slopes are not independent; they interact. Thus, we need to use a multivariate normal distribution. The means are given by the prior means defined right above, while the covariance matrix is built from the above prior variances and the prior correlation matrix. The output shape here is determined by the number of cafés: We want an intercept and a slope for every café.\n\n\n# mvn, multivariate distribution of intercepts and slopes\n# shape: batch size, 20, 2\nfunction(a,b,sigma_cafe,sigma,chol_rho) \n  tfd_sample_distribution(\n    tfd_multivariate_normal_tri_l(\n      loc = tf$concat(list(a,b), axis = -1L),\n      scale_tril = tf$linalg$LinearOperatorDiag(sigma_cafe)$matmul(chol_rho)),\n  sample_shape = n_cafes)\n\nFinally, we sample the actual waiting times. This code pulls out the correct intercepts and slopes from the multivariate normal and outputs the mean waiting time, dependent on what café we’re in and whether it’s morning or afternoon.\n\n\n        # waiting time\n        # shape: batch size, 200\n        function(mvn, a, b, sigma_cafe, sigma)\n          tfd_independent(\n            # need to pull out the correct cafe_id in the middle column\n            tfd_normal(\n              loc = (tf$gather(mvn[ , , 1], cafe_id, axis = -1L) +\n                       tf$gather(mvn[ , , 2], cafe_id, axis = -1L) * afternoon), \n              scale=sigma), \n        reinterpreted_batch_ndims=1\n        )\n\nBefore running the sampling, it’s always a good idea to do a quick check on the model.\n\n\nn_cafes <- 20\ncafe_id <- tf$cast((d$cafe - 1) %% 20, tf$int64)\n\nafternoon <- d$afternoon\nwait <- d$wait\n\nWe sample from the model and then, check the log probability.\n\n\nm <- model(cafe_id)\n\ns <- m %>% tfd_sample(3)\nm %>% tfd_log_prob(s)\n\nWe want a scalar log probability per member in the batch, which is what we get.\n\ntf.Tensor([-466.1392  -149.92587 -196.51688], shape=(3,), dtype=float32)\nRunning the chains\nThe actual Monte Carlo sampling works just like in the previous post, with one exception. Sampling happens in unconstrained parameter space, but at the end we need to get valid correlation matrix parameters rho and valid variances sigma and sigma_cafe. Conversion between spaces is done via TFP bijectors. Luckily, this is not something we have to do as users; all we need to specify are appropriate bijectors. For the normal distributions in the model, there is nothing to do.\n\n\nconstraining_bijectors <- list(\n  # make sure the rho[1:4] parameters are valid for a Cholesky factor\n  tfb_correlation_cholesky(),\n  # make sure variance is positive\n  tfb_exp(),\n  # make sure variance is positive\n  tfb_exp(),\n  tfb_identity(),\n  tfb_identity(),\n  tfb_identity()\n)\n\nNow we can set up the Hamiltonian Monte Carlo sampler.\n\n\nn_steps <- 500\nn_burnin <- 500\nn_chains <- 4\n\n# set up the optimization objective\nlogprob <- function(rho, sigma, sigma_cafe, b, a, mvn)\n  m %>% tfd_log_prob(list(rho, sigma, sigma_cafe, b, a, mvn, wait))\n\n# initial states for the sampling procedure\nc(initial_rho, initial_sigma, initial_sigma_cafe, initial_b, initial_a, initial_mvn, .) %<-% \n  (m %>% tfd_sample(n_chains))\n\n# HMC sampler, with the above bijectors and step size adaptation\nhmc <- mcmc_hamiltonian_monte_carlo(\n  target_log_prob_fn = logprob,\n  num_leapfrog_steps = 3,\n  step_size = list(0.1, 0.1, 0.1, 0.1, 0.1, 0.1)\n) %>%\n  mcmc_transformed_transition_kernel(bijector = constraining_bijectors) %>%\n  mcmc_simple_step_size_adaptation(target_accept_prob = 0.8,\n                                   num_adaptation_steps = n_burnin)\n\nAgain, we can obtain additional diagnostics (here: step sizes and acceptance rates) by registering a trace function:\n\n\ntrace_fn <- function(state, pkr) {\n  list(pkr$inner_results$inner_results$is_accepted,\n       pkr$inner_results$inner_results$accepted_results$step_size)\n}\n\nHere, then, is the sampling function. Note how we use tf_function to put it on the graph. At least as of today, this makes a huge difference in sampling performance when using eager execution.\n\n\nrun_mcmc <- function(kernel) {\n  kernel %>% mcmc_sample_chain(\n    num_results = n_steps,\n    num_burnin_steps = n_burnin,\n    current_state = list(initial_rho,\n                         tf$ones_like(initial_sigma),\n                         tf$ones_like(initial_sigma_cafe),\n                         initial_b,\n                         initial_a,\n                         initial_mvn),\n    trace_fn = trace_fn\n  )\n}\n\nrun_mcmc <- tf_function(run_mcmc)\nres <- hmc %>% run_mcmc()\n\nmcmc_trace <- res$all_states\n\nSo how do our samples look, and what do we get in terms of posteriors? Let’s see.\nResults\nAt this moment, mcmc_trace is a list of tensors of different shapes, dependent on how we defined the parameters. We need to do a bit of post-processing to be able to summarise and display the results.\n\n\n# the actual mcmc samples\n# for the trace plots, we want to have them in shape (500, 4, 49)\n# that is: (number of steps, number of chains, number of parameters)\nsamples <- abind(\n  # rho 1:4\n  as.array(mcmc_trace[[1]] %>% tf$reshape(list(tf$cast(n_steps, tf$int32), tf$cast(n_chains, tf$int32), 4L))),\n  # sigma\n  as.array(mcmc_trace[[2]]),  \n  # sigma_cafe 1:2\n  as.array(mcmc_trace[[3]][ , , 1]),    \n  as.array(mcmc_trace[[3]][ , , 2]), \n  # b\n  as.array(mcmc_trace[[4]]),  \n  # a\n  as.array(mcmc_trace[[5]]),  \n  # mvn 10:49\n  as.array( mcmc_trace[[6]] %>% tf$reshape(list(tf$cast(n_steps, tf$int32), tf$cast(n_chains, tf$int32), 40L))),\n  along = 3) \n\n# the effective sample sizes\n# we want them in shape (4, 49), which is (number of chains * number of parameters)\ness <- mcmc_effective_sample_size(mcmc_trace) \ness <- cbind(\n  # rho 1:4\n  as.matrix(ess[[1]] %>% tf$reshape(list(tf$cast(n_chains, tf$int32), 4L))),\n  # sigma\n  as.matrix(ess[[2]]),  \n  # sigma_cafe 1:2\n  as.matrix(ess[[3]][ , 1, drop = FALSE]),    \n  as.matrix(ess[[3]][ , 2, drop = FALSE]), \n  # b\n  as.matrix(ess[[4]]),  \n  # a\n  as.matrix(ess[[5]]),  \n  # mvn 10:49\n  as.matrix(ess[[6]] %>% tf$reshape(list(tf$cast(n_chains, tf$int32), 40L)))\n  ) \n\n# the rhat values\n# we want them in shape (49), which is (number of parameters)\nrhat <- mcmc_potential_scale_reduction(mcmc_trace)\nrhat <- c(\n  # rho 1:4\n  as.double(rhat[[1]] %>% tf$reshape(list(4L))),\n  # sigma\n  as.double(rhat[[2]]),  \n  # sigma_cafe 1:2\n  as.double(rhat[[3]][1]),    \n  as.double(rhat[[3]][2]), \n  # b\n  as.double(rhat[[4]]),  \n  # a\n  as.double(rhat[[5]]),  \n  # mvn 10:49\n  as.double(rhat[[6]] %>% tf$reshape(list(40L)))\n  ) \n\nTrace plots\nHow well do the chains mix?\n\n\nprep_tibble <- function(samples) {\n  as_tibble(samples, .name_repair = ~ c(\"chain_1\", \"chain_2\", \"chain_3\", \"chain_4\")) %>% \n    add_column(sample = 1:n_steps) %>%\n    gather(key = \"chain\", value = \"value\", -sample)\n}\n\nplot_trace <- function(samples) {\n  prep_tibble(samples) %>% \n    ggplot(aes(x = sample, y = value, color = chain)) +\n    geom_line() + \n    theme_light() +\n    theme(legend.position = \"none\",\n          axis.title = element_blank(),\n          axis.text = element_blank(),\n          axis.ticks = element_blank())\n}\n\nplot_traces <- function(sample_array, num_params) {\n  plots <- purrr::map(1:num_params, ~ plot_trace(sample_array[ , , .x]))\n  do.call(grid.arrange, plots)\n}\n\nplot_traces(samples, 49)\n\n\n\n\nAwesome! (The first two parameters of rho, the Cholesky factor of the correlation matrix, need to stay fixed at 1 and 0, respectively.)\nNow, on to some summary statistics on the posteriors of the parameters.\nParameters\nLike last time, we display posterior means and standard deviations, as well as the highest posterior density interval (HPDI). We add effective sample sizes and rhat values.\n\n\ncolumn_names <- c(\n  paste0(\"rho_\", 1:4),\n  \"sigma\",\n  paste0(\"sigma_cafe_\", 1:2),\n  \"b\",\n  \"a\",\n  c(rbind(paste0(\"a_cafe_\", 1:20), paste0(\"b_cafe_\", 1:20)))\n)\n\nall_samples <- matrix(samples, nrow = n_steps * n_chains, ncol = 49)\nall_samples <- all_samples %>%\n  as_tibble(.name_repair = ~ column_names)\n\nall_samples %>% glimpse()\n\nmeans <- all_samples %>% \n  summarise_all(list (mean)) %>% \n  gather(key = \"key\", value = \"mean\")\n\nsds <- all_samples %>% \n  summarise_all(list (sd)) %>% \n  gather(key = \"key\", value = \"sd\")\n\nhpdis <-\n  all_samples %>%\n  summarise_all(list(~ list(hdi(.) %>% t() %>% as_tibble()))) %>% \n   unnest() \n \n hpdis_lower <- hpdis %>% select(-contains(\"upper\")) %>%\n   rename(lower0 = lower) %>%\n   gather(key = \"key\", value = \"lower\") %>% \n   arrange(as.integer(str_sub(key, 6))) %>%\n   mutate(key = column_names)\n \n hpdis_upper <- hpdis %>% select(-contains(\"lower\")) %>%\n   rename(upper0 = upper) %>%\n   gather(key = \"key\", value = \"upper\") %>% \n   arrange(as.integer(str_sub(key, 6))) %>%\n   mutate(key = column_names)\n\nsummary <- means %>% \n  inner_join(sds, by = \"key\") %>% \n  inner_join(hpdis_lower, by = \"key\") %>%\n  inner_join(hpdis_upper, by = \"key\")\n\ness <- apply(ess, 2, mean)\n\nsummary_with_diag <- summary %>% add_column(ess = ess, rhat = rhat)\nprint(summary_with_diag, n = 49)\n\n\n# A tibble: 49 x 7\n   key            mean     sd  lower   upper   ess   rhat\n   <chr>         <dbl>  <dbl>  <dbl>   <dbl> <dbl>  <dbl>\n 1 rho_1         1     0       1      1        NaN    NaN   \n 2 rho_2         0     0       0      0       NaN     NaN   \n 3 rho_3        -0.517 0.176  -0.831 -0.195   42.4   1.01\n 4 rho_4         0.832 0.103   0.644  1.000   46.5   1.02\n 5 sigma         0.473 0.0264  0.420  0.523  424.    1.00\n 6 sigma_cafe_1  0.967 0.163   0.694  1.29    97.9   1.00\n 7 sigma_cafe_2  0.607 0.129   0.386  0.861   42.3   1.03\n 8 b            -1.14  0.141  -1.43  -0.864   95.1   1.00\n 9 a             3.66  0.218   3.22   4.07    75.3   1.01\n10 a_cafe_1      4.20  0.192   3.83   4.57    83.9   1.01\n11 b_cafe_1     -1.13  0.251  -1.63  -0.664   63.6   1.02\n12 a_cafe_2      2.17  0.195   1.79   2.54    59.3   1.01\n13 b_cafe_2     -0.923 0.260  -1.42  -0.388   46.0   1.01\n14 a_cafe_3      4.40  0.195   4.02   4.79    56.7   1.01\n15 b_cafe_3     -1.97  0.258  -2.52  -1.51    43.9   1.01\n16 a_cafe_4      3.22  0.199   2.80   3.57    58.7   1.02\n17 b_cafe_4     -1.20  0.254  -1.70  -0.713   36.3   1.01\n18 a_cafe_5      1.86  0.197   1.45   2.20    52.8   1.03\n19 b_cafe_5     -0.113 0.263  -0.615  0.390   34.6   1.04\n20 a_cafe_6      4.26  0.210   3.87   4.67    43.4   1.02\n21 b_cafe_6     -1.30  0.277  -1.80  -0.713   41.4   1.05\n22 a_cafe_7      3.61  0.198   3.23   3.98    44.9   1.01\n23 b_cafe_7     -1.02  0.263  -1.51  -0.489   37.7   1.03\n24 a_cafe_8      3.95  0.189   3.59   4.31    73.1   1.01\n25 b_cafe_8     -1.64  0.248  -2.10  -1.13    60.7   1.02\n26 a_cafe_9      3.98  0.212   3.57   4.37    76.3   1.03\n27 b_cafe_9     -1.29  0.273  -1.83  -0.776   57.8   1.05\n28 a_cafe_10     3.60  0.187   3.24   3.96   104.    1.01\n29 b_cafe_10    -1.00  0.245  -1.47  -0.512   70.4   1.00\n30 a_cafe_11     1.95  0.200   1.56   2.35    55.9   1.03\n31 b_cafe_11    -0.449 0.266  -1.00   0.0619  42.5   1.04\n32 a_cafe_12     3.84  0.195   3.46   4.22    76.0   1.02\n33 b_cafe_12    -1.17  0.259  -1.65  -0.670   62.5   1.03\n34 a_cafe_13     3.88  0.201   3.50   4.29    62.2   1.02\n35 b_cafe_13    -1.81  0.270  -2.30  -1.29    48.3   1.03\n36 a_cafe_14     3.19  0.212   2.82   3.61    65.9   1.07\n37 b_cafe_14    -0.961 0.278  -1.49  -0.401   49.9   1.06\n38 a_cafe_15     4.46  0.212   4.08   4.91    62.0   1.09\n39 b_cafe_15    -2.20  0.290  -2.72  -1.59    47.8   1.11\n40 a_cafe_16     3.41  0.193   3.02   3.78    62.7   1.02\n41 b_cafe_16    -1.07  0.253  -1.54  -0.567   48.5   1.05\n42 a_cafe_17     4.22  0.201   3.82   4.60    58.7   1.01\n43 b_cafe_17    -1.24  0.273  -1.74  -0.703   43.8   1.01\n44 a_cafe_18     5.77  0.210   5.34   6.18    66.0   1.02\n45 b_cafe_18    -1.05  0.284  -1.61  -0.511   49.8   1.02\n46 a_cafe_19     3.23  0.203   2.88   3.65    52.7   1.02\n47 b_cafe_19    -0.232 0.276  -0.808  0.243   45.2   1.01\n48 a_cafe_20     3.74  0.212   3.35   4.21    48.2   1.04\n49 b_cafe_20    -1.09  0.281  -1.58  -0.506   36.5   1.05\nSo what do we have? If you run this “live”, for the rows a_cafe_n resp. b_cafe_n, you see a nice alternation of white and red coloring: For all cafés, the inferred slopes are negative.\nThe inferred slope prior (b) is around -1.14, which is not too far off from the value we used for sampling: 1.\nThe rho posterior estimates, admittedly, are less useful unless you are accustomed to compose Cholesky factors in your head. We compute the resulting posterior correlations and their mean:\n\n\nrhos <- all_samples[ , 1:4] %>% tibble()\n\nrhos <- rhos %>%\n  apply(1, list) %>%\n  unlist(recursive = FALSE) %>%\n  lapply(function(x) matrix(x, byrow = TRUE, nrow = 2) %>% tcrossprod())\n\nrho <- rhos %>% purrr::map(~ .x[1,2]) %>% unlist()\n\nmean_rho <- mean(rho)\nmean_rho\n\n\n-0.5166775\nThe value we used for sampling was -0.7, so we see the regularization effect. In case you’re wondering, for the same data Stan yields an estimate of -0.5.\nFinally, let’s display equivalents to McElreath’s figures illustrating shrinkage on the parameter (café-specific intercepts and slopes) as well as the outcome (morning resp. afternoon waiting times) scales.\nShrinkage\nAs expected, we see that the individual intercepts and slopes are pulled towards the mean – the more, the further away they are from the center.\n\n\n# just like McElreath, compute unpooled estimates directly from data\na_empirical <- d %>% \n  filter(afternoon == 0) %>%\n  group_by(cafe) %>% \n  summarise(a = mean(wait)) %>%\n  select(a)\n\nb_empirical <- d %>% \n  filter(afternoon == 1) %>%\n  group_by(cafe) %>% \n  summarise(b = mean(wait)) %>%\n  select(b) -\n  a_empirical\n\nempirical_estimates <- bind_cols(\n  a_empirical,\n  b_empirical,\n  type = rep(\"data\", 20))\n\nposterior_estimates <- tibble(\n  a = means %>% filter(\n  str_detect(key, \"^a_cafe\")) %>% select(mean) %>% pull(),\n  b = means %>% filter(\n    str_detect(key, \"^b_cafe\")) %>% select(mean)  %>% pull(),\n  type = rep(\"posterior\", 20))\n  \nall_estimates <- bind_rows(empirical_estimates, posterior_estimates)\n\n# compute posterior mean bivariate Gaussian\n# again following McElreath\nmu_est <- c(means[means$key == \"a\", 2], means[means$key == \"b\", 2]) %>% unlist()\nrho_est <- mean_rho\nsa_est <- means[means$key == \"sigma_cafe_1\", 2] %>% unlist()\nsb_est <- means[means$key == \"sigma_cafe_2\", 2] %>% unlist()\ncov_ab <- sa_est * sb_est * rho_est\nsigma_est <- matrix(c(sa_est^2, cov_ab, cov_ab, sb_est^2), ncol=2) \n\nalpha_levels <- c(0.1, 0.3, 0.5, 0.8, 0.99)\nnames(alpha_levels) <- alpha_levels\n\ncontour_data <- plyr::ldply(\n  alpha_levels,\n  ellipse,\n  x = sigma_est,\n  scale = c(1, 1),\n  centre = mu_est\n)\n\nggplot() +\n  geom_point(data = all_estimates, mapping = aes(x = a, y = b, color = type)) + \n  geom_path(data = contour_data, mapping = aes(x = x, y = y, group = .id))\n\n\n\n\nThe same behavior is visible on the outcome scale.\n\n\nwait_times  <- all_estimates %>%\n  mutate(morning = a, afternoon = a + b)\n\n# simulate from posterior means\nv <- MASS::mvrnorm(1e4 , mu_est , sigma_est)\nv[ ,2] <- v[ ,1] + v[ ,2] # calculate afternoon wait\n# construct empirical covariance matrix\nsigma_est2 <- cov(v)\nmu_est2 <- mu_est\nmu_est2[2] <- mu_est[1] + mu_est[2]\n\ncontour_data <- plyr::ldply(\n  alpha_levels,\n  ellipse,\n  x = sigma_est2 %>% unname(),\n  scale = c(1, 1),\n  centre = mu_est2\n)\n\nggplot() +\n  geom_point(data = wait_times, mapping = aes(x = morning, y = afternoon, color = type)) + \n  geom_path(data = contour_data, mapping = aes(x = x, y = y, group = .id))\n\n\n\n\nWrapping up\nBy now, we hope we have convinced you of the power inherent in Bayesian modeling, as well as conveyed some ideas on how this is achievable with TensorFlow Probability. As with every DSL though, it takes time to proceed from understanding worked examples to design your own models. And not just time – it helps to have seen a lot of different models, focusing on different tasks and applications. On this blog, we plan to loosely follow up on Bayesian modeling with TFP, picking up some of the tasks and challenges elaborated on in the later chapters of McElreath’s book. Thanks for reading!\ncf. the Wikipedia article on multilevel models for a collection of terms encountered when dealing with this subject, and e.g. Gelman’s dissection of various ways random effects are defined↩︎\nWe won’t overload this post by explicitly comparing results here, but we did that when writing the code.↩︎\nDisclaimer: We have not verified whether this is an adequate model of the world, but it really doesn’t matter either.↩︎\n",
    "preview": "posts/2019-05-24-varying-slopes/images/thumb.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 509,
    "preview_height": 249
  },
  {
    "path": "posts/2019-05-06-tadpoles-on-tensorflow/",
    "title": "Tadpoles on TensorFlow: Hierarchical partial pooling with tfprobability",
    "description": "This post is a first introduction to MCMC modeling with tfprobability, the R interface to TensorFlow Probability (TFP). Our example is a multi-level model describing tadpole mortality, which may be known to the reader from Richard McElreath's wonderful \"Statistical Rethinking\".",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-05-06",
    "categories": [
      "Bayesian Modeling",
      "TensorFlow/Keras"
    ],
    "contents": "\nBefore we jump into the technicalities: This post is, of course, dedicated to McElreath who wrote one of most intriguing books on Bayesian (or should we just say - scientific?) modeling we’re aware of. If you haven’t read Statistical Rethinking, and are interested in modeling, you might definitely want to check it out. In this post, we’re not going to try to re-tell the story: Our clear focus will, instead, be a demonstration of how to do MCMC with tfprobability.1\nConcretely, this post has two parts. The first is a quick overview of how to use tfd_joint_sequential_distribution to construct a model, and then sample from it using Hamiltonian Monte Carlo. This part can be consulted for quick code look-up, or as a frugal template of the whole process. The second part then walks through a multi-level model in more detail, showing how to extract, post-process and visualize sampling as well as diagnostic outputs.\nReedfrogs\nThe data comes with the rethinking package.\n\n\nlibrary(rethinking)\n\ndata(reedfrogs)\nd <- reedfrogs\nstr(d)\n\n\n'data.frame':   48 obs. of  5 variables:\n $ density : int  10 10 10 10 10 10 10 10 10 10 ...\n $ pred    : Factor w/ 2 levels \"no\",\"pred\": 1 1 1 1 1 1 1 1 2 2 ...\n $ size    : Factor w/ 2 levels \"big\",\"small\": 1 1 1 1 2 2 2 2 1 1 ...\n $ surv    : int  9 10 7 10 9 9 10 9 4 9 ...\n $ propsurv: num  0.9 1 0.7 1 0.9 0.9 1 0.9 0.4 0.9 ...\nThe task is modeling survivor counts among tadpoles, where tadpoles are held in tanks of different sizes (equivalently, different numbers of inhabitants). Each row in the dataset describes one tank, with its initial count of inhabitants (density) and number of survivors (surv). In the technical overview part, we build a simple unpooled model that describes every tank in isolation. Then, in the detailed walk-through, we’ll see how to construct a varying intercepts model that allows for information sharing between tanks.\nConstructing models with tfd_joint_distribution_sequential\ntfd_joint_distribution_sequential represents a model as a list of conditional distributions. This is easiest to see on a real example, so we’ll jump right in, creating an unpooled model of the tadpole data.\nThis is the how the model specification would look in Stan:\n\nmodel{\n    vector[48] p;\n    a ~ normal( 0 , 1.5 );\n    for ( i in 1:48 ) {\n        p[i] = a[tank[i]];\n        p[i] = inv_logit(p[i]);\n    }\n    S ~ binomial( N , p );\n}\nAnd here is tfd_joint_distribution_sequential:\n\n\nlibrary(tensorflow)\n\n# make sure you have at least version 0.7 of TensorFlow Probability \n# as of this writing, it is required of install the master branch:\n# install_tensorflow(version = \"nightly\")\nlibrary(tfprobability)\n\nn_tadpole_tanks <- nrow(d)\nn_surviving <- d$surv\nn_start <- d$density\n\nm1 <- tfd_joint_distribution_sequential(\n  list(\n    # normal prior of per-tank logits\n    tfd_multivariate_normal_diag(\n      loc = rep(0, n_tadpole_tanks),\n      scale_identity_multiplier = 1.5),\n    # binomial distribution of survival counts\n    function(l)\n      tfd_independent(\n        tfd_binomial(total_count = n_start, logits = l),\n        reinterpreted_batch_ndims = 1\n      )\n  )\n)\n\nThe model consists of two distributions: Prior means and variances for the 48 tadpole tanks are specified by tfd_multivariate_normal_diag; then tfd_binomial generates survival counts for each tank. Note how the first distribution is unconditional, while the second depends on the first. Note too how the second has to be wrapped in tfd_independent to avoid wrong broadcasting. (This is an aspect of tfd_joint_distribution_sequential usage that deserves to be documented more systematically, which is surely going to happen.2 Just think that this functionality was added to TFP master only three weeks ago!)\nAs an aside, the model specification here ends up shorter than in Stan as tfd_binomial optionally takes logits as parameters.\nAs with every TFP distribution, you can do a quick functionality check by sampling from the model:3\n\n\n# sample a batch of 2 values \n# we get samples for every distribution in the model\ns <- m1 %>% tfd_sample(2)\n\n\n[[1]]\nTensor(\"MultivariateNormalDiag/sample/affine_linear_operator/forward/add:0\",\nshape=(2, 48), dtype=float32)\n\n[[2]]\nTensor(\"IndependentJointDistributionSequential/sample/Beta/sample/Reshape:0\",\nshape=(2, 48), dtype=float32)\nand computing log probabilities:\n\n\n# we should get only the overall log probability of the model\nm1 %>% tfd_log_prob(s)\n\n\nt[[1]]\nTensor(\"MultivariateNormalDiag/sample/affine_linear_operator/forward/add:0\",\nshape=(2, 48), dtype=float32)\n\n[[2]]\nTensor(\"IndependentJointDistributionSequential/sample/Beta/sample/Reshape:0\",\nshape=(2, 48), dtype=float32)\nNow, let’s see how we can sample from this model using Hamiltonian Monte Carlo.\nRunning Hamiltonian Monte Carlo in TFP\nWe define a Hamiltonian Monte Carlo kernel with dynamic step size adaptation based on a desired acceptance probability.\n\n\n# number of steps to run burnin\nn_burnin <- 500\n\n# optimization target is the likelihood of the logits given the data\nlogprob <- function(l)\n  m1 %>% tfd_log_prob(list(l, n_surviving))\n\nhmc <- mcmc_hamiltonian_monte_carlo(\n  target_log_prob_fn = logprob,\n  num_leapfrog_steps = 3,\n  step_size = 0.1,\n) %>%\n  mcmc_simple_step_size_adaptation(\n    target_accept_prob = 0.8,\n    num_adaptation_steps = n_burnin\n  )\n\nWe then run the sampler, passing in an initial state. If we want to run \\(n\\) chains, that state has to be of length \\(n\\), for every parameter in the model (here we have just one).\nThe sampling function, mcmc_sample_chain, may optionally be passed a trace_fn that tells TFP which kinds of meta information to save. Here we save acceptance ratios and step sizes.\n\n\n# number of steps after burnin\nn_steps <- 500\n# number of chains\nn_chain <- 4\n\n# get starting values for the parameters\n# their shape implicitly determines the number of chains we will run\n# see current_state parameter passed to mcmc_sample_chain below\nc(initial_logits, .) %<-% (m1 %>% tfd_sample(n_chain))\n\n# tell TFP to keep track of acceptance ratio and step size\ntrace_fn <- function(state, pkr) {\n  list(pkr$inner_results$is_accepted,\n       pkr$inner_results$accepted_results$step_size)\n}\n\nres <- hmc %>% mcmc_sample_chain(\n  num_results = n_steps,\n  num_burnin_steps = n_burnin,\n  current_state = initial_logits,\n  trace_fn = trace_fn\n)\n\nWhen sampling is finished, we can access the samples as res$all_states:\n\n\nmcmc_trace <- res$all_states\nmcmc_trace\n\n\nTensor(\"mcmc_sample_chain/trace_scan/TensorArrayStack/TensorArrayGatherV3:0\",\nshape=(500, 4, 48), dtype=float32)\nThis is the shape of the samples for l, the 48 per-tank logits: 500 samples times 4 chains times 48 parameters.\nFrom these samples, we can compute effective sample size and \\(rhat\\) (alias mcmc_potential_scale_reduction):\n\n\n# Tensor(\"Mean:0\", shape=(48,), dtype=float32)\ness <- mcmc_effective_sample_size(mcmc_trace) %>% tf$reduce_mean(axis = 0L)\n\n# Tensor(\"potential_scale_reduction/potential_scale_reduction_single_state/sub_1:0\", shape=(48,), dtype=float32)\nrhat <- mcmc_potential_scale_reduction(mcmc_trace)\n\nWhereas diagnostic information is available in res$trace:\n\n\n# Tensor(\"mcmc_sample_chain/trace_scan/TensorArrayStack_1/TensorArrayGatherV3:0\",\n# shape=(500, 4), dtype=bool)\nis_accepted <- res$trace[[1]] \n\n# Tensor(\"mcmc_sample_chain/trace_scan/TensorArrayStack_2/TensorArrayGatherV3:0\",\n# shape=(500,), dtype=float32)\nstep_size <- res$trace[[2]] \n\nAfter this quick outline, let’s move on to the topic promised in the title: multi-level modeling, or partial pooling. This time, we’ll also take a closer look at sampling results and diagnostic outputs.\nMulti-level tadpoles 4\nThe multi-level model – or varying intercepts model, in this case: we’ll get to varying slopes in a later post – adds a hyperprior to the model. Instead of deciding on a mean and variance of the normal prior the logits are drawn from, we let the model learn means and variances for individual tanks. These per-tank means, while being priors for the binomial logits, are assumed to be normally distributed, and are themselves regularized by a normal prior for the mean and an exponential prior for the variance.\nFor the Stan-savvy, here is the Stan formulation of this model.\n\n\nmodel{\n    vector[48] p;\n    sigma ~ exponential( 1 );\n    a_bar ~ normal( 0 , 1.5 );\n    a ~ normal( a_bar , sigma );\n    for ( i in 1:48 ) {\n        p[i] = a[tank[i]];\n        p[i] = inv_logit(p[i]);\n    }\n    S ~ binomial( N , p );\n}\n\nAnd here it is with TFP:\n\n\nm2 <- tfd_joint_distribution_sequential(\n  list(\n    # a_bar, the prior for the mean of the normal distribution of per-tank logits\n    tfd_normal(loc = 0, scale = 1.5),\n    # sigma, the prior for the variance of the normal distribution of per-tank logits\n    tfd_exponential(rate = 1),\n    # normal distribution of per-tank logits\n    # parameters sigma and a_bar refer to the outputs of the above two distributions\n    function(sigma, a_bar) \n      tfd_sample_distribution(\n        tfd_normal(loc = a_bar, scale = sigma),\n        sample_shape = list(n_tadpole_tanks)\n      ), \n    # binomial distribution of survival counts\n    # parameter l refers to the output of the normal distribution immediately above\n    function(l)\n      tfd_independent(\n        tfd_binomial(total_count = n_start, logits = l),\n        reinterpreted_batch_ndims = 1\n      )\n  )\n)\n\nTechnically, dependencies in tfd_joint_distribution_sequential are defined via spatial proximity in the list: In the learned prior for the logits\n\n\nfunction(sigma, a_bar) \n      tfd_sample_distribution(\n        tfd_normal(loc = a_bar, scale = sigma),\n        sample_shape = list(n_tadpole_tanks)\n      )\n\nsigma refers to the distribution immediately above, and a_bar to the one above that.\nAnalogously, in the distribution of survival counts\n\n\nfunction(l)\n      tfd_independent(\n        tfd_binomial(total_count = n_start, logits = l),\n        reinterpreted_batch_ndims = 1\n      )\n\nl refers to the distribution immediately preceding its own definition.\nAgain, let’s sample from this model to see if shapes are correct.\n\n\ns <- m2 %>% tfd_sample(2)\ns \n\nThey are.\n\n[[1]]\nTensor(\"Normal/sample_1/Reshape:0\", shape=(2,), dtype=float32)\n\n[[2]]\nTensor(\"Exponential/sample_1/Reshape:0\", shape=(2,), dtype=float32)\n\n[[3]]\nTensor(\"SampleJointDistributionSequential/sample_1/Normal/sample/Reshape:0\",\nshape=(2, 48), dtype=float32)\n\n[[4]]\nTensor(\"IndependentJointDistributionSequential/sample_1/Beta/sample/Reshape:0\",\nshape=(2, 48), dtype=float32)\nAnd to make sure we get one overall log_prob per batch:\n\n\nm2 %>% tfd_log_prob(s)\n\n\nTensor(\"JointDistributionSequential/log_prob/add_3:0\", shape=(2,), dtype=float32)\nTraining this model works like before, except that now the initial state comprises three parameters, a_bar, sigma and l:\n\n\nc(initial_a, initial_s, initial_logits, .) %<-% (m2 %>% tfd_sample(n_chain))\n\nHere is the sampling routine:\n\n\n# the joint log probability now is based on three parameters\nlogprob <- function(a, s, l)\n  m2 %>% tfd_log_prob(list(a, s, l, n_surviving))\n\nhmc <- mcmc_hamiltonian_monte_carlo(\n  target_log_prob_fn = logprob,\n  num_leapfrog_steps = 3,\n  # one step size for each parameter\n  step_size = list(0.1, 0.1, 0.1),\n) %>%\n  mcmc_simple_step_size_adaptation(target_accept_prob = 0.8,\n                                   num_adaptation_steps = n_burnin)\n\nrun_mcmc <- function(kernel) {\n  kernel %>% mcmc_sample_chain(\n    num_results = n_steps,\n    num_burnin_steps = n_burnin,\n    current_state = list(initial_a, tf$ones_like(initial_s), initial_logits),\n    trace_fn = trace_fn\n  )\n}\n\nres <- hmc %>% run_mcmc()\n \nmcmc_trace <- res$all_states\n\nThis time, mcmc_trace is a list of three: We have\n\n[[1]]\nTensor(\"mcmc_sample_chain/trace_scan/TensorArrayStack/TensorArrayGatherV3:0\",\nshape=(500, 4), dtype=float32)\n\n[[2]]\nTensor(\"mcmc_sample_chain/trace_scan/TensorArrayStack_1/TensorArrayGatherV3:0\",\nshape=(500, 4), dtype=float32)\n\n[[3]]\nTensor(\"mcmc_sample_chain/trace_scan/TensorArrayStack_2/TensorArrayGatherV3:0\",\nshape=(500, 4, 48), dtype=float32)\nNow let’s create graph nodes for the results and information we’re interested in.\n\n\n# as above, this is the raw result\nmcmc_trace_ <- res$all_states\n\n# we perform some reshaping operations directly in tensorflow\nall_samples_ <-\n  tf$concat(\n    list(\n      mcmc_trace_[[1]] %>% tf$expand_dims(axis = -1L),\n      mcmc_trace_[[2]]  %>% tf$expand_dims(axis = -1L),\n      mcmc_trace_[[3]]\n    ),\n    axis = -1L\n  ) %>%\n  tf$reshape(list(2000L, 50L))\n\n# diagnostics, also as above\nis_accepted_ <- res$trace[[1]]\nstep_size_ <- res$trace[[2]]\n\n# effective sample size\n# again we use tensorflow to get conveniently shaped outputs\ness_ <- mcmc_effective_sample_size(mcmc_trace) \ness_ <- tf$concat(\n  list(\n    ess_[[1]] %>% tf$expand_dims(axis = -1L),\n    ess_[[2]]  %>% tf$expand_dims(axis = -1L),\n    ess_[[3]]\n  ),\n  axis = -1L\n) \n\n# rhat, conveniently post-processed\nrhat_ <- mcmc_potential_scale_reduction(mcmc_trace)\nrhat_ <- tf$concat(\n  list(\n    rhat_[[1]] %>% tf$expand_dims(axis = -1L),\n    rhat_[[2]]  %>% tf$expand_dims(axis = -1L),\n    rhat_[[3]]\n  ),\n  axis = -1L\n) \n\nAnd we’re ready to actually run the chains.\n\n\n# so far, no sampling has been done!\n# the actual sampling happens when we create a Session \n# and run the above-defined nodes\nsess <- tf$Session()\neval <- function(...) sess$run(list(...))\n\nc(mcmc_trace, all_samples, is_accepted, step_size, ess, rhat) %<-%\n  eval(mcmc_trace_, all_samples_, is_accepted_, step_size_, ess_, rhat_)\n\nThis time, let’s actually inspect those results.\nMulti-level tadpoles: Results\nFirst, how do the chains behave?\nTrace plots\nExtract the samples for a_bar and sigma, as well as one of the learned priors for the logits:\n\n\na_bar <- mcmc_trace[[1]] %>% as.matrix()\nsigma <- mcmc_trace[[2]] %>% as.matrix()\na_1 <- mcmc_trace[[3]][ , , 1] %>% as.matrix()\n\nHere’s a trace plot for a_bar:\n\n\nprep_tibble <- function(samples) {\n  as_tibble(samples, .name_repair = ~ c(\"chain_1\", \"chain_2\", \"chain_3\", \"chain_4\")) %>% \n    add_column(sample = 1:500) %>%\n    gather(key = \"chain\", value = \"value\", -sample)\n}\n\nplot_trace <- function(samples, param_name) {\n  prep_tibble(samples) %>% \n    ggplot(aes(x = sample, y = value, color = chain)) +\n    geom_line() + \n    ggtitle(param_name)\n}\n\nplot_trace(a_bar, \"a_bar\")\n\n\n\n\nAnd here for sigma and a_1:\n\n\n\n\n\n\nHow about the posterior distributions of the parameters, first and foremost, the varying intercepts a_1 … a_48?\nPosterior distributions\n\n\nplot_posterior <- function(samples) {\n  prep_tibble(samples) %>% \n    ggplot(aes(x = value, color = chain)) +\n    geom_density() +\n    theme_classic() +\n    theme(legend.position = \"none\",\n          axis.title = element_blank(),\n          axis.text = element_blank(),\n          axis.ticks = element_blank())\n    \n}\n\nplot_posteriors <- function(sample_array, num_params) {\n  plots <- purrr::map(1:num_params, ~ plot_posterior(sample_array[ , , .x] %>% as.matrix()))\n  do.call(grid.arrange, plots)\n}\n\nplot_posteriors(mcmc_trace[[3]], dim(mcmc_trace[[3]])[3])\n\n\n\n\nNow let’s see the corresponding posterior means and highest posterior density intervals. (The below code includes the hyperpriors in summary as we’ll want to display a complete precis-like output soon.)\nPosterior means and HPDIs\n\n\nall_samples <- all_samples %>%\n  as_tibble(.name_repair = ~ c(\"a_bar\", \"sigma\", paste0(\"a_\", 1:48))) \n\nmeans <- all_samples %>% \n  summarise_all(list (~ mean)) %>% \n  gather(key = \"key\", value = \"mean\")\n\nsds <- all_samples %>% \n  summarise_all(list (~ sd)) %>% \n  gather(key = \"key\", value = \"sd\")\n\nhpdis <-\n  all_samples %>%\n  summarise_all(list(~ list(hdi(.) %>% t() %>% as_tibble()))) %>% \n  unnest() \n\nhpdis_lower <- hpdis %>% select(-contains(\"upper\")) %>%\n  rename(lower0 = lower) %>%\n  gather(key = \"key\", value = \"lower\") %>% \n  arrange(as.integer(str_sub(key, 6))) %>%\n  mutate(key = c(\"a_bar\", \"sigma\", paste0(\"a_\", 1:48)))\n\nhpdis_upper <- hpdis %>% select(-contains(\"lower\")) %>%\n  rename(upper0 = upper) %>%\n  gather(key = \"key\", value = \"upper\") %>% \n  arrange(as.integer(str_sub(key, 6))) %>%\n  mutate(key = c(\"a_bar\", \"sigma\", paste0(\"a_\", 1:48)))\n\nsummary <- means %>% \n  inner_join(sds, by = \"key\") %>% \n  inner_join(hpdis_lower, by = \"key\") %>%\n  inner_join(hpdis_upper, by = \"key\")\n\n\nsummary %>% \n  filter(!key %in% c(\"a_bar\", \"sigma\")) %>%\n  mutate(key_fct = factor(key, levels = unique(key))) %>%\n  ggplot(aes(x = key_fct, y = mean, ymin = lower, ymax = upper)) +\n   geom_pointrange() + \n   coord_flip() +  \n   xlab(\"\") + ylab(\"post. mean and HPDI\") +\n   theme_minimal() \n\n\n\n\nNow for an equivalent to precis. We already computed means, standard deviations and the HPDI interval. Let’s add n_eff, the effective number of samples, and rhat, the Gelman-Rubin statistic.\nComprehensive summary (a.k.a. “precis”)\n\n\nis_accepted <- is_accepted %>% as.integer() %>% mean()\nstep_size <- purrr::map(step_size, mean)\n\ness <- apply(ess, 2, mean)\n\nsummary_with_diag <- summary %>% add_column(ess = ess, rhat = rhat)\nsummary_with_diag\n\n\n# A tibble: 50 x 7\n   key    mean    sd  lower upper   ess  rhat\n   <chr> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl>\n 1 a_bar  1.35 0.266  0.792  1.87 405.   1.00\n 2 sigma  1.64 0.218  1.23   2.05  83.6  1.00\n 3 a_1    2.14 0.887  0.451  3.92  33.5  1.04\n 4 a_2    3.16 1.13   1.09   5.48  23.7  1.03\n 5 a_3    1.01 0.698 -0.333  2.31  65.2  1.02\n 6 a_4    3.02 1.04   1.06   5.05  31.1  1.03\n 7 a_5    2.11 0.843  0.625  3.88  49.0  1.05\n 8 a_6    2.06 0.904  0.496  3.87  39.8  1.03\n 9 a_7    3.20 1.27   1.11   6.12  14.2  1.02\n10 a_8    2.21 0.894  0.623  4.18  44.7  1.04\n# ... with 40 more rows\nFor the varying intercepts, effective sample sizes are pretty low, indicating we might want to investigate possible reasons.\nLet’s also display posterior survival probabilities, analogously to figure 13.2 in the book.\nPosterior survival probabilities\n\n\nsim_tanks <- rnorm(8000, a_bar, sigma)\ntibble(x = sim_tanks) %>% ggplot(aes(x = x)) + geom_density() + xlab(\"distribution of per-tank logits\")\n\n\n\n\n\n\n# our usual sigmoid by another name (undo the logit)\nlogistic <- function(x) 1/(1 + exp(-x))\nprobs <- map_dbl(sim_tanks, logistic)\ntibble(x = probs) %>% ggplot(aes(x = x)) + geom_density() + xlab(\"probability of survival\")\n\n\n\n\nFinally, we want to make sure we see the shrinkage behavior displayed in figure 13.1 in the book.\nShrinkage\n\n\nsummary %>% \n  filter(!key %in% c(\"a_bar\", \"sigma\")) %>%\n  select(key, mean) %>%\n  mutate(est_survival = logistic(mean)) %>%\n  add_column(act_survival = d$propsurv) %>%\n  select(-mean) %>%\n  gather(key = \"type\", value = \"value\", -key) %>%\n  ggplot(aes(x = key, y = value, color = type)) +\n  geom_point() +\n  geom_hline(yintercept = mean(d$propsurv), size = 0.5, color = \"cyan\" ) +\n  xlab(\"\") +\n  ylab(\"\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank())\n\n\n\n\nWe see results similar in spirit to McElreath’s: estimates are shrunken to the mean (the cyan-colored line). Also, shrinkage seems to be more active in smaller tanks, which are the lower-numbered ones on the left of the plot.\nOutlook\nIn this post, we saw how to construct a varying intercepts model with tfprobability, as well as how to extract sampling results and relevant diagnostics. In an upcoming post, we’ll move on to varying slopes. With non-negligible probability, our example will build on one of Mc Elreath’s again… Thanks for reading!\nFor a supplementary introduction to Bayesian modeling focusing on complete coverage, yet starting from the very beginning, you might want to consult Ben Lambert’s Student’s Guide to Bayesian Statistics.↩︎\nAs of today, lots of useful information is available in Modeling with JointDistribution and Multilevel Modeling Primer, but some experimentation may needed to adapt the – numerous! – examples to your needs.↩︎\nUpdated footnote, as of May 13th: When this post was written, we were still experimenting with the use of tf.function from R, so it seemed safest to code the complete example in graph mode. The next post on MCMC will use eager execution, and show how to achieve good performance by placing the actual sampling procedure on the graph.↩︎\nyep, it’s a quote↩︎\n",
    "preview": "posts/2019-05-06-tadpoles-on-tensorflow/images/thumb.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 1612,
    "preview_height": 659
  },
  {
    "path": "posts/2019-04-24-autoregressive-flows/",
    "title": "Experimenting with autoregressive flows in TensorFlow Probability",
    "description": "Continuing from the recent introduction to bijectors in TensorFlow Probability (TFP), this post brings autoregressivity to the table. Using TFP through the new R package tfprobability, we look at the implementation of masked autoregressive flows (MAF) and put them to use on two different datasets.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-04-24",
    "categories": [
      "Probabilistic ML/DL",
      "Unsupervised Learning",
      "TensorFlow/Keras"
    ],
    "contents": "\nIn the first part of this mini-series on autoregressive flow models, we looked at bijectors in TensorFlow Probability (TFP), and saw how to use them for sampling and density estimation. We singled out the affine bijector to demonstrate the mechanics of flow construction: We start from a distribution that is easy to sample from, and that allows for straightforward calculation of its density. Then, we attach some number of invertible transformations, optimizing for data-likelihood under the final transformed distribution. The efficiency of that (log)likelihood calculation is where normalizing flows excel: Loglikelihood under the (unknown) target distribution is obtained as a sum of the density under the base distribution of the inverse-transformed data plus the absolute log determinant of the inverse Jacobian.\nNow, an affine flow will seldom be powerful enough to model nonlinear, complex transformations. In constrast, autoregressive models have shown substantive success in density estimation as well as sample generation. Combined with more involved architectures, feature engineering, and extensive compute, the concept of autoregressivity has powered – and is powering – state-of-the-art architectures in areas such as image, speech and video modeling.\nThis post will be concerned with the building blocks of autoregressive flows in TFP. While we won’t exactly be building state-of-the-art models, we’ll try to understand and play with some major ingredients, hopefully enabling the reader to do her own experiments on her own data.\nThis post has three parts: First, we’ll look at autoregressivity and its implementation in TFP. Then, we try to (approximately) reproduce one of the experiments in the “MAF paper” (Masked Autoregressive Flows for Distribution Estimation (Papamakarios, Pavlakou, and Murray 2017)) – essentially a proof of concept. Finally, for the third time on this blog, we come back to the task of analysing audio data, with mixed results.\nAutoregressivity and masking\nIn distribution estimation, autoregressivity enters the scene via the chain rule of probability that decomposes a joint density into a product of conditional densities:\n\\[\np(\\mathbf{x}) = \\prod_{i}p(\\mathbf{x}_i|\\mathbf{x}_{1:i−1})\n\\]\nIn practice, this means that autoregressive models have to impose an order on the variables - an order which might or might not “make sense”. Approaches here include choosing orderings at random and/or using different orderings for each layer. While in recurrent neural networks, autoregressivity is conserved due to the recurrence relation inherent in state updating, it is not clear a priori how autoregressivity is to be achieved in a densely connected architecture. A computationally efficient solution was proposed in MADE: Masked Autoencoder for Distribution Estimation(Germain et al. 2015): Starting from a densely connected layer, mask out all connections that should not be allowed, i.e., all connections from input feature \\(i\\) to said layer’s activations \\(1 ... i-1\\). Or expressed differently, activation \\(i\\) may be connected to input features \\(1 ... i-1\\) only. Then when adding more layers, care must be taken to ensure that all required connections are masked so that at the end, output \\(i\\) will only ever have seen inputs \\(1 ... i-1\\).\nThus masked autoregressive flows are a fusion of two major approaches - autoregressive models (which need not be flows) and flows (which need not be autoregressive). In TFP these are provided by MaskedAutoregressiveFlow,1 to be used as a bijector in a TransformedDistribution.2\nWhile the documentation shows how to use this bijector, the step from theoretical understanding to coding a “black box” may seem wide. If you’re anything like the author, here you might feel the urge to “look under the hood” and verify that things really are the way you’re assuming. So let’s give in to curiosity and allow ourselves a little escapade into the source code.\nPeeking ahead, this is how we’ll construct a masked autoregressive flow in TFP (again using the still new-ish R bindings provided by tfprobability):\n\n\nlibrary(tfprobability)\n\nmaf <- tfb_masked_autoregressive_flow(\n    shift_and_log_scale_fn = tfb_masked_autoregressive_default_template(\n      hidden_layers = list(num_hidden, num_hidden),\n      activation = tf$nn$tanh)\n)\n\nPulling apart the relevant entities here, tfb_masked_autoregressive_flow is a bijector, with the usual methods tfb_forward(), tfb_inverse(), tfb_forward_log_det_jacobian() and tfb_inverse_log_det_jacobian(). The default shift_and_log_scale_fn, tfb_masked_autoregressive_default_template, constructs a little neural network of its own, with a configurable number of hidden units per layer, a configurable activation function and optionally, other configurable parameters to be passed to the underlying dense layers. It’s these dense layers that have to respect the autoregressive property. Can we take a look at how this is done? Yes we can, provided we’re not afraid of a little Python.\nmasked_autoregressive_default_template (now leaving out the tfb_ as we’ve entered Python-land) uses masked_dense to do what you’d suppose a thus-named function might be doing: construct a dense layer that has part of the weight matrix masked out. How? We’ll see after a few Python setup statements.\n\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_probability as tfp\ntfd = tfp.distributions\ntfb = tfp.bijectors\ntf.enable_eager_execution()\n\nThe following code snippets are taken from masked_dense (in its current form on master), and when possible, simplified for better readability, accommodating just the specifics of the chosen example - a toy matrix of shape 2x3:\n\n\n# construct some toy input data (this line obviously not from the original code)\ninputs = tf.constant(np.arange(1.,7), shape = (2, 3))\n\n# (partly) determine shape of mask from shape of input\ninput_depth = tf.compat.dimension_value(inputs.shape.with_rank_at_least(1)[-1])\nnum_blocks = input_depth\nnum_blocks # 3\n\nOur toy layer should have 4 units:\n\n\nunits = 4\n\nThe mask is initialized to all zeros. Considering it will be used to elementwise multiply the weight matrix, we’re a bit surprised at its shape (shouldn’t it be the other way round?). No worries; all will turn out correct in the end.\n\n\nmask = np.zeros([units, input_depth], dtype=tf.float32.as_numpy_dtype())\nmask\n\n\narray([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]], dtype=float32)\nNow to “whitelist” the allowed connections, we have to fill in ones whenever information flow is allowed by the autoregressive property:\n\n\ndef _gen_slices(num_blocks, n_in, n_out):\n  slices = []\n  col = 0\n  d_in = n_in // num_blocks\n  d_out = n_out // num_blocks\n  row = d_out \n  for _ in range(num_blocks):\n    row_slice = slice(row, None)\n    col_slice = slice(col, col + d_in)\n    slices.append([row_slice, col_slice])\n    col += d_in\n    row += d_out\n  return slices\n\nslices = _gen_slices(num_blocks, input_depth, units)\nfor [row_slice, col_slice] in slices:\n  mask[row_slice, col_slice] = 1\n\nmask\n\n\narray([[0., 0., 0.],\n       [1., 0., 0.],\n       [1., 1., 0.],\n       [1., 1., 1.]], dtype=float32)\nAgain, does this look mirror-inverted? A transpose fixes shape and logic both:\n\n\nmask = mask.t\nmask\n\n\narray([[0., 1., 1., 1.],\n       [0., 0., 1., 1.],\n       [0., 0., 0., 1.]], dtype=float32)\nNow that we have the mask, we can create the layer (interestingly, as of this writing not (yet?) a tf.keras layer):\n\n\nlayer = tf.compat.v1.layers.Dense(\n        units,\n        kernel_initializer=masked_initializer, # 1\n        kernel_constraint=lambda x: mask * x   # 2\n        )\n\nHere we see masking going on in two ways. For one, the weight initializer is masked:\n\n\nkernel_initializer = tf.compat.v1.glorot_normal_initializer()\n\ndef masked_initializer(shape, dtype=None, partition_info=None):\n return mask * kernel_initializer(shape, dtype, partition_info)\n\nAnd secondly, a kernel constraint makes sure that after optimization, the relative units are zeroed out again:\n\n\nkernel_constraint=lambda x: mask * x \n\nJust for fun, let’s apply the layer to our toy input:\n\n\nlayer.apply(inputs)\n\n\n<tf.Tensor: id=30, shape=(2, 4), dtype=float64, numpy=\narray([[ 0.        , -0.7489589 , -0.43329933,  1.42710014],\n       [ 0.        , -2.9958356 , -1.71647246,  1.09258015]])>\nZeroes where expected. And double-checking on the weight matrix…\n\n\nlayer.kernel\n\n\n<tf.Variable 'dense/kernel:0' shape=(3, 4) dtype=float64, numpy=\narray([[ 0.        , -0.7489589 , -0.42214942, -0.6473454 ],\n       [-0.        ,  0.        , -0.00557496, -0.46692933],\n       [-0.        , -0.        , -0.        ,  1.00276807]])>\nGood. Now hopefully after this little deep dive, things have become a bit more concrete. Of course in a bigger model, the autoregressive property has to be conserved between layers as well.\nOn to the second topic, application of MAF to a real-world dataset.\nMasked Autoregressive Flow\nThe MAF paper(Papamakarios, Pavlakou, and Murray 2017) applied masked autoregressive flows (as well as single-layer-MADE(Germain et al. 2015) and Real NVP (Dinh, Sohl-Dickstein, and Bengio 2016)) to a number of datasets, including MNIST, CIFAR-10 and several datasets from the UCI Machine Learning Repository.\nWe pick one of the UCI datasets: Gas sensors for home activity monitoring. On this dataset, the MAF authors obtained the best results using a MAF with 10 flows, so this is what we will try.\n\n\n\nFigure 1: Figure from Masked Autoregressive Flow for Density Estimation(Papamakarios, Pavlakou, and Murray 2017)\n\n\n\nCollecting information from the paper, we know that\ndata was included from the file ethylene_CO.txt only;\ndiscrete columns were eliminated, as well as all columns with correlations > .98; and\nthe remaining 8 columns3 were standardised (z-transformed).\nRegarding the neural network architecture, we gather that\neach of the 10 MAF layers was followed by a batchnorm;\nas to feature order, the first MAF layer used the variable order that came with the dataset; then every consecutive layer reversed it;\nspecifically for this dataset and as opposed to all other UCI datasets, tanh was used for activation instead of relu;\nthe Adam optimizer was used, with a learning rate of 1e-4;\nthere were two hidden layers for each MAF, with 100 units each;\ntraining went on until no improvement occurred for 30 consecutive epochs on the validation set; and\nthe base distribution was a multivariate Gaussian.\nThis is all useful information for our attempt to estimate this dataset, but the essential bit is this. In case you knew the dataset already, you might have been wondering how the authors would deal with the dimensionality of the data: It is a time series, and the MADE architecture explored above introduces autoregressivity between features, not time steps. So how is the additional temporal autoregressivity to be handled? The answer is: The time dimension is essentially removed. In the authors’ words,\n\n[…] it is a time series but was treated as if each example were an i.i.d. sample from the marginal distribution.\n\nThis undoubtedly is useful information for our present modeling attempt, but it also tells us something else: We might have to look beyond MADE layers for actual time series modeling.\nNow though let’s look at this example of using MAF for multivariate modeling, with no time or spatial dimension to be taken into account.\nFollowing the hints the authors gave us, this is what we do.\n\n\n# load libraries -------------------------------------------------------------\nlibrary(tensorflow)\nlibrary(tfprobability)\n\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nlibrary(tfdatasets)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(purrr)\nlibrary(caret)\n\n# read data ------------------------------------------------------------------\ndf <- read_table2(\"ethylene_CO.txt\",\n                  skip = 1,\n                  col_names = FALSE)\nglimpse(df)\n\n\nObservations: 4,208,261\nVariables: 19\n$ X1  <dbl> 0.00, 0.01, 0.01, 0.03, 0.04, 0.05, 0.06, 0.07, 0.07, 0.09,...\n$ X2  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n$ X3  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n$ X4  <dbl> -50.85, -49.40, -40.04, -47.14, -33.58, -48.59, -48.27, -47.14,... \n$ X5  <dbl> -1.95, -5.53, -16.09, -10.57, -20.79, -11.54, -9.11, -4.56,...\n$ X6  <dbl> -41.82, -42.78, -27.59, -32.28, -33.25, -36.16, -31.31, -16.57,... \n$ X7  <dbl> 1.30, 0.49, 0.00, 4.40, 6.03, 6.03, 5.37, 4.40, 23.98, 2.77,...\n$ X8  <dbl> -4.07, 3.58, -7.16, -11.22, 3.42, 0.33, -7.97, -2.28, -2.12,...\n$ X9  <dbl> -28.73, -34.55, -42.14, -37.94, -34.22, -29.05, -30.34, -24.35,...\n$ X10 <dbl> -13.49, -9.59, -12.52, -7.16, -14.46, -16.74, -8.62, -13.17,...\n$ X11 <dbl> -3.25, 5.37, -5.86, -1.14, 8.31, -1.14, 7.00, -6.34, -0.81,...\n$ X12 <dbl> 55139.95, 54395.77, 53960.02, 53047.71, 52700.28, 51910.52,...\n$ X13 <dbl> 50669.50, 50046.91, 49299.30, 48907.00, 48330.96, 47609.00,...\n$ X14 <dbl> 9626.26, 9433.20, 9324.40, 9170.64, 9073.64, 8982.88, 8860.51,...\n$ X15 <dbl> 9762.62, 9591.21, 9449.81, 9305.58, 9163.47, 9021.08, 8966.48,...\n$ X16 <dbl> 24544.02, 24137.13, 23628.90, 23101.66, 22689.54, 22159.12,...\n$ X17 <dbl> 21420.68, 20930.33, 20504.94, 20101.42, 19694.07, 19332.57,...\n$ X18 <dbl> 7650.61, 7498.79, 7369.67, 7285.13, 7156.74, 7067.61, 6976.13,...\n$ X19 <dbl> 6928.42, 6800.66, 6697.47, 6578.52, 6468.32, 6385.31, 6300.97,...\n\n\n# we don't know if we'll end up with the same columns as the authors did,\n# but we try (at least we do end up with 8 columns)\ndf <- df[,-(1:3)]\nhc <- findCorrelation(cor(df), cutoff = 0.985)\ndf2 <- df[,-c(hc)]\n\n# scale\ndf2 <- scale(df2)\ndf2\n\n\n# A tibble: 4,208,261 x 8\n      X4     X5     X8    X9    X13    X16    X17   X18\n   <dbl>  <dbl>  <dbl> <dbl>  <dbl>  <dbl>  <dbl> <dbl>\n 1 -50.8  -1.95  -4.07 -28.7 50670. 24544. 21421. 7651.\n 2 -49.4  -5.53   3.58 -34.6 50047. 24137. 20930. 7499.\n 3 -40.0 -16.1   -7.16 -42.1 49299. 23629. 20505. 7370.\n 4 -47.1 -10.6  -11.2  -37.9 48907  23102. 20101. 7285.\n 5 -33.6 -20.8    3.42 -34.2 48331. 22690. 19694. 7157.\n 6 -48.6 -11.5    0.33 -29.0 47609  22159. 19333. 7068.\n 7 -48.3  -9.11  -7.97 -30.3 47047. 21932. 19028. 6976.\n 8 -47.1  -4.56  -2.28 -24.4 46758. 21504. 18780. 6900.\n 9 -42.3  -2.77  -2.12 -27.6 46197. 21125. 18439. 6827.\n10 -44.6   3.58  -0.65 -35.5 45652. 20836. 18209. 6790.\n# … with 4,208,251 more rows\nNow set up the data generation process:\n\n\n# train-test split\nn_rows <- nrow(df2) # 4208261\ntrain_ids <- sample(1:n_rows, 0.5 * n_rows)\nx_train <- df2[train_ids, ]\nx_test <- df2[-train_ids, ]\n\n# create datasets\nbatch_size <- 100\ntrain_dataset <- tf$cast(x_train, tf$float32) %>%\n  tensor_slices_dataset %>%\n  dataset_batch(batch_size)\n\ntest_dataset <- tf$cast(x_test, tf$float32) %>%\n  tensor_slices_dataset %>%\n  dataset_batch(nrow(x_test))\n\nTo construct the flow, the first thing needed is the base distribution.\n\n\nbase_dist <- tfd_multivariate_normal_diag(loc = rep(0, ncol(df2)))\n\nNow for the flow, by default constructed with batchnorm and permutation of feature order.\n\n\nnum_hidden <- 100\ndim <- ncol(df2)\n\nuse_batchnorm <- TRUE\nuse_permute <- TRUE\nnum_mafs <-10\nnum_layers <- 3 * num_mafs\n\nbijectors <- vector(mode = \"list\", length = num_layers)\n\nfor (i in seq(1, num_layers, by = 3)) {\n  maf <- tfb_masked_autoregressive_flow(\n    shift_and_log_scale_fn = tfb_masked_autoregressive_default_template(\n      hidden_layers = list(num_hidden, num_hidden),\n      activation = tf$nn$tanh))\n  bijectors[[i]] <- maf\n  if (use_batchnorm)\n    bijectors[[i + 1]] <- tfb_batch_normalization()\n  if (use_permute)\n    bijectors[[i + 2]] <- tfb_permute((ncol(df2) - 1):0)\n}\n\nif (use_permute) bijectors <- bijectors[-num_layers]\n\nflow <- bijectors %>%\n  discard(is.null) %>%\n  # tfb_chain expects arguments in reverse order of application\n  rev() %>%\n  tfb_chain()\n\ntarget_dist <- tfd_transformed_distribution(\n  distribution = base_dist,\n  bijector = flow\n)\n\nAnd configuring the optimizer:\n\n\noptimizer <- tf$train$AdamOptimizer(1e-4)\n\nUnder that isotropic Gaussian we chose as a base distribution, how likely are the data?\n\n\nbase_loglik <- base_dist %>% \n  tfd_log_prob(x_train) %>% \n  tf$reduce_mean()\nbase_loglik %>% as.numeric()        # -11.33871\n\nbase_loglik_test <- base_dist %>% \n  tfd_log_prob(x_test) %>% \n  tf$reduce_mean()\nbase_loglik_test %>% as.numeric()   # -11.36431\n\nAnd, just as a quick sanity check: What is the loglikelihood of the data under the transformed distribution before any training?\n\n\ntarget_loglik_pre <-\n  target_dist %>% tfd_log_prob(x_train) %>% tf$reduce_mean()\ntarget_loglik_pre %>% as.numeric()        # -11.22097\n\ntarget_loglik_pre_test <-\n  target_dist %>% tfd_log_prob(x_test) %>% tf$reduce_mean()\ntarget_loglik_pre_test %>% as.numeric()   # -11.36431\n\nThe values match - good. Here now is the training loop. Being impatient, we already keep checking the loglikelihood on the (complete) test set to see if we’re making any progress.\n\n\nn_epochs <- 10\n\nfor (i in 1:n_epochs) {\n  \n  agg_loglik <- 0\n  num_batches <- 0\n  iter <- make_iterator_one_shot(train_dataset)\n  \n  until_out_of_range({\n    batch <- iterator_get_next(iter)\n    loss <-\n      function()\n        - tf$reduce_mean(target_dist %>% tfd_log_prob(batch))\n    optimizer$minimize(loss)\n    \n    loglik <- tf$reduce_mean(target_dist %>% tfd_log_prob(batch))\n    agg_loglik <- agg_loglik + loglik\n    num_batches <- num_batches + 1\n    \n    test_iter <- make_iterator_one_shot(test_dataset)\n    test_batch <- iterator_get_next(test_iter)\n    loglik_test_current <- target_dist %>% tfd_log_prob(test_batch) %>% tf$reduce_mean()\n    \n    if (num_batches %% 100 == 1)\n      cat(\n        \"Epoch \",\n        i,\n        \": \",\n        \"Batch \",\n        num_batches,\n        \": \",\n        (agg_loglik %>% as.numeric()) / num_batches,\n        \" --- test: \",\n        loglik_test_current %>% as.numeric(),\n        \"\\n\"\n      )\n  })\n}\n\nWith both training and test sets amounting to over 2 million records each, we did not have the patience to run this model until no improvement occurred for 30 consecutive epochs on the validation set (like the authors did). However, the picture we get from one complete epoch’s run is pretty clear: The setup seems to work pretty okay.\n\nEpoch  1 :  Batch      1:  -8.212026  --- test:  -10.09264 \nEpoch  1 :  Batch   1001:   2.222953  --- test:   1.894102 \nEpoch  1 :  Batch   2001:   2.810996  --- test:   2.147804 \nEpoch  1 :  Batch   3001:   3.136733  --- test:   3.673271 \nEpoch  1 :  Batch   4001:   3.335549  --- test:   4.298822 \nEpoch  1 :  Batch   5001:   3.474280  --- test:   4.502975 \nEpoch  1 :  Batch   6001:   3.606634  --- test:   4.612468 \nEpoch  1 :  Batch   7001:   3.695355  --- test:   4.146113 \nEpoch  1 :  Batch   8001:   3.767195  --- test:   3.770533 \nEpoch  1 :  Batch   9001:   3.837641  --- test:   4.819314 \nEpoch  1 :  Batch  10001:   3.908756  --- test:   4.909763 \nEpoch  1 :  Batch  11001:   3.972645  --- test:   3.234356 \nEpoch  1 :  Batch  12001:   4.020613  --- test:   5.064850 \nEpoch  1 :  Batch  13001:   4.067531  --- test:   4.916662 \nEpoch  1 :  Batch  14001:   4.108388  --- test:   4.857317 \nEpoch  1 :  Batch  15001:   4.147848  --- test:   5.146242 \nEpoch  1 :  Batch  16001:   4.177426  --- test:   4.929565 \nEpoch  1 :  Batch  17001:   4.209732  --- test:   4.840716 \nEpoch  1 :  Batch  18001:   4.239204  --- test:   5.222693 \nEpoch  1 :  Batch  19001:   4.264639  --- test:   5.279918 \nEpoch  1 :  Batch  20001:   4.291542  --- test:   5.29119 \nEpoch  1 :  Batch  21001:   4.314462  --- test:   4.872157 \nEpoch  2 :  Batch      1:   5.212013  --- test:   4.969406 \nWith these training results, we regard the proof of concept as basically successful. However, from our experiments we also have to say that the choice of hyperparameters seems to matter a lot. For example, use of the relu activation function instead of tanh resulted in the network basically learning nothing. (As per the authors, relu worked fine on other datasets that had been z-transformed in just the same way.)\nBatch normalization here was obligatory - and this might go for flows in general. The permutation bijectors, on the other hand, did not make much of a difference on this dataset. Overall the impression is that for flows, we might either need a “bag of tricks” (like is commonly said about GANs), or more involved architectures (see “Outlook” below).\nFinally, we wind up with an experiment, coming back to our favorite audio data, already featured in two posts: Simple Audio Classification with Keras and Audio classification with Keras: Looking closer at the non-deep learning parts.\nAnalysing audio data with MAF\nThe dataset in question consists of recordings of 30 words, pronounced by a number of different speakers. In those previous posts, a convnet was trained to map spectrograms to those 30 classes. Now instead we want to try something different: Train an MAF on one of the classes - the word “zero”, say - and see if we can use the trained network to mark “non-zero” words as less likely: perform anomaly detection, in a way. Spoiler alert: The results were not too encouraging, and if you are interested in a task like this, you might want to consider a different architecture (again, see “Outlook” below).\nNonetheless, we quickly relate what was done, as this task is a nice example of handling data where features vary over more than one axis.\nPreprocessing starts as in the aforementioned previous posts. Here though, we explicitly use eager execution, and may sometimes hard-code known values to keep the code snippets short.\n\n\nlibrary(tensorflow)\nlibrary(tfprobability)\n\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nlibrary(tfdatasets)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(purrr)\nlibrary(caret)\nlibrary(stringr)\n\n# make decode_wav() run with the current release 1.13.1 as well as with the current master branch\ndecode_wav <- function() if (reticulate::py_has_attr(tf, \"audio\")) tf$audio$decode_wav\n  else tf$contrib$framework$python$ops$audio_ops$decode_wav\n# same for stft()\nstft <- function() if (reticulate::py_has_attr(tf, \"signal\")) tf$signal$stft else tf$spectral$stft\n\nfiles <- fs::dir_ls(path = \"audio/data_1/speech_commands_v0.01/\", # replace by yours\n                    recursive = TRUE,\n                    glob = \"*.wav\")\n\nfiles <- files[!str_detect(files, \"background_noise\")]\n\ndf <- tibble(\n  fname = files,\n  class = fname %>%\n    str_extract(\"v0.01/.*/\") %>%\n    str_replace_all(\"v0.01/\", \"\") %>%\n    str_replace_all(\"/\", \"\")\n)\n\nWe train the MAF on pronunciations of the word “zero”.\n\n\nfor (c in unique(df$class)) {\n  assign(paste0(\"df_\", c), df %>% filter(class == c) %>% select(fname))\n}\n\ndf_ <- df_zero # 2 * 1178 rows\nidx_train <- sample(1:nrow(df_), 0.5 * nrow(df_))\ndf_train <- df_[idx_train, ]\ndf_test <- df_[-idx_train, ]\n\nFollowing the approach detailed in Audio classification with Keras: Looking closer at the non-deep learning parts, we’d like to train the network on spectrograms instead of the raw time domain data. Using the same settings for frame_length and frame_step of the Short Term Fourier Transform as in that post, we’d arrive at data shaped number of frames x number of FFT coefficients. To make this work with the masked_dense() employed in tfb_masked_autoregressive_flow(), the data would then have to be flattened, yielding an impressive 25186 features in the joint distribution.\nWith the architecture defined as above in the GAS example, this lead to the network not making much progress. Neither did leaving the data in time domain form, with 16000 features in the joint distribution. Thus, we decided to work with the FFT coefficients computed over the complete window instead, resulting in 257 joint features.4\n\n\nbatch_size <- 100\n\nsampling_rate <- 16000L\ndata_generator <- function(df,\n                           batch_size) {\n  \n  ds <- tensor_slices_dataset(df) \n  \n  ds <- ds %>%\n    dataset_map(function(obs) {\n      wav <-\n        decode_wav()(tf$read_file(tf$reshape(obs$fname, list())))\n      samples <- wav$audio[ ,1]\n      \n      # some wave files have fewer than 16000 samples\n      padding <- list(list(0L, sampling_rate - tf$shape(samples)[1]))\n      padded <- tf$pad(samples, padding)\n      \n      stft_out <- stft()(padded, 16000L, 1L, 512L)\n      magnitude_spectrograms <- tf$abs(stft_out) %>% tf$squeeze()\n    })\n  \n  ds %>% dataset_batch(batch_size)\n  \n}\n\nds_train <- data_generator(df_train, batch_size)\nbatch <- ds_train %>% \n  make_iterator_one_shot() %>%\n  iterator_get_next()\n\ndim(batch) # 100 x 257\n\nTraining then proceeded as on the GAS dataset.\n\n\n# define MAF\nbase_dist <-\n  tfd_multivariate_normal_diag(loc = rep(0, dim(batch)[2]))\n\nnum_hidden <- 512 \nuse_batchnorm <- TRUE\nuse_permute <- TRUE\nnum_mafs <- 10 \nnum_layers <- 3 * num_mafs\n\n# store bijectors in a list\nbijectors <- vector(mode = \"list\", length = num_layers)\n\n# fill list, optionally adding batchnorm and permute bijectors\nfor (i in seq(1, num_layers, by = 3)) {\n  maf <- tfb_masked_autoregressive_flow(\n    shift_and_log_scale_fn = tfb_masked_autoregressive_default_template(\n      hidden_layers = list(num_hidden, num_hidden),\n      activation = tf$nn$tanh,\n      ))\n  bijectors[[i]] <- maf\n  if (use_batchnorm)\n    bijectors[[i + 1]] <- tfb_batch_normalization()\n  if (use_permute)\n    bijectors[[i + 2]] <- tfb_permute((dim(batch)[2] - 1):0)\n}\n\nif (use_permute) bijectors <- bijectors[-num_layers]\nflow <- bijectors %>%\n  # possibly clean out empty elements (if no batchnorm or no permute)\n  discard(is.null) %>%\n  rev() %>%\n  tfb_chain()\n\ntarget_dist <- tfd_transformed_distribution(distribution = base_dist,\n                                            bijector = flow)\n\noptimizer <- tf$train$AdamOptimizer(1e-3)\n\n# train MAF\nn_epochs <- 100\nfor (i in 1:n_epochs) {\n  agg_loglik <- 0\n  num_batches <- 0\n  iter <- make_iterator_one_shot(ds_train)\n  until_out_of_range({\n    batch <- iterator_get_next(iter)\n    loss <-\n      function()\n        - tf$reduce_mean(target_dist %>% tfd_log_prob(batch))\n    optimizer$minimize(loss)\n    \n    loglik <- tf$reduce_mean(target_dist %>% tfd_log_prob(batch))\n    agg_loglik <- agg_loglik + loglik\n    num_batches <- num_batches + 1\n    \n    loglik_test_current <- \n      target_dist %>% tfd_log_prob(ds_test) %>% tf$reduce_mean()\n\n    if (num_batches %% 20 == 1)\n      cat(\n        \"Epoch \",\n        i,\n        \": \",\n        \"Batch \",\n        num_batches,\n        \": \",\n        ((agg_loglik %>% as.numeric()) / num_batches) %>% round(1),\n        \" --- test: \",\n        loglik_test_current %>% as.numeric() %>% round(1),\n        \"\\n\"\n      )\n  })\n}\n\nDuring training, we also monitored loglikelihoods on three different classes, cat, bird and wow5. Here are the loglikelihoods from the first 10 epochs. “Batch” refers to the current training batch (first batch in the epoch), all other values refer to complete datasets (the complete test set and the three sets selected for comparison).\n\nepoch   |   batch  |   test   |   \"cat\"  |   \"bird\"  |   \"wow\"  |\n--------|----------|----------|----------|-----------|----------|\n1       |   1443.5 |   1455.2 |   1398.8 |    1434.2 |   1546.0 |\n2       |   1935.0 |   2027.0 |   1941.2 |    1952.3 |   2008.1 | \n3       |   2004.9 |   2073.1 |   2003.5 |    2000.2 |   2072.1 |\n4       |   2063.5 |   2131.7 |   2056.0 |    2061.0 |   2116.4 |        \n5       |   2120.5 |   2172.6 |   2096.2 |    2085.6 |   2150.1 |\n6       |   2151.3 |   2206.4 |   2127.5 |    2110.2 |   2180.6 | \n7       |   2174.4 |   2224.8 |   2142.9 |    2163.2 |   2195.8 |\n8       |   2203.2 |   2250.8 |   2172.0 |    2061.0 |   2221.8 |        \n9       |   2224.6 |   2270.2 |   2186.6 |    2193.7 |   2241.8 |\n10      |   2236.4 |   2274.3 |   2191.4 |    2199.7 |   2243.8 |        \nWhile this does not look too bad, a complete comparison against all twenty-nine non-target classes had “zero” outperformed by seven other classes, with the remaining twenty-two lower in loglikelihood. We don’t have a model for anomaly detection, as yet.\nOutlook\nAs already alluded to several times, for data with temporal and/or spatial orderings more evolved architectures may prove useful. The very successful PixelCNN family is based on masked convolutions, with more recent developments bringing further refinements (e.g. Gated PixelCNN (Oord et al. 2016), PixelCNN++ (Salimans et al. 2017). Attention, too, may be masked and thus rendered autoregressive, as employed in the hybrid PixelSNAIL (Chen et al. 2017) and the - not surprisingly given its name - transformer-based ImageTransformer (Parmar et al. 2018).\nTo conclude, - while this post was interested in the intersection of flows and autoregressivity - and last not least the use therein of TFP bijectors - an upcoming one might dive deeper into autoregressive models specifically… and who knows, perhaps come back to the audio data for a fourth time.\n\n\nChen, Xi, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. 2017. “PixelSNAIL: An Improved Autoregressive Generative Model.” CoRR abs/1712.09763. http://arxiv.org/abs/1712.09763.\n\n\nDinh, Laurent, Jascha Sohl-Dickstein, and Samy Bengio. 2016. “Density Estimation Using Real Nvp.” CoRR abs/1605.08803. http://arxiv.org/abs/1605.08803.\n\n\nGermain, Mathieu, Karol Gregor, Iain Murray, and Hugo Larochelle. 2015. “MADE: Masked Autoencoder for Distribution Estimation.” CoRR abs/1502.03509. http://arxiv.org/abs/1502.03509.\n\n\nOord, Aaron van den, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. 2016. “Conditional Image Generation with Pixelcnn Decoders.” CoRR abs/1606.05328. http://arxiv.org/abs/1606.05328.\n\n\nPapamakarios, George, Theo Pavlakou, and Iain Murray. 2017. “Masked Autoregressive Flow for Density Estimation.” arXiv E-Prints, May, arXiv:1705.07057. http://arxiv.org/abs/1705.07057.\n\n\nParmar, Niki, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, and Alexander Ku. 2018. “Image Transformer.” CoRR abs/1802.05751. http://arxiv.org/abs/1802.05751.\n\n\nSalimans, Tim, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. 2017. “PixelCNN++: Improving the Pixelcnn with Discretized Logistic Mixture Likelihood and Other Modifications.” CoRR abs/1701.05517. http://arxiv.org/abs/1701.05517.\n\n\ntfb_masked_autoregressive_flow, in R↩︎\nFor a comparison of Masked Autoregressive Flow to its siblings Real NVP (also available as a TFP bijector) and Inverse Autogressive Flow (to be obtained as an inverse of MAF in TFP), see Eric Jang’s excellent tutorial.↩︎\nnot specified individually in the paper↩︎\nWe quickly experimented with a higher number of FFT coefficients, but the approach did not seem that promising.↩︎\nto avoid clutter we don’t show the respective code↩︎\n",
    "preview": "posts/2019-04-24-autoregressive-flows/images/made.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 686,
    "preview_height": 398
  },
  {
    "path": "posts/2019-04-16-autokeras/",
    "title": "Auto-Keras: Tuning-free deep learning from R",
    "description": "Sometimes in deep learning, architecture design and hyperparameter tuning pose substantial challenges. Using Auto-Keras, none of these is needed: We start a search procedure and extract the best-performing model. This post presents Auto-Keras in action on the well-known MNIST dataset.",
    "author": [
      {
        "name": "Juan Cruz Rodriguez",
        "url": "https://jcrodriguez.rbind.io"
      }
    ],
    "date": "2019-04-16",
    "categories": [
      "TensorFlow/Keras",
      "Packages/Releases"
    ],
    "contents": "\n\n\n\nToday, we’re happy to feature a guest post written by Juan Cruz, showing how to use Auto-Keras from R. Juan holds a master’s degree in Computer Science. Currently, he is finishing his master’s degree in Applied Statistics, as well as a Ph.D. in Computer Science, at the Universidad Nacional de Córdoba. He started his R journey almost six years ago, applying statistical methods to biology data. He enjoys software projects focused on making machine learning and data science available to everyone.\nIn the past few years, artificial intelligence has been a subject of intense media hype. Machine learning, deep learning, and artificial intelligence come up in countless articles, often outside of technology-minded publications. For most any topic, a brief search on the web yields dozens of texts suggesting the application of one or the other deep learning model.\nHowever, tasks such as feature engineering, hyperparameter tuning, or network design, are by no means easy for people without a rich computer science background. Lately, research started to emerge in the area of what is known as Neural Architecture Search (NAS) (Baker et al. 2016; Pham et al. 2018; Zoph and Le 2016; Luo et al. 2018; Liu et al. 2017; Real et al. 2018; Jin, Song, and Hu 2018). The main goal of NAS algorithms is, given a specific tagged dataset, to search for the most optimal neural network to perform a certain task on that dataset. In this sense, NAS algorithms allow the user to not have to worry about any task related to data science engineering. In other words, given a tagged dataset and a task, e.g., image classification, or text classification among others, the NAS algorithm will train several high-performance deep learning models and return the one that outperforms the rest.\nSeveral NAS algorithms were developed on different platforms (e.g. Google Cloud AutoML), or as libraries of certain programming languages (e.g. Auto-Keras, TPOT, Auto-Sklearn). However, for a language that brings together experts from such diverse disciplines as is the R programming language, to the best of our knowledge, there is no NAS tool to this day. In this post, we present the Auto-Keras R package, an interface from R to the Auto-Keras Python library (Jin, Song, and Hu 2018). Thanks to the use of Auto-Keras, R programmers with few lines of code will be able to train several deep learning models for their data and get the one that outperforms the others.\nLet’s dive into Auto-Keras!\nAuto-Keras\nNote: the Python Auto-Keras library is only compatible with Python 3.6. So make sure this version is currently installed, and correctly set to be used by the reticulate R library.\nInstallation\nTo begin, install the autokeras R package from GitHub as follows:\n\n\nif (!require(\"remotes\")) {\n  install.packages(\"remotes\")\n}\nremotes::install_github(\"jcrodriguez1989/autokeras\")\n\nThe Auto-Keras R interface uses the Keras and TensorFlow backend engines by default. To install both the core Auto-Keras library as well as the Keras and TensorFlow backends use the install_autokeras() function:\n\n\nlibrary(\"autokeras\")\ninstall_autokeras()\n\nThis will provide you with default CPU-based installations of Keras and TensorFlow. If you want a more customized installation, e.g. if you want to take advantage of NVIDIA GPUs, see the documentation for install_keras() from the keras R library.\nMNIST Example\nWe can learn the basics of Auto-Keras by walking through a simple example: recognizing handwritten digits from the MNIST dataset. MNIST consists of 28 x 28 grayscale images of handwritten digits like this:\n\nThe dataset also includes labels for each image, telling us which digit it is. For example, the label for the above image is 2.\nLoading the Data\nThe MNIST dataset is included with Keras and can be accessed using the dataset_mnist() function from the keras R library. Here we load the dataset, and then create variables for our test and training data:\n\n\nlibrary(\"keras\")\nmnist <- dataset_mnist() # load mnist dataset\nc(x_train, y_train) %<-% mnist$train # get train\nc(x_test, y_test) %<-% mnist$test # and test data\n\nThe x data is a 3-d array (images,width,height) of grayscale integer values ranging between 0 to 255.\n\n\nx_train[1, 14:20, 14:20] # show some pixels from the first image\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n[1,]  241  225  160  108    1    0    0\n[2,]   81  240  253  253  119   25    0\n[3,]    0   45  186  253  253  150   27\n[4,]    0    0   16   93  252  253  187\n[5,]    0    0    0    0  249  253  249\n[6,]    0   46  130  183  253  253  207\n[7,]  148  229  253  253  253  250  182\nThe y data is an integer vector with values ranging from 0 to 9.\n\n\nn_imgs <- 8\nhead(y_train, n = n_imgs) # show first 8 labels\n\n\n[1] 5 0 4 1 9 2 1 3\nEach of these images can be plotted in R:\n\n\nlibrary(\"ggplot2\")\nlibrary(\"tidyr\")\n# get each of the first n_imgs from the x_train dataset and\n# convert them to wide format\nmnist_to_plot <-\n  do.call(rbind, lapply(seq_len(n_imgs), function(i) {\n    samp_img <- x_train[i, , ] %>%\n      as.data.frame()\n    colnames(samp_img) <- seq_len(ncol(samp_img))\n    data.frame(\n      img = i,\n      gather(samp_img, \"x\", \"value\", convert = TRUE),\n      y = seq_len(nrow(samp_img))\n    )\n  }))\nggplot(mnist_to_plot, aes(x = x, y = y, fill = value)) + geom_tile() +\n  scale_fill_gradient(low = \"black\", high = \"white\", na.value = NA) +\n  scale_y_reverse() + theme_minimal() + theme(panel.grid = element_blank()) +\n  theme(aspect.ratio = 1) + xlab(\"\") + ylab(\"\") + facet_wrap(~img, nrow = 2)\n\n\nData ready, let’s get the model!\nData pre-processing? Model definition? Metrics, epochs definition, anyone? No, none of them are required by Auto-Keras. For image classification tasks, it is enough for Auto-Keras to be passed the x_train and y_train objects as defined above.\nSo, to train several deep learning models for two hours, it is enough to run:\n\n\n# train an Image Classifier for two hours\nclf <- model_image_classifier(verbose = TRUE) %>%\n  fit(x_train, y_train, time_limit = 2 * 60 * 60)\n\n\nSaving Directory: /tmp/autokeras_ZOG76O\nPreprocessing the images.\nPreprocessing finished.\n\nInitializing search.\nInitialization finished.\n\n\n+----------------------------------------------+\n|               Training model 0               |\n+----------------------------------------------+\n\nNo loss decrease after 5 epochs.\n\n\nSaving model.\n+--------------------------------------------------------------------------+\n|        Model ID        |          Loss          |      Metric Value      |\n+--------------------------------------------------------------------------+\n|           0            |  0.19463148526847363   |   0.9843999999999999   |\n+--------------------------------------------------------------------------+\n\n\n+----------------------------------------------+\n|               Training model 1               |\n+----------------------------------------------+\n\nNo loss decrease after 5 epochs.\n\n\nSaving model.\n+--------------------------------------------------------------------------+\n|        Model ID        |          Loss          |      Metric Value      |\n+--------------------------------------------------------------------------+\n|           1            |   0.210642946138978    |         0.984          |\n+--------------------------------------------------------------------------+\nEvaluate it:\n\n\nclf %>% evaluate(x_test, y_test)\n\n\n[1] 0.9866\nAnd then just get the best-trained model with:\n\n\nclf %>% final_fit(x_train, y_train, x_test, y_test, retrain = TRUE)\n\n\nNo loss decrease after 30 epochs.\nEvaluate the final model:\n\n\nclf %>% evaluate(x_test, y_test)\n\n\n[1] 0.9918\nAnd the model can be saved to take it into production with:\n\n\nclf %>% export_autokeras_model(\"./myMnistModel.pkl\")\n\nConclusions\nIn this post, the Auto-Keras R package was presented. It was shown that, with almost no deep learning knowledge, it is possible to train models and get the one that returns the best results for the desired task. Here we trained models for two hours. However, we have also tried training for 24 hours, resulting in 15 models being trained, to a final accuracy of 0.9928. Although Auto-Keras will not return a model as efficient as one generated manually by an expert, this new library has its place as an excellent starting point in the world of deep learning. Auto-Keras is an open-source R package, and is freely available in https://github.com/jcrodriguez1989/autokeras/.\nAlthough the Python Auto-Keras library is currently in a pre-release version and comes with not too many types of training tasks, this is likely to change soon, as the project it was recently added to the keras-team set of repositories. This will undoubtedly further its progress a lot. So stay tuned, and thanks for reading!\nReproducibility\nTo correctly reproduce the results of this post, we recommend using the Auto-Keras docker image by typing:\n\n\ndocker pull jcrodriguez1989/r-autokeras:0.1.0\ndocker run -it jcrodriguez1989/r-autokeras:0.1.0 /bin/bash\n\n\n\nBaker, Bowen, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. 2016. “Designing Neural Network Architectures Using Reinforcement Learning.” arXiv Preprint arXiv:1611.02167.\n\n\nJin, Haifeng, Qingquan Song, and Xia Hu. 2018. “Auto-Keras: An Efficient Neural Architecture Search System.” arXiv Preprint arXiv:1806.10282.\n\n\nLiu, Hanxiao, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu. 2017. “Hierarchical Representations for Efficient Architecture Search.” arXiv Preprint arXiv:1711.00436.\n\n\nLuo, Renqian, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. 2018. “Neural Architecture Optimization.” In Advances in Neural Information Processing Systems, 7816–27.\n\n\nPham, Hieu, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeff Dean. 2018. “Efficient Neural Architecture Search via Parameter Sharing.” arXiv Preprint arXiv:1802.03268.\n\n\nReal, Esteban, Alok Aggarwal, Yanping Huang, and Quoc V Le. 2018. “Regularized Evolution for Image Classifier Architecture Search.” arXiv Preprint arXiv:1802.01548.\n\n\nZoph, Barret, and Quoc V Le. 2016. “Neural Architecture Search with Reinforcement Learning.” arXiv Preprint arXiv:1611.01578.\n\n\n\n\n",
    "preview": "posts/2019-04-16-autokeras/images/thumbnail.jpg",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-04-05-bijectors-flows/",
    "title": "Getting into the flow: Bijectors in TensorFlow Probability",
    "description": "Normalizing flows are one of the lesser known, yet fascinating and successful architectures in unsupervised deep learning. In this post we provide a basic introduction to flows using tfprobability, an R wrapper to TensorFlow Probability. Upcoming posts will build on this, using more complex flows on more complex data.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-04-05",
    "categories": [
      "Probabilistic ML/DL",
      "TensorFlow/Keras",
      "Concepts",
      "Unsupervised Learning"
    ],
    "contents": "\nAs of today, deep learning’s greatest successes have taken place in the realm of supervised learning, requiring lots and lots of annotated training data. However, data does not (normally) come with annotations or labels. Also, unsupervised learning is attractive because of the analogy to human cognition.\nOn this blog so far, we have seen two major architectures for unsupervised learning: variational autoencoders and generative adversarial networks. Lesser known, but appealing for conceptual as well as for performance reasons are normalizing flows (Jimenez Rezende and Mohamed 2015). In this and the next post, we’ll introduce flows, focusing on how to implement them using TensorFlow Probability (TFP).\nIn contrast to previous posts involving TFP that accessed its functionality using low-level $-syntax, we now make use of tfprobability, an R wrapper in the style of keras, tensorflow and tfdatasets. A note regarding this package: It is still under heavy development and the API may change. As of this writing, wrappers do not yet exist for all TFP modules, but all TFP functionality is available using $-syntax if need be.\nDensity estimation and sampling\nBack to unsupervised learning, and specifically thinking of variational autoencoders, what are the main things they give us? One thing that’s seldom missing from papers on generative methods are pictures of super-real-looking faces (or bed rooms, or animals …). So evidently sampling (or: generation) is an important part. If we can sample from a model and obtain real-seeming entities, this means the model has learned something about how things are distributed in the world: it has learned a distribution. In the case of variational autoencoders, there is more: The entities are supposed to be determined by a set of distinct, disentangled (hopefully!) latent factors. But this is not the assumption in the case of normalizing flows, so we are not going to elaborate on this here.\nAs a recap, how do we sample from a VAE? We draw from \\(z\\), the latent variable, and run the decoder network on it. The result should - we hope - look like it comes from the empirical data distribution. It should not, however, look exactly like any of the items used to train the VAE, or else we have not learned anything useful.\nThe second thing we may get from a VAE is an assessment of the plausibility of individual data, to be used, for example, in anomaly detection. Here “plausibility” is vague on purpose: With VAE, we don’t have a means to compute an actual density under the posterior.\nWhat if we want, or need, both: generation of samples as well as density estimation? This is where normalizing flows come in.\nNormalizing flows\nA flow is a sequence of differentiable, invertible mappings from data to a “nice” distribution, something we can easily sample from and use to calculate a density. Let’s take as example the canonical way to generate samples from some distribution, the exponential, say.\nWe start by asking our random number generator for some number between 0 and 1:1\n\n\nu <- runif(1)\n\nThis number we treat as coming from a cumulative probability distribution (CDF) - from an exponential CDF, to be precise. Now that we have a value from the CDF, all we need to do is map that “back” to a value. That mapping CDF -> value we’re looking for is just the inverse of the CDF of an exponential distribution, the CDF being\n\\[F(x) = 1 - e^{-\\lambda x}\\]\nThe inverse then is\n\\[\nF^{-1}(u) = -\\frac{1}{\\lambda} ln (1 - u)\n\\]\nwhich means we may get our exponential sample doing\n\n\nlambda <- 0.5 # pick some lambda\nx <- -1/lambda * log(1-u)\n\nWe see the CDF is actually a flow (or a building block thereof, if we picture most flows as comprising several transformations), since\nIt maps data to a uniform distribution between 0 and 1, allowing to assess data likelihood.\nConversely, it maps a probability to an actual value, thus allowing to generate samples.\nFrom this example, we see why a flow should be invertible, but we don’t yet see why it should be differentiable. This will become clear shortly, but first let’s take a look at how flows are available in tfprobability.\nBijectors\nTFP comes with a treasure trove of transformations, called bijectors, ranging from simple computations like exponentiation to more complex ones like the discrete cosine transform.\nTo get started, let’s use tfprobability to generate samples from the normal distribution. There is a bijector tfb_normal_cdf() that takes input data to the interval \\([0,1]\\). Its inverse transform then yields a random variable with the standard normal distribution:\n\n\nlibrary(tfprobability)\nlibrary(tensorflow)\ntfe_enable_eager_execution()\n\nlibrary(ggplot2)\n\nb <- tfb_normal_cdf()\nu <- runif(1000)\nx <- b %>% tfb_inverse(u) %>% as.numeric()\n\nx %>% data.frame(x = .) %>% ggplot(aes(x = x)) + geom_density()\n\n\nConversely, we can use this bijector to determine the (log) probability of a sample from the normal distribution. We’ll check against a straightforward use of tfd_normal in the distributions module:\n\n\nx <- 2.01\nd_n <- tfd_normal(loc = 0, scale = 1) \n\nd_n %>% tfd_log_prob(x) %>% as.numeric() # -2.938989\n\nTo obtain that same log probability from the bijector, we add two components:\nFirstly, we run the sample through the forward transformation and compute log probability under the uniform distribution.\nSecondly, as we’re using the uniform distribution to determine probability of a normal sample, we need to track how probability changes under this transformation. This is done by calling tfb_forward_log_det_jacobian (to be further elaborated on below).\n\n\nb <- tfb_normal_cdf()\nd_u <- tfd_uniform()\n\nl <- d_u %>% tfd_log_prob(b %>% tfb_forward(x))\nj <- b %>% tfb_forward_log_det_jacobian(x, event_ndims = 0)\n\n(l + j) %>% as.numeric() # -2.938989\n\nWhy does this work? Let’s get some background.\nProbability mass is conserved\nFlows are based on the principle that under transformation, probability mass is conserved. Say we have a flow from \\(x\\) to \\(z\\): \\[z = f(x)\\]\nSuppose we sample from \\(z\\) and then, compute the inverse transform to obtain \\(x\\). We know the probability of \\(z\\). What is the probability that \\(x\\), the transformed sample, lies between \\(x_0\\) and \\(x_0 + dx\\)?\nThis probability is \\(p(x) \\ dx\\), the density times the length of the interval. This has to equal the probability that \\(z\\) lies between \\(f(x)\\) and \\(f(x + dx)\\). That new interval has length \\(f'(x) dx\\), so:\n\\[p(x) dx = p(z) f'(x) dx\\]\nOr equivalently\n\\[p(x) = p(z) * dz/dx\\]\nThus, the sample probability \\(p(x)\\) is determined by the base probability \\(p(z)\\) of the transformed distribution, multiplied by how much the flow stretches space.\nThe same goes in higher dimensions: Again, the flow is about the change in probability volume between the \\(z\\) and \\(y\\) spaces:\n\\[p(x) =  p(z) \\frac{vol(dz)}{vol(dx)}\\]\nIn higher dimensions, the Jacobian replaces the derivative. Then, the change in volume is captured by the absolute value of its determinant:\n\\[p(\\mathbf{x}) = p(f(\\mathbf{x})) \\ \\bigg|det\\frac{\\partial f({\\mathbf{x})}}{\\partial{\\mathbf{x}}}\\bigg|\\]\nIn practice, we work with log probabilities, so\n\\[log \\ p(\\mathbf{x}) = log \\ p(f(\\mathbf{x})) + log \\ \\bigg|det\\frac{\\partial f({\\mathbf{x})}}{\\partial{\\mathbf{x}}}\\bigg| \\]\nLet’s see this with another bijector example, tfb_affine_scalar. Below, we construct a mini-flow that maps a few arbitrary chosen \\(x\\) values to double their value (scale = 2):\n\n\nx <- c(0, 0.5, 1)\nb <- tfb_affine_scalar(shift = 0, scale = 2)\n\nTo compare densities under the flow, we choose the normal distribution, and look at the log densities:\n\n\nd_n <- tfd_normal(loc = 0, scale = 1)\nd_n %>% tfd_log_prob(x) %>% as.numeric() # -0.9189385 -1.0439385 -1.4189385\n\nNow apply the flow and compute the new log densities as a sum of the log densities of the corresponding \\(x\\) values and the log determinant of the Jacobian:\n\n\nz <- b %>% tfb_forward(x)\n\n(d_n  %>% tfd_log_prob(b %>% tfb_inverse(z))) +\n  (b %>% tfb_inverse_log_det_jacobian(z, event_ndims = 0)) %>%\n  as.numeric() # -1.6120857 -1.7370857 -2.1120858\n\nWe see that as the values get stretched in space (we multiply by 2), the individual log densities go down. We can verify the cumulative probability stays the same using tfd_transformed_distribution():\n\n\nd_t <- tfd_transformed_distribution(distribution = d_n, bijector = b)\nd_n %>% tfd_cdf(x) %>% as.numeric()  # 0.5000000 0.6914625 0.8413447\n\nd_t %>% tfd_cdf(y) %>% as.numeric()  # 0.5000000 0.6914625 0.8413447\n\nSo far, the flows we saw were static - how does this fit into the framework of neural networks?\nTraining a flow\nGiven that flows are bidirectional, there are two ways to think about them. Above, we have mostly stressed the inverse mapping: We want a simple distribution we can sample from, and which we can use to compute a density. In that line, flows are sometimes called “mappings from data to noise” - noise mostly being an isotropic Gaussian. However in practice, we don’t have that “noise” yet, we just have data. So in practice, we have to learn a flow that does such a mapping. We do this by using bijectors with trainable parameters. We’ll see a very simple example here, and leave “real world flows” to the next post.\nThe example is based on part 1 of Eric Jang’s introduction to normalizing flows. The main difference (apart from simplification to show the basic pattern) is that we’re using eager execution.\nWe start from a two-dimensional, isotropic Gaussian, and we want to model data that’s also normal, but with a mean of 1 and a variance of 2 (in both dimensions).\n\n\nlibrary(tensorflow)\nlibrary(tfprobability)\n\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nlibrary(tfdatasets)\n\n# where we start from\nbase_dist <- tfd_multivariate_normal_diag(loc = c(0, 0))\n\n# where we want to go\ntarget_dist <- tfd_multivariate_normal_diag(loc = c(1, 1), scale_identity_multiplier = 2)\n\n# create training data from the target distribution\ntarget_samples <- target_dist %>% tfd_sample(1000) %>% tf$cast(tf$float32)\n\nbatch_size <- 100\ndataset <- tensor_slices_dataset(target_samples) %>%\n  dataset_shuffle(buffer_size = dim(target_samples)[1]) %>%\n  dataset_batch(batch_size)\n\nNow we’ll build a tiny neural network, consisting of an affine transformation and a nonlinearity. For the former, we can make use of tfb_affine, the multi-dimensional relative of tfb_affine_scalar. As to nonlinearities, currently TFP comes with tfb_sigmoid and tfb_tanh, but we can build our own parameterized ReLU using tfb_inline:\n\n\n# alpha is a learnable parameter\nbijector_leaky_relu <- function(alpha) {\n  \n  tfb_inline(\n    # forward transform leaves positive values untouched and scales negative ones by alpha\n    forward_fn = function(x)\n      tf$where(tf$greater_equal(x, 0), x, alpha * x),\n    # inverse transform leaves positive values untouched and scales negative ones by 1/alpha\n    inverse_fn = function(y)\n      tf$where(tf$greater_equal(y, 0), y, 1/alpha * y),\n    # volume change is 0 when positive and 1/alpha when negative\n    inverse_log_det_jacobian_fn = function(y) {\n      I <- tf$ones_like(y)\n      J_inv <- tf$where(tf$greater_equal(y, 0), I, 1/alpha * I)\n      log_abs_det_J_inv <- tf$log(tf$abs(J_inv))\n      tf$reduce_sum(log_abs_det_J_inv, axis = 1L)\n    },\n    forward_min_event_ndims = 1\n  )\n}\n\nDefine the learnable variables for the affine and the PReLU layers:\n\n\nd <- 2 # dimensionality\nr <- 2 # rank of update\n\n# shift of affine bijector\nshift <- tf$get_variable(\"shift\", d)\n# scale of affine bijector\nL <- tf$get_variable('L', c(d * (d + 1) / 2))\n# rank-r update\nV <- tf$get_variable(\"V\", c(d, r))\n\n# scaling factor of parameterized relu\nalpha <- tf$abs(tf$get_variable('alpha', list())) + 0.01\n\nWith eager execution, the variables have to be used inside the loss function, so that is where we define the bijectors. Our little flow now is a tfb_chain of bijectors, and we wrap it in a TransformedDistribution (tfd_transformed_distribution) that links source and target distributions.\n\n\nloss <- function() {\n  \n affine <- tfb_affine(\n        scale_tril = tfb_fill_triangular() %>% tfb_forward(L),\n        scale_perturb_factor = V,\n        shift = shift\n      )\n lrelu <- bijector_leaky_relu(alpha = alpha)  \n \n flow <- list(lrelu, affine) %>% tfb_chain()\n \n dist <- tfd_transformed_distribution(distribution = base_dist,\n                          bijector = flow)\n  \n l <- -tf$reduce_mean(dist$log_prob(batch))\n # keep track of progress\n print(round(as.numeric(l), 2))\n l\n}\n\nNow we can actually run the training!\n\n\noptimizer <- tf$train$AdamOptimizer(1e-4)\n\nn_epochs <- 100\nfor (i in 1:n_epochs) {\n  iter <- make_iterator_one_shot(dataset)\n  until_out_of_range({\n    batch <- iterator_get_next(iter)\n    optimizer$minimize(loss)\n  })\n}\n\nOutcomes will differ depending on random initialization, but you should see a steady (if slow) progress. Using bijectors, we have actually trained and defined a little neural network.\nOutlook\nUndoubtedly, this flow is too simple to model complex data, but it’s instructive to have seen the basic principles before delving into more complex flows. In the next post, we’ll check out autoregressive flows, again using TFP and tfprobability.\n\n\nJimenez Rezende, Danilo, and Shakir Mohamed. 2015. “Variational Inference with Normalizing Flows.” arXiv E-Prints, May, arXiv:1505.05770. http://arxiv.org/abs/1505.05770.\n\n\nYes, using runif(). Just imagine there were no corresponding rexp() in R…↩︎\n",
    "preview": "posts/2019-04-05-bijectors-flows/images/flows.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 904,
    "preview_height": 325
  },
  {
    "path": "posts/2019-03-15-concepts-way-to-dl/",
    "title": "Math, code, concepts: A third road to deep learning",
    "description": "Not everybody who wants to get into deep learning has a strong background in math or programming. This post elaborates on a concepts-driven, abstraction-based way to learn what it's all about.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-03-15",
    "categories": [
      "Meta",
      "Concepts"
    ],
    "contents": "\nIn the previous version of their awesome deep learning MOOC, I remember fast.ai’s Jeremy Howard saying something like this:\n\nYou are either a math person or a code person, and […] 1\n\nI may be wrong about the either, and this is not about either versus, say, both. What if in reality, you’re none of the above?\nWhat if you come from a background that is close to neither math and statistics, nor computer science: the humanities, say? You may not have that intuitive, fast, effortless-looking understanding of LaTeX formulae that comes with natural talent and/or years of training, or both - the same goes for computer code.\nUnderstanding always has to start somewhere, so it will have to start with math or code (or both). Also, it’s always iterative, and iterations will often alternate between math and code. But what are things you can do when primarily, you’d say you are a concepts person?\nWhen meaning doesn’t automatically emerge from formulae, it helps to look for materials (blog posts, articles, books) that stress the concepts those formulae are all about. By concepts, I mean abstractions, concise, verbal characterizations of what a formula signifies.2\nLet’s try to make conceptual a bit more concrete. At least three aspects come to mind: useful abstractions, chunking (composing symbols into meaningful blocks), and action (what does that entity actually do?)\nAbstraction\nTo many people, in school, math meant nothing. Calculus was about manufacturing cans: How can we get as much soup as possible into the can while economizing on tin. How about this instead: Calculus3 is about how one thing changes as another changes? Suddenly, you start thinking: What, in my world, can I apply this to?\nA neural network is trained using backprop - just the chain rule of calculus, many texts say. How about life. How would my present be different had I spent more time exercising the ukulele? Then, how much more time would I have spent exercising the ukulele if my mother hadn’t discouraged me so much? And then - how much less discouraging would she have been had she not been forced to give up her own career as a circus artist? And so on.\nAs a more concrete example, take optimizers. With gradient descent as a baseline, what, in a nutshell, is different about momentum, RMSProp, Adam?\nStarting with momentum, this is the formula in one of the go-to posts, Sebastian Ruder’s http://ruder.io/optimizing-gradient-descent/ 4\n\\[v_t = \\gamma v_{t-1} + \\eta \\nabla_{\\theta} J(\\theta) \\\\\n\\theta = \\theta - v_t\\]\nThe formula tells us that the change to the weights5 is made up of two parts: the gradient of the loss6 with respect to the weights, computed at some point in time \\(t\\) (and scaled by the learning rate7), and the previous change computed at time \\(t-1\\) and discounted by some factor \\(\\gamma\\). What does this actually tell us?\nIn his Coursera MOOC, Andrew Ng introduces momentum (and RMSProp, and Adam) after two videos that aren’t even about deep learning. He introduces exponential moving averages, which will be familiar to many R users: We calculate a running average where at each point in time, the running result is weighted by a certain factor (0.9, say), and the current observation by 1 minus that factor (0.1, in this example). Now look at how momentum is presented:8\n\\[v = \\beta v + (1-\\beta) dW \\\\ \nW = W - \\alpha v\\]\nWe immediately see how \\(v\\) is the exponential moving average of gradients, and it is this that gets subtracted from the weights (scaled by the learning rate).\nBuilding on that abstraction in the viewers’ minds, Ng goes on to present RMSProp. This time, a moving average is kept of the squared weights 9, and at each time, this average (or rather, its square root) is used to scale the current gradient.\n\\[s = \\beta s + (1-\\beta) dW^2 \\\\ \nW = W - \\alpha \\frac{dW}{\\sqrt s}\\]\nIf you know a bit about Adam, you can guess what comes next: Why not have moving averages in the numerator as well as the denominator?\n\\[v = \\beta_1 v + (1-\\beta_1) dW \\\\ \ns = \\beta_2 s + (1-\\beta_2) dW^2 \\\\\nW = W - \\alpha \\frac{v}{\\sqrt s + \\epsilon}\\]\nOf course, actual implementations may differ in details, and not always expose those features that clearly. But for understanding and memorization, abstractions like this one - exponential moving average - do a lot. Let’s now see about chunking.\nChunking\nLooking again at the above formula from Sebastian Ruder’s post,\n\\[v_t = \\gamma v_{t-1} + \\eta \\nabla_{\\theta} J(\\theta) \\\\\n\\theta = \\theta - v_t\\]\nhow easy is it to parse the first line? Of course that depends on experience, but let’s focus on the formula itself.10\nReading that first line, we mentally build something like an AST (abstract syntax tree). Exploiting programming language vocabulary even further, operator precedence is crucial: To understand the right half of the tree, we want to first parse \\(\\nabla_{\\theta} J(\\theta)\\), and then only take \\(\\eta\\) into consideration.\nMoving on to larger formulae, the problem of operator precedence becomes one of chunking: Take that bunch of symbols and see it as a whole. We could call this abstraction again, just like above. But here, the focus is not on naming things or verbalizing, but on seeing: Seeing at a glance that when you read\n\\[\\frac{e^{z_i}}{\\sum_j{e^{z_j}}}\\]\nit is “just a softmax”. Again, my inspiration for this comes from Jeremy Howard, who I remember demonstrating, in one of the fastai lectures, that this is how you read a paper.\nLet’s turn to a more complex example. Last year’s article on Attention-based Neural Machine Translation with Keras included a short exposition of attention, featuring four steps:\nScoring encoder hidden states as to inasmuch they are a fit to the current decoder hidden state.\nChoosing Luong-style attention now11, we have\n\\[score(\\mathbf{h}_t,\\bar{\\mathbf{h}_s}) = \\mathbf{h}_t^T \\mathbf{W}\\bar{\\mathbf{h}_s}\\]\nOn the right, we see three symbols, which may appear meaningless at first but if we mentally “fade out” the weight matrix in the middle, a dot product appears, indicating that essentially, this is calculating similarity.\nNow comes what’s called attention weights: At the current timestep, which encoder states matter most?\n\\[\\alpha_{ts} = \\frac{exp(score(\\mathbf{h}_t,\\bar{\\mathbf{h}_s}))}{\\sum_{s'=1}^{S}{score(\\mathbf{h}_t,\\bar{\\mathbf{h}_{s'}})}}\\]\nScrolling up a bit, we see that this, in fact, is “just a softmax” (even though the physical appearance is not the same). Here, it is used to normalize the scores, making them sum to 1.\nNext up is the context vector:\n\\[\\mathbf{c}_t= \\sum_s{\\alpha_{ts} \\bar{\\mathbf{h}_s}}\\]\nWithout much thinking - but remembering from right above that the \\(\\alpha\\)s represent attention weights - we see a weighted average.\nFinally, in step\nwe need to actually combine that context vector with the current hidden state (here, done by training a fully connected layer on their concatenation):\n\\[\\mathbf{a}_t = tanh(\\mathbf{W_c} [ \\mathbf{c}_t ; \\mathbf{h}_t])\\]\nThis last step may be a better example of abstraction than of chunking, but anyway those are closely related: We need to chunk adequately to name concepts, and intuition about concepts helps chunk correctly. Closely related to abstraction, too, is analyzing what entities do.\nAction\nAlthough not deep learning related (in a narrow sense), my favorite quote comes from one of Gilbert Strang’s lectures on linear algebra:12\n\nMatrices don’t just sit there, they do something.\n\nIf in school calculus was about saving production materials, matrices were about matrix multiplication - the rows-by-columns way. (Or perhaps they existed for us to be trained to compute determinants, seemingly useless numbers that turn out to have a meaning, as we are going to see in a future post.) Conversely, based on the much more illuminating matrix multiplication as linear combination of columns (resp. rows) view, Gilbert Strang introduces types of matrices as agents, concisely named by initial.\nFor example, when multiplying another matrix \\(A\\) on the right, this permutation matrix \\(P\\)\n\\[\\mathbf{P} = \\left[\\begin{array}\n{rrr}\n0 & 0 & 1 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{array}\\right]\n\\]\nputs \\(A\\)’s third row first, its first row second, and its second row third:\n\\[\\mathbf{PA} = \\left[\\begin{array}\n{rrr}\n0 & 0 & 1 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{array}\\right]\n\\left[\\begin{array}\n{rrr}\n0 & 1 & 1 \\\\\n1 & 3 & 7 \\\\\n2 & 4 & 8\n\\end{array}\\right] =\n\\left[\\begin{array}\n{rrr}\n2 & 4 & 8 \\\\\n0 & 1 & 1 \\\\\n1 & 3 & 7\n\\end{array}\\right]\n\\]\nIn the same way, reflection, rotation, and projection matrices are presented via their actions. The same goes for one of the most interesting topics in linear algebra from the point of view of the data scientist: matrix factorizations. \\(LU\\), \\(QR\\), eigendecomposition, \\(SVD\\) are all characterized by what they do. 13\nWho are the agents in neural networks? Activation functions are agents; this is where we have to mention softmax for the third time: Its strategy was described in Winner takes all: A look at activations and cost functions.\nAlso, optimizers are agents, and this is where we finally include some code. The explicit training loop used in all of the eager execution blog posts so far\n\nwith(tf$GradientTape() %as% tape, {\n     \n  # run model on current batch\n  preds <- model(x)\n     \n  # compute the loss\n  loss <- mse_loss(y, preds, x)\n})\n    \n# get gradients of loss w.r.t. model weights\ngradients <- tape$gradient(loss, model$variables)\n    \n# update model weights\noptimizer$apply_gradients(\n  purrr::transpose(list(gradients, model$variables)),\n  global_step = tf$train$get_or_create_global_step()\n)\nhas the optimizer do a single thing: apply the gradients it gets passed from the gradient tape.14 Thinking back to the characterization of different optimizers we saw above, this piece of code adds vividness to the thought that optimizers differ in what they actually do once they got those gradients.\nConclusion\nWrapping up, the goal here was to elaborate a bit on a conceptual, abstraction-driven way to get more familiar with the math involved in deep learning (or machine learning, in general). Certainly, the three aspects highlighted interact, overlap, form a whole, and there are other aspects to it. Analogy may be one, but it was left out here because it seems even more subjective, and less general. Comments describing user experiences are very welcome.\nIf I don’t remember correctly: please just allow me to use this as the perfect intro to this post.↩︎\nCertainly visualization may be very useful, too, depending on the topic/algorithm and on “how visual a person” you are. But there is no need to stress the importance of visualization - everybody agrees on it - so this post is dedicated to verbal/conceptual methods.↩︎\nDifferential calculus, to be precise.↩︎\nThis is of course an excellent article that does mention concepts. It is just not intended for beginners, in contrast to the approach highlighted below.↩︎\n\\(\\theta\\)↩︎\n\\(J\\)↩︎\n\\(\\eta\\)↩︎\nFollowing the notation from the video, marginally simplified. Here \\(\\beta\\) is the scale factor applied to the running average, \\(dW\\) is the gradient of the loss with respect to the weights, and \\(\\alpha\\) is the learning rate.↩︎\n\\(W^2\\)↩︎\n As a side note, picking up on Jeremy Howard again: The Greek letters of course don’t make things any easier, but even with a history of studying ancient Greek for five years those formulae aren’t necessarily parsed easily.↩︎\nThe original post showed Bahdanau-style attention.↩︎\nAt least that’s what I remember him saying, approximately. The exact wording does not matter here.↩︎\nAlthough, if this were the paragraph about abstractions, Gilbert Strang’s books would yield perfect examples as well.↩︎\nIn other use cases, the optimizer class may also be used to compute gradients. But that method (optimizer$compute_gradients) is defined in the optimizer superclass and not subclass specific.↩︎\n",
    "preview": "posts/2019-03-15-concepts-way-to-dl/images/prev.jpg",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-02-07-audio-background/",
    "title": "Audio classification with Keras: Looking closer at the non-deep learning parts",
    "description": "Sometimes, deep learning is seen - and welcomed - as a way to avoid laborious preprocessing of data. However, there are cases where preprocessing of sorts does not only help improve prediction, but constitutes a fascinating topic in itself. One such case is audio classification. In this post, we build on a previous post on this blog, this time focusing on explaining some of the non-deep learning background. We then link the concepts explained to updated for near-future releases TensorFlow code.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-02-07",
    "categories": [
      "TensorFlow/Keras",
      "Concepts",
      "Audio Processing"
    ],
    "contents": "\nAbout half a year ago, this blog featured a post, written by Daniel Falbel, on how to use Keras to classify pieces of spoken language. The article got a lot of attention and not surprisingly, questions arose how to apply that code to different datasets. We’ll take this as a motivation to explore in more depth the preprocessing done in that post: If we know why the input to the network looks the way it looks, we will be able to modify the model specification appropriately if need be.\nIn case you have a background in speech recognition, or even general signal processing, for you the introductory part of this post will probably not contain much news. However, you might still be interested in the code part, which shows how to do things like creating spectrograms with current versions of TensorFlow.1 If you don’t have that background, we’re inviting you on a (hopefully) fascinating journey, slightly touching on one of the greater mysteries of this universe.2\nWe’ll use the same dataset as Daniel did in his post, that is, version 1 of the Google speech commands dataset(Warden 2018) The dataset consists of ~ 65,000 WAV files, of length one second or less. Each file is a recording of one of thirty words, uttered by different speakers.\nThe goal then is to train a network to discriminate between spoken words. How should the input to the network look? The WAV files contain amplitudes of sound waves over time. Here are a few examples, corresponding to the words bird, down, sheila, and visual:3\n\nTime domain and frequency domain\nA sound wave is a signal extending in time, analogously to how what enters our visual system extends in space. At each point in time, the current signal is dependent on its past. The obvious architecture to use in modeling it thus seems to be a recurrent neural network.\nHowever, the information contained in the sound wave can be represented in an alternative way: namely, using the frequencies that make up the signal.\nHere we see a sound wave (top) and its frequency representation (bottom).\n\nIn the time representation (referred to as the time domain), the signal is composed of consecutive amplitudes over time. In the frequency domain, it is represented as magnitudes of different frequencies. It may appear as one of the greatest mysteries in this world that you can convert between those two without loss of information, that is: Both representations are essentially equivalent!\nConversion from the time domain to the frequency domain is done using the Fourier transform; to convert back, the Inverse Fourier Transform is used. There exist different types of Fourier transforms depending on whether time is viewed as continuous or discrete, and whether the signal itself is continuous or discrete. In the “real world”, where usually for us, real means virtual as we’re working with digitized signals, the time domain as well as the signal are represented as discrete and so, the Discrete Fourier Transform (DFT) is used. The DFT itself is computed using the FFT (Fast Fourier Transform) algorithm, resulting in significant speedup over a naive implementation.\nLooking back at the above example sound wave, it is a compound of four sine waves, of frequencies 8Hz, 16Hz, 32Hz, and 64Hz, whose amplitudes are added and displayed over time. The compound wave here is assumed to extend infinitely in time. Unlike speech, which changes over time, it can be characterized by a single enumeration of the magnitudes of the frequencies it is composed of. So here the spectrogram, the characterization of a signal by magnitudes of constituent frequencies varying over time, looks essentially one-dimensional.\nHowever, when we ask Praat to create a spectrogram of one of our example sounds (a seven), it could look like this:\n\nHere we see a two-dimensional image of frequency magnitudes over time (higher magnitudes indicated by darker coloring). This two-dimensional representation may be fed to a network, in place of the one-dimensional amplitudes. Accordingly, if we decide to do so we’ll use a convnet instead of an RNN.\nSpectrograms will look different depending on how we create them. We’ll take a look at the essential options in a minute. First though, let’s see what we can’t always do: ask for all frequencies that were contained in the analog signal.4\nSampling\nAbove, we said that both representations, time domain and frequency domain, were essentially equivalent. In our virtual real world, this is only true if the signal we’re working with has been digitized correctly, or as this is commonly phrased, if it has been “properly sampled”.\nTake speech as an example: As an analog signal, speech per se is continuous in time; for us to be able to work with it on a computer, it needs to be converted to happen in discrete time. This conversion of the independent variable (time in our case, space in e.g. image processing) from continuous to discrete is called sampling.\nIn this process of discretization, a crucial decision to be made is the sampling rate to use. The sampling rate has to be at least double the highest frequency in the signal. If it’s not, loss of information will occur. The way this is most often put is the other way round: To preserve all information, the analog signal may not contain frequencies above one-half the sampling rate. This frequency - half the sampling rate - is called the Nyquist rate.\nIf the sampling rate is too low, aliasing takes place: Higher frequencies alias themselves as lower frequencies. This means that not only can’t we get them, they also corrupt the magnitudes of corresponding lower frequencies they are being added to. Here’s a schematic example of how a high-frequency signal could alias itself as being lower-frequency. Imagine the high-frequency wave being sampled at integer points (grey circles) only:\n\nIn the case of the speech commands dataset, all sound waves have been sampled at 16 kHz. This means that when we ask Praat for a spectogram, we should not ask for frequencies higher than 8kHz. Here is what happens if we ask for frequencies up to 16kHz instead - we just don’t get them:\n\nNow let’s see what options we do have when creating spectrograms.\nSpectrogram creation options\nIn the above simple sine wave example, the signal stayed constant over time. However in speech utterances, the magnitudes of constituent frequencies change over time. Ideally thus, we’d have an exact frequency representation for every point in time. As an approximation to this ideal, the signal is divided into overlapping windows, and the Fourier transform is computed for each time slice separately. This is called the Short Time Fourier Transform (STFT).\nWhen we compute the spectrogram via the STFT, we need to tell it what size windows to use, and how big to make the overlap. The longer the windows we use, the better the resolution we get in the frequency domain. However, what we gain in resolution there, we lose in the time domain, as we’ll have fewer windows representing the signal. This is a general principle in signal processing: Resolution in the time and frequency domains are inversely related.\nTo make this more concrete, let’s again look at a simple example. Here is the spectrogram of a synthetic sine wave, composed of two components at 1000 Hz and 1200 Hz. The window length was left at its (Praat) default, 5 milliseconds:\n\nWe see that with a short window like that, the two different frequencies are mangled into one in the spectrogram. Now enlarge the window to 30 milliseconds, and they are clearly differentiated:\n\nThe above spectrogram of the word “seven” was produced using Praats default of 5 milliseconds. What happens if we use 30 milliseconds instead?\n\nWe get better frequency resolution, but at the price of lower resolution in the time domain. The window length used during preprocessing is a parameter we might want to experiment with later, when training a network.\nAnother input to the STFT to play with is the type of window used to weight the samples in a time slice. Here again are three spectrograms of the above recording of seven, using, respectively, a Hamming, a Hann, and a Gaussian window:\n\nWhile the spectrograms using the Hann and Gaussian windows don’t look much different, the Hamming window seems to have introduced some artifacts.\nBeyond the spectrogram: Mel scale and Mel-Frequency Cepstral Coefficients (MFCCs)\nPreprocessing options don’t end with the spectrogram. A popular transformation applied to the spectrogram is conversion to mel scale, a scale based on how humans actually perceive differences in pitch. We don’t elaborate further on this here,5 but we do briefly comment on the respective TensorFlow code below, in case you’d like to experiment with this. In the past, coefficients transformed to Mel scale have sometimes been further processed to obtain the so-called Mel-Frequency Cepstral Coefficients (MFCCs). Again, we just show the code. For excellent reading on Mel scale conversion and MFCCs (including the reason why MFCCs are less often used nowadays) see this post by Haytham Fayek.\nBack to our original task of speech classification. Now that we’ve gained a bit of insight in what is involved, let’s see how to perform these transformations in TensorFlow.\nPreprocessing for audio classification using TensorFlow\nCode will be represented in snippets according to the functionality it provides, so we may directly map it to what was explained conceptually above. A complete example is available here. The complete example builds on Daniel’s original code as much as possible,6 with two exceptions:\nThe code runs in eager as well as in static graph mode. If you decide you only ever need eager mode, there are a few places that can be simplified. This is partly related to the fact that in eager mode, TensorFlow operations in place of tensors return values, which we can directly pass on to TensorFlow functions expecting values, not tensors. In addition, less conversion code is needed when manipulating intermediate values in R.\nWith TensorFlow 1.13 being released any day, and preparations for TF 2.0 running at full speed, we want the code to necessitate as few modifications as possible to run on the next major version of TF. One big difference is that there will no longer be a contrib module. In the original post, contrib was used to read in the .wav files as well as compute the spectrograms. Here, we will use functionality from tf.audio and tf.signal instead.\nAll operations shown below will run inside tf.dataset code, which on the R side is accomplished using the tfdatasets package. To explain the individual operations, we look at a single file, but later we’ll also display the data generator as a whole.\nFor stepping through individual lines, it’s always helpful to have eager mode enabled, independently of whether ultimately we’ll execute in eager or graph mode:\n\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\n\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nWe pick a random .wav file and decode it using tf$audio$decode_wav.7This will give us access to two tensors: the samples themselves, and the sampling rate.\n\n\nfname <- \"data/speech_commands_v0.01/bird/00b01445_nohash_0.wav\"\nwav <- tf$audio$decode_wav(tf$read_file(fname))\n\nwav$sample_rate contains the sampling rate. As expected, it is 16000, or 16kHz:\n\n\nsampling_rate <- wav$sample_rate %>% as.numeric()\nsampling_rate\n\n\n16000\nThe samples themselves are accessible as wav$audio, but their shape is (16000, 1), so we have to transpose the tensor to get the usual (batch_size, number of samples) format we need for further processing.\n\n\nsamples <- wav$audio\nsamples <- samples %>% tf$transpose(perm = c(1L, 0L))\nsamples\n\n\ntf.Tensor(\n[[-0.00750732  0.04653931  0.02041626 ... -0.01004028 -0.01300049\n  -0.00250244]], shape=(1, 16000), dtype=float32)\nComputing the spectogram\nTo compute the spectrogram, we use tf$signal$stft (where stft stands for Short Time Fourier Transform). stft expects three non-default arguments: Besides the input signal itself, there are the window size, frame_length, and the stride to use when determining the overlapping windows, frame_step. Both are expressed in units of number of samples. So if we decide on a window length of 30 milliseconds and a stride of 10 milliseconds …\n\n\nwindow_size_ms <- 30\nwindow_stride_ms <- 10\n\n… we arrive at the following call:\n\n\nsamples_per_window <- sampling_rate * window_size_ms/1000 \nstride_samples <-  sampling_rate * window_stride_ms/1000 \n\nstft_out <- tf$signal$stft(\n  samples,\n  frame_length = as.integer(samples_per_window),\n  frame_step = as.integer(stride_samples)\n)\n\nInspecting the tensor we got back, stft_out, we see, for our single input wave, a matrix of 98 x 257 complex values:\n\ntf.Tensor(\n[[[ 1.03279948e-04+0.00000000e+00j -1.95371482e-04-6.41121820e-04j\n   -1.60833192e-03+4.97534114e-04j ... -3.61620914e-05-1.07343149e-04j\n   -2.82576875e-05-5.88812982e-05j  2.66879797e-05+0.00000000e+00j] \n   ... \n   ]],\nshape=(1, 98, 257), dtype=complex64)\nHere 98 is the number of periods, which we can compute in advance, based on the number of samples in a window and the size of the stride:8\n\n\nn_periods <- length(seq(samples_per_window/2, sampling_rate - samples_per_window/2, stride_samples))\n\n257 is the number of frequencies we obtained magnitudes for. By default, stft will apply a Fast Fourier Transform of size smallest power of 2 greater or equal to the number of samples in a window,9 and then return the fft_length / 2 + 1 unique components of the FFT: the zero-frequency term and the positive-frequency terms.10\nIn our case, the number of samples in a window is 480. The nearest enclosing power of 2 being 512, we end up with 512/2 + 1 = 257 coefficients. This too we can compute in advance:11\n\n\nfft_size <- as.integer(2^trunc(log(samples_per_window, 2)) + 1) \n\nBack to the output of the STFT. Taking the elementwise magnitude of the complex values, we obtain an energy spectrogram:\n\n\nmagnitude_spectrograms <- tf$abs(stft_out)\n\nIf we stop preprocessing here, we will usually want to log transform the values to better match the sensitivity of the human auditory system:\n\n\nlog_magnitude_spectrograms = tf$log(magnitude_spectrograms + 1e-6)\n\nMel spectrograms and Mel-Frequency Cepstral Coefficients (MFCCs)\nIf instead we choose to use Mel spectrograms, we can obtain a transformation matrix that will convert the original spectrograms to Mel scale:\n\n\nlower_edge_hertz <- 0\nupper_edge_hertz <- 2595 * log10(1 + (sampling_rate/2)/700)\nnum_mel_bins <- 64L\nnum_spectrogram_bins <- magnitude_spectrograms$shape[-1]$value\n\nlinear_to_mel_weight_matrix <- tf$signal$linear_to_mel_weight_matrix(\n  num_mel_bins,\n  num_spectrogram_bins,\n  sampling_rate,\n  lower_edge_hertz,\n  upper_edge_hertz\n)\n\nApplying that matrix, we obtain a tensor of size (batch_size, number of periods, number of Mel coefficients) which again, we can log-compress if we want:\n\n\nmel_spectrograms <- tf$tensordot(magnitude_spectrograms, linear_to_mel_weight_matrix, 1L)\nlog_mel_spectrograms <- tf$log(mel_spectrograms + 1e-6)\n\nJust for completeness’ sake, finally we show the TensorFlow code used to further compute MFCCs. We don’t include this in the complete example as with MFCCs, we would need a different network architecture.\n\n\nnum_mfccs <- 13\nmfccs <- tf$signal$mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[, , 1:num_mfccs]\n\nAccommodating different-length inputs\nIn our complete example, we determine the sampling rate from the first file read, thus assuming all recordings have been sampled at the same rate. We do allow for different lengths though. For example in our dataset, had we used this file, just 0.65 seconds long, for demonstration purposes:\n\n\nfname <- \"data/speech_commands_v0.01/bird/1746d7b6_nohash_0.wav\"\n\nwe’d have ended up with just 63 periods in the spectrogram. As we have to define a fixed input_size for the first conv layer, we need to pad the corresponding dimension to the maximum possible length, which is n_periods computed above. The padding actually takes place as part of dataset definition. Let’s quickly see dataset definition as a whole, leaving out the possible generation of Mel spectrograms.12\n\n\ndata_generator <- function(df,\n                           window_size_ms,\n                           window_stride_ms) {\n  \n  # assume sampling rate is the same in all samples\n  sampling_rate <-\n    tf$audio$decode_wav(tf$read_file(tf$reshape(df$fname[[1]], list()))) %>% .$sample_rate\n  \n  samples_per_window <- (sampling_rate * window_size_ms) %/% 1000L  \n  stride_samples <-  (sampling_rate * window_stride_ms) %/% 1000L   \n  \n  n_periods <-\n    tf$shape(\n      tf$range(\n        samples_per_window %/% 2L,\n        16000L - samples_per_window %/% 2L,\n        stride_samples\n      )\n    )[1] + 1L\n  \n  n_fft_coefs <-\n    (2 ^ tf$ceil(tf$log(\n      tf$cast(samples_per_window, tf$float32)\n    ) / tf$log(2)) /\n      2 + 1L) %>% tf$cast(tf$int32)\n  \n  ds <- tensor_slices_dataset(df) %>%\n    dataset_shuffle(buffer_size = buffer_size)\n  \n  ds <- ds %>%\n    dataset_map(function(obs) {\n      wav <-\n        tf$audio$decode_wav(tf$read_file(tf$reshape(obs$fname, list())))\n      samples <- wav$audio\n      samples <- samples %>% tf$transpose(perm = c(1L, 0L))\n      \n      stft_out <- tf$signal$stft(samples,\n                                 frame_length = samples_per_window,\n                                 frame_step = stride_samples)\n      \n      magnitude_spectrograms <- tf$abs(stft_out)\n      log_magnitude_spectrograms <- tf$log(magnitude_spectrograms + 1e-6)\n      \n      response <- tf$one_hot(obs$class_id, 30L)\n\n      input <- tf$transpose(log_magnitude_spectrograms, perm = c(1L, 2L, 0L))\n      list(input, response)\n    })\n  \n  ds <- ds %>%\n    dataset_repeat()\n  \n  ds %>%\n    dataset_padded_batch(\n      batch_size = batch_size,\n      padded_shapes = list(tf$stack(list(\n        n_periods, n_fft_coefs,-1L\n      )),\n      tf$constant(-1L, shape = shape(1L))),\n      drop_remainder = TRUE\n    )\n}\n\nThe logic is the same as described above, only the code has been generalized to work in eager as well as graph mode. The padding is taken care of by dataset_padded_batch(), which needs to be told the maximum number of periods and the maximum number of coefficients.\nTime for experimentation\nBuilding on the complete example, now is the time for experimentation: How do different window sizes affect classification accuracy? Does transformation to the mel scale yield improved results?13 You might also want to try passing a non-default window_fn to stft (the default being the Hann window) and see how that affects the results. And of course, the straightforward definition of the network leaves a lot of room for improvement.\nWrapping up\nSpeaking of the network: Now that we’ve gained more insight into what is contained in a spectrogram, we might start asking, is a convnet really an adequate solution here? Normally we use convnets on images: two-dimensional data where both dimensions represent the same kind of information. Thus with images, it is natural to have square filter kernels. In a spectrogram though, the time axis and the frequency axis represent fundamentally different types of information, and it is not clear at all that we should treat them equally. Also, whereas in images, the translation invariance of convnets is a desired feature, this is not the case for the frequency axis in a spectrogram.\nClosing the circle, we discover that due to deeper knowledge about the subject domain, we are in a better position to reason about (hopefully) successful network architectures. We leave it to the creativity of our readers to continue the search…\n\n\nWarden, P. 2018. “Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.” ArXiv E-Prints, April. https://arxiv.org/abs/1804.03209.\n\n\nAs well as TensorFlow 2.0, more or less.↩︎\nReferring to the Fourier Transform. To cite an authority for this characterization, it is e.g. found in Brad Osgood’s lecture on The Fourier Transform and its Applications.↩︎\nTo display these sound waves, and later on to create spectrograms, we use Praat, a speech analysis and synthesis program that has a lot more functionality than what we’re making use of here.↩︎\nIn practice, working with datasets created for speech analysis, there will be no problems due to low sampling rates (the topic we talk about below). However, the topic is too essential - and interesting! - to skip over in an introductory post like this one.)↩︎\nCf. discussions about the validity of the original experiments.↩︎\nIn particular, we’re leaving the convnet and training code itself nearly unchanged.↩︎\nAs of this writing, tf.audio is only available in the TensorFlow nightly builds. If the decode_wav line fails, simply replace tf$audio by tf$contrib$framework$python$ops$audio_ops.↩︎\nThis code, taken from the original post, is applicable when executing eagerly or when working “on the R side”. The complete example, which dynamically determines the sampling rate and performs all operations so they work inside a static TensorFlow graph, has a more intimidating-looking equivalent that essentially does the same thing.↩︎\nIn the former case, the samples dimension in the time domain will be padded with zeros.↩︎\nFor real signals, the negative-frequency terms are redundant.↩︎\nAgain, the code in the full example looks a bit more involved because it is supposed to be runnable on a static TensorFlow graph.↩︎\nWhich is contained in the complete example code, though.↩︎\nFor us, it didn’t.↩︎\n",
    "preview": "posts/2019-02-07-audio-background/images/seven2.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 1714,
    "preview_height": 846
  },
  {
    "path": "posts/2019-01-24-vq-vae/",
    "title": "Discrete Representation Learning with VQ-VAE and TensorFlow Probability",
    "description": "Mostly when thinking of Variational Autoencoders (VAEs), we picture the prior as an isotropic Gaussian. But this is by no means a necessity. The Vector Quantised Variational Autoencoder (VQ-VAE) described in van den Oord et al's \"Neural Discrete Representation Learning\" features a discrete latent space that allows to learn impressively concise latent representations. In this post, we combine elements of Keras, TensorFlow, and TensorFlow Probability to see if we can generate convincing letters resembling those in Kuzushiji-MNIST.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-01-24",
    "categories": [
      "TensorFlow/Keras",
      "Probabilistic ML/DL",
      "Unsupervised Learning"
    ],
    "contents": "\nAbout two weeks ago, we introduced TensorFlow Probability (TFP), showing how to create and sample from distributions and put them to use in a Variational Autoencoder (VAE) that learns its prior. Today, we move on to a different specimen in the VAE model zoo: the Vector Quantised Variational Autoencoder (VQ-VAE) described in Neural Discrete Representation Learning (Oord, Vinyals, and Kavukcuoglu 2017). This model differs from most VAEs in that its approximate posterior is not continuous, but discrete - hence the “quantised” in the article’s title. We’ll quickly look at what this means, and then dive directly into the code, combining Keras layers, eager execution, and TFP.\nDiscrete codes\nMany phenomena are best thought of, and modeled, as discrete. This holds for phonemes and lexemes in language, higher-level structures in images (think objects instead of pixels),and tasks that necessitate reasoning and planning. The latent code used in most VAEs, however, is continuous - usually it’s a multivariate Gaussian. Continuous-space VAEs have been found very successful in reconstructing their input, but often they suffer from something called posterior collapse: The decoder is so powerful that it may create realistic output given just any input. This means there is no incentive to learn an expressive latent space.\nIn VQ-VAE, however, each input sample gets mapped deterministically to one of a set of embedding vectors.1 Together, these embedding vectors constitute the prior for the latent space. As such, an embedding vector contains a lot more information than a mean and a variance, and thus, is much harder to ignore by the decoder.\nThe question then is: Where is that magical hat, for us to pull out meaningful embeddings?\nLearning a discrete embedding space\nFrom the above conceptual description, we now have two questions to answer. First, by what mechanism do we assign input samples (that went through the encoder) to appropriate embedding vectors? And second: How can we learn embedding vectors that actually are useful representations - that when fed to a decoder, will result in entities perceived as belonging to the same species?\nAs regards assignment, a tensor emitted from the encoder is simply mapped to its nearest neighbor in embedding space, using Euclidean distance. The embedding vectors are then updated using exponential moving averages.2 As we’ll see soon, this means that they are actually not being learned using gradient descent - a feature worth pointing out as we don’t come across it every day in deep learning.\nConcretely, how then should the loss function and training process look? This will probably easiest be seen in code.\nCoding the VQ-VAE\nThe complete code for this example, including utilities for model saving and image visualization, is available on github as part of the Keras examples. Order of presentation here may differ from actual execution order for expository purposes, so please to actually run the code consider making use of the example on github.\nSetup and data loading\nAs in all our prior posts on VAEs, we use eager execution, which presupposes the TensorFlow implementation of Keras.\n\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\ntfp <- import(\"tensorflow_probability\")\ntfd <- tfp$distributions\n\nlibrary(tfdatasets)\nlibrary(dplyr)\nlibrary(glue)\n# used for set_defaults; please get the development version:\n# devtools::install_github(\"thomasp85/curry\")\nlibrary(curry) \n\nAs in our previous post on doing VAE with TFP, we’ll use Kuzushiji-MNIST(Clanuwat et al. 2018) as input. Now is the time to look at what we ended up generating that time and place your bet: How will that compare against the discrete latent space of VQ-VAE?\n\n\nnp <- import(\"numpy\")\n \nkuzushiji <- np$load(\"kmnist-train-imgs.npz\")\nkuzushiji <- kuzushiji$get(\"arr_0\")\n\ntrain_images <- kuzushiji %>%\n  k_expand_dims() %>%\n  k_cast(dtype = \"float32\")\n\ntrain_images <- train_images %>% `/`(255)\n\nbuffer_size <- 60000\nbatch_size <- 64\nnum_examples_to_generate <- batch_size\n\nbatches_per_epoch <- buffer_size / batch_size\n\ntrain_dataset <- tensor_slices_dataset(train_images) %>%\n  dataset_shuffle(buffer_size) %>%\n  dataset_batch(batch_size, drop_remainder = TRUE)\n\nHyperparameters\nIn addition to the “usual” hyperparameters we have in deep learning, the VQ-VAE infrastructure introduces a few model-specific ones. First of all, the embedding space is of dimensionality number of embedding vectors times embedding vector size:\n\n\n# number of embedding vectors\nnum_codes <- 64L\n# dimensionality of the embedding vectors\ncode_size <- 16L\n\nThe latent space in our example will be of size one, that is, we have a single embedding vector representing the latent code for each input sample. This will be fine for our dataset, but it should be noted that van den Oord et al. used far higher-dimensional latent spaces on e.g. ImageNet and Cifar-10.3\n\n\nlatent_size <- 1\n\nEncoder model\nThe encoder uses convolutional layers to extract image features. Its output is a 3-d tensor of shape batchsize * 1 * code_size.\n\n\nactivation <- \"elu\"\n# modularizing the code just a little bit\ndefault_conv <- set_defaults(layer_conv_2d, list(padding = \"same\", activation = activation))\n\n\n\nbase_depth <- 32\n\nencoder_model <- function(name = NULL,\n                          code_size) {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$conv1 <- default_conv(filters = base_depth, kernel_size = 5)\n    self$conv2 <- default_conv(filters = base_depth, kernel_size = 5, strides = 2)\n    self$conv3 <- default_conv(filters = 2 * base_depth, kernel_size = 5)\n    self$conv4 <- default_conv(filters = 2 * base_depth, kernel_size = 5, strides = 2)\n    self$conv5 <- default_conv(filters = 4 * latent_size, kernel_size = 7, padding = \"valid\")\n    self$flatten <- layer_flatten()\n    self$dense <- layer_dense(units = latent_size * code_size)\n    self$reshape <- layer_reshape(target_shape = c(latent_size, code_size))\n    \n    function (x, mask = NULL) {\n      x %>% \n        # output shape:  7 28 28 32 \n        self$conv1() %>% \n        # output shape:  7 14 14 32 \n        self$conv2() %>% \n        # output shape:  7 14 14 64 \n        self$conv3() %>% \n        # output shape:  7 7 7 64 \n        self$conv4() %>% \n        # output shape:  7 1 1 4 \n        self$conv5() %>% \n        # output shape:  7 4 \n        self$flatten() %>% \n        # output shape:  7 16 \n        self$dense() %>% \n        # output shape:  7 1 16\n        self$reshape()\n    }\n  })\n}\n\nAs always, let’s make use of the fact that we’re using eager execution, and see a few example outputs.\n\n\niter <- make_iterator_one_shot(train_dataset)\nbatch <-  iterator_get_next(iter)\n\nencoder <- encoder_model(code_size = code_size)\nencoded  <- encoder(batch)\nencoded\n\n\ntf.Tensor(\n[[[ 0.00516277 -0.00746826  0.0268365  ... -0.012577   -0.07752544\n   -0.02947626]]\n...\n\n [[-0.04757921 -0.07282603 -0.06814402 ... -0.10861694 -0.01237121\n    0.11455103]]], shape=(64, 1, 16), dtype=float32)\nNow, each of these 16d vectors needs to be mapped to the embedding vector it is closest to. This mapping is taken care of by another model: vector_quantizer.\nVector quantizer model\nThis is how we will instantiate the vector quantizer:\n\n\nvector_quantizer <- vector_quantizer_model(num_codes = num_codes, code_size = code_size)\n\nThis model serves two purposes: First, it acts as a store for the embedding vectors. Second, it matches encoder output to available embeddings.\nHere, the current state of embeddings is stored in codebook. ema_means and ema_count are for bookkeeping purposes only (note how they are set to be non-trainable). We’ll see them in use shortly.\n\n\nvector_quantizer_model <- function(name = NULL, num_codes, code_size) {\n  \n    keras_model_custom(name = name, function(self) {\n      \n      self$num_codes <- num_codes\n      self$code_size <- code_size\n      self$codebook <- tf$get_variable(\n        \"codebook\",\n        shape = c(num_codes, code_size), \n        dtype = tf$float32\n        )\n      self$ema_count <- tf$get_variable(\n        name = \"ema_count\", shape = c(num_codes),\n        initializer = tf$constant_initializer(0),\n        trainable = FALSE\n        )\n      self$ema_means = tf$get_variable(\n        name = \"ema_means\",\n        initializer = self$codebook$initialized_value(),\n        trainable = FALSE\n        )\n      \n      function (x, mask = NULL) { \n        \n        # to be filled in shortly ...\n        \n      }\n    })\n}\n\nIn addition to the actual embeddings, in its call method vector_quantizer holds the assignment logic. First, we compute the Euclidean distance of each encoding to the vectors in the codebook (tf$norm). We assign each encoding to the closest as by that distance embedding (tf$argmin) and one-hot-encode the assignments (tf$one_hot). Finally, we isolate the corresponding vector by masking out all others and summing up what’s left over (multiplication followed by tf$reduce_sum).\nRegarding the axis argument used with many TensorFlow functions, please take into consideration that in contrast to their k_* siblings, raw TensorFlow (tf$*) functions expect axis numbering to be 0-based. We also have to add the L’s after the numbers to conform to TensorFlow’s datatype requirements.\n\n\nvector_quantizer_model <- function(name = NULL, num_codes, code_size) {\n  \n    keras_model_custom(name = name, function(self) {\n      \n      # here we have the above instance fields\n      \n      function (x, mask = NULL) {\n    \n        # shape: bs * 1 * num_codes\n         distances <- tf$norm(\n          tf$expand_dims(x, axis = 2L) -\n            tf$reshape(self$codebook, \n                       c(1L, 1L, self$num_codes, self$code_size)),\n                       axis = 3L \n        )\n        \n        # bs * 1\n        assignments <- tf$argmin(distances, axis = 2L)\n        \n        # bs * 1 * num_codes\n        one_hot_assignments <- tf$one_hot(assignments, depth = self$num_codes)\n        \n        # bs * 1 * code_size\n        nearest_codebook_entries <- tf$reduce_sum(\n          tf$expand_dims(\n            one_hot_assignments, -1L) * \n            tf$reshape(self$codebook, c(1L, 1L, self$num_codes, self$code_size)),\n                       axis = 2L \n                       )\n        list(nearest_codebook_entries, one_hot_assignments)\n      }\n    })\n  }\n\nNow that we’ve seen how the codes are stored, let’s add functionality for updating them. As we said above, they are not learned via gradient descent. Instead, they are exponential moving averages, continually updated by whatever new “class member” they get assigned.\nSo here is a function update_ema that will take care of this.\nupdate_ema uses TensorFlow moving_averages to\nfirst, keep track of the number of currently assigned samples per code (updated_ema_count), and\nsecond, compute and assign the current exponential moving average (updated_ema_means).\n\n\nmoving_averages <- tf$python$training$moving_averages\n\n# decay to use in computing exponential moving average\ndecay <- 0.99\n\nupdate_ema <- function(\n  vector_quantizer,\n  one_hot_assignments,\n  codes,\n  decay) {\n \n  updated_ema_count <- moving_averages$assign_moving_average(\n    vector_quantizer$ema_count,\n    tf$reduce_sum(one_hot_assignments, axis = c(0L, 1L)),\n    decay,\n    zero_debias = FALSE\n  )\n\n  updated_ema_means <- moving_averages$assign_moving_average(\n    vector_quantizer$ema_means,\n    # selects all assigned values (masking out the others) and sums them up over the batch\n    # (will be divided by count later, so we get an average)\n    tf$reduce_sum(\n      tf$expand_dims(codes, 2L) *\n        tf$expand_dims(one_hot_assignments, 3L), axis = c(0L, 1L)),\n    decay,\n    zero_debias = FALSE\n  )\n\n  updated_ema_count <- updated_ema_count + 1e-5\n  updated_ema_means <-  updated_ema_means / tf$expand_dims(updated_ema_count, axis = -1L)\n  \n  tf$assign(vector_quantizer$codebook, updated_ema_means)\n}\n\nBefore we look at the training loop, let’s quickly complete the scene adding in the last actor, the decoder.\nDecoder model\nThe decoder is pretty standard, performing a series of deconvolutions and finally, returning a probability for each image pixel.\n\n\ndefault_deconv <- set_defaults(\n  layer_conv_2d_transpose,\n  list(padding = \"same\", activation = activation)\n)\n\ndecoder_model <- function(name = NULL,\n                          input_size,\n                          output_shape) {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$reshape1 <- layer_reshape(target_shape = c(1, 1, input_size))\n    self$deconv1 <-\n      default_deconv(\n        filters = 2 * base_depth,\n        kernel_size = 7,\n        padding = \"valid\"\n      )\n    self$deconv2 <-\n      default_deconv(filters = 2 * base_depth, kernel_size = 5)\n    self$deconv3 <-\n      default_deconv(\n        filters = 2 * base_depth,\n        kernel_size = 5,\n        strides = 2\n      )\n    self$deconv4 <-\n      default_deconv(filters = base_depth, kernel_size = 5)\n    self$deconv5 <-\n      default_deconv(filters = base_depth,\n                     kernel_size = 5,\n                     strides = 2)\n    self$deconv6 <-\n      default_deconv(filters = base_depth, kernel_size = 5)\n    self$conv1 <-\n      default_conv(filters = output_shape[3],\n                   kernel_size = 5,\n                   activation = \"linear\")\n    \n    function (x, mask = NULL) {\n      \n      x <- x %>%\n        # output shape:  7 1 1 16\n        self$reshape1() %>%\n        # output shape:  7 7 7 64\n        self$deconv1() %>%\n        # output shape:  7 7 7 64\n        self$deconv2() %>%\n        # output shape:  7 14 14 64\n        self$deconv3() %>%\n        # output shape:  7 14 14 32\n        self$deconv4() %>%\n        # output shape:  7 28 28 32\n        self$deconv5() %>%\n        # output shape:  7 28 28 32\n        self$deconv6() %>%\n        # output shape:  7 28 28 1\n        self$conv1()\n      \n      tfd$Independent(tfd$Bernoulli(logits = x),\n                      reinterpreted_batch_ndims = length(output_shape))\n    }\n  })\n}\n\ninput_shape <- c(28, 28, 1)\ndecoder <- decoder_model(input_size = latent_size * code_size,\n                         output_shape = input_shape)\n\nNow we’re ready to train. One thing we haven’t really talked about yet is the cost function: Given the differences in architecture (compared to standard VAEs), will the losses still look as expected (the usual add-up of reconstruction loss and KL divergence)? We’ll see that in a second.\nTraining loop\nHere’s the optimizer we’ll use. Losses will be calculated inline.\n\n\noptimizer <- tf$train$AdamOptimizer(learning_rate = learning_rate)\n\nThe training loop, as usual, is a loop over epochs, where each iteration is a loop over batches obtained from the dataset. For each batch, we have a forward pass, recorded by a gradientTape, based on which we calculate the loss. The tape will then determine the gradients of all trainable weights throughout the model, and the optimizer will use those gradients to update the weights.\nSo far, all of this conforms to a scheme we’ve oftentimes seen before. One point to note though: In this same loop, we also call update_ema to recalculate the moving averages, as those are not operated on during backprop. Here is the essential functionality:4\n\n\nnum_epochs <- 20\n\nfor (epoch in seq_len(num_epochs)) {\n  \n  iter <- make_iterator_one_shot(train_dataset)\n  \n  until_out_of_range({\n    \n    x <-  iterator_get_next(iter)\n    with(tf$GradientTape(persistent = TRUE) %as% tape, {\n      \n      # do forward pass\n      # calculate losses\n      \n    })\n    \n    encoder_gradients <- tape$gradient(loss, encoder$variables)\n    decoder_gradients <- tape$gradient(loss, decoder$variables)\n    \n    optimizer$apply_gradients(purrr::transpose(list(\n      encoder_gradients, encoder$variables\n    )),\n    global_step = tf$train$get_or_create_global_step())\n    \n    optimizer$apply_gradients(purrr::transpose(list(\n      decoder_gradients, decoder$variables\n    )),\n    global_step = tf$train$get_or_create_global_step())\n    \n    update_ema(vector_quantizer,\n               one_hot_assignments,\n               codes,\n               decay)\n\n    # periodically display some generated images\n    # see code on github \n    # visualize_images(\"kuzushiji\", epoch, reconstructed_images, random_images)\n  })\n}\n\nNow, for the actual action. Inside the context of the gradient tape, we first determine which encoded input sample gets assigned to which embedding vector.\n\n\ncodes <- encoder(x)\nc(nearest_codebook_entries, one_hot_assignments) %<-% vector_quantizer(codes)\n\nNow, for this assignment operation there is no gradient. Instead what we can do is pass the gradients from decoder input straight through to encoder output. Here tf$stop_gradient exempts nearest_codebook_entries from the chain of gradients, so encoder and decoder are linked by codes:\n\n\ncodes_straight_through <- codes + tf$stop_gradient(nearest_codebook_entries - codes)\ndecoder_distribution <- decoder(codes_straight_through)\n\nIn sum, backprop will take care of the decoder’s as well as the encoder’s weights, whereas the latent embeddings are updated using moving averages, as we’ve seen already.\nNow we’re ready to tackle the losses. There are three components:\nFirst, the reconstruction loss, which is just the log probability of the actual input under the distribution learned by the decoder.\n\n\nreconstruction_loss <- -tf$reduce_mean(decoder_distribution$log_prob(x))\n\nSecond, we have the commitment loss, defined as the mean squared deviation of the encoded input samples from the nearest neighbors they’ve been assigned to: We want the network to “commit” to a concise set of latent codes!\n\n\ncommitment_loss <- tf$reduce_mean(tf$square(codes - tf$stop_gradient(nearest_codebook_entries)))\n\nFinally, we have the usual KL diverge to a prior. As, a priori, all assignments are equally probable, this component of the loss is constant and can oftentimes be dispensed of. We’re adding it here mainly for illustrative purposes.\n\n\nprior_dist <- tfd$Multinomial(\n  total_count = 1,\n  logits = tf$zeros(c(latent_size, num_codes))\n  )\nprior_loss <- -tf$reduce_mean(\n  tf$reduce_sum(prior_dist$log_prob(one_hot_assignments), 1L)\n  )\n\nSumming up all three components, we arrive at the overall loss:5\n\n\nbeta <- 0.25\nloss <- reconstruction_loss + beta * commitment_loss + prior_loss\n\nBefore we look at the results, let’s see what happens inside gradientTape at a single glance:\n\n\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\n      \n  codes <- encoder(x)\n  c(nearest_codebook_entries, one_hot_assignments) %<-% vector_quantizer(codes)\n  codes_straight_through <- codes + tf$stop_gradient(nearest_codebook_entries - codes)\n  decoder_distribution <- decoder(codes_straight_through)\n      \n  reconstruction_loss <- -tf$reduce_mean(decoder_distribution$log_prob(x))\n  commitment_loss <- tf$reduce_mean(tf$square(codes - tf$stop_gradient(nearest_codebook_entries)))\n  prior_dist <- tfd$Multinomial(\n    total_count = 1,\n    logits = tf$zeros(c(latent_size, num_codes))\n  )\n  prior_loss <- -tf$reduce_mean(tf$reduce_sum(prior_dist$log_prob(one_hot_assignments), 1L))\n  \n  loss <- reconstruction_loss + beta * commitment_loss + prior_loss\n})\n\nResults\nAnd here we go. This time, we can’t have the 2d “morphing view” one generally likes to display with VAEs (there just is no 2d latent space). Instead, the two images below are (1) letters generated from random input and (2) reconstructed actual letters, each saved after training for nine epochs.\nLeft: letters generated from random input. Right: reconstructed input letters.Two things jump to the eye: First, the generated letters are significantly sharper than their continuous-prior counterparts (from the previous post). And second, would you have been able to tell the random image from the reconstruction image?\nConclusion\nAt this point, we’ve hopefully convinced you of the power and effectiveness of this discrete-latents approach. However, you might secretly have hoped we’d apply this to more complex data, such as the elements of speech we mentioned in the introduction, or higher-resolution images as found in ImageNet.6\nThe truth is that there’s a continuous tradeoff between the number of new and exciting techniques we can show, and the time we can spend on iterations to successfully apply these techniques to complex datasets. In the end it’s you, our readers, who will put these techniques to meaningful use on relevant, real world data.\n\n\nClanuwat, Tarin, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. 2018. “Deep Learning for Classical Japanese Literature.” December 3, 2018. http://arxiv.org/abs/cs.CV/1812.01718.\n\n\nOord, Aaron van den, Oriol Vinyals, and Koray Kavukcuoglu. 2017. “Neural Discrete Representation Learning.” CoRR abs/1711.00937. http://arxiv.org/abs/1711.00937.\n\n\nAssuming a 1d latent space, that is. The authors actually used 1d, 2d and 3d spaces in their experiments.↩︎\nIn the paper, the authors actually mention this as one of two ways to learn the prior, the other one being vector quantisation.↩︎\nTo be specific, the authors indicate that they used a field of 32 x 32 latents for ImageNet, and 8 x 8 x 10 for CIFAR10.↩︎\nThe code on github additionally contains functionality to display generated images, output the losses, and save checkpoints.↩︎\nHere beta is a scaling parameter found surprisingly unimportant by the paper authors.↩︎\nAlthough we have to say we find that Kuzushiji-MNIST beats MNIST by far, in complexity and aesthetics!↩︎\n",
    "preview": "posts/2019-01-24-vq-vae/images/thumb1.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 510,
    "preview_height": 287
  },
  {
    "path": "posts/2019-01-08-getting-started-with-tf-probability/",
    "title": "Getting started with TensorFlow Probability from R",
    "description": "TensorFlow Probability offers a vast range of functionality ranging from distributions over probabilistic network layers to probabilistic inference. It works seamlessly with core TensorFlow and (TensorFlow) Keras. In this post, we provide a short introduction to the distributions layer and then, use it for sampling and calculating probabilities in a Variational Autoencoder.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-01-08",
    "categories": [
      "TensorFlow/Keras",
      "Probabilistic ML/DL",
      "Unsupervised Learning"
    ],
    "contents": "\nWith the abundance of great libraries, in R, for statistical computing, why would you be interested in TensorFlow Probability (TFP, for short)? Well - let’s look at a list of its components:\nDistributions and bijectors (bijectors are reversible, composable maps)\nProbabilistic modeling (Edward2 and probabilistic network layers)\nProbabilistic inference (via MCMC or variational inference)\nNow imagine all these working seamlessly with the TensorFlow framework - core, Keras, contributed modules - and also, running distributed and on GPU. The field of possible applications is vast - and far too diverse to cover as a whole in an introductory blog post.\nInstead, our aim here is to provide a first introduction to TFP, focusing on direct applicability to and interoperability with deep learning. We’ll quickly show how to get started with one of the basic building blocks: distributions. Then, we’ll build a variational autoencoder similar to that in Representation learning with MMD-VAE. This time though, we’ll make use of TFP to sample from the prior and approximate posterior distributions.\nWe’ll regard this post as a “proof on concept” for using TFP with Keras - from R - and plan to follow up with more elaborate examples from the area of semi-supervised representation learning.\nInstalling and using TFP\nTo install TFP together with TensorFlow, simply append tensorflow-probability to the default list of extra packages:1\n\n\nlibrary(tensorflow)\ninstall_tensorflow(\n  extra_packages = c(\"keras\", \"tensorflow-hub\", \"tensorflow-probability\"),\n  version = \"1.12\"\n)\n\nNow to use TFP, all we need to do is import it and create some useful handles.\n\n\nlibrary(tensorflow)\ntfp <- import(\"tensorflow_probability\")\ntfd <- tfp$distributions\n\nAnd here we go, sampling from a standard normal distribution.\n\n\nn <- tfd$Normal(loc = 0, scale = 1)\nn$sample(6L)\n\n\ntf.Tensor(\n\"Normal_1/sample/Reshape:0\", shape=(6,), dtype=float32\n)\nNow that’s nice, but it’s 2019, we don’t want to have to create a session to evaluate these tensors anymore. In the variational autoencoder example below, we are going to see how TFP and TF eager execution are the perfect match, so why not start using it now.\nTo use eager execution, we have to execute the following lines in a fresh (R) session:\n\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\n… and import TFP, same as above.\n\n\ntfp <- import(\"tensorflow_probability\")\ntfd <- tfp$distributions\n\nNow let’s quickly look at TFP distributions.\nUsing distributions\nHere’s that standard normal again.\n\n\nn <- tfd$Normal(loc = 0, scale = 1)\n\nThings commonly done with a distribution include sampling:\n\n\n# just as in low-level tensorflow, we need to append L to indicate integer arguments\nn$sample(6L) \n\n\ntf.Tensor(\n[-0.34403768 -0.14122334 -1.3832929   1.618252    1.364448   -1.1299014 ],\nshape=(6,),\ndtype=float32\n)\nAs well as getting the log probability. Here we do that simultaneously for three values.\n\n\nn$log_prob(c(-1, 0, 1))\n\n\ntf.Tensor(\n[-1.4189385 -0.9189385 -1.4189385], shape=(3,), dtype=float32\n)\nWe can do the same things with lots of other distributions, e.g., the Bernoulli:\n\n\nb <- tfd$Bernoulli(0.9)\nb$sample(10L)\n\n\ntf.Tensor(\n[1 1 1 0 1 1 0 1 0 1], shape=(10,), dtype=int32\n)\n\n\nb$log_prob(c(0,1,0,0))\n\n\ntf.Tensor(\n[-1.2411538 -0.3411539 -1.2411538 -1.2411538], shape=(4,), dtype=float32\n)\nNote that in the last chunk, we are asking for the log probabilities of four independent draws.\nBatch shapes and event shapes\nIn TFP, we can do the following.\n\n\nns <- tfd$Normal(\n  loc = c(1, 10, -200),\n  scale = c(0.1, 0.1, 1)\n)\nns\n\n\ntfp.distributions.Normal(\n\"Normal/\", batch_shape=(3,), event_shape=(), dtype=float32\n)\nContrary to what it might look like, this is not a multivariate normal. As indicated by batch_shape=(3,), this is a “batch” of independent univariate distributions. The fact that these are univariate is seen in event_shape=(): Each of them lives in one-dimensional event space.\nIf instead we create a single, two-dimensional multivariate normal:\n\n\nn <- tfd$MultivariateNormalDiag(loc = c(0, 10), scale_diag = c(1, 4))\nn\n\n\ntfp.distributions.MultivariateNormalDiag(\n\"MultivariateNormalDiag/\", batch_shape=(), event_shape=(2,), dtype=float32\n)\nwe see batch_shape=(), event_shape=(2,), as expected.\nOf course, we can combine both, creating batches of multivariate distributions:\n\n\nnd_batch <- tfd$MultivariateNormalFullCovariance(\n  loc = list(c(0., 0.), c(1., 1.), c(2., 2.)),\n  covariance_matrix = list(\n    matrix(c(1, .1, .1, 1), ncol = 2),\n    matrix(c(1, .3, .3, 1), ncol = 2),\n    matrix(c(1, .5, .5, 1), ncol = 2))\n)\n\nThis example defines a batch of three two-dimensional multivariate normal distributions.\nConverting between batch shapes and event shapes\nStrange as it may sound, situations arise where we want to transform distribution shapes between these types - in fact, we’ll see such a case very soon.\ntfd$Independent is used to convert dimensions in batch_shape to dimensions in event_shape.\nHere is a batch of three independent Bernoulli distributions.\n\n\nbs <- tfd$Bernoulli(probs=c(.3,.5,.7))\nbs\n\n\ntfp.distributions.Bernoulli(\n\"Bernoulli/\", batch_shape=(3,), event_shape=(), dtype=int32\n)\nWe can convert this to a virtual “three-dimensional” Bernoulli like this:\n\n\nb <- tfd$Independent(bs, reinterpreted_batch_ndims = 1L)\nb\n\n\ntfp.distributions.Independent(\n\"IndependentBernoulli/\", batch_shape=(), event_shape=(3,), dtype=int32\n)\nHere reinterpreted_batch_ndims tells TFP how many of the batch dimensions are being used for the event space, starting to count from the right of the shape list.\nWith this basic understanding of TFP distributions, we’re ready to see them used in a VAE.\nVariational autoencoder using TFP\nWe’ll take the (not so) deep convolutional architecture from Representation learning with MMD-VAE and use distributions for sampling and computing probabilities. Optionally, our new VAE will be able to learn the prior distribution.\nConcretely, the following exposition will consist of three parts. First, we present common code applicable to both a VAE with a static prior, and one that learns the parameters of the prior distribution. Then, we have the training loop for the first (static-prior) VAE. Finally, we discuss the training loop and additional model involved in the second (prior-learning) VAE.\nPresenting both versions one after the other leads to code duplications, but avoids scattering confusing if-else branches throughout the code.\nThe second VAE is available as part of the Keras examples so you don’t have to copy out code snippets. The code also contains additional functionality not discussed and replicated here, such as for saving model weights.\nSo, let’s start with the common part.\nAt the risk of repeating ourselves, here again are the preparatory steps (including a few additional library loads).\n\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\ntfp <- import(\"tensorflow_probability\")\ntfd <- tfp$distributions\n\nlibrary(tfdatasets)\nlibrary(dplyr)\nlibrary(glue)\n\nDataset\nFor a change from MNIST and Fashion-MNIST, we’ll use the brand new Kuzushiji-MNIST(Clanuwat et al. 2018).\nFrom: Deep Learning for Classical Japanese Literature (Clanuwat et al. 2018)\n\nnp <- import(\"numpy\")\n\nkuzushiji <- np$load(\"kmnist-train-imgs.npz\")\nkuzushiji <- kuzushiji$get(\"arr_0\")\n \ntrain_images <- kuzushiji %>%\n  k_expand_dims() %>%\n  k_cast(dtype = \"float32\")\n\ntrain_images <- train_images %>% `/`(255)\n\nAs in that other post, we stream the data via tfdatasets:\n\nbuffer_size <- 60000\nbatch_size <- 256\nbatches_per_epoch <- buffer_size / batch_size\n\ntrain_dataset <- tensor_slices_dataset(train_images) %>%\n  dataset_shuffle(buffer_size) %>%\n  dataset_batch(batch_size)\nNow let’s see what changes in the encoder and decoder models.\nEncoder\nThe encoder differs from what we had without TFP in that it does not return the approximate posterior means and variances directly as tensors. Instead, it returns a batch of multivariate normal distributions:\n\n\n# you might want to change this depending on the dataset\nlatent_dim <- 2\n\nencoder_model <- function(name = NULL) {\n\n  keras_model_custom(name = name, function(self) {\n  \n    self$conv1 <-\n      layer_conv_2d(\n        filters = 32,\n        kernel_size = 3,\n        strides = 2,\n        activation = \"relu\"\n      )\n    self$conv2 <-\n      layer_conv_2d(\n        filters = 64,\n        kernel_size = 3,\n        strides = 2,\n        activation = \"relu\"\n      )\n    self$flatten <- layer_flatten()\n    self$dense <- layer_dense(units = 2 * latent_dim)\n    \n    function (x, mask = NULL) {\n      x <- x %>%\n        self$conv1() %>%\n        self$conv2() %>%\n        self$flatten() %>%\n        self$dense()\n        \n      tfd$MultivariateNormalDiag(\n        loc = x[, 1:latent_dim],\n        scale_diag = tf$nn$softplus(x[, (latent_dim + 1):(2 * latent_dim)] + 1e-5)\n      )\n    }\n  })\n}\n\nLet’s try this out.\n\n\nencoder <- encoder_model()\n\niter <- make_iterator_one_shot(train_dataset)\nx <-  iterator_get_next(iter)\n\napprox_posterior <- encoder(x)\napprox_posterior\n\n\ntfp.distributions.MultivariateNormalDiag(\n\"MultivariateNormalDiag/\", batch_shape=(256,), event_shape=(2,), dtype=float32\n)\n\n\napprox_posterior$sample()\n\n\ntf.Tensor(\n[[ 5.77791929e-01 -1.64988488e-02]\n [ 7.93901443e-01 -1.00042784e+00]\n [-1.56279251e-01 -4.06365871e-01]\n ...\n ...\n [-6.47531569e-01  2.10889503e-02]], shape=(256, 2), dtype=float32)\n\nWe don’t know about you, but we still enjoy the ease of inspecting values with eager execution - a lot.\nNow, on to the decoder, which too returns a distribution instead of a tensor.\nDecoder\nIn the decoder, we see why transformations between batch shape and event shape are useful. The output of self$deconv3 is four-dimensional. What we need is an on-off-probability for every pixel. Formerly, this was accomplished by feeding the tensor into a dense layer and applying a sigmoid activation. Here, we use tfd$Independent to effectively tranform the tensor into a probability distribution over three-dimensional images (width, height, channel(s)).\n\n\ndecoder_model <- function(name = NULL) {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$dense <- layer_dense(units = 7 * 7 * 32, activation = \"relu\")\n    self$reshape <- layer_reshape(target_shape = c(7, 7, 32))\n    self$deconv1 <-\n      layer_conv_2d_transpose(\n        filters = 64,\n        kernel_size = 3,\n        strides = 2,\n        padding = \"same\",\n        activation = \"relu\"\n      )\n    self$deconv2 <-\n      layer_conv_2d_transpose(\n        filters = 32,\n        kernel_size = 3,\n        strides = 2,\n        padding = \"same\",\n        activation = \"relu\"\n      )\n    self$deconv3 <-\n      layer_conv_2d_transpose(\n        filters = 1,\n        kernel_size = 3,\n        strides = 1,\n        padding = \"same\"\n      )\n    \n    function (x, mask = NULL) {\n      x <- x %>%\n        self$dense() %>%\n        self$reshape() %>%\n        self$deconv1() %>%\n        self$deconv2() %>%\n        self$deconv3()\n      \n      tfd$Independent(tfd$Bernoulli(logits = x),\n                      reinterpreted_batch_ndims = 3L)\n      \n    }\n  })\n}\n\nLet’s try this out too.\n\n\ndecoder <- decoder_model()\ndecoder_likelihood <- decoder(approx_posterior_sample)\n\n\ntfp.distributions.Independent(\n\"IndependentBernoulli/\", batch_shape=(256,), event_shape=(28, 28, 1), dtype=int32\n)\nThis distribution will be used to generate the “reconstructions”, as well as determine the loglikelihood of the original samples.\nKL loss and optimizer\nBoth VAEs discussed below will need an optimizer …\n\n\noptimizer <- tf$train$AdamOptimizer(1e-4)\n\n… and both will delegate to compute_kl_loss to compute the KL part of the loss.\nThis helper function simply subtracts the log likelihood of the samples2 under the prior from their loglikelihood under the approximate posterior.\n\n\ncompute_kl_loss <- function(\n  latent_prior,\n  approx_posterior,\n  approx_posterior_sample) {\n  \n  kl_div <- approx_posterior$log_prob(approx_posterior_sample) -\n    latent_prior$log_prob(approx_posterior_sample)\n  avg_kl_div <- tf$reduce_mean(kl_div)\n  avg_kl_div\n}\n\nNow that we’ve looked at the common parts, we first discuss how to train a VAE with a static prior.\nVAE with static prior\nIn this VAE, we use TFP to create the usual isotropic Gaussian prior. We then directly sample from this distribution in the training loop.\n\n\nlatent_prior <- tfd$MultivariateNormalDiag(\n  loc  = tf$zeros(list(latent_dim)),\n  scale_identity_multiplier = 1\n)\n\nAnd here is the complete training loop. We’ll point out the crucial TFP-related steps below.\n\n\nfor (epoch in seq_len(num_epochs)) {\n  iter <- make_iterator_one_shot(train_dataset)\n  \n  total_loss <- 0\n  total_loss_nll <- 0\n  total_loss_kl <- 0\n  \n  until_out_of_range({\n    x <-  iterator_get_next(iter)\n    \n    with(tf$GradientTape(persistent = TRUE) %as% tape, {\n      approx_posterior <- encoder(x)\n      approx_posterior_sample <- approx_posterior$sample()\n      decoder_likelihood <- decoder(approx_posterior_sample)\n      \n      nll <- -decoder_likelihood$log_prob(x)\n      avg_nll <- tf$reduce_mean(nll)\n      \n      kl_loss <- compute_kl_loss(\n        latent_prior,\n        approx_posterior,\n        approx_posterior_sample\n      )\n\n      loss <- kl_loss + avg_nll\n    })\n    \n    total_loss <- total_loss + loss\n    total_loss_nll <- total_loss_nll + avg_nll\n    total_loss_kl <- total_loss_kl + kl_loss\n    \n    encoder_gradients <- tape$gradient(loss, encoder$variables)\n    decoder_gradients <- tape$gradient(loss, decoder$variables)\n    \n    optimizer$apply_gradients(purrr::transpose(list(\n      encoder_gradients, encoder$variables\n    )),\n    global_step = tf$train$get_or_create_global_step())\n    optimizer$apply_gradients(purrr::transpose(list(\n      decoder_gradients, decoder$variables\n    )),\n    global_step = tf$train$get_or_create_global_step())\n \n  })\n  \n  cat(\n    glue(\n      \"Losses (epoch): {epoch}:\",\n      \"  {(as.numeric(total_loss_nll)/batches_per_epoch) %>% round(4)} nll\",\n      \"  {(as.numeric(total_loss_kl)/batches_per_epoch) %>% round(4)} kl\",\n      \"  {(as.numeric(total_loss)/batches_per_epoch) %>% round(4)} total\"\n    ),\n    \"\\n\"\n  )\n}\n\nAbove, playing around with the encoder and the decoder, we’ve already seen how\n\n\napprox_posterior <- encoder(x)\n\ngives us a distribution we can sample from. We use it to obtain samples from the approximate posterior:\n\n\napprox_posterior_sample <- approx_posterior$sample()\n\nThese samples, we take them and feed them to the decoder, who gives us on-off-likelihoods for image pixels.\n\n\ndecoder_likelihood <- decoder(approx_posterior_sample)\n\nNow the loss consists of the usual ELBO components: reconstruction loss and KL divergence. The reconstruction loss we directly obtain from TFP, using the learned decoder distribution to assess the likelihood of the original input.\n\n\nnll <- -decoder_likelihood$log_prob(x)\navg_nll <- tf$reduce_mean(nll)\n\nThe KL loss we get from compute_kl_loss, the helper function we saw above:\n\n\nkl_loss <- compute_kl_loss(\n        latent_prior,\n        approx_posterior,\n        approx_posterior_sample\n      )\n\nWe add both and arrive at the overall VAE loss:\n\n\nloss <- kl_loss + avg_nll\n\nApart from these changes due to using TFP, the training process is just normal backprop, the way it looks using eager execution.\nVAE with learnable prior (mixture of Gaussians)\nNow let’s see how instead of using the standard isotropic Gaussian, we could learn a mixture of Gaussians. The choice of number of distributions here is pretty arbitrary. Just as with latent_dim, you might want to experiment and find out what works best on your dataset.\n\n\nmixture_components <- 16\n\nlearnable_prior_model <- function(name = NULL, latent_dim, mixture_components) {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$loc <-\n      tf$get_variable(\n        name = \"loc\",\n        shape = list(mixture_components, latent_dim),\n        dtype = tf$float32\n      )\n    self$raw_scale_diag <- tf$get_variable(\n      name = \"raw_scale_diag\",\n      shape = c(mixture_components, latent_dim),\n      dtype = tf$float32\n    )\n    self$mixture_logits <-\n      tf$get_variable(\n        name = \"mixture_logits\",\n        shape = c(mixture_components),\n        dtype = tf$float32\n      )\n      \n    function (x, mask = NULL) {\n        tfd$MixtureSameFamily(\n          components_distribution = tfd$MultivariateNormalDiag(\n            loc = self$loc,\n            scale_diag = tf$nn$softplus(self$raw_scale_diag)\n          ),\n          mixture_distribution = tfd$Categorical(logits = self$mixture_logits)\n        )\n      }\n    })\n  }\n\nIn TFP terminology, components_distribution is the underlying distribution type, and mixture_distribution holds the probabilities that individual components are chosen.\nNote how self$loc, self$raw_scale_diag and self$mixture_logits are TensorFlow Variables and thus, persistent and updatable by backprop.\nNow we create the model.\n\n\nlatent_prior_model <- learnable_prior_model(\n  latent_dim = latent_dim,\n  mixture_components = mixture_components\n)\n\nHow do we obtain a latent prior distribution we can sample from? A bit unusually, this model will be called without an input:\n\n\nlatent_prior <- latent_prior_model(NULL)\nlatent_prior\n\n\ntfp.distributions.MixtureSameFamily(\n\"MixtureSameFamily/\", batch_shape=(), event_shape=(2,), dtype=float32\n)\nHere now is the complete training loop. Note how we have a third model to backprop through.\n\n\nfor (epoch in seq_len(num_epochs)) {\n  iter <- make_iterator_one_shot(train_dataset)\n  \n  total_loss <- 0\n  total_loss_nll <- 0\n  total_loss_kl <- 0\n  \n  until_out_of_range({\n    x <-  iterator_get_next(iter)\n    \n    with(tf$GradientTape(persistent = TRUE) %as% tape, {\n      approx_posterior <- encoder(x)\n      \n      approx_posterior_sample <- approx_posterior$sample()\n      decoder_likelihood <- decoder(approx_posterior_sample)\n      \n      nll <- -decoder_likelihood$log_prob(x)\n      avg_nll <- tf$reduce_mean(nll)\n      \n      latent_prior <- latent_prior_model(NULL)\n      \n      kl_loss <- compute_kl_loss(\n        latent_prior,\n        approx_posterior,\n        approx_posterior_sample\n      )\n\n      loss <- kl_loss + avg_nll\n    })\n    \n    total_loss <- total_loss + loss\n    total_loss_nll <- total_loss_nll + avg_nll\n    total_loss_kl <- total_loss_kl + kl_loss\n    \n    encoder_gradients <- tape$gradient(loss, encoder$variables)\n    decoder_gradients <- tape$gradient(loss, decoder$variables)\n    prior_gradients <-\n      tape$gradient(loss, latent_prior_model$variables)\n    \n    optimizer$apply_gradients(purrr::transpose(list(\n      encoder_gradients, encoder$variables\n    )),\n    global_step = tf$train$get_or_create_global_step())\n    optimizer$apply_gradients(purrr::transpose(list(\n      decoder_gradients, decoder$variables\n    )),\n    global_step = tf$train$get_or_create_global_step())\n    optimizer$apply_gradients(purrr::transpose(list(\n      prior_gradients, latent_prior_model$variables\n    )),\n    global_step = tf$train$get_or_create_global_step())\n    \n  })\n  \n  checkpoint$save(file_prefix = checkpoint_prefix)\n  \n  cat(\n    glue(\n      \"Losses (epoch): {epoch}:\",\n      \"  {(as.numeric(total_loss_nll)/batches_per_epoch) %>% round(4)} nll\",\n      \"  {(as.numeric(total_loss_kl)/batches_per_epoch) %>% round(4)} kl\",\n      \"  {(as.numeric(total_loss)/batches_per_epoch) %>% round(4)} total\"\n    ),\n    \"\\n\"\n  )\n}  \n\nAnd that’s it! For us, both VAEs yielded similar results, and we did not experience great differences from experimenting with latent dimensionality and the number of mixture distributions. But again, we wouldn’t want to generalize to other datasets, architectures, etc.\nSpeaking of results, how do they look? Here we see letters generated after 40 epochs of training. On the left are random letters, on the right, the usual VAE grid display of latent space.\n\nWrapping up\nHopefully, we’ve succeeded in showing that TensorFlow Probability, eager execution, and Keras make for an attractive combination! If you relate total amount of code required to the complexity of the task, as well as depth of the concepts involved, this should appear as a pretty concise implementation.\nIn the nearer future, we plan to follow up with more involved applications of TensorFlow Probability, mostly from the area of representation learning. Stay tuned!\n\n\nClanuwat, Tarin, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. 2018. “Deep Learning for Classical Japanese Literature.” December 3, 2018. http://arxiv.org/abs/cs.CV/1812.01718.\n\n\nAt the time we’re publishing this post, we need to pass the “version” argument to enforce compatibility between TF and TFP. This is due to a quirk in TF’s handling of embedded Python that has us temporarily install TF 1.10 by default, until that quirk is to disappear with the upcoming release of TF 1.13.↩︎\nJust to be clear: by samples here we mean “samples from the approximate posterior”↩︎\n",
    "preview": "posts/2019-01-08-getting-started-with-tf-probability/images/thumb.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 884,
    "preview_height": 584
  },
  {
    "path": "posts/2018-12-18-object-detection-concepts/",
    "title": "Concepts in object detection",
    "description": "As shown in a previous post, naming and locating a single object in an image is a task that may be approached in a straightforward way. This is not the same with general object detection, though - naming and locating several objects at once, with no prior information about how many objects are supposed to be detected.\nIn this post, we explain the steps involved in coding a basic single-shot object detector: Not unlike SSD (Single-shot Multibox Detector), but simplified and designed not for best performance, but comprehensibility.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-12-18",
    "categories": [
      "TensorFlow/Keras",
      "Image Recognition & Image Processing"
    ],
    "contents": "\n\n\n\nA few weeks ago, we provided an introduction to the task of naming and locating objects in images. Crucially, we confined ourselves to detecting a single object in an image. Reading that article, you might have thought “can’t we just extend this approach to several objects”? The short answer is, not in a straightforward way. We’ll see a longer answer shortly.\nIn this post, we want to detail one viable approach, explaining (and coding) the steps involved. We won’t, however, end up with a production-ready model. So if you read on, you won’t have a model you can export and put on your smartphone, for use in the wild. You should, however, have learned a bit about how this - object detection - is even possible. After all, it might look like magic!\nThe code below is heavily based on fast.ai’s implementation of SSD. While this is not the first time we’re “porting” fast.ai models, in this case we found differences in execution models between PyTorch and TensorFlow to be especially striking, and we will briefly touch on this in our discussion.\nSo why is object detection hard?\nAs we saw, we can classify and detect a single object as follows. We make use of a powerful feature extractor, such as Resnet 50, add a few conv layers for specialization, and then, concatenate two outputs: one that indicates class, and one that has four coordinates specifying a bounding box.\nNow, to detect several objects, can’t we just have several class outputs, and several bounding boxes? Unfortunately we can’t. Assume there are two cute cats in the image, and we have just two bounding box detectors. How does each of them know which cat to detect? What happens in practice is that both of them try to designate both cats, so we end up with two bounding boxes in the middle - where there’s no cat. It’s a bit like averaging a bimodal distribution.\nWhat can be done? Overall, there are three approaches to object detection, differing in performance in both common senses of the word: execution time and precision.\nProbably the first option you’d think of (if you haven’t been exposed to the topic before) is running the algorithm over the image piece by piece. This is called the sliding windows approach, and even though in a naive implementation, it would require excessive time, it can be run effectively if making use of fully convolutional models (cf. Overfeat (Sermanet et al. 2013)).\nCurrently the best precision is gained from region proposal approaches (R-CNN(Girshick et al. 2013), Fast R-CNN(Girshick 2015), Faster R-CNN(Ren et al. 2015)). These operate in two steps. A first step points out regions of interest in an image. Then, a convnet classifies and localizes the objects in each region. In the first step, originally non-deep-learning algorithms were used. With Faster R-CNN though, a convnet takes care of region proposal as well, such that the method now is “fully deep learning”.\nLast but not least, there is the class of single shot detectors, like YOLO(Redmon et al. 2015)(Redmon and Farhadi 2016)(Redmon and Farhadi 2018)and SSD(Liu et al. 2015). Just as Overfeat, these do a single pass only, but they add an additional feature that boosts precision: anchor boxes.\nUse of anchor boxes in SSD. Figure from (Liu et al. 2015)Anchor boxes are prototypical object shapes, arranged systematically over the image. In the simplest case, these can just be rectangles (squares) spread out systematically in a grid. A simple grid already solves the basic problem we started with, above: How does each detector know which object to detect? In a single-shot approach like SSD, each detector is mapped to - responsible for - a specific anchor box. We’ll see how this can be achieved below.\nWhat if we have several objects in a grid cell? We can assign more than one anchor box to each cell. Anchor boxes are created with different aspect ratios, to provide a good match to entities of different proportions, such as people or trees on the one hand, and bicycles or balconies on the other. You can see these different anchor boxes in the above figure, in illustrations b and c.\nNow, what if an object spans several grid cells, or even the whole image? It won’t have sufficient overlap with any of the boxes to allow for successful detection. For that reason, SSD puts detectors at several stages in the model - a set of detectors after each successive step of downscaling. We see 8x8 and 4x4 grids in the figure above.\nIn this post, we show how to code a very basic single-shot approach, inspired by SSD but not going to full lengths. We’ll have a basic 16x16 grid of uniform anchors, all applied at the same resolution. In the end, we indicate how to extend this to different aspect ratios and resolutions, focusing on the model architecture.\nA basic single-shot detector\nWe’re using the same dataset as in Naming and locating objects in images - Pascal VOC, the 2007 edition - and we start out with the same preprocessing steps, up and until we have an object imageinfo that contains, in every row, information about a single object in an image.1\nFurther preprocessing\nTo be able to detect multiple objects, we need to aggregate all information on a single image into a single row.\n\n\nimageinfo4ssd <- imageinfo %>%\n  select(category_id,\n         file_name,\n         name,\n         x_left,\n         y_top,\n         x_right,\n         y_bottom,\n         ends_with(\"scaled\"))\n\nimageinfo4ssd <- imageinfo4ssd %>%\n  group_by(file_name) %>%\n  summarise(\n    categories = toString(category_id),\n    name = toString(name),\n    xl = toString(x_left_scaled),\n    yt = toString(y_top_scaled),\n    xr = toString(x_right_scaled),\n    yb = toString(y_bottom_scaled),\n    xl_orig = toString(x_left),\n    yt_orig = toString(y_top),\n    xr_orig = toString(x_right),\n    yb_orig = toString(y_bottom),\n    cnt = n()\n  )\n\nLet’s check we got this right.\n\n\nexample <- imageinfo4ssd[5, ]\nimg <- image_read(file.path(img_dir, example$file_name))\nname <- (example$name %>% str_split(pattern = \", \"))[[1]]\nx_left <- (example$xl_orig %>% str_split(pattern = \", \"))[[1]]\nx_right <- (example$xr_orig %>% str_split(pattern = \", \"))[[1]]\ny_top <- (example$yt_orig %>% str_split(pattern = \", \"))[[1]]\ny_bottom <- (example$yb_orig %>% str_split(pattern = \", \"))[[1]]\n\nimg <- image_draw(img)\nfor (i in 1:example$cnt) {\n  rect(x_left[i],\n       y_bottom[i],\n       x_right[i],\n       y_top[i],\n       border = \"white\",\n       lwd = 2)\n  text(\n    x = as.integer(x_right[i]),\n    y = as.integer(y_top[i]),\n    labels = name[i],\n    offset = 1,\n    pos = 2,\n    cex = 1,\n    col = \"white\"\n  )\n}\ndev.off()\nprint(img)\n\n\nNow we construct the anchor boxes.\nAnchors\nLike we said above, here we will have one anchor box per cell. Thus, grid cells and anchor boxes, in our case, are the same thing, and we’ll call them by both names, interchangingly, depending on the context. Just keep in mind that in more complex models, these will most probably be different entities.\nOur grid will be of size 4x4. We will need the cells’ coordinates, and we’ll start with a center x - center y - height - width representation.\nHere, first, are the center coordinates.\n\n\ncells_per_row <- 4\ngridsize <- 1/cells_per_row\nanchor_offset <- 1 / (cells_per_row * 2) \n\nanchor_xs <- seq(anchor_offset, 1 - anchor_offset, length.out = 4) %>%\n  rep(each = cells_per_row)\nanchor_ys <- seq(anchor_offset, 1 - anchor_offset, length.out = 4) %>%\n  rep(cells_per_row)\n\nWe can plot them.\n\n\nggplot(data.frame(x = anchor_xs, y = anchor_ys), aes(x, y)) +\n  geom_point() +\n  coord_cartesian(xlim = c(0,1), ylim = c(0,1)) +\n  theme(aspect.ratio = 1)\n\n\nThe center coordinates are supplemented by height and width:\n\n\nanchor_centers <- cbind(anchor_xs, anchor_ys)\nanchor_height_width <- matrix(1 / cells_per_row, nrow = 16, ncol = 2)\n\nCombining centers, heights and widths gives us the first representation.\n\n\nanchors <- cbind(anchor_centers, anchor_height_width)\nanchors\n\n\n       [,1]  [,2] [,3] [,4]\n [1,] 0.125 0.125 0.25 0.25\n [2,] 0.125 0.375 0.25 0.25\n [3,] 0.125 0.625 0.25 0.25\n [4,] 0.125 0.875 0.25 0.25\n [5,] 0.375 0.125 0.25 0.25\n [6,] 0.375 0.375 0.25 0.25\n [7,] 0.375 0.625 0.25 0.25\n [8,] 0.375 0.875 0.25 0.25\n [9,] 0.625 0.125 0.25 0.25\n[10,] 0.625 0.375 0.25 0.25\n[11,] 0.625 0.625 0.25 0.25\n[12,] 0.625 0.875 0.25 0.25\n[13,] 0.875 0.125 0.25 0.25\n[14,] 0.875 0.375 0.25 0.25\n[15,] 0.875 0.625 0.25 0.25\n[16,] 0.875 0.875 0.25 0.25\nIn subsequent manipulations, we will sometimes we need a different representation: the corners (top-left, top-right, bottom-right, bottom-left) of the grid cells.\n\n\nhw2corners <- function(centers, height_width) {\n  cbind(centers - height_width / 2, centers + height_width / 2) %>% unname()\n}\n\n# cells are indicated by (xl, yt, xr, yb)\n# successive rows first go down in the image, then to the right\nanchor_corners <- hw2corners(anchor_centers, anchor_height_width)\nanchor_corners\n\n\n      [,1] [,2] [,3] [,4]\n [1,] 0.00 0.00 0.25 0.25\n [2,] 0.00 0.25 0.25 0.50\n [3,] 0.00 0.50 0.25 0.75\n [4,] 0.00 0.75 0.25 1.00\n [5,] 0.25 0.00 0.50 0.25\n [6,] 0.25 0.25 0.50 0.50\n [7,] 0.25 0.50 0.50 0.75\n [8,] 0.25 0.75 0.50 1.00\n [9,] 0.50 0.00 0.75 0.25\n[10,] 0.50 0.25 0.75 0.50\n[11,] 0.50 0.50 0.75 0.75\n[12,] 0.50 0.75 0.75 1.00\n[13,] 0.75 0.00 1.00 0.25\n[14,] 0.75 0.25 1.00 0.50\n[15,] 0.75 0.50 1.00 0.75\n[16,] 0.75 0.75 1.00 1.00\nLet’s take our sample image again and plot it, this time including the grid cells. Note that we display the scaled image now - the way the network is going to see it.\n\n\nexample <- imageinfo4ssd[5, ]\nname <- (example$name %>% str_split(pattern = \", \"))[[1]]\nx_left <- (example$xl %>% str_split(pattern = \", \"))[[1]]\nx_right <- (example$xr %>% str_split(pattern = \", \"))[[1]]\ny_top <- (example$yt %>% str_split(pattern = \", \"))[[1]]\ny_bottom <- (example$yb %>% str_split(pattern = \", \"))[[1]]\n\n\nimg <- image_read(file.path(img_dir, example$file_name))\nimg <- image_resize(img, geometry = \"224x224!\")\nimg <- image_draw(img)\n\nfor (i in 1:example$cnt) {\n  rect(x_left[i],\n       y_bottom[i],\n       x_right[i],\n       y_top[i],\n       border = \"white\",\n       lwd = 2)\n  text(\n    x = as.integer(x_right[i]),\n    y = as.integer(y_top[i]),\n    labels = name[i],\n    offset = 0,\n    pos = 2,\n    cex = 1,\n    col = \"white\"\n  )\n}\nfor (i in 1:nrow(anchor_corners)) {\n  rect(\n    anchor_corners[i, 1] * 224,\n    anchor_corners[i, 4] * 224,\n    anchor_corners[i, 3] * 224,\n    anchor_corners[i, 2] * 224,\n    border = \"cyan\",\n    lwd = 1,\n    lty = 3\n  )\n}\n\ndev.off()\nprint(img)\n\n\nNow it’s time to address the possibly greatest mystery when you’re new to object detection: How do you actually construct the ground truth input to the network?\nThat is the so-called “matching problem”.\nMatching problem\nTo train the network, we need to assign the ground truth boxes to the grid cells/anchor boxes. We do this based on overlap between bounding boxes on the one hand, and anchor boxes on the other. Overlap is computed using Intersection over Union (IoU, =Jaccard Index), as usual.\nAssume we’ve already computed the Jaccard index for all ground truth box - grid cell combinations. We then use the following algorithm:\nFor each ground truth object, find the grid cell it maximally overlaps with.\nFor each grid cell, find the object it overlaps with most.\nIn both cases, identify the entity of greatest overlap as well as the amount of overlap.\nWhen criterium (1) applies, it overrides criterium (2).\nWhen criterium (1) applies, set the amount overlap to a constant, high value: 1.99.\nReturn the combined result, that is, for each grid cell, the object and amount of best (as per the above criteria) overlap.\nHere’s the implementation.\n\n\n# overlaps shape is: number of ground truth objects * number of grid cells\nmap_to_ground_truth <- function(overlaps) {\n  \n  # for each ground truth object, find maximally overlapping cell (crit. 1)\n  # measure of overlap, shape: number of ground truth objects\n  prior_overlap <- apply(overlaps, 1, max)\n  # which cell is this, for each object\n  prior_idx <- apply(overlaps, 1, which.max)\n  \n  # for each grid cell, what object does it overlap with most (crit. 2)\n  # measure of overlap, shape: number of grid cells\n  gt_overlap <-  apply(overlaps, 2, max)\n  # which object is this, for each cell\n  gt_idx <- apply(overlaps, 2, which.max)\n  \n  # set all definitely overlapping cells to respective object (crit. 1)\n  gt_overlap[prior_idx] <- 1.99\n  \n  # now still set all others to best match by crit. 2\n  # actually it's other way round, we start from (2) and overwrite with (1)\n  for (i in 1:length(prior_idx)) {\n    # iterate over all cells \"absolutely assigned\"\n    p <- prior_idx[i] # get respective grid cell\n    gt_idx[p] <- i # assign this cell the object number\n  }\n  \n  # return: for each grid cell, object it overlaps with most + measure of overlap\n  list(gt_overlap, gt_idx)\n  \n}\n\nNow here’s the IoU calculation we need for that. We can’t just use the IoU function from the previous post because this time, we want to compute overlaps with all grid cells simultaneously. It’s easiest to do this using tensors, so we temporarily convert the R matrices to tensors:\n\n\n# compute IOU\njaccard <- function(bbox, anchor_corners) {\n  bbox <- k_constant(bbox)\n  anchor_corners <- k_constant(anchor_corners)\n  intersection <- intersect(bbox, anchor_corners)\n  union <-\n    k_expand_dims(box_area(bbox), axis = 2)  + k_expand_dims(box_area(anchor_corners), axis = 1) - intersection\n    res <- intersection / union\n  res %>% k_eval()\n}\n\n# compute intersection for IOU\nintersect <- function(box1, box2) {\n  box1_a <- box1[, 3:4] %>% k_expand_dims(axis = 2)\n  box2_a <- box2[, 3:4] %>% k_expand_dims(axis = 1)\n  max_xy <- k_minimum(box1_a, box2_a)\n  \n  box1_b <- box1[, 1:2] %>% k_expand_dims(axis = 2)\n  box2_b <- box2[, 1:2] %>% k_expand_dims(axis = 1)\n  min_xy <- k_maximum(box1_b, box2_b)\n  \n  intersection <- k_clip(max_xy - min_xy, min = 0, max = Inf)\n  intersection[, , 1] * intersection[, , 2]\n  \n}\n\nbox_area <- function(box) {\n  (box[, 3] - box[, 1]) * (box[, 4] - box[, 2]) \n}\n\nBy now you might be wondering - when does all this happen? Interestingly, the example we’re following, fast.ai’s object detection notebook, does all this as part of the loss calculation! In TensorFlow, this is possible in principle (requiring some juggling of tf$cond, tf$while_loop etc., as well as a bit of creativity finding replacements for non-differentiable operations). But, simple facts - like the Keras loss function expecting the same shapes for y_true and y_pred - made it impossible to follow the fast.ai approach. Instead, all matching will take place in the data generator.\nData generator\nThe generator has the familiar structure, known from the predecessor post. Here is the complete code - we’ll talk through the details immediately.\n\n\nbatch_size <- 16\nimage_size <- target_width # same as height\n\nthreshold <- 0.4\n\nclass_background <- 21\n\nssd_generator <-\n  function(data,\n           target_height,\n           target_width,\n           shuffle,\n           batch_size) {\n    i <- 1\n    function() {\n      if (shuffle) {\n        indices <- sample(1:nrow(data), size = batch_size)\n      } else {\n        if (i + batch_size >= nrow(data))\n          i <<- 1\n        indices <- c(i:min(i + batch_size - 1, nrow(data)))\n        i <<- i + length(indices)\n      }\n      \n      x <-\n        array(0, dim = c(length(indices), target_height, target_width, 3))\n      y1 <- array(0, dim = c(length(indices), 16))\n      y2 <- array(0, dim = c(length(indices), 16, 4))\n      \n      for (j in 1:length(indices)) {\n        x[j, , , ] <-\n          load_and_preprocess_image(data[[indices[j], \"file_name\"]], target_height, target_width)\n        \n        class_string <- data[indices[j], ]$categories\n        xl_string <- data[indices[j], ]$xl\n        yt_string <- data[indices[j], ]$yt\n        xr_string <- data[indices[j], ]$xr\n        yb_string <- data[indices[j], ]$yb\n        \n        classes <-  str_split(class_string, pattern = \", \")[[1]]\n        xl <-\n          str_split(xl_string, pattern = \", \")[[1]] %>% as.double() %>% `/`(image_size)\n        yt <-\n          str_split(yt_string, pattern = \", \")[[1]] %>% as.double() %>% `/`(image_size)\n        xr <-\n          str_split(xr_string, pattern = \", \")[[1]] %>% as.double() %>% `/`(image_size)\n        yb <-\n          str_split(yb_string, pattern = \", \")[[1]] %>% as.double() %>% `/`(image_size)\n    \n        # rows are objects, columns are coordinates (xl, yt, xr, yb)\n        # anchor_corners are 16 rows with corresponding coordinates\n        bbox <- cbind(xl, yt, xr, yb)\n        overlaps <- jaccard(bbox, anchor_corners)\n        \n        c(gt_overlap, gt_idx) %<-% map_to_ground_truth(overlaps)\n        gt_class <- classes[gt_idx]\n        \n        pos <- gt_overlap > threshold\n        gt_class[gt_overlap < threshold] <- 21\n                \n        # columns correspond to objects\n        boxes <- rbind(xl, yt, xr, yb)\n        # columns correspond to object boxes according to gt_idx\n        gt_bbox <- boxes[, gt_idx]\n        # set those with non-sufficient overlap to 0\n        gt_bbox[, !pos] <- 0\n        gt_bbox <- gt_bbox %>% t()\n        \n        y1[j, ] <- as.integer(gt_class) - 1\n        y2[j, , ] <- gt_bbox\n        \n      }\n\n      x <- x %>% imagenet_preprocess_input()\n      y1 <- y1 %>% to_categorical(num_classes = class_background)\n      list(x, list(y1, y2))\n    }\n  }\n\nBefore the generator can trigger any calculations, it needs to first split apart the multiple classes and bounding box coordinates that come in one row of the dataset.\nTo make this more concrete, we show what happens for the “2 people and 2 airplanes” image we just displayed.\nWe copy out code chunk-by-chunk from the generator so results can actually be displayed for inspection.\n\n\ndata <- imageinfo4ssd\nindices <- 1:8\n\nj <- 5 # this is our image\n\nclass_string <- data[indices[j], ]$categories\nxl_string <- data[indices[j], ]$xl\nyt_string <- data[indices[j], ]$yt\nxr_string <- data[indices[j], ]$xr\nyb_string <- data[indices[j], ]$yb\n        \nclasses <-  str_split(class_string, pattern = \", \")[[1]]\nxl <- str_split(xl_string, pattern = \", \")[[1]] %>% as.double() %>% `/`(image_size)\nyt <- str_split(yt_string, pattern = \", \")[[1]] %>% as.double() %>% `/`(image_size)\nxr <- str_split(xr_string, pattern = \", \")[[1]] %>% as.double() %>% `/`(image_size)\nyb <- str_split(yb_string, pattern = \", \")[[1]] %>% as.double() %>% `/`(image_size)\n\nSo here are that image’s classes:\n\n\nclasses\n\n\n[1] \"1\"  \"1\"  \"15\" \"15\"\nAnd its left bounding box coordinates:\n\n\nxl\n\n\n[1] 0.20535714 0.26339286 0.38839286 0.04910714\nNow we can cbind those vectors together to obtain a object (bbox) where rows are objects, and coordinates are in the columns:\n\n\n# rows are objects, columns are coordinates (xl, yt, xr, yb)\nbbox <- cbind(xl, yt, xr, yb)\nbbox\n\n\n          xl        yt         xr        yb\n[1,] 0.20535714 0.2723214 0.75000000 0.6473214\n[2,] 0.26339286 0.3080357 0.39285714 0.4330357\n[3,] 0.38839286 0.6383929 0.42410714 0.8125000\n[4,] 0.04910714 0.6696429 0.08482143 0.8437500\nSo we’re ready to compute these boxes’ overlap with all of the 16 grid cells. Recall that anchor_corners stores the grid cells in an analogous way, the cells being in the rows and the coordinates in the columns.\n\n\n# anchor_corners are 16 rows with corresponding coordinates\noverlaps <- jaccard(bbox, anchor_corners)\n\nNow that we have the overlaps, we can call the matching logic:\n\n\nc(gt_overlap, gt_idx) %<-% map_to_ground_truth(overlaps)\ngt_overlap\n\n\n [1] 0.00000000 0.03961473 0.04358353 1.99000000 0.00000000 1.99000000 1.99000000 0.03357313 0.00000000\n[10] 0.27127662 0.16019417 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000\nLooking for the value 1.99 in the above - the value indicating maximal, by the above criteria, overlap of an object with a grid cell - we see that box 4 (counting in column-major order here like R does) got matched (to a person, as we’ll see soon), box 6 did (to an airplane), and box 7 did (to a person). How about the other airplane? It got lost in the matching.\nThis is not a problem of the matching algorithm though - it would disappear if we had more than one anchor box per grid cell.\nLooking for the objects just mentioned in the class index, gt_idx, we see that indeed box 4 got matched to object 4 (a person), box 6 got matched to object 2 (an airplane), and box 7 got matched to object 3 (the other person):\n\n\ngt_idx\n\n\n[1] 1 1 4 4 1 2 3 3 1 1 1 1 1 1 1 1\nBy the way, don’t worry about the abundance of 1s here. These are remnants from using which.max to determine maximal overlap, and will disappear soon.\nInstead of thinking in object numbers, we should think in object classes (the respective numerical codes, that is).\n\n\ngt_class <- classes[gt_idx]\ngt_class\n\n\n [1] \"1\"  \"1\"  \"15\" \"15\" \"1\"  \"1\"  \"15\" \"15\" \"1\"  \"1\"  \"1\"  \"1\"  \"1\"  \"1\"  \"1\"  \"1\"\nSo far, we take into account even the very slightest overlap - of 0.1 percent, say. Of course, this makes no sense. We set all cells with an overlap < 0.4 to the background class:\n\n\npos <- gt_overlap > threshold\ngt_class[gt_overlap < threshold] <- 21\n\ngt_class\n\n\n[1] \"21\" \"21\" \"21\" \"15\" \"21\" \"1\"  \"15\" \"21\" \"21\" \"21\" \"21\" \"21\" \"21\" \"21\" \"21\" \"21\"\nNow, to construct the targets for learning, we need to put the mapping we found into a data structure.\nThe following gives us a 16x4 matrix of cells and the boxes they are responsible for:\n\n\norig_boxes <- rbind(xl, yt, xr, yb)\n# columns correspond to object boxes according to gt_idx\ngt_bbox <- orig_boxes[, gt_idx]\n# set those with non-sufficient overlap to 0\ngt_bbox[, !pos] <- 0\ngt_bbox <- gt_bbox %>% t()\n\ngt_bbox\n\n\n              xl        yt         xr        yb\n [1,] 0.00000000 0.0000000 0.00000000 0.0000000\n [2,] 0.00000000 0.0000000 0.00000000 0.0000000\n [3,] 0.00000000 0.0000000 0.00000000 0.0000000\n [4,] 0.04910714 0.6696429 0.08482143 0.8437500\n [5,] 0.00000000 0.0000000 0.00000000 0.0000000\n [6,] 0.26339286 0.3080357 0.39285714 0.4330357\n [7,] 0.38839286 0.6383929 0.42410714 0.8125000\n [8,] 0.00000000 0.0000000 0.00000000 0.0000000\n [9,] 0.00000000 0.0000000 0.00000000 0.0000000\n[10,] 0.00000000 0.0000000 0.00000000 0.0000000\n[11,] 0.00000000 0.0000000 0.00000000 0.0000000\n[12,] 0.00000000 0.0000000 0.00000000 0.0000000\n[13,] 0.00000000 0.0000000 0.00000000 0.0000000\n[14,] 0.00000000 0.0000000 0.00000000 0.0000000\n[15,] 0.00000000 0.0000000 0.00000000 0.0000000\n[16,] 0.00000000 0.0000000 0.00000000 0.0000000\nTogether, gt_bbox and gt_class make up the network’s learning targets.\n\n\ny1[j, ] <- as.integer(gt_class) - 1\ny2[j, , ] <- gt_bbox\n\nTo summarize, our target is a list of two outputs:\nthe bounding box ground truth of dimensionality number of grid cells times number of box coordinates, and\nthe class ground truth of size number of grid cells times number of classes.\nWe can verify this by asking the generator for a batch of inputs and targets:\n\n\ntrain_gen <- ssd_generator(\n  imageinfo4ssd,\n  target_height = target_height,\n  target_width = target_width,\n  shuffle = TRUE,\n  batch_size = batch_size\n)\n\nbatch <- train_gen()\nc(x, c(y1, y2)) %<-% batch\ndim(y1)\n\n\n[1] 16 16 21\n\n\ndim(y2)\n\n\n[1] 16 16  4\nFinally, we’re ready for the model.\nThe model\nWe start from Resnet 50 as a feature extractor. This gives us tensors of size 7x7x2048.\n\n\nfeature_extractor <- application_resnet50(\n  include_top = FALSE,\n  input_shape = c(224, 224, 3)\n)\n\nThen, we append a few conv layers. Three of those layers are “just” there for capacity; the last one though has a additional task: By virtue of strides = 2, it downsamples its input to from 7x7 to 4x4 in the height/width dimensions.\nThis resolution of 4x4 gives us exactly the grid we need!\n\n\ninput <- feature_extractor$input\n\ncommon <- feature_extractor$output %>%\n  layer_conv_2d(\n    filters = 256,\n    kernel_size = 3,\n    padding = \"same\",\n    activation = \"relu\",\n    name = \"head_conv1_1\"\n  ) %>%\n  layer_batch_normalization() %>%\n  layer_conv_2d(\n    filters = 256,\n    kernel_size = 3,\n    padding = \"same\",\n    activation = \"relu\",\n    name = \"head_conv1_2\"\n  ) %>%\n  layer_batch_normalization() %>%\n  layer_conv_2d(\n    filters = 256,\n    kernel_size = 3,\n    padding = \"same\",\n    activation = \"relu\",\n    name = \"head_conv1_3\"\n  ) %>%\n  layer_batch_normalization() %>%\n  layer_conv_2d(\n    filters = 256,\n    kernel_size = 3,\n    strides = 2,\n    padding = \"same\",\n    activation = \"relu\",\n    name = \"head_conv2\"\n  ) %>%\n  layer_batch_normalization() \n\nNow we can do as we did in that other post, attach one output for the bounding boxes and one for the classes.\nNote how we don’t aggregate over the spatial grid though. Instead, we reshape it so the 4x4 grid cells appear sequentially.\nHere first is the class output. We have 21 classes (the 20 classes from PASCAL, plus background), and we need to classify each cell. We thus end up with an output of size 16x21.\n\n\nclass_output <-\n  layer_conv_2d(\n    common,\n    filters = 21,\n    kernel_size = 3,\n    padding = \"same\",\n    name = \"class_conv\"\n  ) %>%\n  layer_reshape(target_shape = c(16, 21), name = \"class_output\")\n\nFor the bounding box output, we apply a tanh activation so that values lie between -1 and 1. This is because they are used to compute offsets to the grid cell centers.\nThese computations happen in the layer_lambda. We start from the actual anchor box centers, and move them around by a scaled-down version of the activations. We then convert these to anchor corners - same as we did above with the ground truth anchors, just operating on tensors, this time.\n\n\nbbox_output <-\n  layer_conv_2d(\n    common,\n    filters = 4,\n    kernel_size = 3,\n    padding = \"same\",\n    name = \"bbox_conv\"\n  ) %>%\n  layer_reshape(target_shape = c(16, 4), name = \"bbox_flatten\") %>%\n  layer_activation(\"tanh\") %>%\n  layer_lambda(\n    f = function(x) {\n      activation_centers <-\n        (x[, , 1:2] / 2 * gridsize) + k_constant(anchors[, 1:2])\n      activation_height_width <-\n        (x[, , 3:4] / 2 + 1) * k_constant(anchors[, 3:4])\n      activation_corners <-\n        k_concatenate(\n          list(\n            activation_centers - activation_height_width / 2,\n            activation_centers + activation_height_width / 2\n          )\n        )\n     activation_corners\n    },\n    name = \"bbox_output\"\n  )\n\nNow that we have all layers, let’s quickly finish up the model definition:\n\n\nmodel <- keras_model(\n  inputs = input,\n  outputs = list(class_output, bbox_output)\n)\n\nThe last ingredient missing, then, is the loss function.\nLoss\nTo the model’s two outputs - a classification output and a regression output - correspond two losses, just as in the basic classification + localization model. Only this time, we have 16 grid cells to take care of.\nClass loss uses tf$nn$sigmoid_cross_entropy_with_logits to compute the binary crossentropy between targets and unnormalized network activation, summing over grid cells and dividing by the number of classes.\n\n\n# shapes are batch_size * 16 * 21\nclass_loss <- function(y_true, y_pred) {\n\n  class_loss  <-\n    tf$nn$sigmoid_cross_entropy_with_logits(labels = y_true, logits = y_pred)\n\n  class_loss <-\n    tf$reduce_sum(class_loss) / tf$cast(n_classes + 1, \"float32\")\n  \n  class_loss\n}\n\nLocalization loss is calculated for all boxes where in fact there is an object present in the ground truth. All other activations get masked out.\nThe loss itself then is just mean absolute error, scaled by a multiplier designed to bring both loss components to similar magnitudes. In practice, it makes sense to experiment a bit here.\n\n\n# shapes are batch_size * 16 * 4\nbbox_loss <- function(y_true, y_pred) {\n\n  # calculate localization loss for all boxes where ground truth was assigned some overlap\n  # calculate mask\n  pos <- y_true[, , 1] + y_true[, , 3] > 0\n  pos <-\n    pos %>% k_cast(tf$float32) %>% k_reshape(shape = c(batch_size, 16, 1))\n  pos <-\n    tf$tile(pos, multiples = k_constant(c(1L, 1L, 4L), dtype = tf$int32))\n    \n  diff <- y_pred - y_true\n  # mask out irrelevant activations\n  diff <- diff %>% tf$multiply(pos)\n  \n  loc_loss <- diff %>% tf$abs() %>% tf$reduce_mean()\n  loc_loss * 100\n}\n\nTraining\nAbove, we’ve already defined the model but we still need to freeze the feature detector’s weights and compile it.\n\n\nmodel %>% freeze_weights()\nmodel %>% unfreeze_weights(from = \"head_conv1_1\")\nmodel\n\n\n\nmodel %>% compile(\n  loss = list(class_loss, bbox_loss),\n  optimizer = \"adam\",\n  metrics = list(\n    class_output = custom_metric(\"class_loss\", metric_fn = class_loss),\n    bbox_output = custom_metric(\"bbox_loss\", metric_fn = bbox_loss)\n  )\n)\n\nAnd we’re ready to train. Training this model is very time consuming, such that for applications “in the real world”, we might want to do optimize the program for memory consumption and runtime. Like we said above, in this post we’re really focusing on understanding the approach.\n\n\nsteps_per_epoch <- nrow(imageinfo4ssd) / batch_size\n\nmodel %>% fit_generator(\n  train_gen,\n  steps_per_epoch = steps_per_epoch,\n  epochs = 5,\n  callbacks = callback_model_checkpoint(\n    \"weights.{epoch:02d}-{loss:.2f}.hdf5\", \n    save_weights_only = TRUE\n  )\n)\n\nAfter 5 epochs, this is what we get from the model. It’s on the right way, but it will need many more epochs to reach decent performance.\n\nApart from training for many more epochs, what could we do? We’ll wrap up the post with two directions for improvement, but won’t implement them completely.\nThe first one actually is quick to implement. Here we go.\nFocal loss\nAbove, we were using cross entropy for the classification loss. Let’s look at what that entails.\nBinary cross entropy for predictions when the ground truth equals 1The figure shows loss incurred when the correct answer is 1. We see that even though loss is highest when the network is very wrong, it still incurs significant loss when it’s “right for all practical purposes” - meaning, its output is just above 0.5.\nIn cases of strong class imbalance, this behavior can be problematic. Much training energy is wasted on getting “even more right” on cases where the net is right already - as will happen with instances of the dominant class. Instead, the network should dedicate more effort to the hard cases - exemplars of the rarer classes.\nIn object detection, the prevalent class is background - no class, really. Instead of getting more and more proficient at predicting background, the network had better learn how to tell apart the actual object classes.\nAn alternative was pointed out by the authors of the RetinaNet paper(Lin et al. 2017): They introduced a parameter \\(\\gamma\\)2 that results in decreasing loss for samples that already have been well classified.\nFocal loss downweights contributions from well-classified examples. Figure from (Lin et al. 2017)Different implementations are found on the net, as well as different settings for the hyperparameters. Here’s a direct port of the fast.ai code:\n\n\nalpha <- 0.25\ngamma <- 1\n\nget_weights <- function(y_true, y_pred) {\n  p <- y_pred %>% k_sigmoid()\n  pt <-  y_true*p + (1-p)*(1-y_true)\n  w <- alpha*y_true + (1-alpha)*(1-y_true)\n  w <-  w * (1-pt)^gamma\n  w\n}\n\nclass_loss_focal  <- function(y_true, y_pred) {\n  \n  w <- get_weights(y_true, y_pred)\n  cx <- tf$nn$sigmoid_cross_entropy_with_logits(labels = y_true, logits = y_pred)\n  weighted_cx <- w * cx\n\n  class_loss <-\n   tf$reduce_sum(weighted_cx) / tf$cast(21, \"float32\")\n  \n  class_loss\n}\n\nFrom testing this loss, it seems to yield better performance, but does not render obsolete the need for substantive training time.\nFinally, let’s see what we’d have to do if we wanted to use several anchor boxes per grid cells.\nMore anchor boxes\nThe “real SSD” has anchor boxes of different aspect ratios, and it puts detectors at different stages of the network. Let’s implement this.\nAnchor box coordinates\nWe create anchor boxes as combinations of\ndifferent scales, and\n\n\nanchor_zooms <- c(0.7, 1, 1.3)\nanchor_zooms\n\n\n[1] 0.7 1.0 1.3\ndifferent aspect ratios:\n\n\nanchor_ratios <- matrix(c(1, 1, 1, 0.5, 0.5, 1), ncol = 2, byrow = TRUE)\nanchor_ratios\n\n\n     [,1] [,2]\n[1,]  1.0  1.0\n[2,]  1.0  0.5\n[3,]  0.5  1.0\nIn this example, we have nine different combinations:\n\n\nanchor_scales <- rbind(\n  anchor_ratios * anchor_zooms[1],\n  anchor_ratios * anchor_zooms[2],\n  anchor_ratios * anchor_zooms[3]\n)\n\nk <- nrow(anchor_scales)\n\nanchor_scales\n\n\n      [,1] [,2]\n [1,] 0.70 0.70\n [2,] 0.70 0.35\n [3,] 0.35 0.70\n [4,] 1.00 1.00\n [5,] 1.00 0.50\n [6,] 0.50 1.00\n [7,] 1.30 1.30\n [8,] 1.30 0.65\n [9,] 0.65 1.30\nWe place detectors at three stages. Resolutions will be 4x4 (as we had before) and additionally, 2x2 and 1x1:\n\n\nanchor_grids <- c(4,2,1)\n\nOnce that’s been determined, we can compute\nx coordinates of the box centers:\n\n\nanchor_offsets <- 1/(anchor_grids * 2)\n\nanchor_x <- map(\n  1:3,\n  function(x) rep(seq(anchor_offsets[x],\n                      1 - anchor_offsets[x],\n                      length.out = anchor_grids[x]),\n                  each = anchor_grids[x])) %>%\n  flatten() %>%\n  unlist()\n\ny coordinates of the box centers:\n\n\nanchor_y <- map(\n  1:3,\n  function(y) rep(seq(anchor_offsets[y],\n                      1 - anchor_offsets[y],\n                      length.out = anchor_grids[y]),\n                  times = anchor_grids[y])) %>%\n  flatten() %>%\n  unlist()\n\nthe x-y representations of the centers:\n\n\nanchor_centers <- cbind(rep(anchor_x, each = k), rep(anchor_y, each = k))\n\nthe sizes of the boxes:\n\n\nanchor_sizes <- map(\n  anchor_grids,\n  function(x)\n   matrix(rep(t(anchor_scales/x), x*x), ncol = 2, byrow = TRUE)\n  ) %>%\n  abind(along = 1)\n\nthe sizes of the base grids (0.25, 0.5, and 1):\n\n\ngrid_sizes <- c(rep(0.25, k * anchor_grids[1]^2),\n                rep(0.5, k * anchor_grids[2]^2),\n                rep(1, k * anchor_grids[3]^2)\n                )\n\nthe centers-width-height representations of the anchor boxes:\n\n\nanchors <- cbind(anchor_centers, anchor_sizes)\n\nand finally, the corners representation of the boxes!\n\n\nhw2corners <- function(centers, height_width) {\n  cbind(centers - height_width / 2, centers + height_width / 2) %>% unname()\n}\n\nanchor_corners <- hw2corners(anchors[ , 1:2], anchors[ , 3:4])\n\nSo here, then, is a plot of the (distinct) box centers: One in the middle, for the 9 large boxes, 4 for the 4 * 9 medium-size boxes, and 16 for the 16 * 9 small boxes.\n\nOf course, even if we aren’t going to train this version, we at least need to see these in action!\n\nHow would a model look that could deal with these?\nModel\nAgain, we’d start from a feature detector …\n\n\nfeature_extractor <- application_resnet50(\n  include_top = FALSE,\n  input_shape = c(224, 224, 3)\n)\n\n… and attach some custom conv layers.\n\n\ninput <- feature_extractor$input\n\ncommon <- feature_extractor$output %>%\n  layer_conv_2d(\n    filters = 256,\n    kernel_size = 3,\n    padding = \"same\",\n    activation = \"relu\",\n    name = \"head_conv1_1\"\n  ) %>%\n  layer_batch_normalization() %>%\n  layer_conv_2d(\n    filters = 256,\n    kernel_size = 3,\n    padding = \"same\",\n    activation = \"relu\",\n    name = \"head_conv1_2\"\n  ) %>%\n  layer_batch_normalization() %>%\n  layer_conv_2d(\n    filters = 256,\n    kernel_size = 3,\n    padding = \"same\",\n    activation = \"relu\",\n    name = \"head_conv1_3\"\n  ) %>%\n  layer_batch_normalization()\n\nThen, things get different. We want to attach detectors (= output layers) to different stages in a pipeline of successive downsamplings. If that doesn’t call for the Keras functional API…\nHere’s the downsizing pipeline.\n\n\n downscale_4x4 <- common %>%\n  layer_conv_2d(\n    filters = 256,\n    kernel_size = 3,\n    strides = 2,\n    padding = \"same\",\n    activation = \"relu\",\n    name = \"downscale_4x4\"\n  ) %>%\n  layer_batch_normalization() \n\n\n\ndownscale_2x2 <- downscale_4x4 %>%\n  layer_conv_2d(\n    filters = 256,\n    kernel_size = 3,\n    strides = 2,\n    padding = \"same\",\n    activation = \"relu\",\n    name = \"downscale_2x2\"\n  ) %>%\n  layer_batch_normalization() \n\n\n\ndownscale_1x1 <- downscale_2x2 %>%\n  layer_conv_2d(\n    filters = 256,\n    kernel_size = 3,\n    strides = 2,\n    padding = \"same\",\n    activation = \"relu\",\n    name = \"downscale_1x1\"\n  ) %>%\n  layer_batch_normalization() \n\nThe bounding box output definitions get a little messier than before, as each output has to take into account its relative anchor box coordinates.\n\n\ncreate_bbox_output <- function(prev_layer, anchor_start, anchor_stop, suffix) {\n  output <- layer_conv_2d(\n    prev_layer,\n    filters = 4 * k,\n    kernel_size = 3,\n    padding = \"same\",\n    name = paste0(\"bbox_conv_\", suffix)\n  ) %>%\n  layer_reshape(target_shape = c(-1, 4), name = paste0(\"bbox_flatten_\", suffix)) %>%\n  layer_activation(\"tanh\") %>%\n  layer_lambda(\n    f = function(x) {\n      activation_centers <-\n        (x[, , 1:2] / 2 * matrix(grid_sizes[anchor_start:anchor_stop], ncol = 1)) +\n        k_constant(anchors[anchor_start:anchor_stop, 1:2])\n      activation_height_width <-\n        (x[, , 3:4] / 2 + 1) * k_constant(anchors[anchor_start:anchor_stop, 3:4])\n      activation_corners <-\n        k_concatenate(\n          list(\n            activation_centers - activation_height_width / 2,\n            activation_centers + activation_height_width / 2\n          )\n        )\n     activation_corners\n    },\n    name = paste0(\"bbox_output_\", suffix)\n  )\n  output\n}\n\nHere they are: Each one attached to it’s respective stage of action in the pipeline.\n\n\nbbox_output_4x4 <- create_bbox_output(downscale_4x4, 1, 144, \"4x4\")\n\n\n\nbbox_output_2x2 <- create_bbox_output(downscale_2x2, 145, 180, \"2x2\")\n\n\n\nbbox_output_1x1 <- create_bbox_output(downscale_1x1, 181, 189, \"1x1\")\n\nThe same principle applies to the class outputs.\n\n\ncreate_class_output <- function(prev_layer, suffix) {\n  output <-\n  layer_conv_2d(\n    prev_layer,\n    filters = 21 * k,\n    kernel_size = 3,\n    padding = \"same\",\n    name = paste0(\"class_conv_\", suffix)\n  ) %>%\n  layer_reshape(target_shape = c(-1, 21), name = paste0(\"class_output_\", suffix))\n  output\n}\n\n\n\nclass_output_4x4 <- create_class_output(downscale_4x4, \"4x4\")\n\n\n\nclass_output_2x2 <- create_class_output(downscale_2x2, \"2x2\")\n\n\n\nclass_output_1x1 <- create_class_output(downscale_1x1, \"1x1\")\n\nAnd glue it all together, to get the model.\n\n\nmodel <- keras_model(\n  inputs = input,\n  outputs = list(\n    bbox_output_1x1,\n    bbox_output_2x2,\n    bbox_output_4x4,\n    class_output_1x1, \n    class_output_2x2, \n    class_output_4x4)\n)\n\nNow, we will stop here. To run this, there is another element that has to be adjusted: the data generator. Our focus being on explaining the concepts though, we’ll leave that to the interested reader.\nConclusion\nWhile we haven’t ended up with a good-performing model for object detection, we do hope that we’ve managed to shed some light on the mystery of object detection. What’s a bounding box? What’s an anchor (resp. prior, rep. default) box? How do you match them up in practice?\nIf you’ve “just” read the papers (YOLO, SSD), but never seen any code, it may seem like all actions happen in some wonderland beyond the horizon. They don’t. But coding them, as we’ve seen, can be cumbersome, even in the very basic versions we’ve implemented. To perform object detection in production, then, a lot more time has to be spent on training and tuning models. But sometimes just learning about how something works can be very satisfying.\nFinally, we’d again like to stress how much this post leans on what the fast.ai guys did. Their work most definitely is enriching not just the PyTorch, but also the R-TensorFlow community!\n\n\nGirshick, Ross B. 2015. “Fast R-Cnn.” CoRR abs/1504.08083. http://arxiv.org/abs/1504.08083.\n\n\nGirshick, Ross B., Jeff Donahue, Trevor Darrell, and Jitendra Malik. 2013. “Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.” CoRR abs/1311.2524. http://arxiv.org/abs/1311.2524.\n\n\nLin, Tsung-Yi, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Dollar. 2017. “Focal Loss for Dense Object Detection.” CoRR abs/1708.02002. http://arxiv.org/abs/1708.02002.\n\n\nLiu, Wei, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott E. Reed, Cheng-Yang Fu, and Alexander C. Berg. 2015. “SSD: Single Shot Multibox Detector.” CoRR abs/1512.02325. http://arxiv.org/abs/1512.02325.\n\n\nRedmon, Joseph, Santosh Kumar Divvala, Ross B. Girshick, and Ali Farhadi. 2015. “You Only Look Once: Unified, Real-Time Object Detection.” CoRR abs/1506.02640. http://arxiv.org/abs/1506.02640.\n\n\nRedmon, Joseph, and Ali Farhadi. 2016. “YOLO9000: Better, Faster, Stronger.” CoRR abs/1612.08242. http://arxiv.org/abs/1612.08242.\n\n\n———. 2018. “YOLOv3: An Incremental Improvement.” CoRR abs/1804.02767. http://arxiv.org/abs/1804.02767.\n\n\nRen, Shaoqing, Kaiming He, Ross B. Girshick, and Jian Sun. 2015. “Faster R-Cnn: Towards Real-Time Object Detection with Region Proposal Networks.” CoRR abs/1506.01497. http://arxiv.org/abs/1506.01497.\n\n\nSermanet, Pierre, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, and Yann LeCun. 2013. “OverFeat: Integrated Recognition, Localization and Detection Using Convolutional Networks.” CoRR abs/1312.6229. http://arxiv.org/abs/1312.6229.\n\n\nWe won’t need imageinfo_maxbb in this post.↩︎\nas well as \\(\\alpha\\), not used in the figure↩︎\n",
    "preview": "posts/2018-12-18-object-detection-concepts/images/results.jpg",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-26-embeddings-fun-and-profit/",
    "title": "Entity embeddings for fun and profit",
    "description": "Embedding layers are not just useful when working with language data. As \"entity embeddings\", they've recently become famous for applications on tabular, small-scale data. In this post, we exemplify two possible use cases, also drawing attention to what not to expect.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-11-26",
    "categories": [
      "TensorFlow/Keras",
      "Tabular Data"
    ],
    "contents": "\nWhat’s useful about embeddings? Depending on who you ask, answers may vary. For many, the most immediate association may be word vectors and their use in natural language processing (translation, summarization, question answering etc.) There, they are famous for modeling semantic and syntactic relationships, as exemplified by this diagram found in one of the most influential papers on word vectors(Mikolov et al. 2013):\nCountries and their capital cities. Figure from (Mikolov et al. 2013)Others will probably bring up entity embeddings, the magic tool that helped win the Rossmann competition(Guo and Berkhahn 2016) and was greatly popularized by fast.ai’s deep learning course. Here, the idea is to make use of data that is not normally helpful in prediction, like high-dimensional categorical variables.\nAnother (related) idea, also widely spread by fast.ai and explained in this blog, is to apply embeddings to collaborative filtering. This basically builds up entity embeddings of users and items based on the criterion how well these “match” (as indicated by existing ratings).\nSo what are embeddings good for? The way we see it, embeddings are what you make of them. The goal in this post is to provide examples of how to use embeddings to uncover relationships and improve prediction. The examples are just that - examples, chosen to demonstrate a method. The most interesting thing really will be what you make of these methods in your area of work or interest.\nEmbeddings for fun (picturing relationships)\nOur first example will stress the “fun” part, but also show how to technically deal with categorical variables in a dataset.\nWe’ll take this year’s StackOverflow developer survey as a basis and pick a few categorical variables that seem interesting - stuff like “what do people value in a job” and of course, what languages and OSes do people use. Don’t take this too seriously, it’s meant to be fun and demonstrate a method, that’s all.1\nPreparing the data\nEquipped with the libraries we’ll need:\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(keras)\nlibrary(purrr)\nlibrary(forcats)\nlibrary(ggrepel)\n\nWe load the data and zoom in on a few categorical variables. Two of them we intend to use as targets: EthicsChoice and JobSatisfaction. EthicsChoice is one of four ethics-related questions and goes\n\n“Imagine that you were asked to write code for a purpose or product that you consider extremely unethical. Do you write the code anyway?”\n\nWith questions like this, it’s never clear what portion of a response should be attributed to social desirability - this question seemed like the least prone to that, which is why we chose it.2\n\n\ndata <- read_csv(\"survey_results_public.csv\")\n\ndata <- data %>% select(\n  FormalEducation,\n  UndergradMajor,\n  starts_with(\"AssessJob\"),\n  EthicsChoice,\n  LanguageWorkedWith,\n  OperatingSystem,\n  EthicsChoice,\n  JobSatisfaction\n)\n\ndata <- data %>% mutate_if(is.character, factor)\n\nThe variables we are interested in show a tendency to have been left unanswered by quite a few respondents, so the easiest way to handle missing data here is to exclude the respective participants completely.\n\n\ndata <- na.omit(data)\n\nThat leaves us with ~48,000 completed (as far as we’re concerned) questionnaires. Looking at the variables’ contents, we see we’ll have to do something with them before we can start training.\n\n\ndata %>% glimpse()\n\n\nObservations: 48,610\nVariables: 16\n$ FormalEducation    <fct> Bachelor’s degree (BA, BS, B.Eng., etc.),...\n$ UndergradMajor     <fct> Mathematics or statistics, A natural scie...\n$ AssessJob1         <int> 10, 1, 8, 8, 5, 6, 6, 6, 9, 7, 3, 1, 6, 7...\n$ AssessJob2         <int> 7, 7, 5, 5, 3, 5, 3, 9, 4, 4, 9, 7, 7, 10...\n$ AssessJob3         <int> 8, 10, 7, 4, 9, 4, 7, 2, 10, 10, 10, 6, 1...\n$ AssessJob4         <int> 1, 8, 1, 9, 4, 2, 4, 4, 3, 2, 6, 10, 4, 1...\n$ AssessJob5         <int> 2, 2, 2, 1, 1, 7, 1, 3, 1, 1, 8, 9, 2, 4,...\n$ AssessJob6         <int> 5, 5, 6, 3, 8, 8, 5, 5, 6, 5, 7, 4, 5, 5,...\n$ AssessJob7         <int> 3, 4, 4, 6, 2, 10, 10, 8, 5, 3, 1, 2, 3, ...\n$ AssessJob8         <int> 4, 3, 3, 2, 7, 1, 8, 7, 2, 6, 2, 3, 1, 3,...\n$ AssessJob9         <int> 9, 6, 10, 10, 10, 9, 9, 10, 7, 9, 4, 8, 9...\n$ AssessJob10        <int> 6, 9, 9, 7, 6, 3, 2, 1, 8, 8, 5, 5, 8, 9,...\n$ EthicsChoice       <fct> No, Depends on what it is, No, Depends on...\n$ LanguageWorkedWith <fct> JavaScript;Python;HTML;CSS, JavaScript;Py...\n$ OperatingSystem    <fct> Linux-based, Linux-based, Windows, Linux-...\n$ JobSatisfaction    <fct> Extremely satisfied, Moderately dissatisf...\n\nTarget variables\nWe want to binarize both target variables. Let’s inspect them, starting with EthicsChoice.\n\n\njslevels <- levels(data$JobSatisfaction)\nelevels <- levels(data$EthicsChoice)\n\ndata <- data %>% mutate(\n  JobSatisfaction = JobSatisfaction %>% fct_relevel(\n    jslevels[1],\n    jslevels[3],\n    jslevels[6],\n    jslevels[5],\n    jslevels[7],\n    jslevels[4],\n    jslevels[2]\n  ),\n  EthicsChoice = EthicsChoice %>% fct_relevel(\n    elevels[2],\n    elevels[1],\n    elevels[3]\n  ) \n)\n\nggplot(data, aes(EthicsChoice)) + geom_bar()\n\nDistribution of answers to: “Imagine that you were asked to write code for a purpose or product that you consider extremely unethical. Do you write the code anyway?”You might agree that with a question containing the phrase a purpose or product that you consider extremely unethical, the answer “depends on what it is” feels closer to “yes” than to “no”. If that seems like too skeptical a thought, it’s still the only binarization that achieves a sensible split.\n\n\ndata <- data %>% mutate(\n  EthicsChoice = if_else(as.numeric(EthicsChoice) == 2, 1, 0)\n  )\n\nLooking at our second target variable, JobSatisfaction:\nDistribution of answers to: \"“How satisfied are you with your current job? If you work more than one job, please answer regarding the one you spend the most hours on.”We think that given the mode at “moderately satisfied”, a sensible way to binarize is a split into “moderately satisfied” and “extremely satisfied” on one side, all remaining options on the other:\n\n\ndata <- data %>% mutate(\n  JobSatisfaction = if_else(as.numeric(JobSatisfaction) > 5, 1, 0)\n  )\n\nPredictors\nAmong the predictors, FormalEducation, UndergradMajor and OperatingSystem look pretty harmless - we already turned them into factors so it should be straightforward to one-hot-encode them. For curiosity’s sake, let’s look at how they’re distributed:\n\n\ndata %>% group_by(FormalEducation) %>%\n  summarise(count = n()) %>%\n  arrange(desc(count))\n\n\n  FormalEducation                                        count\n  <fct>                                                  <int>\n1 Bachelor’s degree (BA, BS, B.Eng., etc.)               25558\n2 Master’s degree (MA, MS, M.Eng., MBA, etc.)            12865\n3 Some college/university study without earning a degree  6474\n4 Associate degree                                        1595\n5 Other doctoral degree (Ph.D, Ed.D., etc.)               1395\n6 Professional degree (JD, MD, etc.)                       723\n\n\ndata %>% group_by(UndergradMajor) %>%\n  summarise(count = n()) %>%\n  arrange(desc(count))\n\n\n  UndergradMajor                                                  count\n   <fct>                                                           <int>\n 1 Computer science, computer engineering, or software engineering 30931\n 2 Another engineering discipline (ex. civil, electrical, mechani…  4179\n 3 Information systems, information technology, or system adminis…  3953\n 4 A natural science (ex. biology, chemistry, physics)              2046\n 5 Mathematics or statistics                                        1853\n 6 Web development or web design                                    1171\n 7 A business discipline (ex. accounting, finance, marketing)       1166\n 8 A humanities discipline (ex. literature, history, philosophy)    1104\n 9 A social science (ex. anthropology, psychology, political scie…   888\n10 Fine arts or performing arts (ex. graphic design, music, studi…   791\n11 I never declared a major                                          398\n12 A health science (ex. nursing, pharmacy, radiology)               130\n\n\ndata %>% group_by(OperatingSystem) %>%\n  summarise(count = n()) %>%\n  arrange(desc(count))\n\n\n  OperatingSystem count\n  <fct>           <int>\n1 Windows         23470\n2 MacOS           14216\n3 Linux-based     10837\n4 BSD/Unix           87\nLanguageWorkedWith, on the other hand, contains sequences of programming languages, concatenated by semicolon. One way to unpack these is using Keras’ text_tokenizer.\n\n\nlanguage_tokenizer <- text_tokenizer(split = \";\", filters = \"\")\nlanguage_tokenizer %>% fit_text_tokenizer(data$LanguageWorkedWith)\n\nWe have 38 languages overall. Actual usage counts aren’t too surprising:\n\n\ndata.frame(\n  name = language_tokenizer$word_counts %>% names(),\n  count = language_tokenizer$word_counts %>% unlist() %>% unname()\n) %>%\n arrange(desc(count))\n\n\n                   name count\n1            javascript 35224\n2                  html 33287\n3                   css 31744\n4                   sql 29217\n5                  java 21503\n6            bash/shell 20997\n7                python 18623\n8                    c# 17604\n9                   php 13843\n10                  c++ 10846\n11           typescript  9551\n12                    c  9297\n13                 ruby  5352\n14                swift  4014\n15                   go  3784\n16          objective-c  3651\n17               vb.net  3217\n18                    r  3049\n19             assembly  2699\n20               groovy  2541\n21                scala  2475\n22               matlab  2465\n23               kotlin  2305\n24                  vba  2298\n25                 perl  2164\n26       visual basic 6  1729\n27         coffeescript  1711\n28                  lua  1556\n29 delphi/object pascal  1174\n30                 rust  1132\n31              haskell  1058\n32                   f#   764\n33              clojure   696\n34               erlang   560\n35                cobol   317\n36                ocaml   216\n37                julia   215\n38                 hack    94\nNow language_tokenizer will nicely create a one-hot representation of the multiple-choice column.\n\n\nlangs <- language_tokenizer %>%\n  texts_to_matrix(data$LanguageWorkedWith, mode = \"count\")\nlangs[1:3, ]\n\n\n> langs[1:3, ]\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n[1,]    0    1    1    1    0    0    0    1    0     0     0     0     0     0     0     0     0     0     0     0     0\n[2,]    0    1    0    0    0    0    1    1    0     0     0     0     0     0     0     0     0     0     0     0     0\n[3,]    0    0    0    0    1    1    1    0    0     0     1     0     1     0     0     0     0     0     1     0     0\n     [,22] [,23] [,24] [,25] [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38] [,39]\n[1,]     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n[2,]     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n[3,]     0     1     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\nWe can simply append these columns to the dataframe (and do a little cleanup):\n\n\ndata <- data %>% cbind(langs[, 2:39]) # the very first column is not useful\ndata <- data %>% rename_at(vars(`1`:`38`), funs(paste0(language_tokenizer$index_word[as.integer(.)])))\ndata <- data %>% select(-LanguageWorkedWith)\n\nWe still have the AssessJob[n] columns to deal with. Here, StackOverflow had people rank what’s important to them about a job. These are the features that were to be ranked:\n\nThe industry that I’d be working in\n\n\nThe financial performance or funding status of the company or organization\n\n\nThe specific department or team I’d be working on\n\n\nThe languages, frameworks, and other technologies I’d be working with\n\n\nThe compensation and benefits offered\n\n\nThe office environment or company culture\n\n\nThe opportunity to work from home/remotely\n\n\nOpportunities for professional development\n\n\nThe diversity of the company or organization\n\n\nHow widely used or impactful the product or service I’d be working on is\n\nColumns AssessJob1 to AssessJob10 contain the respective ranks, that is, values between 1 and 10.\nBased on introspection about the cognitive effort to actually establish an order among 10 items, we decided to pull out the three top-ranked features per person and treat them as equal. Technically, a first step extracts and concatenate these, yielding an intermediary result of e.g.\n\n$ job_vals<fct> languages_frameworks;compensation;remote, industry;compensation;development, languages_frameworks;compensation;development\n\n\ndata <- data %>% mutate(\n  val_1 = if_else(\n   AssessJob1 == 1, \"industry\", if_else(\n    AssessJob2 == 1, \"company_financial_status\", if_else(\n      AssessJob3 == 1, \"department\", if_else(\n        AssessJob4 == 1, \"languages_frameworks\", if_else(\n          AssessJob5 == 1, \"compensation\", if_else(\n            AssessJob6 == 1, \"company_culture\", if_else(\n              AssessJob7 == 1, \"remote\", if_else(\n                AssessJob8 == 1, \"development\", if_else(\n                  AssessJob10 == 1, \"diversity\", \"impact\"))))))))),\n  val_2 = if_else(\n    AssessJob1 == 2, \"industry\", if_else(\n      AssessJob2 == 2, \"company_financial_status\", if_else(\n        AssessJob3 == 2, \"department\", if_else(\n          AssessJob4 == 2, \"languages_frameworks\", if_else(\n            AssessJob5 == 2, \"compensation\", if_else(\n              AssessJob6 == 2, \"company_culture\", if_else(\n                AssessJob7 == 1, \"remote\", if_else(\n                  AssessJob8 == 1, \"development\", if_else(\n                    AssessJob10 == 1, \"diversity\", \"impact\"))))))))),\n  val_3 = if_else(\n    AssessJob1 == 3, \"industry\", if_else(\n      AssessJob2 == 3, \"company_financial_status\", if_else(\n        AssessJob3 == 3, \"department\", if_else(\n          AssessJob4 == 3, \"languages_frameworks\", if_else(\n            AssessJob5 == 3, \"compensation\", if_else(\n              AssessJob6 == 3, \"company_culture\", if_else(\n                AssessJob7 == 3, \"remote\", if_else(\n                  AssessJob8 == 3, \"development\", if_else(\n                    AssessJob10 == 3, \"diversity\", \"impact\")))))))))\n  )\n\ndata <- data %>% mutate(\n  job_vals = paste(val_1, val_2, val_3, sep = \";\") %>% factor()\n)\n\ndata <- data %>% select(\n  -c(starts_with(\"AssessJob\"), starts_with(\"val_\"))\n)\n\nNow that column looks exactly like LanguageWorkedWith looked before, so we can use the same method as above to produce a one-hot-encoded version.\n\n\nvalues_tokenizer <- text_tokenizer(split = \";\", filters = \"\")\nvalues_tokenizer %>% fit_text_tokenizer(data$job_vals)\n\nSo what actually do respondents value most?\n\n                      name count\n1              compensation 27020\n2      languages_frameworks 24216\n3           company_culture 20432\n4               development 15981\n5                    impact 14869\n6                department 10452\n7                    remote 10396\n8                  industry  8294\n9                 diversity  7594\n10 company_financial_status  6576\nUsing the same method as above\n\n\njob_values <- values_tokenizer %>% texts_to_matrix(data$job_vals, mode = \"count\")\ndata <- data %>% cbind(job_values[, 2:11])\ndata <- data %>% rename_at(vars(`1`:`10`), funs(paste0(values_tokenizer$index_word[as.integer(.)])))\ndata <- data %>% select(-job_vals)\ndata %>% glimpse()\n\nwe end up with a dataset that looks like this\n\n> data %>% glimpse()\nObservations: 48,610\nVariables: 53\n$ FormalEducation          <fct> Bachelor’s degree (BA, BS, B.Eng., etc.), Bach...\n$ UndergradMajor           <fct> Mathematics or statistics, A natural science (...\n$ OperatingSystem          <fct> Linux-based, Linux-based, Windows, Linux-based...\n$ JS                       <dbl> 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0...\n$ EC                       <dbl> 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0...\n$ javascript               <dbl> 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1...\n$ html                     <dbl> 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1...\n$ css                      <dbl> 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1...\n$ sql                      <dbl> 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1...\n$ java                     <dbl> 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1...\n$ `bash/shell`             <dbl> 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1...\n$ python                   <dbl> 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0...\n$ `c#`                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0...\n$ php                      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1...\n$ `c++`                    <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0...\n$ typescript               <dbl> 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1...\n$ c                        <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0...\n$ ruby                     <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ swift                    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1...\n$ go                       <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0...\n$ `objective-c`            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ vb.net                   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ r                        <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ assembly                 <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ groovy                   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ scala                    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ matlab                   <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ kotlin                   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ vba                      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ perl                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ `visual basic 6`         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ coffeescript             <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ lua                      <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ `delphi/object pascal`   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ rust                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ haskell                  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ `f#`                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ clojure                  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ erlang                   <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ cobol                    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ ocaml                    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ julia                    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ hack                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ compensation             <dbl> 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0...\n$ languages_frameworks     <dbl> 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0...\n$ company_culture          <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ development              <dbl> 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0...\n$ impact                   <dbl> 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1...\n$ department               <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0...\n$ remote                   <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 1, 0...\n$ industry                 <dbl> 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1...\n$ diversity                <dbl> 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0...\n$ company_financial_status <dbl> 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1...\nwhich we further reduce to a design matrix X removing the binarized target variables\n\n\nX <- data %>% select(-c(JobSatisfaction, EthicsChoice))\n\nFrom here on, different actions will ensue depending on whether we choose the road of working with a one-hot model or an embeddings model of the predictors.\nThere is one other thing though to be done before: We want to work with the same train-test split in both cases.\n\n\ntrain_indices <- sample(1:nrow(X), 0.8 * nrow(X))\n\nOne-hot model\nGiven this is a post about embeddings, why show a one-hot model? First, for instructional reasons - you don’t see many of examples of deep learning on categorical data in the wild. Second, … but we’ll turn to that after having shown both models.\nFor the one-hot model, all that remains to be done is using Keras’ to_categorical on the three remaining variables that are not yet in one-hot form.\n\n\nX_one_hot <- X %>% map_if(is.factor, ~ as.integer(.x) - 1) %>%\n  map_at(\"FormalEducation\", ~ to_categorical(.x) %>% \n           array_reshape(c(length(.x), length(levels(data$FormalEducation))))) %>%\n  map_at(\"UndergradMajor\", ~ to_categorical(.x) %>% \n           array_reshape(c(length(.x), length(levels(data$UndergradMajor))))) %>%\n  map_at(\"OperatingSystem\", ~ to_categorical(.x) %>%\n           array_reshape(c(length(.x), length(levels(data$OperatingSystem))))) %>%\n  abind::abind(along = 2)\n\nWe divide up our dataset into train and validation parts\n\n\nx_train <- X_one_hot[train_indices, ] %>% as.matrix()\nx_valid <- X_one_hot[-train_indices, ] %>% as.matrix()\ny_train <- data$EthicsChoice[train_indices] %>% as.matrix()\ny_valid <- data$EthicsChoice[-train_indices] %>% as.matrix()\n\nand define a pretty straightforward MLP.\n\n\nmodel <- keras_model_sequential() %>%\n  layer_dense(\n    units = 128,\n    activation = \"selu\"\n  ) %>%\n  layer_dropout(0.5) %>%\n  layer_dense(\n    units = 128,\n    activation = \"selu\"\n  ) %>%\n  layer_dropout(0.5) %>%\n  layer_dense(\n    units = 128,\n    activation = \"selu\"\n  ) %>%\n  layer_dropout(0.5) %>%\n  layer_dense(\n    units = 128,\n    activation = \"selu\"\n  ) %>%\n  layer_dropout(0.5) %>%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %>% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = \"adam\",\n  metrics = \"accuracy\"\n  )\n\nTraining this model:\n\n\nhistory <- model %>% fit(\n  x_train,\n  y_train,\n  validation_data = list(x_valid, y_valid),\n  epochs = 20,\n  batch_size = 100\n)\n\nplot(history)\n\n…results in an accuracy on the validation set of 0.64 - not an impressive number per se, but interesting given the small amount of predictors and the choice of target variable.3\n\nEmbeddings model\nIn the embeddings model, we don’t need to use to_categorical on the remaining factors, as embedding layers can work with integer input data. We thus just convert the factors to integers:\n\n\nX_embed <- X %>%\n  mutate_if(is.factor, compose(partial(`-`, 1, .first = FALSE), as.integer))\n\nNow for the model. Effectively we have five groups of entities here: formal education, undergrad major, operating system, languages worked with, and highest-counting values with respect to jobs. Each of these groups get embedded separately, so we need to use the Keras functional API and declare five different inputs.\n\n\ninput_fe <- layer_input(shape = 1)        # formal education, encoded as integer\ninput_um <- layer_input(shape = 1)        # undergrad major, encoded as integer\ninput_os <- layer_input(shape = 1)        # operating system, encoded as integer\ninput_langs <- layer_input(shape = 38)    # languages worked with, multi-hot-encoded\ninput_vals <- layer_input(shape = 10)     # values, multi-hot-encoded\n\nHaving embedded them separately, we concatenate the outputs for further common processing.\n\n\nconcat <- layer_concatenate(\n  list(\n    input_fe %>%\n      layer_embedding(\n        input_dim = length(levels(data$FormalEducation)),\n        output_dim = 64,\n        name = \"fe\"\n      ) %>%\n      layer_flatten(),\n    input_um %>%\n      layer_embedding(\n        input_dim = length(levels(data$UndergradMajor)),\n        output_dim = 64,\n        name = \"um\"\n      ) %>%\n      layer_flatten(),\n    input_os %>%\n      layer_embedding(\n        input_dim = length(levels(data$OperatingSystem)),\n        output_dim = 64,\n        name = \"os\"\n      ) %>%\n      layer_flatten(),\n    input_langs %>%\n       layer_embedding(input_dim = 38, output_dim = 256,\n                       name = \"langs\")%>%\n       layer_flatten(),\n    input_vals %>%\n      layer_embedding(input_dim = 10, output_dim = 128,\n                      name = \"vals\")%>%\n      layer_flatten()\n  )\n)\n\noutput <- concat %>%\n  layer_dense(\n    units = 128,\n    activation = \"relu\"\n  ) %>%\n  layer_dropout(0.5) %>%\n  layer_dense(\n    units = 128,\n    activation = \"relu\"\n  ) %>%\n  layer_dropout(0.5) %>%\n  layer_dense(\n    units = 128,\n    activation = \"relu\"\n  ) %>%\n  layer_dense(\n    units = 128,\n    activation = \"relu\"\n  ) %>%\n  layer_dropout(0.5) %>%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\nSo there go model definition and compilation:\n\n\nmodel <- keras_model(list(input_fe, input_um, input_os, input_langs, input_vals), output)\n\nmodel %>% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = \"adam\",\n  metrics = \"accuracy\"\n  )\n\nNow to pass the data to the model, we need to chop it up into ranges of columns matching the inputs.\n\n\ny_train <- data$EthicsChoice[train_indices] %>% as.matrix()\ny_valid <- data$EthicsChoice[-train_indices] %>% as.matrix()\n\nx_train <-\n  list(\n    X_embed[train_indices, 1, drop = FALSE] %>% as.matrix() ,\n    X_embed[train_indices , 2, drop = FALSE] %>% as.matrix(),\n    X_embed[train_indices , 3, drop = FALSE] %>% as.matrix(),\n    X_embed[train_indices , 4:41, drop = FALSE] %>% as.matrix(),\n    X_embed[train_indices , 42:51, drop = FALSE] %>% as.matrix()\n  )\nx_valid <- list(\n  X_embed[-train_indices, 1, drop = FALSE] %>% as.matrix() ,\n  X_embed[-train_indices , 2, drop = FALSE] %>% as.matrix(),\n  X_embed[-train_indices , 3, drop = FALSE] %>% as.matrix(),\n  X_embed[-train_indices , 4:41, drop = FALSE] %>% as.matrix(),\n  X_embed[-train_indices , 42:51, drop = FALSE] %>% as.matrix()\n)\n\nAnd we’re ready to train.\n\n\nmodel %>% fit(\n  x_train,\n  y_train,\n  validation_data = list(x_valid, y_valid),\n  epochs = 20,\n  batch_size = 100\n)\n\nUsing the same train-test split as before, this results in an accuracy of … ~0.64 (just as before). Now we said from the start that using embeddings could serve different purposes, and that in this first use case, we wanted to demonstrate their use for extracting latent relationships. And in any case you could argue that the task is too hard - probably there just is not much of a relationship between the predictors we chose and the target.\nBut this also warrants a more general comment. With all current enthusiasm about using embeddings on tabular data, we are not aware of any systematic comparisons with one-hot-encoded data as regards the actual effect on performance, nor do we know of systematic analyses under what circumstances embeddings will probably be of help. Our working hypothesis is that in the setup we chose, the dimensionality of the original data is so low that the information can simply be encoded “as is” by the network - as long as we create it with sufficient capacity. Our second use case will therefore use data where - hopefully - this won’t be the case.\nBut before, let’s get to the main purpose of this use case: How can we extract those latent relationships from the network?\nExtracting relationships from the learned embeddings\nWe’ll show the code here for the job values embeddings, - it is directly transferable to the other ones. The embeddings, that’s just the weight matrix of the respective layer, of dimension number of different values times embedding size.\n\n\nemb_vals <- (model$get_layer(\"vals\") %>% get_weights())[[1]]\nemb_vals %>% dim() # 10x128\n\nWe can then perform dimensionality reduction on the raw values, e.g., PCA\n\n\npca <- prcomp(emb_vals, center = TRUE, scale. = TRUE, rank = 2)$x[, c(\"PC1\", \"PC2\")]\n\nand plot the results.\n\n\npca %>%\n  as.data.frame() %>%\n  mutate(class = attr(values_tokenizer$word_index, \"names\")) %>%\n  ggplot(aes(x = PC1, y = PC2)) +\n  geom_point() +\n  geom_label_repel(aes(label = class))\n\nThis is what we get (displaying four of the five variables we used embeddings on):\nTwo first principal components of the embeddings for undergrad major (top left), operating system (top right), programming language used (bottom left), and primary values with respect to jobs (bottom right)Now we’ll definitely refrain from taking this too seriously, given the modest accuracy on the prediction task that lead to these embedding matrices.4 Certainly when assessing the obtained factorization, performance on the main task has to be taken into account.\nBut we’d like to point out something else too: In contrast to unsupervised and semi-supervised techniques like PCA or autoencoders, we made use of an extraneous variable (the ethical behavior to be predicted). So any learned relationships are never “absolute”, but always to be seen in relation to the way they were learned. This is why we chose an additional target variable, JobSatisfaction, so we could compare the embeddings learned on two different tasks. We won’t refer the concrete results here as accuracy turned out to be even lower than with EthicsChoice. We do, however, want to stress this inherent difference to representations learned by, e.g., autoencoders.\nNow let’s address the second use case.\nEmbedding for profit (improving accuracy)\nOur second task here is about fraud detection. The dataset is contained in the DMwR2 package and is called sales:\n\n\ndata(sales, package = \"DMwR2\")\nsales\n\n\n# A tibble: 401,146 x 5\n   ID    Prod  Quant   Val Insp \n   <fct> <fct> <int> <dbl> <fct>\n 1 v1    p1      182  1665 unkn \n 2 v2    p1     3072  8780 unkn \n 3 v3    p1    20393 76990 unkn \n 4 v4    p1      112  1100 unkn \n 5 v3    p1     6164 20260 unkn \n 6 v5    p2      104  1155 unkn \n 7 v6    p2      350  5680 unkn \n 8 v7    p2      200  4010 unkn \n 9 v8    p2      233  2855 unkn \n10 v9    p2      118  1175 unkn \n# ... with 401,136 more rows\nEach row indicates a transaction reported by a salesperson, - ID being the salesperson ID, Prod a product ID, Quant the quantity sold, Val the amount of money it was sold for, and Insp indicating one of three possibilities: (1) the transaction was examined and found fraudulent, (2) it was examined and found okay, and (3) it has not been examined (the vast majority of cases).\nWhile this dataset “cries” for semi-supervised techniques (to make use of the overwhelming amount of unlabeled data), we want to see if using embeddings can help us improve accuracy on a supervised task.\nWe thus recklessly throw away incomplete data as well as all unlabeled entries\n\n\nsales <- filter(sales, !(is.na(Quant)))\nsales <- filter(sales, !(is.na(Val)))\n\nsales <- droplevels(sales %>% filter(Insp != \"unkn\"))\nnrow(sales)\n\nwhich leaves us with 15546 transactions.\nOne-hot model\nNow we prepare the data for the one-hot model we want to compare against:\nWith 2821 levels, salesperson ID is far too high-dimensional to work well with one-hot encoding, so we completely drop that column.\nProduct id (Prod) has “just” 797 levels, but with one-hot-encoding, that still results in significant memory demand. We thus zoom in on the 500 top-sellers.\nThe continuous variables Quant and Val are normalized to values between 0 and 1 so they fit with the one-hot-encoded Prod.\n\n\nsales_1hot <- sales\n\nnormalize <- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n\ntop_n <- 500\ntop_prods <- sales_1hot %>% \n  group_by(Prod) %>% \n  summarise(cnt = n()) %>% \n  arrange(desc(cnt)) %>%\n  head(top_n) %>%\n  select(Prod) %>%\n  pull()\nsales_1hot <- droplevels(sales_1hot %>% filter(Prod %in% top_prods))\n\nsales_1hot <- sales_1hot %>%\n  select(-ID) %>%\n  map_if(is.factor, ~ as.integer(.x) - 1) %>%\n  map_at(\"Prod\", ~ to_categorical(.x) %>% array_reshape(c(length(.x), top_n))) %>%\n  map_at(\"Quant\", ~ normalize(.x) %>% array_reshape(c(length(.x), 1))) %>%\n  map_at(\"Val\", ~ normalize(.x) %>% array_reshape(c(length(.x), 1))) %>%\n  abind(along = 2)\n\nWe then perform the usual train-test split.\n\n\ntrain_indices <- sample(1:nrow(sales_1hot), 0.7 * nrow(sales_1hot))\n\nX_train <- sales_1hot[train_indices, 1:502] \ny_train <-  sales_1hot[train_indices, 503] %>% as.matrix()\n\nX_valid <- sales_1hot[-train_indices, 1:502] \ny_valid <-  sales_1hot[-train_indices, 503] %>% as.matrix()\n\nFor classification on this dataset, which will be the baseline to beat?\n\n\nxtab_train  <- y_train %>% table()\nxtab_valid  <- y_valid %>% table()\nlist(xtab_train[1]/(xtab_train[1] + xtab_train[2]), xtab_valid[1]/(xtab_valid[1] + xtab_valid[2]))\n\n\n[[1]]\n        0 \n0.9393547 \n\n[[2]]\n        0 \n0.9384437 \nSo if we don’t get beyond 94% accuracy on both training and validation sets, we may just as well predict “okay” for every transaction.\nHere then is the model, plus the training routine and evaluation:\n\n\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 256, activation = \"selu\") %>%\n  layer_dropout(dropout_rate) %>% \n  layer_dense(units = 256, activation = \"selu\") %>%\n  layer_dropout(dropout_rate) %>% \n  layer_dense(units = 256, activation = \"selu\") %>%\n  layer_dropout(dropout_rate) %>% \n  layer_dense(units = 256, activation = \"selu\") %>%\n  layer_dropout(dropout_rate) %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %>% compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = c(\"accuracy\"))\n\nmodel %>% fit(\n  X_train,\n  y_train,\n  validation_data = list(X_valid, y_valid),\n  class_weights = list(\"0\" = 0.1, \"1\" = 0.9),\n  batch_size = 128,\n  epochs = 200\n)\n\nmodel %>% evaluate(X_train, y_train, batch_size = 100) \nmodel %>% evaluate(X_valid, y_valid, batch_size = 100) \n\nThis model achieved optimal validation accuracy at a dropout rate of 0.2. At that rate, training accuracy was 0.9761, and validation accuracy was 0.9507. At all dropout rates lower than 0.7, validation accuracy did indeed surpass the majority vote baseline.\nCan we further improve performance by embedding the product id?\nEmbeddings model\nFor better comparability, we again discard salesperson information and cap the number of different products at 500. Otherwise, data preparation goes as expected for this model:\n\n\nsales_embed <- sales\n\ntop_prods <- sales_embed %>% \n  group_by(Prod) %>% \n  summarise(cnt = n()) %>% \n  arrange(desc(cnt)) %>% \n  head(top_n) %>% \n  select(Prod) %>% \n  pull()\n\nsales_embed <- droplevels(sales_embed %>% filter(Prod %in% top_prods))\n\nsales_embed <- sales_embed %>%\n  select(-ID) %>%\n  mutate_if(is.factor, ~ as.integer(.x) - 1) %>%\n  mutate(Quant = scale(Quant)) %>%\n  mutate(Val = scale(Val))\n\nX_train <- sales_embed[train_indices, 1:3] %>% as.matrix()\ny_train <-  sales_embed[train_indices, 4] %>% as.matrix()\n\nX_valid <- sales_embed[-train_indices, 1:3] %>% as.matrix()\ny_valid <-  sales_embed[-train_indices, 4] %>% as.matrix()\n\nThe model we define is as similar as possible to the one-hot alternative:\n\n\nprod_input <- layer_input(shape = 1)\ncont_input <- layer_input(shape = 2)\n\nprod_embed <- prod_input %>% \n  layer_embedding(input_dim = sales_embed$Prod %>% max() + 1,\n                  output_dim = 256\n                  ) %>%\n  layer_flatten()\ncont_dense <- cont_input %>% layer_dense(units = 256, activation = \"selu\")\n\noutput <- layer_concatenate(\n  list(prod_embed, cont_dense)) %>%\n  layer_dropout(dropout_rate) %>% \n  layer_dense(units = 256, activation = \"selu\") %>%\n  layer_dropout(dropout_rate) %>% \n  layer_dense(units = 256, activation = \"selu\") %>%\n  layer_dropout(dropout_rate) %>% \n  layer_dense(units = 256, activation = \"selu\") %>%\n  layer_dropout(dropout_rate) %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n  \nmodel <- keras_model(inputs = list(prod_input, cont_input), outputs = output)\n\nmodel %>% compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = \"accuracy\")\n\nmodel %>% fit(\n  list(X_train[ , 1], X_train[ , 2:3]),\n  y_train,\n  validation_data = list(list(X_valid[ , 1], X_valid[ , 2:3]), y_valid),\n  class_weights = list(\"0\" = 0.1, \"1\" = 0.9),\n  batch_size = 128,\n  epochs = 200\n)\n\nmodel %>% evaluate(list(X_train[ , 1], X_train[ , 2:3]), y_train) \nmodel %>% evaluate(list(X_valid[ , 1], X_valid[ , 2:3]), y_valid)        \n\nThis time, accuracies are in fact higher: At the optimal dropout rate (0.3 in this case), training resp. validation accuracy are at 0.9913 and 0.9666, respectively. Quite a difference!\nSo why did we choose this dataset? In contrast to our previous dataset, here the categorical variable is high-dimensional, so well suited for compression and densification. It is interesting that we can make such good use of an ID without knowing what it stands for!\nConclusion\nIn this post, we’ve shown two use cases of embeddings in “simple” tabular data. As stated in the introduction, to us, embeddings are what you make of them. In that vein, if you’ve used embeddings to accomplish things that mattered to your task at hand, please comment and tell us about it!\n\n\nGuo, Cheng, and Felix Berkhahn. 2016. “Entity Embeddings of Categorical Variables.” CoRR abs/1604.06737. http://arxiv.org/abs/1604.06737.\n\n\nMikolov, Tomas, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Distributed Representations of Words and Phrases and Their Compositionality.” CoRR abs/1310.4546. http://arxiv.org/abs/1310.4546.\n\n\nWe did think it prudent though to omit variables like country, ethnicity or gender.↩︎\nat least given the way we binarized answers (more on that soon)↩︎\nAs usual when not working with one the “flagship areas” of deep learning, comparisons against other machine learning methods would be interesting. We did, however, not want to further elongate the post, nor distract from its main focus, namely, the use of embeddings with categorical data.↩︎\nNo, no, of course we’re not implying that for programming languages, the second principal component, with R and assembly at its extremes, stands for high-level vs. low-level language here.↩︎\n",
    "preview": "posts/2018-11-26-embeddings-fun-and-profit/images/thumb.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 820,
    "preview_height": 410
  },
  {
    "path": "posts/2018-11-12-uncertainty_estimates_dropout/",
    "title": "You sure? A Bayesian approach to obtaining uncertainty estimates from neural networks",
    "description": "In deep learning, there is no obvious way of obtaining uncertainty estimates. In 2016, Gal and Ghahramani proposed a method that is both theoretically grounded and practical: use dropout at test time. In this post, we introduce a refined version of this method (Gal et al. 2017) that has the network itself learn how uncertain it is.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-11-12",
    "categories": [
      "Image Recognition & Image Processing",
      "Probabilistic ML/DL",
      "TensorFlow/Keras"
    ],
    "contents": "\nIf there were a set of survival rules for data scientists, among them would have to be this: Always report uncertainty estimates with your predictions. However, here we are, working with neural networks, and unlike lm, a Keras model does not conveniently output something like a standard error for the weights. We might try to think of rolling your own uncertainty measure - for example, averaging predictions from networks trained from different random weight initializations, for different numbers of epochs, or on different subsets of the data. But we might still be worried that our method is quite a bit, well … ad hoc.\nIn this post, we’ll see a both practical as well as theoretically grounded approach to obtaining uncertainty estimates from neural networks. First, however, let’s quickly talk about why uncertainty is that important - over and above its potential to save a data scientist’s job.\nWhy uncertainty?\nIn a society where automated algorithms are - and will be - entrusted with more and more life-critical tasks, one answer immediately jumps to mind: If the algorithm correctly quantifies its uncertainty, we may have human experts inspect the more uncertain predictions and potentially revise them.\nThis will only work if the network’s self-indicated uncertainty really is indicative of a higher probability of misclassification. Leibig et al.(Leibig et al. 2017) used a predecessor of the method described below to assess neural network uncertainty in detecting diabetic retinopathy. They found that indeed, the distributions of uncertainty were different depending on whether the answer was correct or not:\nFigure from Leibig et al. 2017 (Leibig et al. 2017). Green: uncertainty estimates for wrong predictions. Blue: uncertainty estimates for correct predictions.In addition to quantifying uncertainty, it can make sense to qualify it. In the Bayesian deep learning literature, a distinction is commonly made between epistemic uncertainty and aleatoric uncertainty (Kendall and Gal 2017). Epistemic uncertainty refers to imperfections in the model - in the limit of infinite data, this kind of uncertainty should be reducible to 0. Aleatoric uncertainty is due to data sampling and measurement processes and does not depend on the size of the dataset.\nSay we train a model for object detection. With more data, the model should become more sure about what makes a unicycle different from a mountainbike. However, let’s assume all that’s visible of the mountainbike is the front wheel, the fork and the head tube. Then it doesn’t look so different from a unicycle any more!\nWhat would be the consequences if we could distinguish both types of uncertainty? If epistemic uncertainty is high, we can try to get more training data. The remaining aleatoric uncertainty should then keep us cautioned to factor in safety margins in our application.\nProbably no further justifications are required of why we might want to assess model uncertainty - but how can we do this?\nUncertainty estimates through Bayesian deep learning\nIn a Bayesian world, in principle, uncertainty is for free as we don’t just get point estimates (the maximum aposteriori) but the full posterior distribution. Strictly speaking, in Bayesian deep learning, priors should be put over the weights, and the posterior be determined according to Bayes’ rule. To the deep learning practitioner, this sounds pretty arduous - and how do you do it using Keras?\nIn 2016 though, Gal and Ghahramani (Gal and Ghahramani 2016) showed that when viewing a neural network as an approximation to a Gaussian process, uncertainty estimates can be obtained in a theoretically grounded yet very practical way: by training a network with dropout and then, using dropout at test time too. At test time, dropout lets us extract Monte Carlo samples from the posterior, which can then be used to approximate the true posterior distribution.\n\nYarin Gal has a nice writeup of the why and how on his blog.\nThis is already good news, but it leaves one question open: How do we choose an appropriate dropout rate? The answer is: let the network learn it.\nLearning dropout and uncertainty\nIn several 2017 papers (Gal, Hron, and Kendall 2017),(Kendall and Gal 2017), Gal and his coworkers demonstrated how a network can be trained to dynamically adapt the dropout rate so it is adequate for the amount and characteristics of the data given.\nBesides the predictive mean of the target variable, it can additionally be made to learn the variance. This means we can calculate both types of uncertainty, epistemic and aleatoric, independently, which is useful in the light of their different implications. We then add them up to obtain the overall predictive uncertainty.\nLet’s make this concrete and see how we can implement and test the intended behavior on simulated data. In the implementation, there are three things warranting our special attention:\nThe wrapper class used to add learnable-dropout behavior to a Keras layer;\nThe loss function designed to minimize aleatoric uncertainty; and\nThe ways we can obtain both uncertainties at test time.\nLet’s start with the wrapper.\nA wrapper for learning dropout\nIn this example, we’ll restrict ourselves to learning dropout for dense layers. Technically, we’ll add a weight and a loss to every dense layer we want to use dropout with. This means we’ll create a custom wrapper class that has access to the underlying layer and can modify it.\nThe logic implemented in the wrapper is derived mathematically in the Concrete Dropout paper (Gal, Hron, and Kendall 2017). The below code is a port to R of the Python Keras version found in the paper’s companion github repo.\nSo first, here is the wrapper class - we’ll see how to use it in just a second:\n\n\nlibrary(keras)\n\n# R6 wrapper class, a subclass of KerasWrapper\nConcreteDropout <- R6::R6Class(\"ConcreteDropout\",\n  \n  inherit = KerasWrapper,\n  \n  public = list(\n    weight_regularizer = NULL,\n    dropout_regularizer = NULL,\n    init_min = NULL,\n    init_max = NULL,\n    is_mc_dropout = NULL,\n    supports_masking = TRUE,\n    p_logit = NULL,\n    p = NULL,\n    \n    initialize = function(weight_regularizer,\n                          dropout_regularizer,\n                          init_min,\n                          init_max,\n                          is_mc_dropout) {\n      self$weight_regularizer <- weight_regularizer\n      self$dropout_regularizer <- dropout_regularizer\n      self$is_mc_dropout <- is_mc_dropout\n      self$init_min <- k_log(init_min) - k_log(1 - init_min)\n      self$init_max <- k_log(init_max) - k_log(1 - init_max)\n    },\n    \n    build = function(input_shape) {\n      super$build(input_shape)\n      \n      self$p_logit <- super$add_weight(\n        name = \"p_logit\",\n        shape = shape(1),\n        initializer = initializer_random_uniform(self$init_min, self$init_max),\n        trainable = TRUE\n      )\n\n      self$p <- k_sigmoid(self$p_logit)\n\n      input_dim <- input_shape[[2]]\n\n      weight <- private$py_wrapper$layer$kernel\n      \n      kernel_regularizer <- self$weight_regularizer * \n                            k_sum(k_square(weight)) / \n                            (1 - self$p)\n      \n      dropout_regularizer <- self$p * k_log(self$p)\n      dropout_regularizer <- dropout_regularizer +  \n                             (1 - self$p) * k_log(1 - self$p)\n      dropout_regularizer <- dropout_regularizer * \n                             self$dropout_regularizer * \n                             k_cast(input_dim, k_floatx())\n\n      regularizer <- k_sum(kernel_regularizer + dropout_regularizer)\n      super$add_loss(regularizer)\n    },\n    \n    concrete_dropout = function(x) {\n      eps <- k_cast_to_floatx(k_epsilon())\n      temp <- 0.1\n      \n      unif_noise <- k_random_uniform(shape = k_shape(x))\n      \n      drop_prob <- k_log(self$p + eps) - \n                   k_log(1 - self$p + eps) + \n                   k_log(unif_noise + eps) - \n                   k_log(1 - unif_noise + eps)\n      drop_prob <- k_sigmoid(drop_prob / temp)\n      \n      random_tensor <- 1 - drop_prob\n      \n      retain_prob <- 1 - self$p\n      x <- x * random_tensor\n      x <- x / retain_prob\n      x\n    },\n\n    call = function(x, mask = NULL, training = NULL) {\n      if (self$is_mc_dropout) {\n        super$call(self$concrete_dropout(x))\n      } else {\n        k_in_train_phase(\n          function()\n            super$call(self$concrete_dropout(x)),\n          super$call(x),\n          training = training\n        )\n      }\n    }\n  )\n)\n\n# function for instantiating custom wrapper\nlayer_concrete_dropout <- function(object, \n                                   layer,\n                                   weight_regularizer = 1e-6,\n                                   dropout_regularizer = 1e-5,\n                                   init_min = 0.1,\n                                   init_max = 0.1,\n                                   is_mc_dropout = TRUE,\n                                   name = NULL,\n                                   trainable = TRUE) {\n  create_wrapper(ConcreteDropout, object, list(\n    layer = layer,\n    weight_regularizer = weight_regularizer,\n    dropout_regularizer = dropout_regularizer,\n    init_min = init_min,\n    init_max = init_max,\n    is_mc_dropout = is_mc_dropout,\n    name = name,\n    trainable = trainable\n  ))\n}\n\nThe wrapper instantiator has default arguments, but two of them should be adapted to the data: weight_regularizer and dropout_regularizer. Following the authors’ recommendations, they should be set as follows.\nFirst, choose a value for hyperparameter \\(l\\). In this view of a neural network as an approximation to a Gaussian process, \\(l\\) is the prior length-scale, our a priori assumption about the frequency characteristics of the data. Here, we follow Gal’s demo in setting l := 1e-4. Then the initial values for weight_regularizer and dropout_regularizer are derived from the length-scale and the sample size.\n\n\n# sample size (training data)\nn_train <- 1000\n# sample size (validation data)\nn_val <- 1000\n# prior length-scale\nl <- 1e-4\n# initial value for weight regularizer \nwd <- l^2/n_train\n# initial value for dropout regularizer\ndd <- 2/n_train\n\nNow let’s see how to use the wrapper in a model.\nDropout model\nIn our demonstration, we’ll have a model with three hidden dense layers, each of which will have its dropout rate calculated by a dedicated wrapper.\n\n\n# we use one-dimensional input data here, but this isn't a necessity\ninput_dim <- 1\n# this too could be > 1 if we wanted\noutput_dim <- 1\nhidden_dim <- 1024\n\ninput <- layer_input(shape = input_dim)\n\noutput <- input %>% layer_concrete_dropout(\n  layer = layer_dense(units = hidden_dim, activation = \"relu\"),\n  weight_regularizer = wd,\n  dropout_regularizer = dd\n  ) %>% layer_concrete_dropout(\n  layer = layer_dense(units = hidden_dim, activation = \"relu\"),\n  weight_regularizer = wd,\n  dropout_regularizer = dd\n  ) %>% layer_concrete_dropout(\n  layer = layer_dense(units = hidden_dim, activation = \"relu\"),\n  weight_regularizer = wd,\n  dropout_regularizer = dd\n)\n\nNow, model output is interesting: We have the model yielding not just the predictive (conditional) mean, but also the predictive variance (\\(\\tau^{-1}\\) in Gaussian process parlance):\n\n\nmean <- output %>% layer_concrete_dropout(\n  layer = layer_dense(units = output_dim),\n  weight_regularizer = wd,\n  dropout_regularizer = dd\n)\n\nlog_var <- output %>% layer_concrete_dropout(\n  layer_dense(units = output_dim),\n  weight_regularizer = wd,\n  dropout_regularizer = dd\n)\n\noutput <- layer_concatenate(list(mean, log_var))\n\nmodel <- keras_model(input, output)\n\nThe significant thing here is that we learn different variances for different data points. We thus hope to be able to account for heteroscedasticity (different degrees of variability) in the data.\nHeteroscedastic loss\nAccordingly, instead of mean squared error we use a cost function that does not treat all estimates alike(Kendall and Gal 2017):\n\\[\\frac{1}{N} \\sum_i{\\frac{1}{2 \\hat{\\sigma}^2_i} \\ (\\mathbf{y}_i - \\mathbf{\\hat{y}}_i)^2 + \\frac{1}{2} log \\ \\hat{\\sigma}^2_i}\\]\nIn addition to the obligatory target vs. prediction check, this cost function contains two regularization terms:\nFirst, \\(\\frac{1}{2 \\hat{\\sigma}^2_i}\\) downweights the high-uncertainty predictions in the loss function. Put plainly: The model is encouraged to indicate high uncertainty when its predictions are false.\nSecond, \\(\\frac{1}{2} log \\ \\hat{\\sigma}^2_i\\) makes sure the network does not simply indicate high uncertainty everywhere.\nThis logic maps directly to the code (except that as usual, we’re calculating with the log of the variance, for reasons of numerical stability):\n\n\nheteroscedastic_loss <- function(y_true, y_pred) {\n    mean <- y_pred[, 1:output_dim]\n    log_var <- y_pred[, (output_dim + 1):(output_dim * 2)]\n    precision <- k_exp(-log_var)\n    k_sum(precision * (y_true - mean) ^ 2 + log_var, axis = 2)\n  }\n\nTraining on simulated data\nNow we generate some test data and train the model.\n\n\ngen_data_1d <- function(n) {\n  sigma <- 1\n  X <- matrix(rnorm(n))\n  w <- 2\n  b <- 8\n  Y <- matrix(X %*% w + b + sigma * rnorm(n))\n  list(X, Y)\n}\n\nc(X, Y) %<-% gen_data_1d(n_train + n_val)\n\nc(X_train, Y_train) %<-% list(X[1:n_train], Y[1:n_train])\nc(X_val, Y_val) %<-% list(X[(n_train + 1):(n_train + n_val)], \n                          Y[(n_train + 1):(n_train + n_val)])\n\nmodel %>% compile(\n  optimizer = \"adam\",\n  loss = heteroscedastic_loss,\n  metrics = c(custom_metric(\"heteroscedastic_loss\", heteroscedastic_loss))\n)\n\nhistory <- model %>% fit(\n  X_train,\n  Y_train,\n  epochs = 30,\n  batch_size = 10\n)\n\nWith training finished, we turn to the validation set to obtain estimates on unseen data - including those uncertainty measures this is all about!\nObtain uncertainty estimates via Monte Carlo sampling\nAs often in a Bayesian setup, we construct the posterior (and thus, the posterior predictive) via Monte Carlo sampling. Unlike in traditional use of dropout, there is no change in behavior between training and test phases: Dropout stays “on”.\nSo now we get an ensemble of model predictions on the validation set:\n\n\nnum_MC_samples <- 20\n\nMC_samples <- array(0, dim = c(num_MC_samples, n_val, 2 * output_dim))\nfor (k in 1:num_MC_samples) {\n  MC_samples[k, , ] <- (model %>% predict(X_val))\n}\n\nRemember, our model predicts the mean as well as the variance. We’ll use the former for calculating epistemic uncertainty, while aleatoric uncertainty is obtained from the latter.\nFirst, we determine the predictive mean as an average of the MC samples’ mean output:\n\n\n# the means are in the first output column\nmeans <- MC_samples[, , 1:output_dim]  \n# average over the MC samples\npredictive_mean <- apply(means, 2, mean) \n\nTo calculate epistemic uncertainty, we again use the mean output, but this time we’re interested in the variance of the MC samples:\n\n\nepistemic_uncertainty <- apply(means, 2, var) \n\nThen aleatoric uncertainty is the average over the MC samples of the variance output.1.\n\n\nlogvar <- MC_samples[, , (output_dim + 1):(output_dim * 2)]\naleatoric_uncertainty <- exp(colMeans(logvar))\n\nNote how this procedure gives us uncertainty estimates individually for every prediction. How do they look?\n\n\ndf <- data.frame(\n  x = X_val,\n  y_pred = predictive_mean,\n  e_u_lower = predictive_mean - sqrt(epistemic_uncertainty),\n  e_u_upper = predictive_mean + sqrt(epistemic_uncertainty),\n  a_u_lower = predictive_mean - sqrt(aleatoric_uncertainty),\n  a_u_upper = predictive_mean + sqrt(aleatoric_uncertainty),\n  u_overall_lower = predictive_mean - \n                    sqrt(epistemic_uncertainty) - \n                    sqrt(aleatoric_uncertainty),\n  u_overall_upper = predictive_mean + \n                    sqrt(epistemic_uncertainty) + \n                    sqrt(aleatoric_uncertainty)\n)\n\nHere, first, is epistemic uncertainty, with shaded bands indicating one standard deviation above resp. below the predicted mean:\n\n\nggplot(df, aes(x, y_pred)) + \n  geom_point() + \n  geom_ribbon(aes(ymin = e_u_lower, ymax = e_u_upper), alpha = 0.3)\n\nEpistemic uncertainty on the validation set, train size = 1000.This is interesting. The training data (as well as the validation data) were generated from a standard normal distribution, so the model has encountered many more examples close to the mean than outside two, or even three, standard deviations. So it correctly tells us that in those more exotic regions, it feels pretty unsure about its predictions.\nThis is exactly the behavior we want: Risk in automatically applying machine learning methods arises due to unanticipated differences between the training and test (real world) distributions. If the model were to tell us “ehm, not really seen anything like that before, don’t really know what to do” that’d be an enormously valuable outcome.\nSo while epistemic uncertainty has the algorithm reflecting on its model of the world - potentially admitting its shortcomings - aleatoric uncertainty, by definition, is irreducible. Of course, that doesn’t make it any less valuable - we’d know we always have to factor in a safety margin. So how does it look here?\nAleatoric uncertainty on the validation set, train size = 1000.Indeed, the extent of uncertainty does not depend on the amount of data seen at training time.\nFinally, we add up both types to obtain the overall uncertainty when making predictions.\nOverall predictive uncertainty on the validation set, train size = 1000.Now let’s try this method on a real-world dataset.\nCombined cycle power plant electrical energy output estimation\nThis dataset is available from the UCI Machine Learning Repository. We explicitly chose a regression task with continuous variables exclusively, to make for a smooth transition from the simulated data.\nIn the dataset providers’ own words\n\nThe dataset contains 9568 data points collected from a Combined Cycle Power Plant over 6 years (2006-2011), when the power plant was set to work with full load. Features consist of hourly average ambient variables Temperature (T), Ambient Pressure (AP), Relative Humidity (RH) and Exhaust Vacuum (V) to predict the net hourly electrical energy output (EP) of the plant.\n\n\nA combined cycle power plant (CCPP) is composed of gas turbines (GT), steam turbines (ST) and heat recovery steam generators. In a CCPP, the electricity is generated by gas and steam turbines, which are combined in one cycle, and is transferred from one turbine to another. While the Vacuum is collected from and has effect on the Steam Turbine, the other three of the ambient variables effect the GT performance.\n\nWe thus have four predictors and one target variable. We’ll train five models: four single-variable regressions and one making use of all four predictors. It probably goes without saying that our goal here is to inspect uncertainty information, not to fine-tune the model.\nSetup\nLet’s quickly inspect those five variables. Here PE is energy output, the target variable.\n\n\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(GGally)\n\ndf <- read_xlsx(\"CCPP/Folds5x2_pp.xlsx\")\nggscatmat(df)\n\n\nWe scale and divide up the data\n\n\ndf_scaled <- scale(df)\n\nX <- df_scaled[, 1:4]\ntrain_samples <- sample(1:nrow(df_scaled), 0.8 * nrow(X))\nX_train <- X[train_samples,]\nX_val <- X[-train_samples,]\n\ny <- df_scaled[, 5] %>% as.matrix()\ny_train <- y[train_samples,]\ny_val <- y[-train_samples,]\n\nand get ready for training a few models.\n\n\nn <- nrow(X_train)\nn_epochs <- 100\nbatch_size <- 100\noutput_dim <- 1\nnum_MC_samples <- 20\nl <- 1e-4\nwd <- l^2/n\ndd <- 2/n\n\nget_model <- function(input_dim, hidden_dim) {\n  \n  input <- layer_input(shape = input_dim)\n  output <-\n    input %>% layer_concrete_dropout(\n      layer = layer_dense(units = hidden_dim, activation = \"relu\"),\n      weight_regularizer = wd,\n      dropout_regularizer = dd\n    ) %>% layer_concrete_dropout(\n      layer = layer_dense(units = hidden_dim, activation = \"relu\"),\n      weight_regularizer = wd,\n      dropout_regularizer = dd\n    ) %>% layer_concrete_dropout(\n      layer = layer_dense(units = hidden_dim, activation = \"relu\"),\n      weight_regularizer = wd,\n      dropout_regularizer = dd\n    )\n  \n  mean <-\n    output %>% layer_concrete_dropout(\n      layer = layer_dense(units = output_dim),\n      weight_regularizer = wd,\n      dropout_regularizer = dd\n    )\n  \n  log_var <-\n    output %>% layer_concrete_dropout(\n      layer_dense(units = output_dim),\n      weight_regularizer = wd,\n      dropout_regularizer = dd\n    )\n  \n  output <- layer_concatenate(list(mean, log_var))\n  \n  model <- keras_model(input, output)\n  \n  heteroscedastic_loss <- function(y_true, y_pred) {\n    mean <- y_pred[, 1:output_dim]\n    log_var <- y_pred[, (output_dim + 1):(output_dim * 2)]\n    precision <- k_exp(-log_var)\n    k_sum(precision * (y_true - mean) ^ 2 + log_var, axis = 2)\n  }\n  \n  model %>% compile(optimizer = \"adam\",\n                    loss = heteroscedastic_loss,\n                    metrics = c(\"mse\"))\n  model\n}\n\nWe’ll train each of the five models with a hidden_dim of 64. We then obtain 20 Monte Carlo sample from the posterior predictive distribution and calculate the uncertainties as before.\nHere we show the code for the first predictor, “AT”. It is similar for all other cases.\n\n\nmodel <- get_model(1, 64)\nhist <- model %>% fit(\n  X_train[ ,1],\n  y_train,\n  validation_data = list(X_val[ , 1], y_val),\n  epochs = n_epochs,\n  batch_size = batch_size\n)\n\nMC_samples <- array(0, dim = c(num_MC_samples, nrow(X_val), 2 * output_dim))\nfor (k in 1:num_MC_samples) {\n  MC_samples[k, ,] <- (model %>% predict(X_val[ ,1]))\n}\n\nmeans <- MC_samples[, , 1:output_dim]  \npredictive_mean <- apply(means, 2, mean) \nepistemic_uncertainty <- apply(means, 2, var) \nlogvar <- MC_samples[, , (output_dim + 1):(output_dim * 2)]\naleatoric_uncertainty <- exp(colMeans(logvar))\n\npreds <- data.frame(\n  x1 = X_val[, 1],\n  y_true = y_val,\n  y_pred = predictive_mean,\n  e_u_lower = predictive_mean - sqrt(epistemic_uncertainty),\n  e_u_upper = predictive_mean + sqrt(epistemic_uncertainty),\n  a_u_lower = predictive_mean - sqrt(aleatoric_uncertainty),\n  a_u_upper = predictive_mean + sqrt(aleatoric_uncertainty),\n  u_overall_lower = predictive_mean - \n                    sqrt(epistemic_uncertainty) - \n                    sqrt(aleatoric_uncertainty),\n  u_overall_upper = predictive_mean + \n                    sqrt(epistemic_uncertainty) + \n                    sqrt(aleatoric_uncertainty)\n)\n\nResult\nNow let’s see the uncertainty estimates for all five models!\nFirst, the single-predictor setup. Ground truth values are displayed in cyan, posterior predictive estimates are black, and the grey bands extend up resp. down by the square root of the calculated uncertainties.\nWe’re starting with ambient temperature, a low-variance predictor. We are surprised how confident the model is that it’s gotten the process logic correct, but high aleatoric uncertainty makes up for this (more or less).\nUncertainties on the validation set using ambient temperature as a single predictor.Now looking at the other predictors, where variance is much higher in the ground truth, it does get a bit difficult to feel comfortable with the model’s confidence. Aleatoric uncertainty is high, but not high enough to capture the true variability in the data. And we certaintly would hope for higher epistemic uncertainty, especially in places where the model introduces arbitrary-looking deviations from linearity.\nUncertainties on the validation set using exhaust vacuum as a single predictor.Uncertainties on the validation set using ambient pressure as a single predictor.Uncertainties on the validation set using relative humidity as a single predictor.Now let’s see uncertainty output when we use all four predictors. We see that now, the Monte Carlo estimates vary a lot more, and accordingly, epistemic uncertainty is a lot higher. Aleatoric uncertainty, on the other hand, got a lot lower. Overall, predictive uncertainty captures the range of ground truth values pretty well.\nUncertainties on the validation set using all 4 predictors.Conclusion\nWe’ve introduced a method to obtain theoretically grounded uncertainty estimates from neural networks. We find the approach intuitively attractive for several reasons: For one, the separation of different types of uncertainty is convincing and practically relevant. Second, uncertainty depends on the amount of data seen in the respective ranges. This is especially relevant when thinking of differences between training and test-time distributions. Third, the idea of having the network “become aware of its own uncertainty” is seductive.\nIn practice though, there are open questions as to how to apply the method. From our real-world test above, we immediately ask: Why is the model so confident2 when the ground truth data has high variance? And, thinking experimentally: How would that vary with different data sizes (rows), dimensionality (columns), and hyperparameter settings (including neural network hyperparameters like capacity, number of epochs trained, and activation functions, but also the Gaussian process prior length-scale \\(\\tau\\))?\nFor practical use, more experimentation with different datasets and hyperparameter settings is certainly warranted. Another direction to follow up is application to tasks in image recognition, such as semantic segmentation. Here we’d be interested in not just quantifying, but also localizing uncertainty, to see which visual aspects of a scene (occlusion, illumination, uncommon shapes) make objects hard to identify.\n\nThis would require a slightly different wrapper class but again, an R implementation could follow the Python example in Yarin Gal’s repository.\n\n\nGal, Yarin, and Zoubin Ghahramani. 2016. “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.” In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, Ny, Usa, June 19-24, 2016, 1050–9. http://jmlr.org/proceedings/papers/v48/gal16.html.\n\n\nGal, Y., J. Hron, and A. Kendall. 2017. “Concrete Dropout.” ArXiv E-Prints, May. http://arxiv.org/abs/1705.07832.\n\n\nKendall, Alex, and Yarin Gal. 2017. “What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?” In Advances in Neural Information Processing Systems 30, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 5574–84. Curran Associates, Inc. http://papers.nips.cc/paper/7141-what-uncertainties-do-we-need-in-bayesian-deep-learning-for-computer-vision.pdf.\n\n\nLeibig, Christian, Vaneeda Allken, Murat Seckin Ayhan, Philipp Berens, and Siegfried Wahl. 2017. “Leveraging Uncertainty Information from Deep Neural Networks for Disease Detection.” bioRxiv. https://doi.org/10.1101/084210.\n\n\nexponentiated because we’ve really been working with the log of the variance↩︎\ntalking epistemic uncertainty↩︎\n",
    "preview": "posts/2018-11-12-uncertainty_estimates_dropout/images/thumb.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 2046,
    "preview_height": 872
  },
  {
    "path": "posts/2018-11-05-naming-locating-objects/",
    "title": "Naming and locating objects in images",
    "description": "Object detection (the act of classifying and localizing multiple objects in a scene) is one of the more difficult, but very relevant in practice deep learning tasks. We'll build up to it in several posts. Here we start with the simpler tasks of naming and locating a single object.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-11-05",
    "categories": [
      "TensorFlow/Keras",
      "Image Recognition & Image Processing"
    ],
    "contents": "\nWe’ve all become used to deep learning’s success in image classification. Greater Swiss Mountain dog or Bernese mountain dog? Red panda or giant panda? No problem. However, in real life it’s not enough to name the single most salient object on a picture. Like it or not, one of the most compelling examples is autonomous driving: We don’t want the algorithm to recognize just that car in front of us, but also the pedestrian about to cross the street. And, just detecting the pedestrian is not sufficient. The exact location of objects matters.\nThe term object detection is commonly used to refer to the task of naming and localizing multiple objects in an image frame. Object detection is difficult; we’ll build up to it in a loose series of posts, focusing on concepts instead of aiming for ultimate performance. Today, we’ll start with a few straightforward building blocks: Classification, both single and multiple; localization; and combining both classification and localization of a single object.\n\nThe structure and approaches of these posts will follow the excellent fast.ai notebook on object detection.\nDataset\nWe’ll be using images and annotations from the Pascal VOC dataset which can be downloaded from this mirror. Specifically, we’ll use data from the 2007 challenge and the same JSON annotation file as used in the fast.ai course.\nQuick download/organization instructions, shamelessly taken from a helpful post on the fast.ai wiki, are as follows:\n\n# mkdir data && cd data\n# curl -OL http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n# curl -OL https://storage.googleapis.com/coco-dataset/external/PASCAL_VOC.zip\n# tar -xf VOCtrainval_06-Nov-2007.tar\n# unzip PASCAL_VOC.zip\n# mv PASCAL_VOC/*.json .\n# rmdir PASCAL_VOC\n# tar -xvf VOCtrainval_06-Nov-2007.tar\nIn words, we take the images and the annotation file from different places:\nhttp://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar provides us with the images, and after unzipping all we care about is the JPEGImages folder.\nFrom https://storage.googleapis.com/coco-dataset/external/PASCAL_VOC.zip all we will be needing is the annotation file, pascal_train2007.json.\nWhether you’re executing the listed commands or arranging files manually, you should eventually end up with directories/files analogous to these:\n\n\nimg_dir <- \"data/VOCdevkit/VOC2007/JPEGImages\"\nannot_file <- \"data/pascal_train2007.json\"\n\nNow we need to extract some information from that json file.\nPreprocessing\nLet’s quickly make sure we have all required libraries loaded.\n\n\nlibrary(keras)\nlibrary(rjson)\nlibrary(magick)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stringr)\n\nAnnotations contain information about three types of things we’re interested in.\n\n\nannotations <- fromJSON(file = annot_file)\nstr(annotations, max.level = 1)\n\n\nList of 4\n $ images     :List of 2501\n $ type       : chr \"instances\"\n $ annotations:List of 7844\n $ categories :List of 20\nFirst, characteristics of the image itself (height and width) and where it’s stored. Not surprisingly, here it’s one entry per image.\n\n\nimageinfo <- annotations$images %>% {\n  tibble(\n    id = map_dbl(., \"id\"),\n    file_name = map_chr(., \"file_name\"),\n    image_height = map_dbl(., \"height\"),\n    image_width = map_dbl(., \"width\")\n  )\n}\n\nThen, object class ids and bounding box coordinates. There may be multiple of these per image. In Pascal VOC, there are 20 object classes, from ubiquitous vehicles (car, aeroplane) over indispensable animals (cat, sheep) to more rare (in popular datasets) types like potted plant or tv monitor.\n\n\nclasses <- c(\n  \"aeroplane\",\n  \"bicycle\",\n  \"bird\",\n  \"boat\",\n  \"bottle\",\n  \"bus\",\n  \"car\",\n  \"cat\",\n  \"chair\",\n  \"cow\",\n  \"diningtable\",\n  \"dog\",\n  \"horse\",\n  \"motorbike\",\n  \"person\",\n  \"pottedplant\",\n  \"sheep\",\n  \"sofa\",\n  \"train\",\n  \"tvmonitor\"\n)\n\nboxinfo <- annotations$annotations %>% {\n  tibble(\n    image_id = map_dbl(., \"image_id\"),\n    category_id = map_dbl(., \"category_id\"),\n    bbox = map(., \"bbox\")\n  )\n}\n\nThe bounding boxes are now stored in a list column and need to be unpacked.\n\n\nboxinfo <- boxinfo %>% \n  mutate(bbox = unlist(map(.$bbox, function(x) paste(x, collapse = \" \"))))\nboxinfo <- boxinfo %>% \n  separate(bbox, into = c(\"x_left\", \"y_top\", \"bbox_width\", \"bbox_height\"))\nboxinfo <- boxinfo %>% mutate_all(as.numeric)\n\nFor the bounding boxes, the annotation file provides x_left and y_top coordinates, as well as width and height. We will mostly be working with corner coordinates, so we create the missing x_right and y_bottom.\nAs usual in image processing, the y axis starts from the top.\n\n\nboxinfo <- boxinfo %>% \n  mutate(y_bottom = y_top + bbox_height - 1, x_right = x_left + bbox_width - 1)\n\nFinally, we still need to match class ids to class names.\n\n\ncatinfo <- annotations$categories %>%  {\n  tibble(id = map_dbl(., \"id\"), name = map_chr(., \"name\"))\n}\n\nSo, putting it all together:\n\n\nimageinfo <- imageinfo %>%\n  inner_join(boxinfo, by = c(\"id\" = \"image_id\")) %>%\n  inner_join(catinfo, by = c(\"category_id\" = \"id\"))\n\nNote that here still, we have several entries per image, each annotated object occupying its own row.\nThere’s one step that will bitterly hurt our localization performance if we later forget it, so let’s do it now already: We need to scale all bounding box coordinates according to the actual image size we’ll use when we pass it to our network.\n\n\ntarget_height <- 224\ntarget_width <- 224\n\nimageinfo <- imageinfo %>% mutate(\n  x_left_scaled = (x_left / image_width * target_width) %>% round(),\n  x_right_scaled = (x_right / image_width * target_width) %>% round(),\n  y_top_scaled = (y_top / image_height * target_height) %>% round(),\n  y_bottom_scaled = (y_bottom / image_height * target_height) %>% round(),\n  bbox_width_scaled =  (bbox_width / image_width * target_width) %>% round(),\n  bbox_height_scaled = (bbox_height / image_height * target_height) %>% round()\n)\n\nLet’s take a glance at our data. Picking one of the early entries and displaying the original image together with the object annotation yields\n\n\nimg_data <- imageinfo[4,]\nimg <- image_read(file.path(img_dir, img_data$file_name))\nimg <- image_draw(img)\nrect(\n  img_data$x_left,\n  img_data$y_bottom,\n  img_data$x_right,\n  img_data$y_top,\n  border = \"white\",\n  lwd = 2\n)\ntext(\n  img_data$x_left,\n  img_data$y_top,\n  img_data$name,\n  offset = 1,\n  pos = 2,\n  cex = 1.5,\n  col = \"white\"\n)\ndev.off()\n\n\nNow as indicated above, in this post we’ll mostly address handling a single object in an image. This means we have to decide, per image, which object to single out.\nA reasonable strategy seems to be choosing the object with the largest ground truth bounding box.\n\n\nimageinfo <- imageinfo %>% mutate(area = bbox_width_scaled * bbox_height_scaled)\n\nimageinfo_maxbb <- imageinfo %>%\n  group_by(id) %>%\n  filter(which.max(area) == row_number())\n\nAfter this operation, we only have 2501 images to work with - not many at all! For classification, we could simply use data augmentation as provided by Keras, but to work with localization we’d have to spin our own augmentation algorithm. We’ll leave this to a later occasion and for now, focus on the basics.\nFinally after train-test split\n\n\ntrain_indices <- sample(1:n_samples, 0.8 * n_samples)\ntrain_data <- imageinfo_maxbb[train_indices,]\nvalidation_data <- imageinfo_maxbb[-train_indices,]\n\nour training set consists of 2000 images with one annotation each. We’re ready to start training, and we’ll start gently, with single-object classification.\nSingle-object classification\nIn all cases, we will use XCeption as a basic feature extractor. Having been trained on ImageNet, we don’t expect much fine tuning to be necessary to adapt to Pascal VOC, so we leave XCeption’s weights untouched\n\n\nfeature_extractor <-\n  application_xception(\n    include_top = FALSE,\n    input_shape = c(224, 224, 3),\n    pooling = \"avg\"\n)\n\nfeature_extractor %>% freeze_weights()\n\nand put just a few custom layers on top.\n\n\nmodel <- keras_model_sequential() %>%\n  feature_extractor %>%\n  layer_batch_normalization() %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_dense(units = 512, activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_dropout(rate = 0.5) %>%\n  layer_dense(units = 20, activation = \"softmax\")\n\nmodel %>% compile(\n  optimizer = \"adam\",\n  loss = \"sparse_categorical_crossentropy\",\n  metrics = list(\"accuracy\")\n)\n\nHow should we pass our data to Keras? We could simple use Keras’ image_data_generator, but given we will need custom generators soon, we’ll build a simple one ourselves. This one delivers images as well as the corresponding targets in a stream. Note how the targets are not one-hot-encoded, but integers - using sparse_categorical_crossentropy as a loss function enables this convenience.\n\nSee the Deep learning with R book for an introduction to writing data generators like this one.\n\n\nbatch_size <- 10\n\nload_and_preprocess_image <- function(image_name, target_height, target_width) {\n  img_array <- image_load(\n    file.path(img_dir, image_name),\n    target_size = c(target_height, target_width)\n    ) %>%\n    image_to_array() %>%\n    xception_preprocess_input() \n  dim(img_array) <- c(1, dim(img_array))\n  img_array\n}\n\nclassification_generator <-\n  function(data,\n           target_height,\n           target_width,\n           shuffle,\n           batch_size) {\n    i <- 1\n    function() {\n      if (shuffle) {\n        indices <- sample(1:nrow(data), size = batch_size)\n      } else {\n        if (i + batch_size >= nrow(data))\n          i <<- 1\n        indices <- c(i:min(i + batch_size - 1, nrow(data)))\n        i <<- i + length(indices)\n      }\n      x <-\n        array(0, dim = c(length(indices), target_height, target_width, 3))\n      y <- array(0, dim = c(length(indices), 1))\n      \n      for (j in 1:length(indices)) {\n        x[j, , , ] <-\n          load_and_preprocess_image(data[[indices[j], \"file_name\"]],\n                                    target_height, target_width)\n        y[j, ] <-\n          data[[indices[j], \"category_id\"]] - 1\n      }\n      x <- x / 255\n      list(x, y)\n    }\n  }\n\ntrain_gen <- classification_generator(\n  train_data,\n  target_height = target_height,\n  target_width = target_width,\n  shuffle = TRUE,\n  batch_size = batch_size\n)\n\nvalid_gen <- classification_generator(\n  validation_data,\n  target_height = target_height,\n  target_width = target_width,\n  shuffle = FALSE,\n  batch_size = batch_size\n)\n\nNow how does training go?\n\n\nmodel %>% fit_generator(\n  train_gen,\n  epochs = 20,\n  steps_per_epoch = nrow(train_data) / batch_size,\n  validation_data = valid_gen,\n  validation_steps = nrow(validation_data) / batch_size,\n  callbacks = list(\n    callback_model_checkpoint(\n      file.path(\"class_only\", \"weights.{epoch:02d}-{val_loss:.2f}.hdf5\")\n    ),\n    callback_early_stopping(patience = 2)\n  )\n)\n\nFor us, after 8 epochs, accuracies on the train resp. validation sets were at 0.68 and 0.74, respectively. Not too bad given given we’re trying to differentiate between 20 classes here.\nNow let’s quickly think what we’d change if we were to classify multiple objects in one image. Changes mostly concern preprocessing steps.\nMultiple object classification\nThis time, we multi-hot-encode our data. For every image (as represented by its filename), here we have a vector of length 20 where 0 indicates absence, 1 means presence of the respective object class:\n\n\nimage_cats <- imageinfo %>% \n  select(category_id) %>%\n  mutate(category_id = category_id - 1) %>%\n  pull() %>%\n  to_categorical(num_classes = 20)\n\nimage_cats <- data.frame(image_cats) %>%\n  add_column(file_name = imageinfo$file_name, .before = TRUE)\n\nimage_cats <- image_cats %>% \n  group_by(file_name) %>% \n  summarise_all(.funs = funs(max))\n\nn_samples <- nrow(image_cats)\ntrain_indices <- sample(1:n_samples, 0.8 * n_samples)\ntrain_data <- image_cats[train_indices,]\nvalidation_data <- image_cats[-train_indices,]\n\nCorrespondingly, we modify the generator to return a target of dimensions batch_size * 20, instead of batch_size * 1.\n\n\nclassification_generator <- \n  function(data,\n           target_height,\n           target_width,\n           shuffle,\n           batch_size) {\n    i <- 1\n    function() {\n      if (shuffle) {\n        indices <- sample(1:nrow(data), size = batch_size)\n      } else {\n        if (i + batch_size >= nrow(data))\n          i <<- 1\n        indices <- c(i:min(i + batch_size - 1, nrow(data)))\n        i <<- i + length(indices)\n      }\n      x <-\n        array(0, dim = c(length(indices), target_height, target_width, 3))\n      y <- array(0, dim = c(length(indices), 20))\n      \n      for (j in 1:length(indices)) {\n        x[j, , , ] <-\n          load_and_preprocess_image(data[[indices[j], \"file_name\"]], \n                                    target_height, target_width)\n        y[j, ] <-\n          data[indices[j], 2:21] %>% as.matrix()\n      }\n      x <- x / 255\n      list(x, y)\n    }\n  }\n\ntrain_gen <- classification_generator(\n  train_data,\n  target_height = target_height,\n  target_width = target_width,\n  shuffle = TRUE,\n  batch_size = batch_size\n)\n\nvalid_gen <- classification_generator(\n  validation_data,\n  target_height = target_height,\n  target_width = target_width,\n  shuffle = FALSE,\n  batch_size = batch_size\n)\n\nNow, the most interesting change is to the model - even though it’s a change to two lines only. Were we to use categorical_crossentropy now (the non-sparse variant of the above), combined with a softmax activation, we would effectively tell the model to pick just one, namely, the most probable object.\n\nSee the introduction to loss functions and activations on this blog for a demonstration.\nInstead, we want to decide: For each object class, is it present in the image or not? Thus, instead of softmax we use sigmoid, paired with binary_crossentropy, to obtain an independent verdict on every class.\n\n\nfeature_extractor <-\n  application_xception(\n    include_top = FALSE,\n    input_shape = c(224, 224, 3),\n    pooling = \"avg\"\n  )\n\nfeature_extractor %>% freeze_weights()\n\nmodel <- keras_model_sequential() %>%\n  feature_extractor %>%\n  layer_batch_normalization() %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_dense(units = 512, activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_dropout(rate = 0.5) %>%\n  layer_dense(units = 20, activation = \"sigmoid\")\n\nmodel %>% compile(optimizer = \"adam\",\n                  loss = \"binary_crossentropy\",\n                  metrics = list(\"accuracy\"))\n\nAnd finally, again, we fit the model:\n\n\nmodel %>% fit_generator(\n  train_gen,\n  epochs = 20,\n  steps_per_epoch = nrow(train_data) / batch_size,\n  validation_data = valid_gen,\n  validation_steps = nrow(validation_data) / batch_size,\n  callbacks = list(\n    callback_model_checkpoint(\n      file.path(\"multiclass\", \"weights.{epoch:02d}-{val_loss:.2f}.hdf5\")\n    ),\n    callback_early_stopping(patience = 2)\n  )\n)\n\nThis time, (binary) accuracy surpasses 0.95 after one epoch already, on both the train and validation sets. Not surprisingly, accuracy is significantly higher here than when we had to single out one of 20 classes (and that, with other confounding objects present in most cases!).\nNow, chances are that if you’ve done any deep learning before, you’ve done image classification in some form, perhaps even in the multiple-object variant. To build up in the direction of object detection, it is time we add a new ingredient: localization.\nSingle-object localization\nFrom here on, we’re back to dealing with a single object per image. So the question now is, how do we learn bounding boxes? If you’ve never heard of this, the answer will sound unbelievably simple (naive even): We formulate this as a regression problem and aim to predict the actual coordinates. To set realistic expectations - we surely shouldn’t expect ultimate precision here. But in a way it’s amazing it does even work at all.\nWhat does this mean, formulate as a regression problem? Concretely, it means we’ll have a dense output layer with 4 units, each corresponding to a corner coordinate.\nSo let’s start with the model this time. Again, we use Xception, but there’s an important difference here: Whereas before, we said pooling = \"avg\" to obtain an output tensor of dimensions batch_size * number of filters, here we don’t do any averaging or flattening out of the spatial grid. This is because it’s exactly the spatial information we’re interested in!\nFor Xception, the output resolution will be 7x7. So a priori, we shouldn’t expect high precision on objects much smaller than about 32x32 pixels (assuming the standard input size of 224x224).\n\n\nfeature_extractor <- application_xception(\n  include_top = FALSE,\n  input_shape = c(224, 224, 3)\n)\n\nfeature_extractor %>% freeze_weights()\n\nNow we append our custom regression module.\n\n\nmodel <- keras_model_sequential() %>%\n  feature_extractor %>%\n  layer_flatten() %>%\n  layer_batch_normalization() %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_dense(units = 512, activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_dropout(rate = 0.5) %>%\n  layer_dense(units = 4)\n\nWe will train with one of the loss functions common in regression tasks, mean absolute error. But in tasks like object detection or segmentation, we’re also interested in a more tangible quantity: How much do estimate and ground truth overlap?\nOverlap is usually measured as Intersection over Union, or Jaccard distance. Intersection over Union is exactly what it says, a ratio between space shared by the objects and space occupied when we take them together.\nTo assess the model’s progress, we can easily code this as a custom metric:\n\n\nmetric_iou <- function(y_true, y_pred) {\n  \n  # order is [x_left, y_top, x_right, y_bottom]\n  intersection_xmin <- k_maximum(y_true[ ,1], y_pred[ ,1])\n  intersection_ymin <- k_maximum(y_true[ ,2], y_pred[ ,2])\n  intersection_xmax <- k_minimum(y_true[ ,3], y_pred[ ,3])\n  intersection_ymax <- k_minimum(y_true[ ,4], y_pred[ ,4])\n  \n  area_intersection <- (intersection_xmax - intersection_xmin) * \n                       (intersection_ymax - intersection_ymin)\n  area_y <- (y_true[ ,3] - y_true[ ,1]) * (y_true[ ,4] - y_true[ ,2])\n  area_yhat <- (y_pred[ ,3] - y_pred[ ,1]) * (y_pred[ ,4] - y_pred[ ,2])\n  area_union <- area_y + area_yhat - area_intersection\n  \n  iou <- area_intersection/area_union\n  k_mean(iou)\n  \n}\n\nModel compilation then goes like\n\n\nmodel %>% compile(\n  optimizer = \"adam\",\n  loss = \"mae\",\n  metrics = list(custom_metric(\"iou\", metric_iou))\n)\n\nNow modify the generator to return bounding box coordinates as targets…\n\n\nlocalization_generator <-\n  function(data,\n           target_height,\n           target_width,\n           shuffle,\n           batch_size) {\n    i <- 1\n    function() {\n      if (shuffle) {\n        indices <- sample(1:nrow(data), size = batch_size)\n      } else {\n        if (i + batch_size >= nrow(data))\n          i <<- 1\n        indices <- c(i:min(i + batch_size - 1, nrow(data)))\n        i <<- i + length(indices)\n      }\n      x <-\n        array(0, dim = c(length(indices), target_height, target_width, 3))\n      y <- array(0, dim = c(length(indices), 4))\n      \n      for (j in 1:length(indices)) {\n        x[j, , , ] <-\n          load_and_preprocess_image(data[[indices[j], \"file_name\"]], \n                                    target_height, target_width)\n        y[j, ] <-\n          data[indices[j], c(\"x_left_scaled\",\n                             \"y_top_scaled\",\n                             \"x_right_scaled\",\n                             \"y_bottom_scaled\")] %>% as.matrix()\n      }\n      x <- x / 255\n      list(x, y)\n    }\n  }\n\ntrain_gen <- localization_generator(\n  train_data,\n  target_height = target_height,\n  target_width = target_width,\n  shuffle = TRUE,\n  batch_size = batch_size\n)\n\nvalid_gen <- localization_generator(\n  validation_data,\n  target_height = target_height,\n  target_width = target_width,\n  shuffle = FALSE,\n  batch_size = batch_size\n)\n\n… and we’re ready to go!\n\n\nmodel %>% fit_generator(\n  train_gen,\n  epochs = 20,\n  steps_per_epoch = nrow(train_data) / batch_size,\n  validation_data = valid_gen,\n  validation_steps = nrow(validation_data) / batch_size,\n  callbacks = list(\n    callback_model_checkpoint(\n      file.path(\"loc_only\", \"weights.{epoch:02d}-{val_loss:.2f}.hdf5\")\n    ),\n    callback_early_stopping(patience = 2)\n  )\n)\n\nAfter 8 epochs, IOU on both training and test sets is around 0.35. This number doesn’t look too good. To learn more about how training went, we need to see some predictions. Here’s a convenience function that displays an image, the ground truth box of the most salient object (as defined above), and if given, class and bounding box predictions.\n\n\nplot_image_with_boxes <- function(file_name,\n                                  object_class,\n                                  box,\n                                  scaled = FALSE,\n                                  class_pred = NULL,\n                                  box_pred = NULL) {\n  img <- image_read(file.path(img_dir, file_name))\n  if(scaled) img <- image_resize(img, geometry = \"224x224!\")\n  img <- image_draw(img)\n  x_left <- box[1]\n  y_bottom <- box[2]\n  x_right <- box[3]\n  y_top <- box[4]\n  rect(\n    x_left,\n    y_bottom,\n    x_right,\n    y_top,\n    border = \"cyan\",\n    lwd = 2.5\n  )\n  text(\n    x_left,\n    y_top,\n    object_class,\n    offset = 1,\n    pos = 2,\n    cex = 1.5,\n    col = \"cyan\"\n  )\n  if (!is.null(box_pred))\n    rect(box_pred[1],\n         box_pred[2],\n         box_pred[3],\n         box_pred[4],\n         border = \"yellow\",\n         lwd = 2.5)\n  if (!is.null(class_pred))\n    text(\n      box_pred[1],\n      box_pred[2],\n      class_pred,\n      offset = 0,\n      pos = 4,\n      cex = 1.5,\n      col = \"yellow\")\n  dev.off()\n  img %>% image_write(paste0(\"preds_\", file_name))\n  plot(img)\n}\n\nFirst, let’s see predictions on sample images from the training set.\n\n\ntrain_1_8 <- train_data[1:8, c(\"file_name\",\n                               \"name\",\n                               \"x_left_scaled\",\n                               \"y_top_scaled\",\n                               \"x_right_scaled\",\n                               \"y_bottom_scaled\")]\n\nfor (i in 1:8) {\n  preds <-\n    model %>% predict(\n      load_and_preprocess_image(train_1_8[i, \"file_name\"], \n                                target_height, target_width),\n      batch_size = 1\n  )\n  plot_image_with_boxes(train_1_8$file_name[i],\n                        train_1_8$name[i],\n                        train_1_8[i, 3:6] %>% as.matrix(),\n                        scaled = TRUE,\n                        box_pred = preds)\n}\n\nSample bounding box predictions on the training set.As you’d guess from looking, the cyan-colored boxes are the ground truth ones. Now looking at the predictions explains a lot about the mediocre IOU values! Let’s take the very first sample image - we wanted the model to focus on the sofa, but it picked the table, which is also a category in the dataset (although in the form of dining table). Similar with the image on the right of the first row - we wanted to it to pick just the dog but it included the person, too (by far the most frequently seen category in the dataset). So we actually made the task a lot more difficult than had we stayed with e.g., ImageNet where normally a single object is salient.\nNow check predictions on the validation set.\nSome bounding box predictions on the validation set.Again, we get a similar impression: The model did learn something, but the task is ill defined. Look at the third image in row 2: Isn’t it pretty consequent the model picks all people instead of singling out some special guy?\nIf single-object localization is that straightforward, how technically involved can it be to output a class label at the same time? As long as we stay with a single object, the answer indeed is: not much.\n\nAs a caveat, please note we’re talking about mapping concepts to technical approaches here. Obtaining ultimate performance is a different thing.\nLet’s finish up today with a constrained combination of classification and localization: detection of a single object.\nSingle-object detection\nCombining regression and classification into one means we’ll want to have two outputs in our model. We’ll thus use the functional API this time. Otherwise, there isn’t much new here: We start with an XCeption output of spatial resolution 7x7, append some custom processing and return two outputs, one for bounding box regression and one for classification.\n\n\nfeature_extractor <- application_xception(\n  include_top = FALSE,\n  input_shape = c(224, 224, 3)\n)\n\ninput <- feature_extractor$input\ncommon <- feature_extractor$output %>%\n  layer_flatten(name = \"flatten\") %>%\n  layer_activation_relu() %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_dense(units = 512, activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_dropout(rate = 0.5)\n\nregression_output <-\n  layer_dense(common, units = 4, name = \"regression_output\")\nclass_output <- layer_dense(\n  common,\n  units = 20,\n  activation = \"softmax\",\n  name = \"class_output\"\n)\n\nmodel <- keras_model(\n  inputs = input,\n  outputs = list(regression_output, class_output)\n)\n\nWhen defining the losses (mean absolute error and categorical crossentropy, just as in the respective single tasks of regression and classification), we could weight them so they end up on approximately a common scale. In fact that didn’t make much of a difference so we show the respective code in commented form.\n\n\nmodel %>% freeze_weights(to = \"flatten\")\n\nmodel %>% compile(\n  optimizer = \"adam\",\n  loss = list(\"mae\", \"sparse_categorical_crossentropy\"),\n  #loss_weights = list(\n  #  regression_output = 0.05,\n  #  class_output = 0.95),\n  metrics = list(\n    regression_output = custom_metric(\"iou\", metric_iou),\n    class_output = \"accuracy\"\n  )\n)\n\nJust like model outputs and losses are both lists, the data generator has to return the ground truth samples in a list. Fitting the model then goes as usual.\n\n\nloc_class_generator <-\n  function(data,\n           target_height,\n           target_width,\n           shuffle,\n           batch_size) {\n    i <- 1\n    function() {\n      if (shuffle) {\n        indices <- sample(1:nrow(data), size = batch_size)\n      } else {\n        if (i + batch_size >= nrow(data))\n          i <<- 1\n        indices <- c(i:min(i + batch_size - 1, nrow(data)))\n        i <<- i + length(indices)\n      }\n      x <-\n        array(0, dim = c(length(indices), target_height, target_width, 3))\n      y1 <- array(0, dim = c(length(indices), 4))\n      y2 <- array(0, dim = c(length(indices), 1))\n      \n      for (j in 1:length(indices)) {\n        x[j, , , ] <-\n          load_and_preprocess_image(data[[indices[j], \"file_name\"]], \n                                    target_height, target_width)\n        y1[j, ] <-\n          data[indices[j], c(\"x_left\", \"y_top\", \"x_right\", \"y_bottom\")] \n            %>% as.matrix()\n        y2[j, ] <-\n          data[[indices[j], \"category_id\"]] - 1\n      }\n      x <- x / 255\n      list(x, list(y1, y2))\n    }\n  }\n\ntrain_gen <- loc_class_generator(\n  train_data,\n  target_height = target_height,\n  target_width = target_width,\n  shuffle = TRUE,\n  batch_size = batch_size\n)\n\nvalid_gen <- loc_class_generator(\n  validation_data,\n  target_height = target_height,\n  target_width = target_width,\n  shuffle = FALSE,\n  batch_size = batch_size\n)\n\nmodel %>% fit_generator(\n  train_gen,\n  epochs = 20,\n  steps_per_epoch = nrow(train_data) / batch_size,\n  validation_data = valid_gen,\n  validation_steps = nrow(validation_data) / batch_size,\n  callbacks = list(\n    callback_model_checkpoint(\n      file.path(\"loc_class\", \"weights.{epoch:02d}-{val_loss:.2f}.hdf5\")\n    ),\n    callback_early_stopping(patience = 2)\n  )\n)\n\nWhat about model predictions? A priori we might expect the bounding boxes to look better than in the regression-only model, as a significant part of the model is shared between classification and localization. Intuitively, I should be able to more precisely indicate the boundaries of something if I have an idea what that something is.\nUnfortunately, that didn’t quite happen. The model has become very biased to detecting a person everywhere, which might be advantageous (thinking safety) in an autonomous driving application but isn’t quite what we’d hoped for here.\nExample class and bounding box predictions on the training set.Example class and bounding box predictions on the validation set.Just to double-check this really has to do with class imbalance, here are the actual frequencies:\n\n\nimageinfo %>% group_by(name)\n  %>% summarise(cnt = n()) \n  %>% arrange(desc(cnt))\n\n\n# A tibble: 20 x 2\n   name          cnt\n   <chr>       <int>\n 1 person       2705\n 2 car           826\n 3 chair         726\n 4 bottle        338\n 5 pottedplant   305\n 6 bird          294\n 7 dog           271\n 8 sofa          218\n 9 boat          208\n10 horse         207\n11 bicycle       202\n12 motorbike     193\n13 cat           191\n14 sheep         191\n15 tvmonitor     191\n16 cow           185\n17 train         158\n18 aeroplane     156\n19 diningtable   148\n20 bus           131\nTo get better performance, we’d need to find a successful way to deal with this. However, handling class imbalance in deep learning is a topic of its own, and here we want to build up in the direction of objection detection. So we’ll make a cut here and in an upcoming post, think about how we can classify and localize multiple objects in an image.\nConclusion\nWe have seen that single-object classification and localization are conceptually straightforward. The big question now is, are these approaches extensible to multiple objects? Or will new ideas have to come in? We’ll follow up on this giving a short overview of approaches and then, singling in on one of those and implementing it.\n\n\n",
    "preview": "posts/2018-11-05-naming-locating-objects/images/preds_train.jpg",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-10-22-mmd-vae/",
    "title": "Representation learning with MMD-VAE",
    "description": "Like GANs, variational autoencoders (VAEs) are often used to generate images. However, VAEs add an additional promise: namely, to model an underlying latent space. Here, we first look at a typical implementation that maximizes the evidence lower bound. Then, we compare it to one of the more recent competitors, MMD-VAE, from the Info-VAE (information maximizing VAE) family.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-10-22",
    "categories": [
      "TensorFlow/Keras",
      "Unsupervised Learning",
      "Image Recognition & Image Processing"
    ],
    "contents": "\nRecently, we showed how to generate images using generative adversarial networks (GANs). GANs may yield amazing results, but the contract there basically is: what you see is what you get. Sometimes this may be all we want. In other cases, we may be more interested in actually modelling a domain. We don’t just want to generate realistic-looking samples - we want our samples to be located at specific coordinates in domain space.\nFor example, imagine our domain to be the space of facial expressions. Then our latent space might be conceived as two-dimensional: In accordance with underlying emotional states, expressions vary on a positive-negative scale. At the same time, they vary in intensity. Now if we trained a VAE on a set of facial expressions adequately covering the ranges, and it did in fact “discover” our hypothesized dimensions, we could then use it to generate previously-nonexisting incarnations of points (faces, that is) in latent space.\nVariational autoencoders are similar to probabilistic graphical models in that they assume a latent space that is responsible for the observations, but unobservable. They are similar to plain autoencoders in that they compress, and then decompress again, the input domain. In contrast to plain autoencoders though, the crucial point here is to devise a loss function that allows to obtain informative representations in latent space.\nIn a nutshell\nIn standard VAEs (Kingma and Welling 2013), the objective is to maximize the evidence lower bound (ELBO):\n\\[ELBO\\ = \\ E[log\\ p(x|z)]\\ -\\ KL(q(z)||p(z))\\]\nIn plain words and expressed in terms of how we use it in practice, the first component is the reconstruction loss we also see in plain (non-variational) autoencoders. The second is the Kullback-Leibler divergence between a prior imposed on the latent space (typically, a standard normal distribution) and the representation of latent space as learned from the data.\n\nFor a well-written and intuitive introduction to VAEs, including the why and how of their optimization, see this Tutorial on variational autoencoders (Doersch 2016).\nA major criticism regarding the traditional VAE loss is that it results in uninformative latent space. Alternatives include \\(\\beta\\)-VAE(Burgess et al. 2018), Info-VAE (Zhao, Song, and Ermon 2017), and more. The MMD-VAE(Zhao, Song, and Ermon 2017) implemented below is a subtype of Info-VAE that instead of making each representation in latent space as similar as possible to the prior, coerces the respective distributions to be as close as possible. Here MMD stands for maximum mean discrepancy, a similarity measure for distributions based on matching their respective moments. We explain this in more detail below.\n\nThe main author of the paper(Zhao, Song, and Ermon 2017) has a tutorial on his website explaining the reasons behind this choice of cost function in a very accessible way.\nOur objective today\nIn this post, we are first going to implement a standard VAE that strives to maximize the ELBO. Then, we compare its performance to that of an Info-VAE using the MMD loss.\nOur focus will be on inspecting the latent spaces and see if, and how, they differ as a consequence of the optimization criteria used.\nThe domain we’re going to model will be glamorous (fashion!), but for the sake of manageability, confined to size 28 x 28: We’ll compress and reconstruct images from the Fashion MNIST dataset that has been developed as a drop-in to MNIST.\nA standard variational autoencoder\nSeeing we haven’t used TensorFlow eager execution for some weeks, we’ll do the model in an eager way. If you’re new to eager execution, don’t worry: As every new technique, it needs some getting accustomed to, but you’ll quickly find that many tasks are made easier if you use it. A simple yet complete, template-like example is available as part of the Keras documentation1.\n\nFor interesting applications using eager execution in combination with Keras, ranging from machine translation to neural style transfer, see the recent posts in the Eager category on this blog.\nSetup and data preparation\nAs usual, we start by making sure we’re using the TensorFlow implementation of Keras and enabling eager execution. Besides tensorflow and keras, we also load tfdatasets for use in data streaming.\nBy the way: No need to copy-paste any of the below code snippets. The two approaches are available among our Keras examples, namely, as eager_cvae.R and mmd_cvae.R.\n\nYou might find it interesting to compare non-eager Keras code implementing a variational autoencoder: see variational_autoencoder_deconv.R.\n\n\n# the following 5 lines have to be executed in this order\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nlibrary(tfdatasets)\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(glue)\n\nThe data comes conveniently with keras, all we need to do is the usual normalization and reshaping.\n\n\nfashion <- dataset_fashion_mnist()\n\nc(train_images, train_labels) %<-% fashion$train\nc(test_images, test_labels) %<-% fashion$test\n\ntrain_x <- train_images %>%\n  `/`(255) %>%\n  k_reshape(c(60000, 28, 28, 1))\n\ntest_x <- test_images %>% `/`(255) %>%\n  k_reshape(c(10000, 28, 28, 1))\n\nWhat do we need the test set for, given we are going to train an unsupervised (a better term being: semi-supervised) model? We’ll use it to see how (previously unknown) data points cluster together in latent space.\nNow prepare for streaming the data to keras:\n\n\nbuffer_size <- 60000\nbatch_size <- 100\nbatches_per_epoch <- buffer_size / batch_size\n\ntrain_dataset <- tensor_slices_dataset(train_x) %>%\n  dataset_shuffle(buffer_size) %>%\n  dataset_batch(batch_size)\n\ntest_dataset <- tensor_slices_dataset(test_x) %>%\n  dataset_batch(10000)\n\nNext up is defining the model.\nEncoder-decoder model\nThe model really is two models: the encoder and the decoder. As we’ll see shortly, in the standard version of the VAE there is a third component in between, performing the so-called reparameterization trick.\nThe encoder is a custom model, comprised of two convolutional layers and a dense layer. It returns the output of the dense layer split into two parts, one storing the mean of the latent variables, the other their variance.\n\n\nlatent_dim <- 2\n\nencoder_model <- function(name = NULL) {\n  \n  keras_model_custom(name = name, function(self) {\n    self$conv1 <-\n      layer_conv_2d(\n        filters = 32,\n        kernel_size = 3,\n        strides = 2,\n        activation = \"relu\"\n      )\n    self$conv2 <-\n      layer_conv_2d(\n        filters = 64,\n        kernel_size = 3,\n        strides = 2,\n        activation = \"relu\"\n      )\n    self$flatten <- layer_flatten()\n    self$dense <- layer_dense(units = 2 * latent_dim)\n    \n    function (x, mask = NULL) {\n      x %>%\n        self$conv1() %>%\n        self$conv2() %>%\n        self$flatten() %>%\n        self$dense() %>%\n        tf$split(num_or_size_splits = 2L, axis = 1L) \n    }\n  })\n}\n\nWe choose the latent space to be of dimension 2 - just because that makes visualization easy. With more complex data, you will probably benefit from choosing a higher dimensionality here.\nSo the encoder compresses real data into estimates of mean and variance of the latent space. We then “indirectly” sample from this distribution (the so-called reparameterization trick):\n\n\nreparameterize <- function(mean, logvar) {\n  eps <- k_random_normal(shape = mean$shape, dtype = tf$float64)\n  eps * k_exp(logvar * 0.5) + mean\n}\n\nThe sampled values will serve as input to the decoder, who will attempt to map them back to the original space. The decoder is basically a sequence of transposed convolutions, upsampling until we reach a resolution of 28x28.\n\n\ndecoder_model <- function(name = NULL) {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$dense <- layer_dense(units = 7 * 7 * 32, activation = \"relu\")\n    self$reshape <- layer_reshape(target_shape = c(7, 7, 32))\n    self$deconv1 <-\n      layer_conv_2d_transpose(\n        filters = 64,\n        kernel_size = 3,\n        strides = 2,\n        padding = \"same\",\n        activation = \"relu\"\n      )\n    self$deconv2 <-\n      layer_conv_2d_transpose(\n        filters = 32,\n        kernel_size = 3,\n        strides = 2,\n        padding = \"same\",\n        activation = \"relu\"\n      )\n    self$deconv3 <-\n      layer_conv_2d_transpose(\n        filters = 1,\n        kernel_size = 3,\n        strides = 1,\n        padding = \"same\"\n      )\n    \n    function (x, mask = NULL) {\n      x %>%\n        self$dense() %>%\n        self$reshape() %>%\n        self$deconv1() %>%\n        self$deconv2() %>%\n        self$deconv3()\n    }\n  })\n}\n\nNote how the final deconvolution does not have the sigmoid activation you might have expected. This is because we will be using tf$nn$sigmoid_cross_entropy_with_logits when calculating the loss.\nSpeaking of losses, let’s inspect them now.\nLoss calculations\nOne way to implement the VAE loss is combining reconstruction loss (cross entropy, in the present case) and Kullback-Leibler divergence. In Keras, the latter is available directly as loss_kullback_leibler_divergence.\nHere, we follow a recent Google Colaboratory notebook in batch-estimating the complete ELBO instead (instead of just estimating reconstruction loss and computing the KL-divergence analytically):\n\\[ELBO \\ batch \\ estimate = log\\ p(x_{batch}|z_{sampled})+log\\ p(z)−log\\ q(z_{sampled}|x_{batch})\\]\nCalculation of the normal loglikelihood is packaged into a function so we can reuse it during the training loop.\n\nNote that we’re calculating with the log of the variance, instead of the variance, for reasons of numerical stability.\n\n\nnormal_loglik <- function(sample, mean, logvar, reduce_axis = 2) {\n  loglik <- k_constant(0.5, dtype = tf$float64) *\n    (k_log(2 * k_constant(pi, dtype = tf$float64)) +\n    logvar +\n    k_exp(-logvar) * (sample - mean) ^ 2)\n  - k_sum(loglik, axis = reduce_axis)\n}\n\nPeeking ahead some, during training we will compute the above as follows.\nFirst,\n\n\ncrossentropy_loss <- tf$nn$sigmoid_cross_entropy_with_logits(\n  logits = preds,\n  labels = x\n)\nlogpx_z <- - k_sum(crossentropy_loss)\n\nyields \\(log \\ p(x|z)\\), the loglikelihood of the reconstructed samples given values sampled from latent space (a.k.a. reconstruction loss).\nThen,\n\n\nlogpz <- normal_loglik(\n  z,\n  k_constant(0, dtype = tf$float64),\n  k_constant(0, dtype = tf$float64)\n)\n\ngives \\(log \\ p(z)\\), the prior loglikelihood of \\(z\\). The prior is assumed to be standard normal, as is most often the case with VAEs.\nFinally,\n\n\nlogqz_x <- normal_loglik(z, mean, logvar)\n\nvields \\(log \\ q(z|x)\\), the loglikelihood of the samples \\(z\\) given mean and variance computed from the observed samples \\(x\\).\nFrom these three components, we will compute the final loss as\n\n\nloss <- -k_mean(logpx_z + logpz - logqz_x)\n\nAfter this peaking ahead, let’s quickly finish the setup so we get ready for training.\nFinal setup\nBesides the loss, we need an optimizer that will strive to diminish it.\n\n\noptimizer <- tf$train$AdamOptimizer(1e-4)\n\nWe instantiate our models …\n\n\nencoder <- encoder_model()\ndecoder <- decoder_model()\n\nand set up checkpointing, so we can later restore trained weights.\n\n\ncheckpoint_dir <- \"./checkpoints_cvae\"\ncheckpoint_prefix <- file.path(checkpoint_dir, \"ckpt\")\ncheckpoint <- tf$train$Checkpoint(\n  optimizer = optimizer,\n  encoder = encoder,\n  decoder = decoder\n)\n\nFrom the training loop, we will, in certain intervals, also call three functions not reproduced here (but available in the code example): generate_random_clothes, used to generate clothes from random samples from the latent space; show_latent_space, that displays the complete test set in latent (2-dimensional, thus easily visualizable) space; and show_grid, that generates clothes according to input values systematically spaced out in a grid.\nLet’s start training! Actually, before we do that, let’s have a look at what these functions display before any training: Instead of clothes, we see random pixels. Latent space has no structure. And different types of clothes do not cluster together in latent space.\n\nTraining loop\nWe’re training for 50 epochs here. For each epoch, we loop over the training set in batches. For each batch, we follow the usual eager execution flow: Inside the context of a GradientTape, apply the model and calculate the current loss; then outside this context calculate the gradients and let the optimizer perform backprop.\nWhat’s special here is that we have two models that both need their gradients calculated and weights adjusted. This can be taken care of by a single gradient tape, provided we create it persistent.\nAfter each epoch, we save current weights and every ten epochs, we also save plots for later inspection.\n\n\nnum_epochs <- 50\n\nfor (epoch in seq_len(num_epochs)) {\n  iter <- make_iterator_one_shot(train_dataset)\n  \n  total_loss <- 0\n  logpx_z_total <- 0\n  logpz_total <- 0\n  logqz_x_total <- 0\n  \n  until_out_of_range({\n    x <-  iterator_get_next(iter)\n    \n    with(tf$GradientTape(persistent = TRUE) %as% tape, {\n      \n      c(mean, logvar) %<-% encoder(x)\n      z <- reparameterize(mean, logvar)\n      preds <- decoder(z)\n      \n      crossentropy_loss <-\n        tf$nn$sigmoid_cross_entropy_with_logits(logits = preds, labels = x)\n      logpx_z <-\n        - k_sum(crossentropy_loss)\n      logpz <-\n        normal_loglik(z,\n                      k_constant(0, dtype = tf$float64),\n                      k_constant(0, dtype = tf$float64)\n        )\n      logqz_x <- normal_loglik(z, mean, logvar)\n      loss <- -k_mean(logpx_z + logpz - logqz_x)\n      \n    })\n\n    total_loss <- total_loss + loss\n    logpx_z_total <- tf$reduce_mean(logpx_z) + logpx_z_total\n    logpz_total <- tf$reduce_mean(logpz) + logpz_total\n    logqz_x_total <- tf$reduce_mean(logqz_x) + logqz_x_total\n    \n    encoder_gradients <- tape$gradient(loss, encoder$variables)\n    decoder_gradients <- tape$gradient(loss, decoder$variables)\n    \n    optimizer$apply_gradients(\n      purrr::transpose(list(encoder_gradients, encoder$variables)),\n      global_step = tf$train$get_or_create_global_step()\n    )\n    optimizer$apply_gradients(\n      purrr::transpose(list(decoder_gradients, decoder$variables)),\n      global_step = tf$train$get_or_create_global_step()\n    )\n    \n  })\n  \n  checkpoint$save(file_prefix = checkpoint_prefix)\n  \n  cat(\n    glue(\n      \"Losses (epoch): {epoch}:\",\n      \"  {(as.numeric(logpx_z_total)/batches_per_epoch) %>% round(2)} logpx_z_total,\",\n      \"  {(as.numeric(logpz_total)/batches_per_epoch) %>% round(2)} logpz_total,\",\n      \"  {(as.numeric(logqz_x_total)/batches_per_epoch) %>% round(2)} logqz_x_total,\",\n      \"  {(as.numeric(total_loss)/batches_per_epoch) %>% round(2)} total\"\n    ),\n    \"\\n\"\n  )\n  \n  if (epoch %% 10 == 0) {\n    generate_random_clothes(epoch)\n    show_latent_space(epoch)\n    show_grid(epoch)\n  }\n}\n\nResults\nHow well did that work? Let’s see the kinds of clothes generated after 50 epochs.\n\nAlso, how disentangled (or not) are the different classes in latent space?\n\nAnd now watch different clothes morph into one another.\n\nHow good are these representations? This is hard to say when there is nothing to compare with.\nSo let’s dive into MMD-VAE and see how it does on the same dataset.\nMMD-VAE\nMMD-VAE promises to generate more informative latent features, so we would hope to see different behavior especially in the clustering and morphing plots.\nData setup is the same, and there are only very slight differences in the model. Please check out the complete code for this example, mmd_vae.R, as here we’ll just highlight the differences.\nDifferences in the model(s)\nThere are three differences as regards model architecture.\nOne, the encoder does not have to return the variance, so there is no need for tf$split. The encoder’s call method now just is\n\n\nfunction (x, mask = NULL) {\n  x %>%\n    self$conv1() %>%\n    self$conv2() %>%\n    self$flatten() %>%\n    self$dense() \n}\n\nBetween the encoder and the decoder, we don’t need the sampling step anymore, so there is no reparameterization. And since we won’t use tf$nn$sigmoid_cross_entropy_with_logits to compute the loss, we let the decoder apply the sigmoid in the last deconvolution layer:\n\n\nself$deconv3 <- layer_conv_2d_transpose(\n  filters = 1,\n  kernel_size = 3,\n  strides = 1,\n  padding = \"same\",\n  activation = \"sigmoid\"\n)\n\nLoss calculations\nNow, as expected, the big novelty is in the loss function.\nThe loss, maximum mean discrepancy (MMD), is based on the idea that two distributions are identical if and only if all moments are identical. Concretely, MMD is estimated using a kernel, such as the Gaussian kernel\n\\[k(z,z')=\\frac{e^{||z-z'||}}{2\\sigma^2}\\]\nto assess similarity between distributions.\nThe idea then is that if two distributions are identical, the average similarity between samples from each distribution should be identical to the average similarity between mixed samples from both distributions:\n\\[MMD(p(z)||q(z))=E_{p(z),p(z')}[k(z,z')]+E_{q(z),q(z')}[k(z,z')]−2E_{p(z),q(z')}[k(z,z')]\\] The following code is a direct port of the author’s original TensorFlow code:\n\n\ncompute_kernel <- function(x, y) {\n  x_size <- k_shape(x)[1]\n  y_size <- k_shape(y)[1]\n  dim <- k_shape(x)[2]\n  tiled_x <- k_tile(\n    k_reshape(x, k_stack(list(x_size, 1, dim))),\n    k_stack(list(1, y_size, 1))\n  )\n  tiled_y <- k_tile(\n    k_reshape(y, k_stack(list(1, y_size, dim))),\n    k_stack(list(x_size, 1, 1))\n  )\n  k_exp(-k_mean(k_square(tiled_x - tiled_y), axis = 3) /\n          k_cast(dim, tf$float64))\n}\n\ncompute_mmd <- function(x, y, sigma_sqr = 1) {\n  x_kernel <- compute_kernel(x, x)\n  y_kernel <- compute_kernel(y, y)\n  xy_kernel <- compute_kernel(x, y)\n  k_mean(x_kernel) + k_mean(y_kernel) - 2 * k_mean(xy_kernel)\n}\n\nTraining loop\nThe training loop differs from the standard VAE example only in the loss calculations. Here are the respective lines:\n\n\n with(tf$GradientTape(persistent = TRUE) %as% tape, {\n      \n      mean <- encoder(x)\n      preds <- decoder(mean)\n      \n      true_samples <- k_random_normal(\n        shape = c(batch_size, latent_dim),\n        dtype = tf$float64\n      )\n      loss_mmd <- compute_mmd(true_samples, mean)\n      loss_nll <- k_mean(k_square(x - preds))\n      loss <- loss_nll + loss_mmd\n      \n    })\n\nSo we simply compute MMD loss as well as reconstruction loss, and add them up. No sampling is involved in this version. Of course, we are curious to see how well that worked!\nResults\nAgain, let’s look at some generated clothes first. It seems like edges are much sharper here.\n\nThe clusters too look more nicely spread out in the two dimensions. And, they are centered at (0,0), as we would have hoped for.\n\nFinally, let’s see clothes morph into one another. Here, the smooth, continuous evolutions are impressive! Also, nearly all space is filled with meaningful objects, which hasn’t been the case above.\n\nMNIST\nFor curiosity’s sake, we generated the same kinds of plots after training on original MNIST. Here, there are hardly any differences visible in generated random digits after 50 epochs of training.\nLeft: random digits as generated after training with ELBO loss. Right: MMD loss.Also the differences in clustering are not that big.\nLeft: latent space as observed after training with ELBO loss. Right: MMD loss.But here too, the morphing looks much more organic with MMD-VAE.\nLeft: Morphing as observed after training with ELBO loss. Right: MMD loss.Conclusion\nTo us, this demonstrates impressively what big a difference the cost function can make when working with VAEs. Another component open to experimentation may be the prior used for the latent space - see this talk for an overview of alternative priors and the “Variational Mixture of Posteriors” paper (Tomczak and Welling 2017) for a popular recent approach.\nFor both cost functions and priors, we expect effective differences to become way bigger still when we leave the controlled environment of (Fashion) MNIST and work with real-world datasets.\n\n\nBurgess, C. P., I. Higgins, A. Pal, L. Matthey, N. Watters, G. Desjardins, and A. Lerchner. 2018. “Understanding Disentangling in Beta-Vae.” ArXiv E-Prints, April. http://arxiv.org/abs/1804.03599.\n\n\nDoersch, C. 2016. “Tutorial on Variational Autoencoders.” ArXiv E-Prints, June. http://arxiv.org/abs/1606.05908.\n\n\nKingma, Diederik P., and Max Welling. 2013. “Auto-Encoding Variational Bayes.” CoRR abs/1312.6114.\n\n\nTomczak, Jakub M., and Max Welling. 2017. “VAE with a Vampprior.” CoRR abs/1705.07120.\n\n\nZhao, Shengjia, Jiaming Song, and Stefano Ermon. 2017. “InfoVAE: Information Maximizing Variational Autoencoders.” CoRR abs/1706.02262. http://arxiv.org/abs/1706.02262.\n\n\nNote: this link was updated as of November 29th, 2019, to point to the most up-to-date version.↩︎\n",
    "preview": "posts/2018-10-22-mmd-vae/images/thumb.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 468,
    "preview_height": 178
  },
  {
    "path": "posts/2018-10-11-activations-intro/",
    "title": "Winner takes all: A look at activations and cost functions",
    "description": "Why do we use the activations we use, and how do they relate to the cost functions they tend to co-appear with? In this post we provide a conceptual introduction.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-10-11",
    "categories": [
      "TensorFlow/Keras",
      "Concepts"
    ],
    "contents": "\nYou’re building a Keras model. If you haven’t been doing deep learning for so long, getting the output activations and cost function right might involve some memorization (or lookup). You might be trying to recall the general guidelines like so:\nSo with my cats and dogs, I’m doing 2-class classification, so I have to use sigmoid activation in the output layer, right, and then, it’s binary crossentropy for the cost function… Or: I’m doing classification on ImageNet, that’s multi-class, so that was softmax for activation, and then, cost should be categorical crossentropy…\nIt’s fine to memorize stuff like this, but knowing a bit about the reasons behind often makes things easier. So we ask: Why is it that these output activations and cost functions go together? And, do they always have to?\nIn a nutshell\nPut simply, we choose activations that make the network predict what we want it to predict. The cost function is then determined by the model.\nThis is because neural networks are normally optimized using maximum likelihood, and depending on the distribution we assume for the output units, maximum likelihood yields different optimization objectives. All of these objectives then minimize the cross entropy (pragmatically: mismatch) between the true distribution and the predicted distribution.\n\nFor a more mathematical development of these topics, see sections 5.5 and 6.2 of Goodfellow et al., Deep Learning.(Goodfellow, Bengio, and Courville 2016)\nLet’s start with the simplest, the linear case.\nRegression\nFor the botanists among us, here’s a super simple network meant to predict sepal width from sepal length:\n\nIn case you’d like a more comprehensive introduction to doing regression with Keras, see the tutorial on the Keras website.\n\n\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 32) %>%\n  layer_dense(units = 1)\n\nmodel %>% compile(\n  optimizer = \"adam\", \n  loss = \"mean_squared_error\"\n)\n\nmodel %>% fit(\n  x = iris$Sepal.Length %>% as.matrix(),\n  y = iris$Sepal.Width %>% as.matrix(),\n  epochs = 50\n)\n\nOur model’s assumption here is that sepal width is normally distributed, given sepal length. Most often, we’re trying to predict the mean of a conditional Gaussian distribution:\n\\[p(y|\\mathbf{x} = N(y; \\mathbf{w}^t\\mathbf{h} + b)\\]\n\nThis formula assumes a single output unit.\nIn that case, the cost function that minimizes cross entropy (equivalently: optimizes maximum likelihood) is mean squared error. And that’s exactly what we’re using as a cost function above.\nAlternatively, we might wish to predict the median of that conditional distribution. In that case, we’d change the cost function to use mean absolute error:\n\n\nmodel %>% compile(\n  optimizer = \"adam\", \n  loss = \"mean_absolute_error\"\n)\n\nNow let’s move on beyond linearity.\nBinary classification\nWe’re enthusiastic bird watchers and want an application to notify us when there’s a bird in our garden - not when the neighbors landed their airplane, though. We’ll thus train a network to distinguish between two classes: birds and airplanes.\n\nFor a more detailed introduction to classification with Keras, see the tutorial on the Keras website.\n\n\n# Using the CIFAR-10 dataset that conveniently comes with Keras.\ncifar10 <- dataset_cifar10()\n\nx_train <- cifar10$train$x / 255\ny_train <- cifar10$train$y\n\nis_bird <- cifar10$train$y == 2\nx_bird <- x_train[is_bird, , ,]\ny_bird <- rep(0, 5000)\n\nis_plane <- cifar10$train$y == 0\nx_plane <- x_train[is_plane, , ,]\ny_plane <- rep(1, 5000)\n\nx <- abind::abind(x_bird, x_plane, along = 1)\ny <- c(y_bird, y_plane)\n\nmodel <- keras_model_sequential() %>%\n  layer_conv_2d(\n    filter = 8,\n    kernel_size = c(3, 3),\n    padding = \"same\",\n    input_shape = c(32, 32, 3),\n    activation = \"relu\"\n  ) %>%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_conv_2d(\n    filter = 8,\n    kernel_size = c(3, 3),\n    padding = \"same\",\n    activation = \"relu\"\n  ) %>%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %>%\nlayer_flatten() %>%\n  layer_dense(units = 32, activation = \"relu\") %>%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %>% compile(\n  optimizer = \"adam\", \n  loss = \"binary_crossentropy\", \n  metrics = \"accuracy\"\n)\n\nmodel %>% fit(\n  x = x,\n  y = y,\n  epochs = 50\n)\n\nAlthough we normally talk about “binary classification”, the way the outcome is usually modeled is as a Bernoulli random variable, conditioned on the input data. So:\n\\[P(y = 1|\\mathbf{x}) = p, \\ 0\\leq p\\leq1\\]\nA Bernoulli random variable takes on values between \\(0\\) and \\(1\\). So that’s what our network should produce. One idea might be to just clip all values of \\(\\mathbf{w}^t\\mathbf{h} + b\\) outside that interval. But if we do this, the gradient in these regions will be \\(0\\): The network cannot learn.\nA better way is to squish the complete incoming interval into the range (0,1), using the logistic sigmoid function\n\\[ \\sigma(x) = \\frac{1}{1 + e^{(-x)}} \\]\nThe sigmoid function squishes its input into the interval (0,1).As you can see, the sigmoid function saturates when its input gets very large, or very small. Is this problematic? It depends. In the end, what we care about is if the cost function saturates. Were we to choose mean squared error here, as in the regression task above, that’s indeed what could happen.1\nHowever, if we follow the general principle of maximum likelihood/cross entropy, the loss will be\n\\[- log P (y|\\mathbf{x})\\]\n\nIf you need the details, see section 6.2.2.2 in Goodfellow et al.\nwhere the \\(log\\) undoes the \\(exp\\) in the sigmoid.\nIn Keras, the corresponding loss function is binary_crossentropy. For a single item, the loss will be\n\\(- log(p)\\) when the ground truth is 1\n\\(- log(1-p)\\) when the ground truth is 0\n\nHere p is the predicted probability, i.e., the output activations of the network.\nHere, you can see that when for an individual example, the network predicts the wrong class and is highly confident about it, this example will contributely very strongly to the loss.\nCross entropy penalizes wrong predictions most when they are highly confident.What happens when we distinguish between more than two classes?\nMulti-class classification\nCIFAR-10 has 10 classes; so now we want to decide which of 10 object classes is present in the image.\nHere first is the code: Not many differences to the above, but note the changes in activation and cost function.\n\n\ncifar10 <- dataset_cifar10()\n\nx_train <- cifar10$train$x / 255\ny_train <- cifar10$train$y\n\nmodel <- keras_model_sequential() %>%\n  layer_conv_2d(\n    filter = 8,\n    kernel_size = c(3, 3),\n    padding = \"same\",\n    input_shape = c(32, 32, 3),\n    activation = \"relu\"\n  ) %>%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_conv_2d(\n    filter = 8,\n    kernel_size = c(3, 3),\n    padding = \"same\",\n    activation = \"relu\"\n  ) %>%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_flatten() %>%\n  layer_dense(units = 32, activation = \"relu\") %>%\n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel %>% compile(\n  optimizer = \"adam\",\n  loss = \"sparse_categorical_crossentropy\",\n  metrics = \"accuracy\"\n)\n\nmodel %>% fit(\n  x = x_train,\n  y = y_train,\n  epochs = 50\n)\n\nSo now we have softmax combined with categorical crossentropy. Why?\nAgain, we want a valid probability distribution: Probabilities for all disjunct events should sum to 1.\nCIFAR-10 has one object per image; so events are disjunct. Then we have a single-draw multinomial distribution (popularly known as “Multinoulli”, mostly due to Murphy’s Machine learning(Murphy 2012)) that can be modeled by the softmax activation:\n\\[softmax(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_j{e^{z_j}}}\\]\nJust as the sigmoid, the softmax can saturate. In this case, that will happen when differences between outputs become very big. Also like with the sigmoid, a \\(log\\) in the cost function undoes the \\(exp\\) that’s responsible for saturation:\n\\[log\\ softmax(\\mathbf{z})_i = z_i - log\\sum_j{e^{z_j}}\\]\nHere \\(z_i\\) is the class we’re estimating the probability of - we see that its contribution to the loss is linear and thus, can never saturate.\nIn Keras, the loss function that does this for us is called categorical_crossentropy. We use sparse_categorical_crossentropy in the code which is the same as categorical_crossentropy but does not need conversion of integer labels to one-hot vectors.\nLet’s take a closer look at what softmax does. Assume these are the raw outputs of our 10 output units:\nSimulated output before application of softmax.Now this is what the normalized probability distribution looks like after taking the softmax:\nFinal output after softmax.Do you see where the winner takes all in the title comes from? This is an important point to keep in mind: Activation functions are not just there to produce certain desired distributions; they can also change relationships between values.\nConclusion\nWe started this post alluding to common heuristics, such as “for multi-class classification, we use softmax activation, combined with categorical crossentropy as the loss function”. Hopefully, we’ve succeeded in showing why these heuristics make sense.\nHowever, knowing that background, you can also infer when these rules do not apply. For example, say you want to detect several objects in an image. In that case, the winner-takes-all strategy is not the most useful, as we don’t want to exaggerate differences between candidates. So here, we’d use sigmoid on all output units instead, to determine a probability of presence per object.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.\n\n\nMurphy, Kevin. 2012. Machine Learning: A Probabilistic Perspective. MIT Press.\n\n\nThe actual outcome depends on the task. In the above simple classification example, training with mean squared error will attain similar accuracy in similar time.↩︎\n",
    "preview": "posts/2018-10-11-activations-intro/images/output.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 800,
    "preview_height": 384
  },
  {
    "path": "posts/2018-10-02-eager-wrapup/",
    "title": "More flexible models with TensorFlow eager execution and Keras",
    "description": "Advanced applications like generative adversarial networks, neural style transfer, and the attention mechanism ubiquitous in natural language processing used to be not-so-simple to implement with the Keras declarative coding paradigm. Now, with the advent of TensorFlow eager execution, things have changed. This post explores using eager execution with R.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-10-02",
    "categories": [
      "TensorFlow/Keras"
    ],
    "contents": "\nIf you have used Keras to create neural networks you are no doubt familiar with the Sequential API, which represents models as a linear stack of layers. The Functional API gives you additional options: Using separate input layers, you can combine text input with tabular data. Using multiple outputs, you can perform regression and classification at the same time. Furthermore, you can reuse layers within and between models.\nWith TensorFlow eager execution, you gain even more flexibility. Using custom models, you define the forward pass through the model completely ad libitum. This means that a lot of architectures get a lot easier to implement, including the applications mentioned above: generative adversarial networks, neural style transfer, various forms of sequence-to-sequence models. In addition, because you have direct access to values, not tensors, model development and debugging are greatly sped up.\nHow does it work?\nIn eager execution, operations are not compiled into a graph, but directly defined in your R code. They return values, not symbolic handles to nodes in a computational graph - meaning, you don’t need access to a TensorFlow session to evaluate them.\n\n\nm1 <- matrix(1:8, nrow = 2, ncol = 4)\nm2 <- matrix(1:8, nrow = 4, ncol = 2)\ntf$matmul(m1, m2)\n\n\ntf.Tensor(\n[[ 50 114]\n [ 60 140]], shape=(2, 2), dtype=int32)\nEager execution, recent though it is, is already supported in the current CRAN releases of keras and tensorflow. The eager execution guide describes the workflow in detail.\nHere’s a quick outline: You define a model, an optimizer, and a loss function. Data is streamed via tfdatasets, including any preprocessing such as image resizing. Then, model training is just a loop over epochs, giving you complete freedom over when (and whether) to execute any actions.\nHow does backpropagation work in this setup? The forward pass is recorded by a GradientTape, and during the backward pass we explicitly calculate gradients of the loss with respect to the model’s weights. These weights are then adjusted by the optimizer.\n\n\nwith(tf$GradientTape() %as% tape, {\n     \n  # run model on current batch\n  preds <- model(x)\n \n  # compute the loss\n  loss <- mse_loss(y, preds, x)\n  \n})\n    \n# get gradients of loss w.r.t. model weights\ngradients <- tape$gradient(loss, model$variables)\n\n# update model weights\noptimizer$apply_gradients(\n  purrr::transpose(list(gradients, model$variables)),\n  global_step = tf$train$get_or_create_global_step()\n)\n\nSee the eager execution guide for a complete example. Here, we want to answer the question: Why are we so excited about it? At least three things come to mind:\nThings that used to be complicated become much easier to accomplish.\nModels are easier to develop, and easier to debug.\nThere is a much better match between our mental models and the code we write.\nWe’ll illustrate these points using a set of eager execution case studies that have recently appeared on this blog.\nComplicated stuff made easier\nA good example of architectures that become much easier to define with eager execution are attention models. Attention is an important ingredient of sequence-to-sequence models, e.g. (but not only) in machine translation.\nWhen using LSTMs on both the encoding and the decoding sides, the decoder, being a recurrent layer, knows about the sequence it has generated so far. It also (in all but the simplest models) has access to the complete input sequence. But where in the input sequence is the piece of information it needs to generate the next output token? It is this question that attention is meant to address.\nNow consider implementing this in code. Each time it is called to produce a new token, the decoder needs to get current input from the attention mechanism. This means we can’t just squeeze an attention layer between the encoder and the decoder LSTM. Before the advent of eager execution, a solution would have been to implement this in low-level TensorFlow code. With eager execution and custom models, we can just use Keras.\n\nAttention is not just relevant to sequence-to-sequence problems, though. In image captioning, the output is a sequence, while the input is a complete image. When generating a caption, attention is used to focus on parts of the image relevant to different time steps in the text-generating process.\n\nEasy inspection\nIn terms of debuggability, just using custom models (without eager execution) already simplifies things. If we have a custom model like simple_dot from the recent embeddings post and are unsure if we’ve got the shapes correct, we can simply add logging statements, like so:\n\n\n\nfunction(x, mask = NULL) {\n  \n  users <- x[, 1]\n  movies <- x[, 2]\n  \n  user_embedding <- self$user_embedding(users)\n  cat(dim(user_embedding), \"\\n\")\n  \n  movie_embedding <- self$movie_embedding(movies)\n  cat(dim(movie_embedding), \"\\n\")\n  \n  dot <- self$dot(list(user_embedding, movie_embedding))\n  cat(dim(dot), \"\\n\")\n  dot\n}\n\nWith eager execution, things get even better: We can print the tensors’ values themselves.1\nBut convenience does not end there. In the training loop we showed above, we can obtain losses, model weights, and gradients just by printing them. For example, add a line after the call to tape$gradient to print the gradients for all layers as a list.\n\n\ngradients <- tape$gradient(loss, model$variables)\nprint(gradients)\n\nMatching the mental model\nIf you’ve read Deep Learning with R, you know that it’s possible to program less straightforward workflows, such as those required for training GANs or doing neural style transfer, using the Keras functional API. However, the graph code does not make it easy to keep track of where you are in the workflow.\nNow compare the example from the generating digits with GANs post. Generator and discriminator each get set up as actors in a drama:\n\n\n\ngenerator <- function(name = NULL) {\n  keras_model_custom(name = name, function(self) {\n    # ...\n  }\n}\n\n\n\ndiscriminator <- function(name = NULL) {\n  keras_model_custom(name = name, function(self) {\n    # ...\n  }\n}\n\nBoth are informed about their respective loss functions and optimizers.\nThen, the duel starts. The training loop is just a succession of generator actions, discriminator actions, and backpropagation through both models. No need to worry about freezing/unfreezing weights in the appropriate places.\n\n\nwith(tf$GradientTape() %as% gen_tape, { with(tf$GradientTape() %as% disc_tape, {\n  \n # generator action\n generated_images <- generator(# ...\n   \n # discriminator assessments\n disc_real_output <- discriminator(# ... \n disc_generated_output <- discriminator(# ...\n      \n # generator loss\n gen_loss <- generator_loss(# ...                        \n # discriminator loss\n disc_loss <- discriminator_loss(# ...\n   \n})})\n   \n# calcucate generator gradients   \ngradients_of_generator <- gen_tape$gradient(#...\n  \n# calcucate discriminator gradients   \ngradients_of_discriminator <- disc_tape$gradient(# ...\n \n# apply generator gradients to model weights       \ngenerator_optimizer$apply_gradients(# ...\n\n# apply discriminator gradients to model weights \ndiscriminator_optimizer$apply_gradients(# ...\n\nThe code ends up so close to how we mentally picture the situation that hardly any memorization is needed to keep in mind the overall design.\nRelatedly, this way of programming lends itself to extensive modularization. This is illustrated by the second post on GANs that includes U-Net like downsampling and upsampling steps.\n\nHere, the downsampling and upsampling layers are each factored out into their own models\n\n\ndownsample <- function(# ...\n  keras_model_custom(name = NULL, function(self) { # ...\n\nsuch that they can be readably composed in the generator’s call method:\n\n\n# model fields\nself$down1 <- downsample(# ...\nself$down2 <- downsample(# ...\n# ...\n# ...\n\n# call method\nfunction(x, mask = NULL, training = TRUE) {       \n     \n  x1 <- x %>% self$down1(training = training)         \n  x2 <- self$down2(x1, training = training)           \n  # ...\n  # ...\n\nWrapping up\nEager execution is still a very recent feature and under development. We are convinced that many interesting use cases will still turn up as this paradigm gets adopted more widely among deep learning practitioners.\nHowever, now already we have a list of use cases illustrating the vast options, gains in usability, modularization and elegance offered by eager execution code.\nFor quick reference, these cover:\nNeural machine translation with attention. This post provides a detailed introduction to eager execution and its building blocks, as well as an in-depth explanation of the attention mechanism used. Together with the next one, it occupies a very special role in this list: It uses eager execution to solve a problem that otherwise could only be solved with hard-to-read, hard-to-write low-level code.\nImage captioning with attention. This post builds on the first in that it does not re-explain attention in detail; however, it ports the concept to spatial attention applied over image regions.\nGenerating digits with convolutional generative adversarial networks (DCGANs). This post introduces using two custom models, each with their associated loss functions and optimizers, and having them go through forward- and backpropagation in sync. It is perhaps the most impressive example of how eager execution simplifies coding by better alignment to our mental model of the situation.\nImage-to-image translation with pix2pix is another application of generative adversarial networks, but uses a more complex architecture based on U-Net-like downsampling and upsampling. It nicely demonstrates how eager execution allows for modular coding, rendering the final program much more readable.\nNeural style transfer. Finally, this post reformulates the style transfer problem in an eager way, again resulting in readable, concise code.\nWhen diving into these applications, it is a good idea to also refer to the eager execution guide so you don’t lose sight of the forest for the trees.\nWe are excited about the use cases our readers will come up with!\nNote that the embeddings example uses standard (graph) execution; refactoring would be needed in order to enable eager execution on it.↩︎\n",
    "preview": "posts/2018-10-02-eager-wrapup/images/m.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 384,
    "preview_height": 126
  },
  {
    "path": "posts/2018-09-26-embeddings-recommender/",
    "title": "Collaborative filtering with embeddings",
    "description": "Embeddings are not just for use in natural language processing. Here we apply embeddings to a common task in collaborative filtering - predicting user ratings - and on our way, strive for a better understanding of what an embedding layer really does.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-09-26",
    "categories": [
      "TensorFlow/Keras",
      "Tabular Data"
    ],
    "contents": "\nWhat’s your first association when you read the word embeddings? For most of us, the answer will probably be word embeddings, or word vectors. A quick search for recent papers on arxiv shows what else can be embedded: equations(Krstovski and Blei 2018), vehicle sensor data(Hallac et al. 2018), graphs(Ahmed et al. 2018), code(Alon et al. 2018), spatial data(Jean et al. 2018), biological entities(Zohra Smaili, Gao, and Hoehndorf 2018) … - and what not.\nWhat is so attractive about this concept? Embeddings incorporate the concept of distributed representations, an encoding of information not at specialized locations (dedicated neurons, say), but as a pattern of activations spread out over a network. No better source to cite than Geoffrey Hinton, who played an important role in the development of the concept(Rumelhart, McClelland, and PDP Research Group 1986):\n\nDistributed representation means a many to many relationship between two types of representation (such as concepts and neurons). Each concept is represented by many neurons. Each neuron participates in the representation of many concepts.1\n\nThe advantages are manifold. Perhaps the most famous effect of using embeddings is that we can learn and make use of semantic similarity.\nLet’s take a task like sentiment analysis. Initially, what we feed the network are sequences of words, essentially encoded as factors. In this setup, all words are equidistant: Orange is as different from kiwi as it is from thunderstorm. An ensuing embedding layer then maps these representations to dense vectors of floating point numbers, which can be checked for mutual similarity via various similarity measures such as cosine distance.\nWe hope that when we feed these “meaningful” vectors to the next layer(s), better classification will result. In addition, we may be interested in exploring that semantic space for its own sake, or use it in multi-modal transfer learning (Frome et al. 2013).\nIn this post, we’d like to do two things: First, we want to show an interesting application of embeddings beyond natural language processing, namely, their use in collaborative filtering. In this, we follow ideas developed in lesson5-movielens.ipynb which is part of fast.ai’s Deep Learning for Coders class. Second, to gather more intuition, we’d like to take a look “under the hood” at how a simple embedding layer can be implemented.\nSo first, let’s jump into collaborative filtering. Just like the notebook that inspired us, we’ll predict movie ratings. We will use the 2016 ml-latest-small dataset from MovieLens that contains ~100000 ratings of ~9900 movies, rated by ~700 users.\nEmbeddings for collaborative filtering\nIn collaborative filtering, we try to generate recommendations based not on elaborate knowledge about our users and not on detailed profiles of our products, but on how users and products go together. Is product \\(\\mathbf{p}\\) a fit for user \\(\\mathbf{u}\\)? If so, we’ll recommend it.\nOften, this is done via matrix factorization. See, for example, this nice article by the winners of the 2009 Netflix prize, introducing the why and how of matrix factorization techniques as used in collaborative filtering.\nHere’s the general principle. While other techniques like non-negative matrix factorization may be more popular, this diagram of singular value decomposition (SVD) found on Facebook Research is particularly instructive.\nFigure from https://research.fb.com/fast-randomized-svd/The diagram takes its example from the context of text analysis, assuming a co-occurrence matrix of hashtags and users (\\(\\mathbf{A}\\)). As stated above, we’ll instead work with a dataset of movie ratings.\nWere we doing matrix factorization, we would need to somehow address the fact that not every user has rated every movie. As we’ll be using embeddings instead, we won’t have that problem. For the sake of argumentation, though, let’s assume for a moment the ratings were a matrix, not a dataframe in tidy format.\nIn that case, \\(\\mathbf{A}\\) would store the ratings, with each row containing the ratings one user gave to all movies.\nThis matrix then gets decomposed into three matrices:\n\\(\\mathbf{\\Sigma}\\) stores the importance of the latent factors governing the relationship between users and movies.\n\\(\\mathbf{U}\\) contains information on how users score on these latent factors. It’s a representation (embedding) of users by the ratings they gave to the movies.\n\\(\\mathbf{V}\\) stores how movies score on these same latent factors. It’s a representation (embedding) of movies by how they got rated by said users.\nAs soon as we have a representation of movies  as well as users  in the same latent space, we can determine their mutual fit by a simple dot product \\(\\mathbf{m^ t}\\mathbf{u}\\). Assuming the user and movie vectors have been normalized to length 1, this is equivalent to calculating the cosine similarity\n\\[cos(\\theta) = \\frac{\\mathbf{x^ t}\\mathbf{y}}{\\mathbf{||x||}\\space\\mathbf{||y||}}\\]\nWhat does all this have to do with embeddings?\nWell, the same overall principles apply when we work with user resp. movie embeddings, instead of vectors obtained from matrix factorization. We’ll have one layer_embedding for users, one layer_embedding for movies, and a layer_lambda that calculates the dot product.\nHere’s a minimal custom model that does exactly this:2\n\n\nsimple_dot <- function(embedding_dim,\n                       n_users,\n                       n_movies,\n                       name = \"simple_dot\") {\n  \n  keras_model_custom(name = name, function(self) {\n    self$user_embedding <-\n      layer_embedding(\n        input_dim = n_users + 1,\n        output_dim = embedding_dim,\n        embeddings_initializer = initializer_random_uniform(minval = 0, maxval = 0.05),\n        name = \"user_embedding\"\n      )\n    self$movie_embedding <-\n      layer_embedding(\n        input_dim = n_movies + 1,\n        output_dim = embedding_dim,\n        embeddings_initializer = initializer_random_uniform(minval = 0, maxval = 0.05),\n        name = \"movie_embedding\"\n      )\n    self$dot <-\n      layer_lambda(\n        f = function(x) {\n          k_batch_dot(x[[1]], x[[2]], axes = 2)\n        }\n      )\n    \n    function(x, mask = NULL) {\n      users <- x[, 1]\n      movies <- x[, 2]\n      user_embedding <- self$user_embedding(users)\n      movie_embedding <- self$movie_embedding(movies)\n      self$dot(list(user_embedding, movie_embedding))\n    }\n  })\n}\n\nWe’re still missing the data though! Let’s load it. Besides the ratings themselves, we’ll also get the titles from movies.csv.\n\n\ndata_dir <- \"ml-latest-small\"\nmovies <- read_csv(file.path(data_dir, \"movies.csv\"))\nratings <- read_csv(file.path(data_dir, \"ratings.csv\"))\n\nWhile user ids have no gaps in this sample, that’s different for movie ids. We therefore convert them to consecutive numbers, so we can later specify an adequate size for the lookup matrix.\n\n\ndense_movies <- ratings %>% select(movieId) %>% distinct() %>% rowid_to_column()\nratings <- ratings %>% inner_join(dense_movies) %>% rename(movieIdDense = rowid)\nratings <- ratings %>% inner_join(movies) %>% select(userId, movieIdDense, rating, title, genres)\n\nLet’s take a note, then, of how many users resp. movies we have.\n\n\nn_movies <- ratings %>% select(movieIdDense) %>% distinct() %>% nrow()\nn_users <- ratings %>% select(userId) %>% distinct() %>% nrow()\n\nWe’ll split off 20% of the data for validation. After training, probably all users will have been seen by the network, while very likely, not all movies will have occurred in the training sample.\n\n\ntrain_indices <- sample(1:nrow(ratings), 0.8 * nrow(ratings))\ntrain_ratings <- ratings[train_indices,]\nvalid_ratings <- ratings[-train_indices,]\n\nx_train <- train_ratings %>% select(c(userId, movieIdDense)) %>% as.matrix()\ny_train <- train_ratings %>% select(rating) %>% as.matrix()\nx_valid <- valid_ratings %>% select(c(userId, movieIdDense)) %>% as.matrix()\ny_valid <- valid_ratings %>% select(rating) %>% as.matrix()\n\nTraining a simple dot product model\nWe’re ready to start the training process. Feel free to experiment with different embedding dimensionalities.\n\n\nembedding_dim <- 64\n\nmodel <- simple_dot(embedding_dim, n_users, n_movies)\n\nmodel %>% compile(\n  loss = \"mse\",\n  optimizer = \"adam\"\n)\n\nhistory <- model %>% fit(\n  x_train,\n  y_train,\n  epochs = 10,\n  batch_size = 32,\n  validation_data = list(x_valid, y_valid),\n  callbacks = list(callback_early_stopping(patience = 2))\n)\n\nHow well does this work? Final RMSE (the square root of the MSE loss we were using) on the validation set is around 1.08 , while popular benchmarks (e.g., of the LibRec recommender system) lie around 0.91. Also, we’re overfitting early. It looks like we need a slightly more sophisticated system.\nTraining curve for simple dot product modelAccounting for user and movie biases\nA problem with our method is that we attribute the rating as a whole to user-movie interaction. However, some users are intrinsically more critical, while others tend to be more lenient. Analogously, films differ by average rating. We hope to get better predictions when factoring in these biases.\nConceptually, we then calculate a prediction like this:\n\\[pred =  avg + bias_m + bias_u + \\mathbf{m^ t}\\mathbf{u}\\]\nThe corresponding Keras model gets just slightly more complex. In addition to the user and movie embeddings we’ve already been working with, the below model embeds the average user and the average movie in 1-d space. We then add both biases to the dot product encoding user-movie interaction. A sigmoid activation normalizes to a value between 0 and 1, which then gets mapped back to the original space.\nNote how in this model, we also use dropout on the user and movie embeddings (again, the best dropout rate is open to experimentation).\n\n\nmax_rating <- ratings %>% summarise(max_rating = max(rating)) %>% pull()\nmin_rating <- ratings %>% summarise(min_rating = min(rating)) %>% pull()\n\ndot_with_bias <- function(embedding_dim,\n                          n_users,\n                          n_movies,\n                          max_rating,\n                          min_rating,\n                          name = \"dot_with_bias\"\n                          ) {\n  keras_model_custom(name = name, function(self) {\n    \n    self$user_embedding <-\n      layer_embedding(input_dim = n_users + 1,\n                      output_dim = embedding_dim,\n                      name = \"user_embedding\")\n    self$movie_embedding <-\n      layer_embedding(input_dim = n_movies + 1,\n                      output_dim = embedding_dim,\n                      name = \"movie_embedding\")\n    self$user_bias <-\n      layer_embedding(input_dim = n_users + 1,\n                      output_dim = 1,\n                      name = \"user_bias\")\n    self$movie_bias <-\n      layer_embedding(input_dim = n_movies + 1,\n                      output_dim = 1,\n                      name = \"movie_bias\")\n    self$user_dropout <- layer_dropout(rate = 0.3)\n    self$movie_dropout <- layer_dropout(rate = 0.6)\n    self$dot <-\n      layer_lambda(\n        f = function(x)\n          k_batch_dot(x[[1]], x[[2]], axes = 2),\n        name = \"dot\"\n      )\n    self$dot_bias <-\n      layer_lambda(\n        f = function(x)\n          k_sigmoid(x[[1]] + x[[2]] + x[[3]]),\n        name = \"dot_bias\"\n      )\n    self$pred <- layer_lambda(\n      f = function(x)\n        x * (self$max_rating - self$min_rating) + self$min_rating,\n      name = \"pred\"\n    )\n    self$max_rating <- max_rating\n    self$min_rating <- min_rating\n    \n    function(x, mask = NULL) {\n      \n      users <- x[, 1]\n      movies <- x[, 2]\n      user_embedding <-\n        self$user_embedding(users) %>% self$user_dropout()\n      movie_embedding <-\n        self$movie_embedding(movies) %>% self$movie_dropout()\n      dot <- self$dot(list(user_embedding, movie_embedding))\n      dot_bias <-\n        self$dot_bias(list(dot, self$user_bias(users), self$movie_bias(movies)))\n      self$pred(dot_bias)\n    }\n  })\n}\n\nHow well does this model perform?\n\n\nmodel <- dot_with_bias(embedding_dim,\n                       n_users,\n                       n_movies,\n                       max_rating,\n                       min_rating)\n\nmodel %>% compile(\n  loss = \"mse\",\n  optimizer = \"adam\"\n)\n\nhistory <- model %>% fit(\n  x_train,\n  y_train,\n  epochs = 10,\n  batch_size = 32,\n  validation_data = list(x_valid, y_valid),\n  callbacks = list(callback_early_stopping(patience = 2))\n)\n\nNot only does it overfit later, it actually reaches a way better RMSE of 0.88 on the validation set!\nTraining curve for dot product model with biasesSpending some time on hyperparameter optimization could very well lead to even better results. As this post focuses on the conceptual side though, we want to see what else we can do with those embeddings.\nEmbeddings: a closer look\nWe can easily extract the embedding matrices from the respective layers. Let’s do this for movies now.\n\n\nmovie_embeddings <- (model %>% get_layer(\"movie_embedding\") %>% get_weights())[[1]]\n\nHow are they distributed? Here’s a heatmap of the first 20 movies. (Note how we increment the row indices by 1, because the very first row in the embedding matrix belongs to a movie id 0 which does not exist in our dataset.) We see that the embeddings look rather uniformly distributed between -0.5 and 0.5.\n\n\nlevelplot(\n  t(movie_embeddings[2:21, 1:64]),\n  xlab = \"\",\n  ylab = \"\",\n  scale = (list(draw = FALSE)))\n\nEmbeddings for first 20 moviesNaturally, we might be interested in dimensionality reduction, and see how specific movies score on the dominant factors. A possible way to achieve this is PCA:\n\n\nmovie_pca <- movie_embeddings %>% prcomp(center = FALSE)\ncomponents <- movie_pca$x %>% as.data.frame() %>% rowid_to_column()\n\nplot(movie_pca)\n\nPCA: Variance explained by componentLet’s just look at the first principal component as the second one already explains much less variance.\nHere are the 10 movies (out of all that were rated at least 20 times) that scored lowest on the first factor:\n\n\nratings_with_pc12 <-\n  ratings %>% inner_join(components %>% select(rowid, PC1, PC2),\n                         by = c(\"movieIdDense\" = \"rowid\"))\n\nratings_grouped <-\n  ratings_with_pc12 %>%\n  group_by(title) %>%\n  summarize(\n    PC1 = max(PC1),\n    PC2 = max(PC2),\n    rating = mean(rating),\n    genres = max(genres),\n    num_ratings = n()\n  )\n\nratings_grouped %>% filter(num_ratings > 20) %>% arrange(PC1) %>% print(n = 10)\n\n\n# A tibble: 1,247 x 6\n   title                                   PC1      PC2 rating genres                   num_ratings\n   <chr>                                 <dbl>    <dbl>  <dbl> <chr>                          <int>\n 1 Starman (1984)                       -1.15  -0.400     3.45 Adventure|Drama|Romance…          22\n 2 Bulworth (1998)                      -0.820  0.218     3.29 Comedy|Drama|Romance              31\n 3 Cable Guy, The (1996)                -0.801 -0.00333   2.55 Comedy|Thriller                   59\n 4 Species (1995)                       -0.772 -0.126     2.81 Horror|Sci-Fi                     55\n 5 Save the Last Dance (2001)           -0.765  0.0302    3.36 Drama|Romance                     21\n 6 Spanish Prisoner, The (1997)         -0.760  0.435     3.91 Crime|Drama|Mystery|Thr…          23\n 7 Sgt. Bilko (1996)                    -0.757  0.249     2.76 Comedy                            29\n 8 Naked Gun 2 1/2: The Smell of Fear,… -0.749  0.140     3.44 Comedy                            27\n 9 Swordfish (2001)                     -0.694  0.328     2.92 Action|Crime|Drama                33\n10 Addams Family Values (1993)          -0.693  0.251     3.15 Children|Comedy|Fantasy           73\n# ... with 1,237 more rows\nAnd here, inversely, are those that scored highest:\n\n\nratings_grouped %>% filter(num_ratings > 20) %>% arrange(desc(PC1)) %>% print(n = 10)\n\n\n A tibble: 1,247 x 6\n   title                                PC1        PC2 rating genres                    num_ratings\n   <chr>                              <dbl>      <dbl>  <dbl> <chr>                           <int>\n 1 Graduate, The (1967)                1.41  0.0432      4.12 Comedy|Drama|Romance               89\n 2 Vertigo (1958)                      1.38 -0.0000246   4.22 Drama|Mystery|Romance|Th…          69\n 3 Breakfast at Tiffany's (1961)       1.28  0.278       3.59 Drama|Romance                      44\n 4 Treasure of the Sierra Madre, The…  1.28 -0.496       4.3  Action|Adventure|Drama|W…          30\n 5 Boot, Das (Boat, The) (1981)        1.26  0.238       4.17 Action|Drama|War                   51\n 6 Flintstones, The (1994)             1.18  0.762       2.21 Children|Comedy|Fantasy            39\n 7 Rock, The (1996)                    1.17 -0.269       3.74 Action|Adventure|Thriller         135\n 8 In the Heat of the Night (1967)     1.15 -0.110       3.91 Drama|Mystery                      22\n 9 Quiz Show (1994)                    1.14 -0.166       3.75 Drama                              90\n10 Striptease (1996)                   1.14 -0.681       2.46 Comedy|Crime                       39\n# ... with 1,237 more rows\nWe’ll leave it to the knowledgeable reader to name these factors, and proceed to our second topic: How does an embedding layer do what it does?\nDo-it-yourself embeddings\nYou may have heard people say all an embedding layer did was just a lookup. Imagine you had a dataset that, in addition to continuous variables like temperature or barometric pressure, contained a categorical column characterization consisting of tags like “foggy” or “cloudy”. Say characterization had 7 possible values, encoded as a factor with levels 1-7.\nWere we going to feed this variable to a non-embedding layer, layer_dense say, we’d have to take care that those numbers do not get taken for integers, thus falsely implying an interval (or at least ordered) scale. But when we use an embedding as the first layer in a Keras model, we feed in integers all the time! For example, in text classification, a sentence might get encoded as a vector padded with zeroes, like this:\n\n2  77   4   5 122   55  1  3   0   0  \nThe thing that makes this work is that the embedding layer actually does perform a lookup. Below, you’ll find a very simple3 custom layer that does essentially the same thing as Keras’ layer_embedding:\nIt has a weight matrix self$embeddings that maps from an input space (movies, say) to the output space of latent factors (embeddings).\nWhen we call the layer, as in\nx <- k_gather(self$embeddings, x)\nit looks up the passed-in row number in the weight matrix, thus retrieving an item’s distributed representation from the matrix.\n\n\nSimpleEmbedding <- R6::R6Class(\n  \"SimpleEmbedding\",\n  \n  inherit = KerasLayer,\n  \n  public = list(\n    output_dim = NULL,\n    emb_input_dim = NULL,\n    embeddings = NULL,\n    \n    initialize = function(emb_input_dim, output_dim) {\n      self$emb_input_dim <- emb_input_dim\n      self$output_dim <- output_dim\n    },\n    \n    build = function(input_shape) {\n      self$embeddings <- self$add_weight(\n        name = 'embeddings',\n        shape = list(self$emb_input_dim, self$output_dim),\n        initializer = initializer_random_uniform(),\n        trainable = TRUE\n      )\n    },\n    \n    call = function(x, mask = NULL) {\n      x <- k_cast(x, \"int32\")\n      k_gather(self$embeddings, x)\n    },\n    \n    compute_output_shape = function(input_shape) {\n      list(self$output_dim)\n    }\n  )\n)\n\nAs usual with custom layers, we still need a wrapper that takes care of instantiation.\n\n\nlayer_simple_embedding <-\n  function(object,\n           emb_input_dim,\n           output_dim,\n           name = NULL,\n           trainable = TRUE) {\n    create_layer(\n      SimpleEmbedding,\n      object,\n      list(\n        emb_input_dim = as.integer(emb_input_dim),\n        output_dim = as.integer(output_dim),\n        name = name,\n        trainable = trainable\n      )\n    )\n  }\n\nDoes this work? Let’s test it on the ratings prediction task! We’ll just substitute the custom layer in the simple dot product model we started out with, and check if we get out a similar RMSE.\nPutting the custom embedding layer to test\nHere’s the simple dot product model again, this time using our custom embedding layer.\n\n\nsimple_dot2 <- function(embedding_dim,\n                       n_users,\n                       n_movies,\n                       name = \"simple_dot2\") {\n  \n  keras_model_custom(name = name, function(self) {\n    self$embedding_dim <- embedding_dim\n    \n    self$user_embedding <-\n      layer_simple_embedding(\n        emb_input_dim = list(n_users + 1),\n        output_dim = embedding_dim,\n        name = \"user_embedding\"\n      )\n    self$movie_embedding <-\n      layer_simple_embedding(\n        emb_input_dim = list(n_movies + 1),\n        output_dim = embedding_dim,\n        name = \"movie_embedding\"\n      )\n    self$dot <-\n      layer_lambda(\n        output_shape = self$embedding_dim,\n        f = function(x) {\n          k_batch_dot(x[[1]], x[[2]], axes = 2)\n        }\n      )\n    \n    function(x, mask = NULL) {\n      users <- x[, 1]\n      movies <- x[, 2]\n      user_embedding <- self$user_embedding(users)\n      movie_embedding <- self$movie_embedding(movies)\n      self$dot(list(user_embedding, movie_embedding))\n    }\n  })\n}\n\nmodel <- simple_dot2(embedding_dim, n_users, n_movies)\n\nmodel %>% compile(\n  loss = \"mse\",\n  optimizer = \"adam\"\n)\n\nhistory <- model %>% fit(\n  x_train,\n  y_train,\n  epochs = 10,\n  batch_size = 32,\n  validation_data = list(x_valid, y_valid),\n  callbacks = list(callback_early_stopping(patience = 2))\n)\n\nWe end up with a RMSE of 1.13 on the validation set, which is not far from the 1.08 we obtained when using layer_embedding. At least, this should tell us that we successfully reproduced the approach.\nConclusion\nOur goals in this post were twofold: Shed some light on how an embedding layer can be implemented, and show how embeddings calculated by a neural network can be used as a substitute for component matrices obtained from matrix decomposition. Of course, this is not the only thing that’s fascinating about embeddings!\nFor example, a very practical question is how much actual predictions can be improved by using embeddings instead of one-hot vectors; another is how learned embeddings might differ depending on what task they were trained on. Last not least - how do latent factors learned via embeddings differ from those learned by an autoencoder?\nIn that spirit, there is no lack of topics for exploration and poking around …\n\n\nAhmed, N. K., R. Rossi, J. Boaz Lee, T. L. Willke, R. Zhou, X. Kong, and H. Eldardiry. 2018. “Learning Role-Based Graph Embeddings.” ArXiv E-Prints, February. http://arxiv.org/abs/1802.02896.\n\n\nAlon, Uri, Meital Zilberstein, Omer Levy, and Eran Yahav. 2018. “Code2vec: Learning Distributed Representations of Code.” CoRR abs/1803.09473. http://arxiv.org/abs/1803.09473.\n\n\nFrome, Andrea, Gregory S. Corrado, Jonathon Shlens, Samy Bengio, Jeffrey Dean, Marc’Aurelio Ranzato, and Tomas Mikolov. 2013. “DeViSE: A Deep Visual-Semantic Embedding Model.” In NIPS, 2121–9.\n\n\nHallac, D., S. Bhooshan, M. Chen, K. Abida, R. Sosic, and J. Leskovec. 2018. “Drive2Vec: Multiscale State-Space Embedding of Vehicular Sensor Data.” ArXiv E-Prints, June. http://arxiv.org/abs/1806.04795.\n\n\nJean, Neal, Sherrie Wang, Anshul Samar, George Azzari, David B. Lobell, and Stefano Ermon. 2018. “Tile2Vec: Unsupervised Representation Learning for Spatially Distributed Data.” CoRR abs/1805.02855. http://arxiv.org/abs/1805.02855.\n\n\nKrstovski, K., and D. M. Blei. 2018. “Equation Embeddings.” ArXiv E-Prints, March. http://arxiv.org/abs/1803.09123.\n\n\nRumelhart, David E., James L. McClelland, and CORPORATE PDP Research Group, eds. 1986. Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 2: Psychological and Biological Models. Cambridge, MA, USA: MIT Press.\n\n\nZohra Smaili, F., X. Gao, and R. Hoehndorf. 2018. “Onto2Vec: Joint Vector-Based Representation of Biological Entities and Their Ontology-Based Annotations.” ArXiv E-Prints, January. http://arxiv.org/abs/1802.00864.\n\n\nFrom: http://www.cs.toronto.edu/~bonner/courses/2014s/csc321/lectures/lec5.pdf↩︎\nCustom models are a recent Keras feature that allow for a flexible definition of the forward pass. While the current use case does not require using a custom model, it nicely illustrates how the network’s logic can quickly be grasped by looking at the call method.↩︎\nIt really is simple; it only works with input length = 1.↩︎\n",
    "preview": "posts/2018-09-26-embeddings-recommender/images/m.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 700,
    "preview_height": 402
  },
  {
    "path": "posts/2018-09-20-eager-pix2pix/",
    "title": "Image-to-image translation with pix2pix",
    "description": "Conditional GANs (cGANs) may be used to generate one type of object based on another - e.g., a map based on a photo, or a color video based on black-and-white. Here, we show how to implement the pix2pix approach with Keras and eager execution.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-09-20",
    "categories": [
      "TensorFlow/Keras",
      "Image Recognition & Image Processing",
      "Unsupervised Learning"
    ],
    "contents": "\nWhat do we need to train a neural network? A common answer is: a model, a cost function, and an optimization algorithm. (I know: I’m leaving out the most important thing here - the data.)\nAs computer programs work with numbers, the cost function has to be pretty specific: We can’t just say predict next month’s demand for lawn mowers please, and do your best, we have to say something like this: Minimize the squared deviation of the estimate from the target value.\nIn some cases it may be straightforward to map a task to a measure of error, in others, it may not. Consider the task of generating non-existing objects of a certain type (like a face, a scene, or a video clip). How do we quantify success? The trick with generative adversarial networks (GANs) is to let the network learn the cost function.\nAs shown in Generating images with Keras and TensorFlow eager execution, in a simple GAN the setup is this: One agent, the generator, keeps on producing fake objects. The other, the discriminator, is tasked to tell apart the real objects from the fake ones. For the generator, loss is augmented when its fraud gets discovered, meaning that the generator’s cost function depends on what the discriminator does. For the discriminator, loss grows when it fails to correctly tell apart generated objects from authentic ones.\nIn a GAN of the type just described, creation starts from white noise. However in the real world, what is required may be a form of transformation, not creation. Take, for example, colorization of black-and-white images, or conversion of aerials to maps. For applications like those, we condition on additional input: Hence the name, conditional adversarial networks.\nPut concretely, this means the generator is passed not (or not only) white noise, but data of a certain input structure, such as edges or shapes. It then has to generate realistic-looking pictures of real objects having those shapes. The discriminator, too, may receive the shapes or edges as input, in addition to the fake and real objects it is tasked to tell apart.\nHere are a few examples of conditioning, taken from the paper we’ll be implementing (see below):\nFigure from Image-to-Image Translation with Conditional Adversarial Networks Isola et al. (2016)In this post, we port to R a Google Colaboratory Notebook using Keras with eager execution. We’re implementing the basic architecture from pix2pix, as described by Isola et al. in their 2016 paper(Isola et al. 2016). It’s an interesting paper to read as it validates the approach on a bunch of different datasets, and shares outcomes of using different loss families, too:\nFigure from Image-to-Image Translation with Conditional Adversarial Networks Isola et al. (2016)Prerequisites\nThe code shown here will work with the current CRAN versions of tensorflow, keras, and tfdatasets. Also, be sure to check that you’re using at least version 1.9 of TensorFlow. If that isn’t the case, as of this writing, this\n\n\nlibrary(tensorflow)\ninstall_tensorflow()\n\nwill get you version 1.10.\nWhen loading libraries, please make sure you’re executing the first 4 lines in the exact order shown. We need to make sure we’re using the TensorFlow implementation of Keras (tf.keras in Python land), and we have to enable eager execution before using TensorFlow in any way.\nNo need to copy-paste any code snippets - you’ll find the complete code (in order necessary for execution) here: eager-pix2pix.R.\n\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nlibrary(tfdatasets)\nlibrary(purrr)\n\nDataset\nFor this post, we’re working with one of the datasets used in the paper, a preprocessed version of the CMP Facade Dataset.\nImages contain the ground truth - that we’d wish for the generator to generate, and for the discriminator to correctly detect as authentic - and the input we’re conditioning on (a coarse segmention into object classes) next to each other in the same file.\nFigure from https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/Preprocessing\nObviously, our preprocessing will have to split the input images into parts. That’s the first thing that happens in the function below.\nAfter that, action depends on whether we’re in the training or testing phases. If we’re training, we perform random jittering, via upsizing the image to 286x286 and then cropping to the original size of 256x256. In about 50% of the cases, we also flipping the image left-to-right.\nIn both cases, training and testing, we normalize the image to the range between -1 and 1.\nNote the use of the tf$image module for image -related operations. This is required as the images will be streamed via tfdatasets, which works on TensorFlow graphs.\n\n\nimg_width <- 256L\nimg_height <- 256L\n\nload_image <- function(image_file, is_train) {\n\n  image <- tf$read_file(image_file)\n  image <- tf$image$decode_jpeg(image)\n  \n  w <- as.integer(k_shape(image)[2])\n  w2 <- as.integer(w / 2L)\n  real_image <- image[ , 1L:w2, ]\n  input_image <- image[ , (w2 + 1L):w, ]\n  \n  input_image <- k_cast(input_image, tf$float32)\n  real_image <- k_cast(real_image, tf$float32)\n\n  if (is_train) {\n    input_image <-\n      tf$image$resize_images(input_image,\n                             c(286L, 286L),\n                             align_corners = TRUE,\n                             method = 2)\n    real_image <- tf$image$resize_images(real_image,\n                                         c(286L, 286L),\n                                         align_corners = TRUE,\n                                         method = 2)\n    \n    stacked_image <-\n      k_stack(list(input_image, real_image), axis = 1)\n    cropped_image <-\n      tf$random_crop(stacked_image, size = c(2L, img_height, img_width, 3L))\n    c(input_image, real_image) %<-% \n      list(cropped_image[1, , , ], cropped_image[2, , , ])\n    \n    if (runif(1) > 0.5) {\n      input_image <- tf$image$flip_left_right(input_image)\n      real_image <- tf$image$flip_left_right(real_image)\n    }\n    \n  } else {\n    input_image <-\n      tf$image$resize_images(\n        input_image,\n        size = c(img_height, img_width),\n        align_corners = TRUE,\n        method = 2\n      )\n    real_image <-\n      tf$image$resize_images(\n        real_image,\n        size = c(img_height, img_width),\n        align_corners = TRUE,\n        method = 2\n      )\n  }\n  \n  input_image <- (input_image / 127.5) - 1\n  real_image <- (real_image / 127.5) - 1\n  \n  list(input_image, real_image)\n}\n\nStreaming the data\nThe images will be streamed via tfdatasets, using a batch size of 1. Note how the load_image function we defined above is wrapped in tf$py_func to enable accessing tensor values in the usual eager way (which by default, as of this writing, is not possible with the TensorFlow datasets API).\n\n\n# change to where you unpacked the data\n# there will be train, val and test subdirectories below\ndata_dir <- \"facades\"\n\nbuffer_size <- 400\nbatch_size <- 1\nbatches_per_epoch <- buffer_size / batch_size\n\ntrain_dataset <-\n  tf$data$Dataset$list_files(file.path(data_dir, \"train/*.jpg\")) %>%\n  dataset_shuffle(buffer_size) %>%\n  dataset_map(function(image) {\n    tf$py_func(load_image, list(image, TRUE), list(tf$float32, tf$float32))\n  }) %>%\n  dataset_batch(batch_size)\n\ntest_dataset <-\n  tf$data$Dataset$list_files(file.path(data_dir, \"test/*.jpg\")) %>%\n  dataset_map(function(image) {\n    tf$py_func(load_image, list(image, TRUE), list(tf$float32, tf$float32))\n  }) %>%\n  dataset_batch(batch_size)\n\nDefining the actors\nGenerator\nFirst, here’s the generator. Let’s start with a birds-eye view.\nThe generator receives as input a coarse segmentation, of size 256x256, and should produce a nice color image of a facade. It first successively downsamples the input, up to a minimal size of 1x1. Then after maximal condensation, it starts upsampling again, until it has reached the required output resolution of 256x256.\nDuring downsampling, as spatial resolution decreases, the number of filters increases. During upsampling, it goes the opposite way.\n\n\ngenerator <- function(name = \"generator\") {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$down1 <- downsample(64, 4, apply_batchnorm = FALSE)\n    self$down2 <- downsample(128, 4)\n    self$down3 <- downsample(256, 4)\n    self$down4 <- downsample(512, 4)\n    self$down5 <- downsample(512, 4)\n    self$down6 <- downsample(512, 4)\n    self$down7 <- downsample(512, 4)\n    self$down8 <- downsample(512, 4)\n    \n    self$up1 <- upsample(512, 4, apply_dropout = TRUE)\n    self$up2 <- upsample(512, 4, apply_dropout = TRUE)\n    self$up3 <- upsample(512, 4, apply_dropout = TRUE)\n    self$up4 <- upsample(512, 4)\n    self$up5 <- upsample(256, 4)\n    self$up6 <- upsample(128, 4)\n    self$up7 <- upsample(64, 4)\n    \n    self$last <- layer_conv_2d_transpose(\n      filters = 3,\n      kernel_size = 4,\n      strides = 2,\n      padding = \"same\",\n      kernel_initializer = initializer_random_normal(0, 0.2),\n      activation = \"tanh\"\n    )\n    \n    function(x, mask = NULL, training = TRUE) {           # x shape == (bs, 256, 256, 3)\n     \n      x1 <- x %>% self$down1(training = training)         # (bs, 128, 128, 64)\n      x2 <- self$down2(x1, training = training)           # (bs, 64, 64, 128)\n      x3 <- self$down3(x2, training = training)           # (bs, 32, 32, 256)\n      x4 <- self$down4(x3, training = training)           # (bs, 16, 16, 512)\n      x5 <- self$down5(x4, training = training)           # (bs, 8, 8, 512)\n      x6 <- self$down6(x5, training = training)           # (bs, 4, 4, 512)\n      x7 <- self$down7(x6, training = training)           # (bs, 2, 2, 512)\n      x8 <- self$down8(x7, training = training)           # (bs, 1, 1, 512)\n\n      x9 <- self$up1(list(x8, x7), training = training)   # (bs, 2, 2, 1024)\n      x10 <- self$up2(list(x9, x6), training = training)  # (bs, 4, 4, 1024)\n      x11 <- self$up3(list(x10, x5), training = training) # (bs, 8, 8, 1024)\n      x12 <- self$up4(list(x11, x4), training = training) # (bs, 16, 16, 1024)\n      x13 <- self$up5(list(x12, x3), training = training) # (bs, 32, 32, 512)\n      x14 <- self$up6(list(x13, x2), training = training) # (bs, 64, 64, 256)\n      x15 <-self$up7(list(x14, x1), training = training)  # (bs, 128, 128, 128)\n      x16 <- self$last(x15)                               # (bs, 256, 256, 3)\n      x16\n    }\n  })\n}\n\nHow can spatial information be preserved if we downsample all the way down to a single pixel? The generator follows the general principle of a U-Net (Ronneberger, Fischer, and Brox 2015), where skip connections exist from layers earlier in the downsampling process to layers later on the way up.\nFigure from (Ronneberger, Fischer, and Brox 2015)Let’s take the line\n\n\nx15 <-self$up7(list(x14, x1), training = training)\n\nfrom the call method.\nHere, the inputs to self$up are x14, which went through all of the down- and upsampling, and x1, the output from the very first downsampling step. The former has resolution 64x64, the latter, 128x128. How do they get combined?\nThat’s taken care of by upsample, technically a custom model of its own. As an aside, we remark how custom models let you pack your code into nice, reusable modules.\n\n\nupsample <- function(filters,\n                     size,\n                     apply_dropout = FALSE,\n                     name = \"upsample\") {\n  \n  keras_model_custom(name = NULL, function(self) {\n    \n    self$apply_dropout <- apply_dropout\n    self$up_conv <- layer_conv_2d_transpose(\n      filters = filters,\n      kernel_size = size,\n      strides = 2,\n      padding = \"same\",\n      kernel_initializer = initializer_random_normal(),\n      use_bias = FALSE\n    )\n    self$batchnorm <- layer_batch_normalization()\n    if (self$apply_dropout) {\n      self$dropout <- layer_dropout(rate = 0.5)\n    }\n    \n    function(xs, mask = NULL, training = TRUE) {\n      \n      c(x1, x2) %<-% xs\n      x <- self$up_conv(x1) %>% self$batchnorm(training = training)\n      if (self$apply_dropout) {\n        x %>% self$dropout(training = training)\n      }\n      x %>% layer_activation(\"relu\")\n      concat <- k_concatenate(list(x, x2))\n      concat\n    }\n  })\n}\n\nx14 is upsampled to double its size, and x1 is appended as is. The axis of concatenation here is axis 4, the feature map / channels axis. x1 comes with 64 channels, x14 comes out of layer_conv_2d_transpose with 64 channels, too (because self$up7 has been defined that way). So we end up with an image of resolution 128x128 and 128 feature maps for the output of step x15.\nDownsampling, too, is factored out to its own model. Here too, the number of filters is configurable.\n\n\ndownsample <- function(filters,\n                       size,\n                       apply_batchnorm = TRUE,\n                       name = \"downsample\") {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$apply_batchnorm <- apply_batchnorm\n    self$conv1 <- layer_conv_2d(\n      filters = filters,\n      kernel_size = size,\n      strides = 2,\n      padding = 'same',\n      kernel_initializer = initializer_random_normal(0, 0.2),\n      use_bias = FALSE\n    )\n    if (self$apply_batchnorm) {\n      self$batchnorm <- layer_batch_normalization()\n    }\n    \n    function(x, mask = NULL, training = TRUE) {\n      \n      x <- self$conv1(x)\n      if (self$apply_batchnorm) {\n        x %>% self$batchnorm(training = training)\n      }\n      x %>% layer_activation_leaky_relu()\n    }\n  })\n}\n\nNow for the discriminator.\nDiscriminator\nAgain, let’s start with a birds-eye view. The discriminator receives as input both the coarse segmentation and the ground truth. Both are concatenated and processed together. Just like the generator, the discriminator is thus conditioned on the segmentation.\nWhat does the discriminator return? The output of self$last has one channel, but a spatial resolution of 30x30: We’re outputting a probability for each of 30x30 image patches (which is why the authors are calling this a PatchGAN).\nThe discriminator thus working on small image patches means it only cares about local structure, and consequently, enforces correctness in the high frequencies only. Correctness in the low frequencies is taken care of by an additional L1 component in the discriminator loss that operates over the whole image (as we’ll see below).\n\n\ndiscriminator <- function(name = \"discriminator\") {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$down1 <- disc_downsample(64, 4, FALSE)\n    self$down2 <- disc_downsample(128, 4)\n    self$down3 <- disc_downsample(256, 4)\n    self$zero_pad1 <- layer_zero_padding_2d()\n    self$conv <- layer_conv_2d(\n      filters = 512,\n      kernel_size = 4,\n      strides = 1,\n      kernel_initializer = initializer_random_normal(),\n      use_bias = FALSE\n    )\n    self$batchnorm <- layer_batch_normalization()\n    self$zero_pad2 <- layer_zero_padding_2d()\n    self$last <- layer_conv_2d(\n      filters = 1,\n      kernel_size = 4,\n      strides = 1,\n      kernel_initializer = initializer_random_normal()\n    )\n    \n    function(x, y, mask = NULL, training = TRUE) {\n      \n      x <- k_concatenate(list(x, y)) %>%            # (bs, 256, 256, channels*2)\n        self$down1(training = training) %>%         # (bs, 128, 128, 64)\n        self$down2(training = training) %>%         # (bs, 64, 64, 128)\n        self$down3(training = training) %>%         # (bs, 32, 32, 256)\n        self$zero_pad1() %>%                        # (bs, 34, 34, 256)\n        self$conv() %>%                             # (bs, 31, 31, 512)\n        self$batchnorm(training = training) %>%\n        layer_activation_leaky_relu() %>%\n        self$zero_pad2() %>%                        # (bs, 33, 33, 512)\n        self$last()                                 # (bs, 30, 30, 1)\n      x\n    }\n  })\n}\n\nAnd here’s the factored-out downsampling functionality, again providing the means to configure the number of filters.\n\n\ndisc_downsample <- function(filters,\n                            size,\n                            apply_batchnorm = TRUE,\n                            name = \"disc_downsample\") {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$apply_batchnorm <- apply_batchnorm\n    self$conv1 <- layer_conv_2d(\n      filters = filters,\n      kernel_size = size,\n      strides = 2,\n      padding = 'same',\n      kernel_initializer = initializer_random_normal(0, 0.2),\n      use_bias = FALSE\n    )\n    if (self$apply_batchnorm) {\n      self$batchnorm <- layer_batch_normalization()\n    }\n    \n    function(x, mask = NULL, training = TRUE) {\n      x <- self$conv1(x)\n      if (self$apply_batchnorm) {\n        x %>% self$batchnorm(training = training)\n      }\n      x %>% layer_activation_leaky_relu()\n    }\n  })\n}\n\nLosses and optimizer\nAs we said in the introduction, the idea of a GAN is to have the network learn the cost function. More concretely, the thing it should learn is the balance between two losses, the generator loss and the discriminator loss. Each of them individually, of course, has to be provided with a loss function, so there are still decisions to be made.\nFor the generator, two things factor into the loss: First, does the discriminator debunk my creations as fake? Second, how big is the absolute deviation of the generated image from the target? The latter factor does not have to be present in a conditional GAN, but was included by the authors to further encourage proximity to the target, and empirically found to deliver better results.\n\n\nlambda <- 100 # value chosen by the authors of the paper\ngenerator_loss <- function(disc_judgment, generated_output, target) {\n    gan_loss <- tf$losses$sigmoid_cross_entropy(\n      tf$ones_like(disc_judgment),\n      disc_judgment\n    )\n    l1_loss <- tf$reduce_mean(tf$abs(target - generated_output))\n    gan_loss + (lambda * l1_loss)\n  }\n\nThe discriminator loss looks as in a standard (un-conditional) GAN. Its first component is determined by how accurately it classifies real images as real, while the second depends on its competence in judging fake images as fake.\n\n\ndiscriminator_loss <- function(real_output, generated_output) {\n  real_loss <- tf$losses$sigmoid_cross_entropy(\n    multi_class_labels = tf$ones_like(real_output),\n    logits = real_output\n  )\n  generated_loss <- tf$losses$sigmoid_cross_entropy(\n    multi_class_labels = tf$zeros_like(generated_output),\n    logits = generated_output\n  )\n  real_loss + generated_loss\n}\n\nFor optimization, we rely on Adam for both the generator and the discriminator.\n\n\ndiscriminator_optimizer <- tf$train$AdamOptimizer(2e-4, beta1 = 0.5)\ngenerator_optimizer <- tf$train$AdamOptimizer(2e-4, beta1 = 0.5)\n\nThe game\nWe’re ready to have the generator and the discriminator play the game! Below, we use defun to compile the respective R functions into TensorFlow graphs, to speed up computations.\n\n\ngenerator <- generator()\ndiscriminator <- discriminator()\n\ngenerator$call = tf$contrib$eager$defun(generator$call)\ndiscriminator$call = tf$contrib$eager$defun(discriminator$call)\n\nWe also create a tf$train$Checkpoint object that will allow us to save and restore training weights.\n\n\ncheckpoint_dir <- \"./checkpoints_pix2pix\"\ncheckpoint_prefix <- file.path(checkpoint_dir, \"ckpt\")\ncheckpoint <- tf$train$Checkpoint(\n    generator_optimizer = generator_optimizer,\n    discriminator_optimizer = discriminator_optimizer,\n    generator = generator,\n    discriminator = discriminator\n)\n\nTraining is a loop over epochs with an inner loop over batches yielded by the dataset. As usual with eager execution, tf$GradientTape takes care of recording the forward pass and determining the gradients, while the optimizer - there are two of them in this setup - adjusts the networks’ weights.\nEvery tenth epoch, we save the weights, and tell the generator to have a go at the first example of the test set, so we can monitor network progress. See generate_images in the companion code for this functionality.\n\n\ntrain <- function(dataset, num_epochs) {\n  \n  for (epoch in 1:num_epochs) {\n    total_loss_gen <- 0\n    total_loss_disc <- 0\n    iter <- make_iterator_one_shot(train_dataset)\n    \n    until_out_of_range({\n      batch <- iterator_get_next(iter)\n      input_image <- batch[[1]]\n      target <- batch[[2]]\n      \n      with(tf$GradientTape() %as% gen_tape, {\n        with(tf$GradientTape() %as% disc_tape, {\n          \n          gen_output <- generator(input_image, training = TRUE)\n          disc_real_output <-\n            discriminator(input_image, target, training = TRUE)\n          disc_generated_output <-\n            discriminator(input_image, gen_output, training = TRUE)\n          gen_loss <-\n            generator_loss(disc_generated_output, gen_output, target)\n          disc_loss <-\n            discriminator_loss(disc_real_output, disc_generated_output)\n          total_loss_gen <- total_loss_gen + gen_loss\n          total_loss_disc <- total_loss_disc + disc_loss\n        })\n      })\n      \n      generator_gradients <- gen_tape$gradient(gen_loss,\n                                               generator$variables)\n      discriminator_gradients <- disc_tape$gradient(disc_loss,\n                                                    discriminator$variables)\n      \n      generator_optimizer$apply_gradients(transpose(list(\n        generator_gradients,\n        generator$variables\n      )))\n      discriminator_optimizer$apply_gradients(transpose(\n        list(discriminator_gradients,\n             discriminator$variables)\n      ))\n      \n    })\n    \n    cat(\"Epoch \", epoch, \"\\n\")\n    cat(\"Generator loss: \",\n        total_loss_gen$numpy() / batches_per_epoch,\n        \"\\n\")\n    cat(\"Discriminator loss: \",\n        total_loss_disc$numpy() / batches_per_epoch,\n        \"\\n\\n\")\n    \n    if (epoch %% 10 == 0) {\n      test_iter <- make_iterator_one_shot(test_dataset)\n      batch <- iterator_get_next(test_iter)\n      input <- batch[[1]]\n      target <- batch[[2]]\n      generate_images(generator, input, target, paste0(\"epoch_\", i))\n    }\n    \n    if (epoch %% 10 == 0) {\n      checkpoint$save(file_prefix = checkpoint_prefix)\n    }\n  }\n}\n\nif (!restore) {\n  train(train_dataset, 200)\n} \n\nThe results\nWhat has the network learned?\nHere’s a pretty typical result from the test set. It doesn’t look so bad.\n\nHere’s another one. Interestingly, the colors used in the fake image match the previous one’s pretty well, even though we used an additional L1 loss to penalize deviations from the original.\n\nThis pick from the test set again shows similar hues, and it might already convey an impression one gets when going through the complete test set: The network has not just learned some balance between creatively turning a coarse mask into a detailed image on the one hand, and reproducing a concrete example on the other hand. It also has internalized the main architectural style present in the dataset.\n\nFor an extreme example, take this. The mask leaves an enormous lot of freedom, while the target image is a pretty untypical (perhaps the most untypical) pick from the test set. The outcome is a structure that could represent a building, or part of a building, of specific texture and color shades.\n\nConclusion\nWhen we say the network has internalized the dominant style of the training set, is this a bad thing? (We’re used to thinking in terms of overfitting on the training set.)\nWith GANs though, one could say it all depends on the purpose. If it doesn’t fit our purpose, one thing we could try is training on several datasets at the same time.\nAgain depending on what we want to achieve, another weakness could be the lack of stochasticity in the model, as stated by the authors of the paper themselves. This will be hard to avoid when working with paired datasets as the ones used in pix2pix. An interesting alternative is CycleGAN(Zhu et al. 2017) that lets you transfer style between complete datasets without using paired instances:\nFigure from Zhu et al. (2017)Finally closing on a more technical note, you may have noticed the prominent checkerboard effects in the above fake examples. This phenomenon (and ways to address it) is superbly explained in a 2016 article on distill.pub (Odena, Dumoulin, and Olah 2016). In our case, it will mostly be due to the use of layer_conv_2d_transpose for upsampling.\nAs per the authors (Odena, Dumoulin, and Olah 2016), a better alternative is upsizing followed by padding and (standard) convolution. If you’re interested, it should be straightforward to modify the example code to use tf$image$resize_images (using ResizeMethod.NEAREST_NEIGHBOR as recommended by the authors), tf$pad and layer_conv2d.\n\n\nIsola, Phillip, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. 2016. “Image-to-Image Translation with Conditional Adversarial Networks.” CoRR abs/1611.07004. http://arxiv.org/abs/1611.07004.\n\n\nOdena, Augustus, Vincent Dumoulin, and Chris Olah. 2016. “Deconvolution and Checkerboard Artifacts.” Distill. https://doi.org/10.23915/distill.00003.\n\n\nRonneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” CoRR abs/1505.04597. http://arxiv.org/abs/1505.04597.\n\n\nZhu, Jun-Yan, Taesung Park, Phillip Isola, and Alexei A. Efros. 2017. “Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks.” CoRR abs/1703.10593. http://arxiv.org/abs/1703.10593.\n\n\n\n\n",
    "preview": "posts/2018-09-20-eager-pix2pix/images/pix2pixlosses.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 842,
    "preview_height": 536
  },
  {
    "path": "posts/2018-09-17-eager-captioning/",
    "title": "Attention-based Image Captioning with Keras",
    "description": "Image captioning is a challenging task at intersection of vision and language. Here, we demonstrate using Keras and eager execution to incorporate an attention mechanism that allows the network to concentrate on image features relevant to the current state of text generation.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-09-17",
    "categories": [
      "Natural Language Processing",
      "TensorFlow/Keras",
      "Image Recognition & Image Processing"
    ],
    "contents": "\nIn image captioning, an algorithm is given an image and tasked with producing a sensible caption. It is a challenging task for several reasons, not the least being that it involves a notion of saliency or relevance. This is why recent deep learning approaches mostly include some “attention” mechanism (sometimes even more than one) to help focusing on relevant image features.\nIn this post, we demonstrate a formulation of image captioning as an encoder-decoder problem, enhanced by spatial attention over image grid cells. The idea comes from a recent paper on Neural Image Caption Generation with Visual Attention (Xu et al. 2015), and employs the same kind of attention algorithm as detailed in our post on machine translation.\nWe’re porting Python code from a recent Google Colaboratory notebook, using Keras with TensorFlow eager execution to simplify our lives.\nPrerequisites\nThe code shown here will work with the current CRAN versions of tensorflow, keras, and tfdatasets. Check that you’re using at least version 1.9 of TensorFlow. If that isn’t the case, as of this writing, this\n\n\nlibrary(tensorflow)\ninstall_tensorflow()\n\nwill get you version 1.10.\nWhen loading libraries, please make sure you’re executing the first 4 lines in this exact order. We need to make sure we’re using the TensorFlow implementation of Keras (tf.keras in Python land), and we have to enable eager execution before using TensorFlow in any way.\nNo need to copy-paste any code snippets - you’ll find the complete code (in order necessary for execution) here: eager-image-captioning.R.\n\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\n\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nnp <- import(\"numpy\")\n\nlibrary(tfdatasets)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(glue)\nlibrary(rjson)\nlibrary(rlang)\nlibrary(dplyr)\nlibrary(magick)\n\nThe dataset\nMS-COCO (“Common Objects in Context”) is one of, perhaps the, reference dataset in image captioning (object detection and segmentation, too). We’ll be using the training images and annotations from 2014 - be warned, depending on your location, the download can take a long time.\nAfter unpacking, let’s define where the images and captions are.\n\n\nannotation_file <- \"train2014/annotations/captions_train2014.json\"\nimage_path <- \"train2014/train2014\"\n\nThe annotations are in JSON format, and there are 414113 of them! Luckily for us we didn’t have to download that many images - every image comes with 5 different captions, for better generalizability.\n\n\nannotations <- fromJSON(file = annotation_file)\nannot_captions <- annotations[[4]]\n\nnum_captions <- length(annot_captions)\n\nWe store both annotations and image paths in lists, for later loading.\n\n\nall_captions <- vector(mode = \"list\", length = num_captions)\nall_img_names <- vector(mode = \"list\", length = num_captions)\n\nfor (i in seq_len(num_captions)) {\n  caption <- paste0(\"<start> \",\n                    annot_captions[[i]][[\"caption\"]],\n                    \" <end>\"\n                    )\n  image_id <- annot_captions[[i]][[\"image_id\"]]\n  full_coco_image_path <- sprintf(\n    \"%s/COCO_train2014_%012d.jpg\",\n    image_path,\n    image_id\n  )\n  all_img_names[[i]] <- full_coco_image_path\n  all_captions[[i]] <- caption\n}\n\nDepending on your computing environment, you will for sure want to restrict the number of examples used. This post will use 30000 captioned images, chosen randomly, and set aside 20% for validation.\nBelow, we take random samples, split into training and validation parts. The companion code will also store the indices on disk, so you can pick up on verification and analysis later.\n\n\nnum_examples <- 30000\n\nrandom_sample <- sample(1:num_captions, size = num_examples)\ntrain_indices <- sample(random_sample, size = length(random_sample) * 0.8)\nvalidation_indices <- setdiff(random_sample, train_indices)\n\nsample_captions <- all_captions[random_sample]\nsample_images <- all_img_names[random_sample]\ntrain_captions <- all_captions[train_indices]\ntrain_images <- all_img_names[train_indices]\nvalidation_captions <- all_captions[validation_indices]\nvalidation_images <- all_img_names[validation_indices]\n\nInterlude\nBefore really diving into the technical stuff, let’s take a moment to reflect on this task. In typical image-related deep learning walk-throughs, we’re used to seeing well-defined problems - even if in some cases, the solution may be hard. Take, for example, the stereotypical dog vs. cat problem. Some dogs may look like cats and some cats may look like dogs, but that’s about it: All in all, in the usual world we live in, it should be a more or less binary question.\nIf, on the other hand, we ask people to describe what they see in a scene, it’s to be expected from the outset that we’ll get different answers. Still, how much consensus there is will very much depend on the concrete dataset we’re using.\nLet’s take a look at some picks from the very first 20 training items sampled randomly above.\nFigure from MS-COCO 2014Now this image does not leave much room for decision what to focus on, and received a very factual caption indeed: “There is a plate with one slice of bacon a half of orange and bread”. If the dataset were all like this, we’d think a machine learning algorithm should do pretty well here.\nPicking another one from the first 20:\nFigure from MS-COCO 2014What would be salient information to you here? The caption provided goes “A smiling little boy has a checkered shirt”. Is the look of the shirt as important as that? You might as well focus on the scenery, - or even something on a completely different level: The age of the photo, or it being an analog one.\nLet’s take a final example.\nFrom MS-COCO 2014What would you say about this scene? The official label we sampled here is “A group of people posing in a funny way for the camera”. Well …\nPlease don’t forget that for each image, the dataset includes five different captions (although our n = 30000 samples probably won’t). So this is not saying the dataset is biased - not at all. Instead, we want to point out the ambiguities and difficulties inherent in the task. Actually, given those difficulties, it’s all the more amazing that the task we’re tackling here - having a network automatically generate image captions - should be possible at all!\nNow let’s see how we can do this.\nExtract image features\nFor the encoding part of our encoder-decoder network, we will make use of InceptionV3 to extract image features. In principle, which features to extract is up to experimentation, - here we just use the last layer before the fully connected top:\n\n\nimage_model <- application_inception_v3(\n  include_top = FALSE,\n  weights = \"imagenet\"\n)\n\nFor an image size of 299x299, the output will be of size (batch_size, 8, 8, 2048), that is, we are making use of 2048 feature maps.\nInceptionV3 being a “big model”, where every pass through the model takes time, we want to precompute features in advance and store them on disk. We’ll use tfdatasets to stream images to the model. This means all our preprocessing has to employ tensorflow functions: That’s why we’re not using the more familiar image_load from keras below.\nOur custom load_image will read in, resize and preprocess the images as required for use with InceptionV3:\n\n\nload_image <- function(image_path) {\n  img <-\n    tf$read_file(image_path) %>%\n    tf$image$decode_jpeg(channels = 3) %>%\n    tf$image$resize_images(c(299L, 299L)) %>%\n    tf$keras$applications$inception_v3$preprocess_input()\n  list(img, image_path)\n}\n\nNow we’re ready to save the extracted features to disk. The (batch_size, 8, 8, 2048)-sized features will be flattened to (batch_size, 64, 2048). The latter shape is what our encoder, soon to be discussed, will receive as input.\n\n\npreencode <- unique(sample_images) %>% unlist() %>% sort()\nnum_unique <- length(preencode)\n\n# adapt this according to your system's capacities  \nbatch_size_4save <- 1\nimage_dataset <-\n  tensor_slices_dataset(preencode) %>%\n  dataset_map(load_image) %>%\n  dataset_batch(batch_size_4save)\n  \nsave_iter <- make_iterator_one_shot(image_dataset)\n  \nuntil_out_of_range({\n  \n  save_count <- save_count + batch_size_4save\n  batch_4save <- save_iter$get_next()\n  img <- batch_4save[[1]]\n  path <- batch_4save[[2]]\n  batch_features <- image_model(img)\n  batch_features <- tf$reshape(\n    batch_features,\n    list(dim(batch_features)[1], -1L, dim(batch_features)[4]\n  )\n                               )\n  for (i in 1:dim(batch_features)[1]) {\n    np$save(path[i]$numpy()$decode(\"utf-8\"),\n            batch_features[i, , ]$numpy())\n  }\n    \n})\n\nBefore we get to the encoder and decoder models though, we need to take care of the captions.\nProcessing the captions\nWe’re using keras text_tokenizer and the text processing functions texts_to_sequences and pad_sequences to transform ascii text into a matrix.\n\n\n# we will use the 5000 most frequent words only\ntop_k <- 5000\ntokenizer <- text_tokenizer(\n  num_words = top_k,\n  oov_token = \"<unk>\",\n  filters = '!\"#$%&()*+.,-/:;=?@[\\\\]^_`{|}~ ')\ntokenizer$fit_on_texts(sample_captions)\n\ntrain_captions_tokenized <-\n  tokenizer %>% texts_to_sequences(train_captions)\nvalidation_captions_tokenized <-\n  tokenizer %>% texts_to_sequences(validation_captions)\n\n# pad_sequences will use 0 to pad all captions to the same length\ntokenizer$word_index[\"<pad>\"] <- 0\n\n# create a lookup dataframe that allows us to go in both directions\nword_index_df <- data.frame(\n  word = tokenizer$word_index %>% names(),\n  index = tokenizer$word_index %>% unlist(use.names = FALSE),\n  stringsAsFactors = FALSE\n)\nword_index_df <- word_index_df %>% arrange(index)\n\ndecode_caption <- function(text) {\n  paste(map(text, function(number)\n    word_index_df %>%\n      filter(index == number) %>%\n      select(word) %>%\n      pull()),\n    collapse = \" \")\n}\n\n# pad all sequences to the same length (the maximum length, in our case)\n# could experiment with shorter padding (truncating the very longest captions)\ncaption_lengths <- map(\n  all_captions[1:num_examples],\n  function(c) str_split(c,\" \")[[1]] %>% length()\n  ) %>% unlist()\nmax_length <- fivenum(caption_lengths)[5]\n\ntrain_captions_padded <-  pad_sequences(\n  train_captions_tokenized,\n  maxlen = max_length,\n  padding = \"post\",\n  truncating = \"post\"\n)\n\nvalidation_captions_padded <- pad_sequences(\n  validation_captions_tokenized,\n  maxlen = max_length,\n  padding = \"post\",\n  truncating = \"post\"\n)\n\nLoading the data for training\nNow that we’ve taken care of pre-extracting the features and preprocessing the captions, we need a way to stream them to our captioning model. For that, we’re using tensor_slices_dataset from tfdatasets, passing in the list of paths to the images and the preprocessed captions. Loading the images is then performed as a TensorFlow graph operation (using tf$pyfunc).\nThe original Colab code also shuffles the data on every iteration. Depending on your hardware, this may take a long time, and given the size of the dataset it is not strictly necessary to get reasonable results. (The results reported below were obtained without shuffling.)\n\n\nbatch_size <- 10\nbuffer_size <- num_examples\n\nmap_func <- function(img_name, cap) {\n  p <- paste0(img_name$decode(\"utf-8\"), \".npy\")\n  img_tensor <- np$load(p)\n  img_tensor <- tf$cast(img_tensor, tf$float32)\n  list(img_tensor, cap)\n}\n\ntrain_dataset <-\n  tensor_slices_dataset(list(train_images, train_captions_padded)) %>%\n  dataset_map(\n    function(item1, item2) tf$py_func(map_func, list(item1, item2), list(tf$float32, tf$int32))\n  ) %>%\n  # optionally shuffle the dataset\n  # dataset_shuffle(buffer_size) %>%\n  dataset_batch(batch_size)\n\nCaptioning model\nThe model is basically the same as that discussed in the machine translation post. Please refer to that article for an explanation of the concepts, as well as a detailed walk-through of the tensor shapes involved at every step. Here, we provide the tensor shapes as comments in the code snippets, for quick overview/comparison.\nHowever, if you develop your own models, with eager execution you can simply insert debugging/logging statements at arbitrary places in the code - even in model definitions. So you can have a function\n\n\nmaybecat <- function(context, x) {\n  if (debugshapes) {\n    name <- enexpr(x)\n    dims <- paste0(dim(x), collapse = \" \")\n    cat(context, \": shape of \", name, \": \", dims, \"\\n\", sep = \"\")\n  }\n}\n\nAnd if you now set\n\n\ndebugshapes <- FALSE\n\nyou can trace - not only tensor shapes, but actual tensor values through your models, as shown below for the encoder. (We don’t display any debugging statements after that, but the sample code has many more.)\nEncoder\nNow it’s time to define some some sizing-related hyperparameters and housekeeping variables:\n\n\n# for encoder output\nembedding_dim <- 256\n# decoder (LSTM) capacity\ngru_units <- 512\n# for decoder output\nvocab_size <- top_k\n# number of feature maps gotten from Inception V3\nfeatures_shape <- 2048\n# shape of attention features (flattened from 8x8)\nattention_features_shape <- 64\n\nThe encoder in this case is just a fully connected layer, taking in the features extracted from Inception V3 (in flattened form, as they were written to disk), and embedding them in 256-dimensional space.\n\n\ncnn_encoder <- function(embedding_dim, name = NULL) {\n    \n  keras_model_custom(name = name, function(self) {\n      \n    self$fc <- layer_dense(units = embedding_dim, activation = \"relu\")\n      \n    function(x, mask = NULL) {\n      # input shape: (batch_size, 64, features_shape)\n      maybecat(\"encoder input\", x)\n      # shape after fc: (batch_size, 64, embedding_dim)\n      x <- self$fc(x)\n      maybecat(\"encoder output\", x)\n      x\n    }\n  })\n}\n\nAttention module\nUnlike in the machine translation post, here the attention module is separated out into its own custom model. The logic is the same though:\n\n\nattention_module <- function(gru_units, name = NULL) {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$W1 = layer_dense(units = gru_units)\n    self$W2 = layer_dense(units = gru_units)\n    self$V = layer_dense(units = 1)\n      \n    function(inputs, mask = NULL) {\n      features <- inputs[[1]]\n      hidden <- inputs[[2]]\n      # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n      # hidden shape == (batch_size, gru_units)\n      # hidden_with_time_axis shape == (batch_size, 1, gru_units)\n      hidden_with_time_axis <- k_expand_dims(hidden, axis = 2)\n        \n      # score shape == (batch_size, 64, 1)\n      score <- self$V(k_tanh(self$W1(features) + self$W2(hidden_with_time_axis)))\n      # attention_weights shape == (batch_size, 64, 1)\n      attention_weights <- k_softmax(score, axis = 2)\n      # context_vector shape after sum == (batch_size, embedding_dim)\n      context_vector <- k_sum(attention_weights * features, axis = 2)\n        \n      list(context_vector, attention_weights)\n    }\n  })\n}\n\nDecoder\nThe decoder at each time step calls the attention module with the features it got from the encoder and its last hidden state, and receives back an attention vector. The attention vector gets concatenated with the current input and further processed by a GRU and two fully connected layers, the last of which gives us the (unnormalized) probabilities for the next word in the caption.\nThe current input at each time step here is the previous word: the correct one during training (teacher forcing), the last generated one during inference.\n\n\nrnn_decoder <- function(embedding_dim, gru_units, vocab_size, name = NULL) {\n    \n  keras_model_custom(name = name, function(self) {\n      \n    self$gru_units <- gru_units\n    self$embedding <- layer_embedding(input_dim = vocab_size, \n                                      output_dim = embedding_dim)\n    self$gru <- if (tf$test$is_gpu_available()) {\n      layer_cudnn_gru(\n        units = gru_units,\n        return_sequences = TRUE,\n        return_state = TRUE,\n        recurrent_initializer = 'glorot_uniform'\n      )\n    } else {\n      layer_gru(\n        units = gru_units,\n        return_sequences = TRUE,\n        return_state = TRUE,\n        recurrent_initializer = 'glorot_uniform'\n      )\n    }\n      \n    self$fc1 <- layer_dense(units = self$gru_units)\n    self$fc2 <- layer_dense(units = vocab_size)\n      \n    self$attention <- attention_module(self$gru_units)\n      \n    function(inputs, mask = NULL) {\n      x <- inputs[[1]]\n      features <- inputs[[2]]\n      hidden <- inputs[[3]]\n        \n      c(context_vector, attention_weights) %<-% \n        self$attention(list(features, hidden))\n        \n      # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n      x <- self$embedding(x)\n        \n      # x shape after concatenation == (batch_size, 1, 2 * embedding_dim)\n      x <- k_concatenate(list(k_expand_dims(context_vector, 2), x))\n        \n      # passing the concatenated vector to the GRU\n      c(output, state) %<-% self$gru(x)\n        \n      # shape == (batch_size, 1, gru_units)\n      x <- self$fc1(output)\n        \n      # x shape == (batch_size, gru_units)\n      x <- k_reshape(x, c(-1, dim(x)[[3]]))\n        \n      # output shape == (batch_size, vocab_size)\n      x <- self$fc2(x)\n        \n      list(x, state, attention_weights)\n        \n    }\n  })\n}\n\nLoss function, and instantiating it all\nNow that we’ve defined our model (built of three custom models), we still need to actually instantiate it (being precise: the two classes we will access from outside, that is, the encoder and the decoder).\nWe also need to instantiate an optimizer (Adam will do), and define our loss function (categorical crossentropy). Note that tf$nn$sparse_softmax_cross_entropy_with_logits expects raw logits instead of softmax activations, and that we’re using the sparse variant because our labels are not one-hot-encoded.\n\n\nencoder <- cnn_encoder(embedding_dim)\ndecoder <- rnn_decoder(embedding_dim, gru_units, vocab_size)\n\noptimizer = tf$train$AdamOptimizer()\n\ncx_loss <- function(y_true, y_pred) {\n  mask <- 1 - k_cast(y_true == 0L, dtype = \"float32\")\n  loss <- tf$nn$sparse_softmax_cross_entropy_with_logits(\n    labels = y_true,\n    logits = y_pred\n  ) * mask\n  tf$reduce_mean(loss)\n}\n\nTraining\nTraining the captioning model is a time-consuming process, and you will for sure want to save the model’s weights! How does this work with eager execution?\nWe create a tf$train$Checkpoint object, passing it the objects to be saved: In our case, the encoder, the decoder, and the optimizer. Later, at the end of each epoch, we will ask it to write the respective weights to disk.\n\n\nrestore_checkpoint <- FALSE\n\ncheckpoint_dir <- \"./checkpoints_captions\"\ncheckpoint_prefix <- file.path(checkpoint_dir, \"ckpt\")\ncheckpoint <- tf$train$Checkpoint(\n  optimizer = optimizer,\n  encoder = encoder,\n  decoder = decoder\n)\n\nAs we’re just starting to train the model, restore_checkpoint is set to false. Later, restoring the weights will be as easy as\n\n\nif (restore_checkpoint) {\n  checkpoint$restore(tf$train$latest_checkpoint(checkpoint_dir))\n}\n\nThe training loop is structured just like in the machine translation case: We loop over epochs, batches, and the training targets, feeding in the correct previous word at every timestep. Again, tf$GradientTape takes care of recording the forward pass and calculating the gradients, and the optimizer applies the gradients to the model’s weights. As each epoch ends, we also save the weights.\n\n\nnum_epochs <- 20\n\nif (!restore_checkpoint) {\n  for (epoch in seq_len(num_epochs)) {\n    \n    total_loss <- 0\n    progress <- 0\n    train_iter <- make_iterator_one_shot(train_dataset)\n    \n    until_out_of_range({\n      \n      batch <- iterator_get_next(train_iter)\n      loss <- 0\n      img_tensor <- batch[[1]]\n      target_caption <- batch[[2]]\n      \n      dec_hidden <- k_zeros(c(batch_size, gru_units))\n      \n      dec_input <- k_expand_dims(\n        rep(list(word_index_df[word_index_df$word == \"<start>\", \"index\"]), \n            batch_size)\n      )\n      \n      with(tf$GradientTape() %as% tape, {\n        \n        features <- encoder(img_tensor)\n        \n        for (t in seq_len(dim(target_caption)[2] - 1)) {\n          c(preds, dec_hidden, weights) %<-%\n            decoder(list(dec_input, features, dec_hidden))\n          loss <- loss + cx_loss(target_caption[, t], preds)\n          dec_input <- k_expand_dims(target_caption[, t])\n        }\n        \n      })\n      \n      total_loss <-\n        total_loss + loss / k_cast_to_floatx(dim(target_caption)[2])\n      \n      variables <- c(encoder$variables, decoder$variables)\n      gradients <- tape$gradient(loss, variables)\n      \n      optimizer$apply_gradients(purrr::transpose(list(gradients, variables)),\n                                global_step = tf$train$get_or_create_global_step()\n      )\n    })\n    cat(paste0(\n      \"\\n\\nTotal loss (epoch): \",\n      epoch,\n      \": \",\n      (total_loss / k_cast_to_floatx(buffer_size)) %>% as.double() %>% round(4),\n      \"\\n\"\n    ))\n    \n    checkpoint$save(file_prefix = checkpoint_prefix)\n  }\n}\n\nPeeking at results\nJust like in the translation case, it’s interesting to look at model performance during training. The companion code has that functionality integrated, so you can watch model progress for yourself.\nThe basic function here is get_caption: It gets passed the path to an image, loads it, obtains its features from Inception V3, and then asks the encoder-decoder model to generate a caption. If at any point the model produces the end symbol, we stop early. Otherwise, we continue until we hit the predefined maximum length.\n\n\nget_caption <-\n  function(image) {\n    attention_matrix <-\n      matrix(0, nrow = max_length, ncol = attention_features_shape)\n    temp_input <- k_expand_dims(load_image(image)[[1]], 1)\n    img_tensor_val <- image_model(temp_input)\n    img_tensor_val <- k_reshape(\n      img_tensor_val,\n      list(dim(img_tensor_val)[1], -1, dim(img_tensor_val)[4])\n    )\n    features <- encoder(img_tensor_val)\n    \n    dec_hidden <- k_zeros(c(1, gru_units))\n    dec_input <-\n      k_expand_dims(\n        list(word_index_df[word_index_df$word == \"<start>\", \"index\"])\n      )\n    \n    result <- \"\"\n    \n    for (t in seq_len(max_length - 1)) {\n      \n      c(preds, dec_hidden, attention_weights) %<-%\n        decoder(list(dec_input, features, dec_hidden))\n      attention_weights <- k_reshape(attention_weights, c(-1))\n      attention_matrix[t,] <- attention_weights %>% as.double()\n      \n      pred_idx <- tf$multinomial(exp(preds), num_samples = 1)[1, 1] \n                    %>% as.double()\n      pred_word <-\n        word_index_df[word_index_df$index == pred_idx, \"word\"]\n      \n      if (pred_word == \"<end>\") {\n        result <-\n          paste(result, pred_word)\n        attention_matrix <-\n          attention_matrix[1:length(str_split(result, \" \")[[1]]), , \n                           drop = FALSE]\n        return (list(result, attention_matrix))\n      } else {\n        result <-\n          paste(result, pred_word)\n        dec_input <- k_expand_dims(list(pred_idx))\n      }\n    }\n    \n    list(str_trim(result), attention_matrix)\n  }\n\nWith that functionality, now let’s actually do that: peek at results while the network is learning!\nWe’ve picked 3 examples each from the training and validation sets. Here they are.\nFirst, our picks from the training set:\n\nThree picks from the training set\nLet’s see the target captions:\na herd of giraffe standing on top of a grass covered field\na view of cards driving down a street\nthe skateboarding flips his board off of the sidewalk\nInterestingly, here we also have a demonstration of how labeled datasets (like anything human) may contain errors. (The samples were not picked for that; instead, they were chosen - without too much screening - for being rather unequivocal in their visual content.)\nNow for the validation candidates.\n\nThree picks from the validation set\nand their official captions:\na left handed pitcher throwing the base ball\na woman taking a bite of a slice of pizza in a restaraunt\na woman hitting swinging a tennis racket at a tennis ball on a tennis court\n(Again, any spelling peculiarities have not been introduced by us.)\nEpoch 1\nNow, what does our network produce after the first epoch? Remember that this means, having seen each one of the 24000 training images once.\nFirst then, here are the captions for the train images:\n\na group of sheep standing in the grass\n\n\na group of cars driving down a street\n\n\na man is standing on a street\n\nNot only is the syntax correct in every case, the content isn’t that bad either!\nHow about the validation set?\n\na baseball player is playing baseball uniform is holding a baseball bat\n\n\na man is holding a table with a table with a table with a table with a table with a table with a table with a table with a table with a table with a table with a table with a table with a table\n\n\na tennis player is holding a tennis court\n\nThis certainly tells that the network has been able to generalize over - let’s not call them concepts, but mappings between visual and textual entities, say It’s true that it will have seen some of these images before, because images come with several captions. You could be more strict setting up your training and validation sets - but here, we don’t really care about objective performance scores and so, it does not really matter.\nLet’ skip directly to epoch 20, our last training epoch, and check for further improvements.\nEpoch 20\nThis is what we get for the training images:\n\na group of many tall giraffe standing next to a sheep\n\n\na view of cards and white gloves on a street\n\n\na skateboarding flips his board\n\nAnd this, for the validation images.\n\na baseball catcher and umpire hit a baseball game\n\n\na man is eating a sandwich\n\n\na female tennis player is in the court\n\nI think we might agree that this still leaves room for improvement - but then, we only trained for 20 epochs and on a very small portion of the dataset.\nIn the above code snippets, you may have noticed the decoder returning an attention_matrix - but we weren’t commenting on it. Now finally, just as in the translation example, have a look what we can make of that.\nWhere does the network look?\nWe can visualize where the network is “looking” as it generates each word by overlaying the original image and the attention matrix. This example is taken from the 4th epoch.\nHere white-ish squares indicate areas receiving stronger focus. Compared to text-to-text translation though, the mapping is inherently less straightforward - where does one “look” when producing words like “and”, “the”, or “in”?\n\nAttention over image areas\nConclusion\nIt probably goes without saying that much better results are to be expected when training on (much!) more data and for much more time.\nApart from that, there are other options, though. The concept implemented here uses spatial attention over a uniform grid, that is, the attention mechanism guides the decoder where on the grid to look next when generating a caption.\nHowever, this is not the only way, and this is not how it works with humans. A much more plausible approach is a mix of top-down and bottom-up attention. E.g., (Anderson et al. 2017) use object detection techniques to bottom-up isolate interesting objects, and an LSTM stack wherein the first LSTM computes top-down attention guided by the output word generated by the second one.\nAnother interesting approach involving attention is using a multimodal attentive translator (Liu et al. 2017), where the image features are encoded and presented in a sequence, such that we end up with sequence models both on the encoding and the decoding sides.\nAnother alternative is to add a learned topic to the information input (Zhu, Xue, and Yuan 2018), which again is a top-down feature found in human cognition.\nIf you find one of these, or yet another, approach more convincing, an eager execution implementation, in the style of the above, will likely be a sound way of implementing it.\n\n\nAnderson, Peter, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. 2017. “Bottom-up and Top-down Attention for Image Captioning and VQA.” CoRR abs/1707.07998. http://arxiv.org/abs/1707.07998.\n\n\nLiu, Chang, Fuchun Sun, Changhu Wang, Feng Wang, and Alan L. Yuille. 2017. “A Multimodal Attentive Translator for Image Captioning.” CoRR abs/1702.05658. http://arxiv.org/abs/1702.05658.\n\n\nXu, Kelvin, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.” CoRR abs/1502.03044. http://arxiv.org/abs/1502.03044.\n\n\nZhu, Zhihao, Zhan Xue, and Zejian Yuan. 2018. “A Topic-Guided Attention for Image Captioning.” CoRR abs/1807.03514v1. https://arxiv.org/abs/1807.03514v1.\n\n\n\n\n",
    "preview": "posts/2018-09-17-eager-captioning/images/showattendandtell.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 627,
    "preview_height": 269
  },
  {
    "path": "posts/2018-09-10-eager-style-transfer/",
    "title": "Neural style transfer with eager execution and Keras",
    "description": "Continuing our series on combining Keras with TensorFlow eager execution, we show how to implement neural style transfer in a straightforward way. Based on this easy-to-adapt example, you can easily perform style transfer on your own images.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-09-10",
    "categories": [
      "TensorFlow/Keras",
      "Unsupervised Learning",
      "Image Recognition & Image Processing"
    ],
    "contents": "\nHow would your summer holiday’s photos look had Edvard Munch painted them? (Perhaps it’s better not to know). Let’s take a more comforting example: How would a nice, summarly river landscape look if painted by Katsushika Hokusai?\nStyle transfer on images is not new, but got a boost when Gatys, Ecker, and Bethge(Gatys, Ecker, and Bethge 2015) showed how to successfully do it with deep learning. The main idea is straightforward: Create a hybrid that is a tradeoff between the content image we want to manipulate, and a style image we want to imitate, by optimizing for maximal resemblance to both at the same time.\nIf you’ve read the chapter on neural style transfer from Deep Learning with R, you may recognize some of the code snippets that follow. However, there is an important difference: This post uses TensorFlow Eager Execution, allowing for an imperative way of coding that makes it easy to map concepts to code. Just like previous posts on eager execution on this blog, this is a port of a Google Colaboratory notebook that performs the same task in Python.\nAs usual, please make sure you have the required package versions installed. And no need to copy the snippets - you’ll find the complete code among the Keras examples.\nPrerequisites\nThe code in this post depends on the most recent versions of several of the TensorFlow R packages. You can install these packages as follows:\n\ninstall.packages(c(\"tensorflow\", \"keras\", \"tfdatasets\"))\nYou should also be sure that you are running the very latest version of TensorFlow (v1.10), which you can install like so:\n\nlibrary(tensorflow)\ninstall_tensorflow()\nThere are additional requirements for using TensorFlow eager execution. First, we need to call tfe_enable_eager_execution() right at the beginning of the program. Second, we need to use the implementation of Keras included in TensorFlow, rather than the base Keras implementation.\n\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\n\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nlibrary(purrr)\nlibrary(glue)\n\nPrerequisites behind us, let’s get started!\nInput images\nHere is our content image - replace by an image of your own:\n\n\n# If you have enough memory on your GPU, no need to load the images\n# at such small size.\n# This is the size I found working for a 4G GPU.\nimg_shape <- c(128, 128, 3)\n\ncontent_path <- \"isar.jpg\"\n\ncontent_image <-  image_load(content_path, target_size = img_shape[1:2])\ncontent_image %>% \n  image_to_array() %>%\n  `/`(., 255) %>%\n  as.raster() %>%\n  plot()\n\n\nAnd here’s the style model, Hokusai’s The Great Wave off Kanagawa, which you can download from Wikimedia Commons:\n\n\nstyle_path <- \"The_Great_Wave_off_Kanagawa.jpg\"\n\nstyle_image <-  image_load(content_path, target_size = img_shape[1:2])\nstyle_image %>% \n  image_to_array() %>%\n  `/`(., 255) %>%\n  as.raster() %>%\n  plot()\n\n\nWe create a wrapper that loads and preprocesses the input images for us. As we will be working with VGG19, a network that has been trained on ImageNet, we need to transform our input images in the same way that was used training it. Later, we’ll apply the inverse transformation to our combination image before displaying it.\n\n\nload_and_preprocess_image <- function(path) {\n  img <- image_load(path, target_size = img_shape[1:2]) %>%\n    image_to_array() %>%\n    k_expand_dims(axis = 1) %>%\n    imagenet_preprocess_input()\n}\n\ndeprocess_image <- function(x) {\n  x <- x[1, , ,]\n  # Remove zero-center by mean pixel\n  x[, , 1] <- x[, , 1] + 103.939\n  x[, , 2] <- x[, , 2] + 116.779\n  x[, , 3] <- x[, , 3] + 123.68\n  # 'BGR'->'RGB'\n  x <- x[, , c(3, 2, 1)]\n  x[x > 255] <- 255\n  x[x < 0] <- 0\n  x[] <- as.integer(x) / 255\n  x\n}\n\nSetting the scene\nWe are going to use a neural network, but we won’t be training it. Neural style transfer is a bit uncommon in that we don’t optimize the network’s weights, but back propagate the loss to the input layer (the image), in order to move it in the desired direction.\nWe will be interested in two kinds of outputs from the network, corresponding to our two goals. Firstly, we want to keep the combination image similar to the content image, on a high level. In a convnet, upper layers map to more holistic concepts, so we are picking a layer high up in the graph to compare outputs from the source and the combination.\nSecondly, the generated image should “look like” the style image. Style corresponds to lower level features like texture, shapes, strokes… So to compare the combination against the style example, we choose a set of lower level conv blocks for comparison and aggregate the results.\n\n\ncontent_layers <- c(\"block5_conv2\")\nstyle_layers <- c(\"block1_conv1\",\n                 \"block2_conv1\",\n                 \"block3_conv1\",\n                 \"block4_conv1\",\n                 \"block5_conv1\")\n\nnum_content_layers <- length(content_layers)\nnum_style_layers <- length(style_layers)\n\nget_model <- function() {\n  vgg <- application_vgg19(include_top = FALSE, weights = \"imagenet\")\n  vgg$trainable <- FALSE\n  style_outputs <- map(style_layers, function(layer) vgg$get_layer(layer)$output)\n  content_outputs <- map(content_layers, function(layer) vgg$get_layer(layer)$output)\n  model_outputs <- c(style_outputs, content_outputs)\n  keras_model(vgg$input, model_outputs)\n}\n\nLosses\nWhen optimizing the input image, we will consider three types of losses. Firstly, the content loss: How different is the combination image from the source? Here, we’re using the sum of the squared errors for comparison.\n\n\ncontent_loss <- function(content_image, target) {\n  k_sum(k_square(target - content_image))\n}\n\nOur second concern is having the styles match as closely as possible. Style is commonly operationalized as the Gram matrix of flattened feature maps in a layer. We thus assume that style is related to how maps in a layer correlate with other.\nWe therefore compute the Gram matrices of the layers we’re interested in (defined above), for the source image as well as the optimization candidate, and compare them, again using the sum of squared errors.\n\n\ngram_matrix <- function(x) {\n  features <- k_batch_flatten(k_permute_dimensions(x, c(3, 1, 2)))\n  gram <- k_dot(features, k_transpose(features))\n  gram\n}\n\nstyle_loss <- function(gram_target, combination) {\n  gram_comb <- gram_matrix(combination)\n  k_sum(k_square(gram_target - gram_comb)) /\n    (4 * (img_shape[3] ^ 2) * (img_shape[1] * img_shape[2]) ^ 2)\n}\n\nThirdly, we don’t want the combination image to look overly pixelated, thus we’re adding in a regularization component, the total variation in the image:\n\n\ntotal_variation_loss <- function(image) {\n  y_ij  <- image[1:(img_shape[1] - 1L), 1:(img_shape[2] - 1L),]\n  y_i1j <- image[2:(img_shape[1]), 1:(img_shape[2] - 1L),]\n  y_ij1 <- image[1:(img_shape[1] - 1L), 2:(img_shape[2]),]\n  a <- k_square(y_ij - y_i1j)\n  b <- k_square(y_ij - y_ij1)\n  k_sum(k_pow(a + b, 1.25))\n}\n\nThe tricky thing is how to combine these losses. We’ve reached acceptable results with the following weightings, but feel free to play around as you see fit:\n\n\ncontent_weight <- 100\nstyle_weight <- 0.8\ntotal_variation_weight <- 0.01\n\nGet model outputs for the content and style images\nWe need the model’s output for the content and style images, but here it suffices to do this just once. We concatenate both images along the batch dimension, pass that input to the model, and get back a list of outputs, where every element of the list is a 4-d tensor. For the style image, we’re interested in the style outputs at batch position 1, whereas for the content image, we need the content output at batch position 2.\nIn the below comments, please note that the sizes of dimensions 2 and 3 will differ if you’re loading images at a different size.\n\n\nget_feature_representations <-\n  function(model, content_path, style_path) {\n    \n    # dim == (1, 128, 128, 3)\n    style_image <-\n      load_and_process_image(style_path) %>% k_cast(\"float32\")\n    # dim == (1, 128, 128, 3)\n    content_image <-\n      load_and_process_image(content_path) %>% k_cast(\"float32\")\n    # dim == (2, 128, 128, 3)\n    stack_images <- k_concatenate(list(style_image, content_image), axis = 1)\n    \n    # length(model_outputs) == 6\n    # dim(model_outputs[[1]]) = (2, 128, 128, 64)\n    # dim(model_outputs[[6]]) = (2, 8, 8, 512)\n    model_outputs <- model(stack_images)\n    \n    style_features <- \n      model_outputs[1:num_style_layers] %>%\n      map(function(batch) batch[1, , , ])\n    content_features <- \n      model_outputs[(num_style_layers + 1):(num_style_layers + num_content_layers)] %>%\n      map(function(batch) batch[2, , , ])\n    \n    list(style_features, content_features)\n  }\n\nComputing the losses\nOn every iteration, we need to pass the combination image through the model, obtain the style and content outputs, and compute the losses. Again, the code is extensively commented with tensor sizes for easy verification, but please keep in mind that the exact numbers presuppose you’re working with 128x128 images.\n\n\ncompute_loss <-\n  function(model, loss_weights, init_image, gram_style_features, content_features) {\n    \n    c(style_weight, content_weight) %<-% loss_weights\n    model_outputs <- model(init_image)\n    style_output_features <- model_outputs[1:num_style_layers]\n    content_output_features <-\n      model_outputs[(num_style_layers + 1):(num_style_layers + num_content_layers)]\n    \n    # style loss\n    weight_per_style_layer <- 1 / num_style_layers\n    style_score <- 0\n    # dim(style_zip[[5]][[1]]) == (512, 512)\n    style_zip <- transpose(list(gram_style_features, style_output_features))\n    for (l in 1:length(style_zip)) {\n      # for l == 1:\n      # dim(target_style) == (64, 64)\n      # dim(comb_style) == (1, 128, 128, 64)\n      c(target_style, comb_style) %<-% style_zip[[l]]\n      style_score <- style_score + weight_per_style_layer * \n        style_loss(target_style, comb_style[1, , , ])\n    }\n    \n    # content loss\n    weight_per_content_layer <- 1 / num_content_layers\n    content_score <- 0\n    content_zip <- transpose(list(content_features, content_output_features))\n    for (l in 1:length(content_zip)) {\n      # dim(comb_content) ==  (1, 8, 8, 512)\n      # dim(target_content) == (8, 8, 512)\n      c(target_content, comb_content) %<-% content_zip[[l]]\n      content_score <- content_score + weight_per_content_layer *\n        content_loss(comb_content[1, , , ], target_content)\n    }\n    \n    # total variation loss\n    variation_loss <- total_variation_loss(init_image[1, , ,])\n    \n    style_score <- style_score * style_weight\n    content_score <- content_score * content_weight\n    variation_score <- variation_loss * total_variation_weight\n    \n    loss <- style_score + content_score + variation_score\n    list(loss, style_score, content_score, variation_score)\n  }\n\nComputing the gradients\nAs soon as we have the losses, obtaining the gradients of the overall loss with respect to the input image is just a matter of calling tape$gradient on the GradientTape. Note that the nested call to compute_loss, and thus the call of the model on our combination image, happens inside the GradientTape context.\n\n\ncompute_grads <- \n  function(model, loss_weights, init_image, gram_style_features, content_features) {\n    with(tf$GradientTape() %as% tape, {\n      scores <-\n        compute_loss(model,\n                     loss_weights,\n                     init_image,\n                     gram_style_features,\n                     content_features)\n    })\n    total_loss <- scores[[1]]\n    list(tape$gradient(total_loss, init_image), scores)\n  }\n\nTraining phase\nNow it’s time to train! While the natural continuation of this sentence would have been “… the model”, the model we’re training here is not VGG19 (that one we’re just using as a tool), but a minimal setup of just:\na Variable that holds our to-be-optimized image\nthe loss functions we defined above\nan optimizer that will apply the calculated gradients to the image variable (tf$train$AdamOptimizer)\nBelow, we get the style features (of the style image) and the content feature (of the content image) just once, then iterate over the optimization process, saving the output every 100 iterations.\nIn contrast to the original article and the Deep Learning with R book, but following the Google notebook instead, we’re not using L-BFGS for optimization, but Adam, as our goal here is to provide a concise introduction to eager execution. However, you could plug in another optimization method if you wanted, replacing optimizer$apply_gradients(list(tuple(grads, init_image))) by an algorithm of your choice (and of course, assigning the result of the optimization to the Variable holding the image).\n\n\nrun_style_transfer <- function(content_path, style_path) {\n  model <- get_model()\n  walk(model$layers, function(layer) layer$trainable = FALSE)\n  \n  c(style_features, content_features) %<-% \n    get_feature_representations(model, content_path, style_path)\n  # dim(gram_style_features[[1]]) == (64, 64)\n  gram_style_features <- map(style_features, function(feature) gram_matrix(feature))\n  \n  init_image <- load_and_process_image(content_path)\n  init_image <- tf$contrib$eager$Variable(init_image, dtype = \"float32\")\n  \n  optimizer <- tf$train$AdamOptimizer(learning_rate = 1,\n                                      beta1 = 0.99,\n                                      epsilon = 1e-1)\n  \n  c(best_loss, best_image) %<-% list(Inf, NULL)\n  loss_weights <- list(style_weight, content_weight)\n  \n  start_time <- Sys.time()\n  global_start <- Sys.time()\n  \n  norm_means <- c(103.939, 116.779, 123.68)\n  min_vals <- -norm_means\n  max_vals <- 255 - norm_means\n  \n  for (i in seq_len(num_iterations)) {\n    # dim(grads) == (1, 128, 128, 3)\n    c(grads, all_losses) %<-% compute_grads(model,\n                                            loss_weights,\n                                            init_image,\n                                            gram_style_features,\n                                            content_features)\n    c(loss, style_score, content_score, variation_score) %<-% all_losses\n    optimizer$apply_gradients(list(tuple(grads, init_image)))\n    clipped <- tf$clip_by_value(init_image, min_vals, max_vals)\n    init_image$assign(clipped)\n    \n    end_time <- Sys.time()\n    \n    if (k_cast_to_floatx(loss) < best_loss) {\n      best_loss <- k_cast_to_floatx(loss)\n      best_image <- init_image\n    }\n    \n    if (i %% 50 == 0) {\n      glue(\"Iteration: {i}\") %>% print()\n      glue(\n        \"Total loss: {k_cast_to_floatx(loss)},\n        style loss: {k_cast_to_floatx(style_score)},\n        content loss: {k_cast_to_floatx(content_score)},\n        total variation loss: {k_cast_to_floatx(variation_score)},\n        time for 1 iteration: {(Sys.time() - start_time) %>% round(2)}\"\n      ) %>% print()\n      \n      if (i %% 100 == 0) {\n        png(paste0(\"style_epoch_\", i, \".png\"))\n        plot_image <- best_image$numpy()\n        plot_image <- deprocess_image(plot_image)\n        plot(as.raster(plot_image), main = glue(\"Iteration {i}\"))\n        dev.off()\n      }\n    }\n  }\n  \n  glue(\"Total time: {Sys.time() - global_start} seconds\") %>% print()\n  list(best_image, best_loss)\n}\n\nReady to run\nNow, we’re ready to start the process:\n\n\nc(best_image, best_loss) %<-% run_style_transfer(content_path, style_path)\n\nIn our case, results didn’t change much after ~ iteration 1000, and this is how our river landscape was looking:\n\n\n\n… definitely more inviting than had it been painted by Edvard Munch!\nConclusion\nWith neural style transfer, some fiddling around may be needed until you get the result you want. But as our example shows, this doesn’t mean the code has to be complicated. Additionally to being easy to grasp, eager execution also lets you add debugging output, and step through the code line-by-line to check on tensor shapes. Until next time in our eager execution series!\n\n\nGatys, Leon A., Alexander S. Ecker, and Matthias Bethge. 2015. “A Neural Algorithm of Artistic Style.” CoRR abs/1508.06576. http://arxiv.org/abs/1508.06576.\n\n\n\n\n",
    "preview": "posts/2018-09-10-eager-style-transfer/images/preview.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 344,
    "preview_height": 231
  },
  {
    "path": "posts/2018-09-07-getting-started/",
    "title": "Getting started with deep learning in R",
    "description": "Many fields are benefiting from the use of deep learning, and with the R keras, tensorflow and related packages, you can now easily do state of the art deep learning in R. In this post, we want to give some orientation as to how to best get started.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-09-07",
    "categories": [
      "TensorFlow/Keras"
    ],
    "contents": "\nThere are good reasons to get into deep learning: Deep learning has been outperforming the respective “classical” techniques in areas like image recognition and natural language processing for a while now, and it has the potential to bring interesting insights even to the analysis of tabular data. For many R users interested in deep learning, the hurdle is not so much the mathematical prerequisites (as many have a background in statistics or empirical sciences), but rather how to get started in an efficient way.\nThis post will give an overview of some materials that should prove useful. In the case that you don’t have that background in statistics or similar, we will also present a few helpful resources to catch up with “the math”.\nKeras tutorials\nThe easiest way to get started is using the Keras API. It is a high-level, declarative (in feel) way of specifying a model, training and testing it, originally developed in Python by Francois Chollet and ported to R by JJ Allaire.\nCheck out the tutorials on the Keras website: They introduce basic tasks like classification and regression, as well as basic workflow elements like saving and restoring models, or assessing model performance.\nBasic classification gets you started doing image classification using the Fashion MNIST dataset.\nText classification shows how to do sentiment analysis on movie reviews, and includes the important topic of how to preprocess text for deep learning.\nBasic regression demonstrates the task of predicting a continuous variable by example of the famous Boston housing dataset that ships with Keras.\nOverfitting and underfitting explains how you can assess if your model is under- or over-fitting, and what remedies to take.\nLast but not least, Save and restore models shows how to save checkpoints during and after training, so you don’t lose the fruit of the network’s labor.\nOnce you’ve seen the basics, the website also has more advanced information on implementing custom logic, monitoring and tuning, as well as using and adapting pre-trained models.\nVideos and book\nIf you want a bit more conceptual background, the Deep Learning with R in motion video series provides a nice introduction to basic concepts of machine learning and deep learning, including things often taken for granted, such as derivatives and gradients.\n\nThe first 2 components of the video series (Getting Started and the MNIST Case Study) are free. The remainder of the videos introduce different neural network architectures by way of detailed case studies.\nThe series is a companion to the Deep Learning with R book by Francois Chollet and JJ Allaire. Like the videos, the book has excellent, high-level explanations of deep learning concepts. At the same time, it contains lots of ready-to-use code, presenting examples for all the major architectures and use cases (including fancy stuff like variational autoencoders and GANs).\n\nInspiration\nIf you’re not pursuing a specific goal, but in general curious about what can be done with deep learning, a good place to follow is the TensorFlow for R Blog. There, you’ll find applications of deep learning to business as well as scientific tasks, as well as technical expositions and introductions to new features.\nIn addition, the TensorFlow for R Gallery highlights several case studies that have proven especially useful for getting started in various areas of application.\nReality\nOnce the ideas are there, realization should follow, and for most of us the question will be: Where can I actually train that model? As soon as real-world-size images are involved, or other kinds of higher-dimensional data, you’ll need a modern, high performance GPU so training on your laptop won’t be an option any more.\nThere are a few different ways you can train in the cloud:\nRStudio provides Amazon EC2 AMIs for cloud GPU instances. The AMI has both RStudio Server and the R TensorFlow package suite preinstalled.\nYou can also try out Paperspace cloud GPU desktops (again with the RStudio and the R TensorFlow package suite preinstalled).\nThe cloudml package provides an interface to the Google Cloud Machine Learning engine, which makes it easy to submit batch GPU training jobs to CloudML.\nMore background\nIf you don’t have a very “mathy” background, you might feel that you’d like to supplement the concepts-focused approach from Deep Learning with R with a bit more low-level basics (just as some people feel the need to know at least a bit of C or Assembler when learning a high-level language).\nPersonal recommendations for such cases would include Andrew Ng’s deep learning specialization on Coursera (videos are free to watch), and the book(s) and recorded lectures on linear algebra by Gilbert Strang.\nOf course, the ultimate reference on deep learning, as of today, is the Deep Learning textbook by Ian Goodfellow, Yoshua Bengio and Aaron Courville. The book covers everything from background in linear algebra, probability theory and optimization via basic architectures such as CNNs or RNNs, on to unsupervised models on the frontier of the very latest research.\nGetting help\nLast not least, should you encounter problems with the software (or with mapping your task to runnable code), a good idea is to create a GitHub issue in the respective repository, e.g., rstudio/keras.\nBest of luck for your deep learning journey with R!\n\n\n",
    "preview": "posts/2018-09-07-getting-started/images/digits.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 557,
    "preview_height": 317
  },
  {
    "path": "posts/2018-08-26-eager-dcgan/",
    "title": "Generating images with Keras and TensorFlow eager execution",
    "description": "Generative adversarial networks (GANs) are a popular deep learning approach to generating new entities (often but not always images). We show how to code them using Keras and TensorFlow eager execution.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-08-26",
    "categories": [
      "TensorFlow/Keras",
      "Unsupervised Learning",
      "Image Recognition & Image Processing"
    ],
    "contents": "\nThe recent announcement of TensorFlow 2.0 names eager execution as the number one central feature of the new major version. What does this mean for R users? As demonstrated in our recent post on neural machine translation, you can use eager execution from R now already, in combination with Keras custom models and the datasets API. It’s good to know you can use it - but why should you? And in which cases?\nIn this and a few upcoming posts, we want to show how eager execution can make developing models a lot easier. The degree of simplication will depend on the task - and just how much easier you’ll find the new way might also depend on your experience using the functional API to model more complex relationships. Even if you think that GANs, encoder-decoder architectures, or neural style transfer didn’t pose any problems before the advent of eager execution, you might find that the alternative is a better fit to how we humans mentally picture problems.\nFor this post, we are porting code from a recent Google Colaboratory notebook implementing the DCGAN architecture.(Radford, Metz, and Chintala 2015) No prior knowledge of GANs is required - we’ll keep this post practical (no maths) and focus on how to achieve your goal, mapping a simple and vivid concept into an astonishingly small number of lines of code.\nAs in the post on machine translation with attention, we first have to cover some prerequisites. By the way, no need to copy out the code snippets - you’ll find the complete code in eager_dcgan.R).\nPrerequisites\nThe code in this post depends on the newest CRAN versions of several of the TensorFlow R packages. You can install these packages as follows:\n\ninstall.packages(c(\"tensorflow\", \"keras\", \"tfdatasets\"))\nYou should also be sure that you are running the very latest version of TensorFlow (v1.10), which you can install like so:\n\nlibrary(tensorflow)\ninstall_tensorflow()\nThere are additional requirements for using TensorFlow eager execution. First, we need to call tfe_enable_eager_execution() right at the beginning of the program. Second, we need to use the implementation of Keras included in TensorFlow, rather than the base Keras implementation.\nWe’ll also use the tfdatasets package for our input pipeline. So we end up with the following preamble to set things up:\n\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\n\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nlibrary(tfdatasets)\n\nThat’s it. Let’s get started.\nSo what’s a GAN?\nGAN stands for Generative Adversarial Network(Goodfellow et al. 2014). It is a setup of two agents, the generator and the discriminator, that act against each other (thus, adversarial). It is generative because the goal is to generate output (as opposed to, say, classification or regression).\nIn human learning, feedback - direct or indirect - plays a central role. Say we wanted to forge a banknote (as long as those still exist). Assuming we can get away with unsuccessful trials, we would get better and better at forgery over time. Optimizing our technique, we would end up rich. This concept of optimizing from feedback is embodied in the first of the two agents, the generator. It gets its feedback from the discriminator, in an upside-down way: If it can fool the discriminator, making it believe that the banknote was real, all is fine; if the discriminator notices the fake, it has to do things differently. For a neural network, that means it has to update its weights.\nHow does the discriminator know what is real and what is fake? It too has to be trained, on real banknotes (or whatever the kind of objects involved) and the fake ones produced by the generator. So the complete setup is two agents competing, one striving to generate realistic-looking fake objects, and the other, to disavow the deception. The purpose of training is to have both evolve and get better, in turn causing the other to get better, too.\nIn this system, there is no objective minimum to the loss function: We want both components to learn and getter better “in lockstep”, instead of one winning out over the other. This makes optimization difficult. In practice therefore, tuning a GAN can seem more like alchemy than like science, and it often makes sense to lean on practices and “tricks” reported by others.\nIn this example, just like in the Google notebook we’re porting, the goal is to generate MNIST digits. While that may not sound like the most exciting task one could imagine, it lets us focus on the mechanics, and allows us to keep computation and memory requirements (comparatively) low.\nLet’s load the data (training set needed only) and then, look at the first actor in our drama, the generator.\nTraining data\n\n\nmnist <- dataset_mnist()\nc(train_images, train_labels) %<-% mnist$train\n\ntrain_images <- train_images %>% \n  k_expand_dims() %>%\n  k_cast(dtype = \"float32\")\n\n# normalize images to [-1, 1] because the generator uses tanh activation\ntrain_images <- (train_images - 127.5) / 127.5\n\nOur complete training set will be streamed once per epoch:\n\n\nbuffer_size <- 60000\nbatch_size <- 256\nbatches_per_epoch <- (buffer_size / batch_size) %>% round()\n\ntrain_dataset <- tensor_slices_dataset(train_images) %>%\n  dataset_shuffle(buffer_size) %>%\n  dataset_batch(batch_size)\n\nThis input will be fed to the discriminator only.\nGenerator\nBoth generator and discriminator are Keras custom models. In contrast to custom layers, custom models allow you to construct models as independent units, complete with custom forward pass logic, backprop and optimization. The model-generating function defines the layers the model (self) wants assigned, and returns the function that implements the forward pass.\nAs we will soon see, the generator gets passed vectors of random noise for input. This vector is transformed to 3d (height, width, channels) and then, successively upsampled to the required output size of (28,28,3).\n\n\ngenerator <-\n  function(name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      \n      self$fc1 <- layer_dense(units = 7 * 7 * 64, use_bias = FALSE)\n      self$batchnorm1 <- layer_batch_normalization()\n      self$leaky_relu1 <- layer_activation_leaky_relu()\n      self$conv1 <-\n        layer_conv_2d_transpose(\n          filters = 64,\n          kernel_size = c(5, 5),\n          strides = c(1, 1),\n          padding = \"same\",\n          use_bias = FALSE\n        )\n      self$batchnorm2 <- layer_batch_normalization()\n      self$leaky_relu2 <- layer_activation_leaky_relu()\n      self$conv2 <-\n        layer_conv_2d_transpose(\n          filters = 32,\n          kernel_size = c(5, 5),\n          strides = c(2, 2),\n          padding = \"same\",\n          use_bias = FALSE\n        )\n      self$batchnorm3 <- layer_batch_normalization()\n      self$leaky_relu3 <- layer_activation_leaky_relu()\n      self$conv3 <-\n        layer_conv_2d_transpose(\n          filters = 1,\n          kernel_size = c(5, 5),\n          strides = c(2, 2),\n          padding = \"same\",\n          use_bias = FALSE,\n          activation = \"tanh\"\n        )\n      \n      function(inputs, mask = NULL, training = TRUE) {\n        self$fc1(inputs) %>%\n          self$batchnorm1(training = training) %>%\n          self$leaky_relu1() %>%\n          k_reshape(shape = c(-1, 7, 7, 64)) %>%\n          self$conv1() %>%\n          self$batchnorm2(training = training) %>%\n          self$leaky_relu2() %>%\n          self$conv2() %>%\n          self$batchnorm3(training = training) %>%\n          self$leaky_relu3() %>%\n          self$conv3()\n      }\n    })\n  }\n\nDiscriminator\nThe discriminator is just a pretty normal convolutional network outputting a score. Here, usage of “score” instead of “probability” is on purpose: If you look at the last layer, it is fully connected, of size 1 but lacking the usual sigmoid activation. This is because unlike Keras’ loss_binary_crossentropy, the loss function we’ll be using here - tf$losses$sigmoid_cross_entropy - works with the raw logits, not the outputs of the sigmoid.\n\n\ndiscriminator <-\n  function(name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      \n      self$conv1 <- layer_conv_2d(\n        filters = 64,\n        kernel_size = c(5, 5),\n        strides = c(2, 2),\n        padding = \"same\"\n      )\n      self$leaky_relu1 <- layer_activation_leaky_relu()\n      self$dropout <- layer_dropout(rate = 0.3)\n      self$conv2 <-\n        layer_conv_2d(\n          filters = 128,\n          kernel_size = c(5, 5),\n          strides = c(2, 2),\n          padding = \"same\"\n        )\n      self$leaky_relu2 <- layer_activation_leaky_relu()\n      self$flatten <- layer_flatten()\n      self$fc1 <- layer_dense(units = 1)\n      \n      function(inputs, mask = NULL, training = TRUE) {\n        inputs %>% self$conv1() %>%\n          self$leaky_relu1() %>%\n          self$dropout(training = training) %>%\n          self$conv2() %>%\n          self$leaky_relu2() %>%\n          self$flatten() %>%\n          self$fc1()\n      }\n    })\n  }\n\nSetting the scene\nBefore we can start training, we need to create the usual components of a deep learning setup: the model (or models, in this case), the loss function(s), and the optimizer(s).\nModel creation is just a function call, with a little extra on top:\n\n\ngenerator <- generator()\ndiscriminator <- discriminator()\n\n# https://www.tensorflow.org/api_docs/python/tf/contrib/eager/defun\ngenerator$call = tf$contrib$eager$defun(generator$call)\ndiscriminator$call = tf$contrib$eager$defun(discriminator$call)\n\ndefun compiles an R function (once per different combination of argument shapes and non-tensor objects values)) into a TensorFlow graph, and is used to speed up computations. This comes with side effects and possibly unexpected behavior - please consult the documentation for the details. Here, we were mainly curious in how much of a speedup we might notice when using this from R - in our example, it resulted in a speedup of 130%.\nOn to the losses. Discriminator loss consists of two parts: Does it correctly identify real images as real, and does it correctly spot fake images as fake. Here real_output and generated_output contain the logits returned from the discriminator - that is, its judgment of whether the respective images are fake or real.\n\n\ndiscriminator_loss <- function(real_output, generated_output) {\n  real_loss <- tf$losses$sigmoid_cross_entropy(\n    multi_class_labels = k_ones_like(real_output),\n    logits = real_output)\n  generated_loss <- tf$losses$sigmoid_cross_entropy(\n    multi_class_labels = k_zeros_like(generated_output),\n    logits = generated_output)\n  real_loss + generated_loss\n}\n\nGenerator loss depends on how the discriminator judged its creations: It would hope for them all to be seen as real.\n\n\ngenerator_loss <- function(generated_output) {\n  tf$losses$sigmoid_cross_entropy(\n    tf$ones_like(generated_output),\n    generated_output)\n}\n\nNow we still need to define optimizers, one for each model.\n\n\ndiscriminator_optimizer <- tf$train$AdamOptimizer(1e-4)\ngenerator_optimizer <- tf$train$AdamOptimizer(1e-4)\n\nTraining loop\nThere are two models, two loss functions and two optimizers, but there is just one training loop, as both models depend on each other. The training loop will be over MNIST images streamed in batches, but we still need input to the generator - a random vector of size 100, in this case.\n\n\nnoise_dim <- 100\n\nLet’s take the training loop step by step. There will be an outer and an inner loop, one over epochs and one over batches. At the start of each epoch, we create a fresh iterator over the dataset:\n\n\nfor (epoch in seq_len(num_epochs)) {\n  start <- Sys.time()\n  total_loss_gen <- 0\n  total_loss_disc <- 0\n  iter <- make_iterator_one_shot(train_dataset)\n\nNow for every batch we obtain from the iterator, we are calling the generator and having it generate images from random noise. Then, we’re calling the dicriminator on real images as well as the fake images just generated. For the discriminator, its relative outputs are directly fed into the loss function. For the generator, its loss will depend on how the discriminator judged its creations:\n\n\nuntil_out_of_range({\n  batch <- iterator_get_next(iter)\n  noise <- k_random_normal(c(batch_size, noise_dim))\n  with(tf$GradientTape() %as% gen_tape, { with(tf$GradientTape() %as% disc_tape, {\n    generated_images <- generator(noise)\n    disc_real_output <- discriminator(batch, training = TRUE)\n    disc_generated_output <-\n       discriminator(generated_images, training = TRUE)\n    gen_loss <- generator_loss(disc_generated_output)\n    disc_loss <- discriminator_loss(disc_real_output, disc_generated_output)\n  }) })\n\nNote that all model calls happen inside tf$GradientTape contexts. This is so the forward passes can be recorded and “played back” to back propagate the losses through the network.\nObtain the gradients of the losses to the respective models’ variables (tape$gradient) and have the optimizers apply them to the models’ weights (optimizer$apply_gradients):\n\n\ngradients_of_generator <-\n  gen_tape$gradient(gen_loss, generator$variables)\ngradients_of_discriminator <-\n  disc_tape$gradient(disc_loss, discriminator$variables)\n      \ngenerator_optimizer$apply_gradients(purrr::transpose(\n  list(gradients_of_generator, generator$variables)\n))\ndiscriminator_optimizer$apply_gradients(purrr::transpose(\n  list(gradients_of_discriminator, discriminator$variables)\n))\n      \ntotal_loss_gen <- total_loss_gen + gen_loss\ntotal_loss_disc <- total_loss_disc + disc_loss\n\nThis ends the loop over batches. Finish off the loop over epochs displaying current losses and saving a few of the generator’s artwork:\n\n\ncat(\"Time for epoch \", epoch, \": \", Sys.time() - start, \"\\n\")\ncat(\"Generator loss: \", total_loss_gen$numpy() / batches_per_epoch, \"\\n\")\ncat(\"Discriminator loss: \", total_loss_disc$numpy() / batches_per_epoch, \"\\n\\n\")\nif (epoch %% 10 == 0)\n  generate_and_save_images(generator,\n                           epoch,\n                           random_vector_for_generation)\n\nHere’s the training loop again, shown as a whole - even including the lines for reporting on progress, it is remarkably concise, and allows for a quick grasp of what is going on:\n\n\ntrain <- function(dataset, epochs, noise_dim) {\n  for (epoch in seq_len(num_epochs)) {\n    start <- Sys.time()\n    total_loss_gen <- 0\n    total_loss_disc <- 0\n    iter <- make_iterator_one_shot(train_dataset)\n    \n    until_out_of_range({\n      batch <- iterator_get_next(iter)\n      noise <- k_random_normal(c(batch_size, noise_dim))\n      with(tf$GradientTape() %as% gen_tape, { with(tf$GradientTape() %as% disc_tape, {\n        generated_images <- generator(noise)\n        disc_real_output <- discriminator(batch, training = TRUE)\n        disc_generated_output <-\n          discriminator(generated_images, training = TRUE)\n        gen_loss <- generator_loss(disc_generated_output)\n        disc_loss <-\n          discriminator_loss(disc_real_output, disc_generated_output)\n      }) })\n      \n      gradients_of_generator <-\n        gen_tape$gradient(gen_loss, generator$variables)\n      gradients_of_discriminator <-\n        disc_tape$gradient(disc_loss, discriminator$variables)\n      \n      generator_optimizer$apply_gradients(purrr::transpose(\n        list(gradients_of_generator, generator$variables)\n      ))\n      discriminator_optimizer$apply_gradients(purrr::transpose(\n        list(gradients_of_discriminator, discriminator$variables)\n      ))\n      \n      total_loss_gen <- total_loss_gen + gen_loss\n      total_loss_disc <- total_loss_disc + disc_loss\n      \n    })\n    \n    cat(\"Time for epoch \", epoch, \": \", Sys.time() - start, \"\\n\")\n    cat(\"Generator loss: \", total_loss_gen$numpy() / batches_per_epoch, \"\\n\")\n    cat(\"Discriminator loss: \", total_loss_disc$numpy() / batches_per_epoch, \"\\n\\n\")\n    if (epoch %% 10 == 0)\n      generate_and_save_images(generator,\n                               epoch,\n                               random_vector_for_generation)\n    \n  }\n}\n\nHere’s the function for saving generated images…\n\n\ngenerate_and_save_images <- function(model, epoch, test_input) {\n  predictions <- model(test_input, training = FALSE)\n  png(paste0(\"images_epoch_\", epoch, \".png\"))\n  par(mfcol = c(5, 5))\n  par(mar = c(0.5, 0.5, 0.5, 0.5),\n      xaxs = 'i',\n      yaxs = 'i')\n  for (i in 1:25) {\n    img <- predictions[i, , , 1]\n    img <- t(apply(img, 2, rev))\n    image(\n      1:28,\n      1:28,\n      img * 127.5 + 127.5,\n      col = gray((0:255) / 255),\n      xaxt = 'n',\n      yaxt = 'n'\n    )\n  }\n  dev.off()\n}\n\n… and we’re ready to go!\n\n\nnum_epochs <- 150\ntrain(train_dataset, num_epochs, noise_dim)\n\nResults\nHere are some generated images after training for 150 epochs:\n\nAs they say, your results will most certainly vary!\nConclusion\nWhile certainly tuning GANs will remain a challenge, we hope we were able to show that mapping concepts to code is not difficult when using eager execution. In case you’ve played around with GANs before, you may have found you needed to pay careful attention to set up the losses the right way, freeze the discriminator’s weights when needed, etc. This need goes away with eager execution. In upcoming posts, we will show further examples where using it makes model development easier.\n\n\nGoodfellow, Ian J., Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. 2014. “Generative Adversarial Nets.” In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, 2672–80. http://papers.nips.cc/paper/5423-generative-adversarial-nets.\n\n\nRadford, Alec, Luke Metz, and Soumith Chintala. 2015. “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.” CoRR abs/1511.06434. http://arxiv.org/abs/1511.06434.\n\n\n\n\n",
    "preview": "posts/2018-08-26-eager-dcgan/images/thumb.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 240,
    "preview_height": 144
  },
  {
    "path": "posts/2018-07-30-attention-layer/",
    "title": "Attention-based Neural Machine Translation with Keras",
    "description": "As sequence to sequence prediction tasks get more involved, attention mechanisms have proven helpful. A prominent example is neural machine translation. Following a recent Google Colaboratory notebook, we show how to implement attention in R.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-07-30",
    "categories": [
      "Natural Language Processing",
      "TensorFlow/Keras"
    ],
    "contents": "\nThese days it is not difficult to find sample code that demonstrates sequence to sequence translation using Keras. However, within the past few years it has been established that depending on the task, incorporating an attention mechanism significantly improves performance. First and foremost, this was the case for neural machine translation (see (Bahdanau, Cho, and Bengio 2014) and (Luong, Pham, and Manning 2015) for prominent work). But other areas performing sequence to sequence translation were profiting from incorporating an attention mechanism, too: E.g., (Xu et al. 2015) applied attention to image captioning, and (Vinyals et al. 2014), to parsing.\nIdeally, using Keras, we’d just have an attention layer managing this for us. Unfortunately, as can be seen googling for code snippets and blog posts, implementing attention in pure Keras is not that straightforward.\nConsequently, until a short time ago, the best thing to do seemed to be translating the TensorFlow Neural Machine Translation Tutorial to R TensorFlow. Then, TensorFlow eager execution happened, and turned out a game changer for a number of things that used to be difficult (not the least of which is debugging). With eager execution, tensor operations are executed immediately, as opposed to of building a graph to be evaluated later. This means we can immediately inspect the values in our tensors - and it also means we can imperatively code loops to perform interleavings of sorts that earlier were more challenging to accomplish.\nUnder these circumstances, it is not surprising that the interactive notebook on neural machine translation, published on Colaboratory, got a lot of attention for its straightforward implementation and highly intellegible explanations. Our goal here is to do the same thing from R. We will not end up with Keras code exactly the way we used to write it, but a hybrid of Keras layers and imperative code enabled by TensorFlow eager execution.\nPrerequisites\nThe code in this post depends on the development versions of several of the TensorFlow R packages. You can install these packages as follows:\n\ndevtools::install_github(c(\n  \"rstudio/reticulate\",\n  \"rstudio/tensorflow\",\n  \"rstudio/keras\",\n  \"rstudio/tfdatasets\"\n))\nYou should also be sure that you are running the very latest version of TensorFlow (v1.9), which you can install like so:\n\nlibrary(tensorflow)\ninstall_tensorflow()\nThere are additional requirements for using TensorFlow eager execution. First, we need to call tfe_enable_eager_execution() right at the beginning of the program. Second, we need to use the implementation of Keras included in TensorFlow, rather than the base Keras implementation. This is because at a later point, we are going to access model$variables which at this point does not exist in base Keras.\nWe’ll also use the tfdatasets package for our input pipeline. So we end up with the below libraries needed for this example.\nOne more aside: Please don’t copy-paste the code from the snippets for execution - you’ll find the complete code for this post here. In the post, we may deviate from required execution order for purposes of narrative.\n\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\n\nlibrary(tensorflow)\ntfe_enable_eager_execution()\n\nlibrary(tfdatasets)\n\nlibrary(purrr)\nlibrary(stringr)\nlibrary(reshape2)\nlibrary(viridis)\nlibrary(ggplot2)\nlibrary(tibble)\n\nPreparing the data\nAs our focus is on implementing the attention mechanism, we’re going to do a quick pass through pre-preprocessing. All operations are contained in short functions that are independently testable (which also makes it easy should you want to experiment with different preprocessing actions).\nThe site https://www.manythings.org/anki/ is a great source for multilingual datasets. For variation, we’ll choose a different dataset from the colab notebook, and try to translate English to Dutch. I’m going to assume you have the unzipped file nld.txt in a subdirectory called data in your current directory. The file contains 28224 sentence pairs, of which we are going to use the first 10000. Under this restriction, sentences range from one-word exclamations\n\nRun!    Ren!\nWow!    Da's niet gek!\nFire!   Vuur!\nover short phrases\n\nAre you crazy?  Ben je gek?\nDo cats dream?  Dromen katten?\nFeed the bird!  Geef de vogel voer!\nto simple sentences such as\n\nMy brother will kill me.    Mijn broer zal me vermoorden.\nNo one knows the future.    Niemand kent de toekomst.\nPlease ask someone else.    Vraag alsjeblieft iemand anders.\n\n\nfilepath <- file.path(\"data\", \"nld.txt\")\n\nlines <- readLines(filepath, n = 10000)\nsentences <- str_split(lines, \"\\t\")\n\nBasic preprocessing includes adding space before punctuation, replacing special characters, reducing multiple spaces to one, and adding <start> and <stop> tokens at the beginnings resp. ends of the sentences.\n\n\nspace_before_punct <- function(sentence) {\n  str_replace_all(sentence, \"([?.!])\", \" \\\\1\")\n}\n\nreplace_special_chars <- function(sentence) {\n  str_replace_all(sentence, \"[^a-zA-Z?.!,¿]+\", \" \")\n}\n\nadd_tokens <- function(sentence) {\n  paste0(\"<start> \", sentence, \" <stop>\")\n}\nadd_tokens <- Vectorize(add_tokens, USE.NAMES = FALSE)\n\npreprocess_sentence <- compose(add_tokens,\n                               str_squish,\n                               replace_special_chars,\n                               space_before_punct)\n\nword_pairs <- map(sentences, preprocess_sentence)\n\nAs usual with text data, we need to create lookup indices to get from words to integers and vice versa: one index each for the source and target languages.\n\n\ncreate_index <- function(sentences) {\n  unique_words <- sentences %>% unlist() %>% paste(collapse = \" \") %>%\n    str_split(pattern = \" \") %>% .[[1]] %>% unique() %>% sort()\n  index <- data.frame(\n    word = unique_words,\n    index = 1:length(unique_words),\n    stringsAsFactors = FALSE\n  ) %>%\n    add_row(word = \"<pad>\",\n                    index = 0,\n                    .before = 1)\n  index\n}\n\nword2index <- function(word, index_df) {\n  index_df[index_df$word == word, \"index\"]\n}\nindex2word <- function(index, index_df) {\n  index_df[index_df$index == index, \"word\"]\n}\n\nsrc_index <- create_index(map(word_pairs, ~ .[[1]]))\ntarget_index <- create_index(map(word_pairs, ~ .[[2]]))\n\nConversion of text to integers uses the above indices as well as Keras’ convenient pad_sequences function, which leaves us with matrices of integers, padded up to maximum sentence length found in the source and target corpora, respectively.\n\n\nsentence2digits <- function(sentence, index_df) {\n  map((sentence %>% str_split(pattern = \" \"))[[1]], function(word)\n    word2index(word, index_df))\n}\n\nsentlist2diglist <- function(sentence_list, index_df) {\n  map(sentence_list, function(sentence)\n    sentence2digits(sentence, index_df))\n}\n\nsrc_diglist <-\n  sentlist2diglist(map(word_pairs, ~ .[[1]]), src_index)\nsrc_maxlen <- map(src_diglist, length) %>% unlist() %>% max()\nsrc_matrix <-\n  pad_sequences(src_diglist, maxlen = src_maxlen,  padding = \"post\")\n\ntarget_diglist <-\n  sentlist2diglist(map(word_pairs, ~ .[[2]]), target_index)\ntarget_maxlen <- map(target_diglist, length) %>% unlist() %>% max()\ntarget_matrix <-\n  pad_sequences(target_diglist, maxlen = target_maxlen, padding = \"post\")\n\nAll that remains to be done is the train-test split.\n\n\ntrain_indices <-\n  sample(nrow(src_matrix), size = nrow(src_matrix) * 0.8)\n\nvalidation_indices <- setdiff(1:nrow(src_matrix), train_indices)\n\nx_train <- src_matrix[train_indices, ]\ny_train <- target_matrix[train_indices, ]\n\nx_valid <- src_matrix[validation_indices, ]\ny_valid <- target_matrix[validation_indices, ]\n\nbuffer_size <- nrow(x_train)\n\n# just for convenience, so we may get a glimpse at translation \n# performance during training\ntrain_sentences <- sentences[train_indices]\nvalidation_sentences <- sentences[validation_indices]\nvalidation_sample <- sample(validation_sentences, 5)\n\nCreating datasets to iterate over\nThis section does not contain much code, but it shows an important technique: the use of datasets. Remember the olden times when we used to pass in hand-crafted generators to Keras models? With tfdatasets, we can scalably feed data directly to the Keras fit function, having various preparatory actions being performed directly in native code. In our case, we will not be using fit, instead iterate directly over the tensors contained in the dataset.\n\n\ntrain_dataset <- \n  tensor_slices_dataset(keras_array(list(x_train, y_train)))  %>%\n  dataset_shuffle(buffer_size = buffer_size) %>%\n  dataset_batch(batch_size, drop_remainder = TRUE)\n\nvalidation_dataset <-\n  tensor_slices_dataset(keras_array(list(x_valid, y_valid))) %>%\n  dataset_shuffle(buffer_size = buffer_size) %>%\n  dataset_batch(batch_size, drop_remainder = TRUE)\n\nNow we are ready to roll! In fact, before talking about that training loop we need to dive into the implementation of the core logic: the custom layers responsible for performing the attention operation.\nAttention encoder\nWe will create two custom layers, only the second of which is going to incorporate attention logic.\nHowever, it’s worth introducing the encoder in detail too, because technically this is not a custom layer but a custom model, as described here.\nCustom models allow you to create member layers and then, specify custom functionality defining the operations to be performed on these layers.\nLet’s look at the complete code for the encoder.\n\n\nattention_encoder <-\n  \n  function(gru_units,\n           embedding_dim,\n           src_vocab_size,\n           name = NULL) {\n    \n    keras_model_custom(name = name, function(self) {\n      \n      self$embedding <-\n        layer_embedding(\n          input_dim = src_vocab_size,\n          output_dim = embedding_dim\n        )\n      \n      self$gru <-\n        layer_gru(\n          units = gru_units,\n          return_sequences = TRUE,\n          return_state = TRUE\n        )\n      \n      function(inputs, mask = NULL) {\n        \n        x <- inputs[[1]]\n        hidden <- inputs[[2]]\n        \n        x <- self$embedding(x)\n        c(output, state) %<-% self$gru(x, initial_state = hidden)\n    \n        list(output, state)\n      }\n    })\n  }\n\nThe encoder has two layers, an embedding and a GRU layer. The ensuing anonymous function specifies what should happen when the layer is called. One thing that might look unexpected is the argument passed to that function: It is a list of tensors, where the first element are the inputs, and the second is the hidden state at the point the layer is called (in traditional Keras RNN usage, we are accustomed to seeing state manipulations being done transparently for us.) As the input to the call flows through the operations, let’s keep track of the shapes involved:\nx, the input, is of size (batch_size, max_length_input), where max_length_input is the number of digits constituting a source sentence. (Remember we’ve padded them to be of uniform length.) In familiar RNN parlance, we could also speak of timesteps here (we soon will).\nAfter the embedding step, the tensors will have an additional axis, as each timestep (token) will have been embedded as an embedding_dim-dimensional vector. So our shapes are now (batch_size, max_length_input, embedding_dim).\nNote how when calling the GRU, we’re passing in the hidden state we received as initial_state. We get back a list: the GRU output and last hidden state.\nAt this point, it helps to look up RNN output shapes in the documentation.\nWe have specified our GRU to return sequences as well as the state. Our asking for the state means we’ll get back a list of tensors: the output, and the last state(s) - a single last state in this case as we’re using GRU. That state itself will be of shape (batch_size, gru_units). Our asking for sequences means the output will be of shape (batch_size, max_length_input, gru_units). So that’s that. We bundle output and last state in a list and pass it to the calling code.\nBefore we show the decoder, we need to say a few things about attention.\nAttention in a nutshell\nAs T. Luong nicely puts it in his thesis, the idea of the attention mechanism is\n\nto provide a ‘random access memory’ of source hidden states which one can constantly refer to as translation progresses.\n\nThis means that at every timestep, the decoder receives not just the previous decoder hidden state, but also the complete output from the encoder. It then “makes up its mind” as to what part of the encoded input matters at the current point in time. Although various attention mechanisms exist, the basic procedure often goes like this.\n\nIn our description, we’re closely following (Luong, Pham, and Manning 2015), in accordance with the colaboratory notebook on NMT.\nFirst, we create a score that relates the decoder hidden state at a given timestep to the encoder hidden states at every timestep.\nThe score function can take different shapes; the following is commonly referred to as Bahdanau style (additive) attention.\nNote that when referring to this as Bahdanau style attention, we - like others - do not imply exact agreement with the formulae in (Bahdanau, Cho, and Bengio 2014). It is about the general way encoder and decoder hidden states are combined - additively or multiplicatively.\n\\[score(\\mathbf{h}_t,\\bar{\\mathbf{h}_s}) = \\mathbf{v}_a^T tanh(\\mathbf{W_1}\\mathbf{h}_t + \\mathbf{W_2}\\bar{\\mathbf{h}_s})\\]\nFrom these scores, we want to find the encoder states that matter most to the current decoder timestep. Basically, we just normalize the scores doing a softmax, which leaves us with a set of attention weights (also called alignment vectors):\n\\[\\alpha_{ts} = \\frac{exp(score(\\mathbf{h}_t,\\bar{\\mathbf{h}_s}))}{\\sum_{s'=1}^{S}{score(\\mathbf{h}_t,\\bar{\\mathbf{h}_{s'}})}}\\]\nFrom these attention weights, we create the context vector. This is basically an average of the source hidden states, weighted by the attention weights:\n\\[\\mathbf{c}_t= \\sum_s{\\alpha_{ts} \\bar{\\mathbf{h}_s}}\\]\nNow we need to relate this to the state the decoder is in. We calculate the attention vector from a concatenation of context vector and current decoder hidden state:\n\\[\\mathbf{a}_t = tanh(\\mathbf{W_c} [ \\mathbf{c}_t ; \\mathbf{h}_t])\\]\nIn sum, we see how at each timestep, the attention mechanism combines information from the sequence of encoder states, and the current decoder hidden state. We’ll soon see a third source of information entering the calculation, which will be dependent on whether we’re in the training or the prediction phase.\nAttention decoder\nNow let’s look at how the attention decoder implements the above logic. We will be following the colab notebook in presenting a slight simplification of the score function, which will not prevent the decoder from successfully translating our example sentences.\n\n\nattention_decoder <-\n  function(object,\n           gru_units,\n           embedding_dim,\n           target_vocab_size,\n           name = NULL) {\n    \n    keras_model_custom(name = name, function(self) {\n      \n      self$gru <-\n        layer_gru(\n          units = gru_units,\n          return_sequences = TRUE,\n          return_state = TRUE\n        )\n      \n      self$embedding <-\n        layer_embedding(input_dim = target_vocab_size, \n                        output_dim = embedding_dim)\n      \n      gru_units <- gru_units\n      self$fc <- layer_dense(units = target_vocab_size)\n      self$W1 <- layer_dense(units = gru_units)\n      self$W2 <- layer_dense(units = gru_units)\n      self$V <- layer_dense(units = 1L)\n \n      function(inputs, mask = NULL) {\n        \n        x <- inputs[[1]]\n        hidden <- inputs[[2]]\n        encoder_output <- inputs[[3]]\n        \n        hidden_with_time_axis <- k_expand_dims(hidden, 2)\n        \n        score <- self$V(k_tanh(self$W1(encoder_output) + \n                                self$W2(hidden_with_time_axis)))\n        \n        attention_weights <- k_softmax(score, axis = 2)\n        \n        context_vector <- attention_weights * encoder_output\n        context_vector <- k_sum(context_vector, axis = 2)\n    \n        x <- self$embedding(x)\n       \n        x <- k_concatenate(list(k_expand_dims(context_vector, 2), x), axis = 3)\n        \n        c(output, state) %<-% self$gru(x)\n   \n        output <- k_reshape(output, c(-1, gru_units))\n    \n        x <- self$fc(output)\n \n        list(x, state, attention_weights)\n        \n      }\n      \n    })\n  }\n\nFirstly, we notice that in addition to the usual embedding and GRU layers we’d expect in a decoder, there are a few additional dense layers. We’ll comment on those as we go.\nThis time, the first argument to what is effectively the call function consists of three parts: input, hidden state, and the output from the encoder.\nFirst we need to calculate the score, which basically means addition of two matrix multiplications. For that addition, the shapes have to match. Now encoder_output is of shape (batch_size, max_length_input, gru_units), while hidden has shape (batch_size, gru_units). We thus add an axis “in the middle”, obtaining hidden_with_time_axis, of shape (batch_size, 1, gru_units).\nAfter applying the tanh and the fully connected layer to the result of the addition, score will be of shape (batch_size, max_length_input, 1). The next step calculates the softmax, to get the attention weights. Now softmax by default is applied on the last axis - but here we’re applying it on the second axis, since it is with respect to the input timesteps we want to normalize the scores.\nAfter normalization, the shape is still (batch_size, max_length_input, 1).\nNext up we compute the context vector, as a weighted average of encoder hidden states. Its shape is (batch_size, gru_units). Note that like with the softmax operation above, we sum over the second axis, which corresponds to the number of timesteps in the input received from the encoder.\nWe still have to take care of the third source of information: the input. Having been passed through the embedding layer, its shape is (batch_size, 1, embedding_dim). Here, the second axis is of dimension 1 as we’re forecasting a single token at a time.\nNow, let’s concatenate the context vector and the embedded input, to arrive at the attention vector. If you compare the code with the formula above, you’ll see that here we’re skipping the tanh and the additional fully connected layer, and just leave it at the concatenation. After concatenation, the shape now is (batch_size, 1, embedding_dim + gru_units).\nThe ensuing GRU operation, as usual, gives us back output and shape tensors. The output tensor is flattened to shape (batch_size, gru_units) and passed through the final densely connected layer, after which the output has shape (batch_size, target_vocab_size). With that, we’re going to be able to forecast the next token for every input in the batch.\nRemains to return everything we’re interested in: the output (to be used for forecasting), the last GRU hidden state (to be passed back in to the decoder), and the attention weights for this batch (for plotting). And that’s that!\nCreating the “model”\nWe’re almost ready to train the model. The model? We don’t have a model yet. The next steps will feel a bit unusual if you’re accustomed to the traditional Keras create model -> compile model -> fit model  workflow. Let’s have a look.\nFirst, we need a few bookkeeping variables.\n\n\nbatch_size <- 32\nembedding_dim <- 64\ngru_units <- 256\n\nsrc_vocab_size <- nrow(src_index)\ntarget_vocab_size <- nrow(target_index)\n\nNow, we create the encoder and decoder objects - it’s tempting to call them layers, but technically both are custom Keras models.\n\n\nencoder <- attention_encoder(\n  gru_units = gru_units,\n  embedding_dim = embedding_dim,\n  src_vocab_size = src_vocab_size\n)\n\ndecoder <- attention_decoder(\n  gru_units = gru_units,\n  embedding_dim = embedding_dim,\n  target_vocab_size = target_vocab_size\n)\n\nSo as we’re going along, assembling a model “from pieces”, we still need a loss function, and an optimizer.\n\n\noptimizer <- tf$train$AdamOptimizer()\n\ncx_loss <- function(y_true, y_pred) {\n  mask <- ifelse(y_true == 0L, 0, 1)\n  loss <-\n    tf$nn$sparse_softmax_cross_entropy_with_logits(labels = y_true,\n                                                   logits = y_pred) * mask\n  tf$reduce_mean(loss)\n}\n\nNow we’re ready to train.\nTraining phase\nIn the training phase, we’re using teacher forcing, which is the established name for feeding the model the (correct) target at time \\(t\\) as input for the next calculation step at time \\(t + 1\\). This is in contrast to the inference phase, when the decoder output is fed back as input to the next time step.\nThe training phase consists of three loops: firstly, we’re looping over epochs, secondly, over the dataset, and thirdly, over the target sequence we’re predicting.\nFor each batch, we’re encoding the source sequence, getting back the output sequence as well as the last hidden state. The hidden state we then use to initialize the decoder. Now, we enter the target sequence prediction loop. For each timestep to be predicted, we call the decoder with the input (which due to teacher forcing is the ground truth from the previous step), its previous hidden state, and the complete encoder output. At each step, the decoder returns predictions, its hidden state and the attention weights.\n\n\nn_epochs <- 50\n\nencoder_init_hidden <- k_zeros(c(batch_size, gru_units))\n\nfor (epoch in seq_len(n_epochs)) {\n  \n  total_loss <- 0\n  iteration <- 0\n    \n  iter <- make_iterator_one_shot(train_dataset)\n    \n  until_out_of_range({\n    \n    batch <- iterator_get_next(iter)\n    loss <- 0\n    x <- batch[[1]]\n    y <- batch[[2]]\n    iteration <- iteration + 1\n      \n    with(tf$GradientTape() %as% tape, {\n      c(enc_output, enc_hidden) %<-% encoder(list(x, encoder_init_hidden))\n \n      dec_hidden <- enc_hidden\n      dec_input <-\n        k_expand_dims(rep(list(\n          word2index(\"<start>\", target_index)\n        ), batch_size))\n        \n\n      for (t in seq_len(target_maxlen - 1)) {\n        c(preds, dec_hidden, weights) %<-%\n          decoder(list(dec_input, dec_hidden, enc_output))\n        loss <- loss + cx_loss(y[, t], preds)\n     \n        dec_input <- k_expand_dims(y[, t])\n      }\n      \n    })\n      \n    total_loss <-\n      total_loss + loss / k_cast_to_floatx(dim(y)[2])\n      \n      paste0(\n        \"Batch loss (epoch/batch): \",\n        epoch,\n        \"/\",\n        iter,\n        \": \",\n        (loss / k_cast_to_floatx(dim(y)[2])) %>% \n          as.double() %>% round(4),\n        \"\\n\"\n      )\n      \n    variables <- c(encoder$variables, decoder$variables)\n    gradients <- tape$gradient(loss, variables)\n      \n    optimizer$apply_gradients(\n      purrr::transpose(list(gradients, variables)),\n      global_step = tf$train$get_or_create_global_step()\n    )\n      \n  })\n    \n    paste0(\n      \"Total loss (epoch): \",\n      epoch,\n      \": \",\n      (total_loss / k_cast_to_floatx(buffer_size)) %>% \n        as.double() %>% round(4),\n      \"\\n\"\n    )\n}\n\nHow does backpropagation work with this new flow? With eager execution, a GradientTape records operations performed on the forward pass. This recording is then “played back” to perform backpropagation. Concretely put, during the forward pass, we have the tape recording the model’s actions, and we keep incrementally updating the loss. Then, outside the tape’s context, we ask the tape for the gradients of the accumulated loss with respect to the model’s variables. Once we know the gradients, we can have the optimizer apply them to those variables. This variables slot, by the way, does not (as of this writing) exist in the base implementation of Keras, which is why we have to resort to the TensorFlow implementation.\nInference\nAs soon as we have a trained model, we can get translating! Actually, we don’t have to wait. We can integrate a few sample translations directly into the training loop, and watch the network progressing (hopefully!). The complete code for this post does it like this, however here we’re arranging the steps in a more didactical order. The inference loop differs from the training procedure mainly it that it does not use teacher forcing. Instead, we feed back the current prediction as input to the next decoding timestep. The actual predicted word is chosen from the exponentiated raw scores returned by the decoder using a multinomial distribution. We also include a function to plot a heatmap that shows where in the source attention is being directed as the translation is produced.\n\n\nevaluate <-\n  function(sentence) {\n    attention_matrix <-\n      matrix(0, nrow = target_maxlen, ncol = src_maxlen)\n    \n    sentence <- preprocess_sentence(sentence)\n    input <- sentence2digits(sentence, src_index)\n    input <-\n      pad_sequences(list(input), maxlen = src_maxlen,  padding = \"post\")\n    input <- k_constant(input)\n    \n    result <- \"\"\n    \n    hidden <- k_zeros(c(1, gru_units))\n    c(enc_output, enc_hidden) %<-% encoder(list(input, hidden))\n    \n    dec_hidden <- enc_hidden\n    dec_input <-\n      k_expand_dims(list(word2index(\"<start>\", target_index)))\n    \n    for (t in seq_len(target_maxlen - 1)) {\n      c(preds, dec_hidden, attention_weights) %<-%\n        decoder(list(dec_input, dec_hidden, enc_output))\n      attention_weights <- k_reshape(attention_weights, c(-1))\n      attention_matrix[t, ] <- attention_weights %>% as.double()\n      \n      pred_idx <-\n        tf$multinomial(k_exp(preds), num_samples = 1)[1, 1] %>% as.double()\n      pred_word <- index2word(pred_idx, target_index)\n      \n      if (pred_word == '<stop>') {\n        result <-\n          paste0(result, pred_word)\n        return (list(result, sentence, attention_matrix))\n      } else {\n        result <-\n          paste0(result, pred_word, \" \")\n        dec_input <- k_expand_dims(list(pred_idx))\n      }\n    }\n    list(str_trim(result), sentence, attention_matrix)\n  }\n\nplot_attention <-\n  function(attention_matrix,\n           words_sentence,\n           words_result) {\n    melted <- melt(attention_matrix)\n    ggplot(data = melted, aes(\n      x = factor(Var2),\n      y = factor(Var1),\n      fill = value\n    )) +\n      geom_tile() + scale_fill_viridis() + guides(fill = FALSE) +\n      theme(axis.ticks = element_blank()) +\n      xlab(\"\") +\n      ylab(\"\") +\n      scale_x_discrete(labels = words_sentence, position = \"top\") +\n      scale_y_discrete(labels = words_result) + \n      theme(aspect.ratio = 1)\n  }\n\n\ntranslate <- function(sentence) {\n  c(result, sentence, attention_matrix) %<-% evaluate(sentence)\n  print(paste0(\"Input: \",  sentence))\n  print(paste0(\"Predicted translation: \", result))\n  attention_matrix <-\n    attention_matrix[1:length(str_split(result, \" \")[[1]]),\n                     1:length(str_split(sentence, \" \")[[1]])]\n  plot_attention(attention_matrix,\n                 str_split(sentence, \" \")[[1]],\n                 str_split(result, \" \")[[1]])\n}\n\nLearning to translate\nUsing the sample code, you can see yourself how learning progresses. This is how it worked in our case. (We are always looking at the same sentences - sampled from the training and test sets, respectively - so we can more easily see the evolution.)\nOn completion of the very first epoch, our network starts every Dutch sentence with Ik. No doubt, there must be many sentences starting in the first person in our corpus!\n(Note: these five sentences are all from the training set.)\n\nInput: <start> I did that easily . <stop>\nPredicted translation: <start> Ik . <stop>\n\nInput: <start> Look in the mirror . <stop>\nPredicted translation: <start> Ik . <stop>\n\nInput: <start> Tom wanted revenge . <stop>\nPredicted translation: <start> Ik . <stop>\n\nInput: <start> It s very kind of you . <stop>\nPredicted translation: <start> Ik . <stop>\n\nInput: <start> I refuse to answer . <stop>\nPredicted translation: <start> Ik . <stop>\nOne epoch later it seems to have picked up common phrases, although their use does not look related to the input. And definitely, it has problems to recognize when it’s over…\n\nInput: <start> I did that easily . <stop>\nPredicted translation: <start> Ik ben een een een een een een een een een een\n\nInput: <start> Look in the mirror . <stop>\nPredicted translation: <start> Tom is een een een een een een een een een een\n\nInput: <start> Tom wanted revenge . <stop>\nPredicted translation: <start> Tom is een een een een een een een een een een\n\nInput: <start> It s very kind of you . <stop>\nPredicted translation: <start> Ik ben een een een een een een een een een een\n\nInput: <start> I refuse to answer . <stop>\nPredicted translation: <start> Ik ben een een een een een een een een een een\nJumping ahead to epoch 7, the translations still are completely wrong, but somehow start capturing overall sentence structure (like the imperative in sentence 2).\n\nInput: <start> I did that easily . <stop>\nPredicted translation: <start> Ik heb je niet . <stop>\n\nInput: <start> Look in the mirror . <stop>\nPredicted translation: <start> Ga naar de buurt . <stop>\n\nInput: <start> Tom wanted revenge . <stop>\nPredicted translation: <start> Tom heeft Tom . <stop>\n\nInput: <start> It s very kind of you . <stop>\nPredicted translation: <start> Het is een auto . <stop>\n\nInput: <start> I refuse to answer . <stop>\nPredicted translation: <start> Ik heb de buurt . <stop>\nFast forward to epoch 17. Samples from the training set are starting to look better:\n\nInput: <start> I did that easily . <stop>\nPredicted translation: <start> Ik heb dat hij gedaan . <stop>\n\nInput: <start> Look in the mirror . <stop>\nPredicted translation: <start> Kijk in de spiegel . <stop>\n\nInput: <start> Tom wanted revenge . <stop>\nPredicted translation: <start> Tom wilde dood . <stop>\n\nInput: <start> It s very kind of you . <stop>\nPredicted translation: <start> Het is erg goed voor je . <stop>\n\nInput: <start> I refuse to answer . <stop>\nPredicted translation: <start> Ik speel te antwoorden . <stop>\nWhereas samples from the test set still look pretty random. Although interestingly, not random in the sense of not having syntactic or semantic structure! Breng de televisie op is a perfectly reasonable sentence, if not the most lucky translation of Think happy thoughts.\n\nInput: <start> It s entirely my fault . <stop>\nPredicted translation: <start> Het is het mijn woord . <stop>\n\nInput: <start> You re trustworthy . <stop>\nPredicted translation: <start> Je bent net . <stop>\n\nInput: <start> I want to live in Italy . <stop>\nPredicted translation: <start> Ik wil in een leugen . <stop>\n\nInput: <start> He has seven sons . <stop>\nPredicted translation: <start> Hij heeft Frans uit . <stop>\n\nInput: <start> Think happy thoughts . <stop>\nPredicted translation: <start> Breng de televisie op . <stop>\nWhere are we at after 30 epochs? By now, the training samples have been pretty much memorized (the third sentence is suffering from political correctness though, matching Tom wanted revenge to Tom wilde vrienden):\n\nInput: <start> I did that easily . <stop>\nPredicted translation: <start> Ik heb dat zonder moeite gedaan . <stop>\n\nInput: <start> Look in the mirror . <stop>\nPredicted translation: <start> Kijk in de spiegel . <stop>\n\nInput: <start> Tom wanted revenge . <stop>\nPredicted translation: <start> Tom wilde vrienden . <stop>\n\nInput: <start> It s very kind of you . <stop>\nPredicted translation: <start> Het is erg aardig van je . <stop>\n\nInput: <start> I refuse to answer . <stop>\nPredicted translation: <start> Ik weiger te antwoorden . <stop>\nHow about the test sentences? They’ve started to look much better. One sentence (Ik wil in Itali leven) has even been translated entirely correctly. And we see something like the concept of numerals appearing (seven translated by acht)…\n\nInput: <start> It s entirely my fault . <stop>\nPredicted translation: <start> Het is bijna mijn beurt . <stop>\n\nInput: <start> You re trustworthy . <stop>\nPredicted translation: <start> Je bent zo zijn . <stop>\n\nInput: <start> I want to live in Italy . <stop>\nPredicted translation: <start> Ik wil in Itali leven . <stop>\n\nInput: <start> He has seven sons . <stop>\nPredicted translation: <start> Hij heeft acht geleden . <stop>\n\nInput: <start> Think happy thoughts . <stop>\nPredicted translation: <start> Zorg alstublieft goed uit . <stop>\nAs you see it can be quite interesting watching the network’s “language capability” evolve. Now, how about subjecting our network to a little MRI scan? Since we’re collecting the attention weights, we can visualize what part of the source text the decoder is attending to at every timestep.\nWhat is the decoder looking at?\nFirst, let’s take an example where word orders in both languages are the same.\n\nInput: <start> It s very kind of you . <stop>\nPredicted translation: <start> Het is erg aardig van je . <stop>\n\n\n\nWe see that overall, given a sample where respective sentences align very well, the decoder pretty much looks where it is supposed to. Let’s pick something a little more complicated.\n\nInput: <start> I did that easily . <stop>\"\nPredicted translation: <start> Ik heb dat zonder moeite gedaan . <stop>\nThe translation is correct, but word order in both languages isn’t the same here: did corresponds to the analytic perfect heb … gedaan. Will we be able to see that in the attention plot?\n\n\n\nThe answer is no. It would be interesting to check again after training for a couple more epochs.\nFinally, let’s investigate this translation from the test set (which is entirely correct):\n\nInput: <start> I want to live in Italy . <stop>\nPredicted translation: <start> Ik wil in Itali leven . <stop>\n\n\n\nThese two sentences don’t align well. We see that Dutch in correctly picks English in (skipping over to live), then Itali attends to Italy. Finally leven is produced without us witnessing the decoder looking back to live. Here again, it would be interesting to watch what happens a few epochs later!\nNext up\nThere are many ways to go from here. For one, we didn’t do any hyperparameter optimization. (See e.g. (Luong, Pham, and Manning 2015) for an extensive experiment on architectures and hyperparameters for NMT.) Second, provided you have access to the required hardware, you might be curious how good an algorithm like this can get when trained on a real big dataset, using a real big network. Third, alternative attention mechanisms have been suggested (see e.g. T. Luong’s thesis which we followed rather closely in the description of attention above).\nLast not least, no one said attention need be useful only in the context of machine translation. Out there, a plenty of sequence prediction (time series) problems are waiting to be explored with respect to its potential usefulness…\n\n\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural Machine Translation by Jointly Learning to Align and Translate.” CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473.\n\n\nLuong, Minh-Thang, Hieu Pham, and Christopher D. Manning. 2015. “Effective Approaches to Attention-Based Neural Machine Translation.” CoRR abs/1508.04025. http://arxiv.org/abs/1508.04025.\n\n\nVinyals, Oriol, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey E. Hinton. 2014. “Grammar as a Foreign Language.” CoRR abs/1412.7449. http://arxiv.org/abs/1412.7449.\n\n\nXu, Kelvin, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.” CoRR abs/1502.03044. http://arxiv.org/abs/1502.03044.\n\n\n\n\n",
    "preview": "posts/2018-07-30-attention-layer/images/attention.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 606,
    "preview_height": 448
  },
  {
    "path": "posts/2018-07-17-activity-detection/",
    "title": "Classifying physical activity from smartphone data",
    "description": "Using Keras to train a convolutional neural network to classify physical activity. The dataset was built from the recordings of 30 subjects performing basic activities and postural transitions while carrying a waist-mounted smartphone with embedded inertial sensors.",
    "author": [
      {
        "name": "Nick Strayer",
        "url": "http://nickstrayer.me"
      }
    ],
    "date": "2018-07-17",
    "categories": [],
    "contents": "\nIntroduction\nIn this post we’ll describe how to use smartphone accelerometer and gyroscope data to predict the physical activities of the individuals carrying the phones. The data used in this post comes from the Smartphone-Based Recognition of Human Activities and Postural Transitions Data Set distributed by the University of California, Irvine. Thirty individuals were tasked with performing various basic activities with an attached smartphone recording movement using an accelerometer and gyroscope.\nBefore we begin, let’s load the various libraries that we’ll use in the analysis:\n\n\nlibrary(keras)     # Neural Networks\nlibrary(tidyverse) # Data cleaning / Visualization\nlibrary(knitr)     # Table printing\nlibrary(rmarkdown) # Misc. output utilities \nlibrary(ggridges)  # Visualization\n\nActivities dataset\nThe data used in this post come from the Smartphone-Based Recognition of Human Activities and Postural Transitions Data Set(Reyes-Ortiz et al. 2016) distributed by the University of California, Irvine.\n\nThroughout this post, data/ is the directory created by downloading and unzipping this dataset.\nWhen downloaded from the link above, the data contains two different ‘parts.’ One that has been pre-processed using various feature extraction techniques such as fast-fourier transform, and another RawData section that simply supplies the raw X,Y,Z directions of an accelerometer and gyroscope. None of the standard noise filtering or feature extraction used in accelerometer data has been applied. This is the data set we will use.\nThe motivation for working with the raw data in this post is to aid the transition of the code/concepts to time series data in less well-characterized domains. While a more accurate model could be made by utilizing the filtered/cleaned data provided, the filtering and transformation can vary greatly from task to task; requiring lots of manual effort and domain knowledge. One of the beautiful things about deep learning is the feature extraction is learned from the data, not outside knowledge.\nActivity labels\nThe data has integer encodings for the activities which, while not important to the model itself, are helpful for use to see. Let’s load them first.\n\n\nactivityLabels <- read.table(\"data/activity_labels.txt\", \n                             col.names = c(\"number\", \"label\")) \n\nactivityLabels %>% kable(align = c(\"c\", \"l\"))\nnumber\nlabel\n1\nWALKING\n2\nWALKING_UPSTAIRS\n3\nWALKING_DOWNSTAIRS\n4\nSITTING\n5\nSTANDING\n6\nLAYING\n7\nSTAND_TO_SIT\n8\nSIT_TO_STAND\n9\nSIT_TO_LIE\n10\nLIE_TO_SIT\n11\nSTAND_TO_LIE\n12\nLIE_TO_STAND\n\nNext, we load in the labels key for the RawData. This file is a list of all of the observations, or individual activity recordings, contained in the data set. The key for the columns is taken from the data README.txt.\n\nColumn 1: experiment number ID, \nColumn 2: user number ID, \nColumn 3: activity number ID \nColumn 4: Label start point \nColumn 5: Label end point \nThe start and end points are in number of signal log samples (recorded at 50hz).\nLet’s take a look at the first 50 rows:\n\n\nlabels <- read.table(\n  \"data/RawData/labels.txt\",\n  col.names = c(\"experiment\", \"userId\", \"activity\", \"startPos\", \"endPos\")\n)\n\nlabels %>% \n  head(50) %>% \n  paged_table()\n\n\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"experiment\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"userId\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"activity\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"startPos\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"endPos\"],\"name\":[5],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"1\",\"3\":\"5\",\"4\":\"250\",\"5\":\"1232\",\"_rn_\":\"1\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"7\",\"4\":\"1233\",\"5\":\"1392\",\"_rn_\":\"2\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"4\",\"4\":\"1393\",\"5\":\"2194\",\"_rn_\":\"3\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"8\",\"4\":\"2195\",\"5\":\"2359\",\"_rn_\":\"4\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"5\",\"4\":\"2360\",\"5\":\"3374\",\"_rn_\":\"5\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"11\",\"4\":\"3375\",\"5\":\"3662\",\"_rn_\":\"6\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"6\",\"4\":\"3663\",\"5\":\"4538\",\"_rn_\":\"7\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"10\",\"4\":\"4539\",\"5\":\"4735\",\"_rn_\":\"8\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"4\",\"4\":\"4736\",\"5\":\"5667\",\"_rn_\":\"9\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"9\",\"4\":\"5668\",\"5\":\"5859\",\"_rn_\":\"10\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"6\",\"4\":\"5860\",\"5\":\"6786\",\"_rn_\":\"11\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"12\",\"4\":\"6787\",\"5\":\"6977\",\"_rn_\":\"12\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"1\",\"4\":\"7496\",\"5\":\"8078\",\"_rn_\":\"13\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"1\",\"4\":\"8356\",\"5\":\"9250\",\"_rn_\":\"14\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"1\",\"4\":\"9657\",\"5\":\"10567\",\"_rn_\":\"15\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"1\",\"4\":\"10750\",\"5\":\"11714\",\"_rn_\":\"16\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"3\",\"4\":\"13191\",\"5\":\"13846\",\"_rn_\":\"17\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"2\",\"4\":\"14069\",\"5\":\"14699\",\"_rn_\":\"18\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"3\",\"4\":\"14869\",\"5\":\"15492\",\"_rn_\":\"19\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"2\",\"4\":\"15712\",\"5\":\"16377\",\"_rn_\":\"20\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"3\",\"4\":\"16530\",\"5\":\"17153\",\"_rn_\":\"21\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"2\",\"4\":\"17298\",\"5\":\"17970\",\"_rn_\":\"22\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"5\",\"4\":\"251\",\"5\":\"1226\",\"_rn_\":\"23\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"7\",\"4\":\"1227\",\"5\":\"1432\",\"_rn_\":\"24\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"4\",\"4\":\"1433\",\"5\":\"2221\",\"_rn_\":\"25\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"8\",\"4\":\"2222\",\"5\":\"2377\",\"_rn_\":\"26\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"5\",\"4\":\"2378\",\"5\":\"3304\",\"_rn_\":\"27\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"11\",\"4\":\"3305\",\"5\":\"3572\",\"_rn_\":\"28\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"6\",\"4\":\"3573\",\"5\":\"4435\",\"_rn_\":\"29\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"10\",\"4\":\"4436\",\"5\":\"4619\",\"_rn_\":\"30\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"4\",\"4\":\"4620\",\"5\":\"5452\",\"_rn_\":\"31\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"9\",\"4\":\"5453\",\"5\":\"5689\",\"_rn_\":\"32\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"6\",\"4\":\"5690\",\"5\":\"6467\",\"_rn_\":\"33\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"12\",\"4\":\"6468\",\"5\":\"6709\",\"_rn_\":\"34\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"1\",\"4\":\"7624\",\"5\":\"8252\",\"_rn_\":\"35\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"1\",\"4\":\"8618\",\"5\":\"9576\",\"_rn_\":\"36\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"1\",\"4\":\"9991\",\"5\":\"10927\",\"_rn_\":\"37\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"1\",\"4\":\"11311\",\"5\":\"12282\",\"_rn_\":\"38\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"3\",\"4\":\"13129\",\"5\":\"13379\",\"_rn_\":\"39\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"3\",\"4\":\"13495\",\"5\":\"13927\",\"_rn_\":\"40\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"2\",\"4\":\"14128\",\"5\":\"14783\",\"_rn_\":\"41\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"3\",\"4\":\"15037\",\"5\":\"15684\",\"_rn_\":\"42\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"2\",\"4\":\"15920\",\"5\":\"16598\",\"_rn_\":\"43\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"3\",\"4\":\"16847\",\"5\":\"17471\",\"_rn_\":\"44\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"2\",\"4\":\"17725\",\"5\":\"18425\",\"_rn_\":\"45\"},{\"1\":\"3\",\"2\":\"2\",\"3\":\"5\",\"4\":\"298\",\"5\":\"1398\",\"_rn_\":\"46\"},{\"1\":\"3\",\"2\":\"2\",\"3\":\"7\",\"4\":\"1399\",\"5\":\"1555\",\"_rn_\":\"47\"},{\"1\":\"3\",\"2\":\"2\",\"3\":\"4\",\"4\":\"1686\",\"5\":\"2627\",\"_rn_\":\"48\"},{\"1\":\"3\",\"2\":\"2\",\"3\":\"8\",\"4\":\"2628\",\"5\":\"2769\",\"_rn_\":\"49\"},{\"1\":\"3\",\"2\":\"2\",\"3\":\"5\",\"4\":\"2770\",\"5\":\"3904\",\"_rn_\":\"50\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  \n\nFile names\nNext, let’s look at the actual files of the user data provided to us in RawData/\n\n\ndataFiles <- list.files(\"data/RawData\")\ndataFiles %>% head()\n\n[1] \"acc_exp01_user01.txt\" \"acc_exp02_user01.txt\"\n[3] \"acc_exp03_user02.txt\" \"acc_exp04_user02.txt\"\n[5] \"acc_exp05_user03.txt\" \"acc_exp06_user03.txt\"\n\nThere is a three-part file naming scheme. The first part is the type of data the file contains: either acc for accelerometer or gyro for gyroscope. Next is the experiment number, and last is the user Id for the recording. Let’s load these into a dataframe for ease of use later.\n\n\nfileInfo <- data_frame(\n  filePath = dataFiles\n) %>%\n  filter(filePath != \"labels.txt\") %>% \n  separate(filePath, sep = '_', \n           into = c(\"type\", \"experiment\", \"userId\"), \n           remove = FALSE) %>% \n  mutate(\n    experiment = str_remove(experiment, \"exp\"),\n    userId = str_remove_all(userId, \"user|\\\\.txt\")\n  ) %>% \n  spread(type, filePath)\n\nfileInfo %>% head() %>% kable()\nexperiment\nuserId\nacc\ngyro\n01\n01\nacc_exp01_user01.txt\ngyro_exp01_user01.txt\n02\n01\nacc_exp02_user01.txt\ngyro_exp02_user01.txt\n03\n02\nacc_exp03_user02.txt\ngyro_exp03_user02.txt\n04\n02\nacc_exp04_user02.txt\ngyro_exp04_user02.txt\n05\n03\nacc_exp05_user03.txt\ngyro_exp05_user03.txt\n06\n03\nacc_exp06_user03.txt\ngyro_exp06_user03.txt\n\nReading and gathering data\nBefore we can do anything with the data provided we need to get it into a model-friendly format. This means we want to have a list of observations, their class (or activity label), and the data corresponding to the recording.\nTo obtain this we will scan through each of the recording files present in dataFiles, look up what observations are contained in the recording, extract those recordings and return everything to an easy to model with dataframe.\n\n\n# Read contents of single file to a dataframe with accelerometer and gyro data.\nreadInData <- function(experiment, userId){\n  genFilePath = function(type) {\n    paste0(\"data/RawData/\", type, \"_exp\",experiment, \"_user\", userId, \".txt\")\n  }  \n  \n  bind_cols(\n    read.table(genFilePath(\"acc\"), col.names = c(\"a_x\", \"a_y\", \"a_z\")),\n    read.table(genFilePath(\"gyro\"), col.names = c(\"g_x\", \"g_y\", \"g_z\"))\n  )\n}\n\n# Function to read a given file and get the observations contained along\n# with their classes.\n\nloadFileData <- function(curExperiment, curUserId) {\n  \n  # load sensor data from file into dataframe\n  allData <- readInData(curExperiment, curUserId)\n\n  extractObservation <- function(startPos, endPos){\n    allData[startPos:endPos,]\n  }\n  \n  # get observation locations in this file from labels dataframe\n  dataLabels <- labels %>% \n    filter(userId == as.integer(curUserId), \n           experiment == as.integer(curExperiment))\n  \n\n  # extract observations as dataframes and save as a column in dataframe.\n  dataLabels %>% \n    mutate(\n      data = map2(startPos, endPos, extractObservation)\n    ) %>% \n    select(-startPos, -endPos)\n}\n\n# scan through all experiment and userId combos and gather data into a dataframe. \nallObservations <- map2_df(fileInfo$experiment, fileInfo$userId, loadFileData) %>% \n  right_join(activityLabels, by = c(\"activity\" = \"number\")) %>% \n  rename(activityName = label)\n\n# cache work. \nwrite_rds(allObservations, \"allObservations.rds\")\nallObservations %>% dim()\n\n\n\n[1] 1214    5\n\nExploring the data\nNow that we have all the data loaded along with the experiment, userId, and activity labels, we can explore the data set.\nLength of recordings\nLet’s first look at the length of the recordings by activity.\n\n\nallObservations %>% \n  mutate(recording_length = map_int(data,nrow)) %>% \n  ggplot(aes(x = recording_length, y = activityName)) +\n  geom_density_ridges(alpha = 0.8)\n\n\nThe fact there is such a difference in length of recording between the different activity types requires us to be a bit careful with how we proceed. If we train the model on every class at once we are going to have to pad all the observations to the length of the longest, which would leave a large majority of the observations with a huge proportion of their data being just padding-zeros. Because of this, we will fit our model to just the largest ‘group’ of observations length activities, these include STAND_TO_SIT, STAND_TO_LIE, SIT_TO_STAND, SIT_TO_LIE, LIE_TO_STAND, and LIE_TO_SIT.\n\nIt is notable that the activities that we have selected here are all ‘transitions.’ So in a way we are creating a change-point detection algorithm.\nAn interesting future direction would be attempting to use another architecture such as an RNN that can handle variable length inputs and training it on all the data. However, you would run the risk of the model learning simply that if the observation is long it is most likely one of the four longest classes which would not generalize to a scenario where you were running this model on a real-time-stream of data.\nFiltering activities\nBased on our work from above, let’s subset the data to just be of the activities of interest.\n\n\ndesiredActivities <- c(\n  \"STAND_TO_SIT\", \"SIT_TO_STAND\", \"SIT_TO_LIE\", \n  \"LIE_TO_SIT\", \"STAND_TO_LIE\", \"LIE_TO_STAND\"  \n)\n\nfilteredObservations <- allObservations %>% \n  filter(activityName %in% desiredActivities) %>% \n  mutate(observationId = 1:n())\n\nfilteredObservations %>% paged_table()\n\n\n{\"columns\":[{\"label\":[\"experiment\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"userId\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"activity\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"data\"],\"name\":[4],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"activityName\"],\"name\":[5],\"type\":[\"fctr\"],\"align\":[\"left\"]},{\"label\":[\"observationId\"],\"name\":[6],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"1\",\"3\":\"7\",\"4\":\"<data.frame [160 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"1\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"7\",\"4\":\"<data.frame [206 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"2\"},{\"1\":\"3\",\"2\":\"2\",\"3\":\"7\",\"4\":\"<data.frame [157 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"3\"},{\"1\":\"4\",\"2\":\"2\",\"3\":\"7\",\"4\":\"<data.frame [160 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"4\"},{\"1\":\"5\",\"2\":\"3\",\"3\":\"7\",\"4\":\"<data.frame [142 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"5\"},{\"1\":\"6\",\"2\":\"3\",\"3\":\"7\",\"4\":\"<data.frame [190 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"6\"},{\"1\":\"7\",\"2\":\"4\",\"3\":\"7\",\"4\":\"<data.frame [236 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"7\"},{\"1\":\"8\",\"2\":\"4\",\"3\":\"7\",\"4\":\"<data.frame [178 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"8\"},{\"1\":\"9\",\"2\":\"5\",\"3\":\"7\",\"4\":\"<data.frame [165 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"9\"},{\"1\":\"10\",\"2\":\"5\",\"3\":\"7\",\"4\":\"<data.frame [235 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"10\"},{\"1\":\"11\",\"2\":\"6\",\"3\":\"7\",\"4\":\"<data.frame [185 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"11\"},{\"1\":\"12\",\"2\":\"6\",\"3\":\"7\",\"4\":\"<data.frame [151 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"12\"},{\"1\":\"13\",\"2\":\"7\",\"3\":\"7\",\"4\":\"<data.frame [114 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"13\"},{\"1\":\"14\",\"2\":\"7\",\"3\":\"7\",\"4\":\"<data.frame [90 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"14\"},{\"1\":\"15\",\"2\":\"8\",\"3\":\"7\",\"4\":\"<data.frame [111 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"15\"},{\"1\":\"16\",\"2\":\"8\",\"3\":\"7\",\"4\":\"<data.frame [129 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"16\"},{\"1\":\"17\",\"2\":\"9\",\"3\":\"7\",\"4\":\"<data.frame [118 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"17\"},{\"1\":\"18\",\"2\":\"9\",\"3\":\"7\",\"4\":\"<data.frame [132 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"18\"},{\"1\":\"19\",\"2\":\"10\",\"3\":\"7\",\"4\":\"<data.frame [133 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"19\"},{\"1\":\"20\",\"2\":\"10\",\"3\":\"7\",\"4\":\"<data.frame [112 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"20\"},{\"1\":\"22\",\"2\":\"11\",\"3\":\"7\",\"4\":\"<data.frame [141 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"21\"},{\"1\":\"23\",\"2\":\"11\",\"3\":\"7\",\"4\":\"<data.frame [172 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"22\"},{\"1\":\"24\",\"2\":\"12\",\"3\":\"7\",\"4\":\"<data.frame [175 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"23\"},{\"1\":\"25\",\"2\":\"12\",\"3\":\"7\",\"4\":\"<data.frame [190 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"24\"},{\"1\":\"26\",\"2\":\"13\",\"3\":\"7\",\"4\":\"<data.frame [138 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"25\"},{\"1\":\"27\",\"2\":\"13\",\"3\":\"7\",\"4\":\"<data.frame [250 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"26\"},{\"1\":\"28\",\"2\":\"14\",\"3\":\"7\",\"4\":\"<data.frame [298 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"27\"},{\"1\":\"29\",\"2\":\"14\",\"3\":\"7\",\"4\":\"<data.frame [177 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"28\"},{\"1\":\"30\",\"2\":\"15\",\"3\":\"7\",\"4\":\"<data.frame [144 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"29\"},{\"1\":\"31\",\"2\":\"15\",\"3\":\"7\",\"4\":\"<data.frame [138 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"30\"},{\"1\":\"32\",\"2\":\"16\",\"3\":\"7\",\"4\":\"<data.frame [180 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"31\"},{\"1\":\"33\",\"2\":\"16\",\"3\":\"7\",\"4\":\"<data.frame [175 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"32\"},{\"1\":\"34\",\"2\":\"17\",\"3\":\"7\",\"4\":\"<data.frame [142 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"33\"},{\"1\":\"35\",\"2\":\"17\",\"3\":\"7\",\"4\":\"<data.frame [154 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"34\"},{\"1\":\"36\",\"2\":\"18\",\"3\":\"7\",\"4\":\"<data.frame [307 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"35\"},{\"1\":\"37\",\"2\":\"18\",\"3\":\"7\",\"4\":\"<data.frame [227 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"36\"},{\"1\":\"38\",\"2\":\"19\",\"3\":\"7\",\"4\":\"<data.frame [179 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"37\"},{\"1\":\"39\",\"2\":\"19\",\"3\":\"7\",\"4\":\"<data.frame [163 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"38\"},{\"1\":\"40\",\"2\":\"20\",\"3\":\"7\",\"4\":\"<data.frame [201 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"39\"},{\"1\":\"41\",\"2\":\"20\",\"3\":\"7\",\"4\":\"<data.frame [219 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"40\"},{\"1\":\"42\",\"2\":\"21\",\"3\":\"7\",\"4\":\"<data.frame [177 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"41\"},{\"1\":\"43\",\"2\":\"21\",\"3\":\"7\",\"4\":\"<data.frame [140 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"42\"},{\"1\":\"44\",\"2\":\"22\",\"3\":\"7\",\"4\":\"<data.frame [214 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"43\"},{\"1\":\"45\",\"2\":\"22\",\"3\":\"7\",\"4\":\"<data.frame [119 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"44\"},{\"1\":\"46\",\"2\":\"23\",\"3\":\"7\",\"4\":\"<data.frame [143 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"45\"},{\"1\":\"47\",\"2\":\"23\",\"3\":\"7\",\"4\":\"<data.frame [222 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"46\"},{\"1\":\"48\",\"2\":\"24\",\"3\":\"7\",\"4\":\"<data.frame [171 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"47\"},{\"1\":\"49\",\"2\":\"24\",\"3\":\"7\",\"4\":\"<data.frame [187 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"48\"},{\"1\":\"50\",\"2\":\"25\",\"3\":\"7\",\"4\":\"<data.frame [138 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"49\"},{\"1\":\"51\",\"2\":\"25\",\"3\":\"7\",\"4\":\"<data.frame [219 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"50\"},{\"1\":\"52\",\"2\":\"26\",\"3\":\"7\",\"4\":\"<data.frame [129 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"51\"},{\"1\":\"53\",\"2\":\"26\",\"3\":\"7\",\"4\":\"<data.frame [151 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"52\"},{\"1\":\"54\",\"2\":\"27\",\"3\":\"7\",\"4\":\"<data.frame [168 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"53\"},{\"1\":\"55\",\"2\":\"27\",\"3\":\"7\",\"4\":\"<data.frame [188 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"54\"},{\"1\":\"56\",\"2\":\"28\",\"3\":\"7\",\"4\":\"<data.frame [259 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"55\"},{\"1\":\"57\",\"2\":\"28\",\"3\":\"7\",\"4\":\"<data.frame [162 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"56\"},{\"1\":\"58\",\"2\":\"29\",\"3\":\"7\",\"4\":\"<data.frame [157 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"57\"},{\"1\":\"59\",\"2\":\"29\",\"3\":\"7\",\"4\":\"<data.frame [169 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"58\"},{\"1\":\"60\",\"2\":\"30\",\"3\":\"7\",\"4\":\"<data.frame [113 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"59\"},{\"1\":\"61\",\"2\":\"30\",\"3\":\"7\",\"4\":\"<data.frame [216 × 6]>\",\"5\":\"STAND_TO_SIT\",\"6\":\"60\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"8\",\"4\":\"<data.frame [165 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"61\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"8\",\"4\":\"<data.frame [156 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"62\"},{\"1\":\"3\",\"2\":\"2\",\"3\":\"8\",\"4\":\"<data.frame [142 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"63\"},{\"1\":\"4\",\"2\":\"2\",\"3\":\"8\",\"4\":\"<data.frame [139 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"64\"},{\"1\":\"5\",\"2\":\"3\",\"3\":\"8\",\"4\":\"<data.frame [110 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"65\"},{\"1\":\"6\",\"2\":\"3\",\"3\":\"8\",\"4\":\"<data.frame [183 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"66\"},{\"1\":\"7\",\"2\":\"4\",\"3\":\"8\",\"4\":\"<data.frame [130 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"67\"},{\"1\":\"8\",\"2\":\"4\",\"3\":\"8\",\"4\":\"<data.frame [143 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"68\"},{\"1\":\"9\",\"2\":\"5\",\"3\":\"8\",\"4\":\"<data.frame [126 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"69\"},{\"1\":\"10\",\"2\":\"5\",\"3\":\"8\",\"4\":\"<data.frame [129 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"70\"},{\"1\":\"11\",\"2\":\"6\",\"3\":\"8\",\"4\":\"<data.frame [114 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"71\"},{\"1\":\"12\",\"2\":\"6\",\"3\":\"8\",\"4\":\"<data.frame [132 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"72\"},{\"1\":\"13\",\"2\":\"7\",\"3\":\"8\",\"4\":\"<data.frame [93 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"73\"},{\"1\":\"14\",\"2\":\"7\",\"3\":\"8\",\"4\":\"<data.frame [74 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"74\"},{\"1\":\"15\",\"2\":\"8\",\"3\":\"8\",\"4\":\"<data.frame [74 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"75\"},{\"1\":\"16\",\"2\":\"8\",\"3\":\"8\",\"4\":\"<data.frame [114 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"76\"},{\"1\":\"16\",\"2\":\"8\",\"3\":\"8\",\"4\":\"<data.frame [117 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"77\"},{\"1\":\"17\",\"2\":\"9\",\"3\":\"8\",\"4\":\"<data.frame [106 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"78\"},{\"1\":\"18\",\"2\":\"9\",\"3\":\"8\",\"4\":\"<data.frame [97 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"79\"},{\"1\":\"19\",\"2\":\"10\",\"3\":\"8\",\"4\":\"<data.frame [89 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"80\"},{\"1\":\"20\",\"2\":\"10\",\"3\":\"8\",\"4\":\"<data.frame [78 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"81\"},{\"1\":\"22\",\"2\":\"11\",\"3\":\"8\",\"4\":\"<data.frame [132 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"82\"},{\"1\":\"23\",\"2\":\"11\",\"3\":\"8\",\"4\":\"<data.frame [132 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"83\"},{\"1\":\"24\",\"2\":\"12\",\"3\":\"8\",\"4\":\"<data.frame [104 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"84\"},{\"1\":\"25\",\"2\":\"12\",\"3\":\"8\",\"4\":\"<data.frame [138 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"85\"},{\"1\":\"26\",\"2\":\"13\",\"3\":\"8\",\"4\":\"<data.frame [142 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"86\"},{\"1\":\"27\",\"2\":\"13\",\"3\":\"8\",\"4\":\"<data.frame [159 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"87\"},{\"1\":\"28\",\"2\":\"14\",\"3\":\"8\",\"4\":\"<data.frame [136 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"88\"},{\"1\":\"29\",\"2\":\"14\",\"3\":\"8\",\"4\":\"<data.frame [140 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"89\"},{\"1\":\"30\",\"2\":\"15\",\"3\":\"8\",\"4\":\"<data.frame [110 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"90\"},{\"1\":\"31\",\"2\":\"15\",\"3\":\"8\",\"4\":\"<data.frame [153 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"91\"},{\"1\":\"32\",\"2\":\"16\",\"3\":\"8\",\"4\":\"<data.frame [109 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"92\"},{\"1\":\"33\",\"2\":\"16\",\"3\":\"8\",\"4\":\"<data.frame [109 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"93\"},{\"1\":\"34\",\"2\":\"17\",\"3\":\"8\",\"4\":\"<data.frame [153 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"94\"},{\"1\":\"35\",\"2\":\"17\",\"3\":\"8\",\"4\":\"<data.frame [158 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"95\"},{\"1\":\"36\",\"2\":\"18\",\"3\":\"8\",\"4\":\"<data.frame [182 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"96\"},{\"1\":\"37\",\"2\":\"18\",\"3\":\"8\",\"4\":\"<data.frame [176 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"97\"},{\"1\":\"38\",\"2\":\"19\",\"3\":\"8\",\"4\":\"<data.frame [135 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"98\"},{\"1\":\"39\",\"2\":\"19\",\"3\":\"8\",\"4\":\"<data.frame [122 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"99\"},{\"1\":\"40\",\"2\":\"20\",\"3\":\"8\",\"4\":\"<data.frame [182 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"100\"},{\"1\":\"41\",\"2\":\"20\",\"3\":\"8\",\"4\":\"<data.frame [121 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"101\"},{\"1\":\"42\",\"2\":\"21\",\"3\":\"8\",\"4\":\"<data.frame [146 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"102\"},{\"1\":\"43\",\"2\":\"21\",\"3\":\"8\",\"4\":\"<data.frame [118 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"103\"},{\"1\":\"44\",\"2\":\"22\",\"3\":\"8\",\"4\":\"<data.frame [104 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"104\"},{\"1\":\"45\",\"2\":\"22\",\"3\":\"8\",\"4\":\"<data.frame [106 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"105\"},{\"1\":\"45\",\"2\":\"22\",\"3\":\"8\",\"4\":\"<data.frame [92 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"106\"},{\"1\":\"46\",\"2\":\"23\",\"3\":\"8\",\"4\":\"<data.frame [121 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"107\"},{\"1\":\"47\",\"2\":\"23\",\"3\":\"8\",\"4\":\"<data.frame [150 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"108\"},{\"1\":\"48\",\"2\":\"24\",\"3\":\"8\",\"4\":\"<data.frame [109 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"109\"},{\"1\":\"49\",\"2\":\"24\",\"3\":\"8\",\"4\":\"<data.frame [124 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"110\"},{\"1\":\"50\",\"2\":\"25\",\"3\":\"8\",\"4\":\"<data.frame [152 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"111\"},{\"1\":\"51\",\"2\":\"25\",\"3\":\"8\",\"4\":\"<data.frame [173 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"112\"},{\"1\":\"52\",\"2\":\"26\",\"3\":\"8\",\"4\":\"<data.frame [182 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"113\"},{\"1\":\"53\",\"2\":\"26\",\"3\":\"8\",\"4\":\"<data.frame [102 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"114\"},{\"1\":\"54\",\"2\":\"27\",\"3\":\"8\",\"4\":\"<data.frame [98 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"115\"},{\"1\":\"55\",\"2\":\"27\",\"3\":\"8\",\"4\":\"<data.frame [101 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"116\"},{\"1\":\"56\",\"2\":\"28\",\"3\":\"8\",\"4\":\"<data.frame [138 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"117\"},{\"1\":\"57\",\"2\":\"28\",\"3\":\"8\",\"4\":\"<data.frame [121 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"118\"},{\"1\":\"58\",\"2\":\"29\",\"3\":\"8\",\"4\":\"<data.frame [175 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"119\"},{\"1\":\"59\",\"2\":\"29\",\"3\":\"8\",\"4\":\"<data.frame [134 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"120\"},{\"1\":\"60\",\"2\":\"30\",\"3\":\"8\",\"4\":\"<data.frame [151 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"121\"},{\"1\":\"61\",\"2\":\"30\",\"3\":\"8\",\"4\":\"<data.frame [128 × 6]>\",\"5\":\"SIT_TO_STAND\",\"6\":\"122\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"9\",\"4\":\"<data.frame [192 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"123\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"9\",\"4\":\"<data.frame [237 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"124\"},{\"1\":\"3\",\"2\":\"2\",\"3\":\"9\",\"4\":\"<data.frame [225 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"125\"},{\"1\":\"4\",\"2\":\"2\",\"3\":\"9\",\"4\":\"<data.frame [154 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"126\"},{\"1\":\"5\",\"2\":\"3\",\"3\":\"9\",\"4\":\"<data.frame [151 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"127\"},{\"1\":\"6\",\"2\":\"3\",\"3\":\"9\",\"4\":\"<data.frame [170 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"128\"},{\"1\":\"7\",\"2\":\"4\",\"3\":\"9\",\"4\":\"<data.frame [224 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"129\"},{\"1\":\"8\",\"2\":\"4\",\"3\":\"9\",\"4\":\"<data.frame [235 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"130\"},{\"1\":\"9\",\"2\":\"5\",\"3\":\"9\",\"4\":\"<data.frame [220 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"131\"},{\"1\":\"10\",\"2\":\"5\",\"3\":\"9\",\"4\":\"<data.frame [259 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"132\"},{\"1\":\"11\",\"2\":\"6\",\"3\":\"9\",\"4\":\"<data.frame [158 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"133\"},{\"1\":\"12\",\"2\":\"6\",\"3\":\"9\",\"4\":\"<data.frame [158 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"134\"},{\"1\":\"13\",\"2\":\"7\",\"3\":\"9\",\"4\":\"<data.frame [257 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"135\"},{\"1\":\"14\",\"2\":\"7\",\"3\":\"9\",\"4\":\"<data.frame [189 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"136\"},{\"1\":\"15\",\"2\":\"8\",\"3\":\"9\",\"4\":\"<data.frame [166 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"137\"},{\"1\":\"16\",\"2\":\"8\",\"3\":\"9\",\"4\":\"<data.frame [147 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"138\"},{\"1\":\"17\",\"2\":\"9\",\"3\":\"9\",\"4\":\"<data.frame [204 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"139\"},{\"1\":\"18\",\"2\":\"9\",\"3\":\"9\",\"4\":\"<data.frame [161 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"140\"},{\"1\":\"19\",\"2\":\"10\",\"3\":\"9\",\"4\":\"<data.frame [181 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"141\"},{\"1\":\"20\",\"2\":\"10\",\"3\":\"9\",\"4\":\"<data.frame [159 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"142\"},{\"1\":\"22\",\"2\":\"11\",\"3\":\"9\",\"4\":\"<data.frame [185 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"143\"},{\"1\":\"23\",\"2\":\"11\",\"3\":\"9\",\"4\":\"<data.frame [198 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"144\"},{\"1\":\"24\",\"2\":\"12\",\"3\":\"9\",\"4\":\"<data.frame [173 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"145\"},{\"1\":\"25\",\"2\":\"12\",\"3\":\"9\",\"4\":\"<data.frame [182 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"146\"},{\"1\":\"26\",\"2\":\"13\",\"3\":\"9\",\"4\":\"<data.frame [226 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"147\"},{\"1\":\"27\",\"2\":\"13\",\"3\":\"9\",\"4\":\"<data.frame [212 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"148\"},{\"1\":\"28\",\"2\":\"14\",\"3\":\"9\",\"4\":\"<data.frame [289 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"149\"},{\"1\":\"29\",\"2\":\"14\",\"3\":\"9\",\"4\":\"<data.frame [172 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"150\"},{\"1\":\"30\",\"2\":\"15\",\"3\":\"9\",\"4\":\"<data.frame [202 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"151\"},{\"1\":\"31\",\"2\":\"15\",\"3\":\"9\",\"4\":\"<data.frame [205 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"152\"},{\"1\":\"32\",\"2\":\"16\",\"3\":\"9\",\"4\":\"<data.frame [195 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"153\"},{\"1\":\"33\",\"2\":\"16\",\"3\":\"9\",\"4\":\"<data.frame [181 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"154\"},{\"1\":\"34\",\"2\":\"17\",\"3\":\"9\",\"4\":\"<data.frame [312 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"155\"},{\"1\":\"35\",\"2\":\"17\",\"3\":\"9\",\"4\":\"<data.frame [202 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"156\"},{\"1\":\"36\",\"2\":\"18\",\"3\":\"9\",\"4\":\"<data.frame [251 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"157\"},{\"1\":\"37\",\"2\":\"18\",\"3\":\"9\",\"4\":\"<data.frame [199 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"158\"},{\"1\":\"38\",\"2\":\"19\",\"3\":\"9\",\"4\":\"<data.frame [176 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"159\"},{\"1\":\"39\",\"2\":\"19\",\"3\":\"9\",\"4\":\"<data.frame [203 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"160\"},{\"1\":\"40\",\"2\":\"20\",\"3\":\"9\",\"4\":\"<data.frame [297 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"161\"},{\"1\":\"41\",\"2\":\"20\",\"3\":\"9\",\"4\":\"<data.frame [281 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"162\"},{\"1\":\"42\",\"2\":\"21\",\"3\":\"9\",\"4\":\"<data.frame [202 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"163\"},{\"1\":\"43\",\"2\":\"21\",\"3\":\"9\",\"4\":\"<data.frame [212 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"164\"},{\"1\":\"44\",\"2\":\"22\",\"3\":\"9\",\"4\":\"<data.frame [136 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"165\"},{\"1\":\"45\",\"2\":\"22\",\"3\":\"9\",\"4\":\"<data.frame [159 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"166\"},{\"1\":\"46\",\"2\":\"23\",\"3\":\"9\",\"4\":\"<data.frame [213 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"167\"},{\"1\":\"47\",\"2\":\"23\",\"3\":\"9\",\"4\":\"<data.frame [219 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"168\"},{\"1\":\"48\",\"2\":\"24\",\"3\":\"9\",\"4\":\"<data.frame [234 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"169\"},{\"1\":\"49\",\"2\":\"24\",\"3\":\"9\",\"4\":\"<data.frame [206 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"170\"},{\"1\":\"50\",\"2\":\"25\",\"3\":\"9\",\"4\":\"<data.frame [199 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"171\"},{\"1\":\"51\",\"2\":\"25\",\"3\":\"9\",\"4\":\"<data.frame [181 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"172\"},{\"1\":\"52\",\"2\":\"26\",\"3\":\"9\",\"4\":\"<data.frame [185 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"173\"},{\"1\":\"53\",\"2\":\"26\",\"3\":\"9\",\"4\":\"<data.frame [213 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"174\"},{\"1\":\"54\",\"2\":\"27\",\"3\":\"9\",\"4\":\"<data.frame [227 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"175\"},{\"1\":\"55\",\"2\":\"27\",\"3\":\"9\",\"4\":\"<data.frame [225 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"176\"},{\"1\":\"56\",\"2\":\"28\",\"3\":\"9\",\"4\":\"<data.frame [226 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"177\"},{\"1\":\"57\",\"2\":\"28\",\"3\":\"9\",\"4\":\"<data.frame [201 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"178\"},{\"1\":\"58\",\"2\":\"29\",\"3\":\"9\",\"4\":\"<data.frame [326 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"179\"},{\"1\":\"59\",\"2\":\"29\",\"3\":\"9\",\"4\":\"<data.frame [201 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"180\"},{\"1\":\"60\",\"2\":\"30\",\"3\":\"9\",\"4\":\"<data.frame [183 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"181\"},{\"1\":\"61\",\"2\":\"30\",\"3\":\"9\",\"4\":\"<data.frame [292 × 6]>\",\"5\":\"SIT_TO_LIE\",\"6\":\"182\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"10\",\"4\":\"<data.frame [197 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"183\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"10\",\"4\":\"<data.frame [184 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"184\"},{\"1\":\"3\",\"2\":\"2\",\"3\":\"10\",\"4\":\"<data.frame [278 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"185\"},{\"1\":\"4\",\"2\":\"2\",\"3\":\"10\",\"4\":\"<data.frame [142 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"186\"},{\"1\":\"5\",\"2\":\"3\",\"3\":\"10\",\"4\":\"<data.frame [224 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"187\"},{\"1\":\"6\",\"2\":\"3\",\"3\":\"10\",\"4\":\"<data.frame [197 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"188\"},{\"1\":\"7\",\"2\":\"4\",\"3\":\"10\",\"4\":\"<data.frame [215 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"189\"},{\"1\":\"8\",\"2\":\"4\",\"3\":\"10\",\"4\":\"<data.frame [210 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"190\"},{\"1\":\"9\",\"2\":\"5\",\"3\":\"10\",\"4\":\"<data.frame [256 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"191\"},{\"1\":\"10\",\"2\":\"5\",\"3\":\"10\",\"4\":\"<data.frame [203 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"192\"},{\"1\":\"11\",\"2\":\"6\",\"3\":\"10\",\"4\":\"<data.frame [173 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"193\"},{\"1\":\"12\",\"2\":\"6\",\"3\":\"10\",\"4\":\"<data.frame [165 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"194\"},{\"1\":\"13\",\"2\":\"7\",\"3\":\"10\",\"4\":\"<data.frame [146 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"195\"},{\"1\":\"14\",\"2\":\"7\",\"3\":\"10\",\"4\":\"<data.frame [123 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"196\"},{\"1\":\"15\",\"2\":\"8\",\"3\":\"10\",\"4\":\"<data.frame [160 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"197\"},{\"1\":\"16\",\"2\":\"8\",\"3\":\"10\",\"4\":\"<data.frame [136 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"198\"},{\"1\":\"16\",\"2\":\"8\",\"3\":\"10\",\"4\":\"<data.frame [165 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"199\"},{\"1\":\"17\",\"2\":\"9\",\"3\":\"10\",\"4\":\"<data.frame [177 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"200\"},{\"1\":\"18\",\"2\":\"9\",\"3\":\"10\",\"4\":\"<data.frame [128 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"201\"},{\"1\":\"19\",\"2\":\"10\",\"3\":\"10\",\"4\":\"<data.frame [169 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"202\"},{\"1\":\"20\",\"2\":\"10\",\"3\":\"10\",\"4\":\"<data.frame [114 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"203\"},{\"1\":\"22\",\"2\":\"11\",\"3\":\"10\",\"4\":\"<data.frame [179 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"204\"},{\"1\":\"23\",\"2\":\"11\",\"3\":\"10\",\"4\":\"<data.frame [222 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"205\"},{\"1\":\"24\",\"2\":\"12\",\"3\":\"10\",\"4\":\"<data.frame [178 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"206\"},{\"1\":\"25\",\"2\":\"12\",\"3\":\"10\",\"4\":\"<data.frame [179 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"207\"},{\"1\":\"26\",\"2\":\"13\",\"3\":\"10\",\"4\":\"<data.frame [190 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"208\"},{\"1\":\"27\",\"2\":\"13\",\"3\":\"10\",\"4\":\"<data.frame [193 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"209\"},{\"1\":\"28\",\"2\":\"14\",\"3\":\"10\",\"4\":\"<data.frame [226 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"210\"},{\"1\":\"29\",\"2\":\"14\",\"3\":\"10\",\"4\":\"<data.frame [147 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"211\"},{\"1\":\"30\",\"2\":\"15\",\"3\":\"10\",\"4\":\"<data.frame [166 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"212\"},{\"1\":\"31\",\"2\":\"15\",\"3\":\"10\",\"4\":\"<data.frame [169 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"213\"},{\"1\":\"32\",\"2\":\"16\",\"3\":\"10\",\"4\":\"<data.frame [184 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"214\"},{\"1\":\"33\",\"2\":\"16\",\"3\":\"10\",\"4\":\"<data.frame [183 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"215\"},{\"1\":\"34\",\"2\":\"17\",\"3\":\"10\",\"4\":\"<data.frame [211 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"216\"},{\"1\":\"35\",\"2\":\"17\",\"3\":\"10\",\"4\":\"<data.frame [271 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"217\"},{\"1\":\"36\",\"2\":\"18\",\"3\":\"10\",\"4\":\"<data.frame [245 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"218\"},{\"1\":\"37\",\"2\":\"18\",\"3\":\"10\",\"4\":\"<data.frame [165 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"219\"},{\"1\":\"38\",\"2\":\"19\",\"3\":\"10\",\"4\":\"<data.frame [197 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"220\"},{\"1\":\"39\",\"2\":\"19\",\"3\":\"10\",\"4\":\"<data.frame [150 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"221\"},{\"1\":\"40\",\"2\":\"20\",\"3\":\"10\",\"4\":\"<data.frame [156 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"222\"},{\"1\":\"41\",\"2\":\"20\",\"3\":\"10\",\"4\":\"<data.frame [192 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"223\"},{\"1\":\"42\",\"2\":\"21\",\"3\":\"10\",\"4\":\"<data.frame [204 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"224\"},{\"1\":\"43\",\"2\":\"21\",\"3\":\"10\",\"4\":\"<data.frame [196 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"225\"},{\"1\":\"44\",\"2\":\"22\",\"3\":\"10\",\"4\":\"<data.frame [185 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"226\"},{\"1\":\"45\",\"2\":\"22\",\"3\":\"10\",\"4\":\"<data.frame [161 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"227\"},{\"1\":\"45\",\"2\":\"22\",\"3\":\"10\",\"4\":\"<data.frame [157 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"228\"},{\"1\":\"46\",\"2\":\"23\",\"3\":\"10\",\"4\":\"<data.frame [174 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"229\"},{\"1\":\"47\",\"2\":\"23\",\"3\":\"10\",\"4\":\"<data.frame [171 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"230\"},{\"1\":\"48\",\"2\":\"24\",\"3\":\"10\",\"4\":\"<data.frame [210 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"231\"},{\"1\":\"49\",\"2\":\"24\",\"3\":\"10\",\"4\":\"<data.frame [180 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"232\"},{\"1\":\"50\",\"2\":\"25\",\"3\":\"10\",\"4\":\"<data.frame [280 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"233\"},{\"1\":\"51\",\"2\":\"25\",\"3\":\"10\",\"4\":\"<data.frame [215 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"234\"},{\"1\":\"52\",\"2\":\"26\",\"3\":\"10\",\"4\":\"<data.frame [162 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"235\"},{\"1\":\"53\",\"2\":\"26\",\"3\":\"10\",\"4\":\"<data.frame [174 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"236\"},{\"1\":\"54\",\"2\":\"27\",\"3\":\"10\",\"4\":\"<data.frame [154 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"237\"},{\"1\":\"55\",\"2\":\"27\",\"3\":\"10\",\"4\":\"<data.frame [149 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"238\"},{\"1\":\"58\",\"2\":\"29\",\"3\":\"10\",\"4\":\"<data.frame [237 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"239\"},{\"1\":\"59\",\"2\":\"29\",\"3\":\"10\",\"4\":\"<data.frame [170 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"240\"},{\"1\":\"60\",\"2\":\"30\",\"3\":\"10\",\"4\":\"<data.frame [167 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"241\"},{\"1\":\"61\",\"2\":\"30\",\"3\":\"10\",\"4\":\"<data.frame [239 × 6]>\",\"5\":\"LIE_TO_SIT\",\"6\":\"242\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"11\",\"4\":\"<data.frame [288 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"243\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"11\",\"4\":\"<data.frame [268 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"244\"},{\"1\":\"3\",\"2\":\"2\",\"3\":\"11\",\"4\":\"<data.frame [418 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"245\"},{\"1\":\"4\",\"2\":\"2\",\"3\":\"11\",\"4\":\"<data.frame [205 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"246\"},{\"1\":\"5\",\"2\":\"3\",\"3\":\"11\",\"4\":\"<data.frame [216 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"247\"},{\"1\":\"6\",\"2\":\"3\",\"3\":\"11\",\"4\":\"<data.frame [235 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"248\"},{\"1\":\"7\",\"2\":\"4\",\"3\":\"11\",\"4\":\"<data.frame [332 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"249\"},{\"1\":\"8\",\"2\":\"4\",\"3\":\"11\",\"4\":\"<data.frame [319 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"250\"},{\"1\":\"9\",\"2\":\"5\",\"3\":\"11\",\"4\":\"<data.frame [346 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"251\"},{\"1\":\"10\",\"2\":\"5\",\"3\":\"11\",\"4\":\"<data.frame [315 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"252\"},{\"1\":\"11\",\"2\":\"6\",\"3\":\"11\",\"4\":\"<data.frame [313 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"253\"},{\"1\":\"12\",\"2\":\"6\",\"3\":\"11\",\"4\":\"<data.frame [280 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"254\"},{\"1\":\"13\",\"2\":\"7\",\"3\":\"11\",\"4\":\"<data.frame [206 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"255\"},{\"1\":\"14\",\"2\":\"7\",\"3\":\"11\",\"4\":\"<data.frame [200 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"256\"},{\"1\":\"15\",\"2\":\"8\",\"3\":\"11\",\"4\":\"<data.frame [211 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"257\"},{\"1\":\"16\",\"2\":\"8\",\"3\":\"11\",\"4\":\"<data.frame [196 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"258\"},{\"1\":\"17\",\"2\":\"9\",\"3\":\"11\",\"4\":\"<data.frame [188 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"259\"},{\"1\":\"18\",\"2\":\"9\",\"3\":\"11\",\"4\":\"<data.frame [259 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"260\"},{\"1\":\"19\",\"2\":\"10\",\"3\":\"11\",\"4\":\"<data.frame [229 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"261\"},{\"1\":\"20\",\"2\":\"10\",\"3\":\"11\",\"4\":\"<data.frame [153 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"262\"},{\"1\":\"22\",\"2\":\"11\",\"3\":\"11\",\"4\":\"<data.frame [193 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"263\"},{\"1\":\"23\",\"2\":\"11\",\"3\":\"11\",\"4\":\"<data.frame [187 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"264\"},{\"1\":\"24\",\"2\":\"12\",\"3\":\"11\",\"4\":\"<data.frame [205 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"265\"},{\"1\":\"25\",\"2\":\"12\",\"3\":\"11\",\"4\":\"<data.frame [167 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"266\"},{\"1\":\"26\",\"2\":\"13\",\"3\":\"11\",\"4\":\"<data.frame [195 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"267\"},{\"1\":\"27\",\"2\":\"13\",\"3\":\"11\",\"4\":\"<data.frame [191 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"268\"},{\"1\":\"28\",\"2\":\"14\",\"3\":\"11\",\"4\":\"<data.frame [342 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"269\"},{\"1\":\"29\",\"2\":\"14\",\"3\":\"11\",\"4\":\"<data.frame [264 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"270\"},{\"1\":\"30\",\"2\":\"15\",\"3\":\"11\",\"4\":\"<data.frame [194 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"271\"},{\"1\":\"31\",\"2\":\"15\",\"3\":\"11\",\"4\":\"<data.frame [263 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"272\"},{\"1\":\"32\",\"2\":\"16\",\"3\":\"11\",\"4\":\"<data.frame [235 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"273\"},{\"1\":\"33\",\"2\":\"16\",\"3\":\"11\",\"4\":\"<data.frame [192 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"274\"},{\"1\":\"34\",\"2\":\"17\",\"3\":\"11\",\"4\":\"<data.frame [238 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"275\"},{\"1\":\"35\",\"2\":\"17\",\"3\":\"11\",\"4\":\"<data.frame [336 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"276\"},{\"1\":\"36\",\"2\":\"18\",\"3\":\"11\",\"4\":\"<data.frame [297 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"277\"},{\"1\":\"37\",\"2\":\"18\",\"3\":\"11\",\"4\":\"<data.frame [244 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"278\"},{\"1\":\"38\",\"2\":\"19\",\"3\":\"11\",\"4\":\"<data.frame [256 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"279\"},{\"1\":\"39\",\"2\":\"19\",\"3\":\"11\",\"4\":\"<data.frame [320 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"280\"},{\"1\":\"40\",\"2\":\"20\",\"3\":\"11\",\"4\":\"<data.frame [425 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"281\"},{\"1\":\"41\",\"2\":\"20\",\"3\":\"11\",\"4\":\"<data.frame [365 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"282\"},{\"1\":\"42\",\"2\":\"21\",\"3\":\"11\",\"4\":\"<data.frame [313 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"283\"},{\"1\":\"43\",\"2\":\"21\",\"3\":\"11\",\"4\":\"<data.frame [274 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"284\"},{\"1\":\"44\",\"2\":\"22\",\"3\":\"11\",\"4\":\"<data.frame [173 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"285\"},{\"1\":\"45\",\"2\":\"22\",\"3\":\"11\",\"4\":\"<data.frame [139 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"286\"},{\"1\":\"46\",\"2\":\"23\",\"3\":\"11\",\"4\":\"<data.frame [253 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"287\"},{\"1\":\"47\",\"2\":\"23\",\"3\":\"11\",\"4\":\"<data.frame [260 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"288\"},{\"1\":\"48\",\"2\":\"24\",\"3\":\"11\",\"4\":\"<data.frame [494 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"289\"},{\"1\":\"49\",\"2\":\"24\",\"3\":\"11\",\"4\":\"<data.frame [248 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"290\"},{\"1\":\"50\",\"2\":\"25\",\"3\":\"11\",\"4\":\"<data.frame [185 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"291\"},{\"1\":\"51\",\"2\":\"25\",\"3\":\"11\",\"4\":\"<data.frame [241 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"292\"},{\"1\":\"52\",\"2\":\"26\",\"3\":\"11\",\"4\":\"<data.frame [178 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"293\"},{\"1\":\"53\",\"2\":\"26\",\"3\":\"11\",\"4\":\"<data.frame [167 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"294\"},{\"1\":\"54\",\"2\":\"27\",\"3\":\"11\",\"4\":\"<data.frame [216 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"295\"},{\"1\":\"55\",\"2\":\"27\",\"3\":\"11\",\"4\":\"<data.frame [165 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"296\"},{\"1\":\"58\",\"2\":\"29\",\"3\":\"11\",\"4\":\"<data.frame [234 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"297\"},{\"1\":\"59\",\"2\":\"29\",\"3\":\"11\",\"4\":\"<data.frame [187 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"298\"},{\"1\":\"60\",\"2\":\"30\",\"3\":\"11\",\"4\":\"<data.frame [156 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"299\"},{\"1\":\"61\",\"2\":\"30\",\"3\":\"11\",\"4\":\"<data.frame [249 × 6]>\",\"5\":\"STAND_TO_LIE\",\"6\":\"300\"},{\"1\":\"1\",\"2\":\"1\",\"3\":\"12\",\"4\":\"<data.frame [191 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"301\"},{\"1\":\"2\",\"2\":\"1\",\"3\":\"12\",\"4\":\"<data.frame [242 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"302\"},{\"1\":\"3\",\"2\":\"2\",\"3\":\"12\",\"4\":\"<data.frame [181 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"303\"},{\"1\":\"4\",\"2\":\"2\",\"3\":\"12\",\"4\":\"<data.frame [181 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"304\"},{\"1\":\"5\",\"2\":\"3\",\"3\":\"12\",\"4\":\"<data.frame [184 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"305\"},{\"1\":\"6\",\"2\":\"3\",\"3\":\"12\",\"4\":\"<data.frame [167 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"306\"},{\"1\":\"7\",\"2\":\"4\",\"3\":\"12\",\"4\":\"<data.frame [166 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"307\"},{\"1\":\"8\",\"2\":\"4\",\"3\":\"12\",\"4\":\"<data.frame [171 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"308\"},{\"1\":\"9\",\"2\":\"5\",\"3\":\"12\",\"4\":\"<data.frame [173 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"309\"},{\"1\":\"10\",\"2\":\"5\",\"3\":\"12\",\"4\":\"<data.frame [142 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"310\"},{\"1\":\"11\",\"2\":\"6\",\"3\":\"12\",\"4\":\"<data.frame [186 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"311\"},{\"1\":\"12\",\"2\":\"6\",\"3\":\"12\",\"4\":\"<data.frame [254 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"312\"},{\"1\":\"13\",\"2\":\"7\",\"3\":\"12\",\"4\":\"<data.frame [206 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"313\"},{\"1\":\"14\",\"2\":\"7\",\"3\":\"12\",\"4\":\"<data.frame [170 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"314\"},{\"1\":\"15\",\"2\":\"8\",\"3\":\"12\",\"4\":\"<data.frame [173 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"315\"},{\"1\":\"17\",\"2\":\"9\",\"3\":\"12\",\"4\":\"<data.frame [168 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"316\"},{\"1\":\"18\",\"2\":\"9\",\"3\":\"12\",\"4\":\"<data.frame [149 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"317\"},{\"1\":\"19\",\"2\":\"10\",\"3\":\"12\",\"4\":\"<data.frame [118 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"318\"},{\"1\":\"20\",\"2\":\"10\",\"3\":\"12\",\"4\":\"<data.frame [137 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"319\"},{\"1\":\"22\",\"2\":\"11\",\"3\":\"12\",\"4\":\"<data.frame [139 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"320\"},{\"1\":\"23\",\"2\":\"11\",\"3\":\"12\",\"4\":\"<data.frame [196 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"321\"},{\"1\":\"24\",\"2\":\"12\",\"3\":\"12\",\"4\":\"<data.frame [207 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"322\"},{\"1\":\"25\",\"2\":\"12\",\"3\":\"12\",\"4\":\"<data.frame [188 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"323\"},{\"1\":\"26\",\"2\":\"13\",\"3\":\"12\",\"4\":\"<data.frame [259 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"324\"},{\"1\":\"27\",\"2\":\"13\",\"3\":\"12\",\"4\":\"<data.frame [197 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"325\"},{\"1\":\"28\",\"2\":\"14\",\"3\":\"12\",\"4\":\"<data.frame [289 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"326\"},{\"1\":\"29\",\"2\":\"14\",\"3\":\"12\",\"4\":\"<data.frame [197 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"327\"},{\"1\":\"30\",\"2\":\"15\",\"3\":\"12\",\"4\":\"<data.frame [201 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"328\"},{\"1\":\"31\",\"2\":\"15\",\"3\":\"12\",\"4\":\"<data.frame [159 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"329\"},{\"1\":\"32\",\"2\":\"16\",\"3\":\"12\",\"4\":\"<data.frame [146 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"330\"},{\"1\":\"33\",\"2\":\"16\",\"3\":\"12\",\"4\":\"<data.frame [156 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"331\"},{\"1\":\"34\",\"2\":\"17\",\"3\":\"12\",\"4\":\"<data.frame [200 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"332\"},{\"1\":\"35\",\"2\":\"17\",\"3\":\"12\",\"4\":\"<data.frame [238 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"333\"},{\"1\":\"36\",\"2\":\"18\",\"3\":\"12\",\"4\":\"<data.frame [206 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"334\"},{\"1\":\"37\",\"2\":\"18\",\"3\":\"12\",\"4\":\"<data.frame [192 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"335\"},{\"1\":\"38\",\"2\":\"19\",\"3\":\"12\",\"4\":\"<data.frame [161 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"336\"},{\"1\":\"39\",\"2\":\"19\",\"3\":\"12\",\"4\":\"<data.frame [140 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"337\"},{\"1\":\"40\",\"2\":\"20\",\"3\":\"12\",\"4\":\"<data.frame [172 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"338\"},{\"1\":\"41\",\"2\":\"20\",\"3\":\"12\",\"4\":\"<data.frame [158 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"339\"},{\"1\":\"42\",\"2\":\"21\",\"3\":\"12\",\"4\":\"<data.frame [206 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"340\"},{\"1\":\"43\",\"2\":\"21\",\"3\":\"12\",\"4\":\"<data.frame [196 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"341\"},{\"1\":\"44\",\"2\":\"22\",\"3\":\"12\",\"4\":\"<data.frame [257 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"342\"},{\"1\":\"46\",\"2\":\"23\",\"3\":\"12\",\"4\":\"<data.frame [176 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"343\"},{\"1\":\"47\",\"2\":\"23\",\"3\":\"12\",\"4\":\"<data.frame [161 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"344\"},{\"1\":\"48\",\"2\":\"24\",\"3\":\"12\",\"4\":\"<data.frame [322 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"345\"},{\"1\":\"49\",\"2\":\"24\",\"3\":\"12\",\"4\":\"<data.frame [196 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"346\"},{\"1\":\"50\",\"2\":\"25\",\"3\":\"12\",\"4\":\"<data.frame [323 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"347\"},{\"1\":\"51\",\"2\":\"25\",\"3\":\"12\",\"4\":\"<data.frame [187 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"348\"},{\"1\":\"52\",\"2\":\"26\",\"3\":\"12\",\"4\":\"<data.frame [244 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"349\"},{\"1\":\"53\",\"2\":\"26\",\"3\":\"12\",\"4\":\"<data.frame [183 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"350\"},{\"1\":\"54\",\"2\":\"27\",\"3\":\"12\",\"4\":\"<data.frame [125 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"351\"},{\"1\":\"55\",\"2\":\"27\",\"3\":\"12\",\"4\":\"<data.frame [153 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"352\"},{\"1\":\"56\",\"2\":\"28\",\"3\":\"12\",\"4\":\"<data.frame [181 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"353\"},{\"1\":\"57\",\"2\":\"28\",\"3\":\"12\",\"4\":\"<data.frame [167 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"354\"},{\"1\":\"58\",\"2\":\"29\",\"3\":\"12\",\"4\":\"<data.frame [179 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"355\"},{\"1\":\"59\",\"2\":\"29\",\"3\":\"12\",\"4\":\"<data.frame [145 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"356\"},{\"1\":\"60\",\"2\":\"30\",\"3\":\"12\",\"4\":\"<data.frame [159 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"357\"},{\"1\":\"61\",\"2\":\"30\",\"3\":\"12\",\"4\":\"<data.frame [147 × 6]>\",\"5\":\"LIE_TO_STAND\",\"6\":\"358\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  \n\nSo after our aggressive pruning of the data we will have a respectable amount of data left upon which our model can learn.\nTraining/testing split\nBefore we go any further into exploring the data for our model, in an attempt to be as fair as possible with our performance measures, we need to split the data into a train and test set. Since each user performed all activities just once (with the exception of one who only did 10 of the 12 activities) by splitting on userId we will ensure that our model sees new people exclusively when we test it.\n\n\n# get all users\nuserIds <- allObservations$userId %>% unique()\n\n# randomly choose 24 (80% of 30 individuals) for training\nset.seed(42) # seed for reproducibility\ntrainIds <- sample(userIds, size = 24)\n\n# set the rest of the users to the testing set\ntestIds <- setdiff(userIds,trainIds)\n\n# filter data. \ntrainData <- filteredObservations %>% \n  filter(userId %in% trainIds)\n\ntestData <- filteredObservations %>% \n  filter(userId %in% testIds)\n\nVisualizing activities\nNow that we have trimmed our data by removing activities and splitting off a test set, we can actually visualize the data for each class to see if there’s any immediately discernible shape that our model may be able to pick up on.\nFirst let’s unpack our data from its dataframe of one-row-per-observation to a tidy version of all the observations.\n\n\nunpackedObs <- 1:nrow(trainData) %>% \n  map_df(function(rowNum){\n    dataRow <- trainData[rowNum, ]\n    dataRow$data[[1]] %>% \n      mutate(\n        activityName = dataRow$activityName, \n        observationId = dataRow$observationId,\n        time = 1:n() )\n  }) %>% \n  gather(reading, value, -time, -activityName, -observationId) %>% \n  separate(reading, into = c(\"type\", \"direction\"), sep = \"_\") %>% \n  mutate(type = ifelse(type == \"a\", \"acceleration\", \"gyro\"))\n\nNow we have an unpacked set of our observations, let’s visualize them!\n\n\nunpackedObs %>% \n  ggplot(aes(x = time, y = value, color = direction)) +\n  geom_line(alpha = 0.2) +\n  geom_smooth(se = FALSE, alpha = 0.7, size = 0.5) +\n  facet_grid(type ~ activityName, scales = \"free_y\") +\n  theme_minimal() +\n  theme( axis.text.x = element_blank() )\n\n\nSo at least in the accelerometer data patterns definitely emerge. One would imagine that the model may have trouble with the differences between LIE_TO_SIT and LIE_TO_STAND as they have a similar profile on average. The same goes for SIT_TO_STAND and STAND_TO_SIT.\nPreprocessing\nBefore we can train the neural network, we need to take a couple of steps to preprocess the data.\nPadding observations\nFirst we will decide what length to pad (and truncate) our sequences to by finding what the 98th percentile length is. By not using the very longest observation length this will help us avoid extra-long outlier recordings messing up the padding.\n\n\npadSize <- trainData$data %>% \n  map_int(nrow) %>% \n  quantile(p = 0.98) %>% \n  ceiling()\npadSize\n\n98% \n334 \n\nNow we simply need to convert our list of observations to matrices, then use the super handy pad_sequences() function in Keras to pad all observations and turn them into a 3D tensor for us.\n\n\nconvertToTensor <- . %>% \n  map(as.matrix) %>% \n  pad_sequences(maxlen = padSize)\n\ntrainObs <- trainData$data %>% convertToTensor()\ntestObs <- testData$data %>% convertToTensor()\n  \ndim(trainObs)\n\n[1] 286 334   6\n\nWonderful, we now have our data in a nice neural-network-friendly format of a 3D tensor with dimensions (<num obs>, <sequence length>, <channels>).\n\nIf we were working with a video instead of sensor data, this would be a 4D Tensor. If we were using FMRI data, this could be a 5D tensor!\nOne-hot encoding\nThere’s one last thing we need to do before we can train our model, and that is turn our observation classes from integers into one-hot, or dummy encoded, vectors. Luckily, again Keras has supplied us with a very helpful function to do just this.\n\n\noneHotClasses <- . %>% \n  {. - 7} %>%        # bring integers down to 0-6 from 7-12\n  to_categorical() # One-hot encode\n\ntrainY <- trainData$activity %>% oneHotClasses()\ntestY <- testData$activity %>% oneHotClasses()\n\nModeling\nArchitecture\nSince we have temporally dense time-series data we will make use of 1D convolutional layers. With temporally-dense data, an RNN has to learn very long dependencies in order to pick up on patterns, CNNs can simply stack a few convolutional layers to build pattern representations of substantial length. Since we are also simply looking for a single classification of activity for each observation, we can just use pooling to ‘summarize’ the CNNs view of the data into a dense layer.\n\nFor more information on the differences between the two architectures for sequence data see (Graves 2012) and (LeCun, Bengio, and Hinton 2015).\nIn addition to stacking two layer_conv_1d() layers, we will use batch norm and dropout (the spatial variant(Tompson et al. 2014) on the convolutional layers and standard on the dense) to regularize the network.\n\n\ninput_shape <- dim(trainObs)[-1]\nnum_classes <- dim(trainY)[2]\n\nfilters <- 24     # number of convolutional filters to learn\nkernel_size <- 8  # how many time-steps each conv layer sees.\ndense_size <- 48  # size of our penultimate dense layer. \n\n# Initialize model\nmodel <- keras_model_sequential()\nmodel %>% \n  layer_conv_1d(\n    filters = filters,\n    kernel_size = kernel_size, \n    input_shape = input_shape,\n    padding = \"valid\", \n    activation = \"relu\"\n  ) %>%\n  layer_batch_normalization() %>%\n  layer_spatial_dropout_1d(0.15) %>% \n  layer_conv_1d(\n    filters = filters/2,\n    kernel_size = kernel_size,\n    activation = \"relu\",\n  ) %>%\n  # Apply average pooling:\n  layer_global_average_pooling_1d() %>% \n  layer_batch_normalization() %>%\n  layer_dropout(0.2) %>% \n  layer_dense(\n    dense_size,\n    activation = \"relu\"\n  ) %>% \n  layer_batch_normalization() %>%\n  layer_dropout(0.25) %>% \n  layer_dense(\n    num_classes, \n    activation = \"softmax\",\n    name = \"dense_output\"\n  ) \n\nsummary(model)\n\n______________________________________________________________________\nLayer (type)                   Output Shape                Param #    \n======================================================================\nconv1d_1 (Conv1D)              (None, 327, 24)             1176       \n______________________________________________________________________\nbatch_normalization_1 (BatchNo (None, 327, 24)             96         \n______________________________________________________________________\nspatial_dropout1d_1 (SpatialDr (None, 327, 24)             0          \n______________________________________________________________________\nconv1d_2 (Conv1D)              (None, 320, 12)             2316       \n______________________________________________________________________\nglobal_average_pooling1d_1 (Gl (None, 12)                  0          \n______________________________________________________________________\nbatch_normalization_2 (BatchNo (None, 12)                  48         \n______________________________________________________________________\ndropout_1 (Dropout)            (None, 12)                  0          \n______________________________________________________________________\ndense_1 (Dense)                (None, 48)                  624        \n______________________________________________________________________\nbatch_normalization_3 (BatchNo (None, 48)                  192        \n______________________________________________________________________\ndropout_2 (Dropout)            (None, 48)                  0          \n______________________________________________________________________\ndense_output (Dense)           (None, 6)                   294        \n======================================================================\nTotal params: 4,746\nTrainable params: 4,578\nNon-trainable params: 168\n______________________________________________________________________\n\nTraining\nNow we can train the model using our test and training data. Note that we use callback_model_checkpoint() to ensure that we save only the best variation of the model (desirable since at some point in training the model may begin to overfit or otherwise stop improving).\n\n\n# Compile model\nmodel %>% compile(\n  loss = \"categorical_crossentropy\",\n  optimizer = \"rmsprop\",\n  metrics = \"accuracy\"\n)\n\ntrainHistory <- model %>%\n  fit(\n    x = trainObs, y = trainY,\n    epochs = 350,\n    validation_data = list(testObs, testY),\n    callbacks = list(\n      callback_model_checkpoint(\"best_model.h5\", \n                                save_best_only = TRUE)\n    )\n  )\n\n\n\n\n\n\n\n\n\n\nThe model is learning something! We get a respectable 94.4% accuracy on the validation data, not bad with six possible classes to choose from. Let’s look into the validation performance a little deeper to see where the model is messing up.\nEvaluation\nNow that we have a trained model let’s investigate the errors that it made on our testing data. We can load the best model from training based upon validation accuracy and then look at each observation, what the model predicted, how high a probability it assigned, and the true activity label.\n\n\n# dataframe to get labels onto one-hot encoded prediction columns\noneHotToLabel <- activityLabels %>% \n  mutate(number = number - 7) %>% \n  filter(number >= 0) %>% \n  mutate(class = paste0(\"V\",number + 1)) %>% \n  select(-number)\n\n# Load our best model checkpoint\nbestModel <- load_model_hdf5(\"best_model.h5\")\n\ntidyPredictionProbs <- bestModel %>% \n  predict(testObs) %>% \n  as_data_frame() %>% \n  mutate(obs = 1:n()) %>% \n  gather(class, prob, -obs) %>% \n  right_join(oneHotToLabel, by = \"class\")\n\npredictionPerformance <- tidyPredictionProbs %>% \n  group_by(obs) %>% \n  summarise(\n    highestProb = max(prob),\n    predicted = label[prob == highestProb]\n  ) %>% \n  mutate(\n    truth = testData$activityName,\n    correct = truth == predicted\n  ) \n\npredictionPerformance %>% paged_table()\n\n\n{\"columns\":[{\"label\":[\"obs\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"highestProb\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"predicted\"],\"name\":[3],\"type\":[\"fctr\"],\"align\":[\"left\"]},{\"label\":[\"truth\"],\"name\":[4],\"type\":[\"fctr\"],\"align\":[\"left\"]},{\"label\":[\"correct\"],\"name\":[5],\"type\":[\"lgl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"0.8970604\",\"3\":\"STAND_TO_SIT\",\"4\":\"STAND_TO_SIT\",\"5\":\"TRUE\"},{\"1\":\"2\",\"2\":\"0.8936158\",\"3\":\"STAND_TO_SIT\",\"4\":\"STAND_TO_SIT\",\"5\":\"TRUE\"},{\"1\":\"3\",\"2\":\"0.9341052\",\"3\":\"STAND_TO_SIT\",\"4\":\"STAND_TO_SIT\",\"5\":\"TRUE\"},{\"1\":\"4\",\"2\":\"0.8462924\",\"3\":\"STAND_TO_SIT\",\"4\":\"STAND_TO_SIT\",\"5\":\"TRUE\"},{\"1\":\"5\",\"2\":\"0.9840696\",\"3\":\"STAND_TO_SIT\",\"4\":\"STAND_TO_SIT\",\"5\":\"TRUE\"},{\"1\":\"6\",\"2\":\"0.9990620\",\"3\":\"STAND_TO_SIT\",\"4\":\"STAND_TO_SIT\",\"5\":\"TRUE\"},{\"1\":\"7\",\"2\":\"0.9979126\",\"3\":\"STAND_TO_SIT\",\"4\":\"STAND_TO_SIT\",\"5\":\"TRUE\"},{\"1\":\"8\",\"2\":\"0.9985707\",\"3\":\"STAND_TO_SIT\",\"4\":\"STAND_TO_SIT\",\"5\":\"TRUE\"},{\"1\":\"9\",\"2\":\"0.9938900\",\"3\":\"STAND_TO_SIT\",\"4\":\"STAND_TO_SIT\",\"5\":\"TRUE\"},{\"1\":\"10\",\"2\":\"0.9877772\",\"3\":\"STAND_TO_SIT\",\"4\":\"STAND_TO_SIT\",\"5\":\"TRUE\"},{\"1\":\"11\",\"2\":\"0.8682424\",\"3\":\"STAND_TO_SIT\",\"4\":\"STAND_TO_SIT\",\"5\":\"TRUE\"},{\"1\":\"12\",\"2\":\"0.9810832\",\"3\":\"STAND_TO_SIT\",\"4\":\"STAND_TO_SIT\",\"5\":\"TRUE\"},{\"1\":\"13\",\"2\":\"0.9777718\",\"3\":\"SIT_TO_STAND\",\"4\":\"SIT_TO_STAND\",\"5\":\"TRUE\"},{\"1\":\"14\",\"2\":\"0.9474822\",\"3\":\"SIT_TO_STAND\",\"4\":\"SIT_TO_STAND\",\"5\":\"TRUE\"},{\"1\":\"15\",\"2\":\"0.6397119\",\"3\":\"SIT_TO_STAND\",\"4\":\"SIT_TO_STAND\",\"5\":\"TRUE\"},{\"1\":\"16\",\"2\":\"0.8954912\",\"3\":\"SIT_TO_STAND\",\"4\":\"SIT_TO_STAND\",\"5\":\"TRUE\"},{\"1\":\"17\",\"2\":\"0.8543505\",\"3\":\"SIT_TO_STAND\",\"4\":\"SIT_TO_STAND\",\"5\":\"TRUE\"},{\"1\":\"18\",\"2\":\"0.9409921\",\"3\":\"SIT_TO_STAND\",\"4\":\"SIT_TO_STAND\",\"5\":\"TRUE\"},{\"1\":\"19\",\"2\":\"0.9647168\",\"3\":\"SIT_TO_STAND\",\"4\":\"SIT_TO_STAND\",\"5\":\"TRUE\"},{\"1\":\"20\",\"2\":\"0.9418934\",\"3\":\"SIT_TO_STAND\",\"4\":\"SIT_TO_STAND\",\"5\":\"TRUE\"},{\"1\":\"21\",\"2\":\"0.8991503\",\"3\":\"SIT_TO_STAND\",\"4\":\"SIT_TO_STAND\",\"5\":\"TRUE\"},{\"1\":\"22\",\"2\":\"0.8605189\",\"3\":\"SIT_TO_STAND\",\"4\":\"SIT_TO_STAND\",\"5\":\"TRUE\"},{\"1\":\"23\",\"2\":\"0.9710508\",\"3\":\"SIT_TO_STAND\",\"4\":\"SIT_TO_STAND\",\"5\":\"TRUE\"},{\"1\":\"24\",\"2\":\"0.9450229\",\"3\":\"SIT_TO_STAND\",\"4\":\"SIT_TO_STAND\",\"5\":\"TRUE\"},{\"1\":\"25\",\"2\":\"0.9901183\",\"3\":\"SIT_TO_LIE\",\"4\":\"SIT_TO_LIE\",\"5\":\"TRUE\"},{\"1\":\"26\",\"2\":\"0.8780518\",\"3\":\"SIT_TO_LIE\",\"4\":\"SIT_TO_LIE\",\"5\":\"TRUE\"},{\"1\":\"27\",\"2\":\"0.8383064\",\"3\":\"SIT_TO_LIE\",\"4\":\"SIT_TO_LIE\",\"5\":\"TRUE\"},{\"1\":\"28\",\"2\":\"0.9977659\",\"3\":\"SIT_TO_LIE\",\"4\":\"SIT_TO_LIE\",\"5\":\"TRUE\"},{\"1\":\"29\",\"2\":\"0.9974304\",\"3\":\"SIT_TO_LIE\",\"4\":\"SIT_TO_LIE\",\"5\":\"TRUE\"},{\"1\":\"30\",\"2\":\"0.9984117\",\"3\":\"SIT_TO_LIE\",\"4\":\"SIT_TO_LIE\",\"5\":\"TRUE\"},{\"1\":\"31\",\"2\":\"0.7808345\",\"3\":\"SIT_TO_LIE\",\"4\":\"SIT_TO_LIE\",\"5\":\"TRUE\"},{\"1\":\"32\",\"2\":\"0.9924496\",\"3\":\"SIT_TO_LIE\",\"4\":\"SIT_TO_LIE\",\"5\":\"TRUE\"},{\"1\":\"33\",\"2\":\"0.9964616\",\"3\":\"SIT_TO_LIE\",\"4\":\"SIT_TO_LIE\",\"5\":\"TRUE\"},{\"1\":\"34\",\"2\":\"0.9928626\",\"3\":\"SIT_TO_LIE\",\"4\":\"SIT_TO_LIE\",\"5\":\"TRUE\"},{\"1\":\"35\",\"2\":\"0.8810695\",\"3\":\"SIT_TO_LIE\",\"4\":\"SIT_TO_LIE\",\"5\":\"TRUE\"},{\"1\":\"36\",\"2\":\"0.9983010\",\"3\":\"SIT_TO_LIE\",\"4\":\"SIT_TO_LIE\",\"5\":\"TRUE\"},{\"1\":\"37\",\"2\":\"0.7238058\",\"3\":\"LIE_TO_SIT\",\"4\":\"LIE_TO_SIT\",\"5\":\"TRUE\"},{\"1\":\"38\",\"2\":\"0.8970255\",\"3\":\"LIE_TO_SIT\",\"4\":\"LIE_TO_SIT\",\"5\":\"TRUE\"},{\"1\":\"39\",\"2\":\"0.5241567\",\"3\":\"LIE_TO_SIT\",\"4\":\"LIE_TO_SIT\",\"5\":\"TRUE\"},{\"1\":\"40\",\"2\":\"0.9724941\",\"3\":\"LIE_TO_SIT\",\"4\":\"LIE_TO_SIT\",\"5\":\"TRUE\"},{\"1\":\"41\",\"2\":\"0.9658489\",\"3\":\"LIE_TO_SIT\",\"4\":\"LIE_TO_SIT\",\"5\":\"TRUE\"},{\"1\":\"42\",\"2\":\"0.8779612\",\"3\":\"LIE_TO_SIT\",\"4\":\"LIE_TO_SIT\",\"5\":\"TRUE\"},{\"1\":\"43\",\"2\":\"0.6016794\",\"3\":\"LIE_TO_SIT\",\"4\":\"LIE_TO_SIT\",\"5\":\"TRUE\"},{\"1\":\"44\",\"2\":\"0.9966748\",\"3\":\"LIE_TO_SIT\",\"4\":\"LIE_TO_SIT\",\"5\":\"TRUE\"},{\"1\":\"45\",\"2\":\"0.9988185\",\"3\":\"LIE_TO_SIT\",\"4\":\"LIE_TO_SIT\",\"5\":\"TRUE\"},{\"1\":\"46\",\"2\":\"0.9109790\",\"3\":\"LIE_TO_SIT\",\"4\":\"LIE_TO_SIT\",\"5\":\"TRUE\"},{\"1\":\"47\",\"2\":\"0.9904966\",\"3\":\"LIE_TO_SIT\",\"4\":\"LIE_TO_SIT\",\"5\":\"TRUE\"},{\"1\":\"48\",\"2\":\"0.9998224\",\"3\":\"LIE_TO_SIT\",\"4\":\"LIE_TO_SIT\",\"5\":\"TRUE\"},{\"1\":\"49\",\"2\":\"0.9429878\",\"3\":\"STAND_TO_LIE\",\"4\":\"STAND_TO_LIE\",\"5\":\"TRUE\"},{\"1\":\"50\",\"2\":\"0.8499932\",\"3\":\"STAND_TO_LIE\",\"4\":\"STAND_TO_LIE\",\"5\":\"TRUE\"},{\"1\":\"51\",\"2\":\"0.6778994\",\"3\":\"SIT_TO_LIE\",\"4\":\"STAND_TO_LIE\",\"5\":\"FALSE\"},{\"1\":\"52\",\"2\":\"0.9997874\",\"3\":\"STAND_TO_LIE\",\"4\":\"STAND_TO_LIE\",\"5\":\"TRUE\"},{\"1\":\"53\",\"2\":\"0.7534851\",\"3\":\"STAND_TO_LIE\",\"4\":\"STAND_TO_LIE\",\"5\":\"TRUE\"},{\"1\":\"54\",\"2\":\"0.9621393\",\"3\":\"STAND_TO_LIE\",\"4\":\"STAND_TO_LIE\",\"5\":\"TRUE\"},{\"1\":\"55\",\"2\":\"0.9653141\",\"3\":\"SIT_TO_LIE\",\"4\":\"STAND_TO_LIE\",\"5\":\"FALSE\"},{\"1\":\"56\",\"2\":\"0.8495207\",\"3\":\"STAND_TO_LIE\",\"4\":\"STAND_TO_LIE\",\"5\":\"TRUE\"},{\"1\":\"57\",\"2\":\"0.9999526\",\"3\":\"STAND_TO_LIE\",\"4\":\"STAND_TO_LIE\",\"5\":\"TRUE\"},{\"1\":\"58\",\"2\":\"0.9915054\",\"3\":\"STAND_TO_LIE\",\"4\":\"STAND_TO_LIE\",\"5\":\"TRUE\"},{\"1\":\"59\",\"2\":\"0.8550630\",\"3\":\"STAND_TO_LIE\",\"4\":\"STAND_TO_LIE\",\"5\":\"TRUE\"},{\"1\":\"60\",\"2\":\"0.9982007\",\"3\":\"STAND_TO_LIE\",\"4\":\"STAND_TO_LIE\",\"5\":\"TRUE\"},{\"1\":\"61\",\"2\":\"0.8607119\",\"3\":\"LIE_TO_STAND\",\"4\":\"LIE_TO_STAND\",\"5\":\"TRUE\"},{\"1\":\"62\",\"2\":\"0.7846971\",\"3\":\"LIE_TO_STAND\",\"4\":\"LIE_TO_STAND\",\"5\":\"TRUE\"},{\"1\":\"63\",\"2\":\"0.8420011\",\"3\":\"LIE_TO_STAND\",\"4\":\"LIE_TO_STAND\",\"5\":\"TRUE\"},{\"1\":\"64\",\"2\":\"0.8904631\",\"3\":\"LIE_TO_STAND\",\"4\":\"LIE_TO_STAND\",\"5\":\"TRUE\"},{\"1\":\"65\",\"2\":\"0.9472615\",\"3\":\"LIE_TO_STAND\",\"4\":\"LIE_TO_STAND\",\"5\":\"TRUE\"},{\"1\":\"66\",\"2\":\"0.8901521\",\"3\":\"LIE_TO_STAND\",\"4\":\"LIE_TO_STAND\",\"5\":\"TRUE\"},{\"1\":\"67\",\"2\":\"0.9170208\",\"3\":\"LIE_TO_STAND\",\"4\":\"LIE_TO_STAND\",\"5\":\"TRUE\"},{\"1\":\"68\",\"2\":\"0.8959852\",\"3\":\"LIE_TO_STAND\",\"4\":\"LIE_TO_STAND\",\"5\":\"TRUE\"},{\"1\":\"69\",\"2\":\"0.7956741\",\"3\":\"LIE_TO_SIT\",\"4\":\"LIE_TO_STAND\",\"5\":\"FALSE\"},{\"1\":\"70\",\"2\":\"0.7266123\",\"3\":\"LIE_TO_STAND\",\"4\":\"LIE_TO_STAND\",\"5\":\"TRUE\"},{\"1\":\"71\",\"2\":\"0.9663992\",\"3\":\"LIE_TO_SIT\",\"4\":\"LIE_TO_STAND\",\"5\":\"FALSE\"},{\"1\":\"72\",\"2\":\"0.8346738\",\"3\":\"LIE_TO_STAND\",\"4\":\"LIE_TO_STAND\",\"5\":\"TRUE\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  \n\nFirst, let’s look at how ‘confident’ the model was by if the prediction was correct or not.\n\n\npredictionPerformance %>% \n  mutate(result = ifelse(correct, 'Correct', 'Incorrect')) %>% \n  ggplot(aes(highestProb)) +\n  geom_histogram(binwidth = 0.01) +\n  geom_rug(alpha = 0.5) +\n  facet_grid(result~.) +\n  ggtitle(\"Probabilities associated with prediction by correctness\")\n\n\nReassuringly it seems the model was, on average, less confident about its classifications for the incorrect results than the correct ones. (Although, the sample size is too small to say anything definitively.)\n\nIf you desire a model that can truly tell you how ‘confident’ it is in a prediction (rather than just a probability), look into bayesian neural networks(Kononenko 1989) and the more recent use of dropout-as-bayes(Gal and Ghahramani 2016).\nLet’s see what activities the model had the hardest time with using a confusion matrix.\n\n\npredictionPerformance %>% \n  group_by(truth, predicted) %>% \n  summarise(count = n()) %>% \n  mutate(good = truth == predicted) %>% \n  ggplot(aes(x = truth,  y = predicted)) +\n  geom_point(aes(size = count, color = good)) +\n  geom_text(aes(label = count), \n            hjust = 0, vjust = 0, \n            nudge_x = 0.1, nudge_y = 0.1) + \n  guides(color = FALSE, size = FALSE) +\n  theme_minimal()\n\n\nWe see that, as the preliminary visualization suggested, the model had a bit of trouble with distinguishing between LIE_TO_SIT and LIE_TO_STAND classes, along with the SIT_TO_LIE and STAND_TO_LIE, which also have similar visual profiles.\nFuture directions\nThe most obvious future direction to take this analysis would be to attempt to make the model more general by working with more of the supplied activity types. Another interesting direction would be to not separate the recordings into distinct ‘observations’ but instead keep them as one streaming set of data, much like a real world deployment of a model would work, and see how well a model could classify streaming data and detect changes in activity.\n\n\nGal, Yarin, and Zoubin Ghahramani. 2016. “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.” In International Conference on Machine Learning, 1050–9.\n\n\nGraves, Alex. 2012. “Supervised Sequence Labelling.” In Supervised Sequence Labelling with Recurrent Neural Networks, 5–13. Springer.\n\n\nKononenko, Igor. 1989. “Bayesian Neural Networks.” Biological Cybernetics 61 (5). Springer: 361–70.\n\n\nLeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. 2015. “Deep Learning.” Nature 521 (7553). Nature Publishing Group: 436.\n\n\nReyes-Ortiz, Jorge-L, Luca Oneto, Albert Samà, Xavier Parra, and Davide Anguita. 2016. “Transition-Aware Human Activity Recognition Using Smartphones.” Neurocomputing 171. Elsevier: 754–67.\n\n\nTompson, Jonathan, Ross Goroshin, Arjun Jain, Yann LeCun, and Christoph Bregler. 2014. “Efficient Object Localization Using Convolutional Networks.” CoRR abs/1411.4280. http://arxiv.org/abs/1411.4280.\n\n\n\n\n",
    "preview": "posts/2018-07-17-activity-detection/index_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 1152,
    "preview_height": 768
  },
  {
    "path": "posts/2018-06-25-sunspots-lstm/",
    "title": "Predicting Sunspot Frequency with Keras",
    "description": "In this post we will examine making time series predictions using the sunspots dataset that ships with base R. Sunspots are dark spots on the sun, associated with lower temperature. Our post will focus on both how to apply deep learning to time series forecasting, and how to properly apply cross validation in this domain.",
    "author": [
      {
        "name": "Matt Dancho",
        "url": "https://github.com/mdancho84"
      },
      {
        "name": "Sigrid Keydana",
        "url": "https://github.com/skeydan"
      }
    ],
    "date": "2018-06-25",
    "categories": [
      "TensorFlow/Keras",
      "Time Series"
    ],
    "contents": "\nForecasting sunspots with deep learning\nIn this post we will examine making time series predictions using the sunspots dataset that ships with base R. Sunspots are dark spots on the sun, associated with lower temperature. Here’s an image from NASA showing the solar phenomenon.\nFigure from https://www.nasa.gov/content/goddard/largest-sunspot-of-solar-cycleWe’re using the monthly version of the dataset, sunspots.month (there is a yearly version, too). It contains 265 years worth of data (from 1749 through 2013) on the number of sunspots per month.\n\n\n\nForecasting this dataset is challenging because of high short term variability as well as long-term irregularities evident in the cycles. For example, maximum amplitudes reached by the low frequency cycle differ a lot, as does the number of high frequency cycle steps needed to reach that maximum low frequency cycle height.\nOur post will focus on two dominant aspects: how to apply deep learning to time series forecasting, and how to properly apply cross validation in this domain. For the latter, we will use the rsample package that allows to do resampling on time series data. As to the former, our goal is not to reach utmost performance but to show the general course of action when using recurrent neural networks to model this kind of data.\nRecurrent neural networks\nWhen our data has a sequential structure, it is recurrent neural networks (RNNs) we use to model it.\nAs of today, among RNNs, the best established architectures are the GRU (Gated Recurrent Unit) and the LSTM (Long Short Term Memory). For today, let’s not zoom in on what makes them special, but on what they have in common with the most stripped-down RNN: the basic recurrence structure.\nIn contrast to the prototype of a neural network, often called Multilayer Perceptron (MLP), the RNN has a state that is carried on over time. This is nicely seen in this diagram from Goodfellow et al., a.k.a. the “bible of deep learning”:\nFigure from: http://www.deeplearningbook.orgAt each time, the state is a combination of the current input and the previous hidden state. This is reminiscent of autoregressive models, but with neural networks, there has to be some point where we halt the dependence.\nThat’s because in order to determine the weights, we keep calculating how our loss changes as the input changes. Now if the input we have to consider, at an arbitrary timestep, ranges back indefinitely - then we will not be able to calculate all those gradients. In practice, then, our hidden state will, at every iteration, be carried forward through a fixed number of steps.\nWe’ll come back to that as soon as we’ve loaded and pre-processed the data.\nSetup, pre-processing, and exploration\nLibraries\nHere, first, are the libraries needed for this tutorial.\n\n\n# Core Tidyverse\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(forcats)\n\n# Time Series\nlibrary(timetk)\nlibrary(tidyquant)\nlibrary(tibbletime)\n\n# Visualization\nlibrary(cowplot)\n\n# Preprocessing\nlibrary(recipes)\n\n# Sampling / Accuracy\nlibrary(rsample)\nlibrary(yardstick) \n\n# Modeling\nlibrary(keras)\nlibrary(tfruns)\n\nIf you have not previously run Keras in R, you will need to install Keras using the install_keras() function.\n\n\n# Install Keras if you have not installed before\ninstall_keras()\n\nData\nsunspot.month is a ts class (not tidy), so we’ll convert to a tidy data set using the tk_tbl() function from timetk. We use this instead of as.tibble() from tibble to automatically preserve the time series index as a zoo yearmon index. Last, we’ll convert the zoo index to date using lubridate::as_date() (loaded with tidyquant) and then change to a tbl_time object to make time series operations easier.\n\n\nsun_spots <- datasets::sunspot.month %>%\n    tk_tbl() %>%\n    mutate(index = as_date(index)) %>%\n    as_tbl_time(index = index)\n\nsun_spots\n\n\n# A time tibble: 3,177 x 2\n# Index: index\n   index      value\n   <date>     <dbl>\n 1 1749-01-01  58  \n 2 1749-02-01  62.6\n 3 1749-03-01  70  \n 4 1749-04-01  55.7\n 5 1749-05-01  85  \n 6 1749-06-01  83.5\n 7 1749-07-01  94.8\n 8 1749-08-01  66.3\n 9 1749-09-01  75.9\n10 1749-10-01  75.5\n# ... with 3,167 more rows\nExploratory data analysis\nThe time series is long (265 years!). We can visualize the time series both in full, and zoomed in on the first 10 years to get a feel for the series.\nVisualizing sunspot data with cowplot\nWe’ll make two ggplots and combine them using cowplot::plot_grid(). Note that for the zoomed in plot, we make use of tibbletime::time_filter(), which is an easy way to perform time-based filtering.\n\n\np1 <- sun_spots %>%\n    ggplot(aes(index, value)) +\n    geom_point(color = palette_light()[[1]], alpha = 0.5) +\n    theme_tq() +\n    labs(\n        title = \"From 1749 to 2013 (Full Data Set)\"\n    )\n\np2 <- sun_spots %>%\n    filter_time(\"start\" ~ \"1800\") %>%\n    ggplot(aes(index, value)) +\n    geom_line(color = palette_light()[[1]], alpha = 0.5) +\n    geom_point(color = palette_light()[[1]]) +\n    geom_smooth(method = \"loess\", span = 0.2, se = FALSE) +\n    theme_tq() +\n    labs(\n        title = \"1749 to 1759 (Zoomed In To Show Changes over the Year)\",\n        caption = \"datasets::sunspot.month\"\n    )\n\np_title <- ggdraw() + \n  draw_label(\"Sunspots\", size = 18, fontface = \"bold\", \n             colour = palette_light()[[1]])\n\nplot_grid(p_title, p1, p2, ncol = 1, rel_heights = c(0.1, 1, 1))\n\n\n\n\nBacktesting: time series cross validation\nWhen doing cross validation on sequential data, the time dependencies on preceding samples must be preserved. We can create a cross validation sampling plan by offsetting the window used to select sequential sub-samples. In essence, we’re creatively dealing with the fact that there’s no future test data available by creating multiple synthetic “futures” - a process often, esp. in finance, called “backtesting”.\nAs mentioned in the introduction, the rsample package includes facitlities for backtesting on time series. The vignette, “Time Series Analysis Example”, describes a procedure that uses the rolling_origin() function to create samples designed for time series cross validation. We’ll use this approach.\nDeveloping a backtesting strategy\nThe sampling plan we create uses 100 years (initial = 12 x 100 samples) for the training set and 50 years (assess = 12 x 50) for the testing (validation) set. We select a skip span of about 22 years (skip = 12 x 22 - 1) to approximately evenly distribute the samples into 6 sets that span the entire 265 years of sunspots history. Last, we select cumulative = FALSE to allow the origin to shift which ensures that models on more recent data are not given an unfair advantage (more observations) over those operating on less recent data. The tibble return contains the rolling_origin_resamples.\n\n\nperiods_train <- 12 * 100\nperiods_test  <- 12 * 50\nskip_span     <- 12 * 22 - 1\n\nrolling_origin_resamples <- rolling_origin(\n  sun_spots,\n  initial    = periods_train,\n  assess     = periods_test,\n  cumulative = FALSE,\n  skip       = skip_span\n)\n\nrolling_origin_resamples\n\n\n# Rolling origin forecast resampling \n# A tibble: 6 x 2\n  splits       id    \n  <list>       <chr> \n1 <S3: rsplit> Slice1\n2 <S3: rsplit> Slice2\n3 <S3: rsplit> Slice3\n4 <S3: rsplit> Slice4\n5 <S3: rsplit> Slice5\n6 <S3: rsplit> Slice6\nVisualizing the backtesting strategy\nWe can visualize the resamples with two custom functions. The first, plot_split(), plots one of the resampling splits using ggplot2. Note that an expand_y_axis argument is added to expand the date range to the full sun_spots dataset date range. This will become useful when we visualize all plots together.\n\n\n# Plotting function for a single split\nplot_split <- function(split, expand_y_axis = TRUE, \n                       alpha = 1, size = 1, base_size = 14) {\n    \n    # Manipulate data\n    train_tbl <- training(split) %>%\n        add_column(key = \"training\") \n    \n    test_tbl  <- testing(split) %>%\n        add_column(key = \"testing\") \n    \n    data_manipulated <- bind_rows(train_tbl, test_tbl) %>%\n        as_tbl_time(index = index) %>%\n        mutate(key = fct_relevel(key, \"training\", \"testing\"))\n        \n    # Collect attributes\n    train_time_summary <- train_tbl %>%\n        tk_index() %>%\n        tk_get_timeseries_summary()\n    \n    test_time_summary <- test_tbl %>%\n        tk_index() %>%\n        tk_get_timeseries_summary()\n    \n    # Visualize\n    g <- data_manipulated %>%\n        ggplot(aes(x = index, y = value, color = key)) +\n        geom_line(size = size, alpha = alpha) +\n        theme_tq(base_size = base_size) +\n        scale_color_tq() +\n        labs(\n          title    = glue(\"Split: {split$id}\"),\n          subtitle = glue(\"{train_time_summary$start} to \", \n                          \"{test_time_summary$end}\"),\n            y = \"\", x = \"\"\n        ) +\n        theme(legend.position = \"none\") \n    \n    if (expand_y_axis) {\n        \n        sun_spots_time_summary <- sun_spots %>% \n            tk_index() %>% \n            tk_get_timeseries_summary()\n        \n        g <- g +\n            scale_x_date(limits = c(sun_spots_time_summary$start, \n                                    sun_spots_time_summary$end))\n    }\n    \n    g\n}\n\nThe plot_split() function takes one split (in this case Slice01), and returns a visual of the sampling strategy. We expand the axis to the range for the full dataset using expand_y_axis = TRUE.\n\n\nrolling_origin_resamples$splits[[1]] %>%\n    plot_split(expand_y_axis = TRUE) +\n    theme(legend.position = \"bottom\")\n\n\n\n\nThe second function, plot_sampling_plan(), scales the plot_split() function to all of the samples using purrr and cowplot.\n\n\n# Plotting function that scales to all splits \nplot_sampling_plan <- function(sampling_tbl, expand_y_axis = TRUE, \n                               ncol = 3, alpha = 1, size = 1, base_size = 14, \n                               title = \"Sampling Plan\") {\n    \n    # Map plot_split() to sampling_tbl\n    sampling_tbl_with_plots <- sampling_tbl %>%\n        mutate(gg_plots = map(splits, plot_split, \n                              expand_y_axis = expand_y_axis,\n                              alpha = alpha, base_size = base_size))\n    \n    # Make plots with cowplot\n    plot_list <- sampling_tbl_with_plots$gg_plots \n    \n    p_temp <- plot_list[[1]] + theme(legend.position = \"bottom\")\n    legend <- get_legend(p_temp)\n    \n    p_body  <- plot_grid(plotlist = plot_list, ncol = ncol)\n    \n    p_title <- ggdraw() + \n        draw_label(title, size = 14, fontface = \"bold\", \n                   colour = palette_light()[[1]])\n    \n    g <- plot_grid(p_title, p_body, legend, ncol = 1, \n                   rel_heights = c(0.05, 1, 0.05))\n    \n    g\n    \n}\n\nWe can now visualize the entire backtesting strategy with plot_sampling_plan(). We can see how the sampling plan shifts the sampling window with each progressive slice of the train/test splits.\n\n\nrolling_origin_resamples %>%\n    plot_sampling_plan(expand_y_axis = T, ncol = 3, alpha = 1, size = 1, base_size = 10, \n                       title = \"Backtesting Strategy: Rolling Origin Sampling Plan\")\n\n\n\n\n\nAnd, we can set expand_y_axis = FALSE to zoom in on the samples.\n\n\nrolling_origin_resamples %>%\n    plot_sampling_plan(expand_y_axis = F, ncol = 3, alpha = 1, size = 1, base_size = 10, \n                       title = \"Backtesting Strategy: Zoomed In\")\n\n\n\n\n\nWe’ll use this backtesting strategy (6 samples from one time series each with 50/10 split in years and a ~20 year offset) when testing the veracity of the LSTM model on the sunspots dataset.\nThe LSTM model\nTo begin, we’ll develop an LSTM model on a single sample from the backtesting strategy, namely, the most recent slice. We’ll then apply the model to all samples to investigate modeling performance.\n\n\nexample_split    <- rolling_origin_resamples$splits[[6]]\nexample_split_id <- rolling_origin_resamples$id[[6]]\n\nWe can reuse the plot_split() function to visualize the split. Set expand_y_axis = FALSE to zoom in on the subsample.\n\n\nplot_split(example_split, expand_y_axis = FALSE, size = 0.5) +\n    theme(legend.position = \"bottom\") +\n    ggtitle(glue(\"Split: {example_split_id}\"))\n\n\nData setup\nTo aid hyperparameter tuning, besides the training set we also need a validation set. For example, we will use a callback, callback_early_stopping, that stops training when no significant performance is seen on the validation set (what’s considered significant is up to you).\nWe will dedicate 2 thirds of the analysis set to training, and 1 third to validation.\n\n\ndf_trn <- analysis(example_split)[1:800, , drop = FALSE]\ndf_val <- analysis(example_split)[801:1200, , drop = FALSE]\ndf_tst <- assessment(example_split)\n\nFirst, let’s combine the training and testing data sets into a single data set with a column key that specifies where they came from (either “training” or “testing)”. Note that the tbl_time object will need to have the index respecified during the bind_rows() step, but this issue should be corrected in dplyr soon.\n\n\ndf <- bind_rows(\n  df_trn %>% add_column(key = \"training\"),\n  df_val %>% add_column(key = \"validation\"),\n  df_tst %>% add_column(key = \"testing\")\n) %>%\n  as_tbl_time(index = index)\n\ndf\n\n\n# A time tibble: 1,800 x 3\n# Index: index\n   index      value key     \n   <date>     <dbl> <chr>   \n 1 1849-06-01  81.1 training\n 2 1849-07-01  78   training\n 3 1849-08-01  67.7 training\n 4 1849-09-01  93.7 training\n 5 1849-10-01  71.5 training\n 6 1849-11-01  99   training\n 7 1849-12-01  97   training\n 8 1850-01-01  78   training\n 9 1850-02-01  89.4 training\n10 1850-03-01  82.6 training\n# ... with 1,790 more rows\nPreprocessing with recipes\nThe LSTM algorithm will usually work better if the input data has been centered and scaled. We can conveniently accomplish this using the recipes package. In addition to step_center and step_scale, we’re using step_sqrt to reduce variance and remov outliers. The actual transformations are executed when we bake the data according to the recipe:\n\n\nrec_obj <- recipe(value ~ ., df) %>%\n    step_sqrt(value) %>%\n    step_center(value) %>%\n    step_scale(value) %>%\n    prep()\n\ndf_processed_tbl <- bake(rec_obj, df)\n\ndf_processed_tbl\n\n\n# A tibble: 1,800 x 3\n   index      value key     \n   <date>     <dbl> <fct>   \n 1 1849-06-01 0.714 training\n 2 1849-07-01 0.660 training\n 3 1849-08-01 0.473 training\n 4 1849-09-01 0.922 training\n 5 1849-10-01 0.544 training\n 6 1849-11-01 1.01  training\n 7 1849-12-01 0.974 training\n 8 1850-01-01 0.660 training\n 9 1850-02-01 0.852 training\n10 1850-03-01 0.739 training\n# ... with 1,790 more rows\nNext, let’s capture the original center and scale so we can invert the steps after modeling. The square root step can then simply be undone by squaring the back-transformed data.\n\n\ncenter_history <- rec_obj$steps[[2]]$means[\"value\"]\nscale_history  <- rec_obj$steps[[3]]$sds[\"value\"]\n\nc(\"center\" = center_history, \"scale\" = scale_history)\n\n\ncenter.value  scale.value \n    6.694468     3.238935 \nReshaping the data\nKeras LSTM expects the input as well as the target data to be in a specific shape. The input has to be a 3-d array of size num_samples, num_timesteps, num_features.\nHere, num_samples is the number of observations in the set. This will get fed to the model in portions of batch_size. The second dimension, num_timesteps, is the length of the hidden state we were talking about above. Finally, the third dimension is the number of predictors we’re using. For univariate time series, this is 1.\nHow long should we choose the hidden state to be? This generally depends on the dataset and our goal. If we did one-step-ahead forecasts - thus, forecasting the following month only - our main concern would be choosing a state length that allows to learn any patterns present in the data.\nNow say we wanted to forecast 12 months instead, as does SILSO, the World Data Center for the production, preservation and dissemination of the international sunspot number. The way we can do this, with Keras, is by wiring the LSTM hidden states to sets of consecutive outputs of the same length. Thus, if we want to produce predictions for 12 months, our LSTM should have a hidden state length of 12.\nThese 12 time steps will then get wired to 12 linear predictor units using a time_distributed() wrapper. That wrapper’s task is to apply the same calculation (i.e., the same weight matrix) to every state input it receives.\nNow, what’s the target array’s format supposed to be? As we’re forecasting several timesteps here, the target data again needs to be 3-dimensional. Dimension 1 again is the batch dimension, dimension 2 again corresponds to the number of timesteps (the forecasted ones), and dimension 3 is the size of the wrapped layer. In our case, the wrapped layer is a layer_dense() of a single unit, as we want exactly one prediction per point in time.\nSo, let’s reshape the data. The main action here is creating the sliding windows of 12 steps of input, followed by 12 steps of output each. This is easiest to understand with a shorter and simpler example. Say our input were the numbers from 1 to 10, and our chosen sequence length (state size) were 4. Tthis is how we would want our training input to look:\n\n1,2,3,4\n2,3,4,5\n3,4,5,6\nAnd our target data, correspondingly:\n\n5,6,7,8\n6,7,8,9\n7,8,9,10\nWe’ll define a short function that does this reshaping on a given dataset. Then finally, we add the third axis that is formally needed (even though that axis is of size 1 in our case).\n\n\n# these variables are being defined just because of the order in which\n# we present things in this post (first the data, then the model)\n# they will be superseded by FLAGS$n_timesteps, FLAGS$batch_size and n_predictions\n# in the following snippet\nn_timesteps <- 12\nn_predictions <- n_timesteps\nbatch_size <- 10\n\n# functions used\nbuild_matrix <- function(tseries, overall_timesteps) {\n  t(sapply(1:(length(tseries) - overall_timesteps + 1), function(x) \n    tseries[x:(x + overall_timesteps - 1)]))\n}\n\nreshape_X_3d <- function(X) {\n  dim(X) <- c(dim(X)[1], dim(X)[2], 1)\n  X\n}\n\n# extract values from data frame\ntrain_vals <- df_processed_tbl %>%\n  filter(key == \"training\") %>%\n  select(value) %>%\n  pull()\nvalid_vals <- df_processed_tbl %>%\n  filter(key == \"validation\") %>%\n  select(value) %>%\n  pull()\ntest_vals <- df_processed_tbl %>%\n  filter(key == \"testing\") %>%\n  select(value) %>%\n  pull()\n\n\n# build the windowed matrices\ntrain_matrix <-\n  build_matrix(train_vals, n_timesteps + n_predictions)\nvalid_matrix <-\n  build_matrix(valid_vals, n_timesteps + n_predictions)\ntest_matrix <- build_matrix(test_vals, n_timesteps + n_predictions)\n\n# separate matrices into training and testing parts\n# also, discard last batch if there are fewer than batch_size samples\n# (a purely technical requirement)\nX_train <- train_matrix[, 1:n_timesteps]\ny_train <- train_matrix[, (n_timesteps + 1):(n_timesteps * 2)]\nX_train <- X_train[1:(nrow(X_train) %/% batch_size * batch_size), ]\ny_train <- y_train[1:(nrow(y_train) %/% batch_size * batch_size), ]\n\nX_valid <- valid_matrix[, 1:n_timesteps]\ny_valid <- valid_matrix[, (n_timesteps + 1):(n_timesteps * 2)]\nX_valid <- X_valid[1:(nrow(X_valid) %/% batch_size * batch_size), ]\ny_valid <- y_valid[1:(nrow(y_valid) %/% batch_size * batch_size), ]\n\nX_test <- test_matrix[, 1:n_timesteps]\ny_test <- test_matrix[, (n_timesteps + 1):(n_timesteps * 2)]\nX_test <- X_test[1:(nrow(X_test) %/% batch_size * batch_size), ]\ny_test <- y_test[1:(nrow(y_test) %/% batch_size * batch_size), ]\n# add on the required third axis\nX_train <- reshape_X_3d(X_train)\nX_valid <- reshape_X_3d(X_valid)\nX_test <- reshape_X_3d(X_test)\n\ny_train <- reshape_X_3d(y_train)\ny_valid <- reshape_X_3d(y_valid)\ny_test <- reshape_X_3d(y_test)\n\nBuilding the LSTM model\nNow that we have our data in the required form, let’s finally build the model. As always in deep learning, an important, and often time-consuming, part of the job is tuning hyperparameters. To keep this post self-contained, and considering this is primarily a tutorial on how to use LSTM in R, let’s assume the following settings were found after extensive experimentation (in reality experimentation did take place, but not to a degree that performance couldn’t possibly be improved).\nInstead of hard coding the hyperparameters, we’ll use tfruns to set up an environment where we could easily perform grid search.\nWe’ll quickly comment on what these parameters do but mainly leave those topics to further posts.\n\n\nFLAGS <- flags(\n  # There is a so-called \"stateful LSTM\" in Keras. While LSTM is stateful\n  # per se, this adds a further tweak where the hidden states get \n  # initialized with values from the item at same position in the previous\n  # batch. This is helpful just under specific circumstances, or if you want\n  # to create an \"infinite stream\" of states, in which case you'd use 1 as \n  # the batch size. Below, we show how the code would have to be changed to\n  # use this, but it won't be further discussed here.\n  flag_boolean(\"stateful\", FALSE),\n  # Should we use several layers of LSTM?\n  # Again, just included for completeness, it did not yield any superior \n  # performance on this task.\n  # This will actually stack exactly one additional layer of LSTM units.\n  flag_boolean(\"stack_layers\", FALSE),\n  # number of samples fed to the model in one go\n  flag_integer(\"batch_size\", 10),\n  # size of the hidden state, equals size of predictions\n  flag_integer(\"n_timesteps\", 12),\n  # how many epochs to train for\n  flag_integer(\"n_epochs\", 100),\n  # fraction of the units to drop for the linear transformation of the inputs\n  flag_numeric(\"dropout\", 0.2),\n  # fraction of the units to drop for the linear transformation of the \n  # recurrent state\n  flag_numeric(\"recurrent_dropout\", 0.2),\n  # loss function. Found to work better for this specific case than mean\n  # squared error\n  flag_string(\"loss\", \"logcosh\"),\n  # optimizer = stochastic gradient descent. Seemed to work better than adam \n  # or rmsprop here (as indicated by limited testing)\n  flag_string(\"optimizer_type\", \"sgd\"),\n  # size of the LSTM layer\n  flag_integer(\"n_units\", 128),\n  # learning rate\n  flag_numeric(\"lr\", 0.003),\n  # momentum, an additional parameter to the SGD optimizer\n  flag_numeric(\"momentum\", 0.9),\n  # parameter to the early stopping callback\n  flag_integer(\"patience\", 10)\n)\n\n# the number of predictions we'll make equals the length of the hidden state\nn_predictions <- FLAGS$n_timesteps\n# how many features = predictors we have\nn_features <- 1\n# just in case we wanted to try different optimizers, we could add here\noptimizer <- switch(FLAGS$optimizer_type,\n                    sgd = optimizer_sgd(lr = FLAGS$lr, \n                                        momentum = FLAGS$momentum)\n                    )\n\n# callbacks to be passed to the fit() function\n# We just use one here: we may stop before n_epochs if the loss on the\n# validation set does not decrease (by a configurable amount, over a \n# configurable time)\ncallbacks <- list(\n  callback_early_stopping(patience = FLAGS$patience)\n)\n\nAfter all these preparations, the code for constructing and training the model is rather short! Let’s first quickly view the “long version”, that would allow you to test stacking several LSTMs or use a stateful LSTM, then go through the final short version (that does neither) and comment on it.\nThis, just for reference, is the complete code.\n\n\nmodel <- keras_model_sequential()\n\nmodel %>%\n  layer_lstm(\n    units = FLAGS$n_units,\n    batch_input_shape = c(FLAGS$batch_size, FLAGS$n_timesteps, n_features),\n    dropout = FLAGS$dropout,\n    recurrent_dropout = FLAGS$recurrent_dropout,\n    return_sequences = TRUE,\n    stateful = FLAGS$stateful\n  )\n\nif (FLAGS$stack_layers) {\n  model %>%\n    layer_lstm(\n      units = FLAGS$n_units,\n      dropout = FLAGS$dropout,\n      recurrent_dropout = FLAGS$recurrent_dropout,\n      return_sequences = TRUE,\n      stateful = FLAGS$stateful\n    )\n}\nmodel %>% time_distributed(layer_dense(units = 1))\n\nmodel %>%\n  compile(\n    loss = FLAGS$loss,\n    optimizer = optimizer,\n    metrics = list(\"mean_squared_error\")\n  )\n\nif (!FLAGS$stateful) {\n  model %>% fit(\n    x          = X_train,\n    y          = y_train,\n    validation_data = list(X_valid, y_valid),\n    batch_size = FLAGS$batch_size,\n    epochs     = FLAGS$n_epochs,\n    callbacks = callbacks\n  )\n  \n} else {\n  for (i in 1:FLAGS$n_epochs) {\n    model %>% fit(\n      x          = X_train,\n      y          = y_train,\n      validation_data = list(X_valid, y_valid),\n      callbacks = callbacks,\n      batch_size = FLAGS$batch_size,\n      epochs     = 1,\n      shuffle    = FALSE\n    )\n    model %>% reset_states()\n  }\n}\n\nif (FLAGS$stateful)\n  model %>% reset_states()\n\nNow let’s step through the simpler, yet better (or equally) performing configuration below.\n\n\n# create the model\nmodel <- keras_model_sequential()\n\n# add layers\n# we have just two, the LSTM and the time_distributed \nmodel %>%\n  layer_lstm(\n    units = FLAGS$n_units, \n    # the first layer in a model needs to know the shape of the input data\n    batch_input_shape  = c(FLAGS$batch_size, FLAGS$n_timesteps, n_features),\n    dropout = FLAGS$dropout,\n    recurrent_dropout = FLAGS$recurrent_dropout,\n    # by default, an LSTM just returns the final state\n    return_sequences = TRUE\n  ) %>% time_distributed(layer_dense(units = 1))\n\nmodel %>%\n  compile(\n    loss = FLAGS$loss,\n    optimizer = optimizer,\n    # in addition to the loss, Keras will inform us about current \n    # MSE while training\n    metrics = list(\"mean_squared_error\")\n  )\n\nhistory <- model %>% fit(\n  x          = X_train,\n  y          = y_train,\n  validation_data = list(X_valid, y_valid),\n  batch_size = FLAGS$batch_size,\n  epochs     = FLAGS$n_epochs,\n  callbacks = callbacks\n)\n\nAs we see, training was stopped after ~55 epochs as validation loss did not decrease any more. We also see that performance on the validation set is way worse than performance on the training set - normally indicating overfitting.\nThis topic too, we’ll leave to a separate discussion another time, but interestingly regularization using higher values of dropout and recurrent_dropout (combined with increasing model capacity) did not yield better generalization performance. This is probably related to the characteristics of this specific time series we mentioned in the introduction.\n\n\nplot(history, metrics = \"loss\")\n\n\nNow let’s see how well the model was able to capture the characteristics of the training set.\n\n\npred_train <- model %>%\n  predict(X_train, batch_size = FLAGS$batch_size) %>%\n  .[, , 1]\n\n# Retransform values to original scale\npred_train <- (pred_train * scale_history + center_history) ^2\ncompare_train <- df %>% filter(key == \"training\")\n\n# build a dataframe that has both actual and predicted values\nfor (i in 1:nrow(pred_train)) {\n  varname <- paste0(\"pred_train\", i)\n  compare_train <-\n    mutate(compare_train,!!varname := c(\n      rep(NA, FLAGS$n_timesteps + i - 1),\n      pred_train[i,],\n      rep(NA, nrow(compare_train) - FLAGS$n_timesteps * 2 - i + 1)\n    ))\n}\n\nWe compute the average RSME over all sequences of predictions.\n\n\ncoln <- colnames(compare_train)[4:ncol(compare_train)]\ncols <- map(coln, quo(sym(.)))\nrsme_train <-\n  map_dbl(cols, function(col)\n    rmse(\n      compare_train,\n      truth = value,\n      estimate = !!col,\n      na.rm = TRUE\n    )) %>% mean()\n\nrsme_train\n\n\n21.01495\nHow do these predictions really look? As a visualization of all predicted sequences would look pretty crowded, we arbitrarily pick start points at regular intervals.\n\n\nggplot(compare_train, aes(x = index, y = value)) + geom_line() +\n  geom_line(aes(y = pred_train1), color = \"cyan\") +\n  geom_line(aes(y = pred_train50), color = \"red\") +\n  geom_line(aes(y = pred_train100), color = \"green\") +\n  geom_line(aes(y = pred_train150), color = \"violet\") +\n  geom_line(aes(y = pred_train200), color = \"cyan\") +\n  geom_line(aes(y = pred_train250), color = \"red\") +\n  geom_line(aes(y = pred_train300), color = \"red\") +\n  geom_line(aes(y = pred_train350), color = \"green\") +\n  geom_line(aes(y = pred_train400), color = \"cyan\") +\n  geom_line(aes(y = pred_train450), color = \"red\") +\n  geom_line(aes(y = pred_train500), color = \"green\") +\n  geom_line(aes(y = pred_train550), color = \"violet\") +\n  geom_line(aes(y = pred_train600), color = \"cyan\") +\n  geom_line(aes(y = pred_train650), color = \"red\") +\n  geom_line(aes(y = pred_train700), color = \"red\") +\n  geom_line(aes(y = pred_train750), color = \"green\") +\n  ggtitle(\"Predictions on the training set\")\n\n\n\n\nThis looks pretty good. From the validation loss, we don’t quite expect the same from the test set, though.\nLet’s see.\n\n\npred_test <- model %>%\n  predict(X_test, batch_size = FLAGS$batch_size) %>%\n  .[, , 1]\n\n# Retransform values to original scale\npred_test <- (pred_test * scale_history + center_history) ^2\npred_test[1:10, 1:5] %>% print()\ncompare_test <- df %>% filter(key == \"testing\")\n\n# build a dataframe that has both actual and predicted values\nfor (i in 1:nrow(pred_test)) {\n  varname <- paste0(\"pred_test\", i)\n  compare_test <-\n    mutate(compare_test,!!varname := c(\n      rep(NA, FLAGS$n_timesteps + i - 1),\n      pred_test[i,],\n      rep(NA, nrow(compare_test) - FLAGS$n_timesteps * 2 - i + 1)\n    ))\n}\n\ncompare_test %>% write_csv(str_replace(model_path, \".hdf5\", \".test.csv\"))\ncompare_test[FLAGS$n_timesteps:(FLAGS$n_timesteps + 10), c(2, 4:8)] %>% print()\n\ncoln <- colnames(compare_test)[4:ncol(compare_test)]\ncols <- map(coln, quo(sym(.)))\nrsme_test <-\n  map_dbl(cols, function(col)\n    rmse(\n      compare_test,\n      truth = value,\n      estimate = !!col,\n      na.rm = TRUE\n    )) %>% mean()\n\nrsme_test\n\n\n31.31616\n\n\nggplot(compare_test, aes(x = index, y = value)) + geom_line() +\n  geom_line(aes(y = pred_test1), color = \"cyan\") +\n  geom_line(aes(y = pred_test50), color = \"red\") +\n  geom_line(aes(y = pred_test100), color = \"green\") +\n  geom_line(aes(y = pred_test150), color = \"violet\") +\n  geom_line(aes(y = pred_test200), color = \"cyan\") +\n  geom_line(aes(y = pred_test250), color = \"red\") +\n  geom_line(aes(y = pred_test300), color = \"green\") +\n  geom_line(aes(y = pred_test350), color = \"cyan\") +\n  geom_line(aes(y = pred_test400), color = \"red\") +\n  geom_line(aes(y = pred_test450), color = \"green\") +  \n  geom_line(aes(y = pred_test500), color = \"cyan\") +\n  geom_line(aes(y = pred_test550), color = \"violet\") +\n  ggtitle(\"Predictions on test set\")\n\n\n\n\nThat’s not as good as on the training set, but not bad either, given this time series is quite challenging.\nHaving defined and run our model on a manually chosen example split, let’s now revert to our overall re-sampling frame.\nBacktesting the model on all splits\nTo obtain predictions on all splits, we move the above code into a function and apply it to all splits. First, here’s the function. It returns a list of two dataframes, one for the training and test sets each, that contain the model’s predictions together with the actual values.\n\n\nobtain_predictions <- function(split) {\n  df_trn <- analysis(split)[1:800, , drop = FALSE]\n  df_val <- analysis(split)[801:1200, , drop = FALSE]\n  df_tst <- assessment(split)\n  \n  df <- bind_rows(\n    df_trn %>% add_column(key = \"training\"),\n    df_val %>% add_column(key = \"validation\"),\n    df_tst %>% add_column(key = \"testing\")\n  ) %>%\n    as_tbl_time(index = index)\n  \n  rec_obj <- recipe(value ~ ., df) %>%\n    step_sqrt(value) %>%\n    step_center(value) %>%\n    step_scale(value) %>%\n    prep()\n  \n  df_processed_tbl <- bake(rec_obj, df)\n  \n  center_history <- rec_obj$steps[[2]]$means[\"value\"]\n  scale_history  <- rec_obj$steps[[3]]$sds[\"value\"]\n  \n  FLAGS <- flags(\n    flag_boolean(\"stateful\", FALSE),\n    flag_boolean(\"stack_layers\", FALSE),\n    flag_integer(\"batch_size\", 10),\n    flag_integer(\"n_timesteps\", 12),\n    flag_integer(\"n_epochs\", 100),\n    flag_numeric(\"dropout\", 0.2),\n    flag_numeric(\"recurrent_dropout\", 0.2),\n    flag_string(\"loss\", \"logcosh\"),\n    flag_string(\"optimizer_type\", \"sgd\"),\n    flag_integer(\"n_units\", 128),\n    flag_numeric(\"lr\", 0.003),\n    flag_numeric(\"momentum\", 0.9),\n    flag_integer(\"patience\", 10)\n  )\n  \n  n_predictions <- FLAGS$n_timesteps\n  n_features <- 1\n  \n  optimizer <- switch(FLAGS$optimizer_type,\n                      sgd = optimizer_sgd(lr = FLAGS$lr, momentum = FLAGS$momentum))\n  callbacks <- list(\n    callback_early_stopping(patience = FLAGS$patience)\n  )\n  \n  train_vals <- df_processed_tbl %>%\n    filter(key == \"training\") %>%\n    select(value) %>%\n    pull()\n  valid_vals <- df_processed_tbl %>%\n    filter(key == \"validation\") %>%\n    select(value) %>%\n    pull()\n  test_vals <- df_processed_tbl %>%\n    filter(key == \"testing\") %>%\n    select(value) %>%\n    pull()\n  \n  train_matrix <-\n    build_matrix(train_vals, FLAGS$n_timesteps + n_predictions)\n  valid_matrix <-\n    build_matrix(valid_vals, FLAGS$n_timesteps + n_predictions)\n  test_matrix <-\n    build_matrix(test_vals, FLAGS$n_timesteps + n_predictions)\n  \n  X_train <- train_matrix[, 1:FLAGS$n_timesteps]\n  y_train <-\n    train_matrix[, (FLAGS$n_timesteps + 1):(FLAGS$n_timesteps * 2)]\n  X_train <-\n    X_train[1:(nrow(X_train) %/% FLAGS$batch_size * FLAGS$batch_size),]\n  y_train <-\n    y_train[1:(nrow(y_train) %/% FLAGS$batch_size * FLAGS$batch_size),]\n  \n  X_valid <- valid_matrix[, 1:FLAGS$n_timesteps]\n  y_valid <-\n    valid_matrix[, (FLAGS$n_timesteps + 1):(FLAGS$n_timesteps * 2)]\n  X_valid <-\n    X_valid[1:(nrow(X_valid) %/% FLAGS$batch_size * FLAGS$batch_size),]\n  y_valid <-\n    y_valid[1:(nrow(y_valid) %/% FLAGS$batch_size * FLAGS$batch_size),]\n  \n  X_test <- test_matrix[, 1:FLAGS$n_timesteps]\n  y_test <-\n    test_matrix[, (FLAGS$n_timesteps + 1):(FLAGS$n_timesteps * 2)]\n  X_test <-\n    X_test[1:(nrow(X_test) %/% FLAGS$batch_size * FLAGS$batch_size),]\n  y_test <-\n    y_test[1:(nrow(y_test) %/% FLAGS$batch_size * FLAGS$batch_size),]\n  \n  X_train <- reshape_X_3d(X_train)\n  X_valid <- reshape_X_3d(X_valid)\n  X_test <- reshape_X_3d(X_test)\n  \n  y_train <- reshape_X_3d(y_train)\n  y_valid <- reshape_X_3d(y_valid)\n  y_test <- reshape_X_3d(y_test)\n  \n  model <- keras_model_sequential()\n  \n  model %>%\n    layer_lstm(\n      units            = FLAGS$n_units,\n      batch_input_shape  = c(FLAGS$batch_size, FLAGS$n_timesteps, n_features),\n      dropout = FLAGS$dropout,\n      recurrent_dropout = FLAGS$recurrent_dropout,\n      return_sequences = TRUE\n    )     %>% time_distributed(layer_dense(units = 1))\n  \n  model %>%\n    compile(\n      loss = FLAGS$loss,\n      optimizer = optimizer,\n      metrics = list(\"mean_squared_error\")\n    )\n  \n  model %>% fit(\n    x          = X_train,\n    y          = y_train,\n    validation_data = list(X_valid, y_valid),\n    batch_size = FLAGS$batch_size,\n    epochs     = FLAGS$n_epochs,\n    callbacks = callbacks\n  )\n  \n  \n  pred_train <- model %>%\n    predict(X_train, batch_size = FLAGS$batch_size) %>%\n    .[, , 1]\n  \n  # Retransform values\n  pred_train <- (pred_train * scale_history + center_history) ^ 2\n  compare_train <- df %>% filter(key == \"training\")\n  \n  for (i in 1:nrow(pred_train)) {\n    varname <- paste0(\"pred_train\", i)\n    compare_train <-\n      mutate(compare_train, !!varname := c(\n        rep(NA, FLAGS$n_timesteps + i - 1),\n        pred_train[i, ],\n        rep(NA, nrow(compare_train) - FLAGS$n_timesteps * 2 - i + 1)\n      ))\n  }\n  \n  pred_test <- model %>%\n    predict(X_test, batch_size = FLAGS$batch_size) %>%\n    .[, , 1]\n  \n  # Retransform values\n  pred_test <- (pred_test * scale_history + center_history) ^ 2\n  compare_test <- df %>% filter(key == \"testing\")\n  \n  for (i in 1:nrow(pred_test)) {\n    varname <- paste0(\"pred_test\", i)\n    compare_test <-\n      mutate(compare_test, !!varname := c(\n        rep(NA, FLAGS$n_timesteps + i - 1),\n        pred_test[i, ],\n        rep(NA, nrow(compare_test) - FLAGS$n_timesteps * 2 - i + 1)\n      ))\n  }\n  list(train = compare_train, test = compare_test)\n  \n}\n\nMapping the function over all splits yields a list of predictions.\n\n\nall_split_preds <- rolling_origin_resamples %>%\n     mutate(predict = map(splits, obtain_predictions))\n\nCalculate RMSE on all splits:\n\n\ncalc_rmse <- function(df) {\n  coln <- colnames(df)[4:ncol(df)]\n  cols <- map(coln, quo(sym(.)))\n  map_dbl(cols, function(col)\n    rmse(\n      df,\n      truth = value,\n      estimate = !!col,\n      na.rm = TRUE\n    )) %>% mean()\n}\n\nall_split_preds <- all_split_preds %>% unnest(predict)\nall_split_preds_train <- all_split_preds[seq(1, 11, by = 2), ]\nall_split_preds_test <- all_split_preds[seq(2, 12, by = 2), ]\n\nall_split_rmses_train <- all_split_preds_train %>%\n  mutate(rmse = map_dbl(predict, calc_rmse)) %>%\n  select(id, rmse)\n\nall_split_rmses_test <- all_split_preds_test %>%\n  mutate(rmse = map_dbl(predict, calc_rmse)) %>%\n  select(id, rmse)\n\nHow does it look? Here’s RMSE on the training set for the 6 splits.\n\n\nall_split_rmses_train\n\n\n# A tibble: 6 x 2\n  id      rmse\n  <chr>  <dbl>\n1 Slice1  22.2\n2 Slice2  20.9\n3 Slice3  18.8\n4 Slice4  23.5\n5 Slice5  22.1\n6 Slice6  21.1\n\n\nall_split_rmses_test\n\n\n# A tibble: 6 x 2\n  id      rmse\n  <chr>  <dbl>\n1 Slice1  21.6\n2 Slice2  20.6\n3 Slice3  21.3\n4 Slice4  31.4\n5 Slice5  35.2\n6 Slice6  31.4\nLooking at these numbers, we see something interesting: Generalization performance is much better for the first three slices of the time series than for the latter ones. This confirms our impression, stated above, that there seems to be some hidden development going on, rendering forecasting more difficult.\nAnd here are visualizations of the predictions on the respective training and test sets.\nFirst, the training sets:\n\n\nplot_train <- function(slice, name) {\n  ggplot(slice, aes(x = index, y = value)) + geom_line() +\n    geom_line(aes(y = pred_train1), color = \"cyan\") +\n    geom_line(aes(y = pred_train50), color = \"red\") +\n    geom_line(aes(y = pred_train100), color = \"green\") +\n    geom_line(aes(y = pred_train150), color = \"violet\") +\n    geom_line(aes(y = pred_train200), color = \"cyan\") +\n    geom_line(aes(y = pred_train250), color = \"red\") +\n    geom_line(aes(y = pred_train300), color = \"red\") +\n    geom_line(aes(y = pred_train350), color = \"green\") +\n    geom_line(aes(y = pred_train400), color = \"cyan\") +\n    geom_line(aes(y = pred_train450), color = \"red\") +\n    geom_line(aes(y = pred_train500), color = \"green\") +\n    geom_line(aes(y = pred_train550), color = \"violet\") +\n    geom_line(aes(y = pred_train600), color = \"cyan\") +\n    geom_line(aes(y = pred_train650), color = \"red\") +\n    geom_line(aes(y = pred_train700), color = \"red\") +\n    geom_line(aes(y = pred_train750), color = \"green\") +\n    ggtitle(name)\n}\n\ntrain_plots <- map2(all_split_preds_train$predict, all_split_preds_train$id, plot_train)\np_body_train  <- plot_grid(plotlist = train_plots, ncol = 3)\np_title_train <- ggdraw() + \n  draw_label(\"Backtested Predictions: Training Sets\", size = 18, fontface = \"bold\")\n\nplot_grid(p_title_train, p_body_train, ncol = 1, rel_heights = c(0.05, 1, 0.05))\n\n\n\n\n\nAnd the test sets:\n\n\nplot_test <- function(slice, name) {\n  ggplot(slice, aes(x = index, y = value)) + geom_line() +\n    geom_line(aes(y = pred_test1), color = \"cyan\") +\n    geom_line(aes(y = pred_test50), color = \"red\") +\n    geom_line(aes(y = pred_test100), color = \"green\") +\n    geom_line(aes(y = pred_test150), color = \"violet\") +\n    geom_line(aes(y = pred_test200), color = \"cyan\") +\n    geom_line(aes(y = pred_test250), color = \"red\") +\n    geom_line(aes(y = pred_test300), color = \"green\") +\n    geom_line(aes(y = pred_test350), color = \"cyan\") +\n    geom_line(aes(y = pred_test400), color = \"red\") +\n    geom_line(aes(y = pred_test450), color = \"green\") +  \n    geom_line(aes(y = pred_test500), color = \"cyan\") +\n    geom_line(aes(y = pred_test550), color = \"violet\") +\n    ggtitle(name)\n}\n\ntest_plots <- map2(all_split_preds_test$predict, all_split_preds_test$id, plot_test)\n\np_body_test  <- plot_grid(plotlist = test_plots, ncol = 3)\np_title_test <- ggdraw() + \n  draw_label(\"Backtested Predictions: Test Sets\", size = 18, fontface = \"bold\")\n\nplot_grid(p_title_test, p_body_test, ncol = 1, rel_heights = c(0.05, 1, 0.05))\n\n\n\n\n\nThis has been a long post, and necessarily will have left a lot of questions open, first and foremost: How do we obtain good settings for the hyperparameters (learning rate, number of epochs, dropout)? How do we choose the length of the hidden state? Or even, can we have an intuition how well LSTM will perform on a given dataset (with its specific characteristics)? We will tackle questions like the above in upcoming posts.\n\n\n",
    "preview": "posts/2018-06-25-sunspots-lstm/images/backtested_test.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 800,
    "preview_height": 416
  },
  {
    "path": "posts/2018-06-06-simple-audio-classification-keras/",
    "title": "Simple Audio Classification with Keras",
    "description": "In this tutorial we will build a deep learning model to classify words. We will use the Speech Commands dataset which consists of 65,000 one-second audio files of people saying 30 different words.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": "https://github.com/dfalbel"
      }
    ],
    "date": "2018-06-06",
    "categories": [
      "TensorFlow/Keras",
      "Audio Processing"
    ],
    "contents": "\nIntroduction\nIn this tutorial we will build a deep learning model to classify words. We will use tfdatasets to handle data IO and pre-processing, and Keras to build and train the model.\nWe will use the Speech Commands dataset which consists of 65,000 one-second audio files of people saying 30 different words. Each file contains a single spoken English word. The dataset was released by Google under CC License.\nOur model is a Keras port of the TensorFlow tutorial on Simple Audio Recognition which in turn was inspired by Convolutional Neural Networks for Small-footprint Keyword Spotting. There are other approaches to the speech recognition task, like recurrent neural networks, dilated (atrous) convolutions or Learning from Between-class Examples for Deep Sound Recognition.\nThe model we will implement here is not the state of the art for audio recognition systems, which are way more complex, but is relatively simple and fast to train. Plus, we show how to efficiently use tfdatasets to preprocess and serve data.\nAudio representation\nMany deep learning models are end-to-end, i.e. we let the model learn useful representations directly from the raw data. However, audio data grows very fast - 16,000 samples per second with a very rich structure at many time-scales. In order to avoid having to deal with raw wave sound data, researchers usually use some kind of feature engineering.\nEvery sound wave can be represented by its spectrum, and digitally it can be computed using the Fast Fourier Transform (FFT).\nBy Phonical - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=64473578A common way to represent audio data is to break it into small chunks, which usually overlap. For each chunk we use the FFT to calculate the magnitude of the frequency spectrum. The spectra are then combined, side by side, to form what we call a spectrogram.\nIt’s also common for speech recognition systems to further transform the spectrum and compute the Mel-Frequency Cepstral Coefficients. This transformation takes into account that the human ear can’t discern the difference between two closely spaced frequencies and smartly creates bins on the frequency axis. A great tutorial on MFCCs can be found here.\nBy Aquegg - Own work, Public Domain, https://commons.wikimedia.org/w/index.php?curid=5544473After this procedure, we have an image for each audio sample and we can use convolutional neural networks, the standard architecture type in image recognition models.\nDownloading\nFirst, let’s download data to a directory in our project. You can either download from this link (~1GB) or from R with:\n\n\ndir.create(\"data\")\n\ndownload.file(\n  url = \"http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\", \n  destfile = \"data/speech_commands_v0.01.tar.gz\"\n)\n\nuntar(\"data/speech_commands_v0.01.tar.gz\", exdir = \"data/speech_commands_v0.01\")\n\nInside the data directory we will have a folder called speech_commands_v0.01. The WAV audio files inside this directory are organised in sub-folders with the label names. For example, all one-second audio files of people speaking the word “bed” are inside the bed directory. There are 30 of them and a special one called _background_noise_ which contains various patterns that could be mixed in to simulate background noise.\nImporting\nIn this step we will list all audio .wav files into a tibble with 3 columns:\nfname: the file name;\nclass: the label for each audio file;\nclass_id: a unique integer number starting from zero for each class - used to one-hot encode the classes.\nThis will be useful to the next step when we will create a generator using the tfdatasets package.\n\n\nlibrary(stringr)\nlibrary(dplyr)\n\nfiles <- fs::dir_ls(\n  path = \"data/speech_commands_v0.01/\", \n  recursive = TRUE, \n  glob = \"*.wav\"\n)\n\nfiles <- files[!str_detect(files, \"background_noise\")]\n\ndf <- data_frame(\n  fname = files, \n  class = fname %>% str_extract(\"1/.*/\") %>% \n    str_replace_all(\"1/\", \"\") %>%\n    str_replace_all(\"/\", \"\"),\n  class_id = class %>% as.factor() %>% as.integer() - 1L\n)\n\nGenerator\nWe will now create our Dataset, which in the context of tfdatasets, adds operations to the TensorFlow graph in order to read and pre-process data. Since they are TensorFlow ops, they are executed in C++ and in parallel with model training.\nThe generator we will create will be responsible for reading the audio files from disk, creating the spectrogram for each one and batching the outputs.\nLet’s start by creating the dataset from slices of the data.frame with audio file names and classes we just created.\n\n\nlibrary(tfdatasets)\nds <- tensor_slices_dataset(df) \n\nNow, let’s define the parameters for spectrogram creation. We need to define window_size_ms which is the size in milliseconds of each chunk we will break the audio wave into, and window_stride_ms, the distance between the centers of adjacent chunks:\n\n\nwindow_size_ms <- 30\nwindow_stride_ms <- 10\n\nNow we will convert the window size and stride from milliseconds to samples. We are considering that our audio files have 16,000 samples per second (1000 ms).\n\n\nwindow_size <- as.integer(16000*window_size_ms/1000)\nstride <- as.integer(16000*window_stride_ms/1000)\n\nWe will obtain other quantities that will be useful for spectrogram creation, like the number of chunks and the FFT size, i.e., the number of bins on the frequency axis. The function we are going to use to compute the spectrogram doesn’t allow us to change the FFT size and instead by default uses the first power of 2 greater than the window size.\n\n\nfft_size <- as.integer(2^trunc(log(window_size, 2)) + 1)\nn_chunks <- length(seq(window_size/2, 16000 - window_size/2, stride))\n\nWe will now use dataset_map which allows us to specify a pre-processing function for each observation (line) of our dataset. It’s in this step that we read the raw audio file from disk and create its spectrogram and the one-hot encoded response vector.\n\n\n# shortcuts to used TensorFlow modules.\naudio_ops <- tf$contrib$framework$python$ops$audio_ops\n\nds <- ds %>%\n  dataset_map(function(obs) {\n    \n    # a good way to debug when building tfdatsets pipelines is to use a print\n    # statement like this:\n    # print(str(obs))\n    \n    # decoding wav files\n    audio_binary <- tf$read_file(tf$reshape(obs$fname, shape = list()))\n    wav <- audio_ops$decode_wav(audio_binary, desired_channels = 1)\n    \n    # create the spectrogram\n    spectrogram <- audio_ops$audio_spectrogram(\n      wav$audio, \n      window_size = window_size, \n      stride = stride,\n      magnitude_squared = TRUE\n    )\n    \n    # normalization\n    spectrogram <- tf$log(tf$abs(spectrogram) + 0.01)\n    \n    # moving channels to last dim\n    spectrogram <- tf$transpose(spectrogram, perm = c(1L, 2L, 0L))\n    \n    # transform the class_id into a one-hot encoded vector\n    response <- tf$one_hot(obs$class_id, 30L)\n    \n    list(spectrogram, response)\n  }) \n\nNow, we will specify how we want batch observations from the dataset. We’re using dataset_shuffle since we want to shuffle observations from the dataset, otherwise it would follow the order of the df object. Then we use dataset_repeat in order to tell TensorFlow that we want to keep taking observations from the dataset even if all observations have already been used. And most importantly here, we use dataset_padded_batch to specify that we want batches of size 32, but they should be padded, ie. if some observation has a different size we pad it with zeroes. The padded shape is passed to dataset_padded_batch via the padded_shapes argument and we use NULL to state that this dimension doesn’t need to be padded.\n\n\nds <- ds %>% \n  dataset_shuffle(buffer_size = 100) %>%\n  dataset_repeat() %>%\n  dataset_padded_batch(\n    batch_size = 32, \n    padded_shapes = list(\n      shape(n_chunks, fft_size, NULL), \n      shape(NULL)\n    )\n  )\n\nThis is our dataset specification, but we would need to rewrite all the code for the validation data, so it’s good practice to wrap this into a function of the data and other important parameters like window_size_ms and window_stride_ms. Below, we will define a function called data_generator that will create the generator depending on those inputs.\n\n\ndata_generator <- function(df, batch_size, shuffle = TRUE, \n                           window_size_ms = 30, window_stride_ms = 10) {\n  \n  window_size <- as.integer(16000*window_size_ms/1000)\n  stride <- as.integer(16000*window_stride_ms/1000)\n  fft_size <- as.integer(2^trunc(log(window_size, 2)) + 1)\n  n_chunks <- length(seq(window_size/2, 16000 - window_size/2, stride))\n  \n  ds <- tensor_slices_dataset(df)\n  \n  if (shuffle) \n    ds <- ds %>% dataset_shuffle(buffer_size = 100)  \n  \n  ds <- ds %>%\n    dataset_map(function(obs) {\n      \n      # decoding wav files\n      audio_binary <- tf$read_file(tf$reshape(obs$fname, shape = list()))\n      wav <- audio_ops$decode_wav(audio_binary, desired_channels = 1)\n      \n      # create the spectrogram\n      spectrogram <- audio_ops$audio_spectrogram(\n        wav$audio, \n        window_size = window_size, \n        stride = stride,\n        magnitude_squared = TRUE\n      )\n      \n      spectrogram <- tf$log(tf$abs(spectrogram) + 0.01)\n      spectrogram <- tf$transpose(spectrogram, perm = c(1L, 2L, 0L))\n      \n      # transform the class_id into a one-hot encoded vector\n      response <- tf$one_hot(obs$class_id, 30L)\n      \n      list(spectrogram, response)\n    }) %>%\n    dataset_repeat()\n  \n  ds <- ds %>% \n    dataset_padded_batch(batch_size, list(shape(n_chunks, fft_size, NULL), shape(NULL)))\n  \n  ds\n}\n\nNow, we can define training and validation data generators. It’s worth noting that executing this won’t actually compute any spectrogram or read any file. It will only define in the TensorFlow graph how it should read and pre-process data.\n\n\nset.seed(6)\nid_train <- sample(nrow(df), size = 0.7*nrow(df))\n\nds_train <- data_generator(\n  df[id_train,], \n  batch_size = 32, \n  window_size_ms = 30, \n  window_stride_ms = 10\n)\nds_validation <- data_generator(\n  df[-id_train,], \n  batch_size = 32, \n  shuffle = FALSE, \n  window_size_ms = 30, \n  window_stride_ms = 10\n)\n\nTo actually get a batch from the generator we could create a TensorFlow session and ask it to run the generator. For example:\n\n\nsess <- tf$Session()\nbatch <- next_batch(ds_train)\nstr(sess$run(batch))\n\n\nList of 2\n $ : num [1:32, 1:98, 1:257, 1] -4.6 -4.6 -4.61 -4.6 -4.6 ...\n $ : num [1:32, 1:30] 0 0 0 0 0 0 0 0 0 0 ...\nEach time you run sess$run(batch) you should see a different batch of observations.\nModel definition\nNow that we know how we will feed our data we can focus on the model definition. The spectrogram can be treated like an image, so architectures that are commonly used in image recognition tasks should work well with the spectrograms too.\nWe will build a convolutional neural network similar to what we have built here for the MNIST dataset.\nThe input size is defined by the number of chunks and the FFT size. Like we explained earlier, they can be obtained from the window_size_ms and window_stride_ms used to generate the spectrogram.\n\n\nwindow_size <- as.integer(16000*window_size_ms/1000)\nstride <- as.integer(16000*window_stride_ms/1000)\nfft_size <- as.integer(2^trunc(log(window_size, 2)) + 1)\nn_chunks <- length(seq(window_size/2, 16000 - window_size/2, stride))\n\nWe will now define our model using the Keras sequential API:\n\n\nmodel <- keras_model_sequential()\nmodel %>%  \n  layer_conv_2d(input_shape = c(n_chunks, fft_size, 1), \n                filters = 32, kernel_size = c(3,3), activation = 'relu') %>% \n  layer_max_pooling_2d(pool_size = c(2, 2)) %>% \n  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>% \n  layer_max_pooling_2d(pool_size = c(2, 2)) %>% \n  layer_conv_2d(filters = 128, kernel_size = c(3,3), activation = 'relu') %>% \n  layer_max_pooling_2d(pool_size = c(2, 2)) %>% \n  layer_conv_2d(filters = 256, kernel_size = c(3,3), activation = 'relu') %>% \n  layer_max_pooling_2d(pool_size = c(2, 2)) %>% \n  layer_dropout(rate = 0.25) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = 'relu') %>% \n  layer_dropout(rate = 0.5) %>% \n  layer_dense(units = 30, activation = 'softmax')\n\nWe used 4 layers of convolutions combined with max pooling layers to extract features from the spectrogram images and 2 dense layers at the top. Our network is comparatively simple when compared to more advanced architectures like ResNet or DenseNet that perform very well on image recognition tasks.\nNow let’s compile our model. We will use categorical cross entropy as the loss function and use the Adadelta optimizer. It’s also here that we define that we will look at the accuracy metric during training.\n\n\nmodel %>% compile(\n  loss = loss_categorical_crossentropy,\n  optimizer = optimizer_adadelta(),\n  metrics = c('accuracy')\n)\n\nModel fitting\nNow, we will fit our model. In Keras we can use TensorFlow Datasets as inputs to the fit_generator function and we will do it here.\n\n\nmodel %>% fit_generator(\n  generator = ds_train,\n  steps_per_epoch = 0.7*nrow(df)/32,\n  epochs = 10, \n  validation_data = ds_validation, \n  validation_steps = 0.3*nrow(df)/32\n)\n\n\nEpoch 1/10\n1415/1415 [==============================] - 87s 62ms/step - loss: 2.0225 - acc: 0.4184 - val_loss: 0.7855 - val_acc: 0.7907\nEpoch 2/10\n1415/1415 [==============================] - 75s 53ms/step - loss: 0.8781 - acc: 0.7432 - val_loss: 0.4522 - val_acc: 0.8704\nEpoch 3/10\n1415/1415 [==============================] - 75s 53ms/step - loss: 0.6196 - acc: 0.8190 - val_loss: 0.3513 - val_acc: 0.9006\nEpoch 4/10\n1415/1415 [==============================] - 75s 53ms/step - loss: 0.4958 - acc: 0.8543 - val_loss: 0.3130 - val_acc: 0.9117\nEpoch 5/10\n1415/1415 [==============================] - 75s 53ms/step - loss: 0.4282 - acc: 0.8754 - val_loss: 0.2866 - val_acc: 0.9213\nEpoch 6/10\n1415/1415 [==============================] - 76s 53ms/step - loss: 0.3852 - acc: 0.8885 - val_loss: 0.2732 - val_acc: 0.9252\nEpoch 7/10\n1415/1415 [==============================] - 75s 53ms/step - loss: 0.3566 - acc: 0.8991 - val_loss: 0.2700 - val_acc: 0.9269\nEpoch 8/10\n1415/1415 [==============================] - 76s 54ms/step - loss: 0.3364 - acc: 0.9045 - val_loss: 0.2573 - val_acc: 0.9284\nEpoch 9/10\n1415/1415 [==============================] - 76s 53ms/step - loss: 0.3220 - acc: 0.9087 - val_loss: 0.2537 - val_acc: 0.9323\nEpoch 10/10\n1415/1415 [==============================] - 76s 54ms/step - loss: 0.2997 - acc: 0.9150 - val_loss: 0.2582 - val_acc: 0.9323\nThe model’s accuracy is 93.23%. Let’s learn how to make predictions and take a look at the confusion matrix.\nMaking predictions\nWe can use thepredict_generator function to make predictions on a new dataset. Let’s make predictions for our validation dataset. The predict_generator function needs a step argument which is the number of times the generator will be called.\nWe can calculate the number of steps by knowing the batch size, and the size of the validation dataset.\n\n\ndf_validation <- df[-id_train,]\nn_steps <- nrow(df_validation)/32 + 1\n\nWe can then use the predict_generator function:\n\n\npredictions <- predict_generator(\n  model, \n  ds_validation, \n  steps = n_steps\n  )\nstr(predictions)\n\n\nnum [1:19424, 1:30] 1.22e-13 7.30e-19 5.29e-10 6.66e-22 1.12e-17 ...\nThis will output a matrix with 30 columns - one for each word and n_steps*batch_size number of rows. Note that it starts repeating the dataset at the end to create a full batch.\nWe can compute the predicted class by taking the column with the highest probability, for example.\n\n\nclasses <- apply(predictions, 1, which.max) - 1\n\nA nice visualization of the confusion matrix is to create an alluvial diagram:\n\n\nlibrary(dplyr)\nlibrary(alluvial)\nx <- df_validation %>%\n  mutate(pred_class_id = head(classes, nrow(df_validation))) %>%\n  left_join(\n    df_validation %>% distinct(class_id, class) %>% rename(pred_class = class),\n    by = c(\"pred_class_id\" = \"class_id\")\n  ) %>%\n  mutate(correct = pred_class == class) %>%\n  count(pred_class, class, correct)\n\nalluvial(\n  x %>% select(class, pred_class),\n  freq = x$n,\n  col = ifelse(x$correct, \"lightblue\", \"red\"),\n  border = ifelse(x$correct, \"lightblue\", \"red\"),\n  alpha = 0.6,\n  hide = x$n < 20\n)\n\nAlluvial PlotWe can see from the diagram that the most relevant mistake our model makes is to classify “tree” as “three”. There are other common errors like classifying “go” as “no”, “up” as “off”. At 93% accuracy for 30 classes, and considering the errors we can say that this model is pretty reasonable.\nThe saved model occupies 25Mb of disk space, which is reasonable for a desktop but may not be on small devices. We could train a smaller model, with fewer layers, and see how much the performance decreases.\nIn speech recognition tasks its also common to do some kind of data augmentation by mixing a background noise to the spoken audio, making it more useful for real applications where it’s common to have other irrelevant sounds happening in the environment.\nThe full code to reproduce this tutorial is available here.\n\n\n",
    "preview": "https://upload.wikimedia.org/wikipedia/commons/6/61/FFT-Time-Frequency-View.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-04-02-rstudio-gpu-paperspace/",
    "title": "GPU Workstations in the Cloud with Paperspace",
    "description": "If you don't have local access to a modern NVIDIA GPU, your best bet is typically to run GPU intensive training jobs in the cloud. Paperspace is a cloud service that provides access to a fully preconfigured Ubuntu 16.04 desktop environment equipped with a GPU.",
    "author": [
      {
        "name": "J.J. Allaire",
        "url": "https://github.com/jjallaire"
      }
    ],
    "date": "2018-04-02",
    "categories": [
      "Cloud"
    ],
    "contents": "\nWe are very pleased to announce the availability of an RStudio TensorFlow template for the Paperspace cloud desktop service.\nIf you don’t have local access to a modern NVIDIA GPU, your best bet is typically to run GPU intensive training jobs in the cloud. Paperspace is a cloud service that provides access to a fully preconfigured Ubuntu 16.04 desktop environment equipped with a GPU. With the addition of the RStudio TensorFlow template you can now provision a ready to use RStudio TensorFlow w/ GPU workstation in just a few clicks. Preconfigured software includes:\nRStudio Desktop and RStudio Server\nNVIDIA GPU libraries (CUDA 8.0 and cuDNN 6.0)\nTensorFlow v1.4 w/ GPU\nThe R keras, tfestimators, and tensorflow packages.\nThe tidyverse suite of packages (ggplot2, dplyr, tidyr, readr, etc.)\nGetting Started\nTo get started, first signup for a Paperspace account (you can use the RSTUDIO promo code when you sign up to receive a $5 account credit).\nThen, create a new Paperspace instance using the RStudio template:\n\n\n\nThen, choose one of the Paperspace GPU instances (as opposed to the CPU instances). For example, here we select the P4000 machine type which includes an NVIDIA Quadro P4000 GPU:\n\n\n\nSee the Cloud Desktop GPUs with Paperspace article on the TensorFlow for R website for full details on getting started.\nTraining a Convolutional MNIST Model\nThe performance gains for training convoluational and recurrent models on GPUs can be substantial. Let’s try training the Keras MNIST CNN example on our new Paperspace instance:\n\n\n\nTraining the model for 12 epochs takes about 1 minute (~ 5 seconds per epoch). On the other hand, training the same model on CPU on a high end Macbook Pro takes 15 minutes! (~ 75 seconds per epoch). Using a Paperspace GPU yields a 15x performance gain in model training.\nThis model was trained on an NVIDIA Quadro P4000, which costs $0.40 per hour. Paperspace instances can be configured to automatically shut down after a period of inactivity to prevent accruing cloud charges when you aren’t actually using the machine.\nIf you are training convolutional or recurrent models and don’t currently have access to a local NVIDIA GPU, using RStudio on Paperspace is a great way to accelerate training performance. You can use the RSTUDIO promo code when you sign up for Paperspace to receive a $5 account credit.\n\n\n",
    "preview": "posts/2018-04-02-rstudio-gpu-paperspace/images/paperspace-mnist-cnn.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 2030,
    "preview_height": 1338
  },
  {
    "path": "posts/2018-03-09-lime-v04-the-kitten-picture-edition/",
    "title": "lime v0.4: The Kitten Picture Edition",
    "description": "A new major release of lime has landed on CRAN. lime is an R port of the Python library of the same name by Marco Ribeiro that allows the user to pry open black box machine learning models and explain their outcomes on a per-observation basis",
    "author": [
      {
        "name": "Thomas Lin Pedersen",
        "url": "https://github.com/thomasp85"
      }
    ],
    "date": "2018-03-09",
    "categories": [
      "Packages/Releases",
      "TensorFlow/Keras",
      "Explainability",
      "Image Recognition & Image Processing"
    ],
    "contents": "\nIntroduction\nI’m happy to report a new major release of lime has landed on CRAN. lime is an R port of the Python library of the same name by Marco Ribeiro that allows the user to pry open black box machine learning models and explain their outcomes on a per-observation basis. It works by modelling the outcome of the black box in the local neighborhood around the observation to explain and using this local model to explain why (not how) the black box did what it did. For more information about the theory of lime I will direct you to the article introducing the methodology.\nNew features\nThe meat of this release centers around two new features that are somewhat linked: Native support for keras models and support for explaining image models.\nkeras and images\nJ.J. Allaire was kind enough to namedrop lime during his keynote introduction of the tensorflow and keras packages and I felt compelled to support them natively. As keras is by far the most popular way to interface with tensorflow it is first in line for build-in support. The addition of keras means that lime now directly supports models from the following packages:\ncaret\nmlr\nxgboost\nh2o\nkeras\nIf you’re working on something too obscure or cutting edge to not be able to use these packages it is still possible to make your model lime compliant by providing predict_model() and model_type() methods for it.\nkeras models are used just like any other model, by passing it into the lime() function along with the training data in order to create an explainer object. Because we’re soon going to talk about image models, we’ll be using one of the pre-trained ImageNet models that is available from keras itself:\n\n\nlibrary(keras)\nlibrary(lime)\nlibrary(magick)\n\nmodel <- application_vgg16(\n  weights = \"imagenet\",\n  include_top = TRUE\n)\nmodel\n\n\nModel\n______________________________________________________________________________________________\nLayer (type)                              Output Shape                         Param #        \n==============================================================================================\ninput_1 (InputLayer)                      (None, 224, 224, 3)                  0              \n______________________________________________________________________________________________\nblock1_conv1 (Conv2D)                     (None, 224, 224, 64)                 1792           \n______________________________________________________________________________________________\nblock1_conv2 (Conv2D)                     (None, 224, 224, 64)                 36928          \n______________________________________________________________________________________________\nblock1_pool (MaxPooling2D)                (None, 112, 112, 64)                 0              \n______________________________________________________________________________________________\nblock2_conv1 (Conv2D)                     (None, 112, 112, 128)                73856          \n______________________________________________________________________________________________\nblock2_conv2 (Conv2D)                     (None, 112, 112, 128)                147584         \n______________________________________________________________________________________________\nblock2_pool (MaxPooling2D)                (None, 56, 56, 128)                  0              \n______________________________________________________________________________________________\nblock3_conv1 (Conv2D)                     (None, 56, 56, 256)                  295168         \n______________________________________________________________________________________________\nblock3_conv2 (Conv2D)                     (None, 56, 56, 256)                  590080         \n______________________________________________________________________________________________\nblock3_conv3 (Conv2D)                     (None, 56, 56, 256)                  590080         \n______________________________________________________________________________________________\nblock3_pool (MaxPooling2D)                (None, 28, 28, 256)                  0              \n______________________________________________________________________________________________\nblock4_conv1 (Conv2D)                     (None, 28, 28, 512)                  1180160        \n______________________________________________________________________________________________\nblock4_conv2 (Conv2D)                     (None, 28, 28, 512)                  2359808        \n______________________________________________________________________________________________\nblock4_conv3 (Conv2D)                     (None, 28, 28, 512)                  2359808        \n______________________________________________________________________________________________\nblock4_pool (MaxPooling2D)                (None, 14, 14, 512)                  0              \n______________________________________________________________________________________________\nblock5_conv1 (Conv2D)                     (None, 14, 14, 512)                  2359808        \n______________________________________________________________________________________________\nblock5_conv2 (Conv2D)                     (None, 14, 14, 512)                  2359808        \n______________________________________________________________________________________________\nblock5_conv3 (Conv2D)                     (None, 14, 14, 512)                  2359808        \n______________________________________________________________________________________________\nblock5_pool (MaxPooling2D)                (None, 7, 7, 512)                    0              \n______________________________________________________________________________________________\nflatten (Flatten)                         (None, 25088)                        0              \n______________________________________________________________________________________________\nfc1 (Dense)                               (None, 4096)                         102764544      \n______________________________________________________________________________________________\nfc2 (Dense)                               (None, 4096)                         16781312       \n______________________________________________________________________________________________\npredictions (Dense)                       (None, 1000)                         4097000        \n==============================================================================================\nTotal params: 138,357,544\nTrainable params: 138,357,544\nNon-trainable params: 0\n______________________________________________________________________________________________\n\nThe vgg16 model is an image classification model that has been build as part of the ImageNet competition where the goal is to classify pictures into 1000 categories with the highest accuracy. As we can see it is fairly complicated.\nIn order to create an explainer we will need to pass in the training data as well. For image data the training data is really only used to tell lime that we are dealing with an image model, so any image will suffice. The format for the training data is simply the path to the images, and because the internet runs on kitten pictures we’ll use one of these:\n\n\nimg <- image_read('https://www.data-imaginist.com/assets/images/kitten.jpg')\nimg_path <- file.path(tempdir(), 'kitten.jpg')\nimage_write(img, img_path)\nplot(as.raster(img))\n\n\nAs with text models the explainer will need to know how to prepare the input data for the model. For keras models this means formatting the image data as tensors. Thankfully keras comes with a lot of tools for reshaping image data:\n\n\nimage_prep <- function(x) {\n  arrays <- lapply(x, function(path) {\n    img <- image_load(path, target_size = c(224,224))\n    x <- image_to_array(img)\n    x <- array_reshape(x, c(1, dim(x)))\n    x <- imagenet_preprocess_input(x)\n  })\n  do.call(abind::abind, c(arrays, list(along = 1)))\n}\nexplainer <- lime(img_path, model, image_prep)\n\nWe now have an explainer model for understanding how the vgg16 neural network makes its predictions. Before we go along, lets see what the model think of our kitten:\n\n\nres <- predict(model, image_prep(img_path))\nimagenet_decode_predictions(res)\n\n\n[[1]]\n  class_name class_description      score\n1  n02124075      Egyptian_cat 0.48913878\n2  n02123045             tabby 0.15177219\n3  n02123159         tiger_cat 0.10270492\n4  n02127052              lynx 0.02638111\n5  n03793489             mouse 0.00852214\nSo, it is pretty sure about the whole cat thing. The reason we need to use imagenet_decode_predictions() is that the output of a keras model is always just a nameless tensor:\n\n\ndim(res)\n\n\n[1]    1 1000\n\n\ndimnames(res)\n\n\nNULL\nWe are used to classifiers knowing the class labels, but this is not the case for keras. Motivated by this, lime now have a way to define/overwrite the class labels of a model, using the as_classifier() function. Let’s redo our explainer:\n\n\nmodel_labels <- readRDS(system.file('extdata', 'imagenet_labels.rds', package = 'lime'))\nexplainer <- lime(img_path, as_classifier(model, model_labels), image_prep)\n\n\nThere is also an as_regressor() function which tells lime, without a doubt, that the model is a regression model. Most models can be introspected to see which type of model they are, but neural networks doesn’t really care. lime guesses the model type from the activation used in the last layer (linear activation == regression), but if that heuristic fails then as_regressor()/as_classifier() can be used.\n\nWe are now ready to poke into the model and find out what makes it think our image is of an Egyptian cat. But… first I’ll have to talk about yet another concept: superpixels (I promise I’ll get to the explanation part in a bit).\nIn order to create meaningful permutations of our image (remember, this is the central idea in lime), we have to define how to do so. The permutations needs to be substantial enough to have an impact on the image, but not so much that the model completely fails to recognise the content in every case - further, they should lead to an interpretable result. The concept of superpixels lends itself well to these constraints. In short, a superpixel is a patch of an area with high homogeneity, and superpixel segmentation is a clustering of image pixels into a number of superpixels. By segmenting the image to explain into superpixels we can turn area of contextual similarity on and off during the permutations and find out if that area is important. It is still necessary to experiment a bit as the optimal number of superpixels depend on the content of the image. Remember, we need them to be large enough to have an impact but not so large that the class probability becomes effectively binary. lime comes with a function to assess the superpixel segmentation before beginning the explanation and it is recommended to play with it a bit — with time you’ll likely get a feel for the right values:\n\n\n# default\nplot_superpixels(img_path)\n\n\n\n\n# Changing some settings\nplot_superpixels(img_path, n_superpixels = 200, weight = 40)\n\n\nThe default is set to a pretty low number of superpixels — if the subject of interest is relatively small it may be necessary to increase the number of superpixels so that the full subject does not end up in one, or a few superpixels. The weight parameter will allow you to make the segments more compact by weighting spatial distance higher than colour distance. For this example we’ll stick with the defaults.\nBe aware that explaining image models is much heavier than tabular or text data. In effect it will create 1000 new images per explanation (default permutation size for images) and run these through the model. As image classification models are often quite heavy, this will result in computation time measured in minutes. The permutation is batched (default to 10 permutations per batch), so you should not be afraid of running out of RAM or hard-drive space.\n\n\nexplanation <- explain(img_path, explainer, n_labels = 2, n_features = 20)\n\nThe output of an image explanation is a data frame of the same format as that from tabular and text data. Each feature will be a superpixel and the pixel range of the superpixel will be used as its description. Usually the explanation will only make sense in the context of the image itself, so the new version of lime also comes with a plot_image_explanation() function to do just that. Let’s see what our explanation have to tell us:\n\n\nplot_image_explanation(explanation)\n\n\nWe can see that the model, for both the major predicted classes, focuses on the cat, which is nice since they are both different cat breeds. The plot function got a few different functions to help you tweak the visual, and it filters low scoring superpixels away by default. An alternative view that puts more focus on the relevant superpixels, but removes the context can be seen by using display = 'block':\n\n\nplot_image_explanation(explanation, display = 'block', threshold = 0.01)\n\n\nWhile not as common with image explanations it is also possible to look at the areas of an image that contradicts the class:\n\n\nplot_image_explanation(explanation, threshold = 0, show_negative = TRUE, fill_alpha = 0.6)\n\n\nAs each explanation takes longer time to create and needs to be tweaked on a per-image basis, image explanations are not something that you’ll create in large batches as you might do with tabular and text data. Still, a few explanations might allow you to understand your model better and be used for communicating the workings of your model. Further, as the time-limiting factor in image explanations are the image classifier and not lime itself, it is bound to improve as image classifiers becomes more performant.\nGrab back\nApart from keras and image support, a slew of other features and improvements have been added. Here’s a quick overview:\nAll explanation plots now include the fit of the ridge regression used to make the explanation. This makes it easy to assess how good the assumptions about local linearity are kept.\nWhen explaining tabular data the default distance measure is now 'gower' from the gower package. gower makes it possible to measure distances between heterogeneous data without converting all features to numeric and experimenting with different exponential kernels.\nWhen explaining tabular data numerical features will no longer be sampled from a normal distribution during permutations, but from a kernel density defined by the training data. This should ensure that the permutations are more representative of the expected input.\nWrapping up\nThis release represents an important milestone for lime in R. With the addition of image explanations the lime package is now on par or above its Python relative, feature-wise. Further development will focus on improving the performance of the model, e.g. by adding parallelisation or improving the local model definition, as well as exploring alternative explanation types such as anchor.\nHappy Explaining!\n\n\n",
    "preview": "posts/2018-03-09-lime-v04-the-kitten-picture-edition/images/unnamed-chunk-8-1.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 1344,
    "preview_height": 672
  },
  {
    "path": "posts/2018-01-29-dl-for-cancer-immunotherapy/",
    "title": "Deep Learning for Cancer Immunotherapy",
    "description": "The aim of this post is to illustrate how deep learning is being applied in cancer immunotherapy (Immuno-oncology or Immunooncology) - a cancer treatment strategy, where the aim is to utilize the cancer patient's own immune system to fight the cancer.",
    "author": [
      {
        "name": "Leon Eyrich Jessen",
        "url": "https://twitter.com/jessenleon"
      }
    ],
    "date": "2018-01-29",
    "categories": [
      "TensorFlow/Keras",
      "Tabular Data"
    ],
    "contents": "\nIntroduction\nIn my research, I apply deep learning to unravel molecular interactions in the human immune system. One application of my research is within cancer immunotherapy (Immuno-oncology or Immunooncology) - a cancer treatment strategy, where the aim is to utilize the cancer patient’s own immune system to fight the cancer.\nThe aim of this post is to illustrates how deep learning is successfully being applied to model key molecular interactions in the human immune system. Molecular interactions are highly context dependent and therefore non-linear. Deep learning is a powerful tool to capture non-linearity and has therefore proven invaluable and highly successful. In particular in modelling the molecular interaction between the Major Histocompability Complex type I (MHCI) and peptides (The state-of-the-art model netMHCpan identifies 96.5% of natural peptides at a very high specificity of 98.5%).\nAdoptive T-cell therapy\nSome brief background before diving in. Special immune cells (T-cells) patrol our body, scanning the cells to check if they are healthy. On the surface of our cells is the MHCI - a highly specialized molecular system, which reflects the health status inside our cells. This is done by displaying small fragments of proteins called peptides, thus reflecting the inside of the cell. T-cells probe these molecular displays to check if the peptides are from our own body (self) or foreign (non-self), e.g. from a virus infection or cancer. If a displayed peptide is non-self, the T-cells has the power to terminate the cell.\nSimon Caulton, Adoptive T-cell therapy, CC BY-SA 3.0\nAdoptive T-cell therapy is a form of cancer immunotherapy that aims to isolate tumor infiltrating T-cells from the tumor in the patient, possibly genetically engineer them to be cancer-specific, grow them in great numbers and reintroduce them into the body to fight the cancer. In order to terminate cancer cells, the T-cell needs to be activated by being exposed to tumor peptides bound to MHCI (pMHCI). By analyzing the tumor genetics, relevant peptides can be identified and depending on the patients particular type of MHCI, we can predict which pMHCI are likely to be present in the tumor in the patient and thus which pMHCIs should be used to activate the T-cells.\nPeptide Classification Model\nFor this use case, we applied three models to classify whether a given peptide is a ‘strong binder’ SB, ‘weak binder’ WB or ‘non-binder’ NB. to MHCI (Specific type: HLA-A*02:01). Thereby, the classification uncovers which peptides, will be presented to the T-cells. The models we tested were:\nA deep feed forward fully connected ANN\nA convolutional ANN (connected to a FFN)\nA random forest (for comparison)\nNext, we’ll dive into building the artificial neural network. If you want to a more detailed explanation of cancer immunotherapy and how it interacts with the human immune system before going further, see the primer on cancer immunotherapy at the end of the post.\nPrerequisites\nThis example utilizes the keras package, several tidyverse packages, as well as the ggseqlogo and PepTools packages. You can install these packages as follows:\n\n\n# Keras + TensorFlow and it's dependencies\ninstall.packages(\"keras\")\nlibrary(keras)\ninstall_keras()\n\n# Tidyverse (readr, ggplot2, etc.)\ninstall.packages(\"tidyverse\")\n\n# Packages for sequence logos and peptides\ndevtools::install_github(\"omarwagih/ggseqlogo\")\ndevtools::install_github(\"leonjessen/PepTools\")\n\nWe can now load all of the packages we need for this example:\n\n\nlibrary(keras)\nlibrary(tidyverse)\nlibrary(PepTools)\n\nPeptide Data\nThe input data for this use case was created by generating 1,000,000 random 9-mer peptides by sampling the one-letter code for the 20 amino acids, i.e. ARNDCQEGHILKMFPSTWYV, and then submitting the peptides to MHCI binding prediction using the current state-of-the-art model netMHCpan. Different variants of MHCI exists, so for this case we chose HLA-A*02:01. This method assigns ‘strong binder’ SB, ‘weak binder’ WB or ‘non-binder’ NB to each peptide.\nSince n(SB) < n(WB) << n(NB), the data was subsequently balanced by down sampling, such that n(SB) = n(WB) = n(NB) = 7,920. Thus, a data set with a total of 23,760 data points was created. 10% of the data points were randomly assigned as test data and the remainder as train data. It should be noted that since the data set originates from a model, the outcome of this particular use case will be a model of a model. However, netMHCpan is very accurate (96.5% of natural ligands are identified at a very high specificity 98.5%).\nIn the following each peptide will be encoded by assigning a vector of 20 values, where each value is the probability of the amino acid mutating into 1 of the 20 others as defined by the BLOSUM62 matrix using the pep_encode() function from the PepTools package. This way each peptide is converted to an ‘image’ matrix with 9 rows and 20 columns.\nLet’s load the data:\n\n\npep_file <- get_file(\n  \"ran_peps_netMHCpan40_predicted_A0201_reduced_cleaned_balanced.tsv\", \n  origin = \"https://git.io/vb3Xa\"\n) \npep_dat <- read_tsv(file = pep_file)\n\nThe example peptide data looks like this:\n\n\npep_dat %>% head(5)\n\n\n# A tibble: 5 x 4\n  peptide   label_chr label_num data_type\n  <chr>     <chr>         <int> <chr>    \n1 LLTDAQRIV WB                1 train    \n2 LMAFYLYEV SB                2 train    \n3 VMSPITLPT WB                1 test     \n4 SLHLTNCFV WB                1 train    \n5 RQFTCMIAV WB                1 train   \nWhere peptide is the 9-mer peptides, label_chr defines whether the peptide was predicted by netMHCpan to be a strong-binder SB, weak-binder WB or NB non-binder to HLA-A*02:01.\nlabel_num is equivalent to label_chr, such that NB = 0, WB = 1 and SB = 2. Finally data_type defines whether the particular data point is part of the train set used to build the model or the ~10% data left out test set, which will be used for final performance evaluation.\nThe data has been balanced, as shown in this summary:\n\n\npep_dat %>% group_by(label_chr, data_type) %>% summarise(n = n())\n\n\n# A tibble: 6 x 3\n# Groups:   label_chr [?]\n  label_chr data_type     n\n  <chr>     <chr>     <int>\n1 NB        test        782\n2 NB        train      7138\n3 SB        test        802\n4 SB        train      7118\n5 WB        test        792\n6 WB        train      7128\nWe can use the ggseqlogo package to visualize the sequence motif for the strong binders using a sequence logo. This allows us to see which positions in the peptide and which amino acids are critical for the binding to MHC (Higher letters indicate more importance):\n\n\npep_dat %>% filter(label_chr=='SB') %>% pull(peptide) %>% ggseqlogo()\n\n\n\n\nFrom the sequence logo, it is evident, that L,M,I,V are found often at p2 and p9 amongst the strong binders. In fact these position are referred to as the anchor positions, which interact with the MHCI. The T-cell on the other hand, will recognize p3-p8.\nData Preparation\nWe are creating a model f, where x is the peptide and y is one of three classes SB, WB and NB, such that f(x) = y. Each x is encoded into a 2-dimensional ‘image’, which we can visualize using the pep_plot_images() function:\n\n\npep_dat %>% filter(label_chr=='SB') %>% head(1) %>% pull(peptide) %>% pep_plot_images\n\n\n\n\nTo feed data into a neural network we need to encode it as a multi-dimensional array (or “tensor”). For this dataset we can do this with the PepTools::pep_encode() function, which takes a character vector of peptides and transforms them into a 3D array of ‘total number of peptides’ x ‘length of each peptide (9)’ x ‘number of unique amino acids (20)’. For example:\n\n\nstr(pep_encode(c(\"LLTDAQRIV\", \"LLTDAQRIV\")))\n\n\n num [1:2, 1:9, 1:20] 0.0445 0.0445 0.0445 0.0445 0.073 ...\nHere’s how we transform the data frame into 3-D arrays of training and test data:\n\n\nx_train <- pep_dat %>% filter(data_type == 'train') %>% pull(peptide)   %>% pep_encode\ny_train <- pep_dat %>% filter(data_type == 'train') %>% pull(label_num) %>% array\nx_test  <- pep_dat %>% filter(data_type == 'test')  %>% pull(peptide)   %>% pep_encode\ny_test  <- pep_dat %>% filter(data_type == 'test')  %>% pull(label_num) %>% array\n\nTo prepare the data for training we convert the 3-d arrays into matrices by reshaping width and height into a single dimension (9x20 peptide ‘images’ are flattened into vectors of lengths 180):\n\n\nx_train <- array_reshape(x_train, c(nrow(x_train), 9, 20, 1))\nx_test  <- array_reshape(x_test, c(nrow(x_test), 9, 20, 1))\n\nThe y data is an integer vector with values ranging from 0 to 2. To prepare this data for training we one-hot encode the vectors into binary class matrices using the Keras to_categorical function:\n\n\ny_train <- to_categorical(y_train, num_classes = 3)\ny_test  <- to_categorical(y_test,  num_classes = 3)\n\nDefining the Model\nThe core data structure of Keras is a model, a way to organize layers. The simplest type of model is the sequential model, a linear stack of layers. We begin by creating a sequential model and then adding layers using the pipe (%>%) operator:\n\n\nmodel <- keras_model_sequential() %>% \n  layer_dense(units  = 180, activation = 'relu', input_shape = 180) %>% \n  layer_dropout(rate = 0.4) %>% \n  layer_dense(units  = 90, activation  = 'relu') %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units  = 3, activation   = 'softmax')\n\nA dense layer is a standard neural network layer with each input node is connected to an output node. A dropout layer sets a random proportion of activations from the previous layer to 0, which helps to prevent overfitting.\nThe input_shape argument to the first layer specifies the shape of the input data (a length 180 numeric vector representing a peptide ‘image’). The final layer outputs a length 3 numeric vector (probabilities for each class SB, WB and NB) using a softmax activation function.\nWe can use the summary() function to print the details of the model:\n\n\nsummary(model)\n\n\nLayer (type)                        Output Shape                    Param #     \n================================================================================\ndense_1 (Dense)                     (None, 180)                     32580       \n________________________________________________________________________________\ndropout_1 (Dropout)                 (None, 180)                     0           \n________________________________________________________________________________\ndense_2 (Dense)                     (None, 90)                      16290       \n________________________________________________________________________________\ndropout_2 (Dropout)                 (None, 90)                      0           \n________________________________________________________________________________\ndense_3 (Dense)                     (None, 3)                       273         \n================================================================================\nTotal params: 49,143\nTrainable params: 49,143\nNon-trainable params: 0\n________________________________________________________________________________\nNext, we compile the model with appropriate loss function, optimizer, and metrics:\n\n\nmodel %>% compile(\n  loss      = 'categorical_crossentropy',\n  optimizer = optimizer_rmsprop(),\n  metrics   = c('accuracy')\n)\n\nTraining and Evaluation\nWe use the fit() function to train the model for 150 epochs using batches of 50 peptide ‘images’:\n\n\nhistory = model %>% fit(\n  x_train, y_train, \n  epochs = 150, \n  batch_size = 50, \n  validation_split = 0.2\n)\n\nWe can visualize the training progress by plotting the history object returned from fit():\n\n\nplot(history)\n\n\n\n\nWe can now evaluate the model’s performance on the original ~10% left out test data:\n\n\nperf = model %>% evaluate(x_test, y_test)\nperf\n\n\n$loss\n[1] 0.2449334\n\n$acc\n[1] 0.9461279\nWe can also visualize the predictions on the test data:\n\n\nacc     = perf$acc %>% round(3)*100\ny_pred  = model %>% predict_classes(x_test)\ny_real  = y_test %>% apply(1,function(x){ return( which(x==1) - 1) })\nresults = tibble(y_real = y_real %>% factor, y_pred = y_pred %>% factor,\n                 Correct = ifelse(y_real == y_pred,\"yes\",\"no\") %>% factor)\ntitle = 'Performance on 10% unseen data - Feed Forward Neural Network'\nxlab  = 'Measured (Real class, as predicted by netMHCpan-4.0)'\nylab  = 'Predicted (Class assigned by Keras/TensorFlow deep FFN)'\nresults %>%\n  ggplot(aes(x = y_pred, y = y_real, colour = Correct)) +\n  geom_point() +\n  ggtitle(label = title, subtitle = paste0(\"Accuracy = \", acc,\"%\")) +\n  xlab(xlab) +\n  ylab(ylab) +\n  scale_color_manual(labels = c('No', 'Yes'),\n                     values = c('tomato','cornflowerblue')) +\n  geom_jitter() +\n  theme_bw()\n\n\n\n\nThe final result was a performance on the 10% unseen data of just short of 95% accuracy.\nConvolutional Neural Network\nIn order to test a more complex architecture, we also implemented a Convolutional Neural Network. To make the comparison, we repeated the data preparation as described above and only changed the architecture by including a single 2d convolutional layer and then feeding that into the same architecture as the FFN above:\n\n\nmodel <- keras_model_sequential() %>%\n  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',\n                input_shape = c(9, 20, 1)) %>%\n  layer_dropout(rate = 0.25) %>% \n  layer_flatten() %>% \n  layer_dense(units  = 180, activation = 'relu') %>% \n  layer_dropout(rate = 0.4) %>% \n  layer_dense(units  = 90, activation  = 'relu') %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units  = 3, activation   = 'softmax')\n\n\n\n\nThis resulted in a performance on the 10% unseen data of 92% accuracy.\nOne might have expected the CNN to be able to better capture the information in the peptide ‘images’. There is however a crucial difference between the peptide ‘images’ and the e.g. MNIST dataset. The peptide ‘images’ do not contain edges and spatially arranged continuous structures, rather they are a set of pixels with p2 always at p2 and likewise for p9, which are determinants for binding.\nRandom Forest\nKnowing that deep ;earning is not necessarily the right tool for all prediction tasks, we also created a random forest model on the exact same data using the randomForest package.\nThe x and y training data was prepared slightly different using PepTools::pep_encode_mat\n\n\n# Setup training data\ntarget  <- 'train'\nx_train <- pep_dat %>% filter(data_type==target) %>% pull(peptide) %>%\n  pep_encode_mat %>% select(-peptide)\ny_train <- pep_dat %>% filter(data_type==target) %>% pull(label_num) %>% factor\n\n# Setup test data\ntarget <- 'test'\nx_test <- pep_dat %>% filter(data_type==target) %>% pull(peptide) %>%\n  pep_encode_mat %>% select(-peptide)\ny_test <- pep_dat %>% filter(data_type==target) %>% pull(label_num) %>% factor\n\nThe random forest model was then run using 100 trees like so:\n\n\nrf_classifier <- randomForest(x = x_train, y = y_train, ntree = 100)\n\nThe results of the model were collected as follows:\n\n\ny_pred    <- predict(rf_classifier, x_test)\nn_correct <- table(observed = y_test, predicted = y_pred) %>% diag %>% sum\nacc       <- (n_correct / length(y_test)) %>% round(3) * 100\nresults   <- tibble(y_real  = y_test,\n                   y_pred  = y_pred,\n                   Correct = ifelse(y_real == y_pred,\"yes\",\"no\") %>% factor)\n\nWe can then visualize the performance as we did with the FFN and the CNN:\n\n\ntitle = \"Performance on 10% unseen data - Random Forest\"\nxlab  = \"Measured (Real class, as predicted by netMHCpan-4.0)\"\nylab  = \"Predicted (Class assigned by random forest)\"\nf_out = \"plots/03_rf_01_results_3_by_3_confusion_matrix.png\"\nresults %>%\n  ggplot(aes(x = y_pred, y = y_real, colour = Correct)) +\n  geom_point() +\n  xlab(xlab) +\n  ylab(ylab) +\n  ggtitle(label = title, subtitle = paste0(\"Accuracy = \", acc,\"%\")) +\n  scale_color_manual(labels = c('No', 'Yes'),\n                     values = c('tomato','cornflowerblue')) +\n  geom_jitter() +\n  theme_bw()\n\n\n\n\nConclusion\nIn this post you have been shown how we build 3 models: A Feed Forward Neural Network (FFN), a Convolutional Neural Network (CNN) and a Random Forest (RF). Using the same data, we obtained performances of ~95%, ~92% and ~82% for the FFN, CNN and RF respectively. The R-code for these models are available here:\nFeed Forward Neural Network\nConvolutional Neural Network\nRandom Forest\nIt is evident that the deep learning models capture the information in the system much better than the random forest model. However, the CNN model didn’t not perform as well as the straightforward FFN. This illustrates one of the pitfalls of deep learning - blind alleys. There are a huge number of architectures available, and when combined with hyperparameter tuning the potential model space is breathtakingly large.\nTo increase the likelihood of finding a good architecture and the right hyper-parameters it is important to know and understand the data you are modeling. Also, if possible include several sources of data. For the case of peptide-MHC interaction, we include not only information of the strength of the binding as measured in the laboratory, but also information from actual human cells, where peptide-MHC complexes are extracted and analysed.\nIt should be noted that when we build models in the research group, a lot of work goes into creating balanced training and test sets. Models are also trained and evaluated using cross-validation, usually 5-fold. We then save each of the five models and create an ensemble prediction - wisdom-of-the-crowd. We are very careful to avoiding overfitting as this of course decreases the models extrapolation performance.\nThere is no doubt that deep learning already plays a major role in unraveling the complexities of the human immune system and associated diseases. With the release of TensorFlow by Google along with the keras and tensorflow R packages we now have the tools available in R to explore this frontier.\nPrimer on Cancer Immunotherapy\nHere is an elaborated background on DNA, proteins and cancer 1. However, brief and simplified as this is naturally a hugely complex subject.\nDNA\nThe cell is the basic unit of life. Each cell in our body harbors ~2 meters (6 feet) of DNA, which is identical across all cells. DNA makes up the blue print for our body - our genetic code - using only four nucleic acids (hence the name DNA = DeoxyriboNucleic Acid). We can represent the genetic code, using: a,c,g and t. Each cell carries ~3,200,000,000 of these letters, which constitute the blue print for our entire body. The letters are organised into ~20,000 genes and from the genes we get proteins. In Bioinformatics, we represent DNA sequences as repeats of the four nucleotides, e.g. ctccgacgaatttcatgttcagggatagct....\nProteins\nComparing with a building - if DNA is the blue print of how to construct a building, then the proteins are the bricks, windows, chimney, plumbing etc. Some proteins are structural (like a brick), whereas others are functional (like a window you can open and close). All ~100,000 proteins in our body are made by of only 20 small molecules called amino acids. Like with DNA, we can represent these 20 amino acids using: A,R,N,D,C,Q,E,G,H,I,L,K,M,F,P,S,T,W,Y and V (note lowercase for DNA and uppercase for amino acids). The average size of a protein in the human body ~300 amino acids and the sequence is the combination of the 20 amino acids making up the protein written consecutively, e.g.: MRYEMGYWTAFRRDCRCTKSVPSQWEAADN.... The attentive reader will notice, that I mentioned ~20,000 genes, from which we get ~100,000 proteins. This is due to the DNA in one gene being able to join in different ways and thus produce more than one protein.\nPeptides\nA peptide is a small fragment of a protein of length ~5-15 amino acids. MHCI predominantly binds peptides containing 9 amino acids - A so called 9-mer. Peptides play a crucial role in the monitoring of cells in our body by the human immune system. The data used in this use case consist solely of 9-mers.\nThe Human Immune System\nInside each cell, proteins are constantly being produced from DNA. In order not to clutter the cell, proteins are also constantly broken down into peptides which are then recycled to produce new proteins. Some of these peptides are caught by a system and bound to MHCI (Major Histocompatibility Complex type 1, MHCI) and transported from inside of the cell to the outside, where the peptide is displayed. The viewer of this display is the human immune system. Special immune cells (T-cells) patrol the body, looking for cells displaying unexpected peptides. If a displayed peptide is unexpected, the T-cells will terminate the cell. The T-cells have been educated to recognize foreign peptides (non-self) and ignore peptides which originate from our own body (self). This is the hallmark of the immune system - Protecting us by distinguishing self from non-self. I the immune system is not active enough and thus fails to recognize non-self arising from an infection it is potentially fatal. On the other hand if the immune system is too active and starts recognizing not only non-self, but also self, you get autoimmune disease, which likewise is potentially fatal.\nCancer\nCancer arises when errors (mutations) occur inside the cell, resulting in changed proteins. This means that if the original protein was e.g. MRYEMGYWTAFRRDCRCTKSVPSQWEAADN..., then the new erroneous protein could be e.g. MRYEMGYWTAFRRDCRCTKSVPSQWEAADR.... The result of this is that the peptide displayed on the cell surface is altered. The T-cells will now recognize the peptide as unexpected and terminate the cell. However, the environment around a cancer tumor is very hostile to the T-cells, which are supposed to recognize and terminate the cell.\nCancer Immunotherapy aims at taking a sample of the tumor and isolate the T-cells, grow them in great numbers and then reintroduce them into the body. Now, despite the hostile environment around the tumor, sheer numbers result in the T-cells out competing the tumor. A special branch of cancer immunotherapy aims at introducing T-cells, which have been specially engineered to recognize a tumor. However, in this case it is of utmost importance to ensure that the T-cell does indeed recognize the tumor and nothing else than the tumor. If introduced T-cells recognize healthy tissue, the outcome can be fatal. It is therefore extremely important to understand the molecular interaction between the sick cell, i.e. the peptide and the MHCI, and the T-cell.\nOur peptide classification model illustrates how deep learning is being applied to increase our understanding of the molecular interactions governing the activation of the T-cells.\nInside Life Science, Genetics by the Numbers: https://publications.nigms.nih.gov/insidelifescience/genetics-numbers.html↩︎\n",
    "preview": "posts/2018-01-29-dl-for-cancer-immunotherapy/images/01_ffn_02_results_3_by_3_confusion_matrix.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 3000,
    "preview_height": 1800
  },
  {
    "path": "posts/2018-01-24-keras-fraud-autoencoder/",
    "title": "Predicting Fraud with Autoencoders and Keras",
    "description": "In this post we will train an autoencoder to detect credit card fraud. We will also demonstrate how to train Keras models in the cloud using CloudML. The basis of our model will be the Kaggle Credit Card Fraud Detection dataset.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": "https://github.com/dfalbel"
      }
    ],
    "date": "2018-01-25",
    "categories": [
      "TensorFlow/Keras",
      "Unsupervised Learning",
      "Cloud"
    ],
    "contents": "\nOverview\nIn this post we will train an autoencoder to detect credit card fraud. We will also demonstrate how to train Keras models in the cloud using CloudML.\nThe basis of our model will be the Kaggle Credit Card Fraud Detection dataset, which was collected during a research collaboration of Worldline and the Machine Learning Group of ULB (Université Libre de Bruxelles) on big data mining and fraud detection.\nThe dataset contains credit card transactions by European cardholders made over a two day period in September 2013. There are 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for only 0.172% of all transactions.\nReading the data\nAfter downloading the data from Kaggle, you can read it in to R with read_csv():\n\n\nlibrary(readr)\ndf <- read_csv(\"data-raw/creditcard.csv\", col_types = list(Time = col_number()))\n\nThe input variables consist of only numerical values which are the result of a PCA transformation. In order to preserve confidentiality, no more information about the original features was provided. The features V1, …, V28 were obtained with PCA. There are however 2 features (Time and Amount) that were not transformed. Time is the seconds elapsed between each transaction and the first transaction in the dataset. Amount is the transaction amount and could be used for cost-sensitive learning. The Class variable takes value 1 in case of fraud and 0 otherwise.\nAutoencoders\nSince only 0.172% of the observations are frauds, we have a highly unbalanced classification problem. With this kind of problem, traditional classification approaches usually don’t work very well because we have only a very small sample of the rarer class.\nAn autoencoder is a neural network that is used to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. For this problem we will train an autoencoder to encode non-fraud observations from our training set. Since frauds are supposed to have a different distribution then normal transactions, we expect that our autoencoder will have higher reconstruction errors on frauds then on normal transactions. This means that we can use the reconstruction error as a quantity that indicates if a transaction is fraudulent or not.\nIf you want to learn more about autoencoders, a good starting point is this video from Larochelle on YouTube and Chapter 14 from the Deep Learning book by Goodfellow et al.\nVisualization\nFor an autoencoder to work well we have a strong initial assumption: that the distribution of variables for normal transactions is different from the distribution for fraudulent ones. Let’s make some plots to verify this. Variables were transformed to a [0,1] interval for plotting.\n\n\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ggridges)\ndf %>%\n  gather(variable, value, -Class) %>%\n  ggplot(aes(y = as.factor(variable), \n             fill = as.factor(Class), \n             x = percent_rank(value))) +\n  geom_density_ridges()\n\n\nWe can see that distributions of variables for fraudulent transactions are very different then from normal ones, except for the Time variable, which seems to have the exact same distribution.\nPreprocessing\nBefore the modeling steps we need to do some preprocessing. We will split the dataset into train and test sets and then we will Min-max normalize our data (this is done because neural networks work much better with small input values). We will also remove the Time variable as it has the exact same distribution for normal and fraudulent transactions.\nBased on the Time variable we will use the first 200,000 observations for training and the rest for testing. This is good practice because when using the model we want to predict future frauds based on transactions that happened before.\n\n\ndf_train <- df %>% filter(row_number(Time) <= 200000) %>% select(-Time)\ndf_test <- df %>% filter(row_number(Time) > 200000) %>% select(-Time)\n\nNow let’s work on normalization of inputs. We created 2 functions to help us. The first one gets descriptive statistics about the dataset that are used for scaling. Then we have a function to perform the min-max scaling. It’s important to note that we applied the same normalization constants for training and test sets.\n\n\nlibrary(purrr)\n\n#' Gets descriptive statistics for every variable in the dataset.\nget_desc <- function(x) {\n  map(x, ~list(\n    min = min(.x),\n    max = max(.x),\n    mean = mean(.x),\n    sd = sd(.x)\n  ))\n} \n\n#' Given a dataset and normalization constants it will create a min-max normalized\n#' version of the dataset.\nnormalization_minmax <- function(x, desc) {\n  map2_dfc(x, desc, ~(.x - .y$min)/(.y$max - .y$min))\n}\n\nNow let’s create normalized versions of our datasets. We also transformed our data frames to matrices since this is the format expected by Keras.\n\n\ndesc <- df_train %>% \n  select(-Class) %>% \n  get_desc()\n\nx_train <- df_train %>%\n  select(-Class) %>%\n  normalization_minmax(desc) %>%\n  as.matrix()\n\nx_test <- df_test %>%\n  select(-Class) %>%\n  normalization_minmax(desc) %>%\n  as.matrix()\n\ny_train <- df_train$Class\ny_test <- df_test$Class\n\nModel definition\nWe will now define our model in Keras, a symmetric autoencoder with 4 dense layers.\n\n\nlibrary(keras)\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_dense(units = 15, activation = \"tanh\", input_shape = ncol(x_train)) %>%\n  layer_dense(units = 10, activation = \"tanh\") %>%\n  layer_dense(units = 15, activation = \"tanh\") %>%\n  layer_dense(units = ncol(x_train))\n\nsummary(model)\n\n\n___________________________________________________________________________________\nLayer (type)                         Output Shape                     Param #      \n===================================================================================\ndense_1 (Dense)                      (None, 15)                       450          \n___________________________________________________________________________________\ndense_2 (Dense)                      (None, 10)                       160          \n___________________________________________________________________________________\ndense_3 (Dense)                      (None, 15)                       165          \n___________________________________________________________________________________\ndense_4 (Dense)                      (None, 29)                       464          \n===================================================================================\nTotal params: 1,239\nTrainable params: 1,239\nNon-trainable params: 0\n___________________________________________________________________________________\nWe will then compile our model, using the mean squared error loss and the Adam optimizer for training.\n\n\nmodel %>% compile(\n  loss = \"mean_squared_error\", \n  optimizer = \"adam\"\n)\n\nTraining the model\nWe can now train our model using the fit() function. Training the model is reasonably fast (~ 14s per epoch on my laptop). We will only feed to our model the observations of normal (non-fraudulent) transactions.\nWe will use callback_model_checkpoint() in order to save our model after each epoch. By passing the argument save_best_only = TRUE we will keep on disk only the epoch with smallest loss value on the test set. We will also use callback_early_stopping() to stop training if the validation loss stops decreasing for 5 epochs.\n\n\ncheckpoint <- callback_model_checkpoint(\n  filepath = \"model.hdf5\", \n  save_best_only = TRUE, \n  period = 1,\n  verbose = 1\n)\n\nearly_stopping <- callback_early_stopping(patience = 5)\n\nmodel %>% fit(\n  x = x_train[y_train == 0,], \n  y = x_train[y_train == 0,], \n  epochs = 100, \n  batch_size = 32,\n  validation_data = list(x_test[y_test == 0,], x_test[y_test == 0,]), \n  callbacks = list(checkpoint, early_stopping)\n)\n\n\nTrain on 199615 samples, validate on 84700 samples\nEpoch 1/100\n199615/199615 [==============================] - 17s 83us/step - loss: 0.0036 - val_loss: 6.8522e-04d from inf to 0.00069, saving model to model.hdf5\nEpoch 2/100\n199615/199615 [==============================] - 17s 86us/step - loss: 4.7817e-04 - val_loss: 4.7266e-04d from 0.00069 to 0.00047, saving model to model.hdf5\nEpoch 3/100\n199615/199615 [==============================] - 19s 94us/step - loss: 3.7753e-04 - val_loss: 4.2430e-04d from 0.00047 to 0.00042, saving model to model.hdf5\nEpoch 4/100\n199615/199615 [==============================] - 19s 94us/step - loss: 3.3937e-04 - val_loss: 4.0299e-04d from 0.00042 to 0.00040, saving model to model.hdf5\nEpoch 5/100\n199615/199615 [==============================] - 19s 94us/step - loss: 3.2259e-04 - val_loss: 4.0852e-04 improve\nEpoch 6/100\n199615/199615 [==============================] - 18s 91us/step - loss: 3.1668e-04 - val_loss: 4.0746e-04 improve\n...\nAfter training we can get the final loss for the test set by using the evaluate() fucntion.\n\n\nloss <- evaluate(model, x = x_test[y_test == 0,], y = x_test[y_test == 0,])\nloss\n\n\n        loss \n0.0003534254 \nTuning with CloudML\nWe may be able to get better results by tuning our model hyperparameters. We can tune, for example, the normalization function, the learning rate, the activation functions and the size of hidden layers. CloudML uses Bayesian optimization to tune hyperparameters of models as described in this blog post.\nWe can use the cloudml package to tune our model, but first we need to prepare our project by creating a training flag for each hyperparameter and a tuning.yml file that will tell CloudML what parameters we want to tune and how.\nThe full script used for training on CloudML can be found at https://github.com/dfalbel/fraud-autoencoder-example. The most important modifications to the code were adding the training flags:\n\n\nFLAGS <- flags(\n  flag_string(\"normalization\", \"minmax\", \"One of minmax, zscore\"),\n  flag_string(\"activation\", \"relu\", \"One of relu, selu, tanh, sigmoid\"),\n  flag_numeric(\"learning_rate\", 0.001, \"Optimizer Learning Rate\"),\n  flag_integer(\"hidden_size\", 15, \"The hidden layer size\")\n)\n\nWe then used the FLAGS variable inside the script to drive the hyperparameters of the model, for example:\n\n\nmodel %>% compile(\n  optimizer = optimizer_adam(lr = FLAGS$learning_rate), \n  loss = 'mean_squared_error',\n)\n\nWe also created a tuning.yml file describing how hyperparameters should be varied during training, as well as what metric we wanted to optimize (in this case it was the validation loss: val_loss).\ntuning.yml\n\ntrainingInput:\n  scaleTier: CUSTOM\n  masterType: standard_gpu\n  hyperparameters:\n    goal: MINIMIZE\n    hyperparameterMetricTag: val_loss\n    maxTrials: 10\n    maxParallelTrials: 5\n    params:\n      - parameterName: normalization\n        type: CATEGORICAL\n        categoricalValues: [zscore, minmax]\n      - parameterName: activation\n        type: CATEGORICAL\n        categoricalValues: [relu, selu, tanh, sigmoid]\n      - parameterName: learning_rate\n        type: DOUBLE\n        minValue: 0.000001\n        maxValue: 0.1\n        scaleType: UNIT_LOG_SCALE\n      - parameterName: hidden_size\n        type: INTEGER\n        minValue: 5\n        maxValue: 50\n        scaleType: UNIT_LINEAR_SCALE\nWe describe the type of machine we want to use (in this case a standard_gpu instance), the metric we want to minimize while tuning, and the the maximum number of trials (i.e. number of combinations of hyperparameters we want to test). We then specify how we want to vary each hyperparameter during tuning.\nYou can learn more about the tuning.yml file at the Tensorflow for R documentation and at Google’s official documentation on CloudML.\nNow we are ready to send the job to Google CloudML. We can do this by running:\n\n\nlibrary(cloudml)\ncloudml_train(\"train.R\", config = \"tuning.yml\")\n\nThe cloudml package takes care of uploading the dataset and installing any R package dependencies required to run the script on CloudML. If you are using RStudio v1.1 or higher, it will also allow you to monitor your job in a background terminal. You can also monitor your job using the Google Cloud Console.\nAfter the job is finished we can collect the job results with:\n\n\njob_collect()\n\nThis will copy the files from the job with the best val_loss performance on CloudML to your local system and open a report summarizing the training run.\n\nSince we used a callback to save model checkpoints during training, the model file was also copied from Google CloudML. Files created during training are copied to the “runs” subdirectory of the working directory from which cloudml_train() is called. You can determine this directory for the most recent run with:\n\n\nlatest_run()$run_dir\n\n\n[1] runs/cloudml_2018_01_23_221244595-03\nYou can also list all previous runs and their validation losses with:\n\n\nls_runs(order = metric_val_loss, decreasing = FALSE)\n\n\n                    run_dir metric_loss metric_val_loss\n1 runs/2017-12-09T21-01-11Z      0.2577          0.1482\n2 runs/2017-12-09T21-00-11Z      0.2655          0.1505\n3 runs/2017-12-09T19-59-44Z      0.2597          0.1402\n4 runs/2017-12-09T19-56-48Z      0.2610          0.1459\n\nUse View(ls_runs()) to view all columns\nIn our case the job downloaded from CloudML was saved to runs/cloudml_2018_01_23_221244595-03/, so the saved model file is available at runs/cloudml_2018_01_23_221244595-03/model.hdf5. We can now use our tuned model to make predictions.\nMaking predictions\nNow that we trained and tuned our model we are ready to generate predictions with our autoencoder. We are interested in the MSE for each observation and we expect that observations of fraudulent transactions will have higher MSE’s.\nFirst, let’s load our model.\n\n\nmodel <- load_model_hdf5(\"runs/cloudml_2018_01_23_221244595-03/model.hdf5\", \n                         compile = FALSE)\n\nNow let’s calculate the MSE for the training and test set observations.\n\n\npred_train <- predict(model, x_train)\nmse_train <- apply((x_train - pred_train)^2, 1, sum)\n\npred_test <- predict(model, x_test)\nmse_test <- apply((x_test - pred_test)^2, 1, sum)\n\nA good measure of model performance in highly unbalanced datasets is the Area Under the ROC Curve (AUC). AUC has a nice interpretation for this problem, it’s the probability that a fraudulent transaction will have higher MSE then a normal one. We can calculate this using the Metrics package, which implements a wide variety of common machine learning model performance metrics.\n\n\nlibrary(Metrics)\nauc(y_train, mse_train)\nauc(y_test, mse_test)\n\n\n[1] 0.9546814\n[1] 0.9403554\nTo use the model in practice for making predictions we need to find a threshold \\(k\\) for the MSE, then if if \\(MSE > k\\) we consider that transaction a fraud (otherwise we consider it normal). To define this value it’s useful to look at precision and recall while varying the threshold \\(k\\).\n\n\npossible_k <- seq(0, 0.5, length.out = 100)\nprecision <- sapply(possible_k, function(k) {\n  predicted_class <- as.numeric(mse_test > k)\n  sum(predicted_class == 1 & y_test == 1)/sum(predicted_class)\n})\n\nqplot(possible_k, precision, geom = \"line\") \n  + labs(x = \"Threshold\", y = \"Precision\")\n\n\n\n\nrecall <- sapply(possible_k, function(k) {\n  predicted_class <- as.numeric(mse_test > k)\n  sum(predicted_class == 1 & y_test == 1)/sum(y_test)\n})\nqplot(possible_k, recall, geom = \"line\") \n  + labs(x = \"Threshold\", y = \"Recall\")\n\n\nA good starting point would be to choose the threshold with maximum precision but we could also base our decision on how much money we might lose from fraudulent transactions.\nSuppose each manual verification of fraud costs us $1 but if we don’t verify a transaction and it’s a fraud we will lose this transaction amount. Let’s find for each threshold value how much money we would lose.\n\n\ncost_per_verification <- 1\n\nlost_money <- sapply(possible_k, function(k) {\n  predicted_class <- as.numeric(mse_test > k)\n  sum(cost_per_verification * predicted_class + (predicted_class == 0) * y_test * df_test$Amount) \n})\n\nqplot(possible_k, lost_money, geom = \"line\") + labs(x = \"Threshold\", y = \"Lost Money\")\n\n\nWe can find the best threshold in this case with:\n\n\npossible_k[which.min(lost_money)]\n\n\n[1] 0.005050505\nIf we needed to manually verify all frauds, it would cost us ~$13,000. Using our model we can reduce this to ~$2,500.\n\n\n",
    "preview": "posts/2018-01-24-keras-fraud-autoencoder/images/preview.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 790,
    "preview_height": 537
  },
  {
    "path": "posts/2018-01-24-analyzing-rtweet-data-with-kerasformula/",
    "title": "Analyzing rtweet Data with kerasformula",
    "description": "The kerasformula package offers a high-level interface for the R interface to Keras. It’s main interface is the kms function, a regression-style interface to keras_model_sequential that uses formulas and sparse matrices. We use kerasformula to predict how popular tweets will be based on how often the tweet was retweeted and favorited.",
    "author": [
      {
        "name": "Pete Mohanty",
        "url": "https://sites.google.com/site/petemohanty/"
      }
    ],
    "date": "2018-01-24",
    "categories": [
      "TensorFlow/Keras",
      "Natural Language Processing"
    ],
    "contents": "\nOverview\nThe kerasformula package offers a high-level interface for the R interface to Keras. It’s main interface is the kms function, a regression-style interface to keras_model_sequential that uses formulas and sparse matrices.\nThe kerasformula package is available on CRAN, and can be installed with:\n\n\n# install the kerasformula package\ninstall.packages(\"kerasformula\")    \n# or devtools::install_github(\"rdrr1990/kerasformula\")\n\nlibrary(kerasformula)\n\n# install the core keras library (if you haven't already done so)\n# see ?install_keras() for options e.g. install_keras(tensorflow = \"gpu\")\ninstall_keras()\n\nThe kms() function\nMany classic machine learning tutorials assume that data come in a relatively homogenous form (e.g., pixels for digit recognition or word counts or ranks) which can make coding somewhat cumbersome when data is contained in a heterogenous data frame. kms() takes advantage of the flexibility of R formulas to smooth this process.\nkms builds dense neural nets and, after fitting them, returns a single object with predictions, measures of fit, and details about the function call. kms accepts a number of parameters including the loss and activation functions found in keras. kms also accepts compiled keras_model_sequential objects allowing for even further customization. This little demo shows how kms can aid is model building and hyperparameter selection (e.g., batch size) starting with raw data gathered using library(rtweet).\n\n\n\nLet’s look at #rstats tweets (excluding retweets) for a six-day period ending January 24, 2018 at 10:40. This happens to give us a nice reasonable number of observations to work with in terms of runtime (and the purpose of this document is to show syntax, not build particularly predictive models).\n\n\nrstats <- search_tweets(\"#rstats\", n = 10000, include_rts = FALSE)\ndim(rstats)\n\n\n  [1] 2840   42\nSuppose our goal is to predict how popular tweets will be based on how often the tweet was retweeted and favorited (which correlate strongly).\n\n\ncor(rstats$favorite_count, rstats$retweet_count, method=\"spearman\")\n\n\n    [1] 0.7051952\nSince few tweeets go viral, the data are quite skewed towards zero.\n\n\n\n\nGetting the most out of formulas\nLet’s suppose we are interested in putting tweets into categories based on popularity but we’re not sure how finely-grained we want to make distinctions. Some of the data, like rstats$mentions_screen_name comes in a list of varying lengths, so let’s write a helper function to count non-NA entries.\n\n\nn <- function(x) {\n  unlist(lapply(x, function(y){length(y) - is.na(y[1])}))\n}\n\nLet’s start with a dense neural net, the default of kms. We can use base R functions to help clean the data–in this case, cut to discretize the outcome, grepl to look for key words, and weekdays and format to capture different aspects of the time the tweet was posted.\n\n\nbreaks <- c(-1, 0, 1, 10, 100, 1000, 10000)\npopularity <- kms(cut(retweet_count + favorite_count, breaks) ~ screen_name + \n                  source + n(hashtags) + n(mentions_screen_name) + \n                  n(urls_url) + nchar(text) +\n                  grepl('photo', media_type) +\n                  weekdays(created_at) + \n                  format(created_at, '%H'), rstats)\nplot(popularity$history) \n  + ggtitle(paste(\"#rstat popularity:\", \n            paste0(round(100*popularity$evaluations$acc, 1), \"%\"),\n            \"out-of-sample accuracy\")) \n  + theme_minimal()\n\npopularity$confusion\n\n\n\npopularity$confusion\n\n                    (-1,0] (0,1] (1,10] (10,100] (100,1e+03] (1e+03,1e+04]\n      (-1,0]            37    12     28        2           0             0\n      (0,1]             14    19     72        1           0             0\n      (1,10]             6    11    187       30           0             0\n      (10,100]           1     3     54       68           0             0\n      (100,1e+03]        0     0      4       10           0             0\n      (1e+03,1e+04]      0     0      0        1           0             0\nThe model only classifies about 55% of the out-of-sample data correctly and that predictive accuracy doesn’t improve after the first ten epochs. The confusion matrix suggests that model does best with tweets that are retweeted a handful of times but overpredicts the 1-10 level. The history plot also suggests that out-of-sample accuracy is not very stable. We can easily change the breakpoints and number of epochs.\n\n\nbreaks <- c(-1, 0, 1, 25, 50, 75, 100, 500, 1000, 10000)\npopularity <- kms(cut(retweet_count + favorite_count, breaks) ~  \n                  n(hashtags) + n(mentions_screen_name) + n(urls_url) +\n                  nchar(text) +\n                  screen_name + source +\n                  grepl('photo', media_type) +\n                  weekdays(created_at) + \n                  format(created_at, '%H'), rstats, Nepochs = 10)\n\nplot(popularity$history) \n  + ggtitle(paste(\"#rstat popularity (new breakpoints):\",\n            paste0(round(100*popularity$evaluations$acc, 1), \"%\"),\n            \"out-of-sample accuracy\")) \n  + theme_minimal()\n\n\nThat helped some (about 5% additional predictive accuracy). Suppose we want to add a little more data. Let’s first store the input formula.\n\n\npop_input <- \"cut(retweet_count + favorite_count, breaks) ~  \n                          n(hashtags) + n(mentions_screen_name) + n(urls_url) +\n                          nchar(text) +\n                          screen_name + source +\n                          grepl('photo', media_type) +\n                          weekdays(created_at) + \n                          format(created_at, '%H')\"\n\nHere we use paste0 to add to the formula by looping over user IDs adding something like:\n\ngrepl(\"12233344455556\", mentions_user_id)\n\n\nmentions <- unlist(rstats$mentions_user_id)\nmentions <- unique(mentions[which(table(mentions) > 5)]) # remove infrequent\nmentions <- mentions[!is.na(mentions)] # drop NA\n\nfor(i in mentions)\n  pop_input <- paste0(pop_input, \" + \", \"grepl(\", i, \", mentions_user_id)\")\n\npopularity <- kms(pop_input, rstats)\n\n\n\n\n\nThat helped a touch but the predictive accuracy is still fairly unstable across epochs…\nCustomizing layers with kms()\nWe could add more data, perhaps add individual words from the text or some other summary stat (mean(text %in% LETTERS) to see if all caps explains popularity). But let’s alter the neural net.\nThe input.formula is used to create a sparse model matrix. For example, rstats$source (Twitter or Twitter-client application type) and rstats$screen_name are character vectors that will be dummied out. How many columns does it have?\n\n\npopularity$P\n\n\n    [1] 1277\nSay we wanted to reshape the layers to transition more gradually from the input shape to the output.\n\n\npopularity <- kms(pop_input, rstats,\n                  layers = list(\n                    units = c(1024, 512, 256, 128, NA),\n                    activation = c(\"relu\", \"relu\", \"relu\", \"relu\", \"softmax\"), \n                    dropout = c(0.5, 0.45, 0.4, 0.35, NA)\n                  ))\n\n\n\n\n\nkms builds a keras_sequential_model(), which is a stack of linear layers. The input shape is determined by the dimensionality of the model matrix (popularity$P) but after that users are free to determine the number of layers and so on. The kms argument layers expects a list, the first entry of which is a vector units with which to call keras::layer_dense(). The first element the number of units in the first layer, the second element for the second layer, and so on (NA as the final element connotes to auto-detect the final number of units based on the observed number of outcomes). activation is also passed to layer_dense() and may take values such as softmax, relu, elu, and linear. (kms also has a separate parameter to control the optimizer; by default kms(... optimizer = 'rms_prop').) The dropout that follows each dense layer rate prevents overfitting (but of course isn’t applicable to the final layer).\nChoosing a Batch Size\nBy default, kms uses batches of 32. Suppose we were happy with our model but didn’t have any particular intuition about what the size should be.\n\n\nNbatch <- c(16, 32, 64)\nNruns <- 4\naccuracy <- matrix(nrow = Nruns, ncol = length(Nbatch))\ncolnames(accuracy) <- paste0(\"Nbatch_\", Nbatch)\n\nest <- list()\nfor(i in 1:Nruns){\n  for(j in 1:length(Nbatch)){\n   est[[i]] <- kms(pop_input, rstats, Nepochs = 2, batch_size = Nbatch[j])\n   accuracy[i,j] <- est[[i]][[\"evaluations\"]][[\"acc\"]]\n  }\n}\n  \ncolMeans(accuracy)\n\n\n    Nbatch_16 Nbatch_32 Nbatch_64 \n    0.5088407 0.3820850 0.5556952 \nFor the sake of curtailing runtime, the number of epochs was set arbitrarily short but, from those results, 64 is the best batch size.\nMaking predictions for new data\nThus far, we have been using the default settings for kms which first splits data into 80% training and 20% testing. Of the 80% training, a certain portion is set aside for validation and that’s what produces the epoch-by-epoch graphs of loss and accuracy. The 20% is only used at the end to assess predictive accuracy. But suppose you wanted to make predictions on a new data set…\n\n\npopularity <- kms(pop_input, rstats[1:1000,])\npredictions <- predict(popularity, rstats[1001:2000,])\npredictions$accuracy\n\n\n    [1] 0.579\n\n\n# predictions$confusion\n\nBecause the formula creates a dummy variable for each screen name and mention, any given set of tweets is all but guaranteed to have different columns. predict.kms_fit is an S3 method that takes the new data and constructs a (sparse) model matrix that preserves the original structure of the training matrix. predict then returns the predictions along with a confusion matrix and accuracy score.\nIf your newdata has the same observed levels of y and columns of x_train (the model matrix), you can also use keras::predict_classes on object$model.\nUsing a compiled Keras model\nThis section shows how to input a model compiled in the fashion typical to library(keras), which is useful for more advanced models. Here is an example for lstm analogous to the imbd with Keras example.\n\n\nk <- keras_model_sequential()\nk %>%\n  layer_embedding(input_dim = popularity$P, output_dim = popularity$P) %>% \n  layer_lstm(units = 512, dropout = 0.4, recurrent_dropout = 0.2) %>% \n  layer_dense(units = 256, activation = \"relu\") %>%\n  layer_dropout(0.3) %>%\n  layer_dense(units = 8, # number of levels observed on y (outcome)  \n              activation = 'sigmoid')\n\nk %>% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = 'rmsprop',\n  metrics = c('accuracy')\n)\n\npopularity_lstm <- kms(pop_input, rstats, k)\n\nQuestions? Comments?\nDrop me a line via the project’s Github repo. Special thanks to @dfalbel and @jjallaire for helpful suggestions!!\n\n\n",
    "preview": "posts/2018-01-24-analyzing-rtweet-data-with-kerasformula/images/densities-1.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 672,
    "preview_height": 480
  },
  {
    "path": "posts/2018-01-11-keras-customer-churn/",
    "title": "Deep Learning With Keras To Predict Customer Churn",
    "description": "Using Keras to predict customer churn based on the IBM Watson Telco Customer Churn dataset. We also demonstrate using the lime package to help explain which features drive individual model predictions. In addition, we use three new packages to assist with Machine Learning: recipes for preprocessing, rsample for sampling data and yardstick for model metrics.",
    "author": [
      {
        "name": "Matt Dancho",
        "url": "https://github.com/mdancho84"
      }
    ],
    "date": "2018-01-11",
    "categories": [
      "TensorFlow/Keras",
      "Tabular Data",
      "Explainability"
    ],
    "contents": "\n\n\nstrong {\n  font-weight: normal;\n}\n\nIntroduction\nCustomer churn is a problem that all companies need to monitor, especially those that depend on subscription-based revenue streams. The simple fact is that most organizations have data that can be used to target these individuals and to understand the key drivers of churn, and we now have Keras for Deep Learning available in R (Yes, in R!!), which predicted customer churn with 82% accuracy.\nWe’re super excited for this article because we are using the new keras package to produce an Artificial Neural Network (ANN) model on the IBM Watson Telco Customer Churn Data Set! As with most business problems, it’s equally important to explain what features drive the model, which is why we’ll use the lime package for explainability. We cross-checked the LIME results with a Correlation Analysis using the corrr package.\nIn addition, we use three new packages to assist with Machine Learning (ML): recipes for preprocessing, rsample for sampling data and yardstick for model metrics. These are relatively new additions to CRAN developed by Max Kuhn at RStudio (creator of the caret package). It seems that R is quickly developing ML tools that rival Python. Good news if you’re interested in applying Deep Learning in R! We are so let’s get going!!\nCustomer Churn: Hurts Sales, Hurts Company\nCustomer churn refers to the situation when a customer ends their relationship with a company, and it’s a costly problem. Customers are the fuel that powers a business. Loss of customers impacts sales. Further, it’s much more difficult and costly to gain new customers than it is to retain existing customers. As a result, organizations need to focus on reducing customer churn.\nThe good news is that machine learning can help. For many businesses that offer subscription based services, it’s critical to both predict customer churn and explain what features relate to customer churn. Older techniques such as logistic regression can be less accurate than newer techniques such as deep learning, which is why we are going to show you how to model an ANN in R with the keras package.\nChurn Modeling With Artificial Neural Networks (Keras)\nArtificial Neural Networks (ANN) are now a staple within the sub-field of Machine Learning called Deep Learning. Deep learning algorithms can be vastly superior to traditional regression and classification methods (e.g. linear and logistic regression) because of the ability to model interactions between features that would otherwise go undetected. The challenge becomes explainability, which is often needed to support the business case. The good news is we get the best of both worlds with keras and lime.\nIBM Watson Dataset (Where We Got The Data)\nThe dataset used for this tutorial is IBM Watson Telco Dataset. According to IBM, the business challenge is…\n\nA telecommunications company [Telco] is concerned about the number of customers leaving their landline business for cable competitors. They need to understand who is leaving. Imagine that you’re an analyst at this company and you have to find out who is leaving and why.\n\nThe dataset includes information about:\nCustomers who left within the last month: The column is called Churn\nServices that each customer has signed up for: phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\nCustomer account information: how long they’ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges\nDemographic info about customers: gender, age range, and if they have partners and dependents\nDeep Learning With Keras (What We Did With The Data)\nIn this example we show you how to use keras to develop a sophisticated and highly accurate deep learning model in R. We walk you through the preprocessing steps, investing time into how to format the data for Keras. We inspect the various classification metrics, and show that an un-tuned ANN model can easily get 82% accuracy on the unseen data. Here’s the deep learning training history visualization.\n\nWe have some fun with preprocessing the data (yes, preprocessing can actually be fun and easy!). We use the new recipes package to simplify the preprocessing workflow.\nWe end by showing you how to explain the ANN with the lime package. Neural networks used to be frowned upon because of the “black box” nature meaning these sophisticated models (ANNs are highly accurate) are difficult to explain using traditional methods. Not any more with LIME! Here’s the feature importance visualization.\n\nWe also cross-checked the LIME results with a Correlation Analysis using the corrr package. Here’s the correlation visualization.\n\n\n\nWe even built a Shiny Application with a Customer Scorecard to monitor customer churn risk and to make recommendations on how to improve customer health! Feel free to take it for a spin.\n\n\n\nCredits\nWe saw that just last week the same Telco customer churn dataset was used in the article, Predict Customer Churn – Logistic Regression, Decision Tree and Random Forest. We thought the article was excellent.\nThis article takes a different approach with Keras, LIME, Correlation Analysis, and a few other cutting edge packages. We encourage the readers to check out both articles because, although the problem is the same, both solutions are beneficial to those learning data science and advanced modeling.\nPrerequisites\nWe use the following libraries in this tutorial:\nkeras: Library that ports Keras from Python enabling deep learning in R. Visit the documentation for more information.\nlime: Used to explain the predictions of black box classifiers. Deep Learning falls into this category.\ntidyquant: Loads the tidyverse (dplyr, ggplot2, etc) and has nice visualization functions with theme_tq(). Visit the tidyquant documentation and the tidyverse documentation for more information on the individual packages.\nrsample: New package for generating resamples. Visit the documentation for more information.\nrecipes: New package for preprocessing machine learning data sets. Visit the documentation for more information.\nyardstick: Tidy methods for measuring model performance. Visit the GitHub Page for more information.\ncorrr: Tidy methods for correlation. Visit the GitHub Page for more information.\nInstall the following packages with install.packages().\n\n\npkgs <- c(\"keras\", \"lime\", \"tidyquant\", \"rsample\", \"recipes\", \"yardstick\", \"corrr\")\ninstall.packages(pkgs)\n\nLoad Libraries\nLoad the libraries.\n\n\n# Load libraries\nlibrary(keras)\nlibrary(lime)\nlibrary(tidyquant)\nlibrary(rsample)\nlibrary(recipes)\nlibrary(yardstick)\nlibrary(corrr)\n\nIf you have not previously run Keras in R, you will need to install Keras using the install_keras() function.\n\n\n# Install Keras if you have not installed before\ninstall_keras()\n\nImport Data\nDownload the IBM Watson Telco Data Set here. Next, use read_csv() to import the data into a nice tidy data frame. We use the glimpse() function to quickly inspect the data. We have the target “Churn” and all other variables are potential predictors. The raw data set needs to be cleaned and preprocessed for ML.\n\n\nchurn_data_raw <- read_csv(\"WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n\nglimpse(churn_data_raw)\n\n\nObservations: 7,043\nVariables: 21\n$ customerID       <chr> \"7590-VHVEG\", \"5575-GNVDE\", \"3668-QPYBK\", \"77...\n$ gender           <chr> \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"...\n$ SeniorCitizen    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n$ Partner          <chr> \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"N...\n$ Dependents       <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"N...\n$ tenure           <int> 1, 34, 2, 45, 2, 8, 22, 10, 28, 62, 13, 16, 5...\n$ PhoneService     <chr> \"No\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"Yes\"...\n$ MultipleLines    <chr> \"No phone service\", \"No\", \"No\", \"No phone ser...\n$ InternetService  <chr> \"DSL\", \"DSL\", \"DSL\", \"DSL\", \"Fiber optic\", \"F...\n$ OnlineSecurity   <chr> \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"No\", \"No\", ...\n$ OnlineBackup     <chr> \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"Yes\", ...\n$ DeviceProtection <chr> \"No\", \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"No\", ...\n$ TechSupport      <chr> \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"N...\n$ StreamingTV      <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"...\n$ StreamingMovies  <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"N...\n$ Contract         <chr> \"Month-to-month\", \"One year\", \"Month-to-month...\n$ PaperlessBilling <chr> \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"Yes\"...\n$ PaymentMethod    <chr> \"Electronic check\", \"Mailed check\", \"Mailed c...\n$ MonthlyCharges   <dbl> 29.85, 56.95, 53.85, 42.30, 70.70, 99.65, 89....\n$ TotalCharges     <dbl> 29.85, 1889.50, 108.15, 1840.75, 151.65, 820....\n$ Churn            <chr> \"No\", \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"No\", ...\nPreprocess Data\nWe’ll go through a few steps to preprocess the data for ML. First, we “prune” the data, which is nothing more than removing unnecessary columns and rows. Then we split into training and testing sets. After that we explore the training set to uncover transformations that will be needed for deep learning. We save the best for last. We end by preprocessing the data with the new recipes package.\nPrune The Data\nThe data has a few columns and rows we’d like to remove:\nThe “customerID” column is a unique identifier for each observation that isn’t needed for modeling. We can de-select this column.\nThe data has 11 NA values all in the “TotalCharges” column. Because it’s such a small percentage of the total population (99.8% complete cases), we can drop these observations with the drop_na() function from tidyr. Note that these may be customers that have not yet been charged, and therefore an alternative is to replace with zero or -99 to segregate this population from the rest.\nMy preference is to have the target in the first column so we’ll include a final select() ooperation to do so.\nWe’ll perform the cleaning operation with one tidyverse pipe (%>%) chain.\n\n\n# Remove unnecessary data\nchurn_data_tbl <- churn_data_raw %>%\n  select(-customerID) %>%\n  drop_na() %>%\n  select(Churn, everything())\n    \nglimpse(churn_data_tbl)\n\n\nObservations: 7,032\nVariables: 20\n$ Churn            <chr> \"No\", \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"No\", ...\n$ gender           <chr> \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"...\n$ SeniorCitizen    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n$ Partner          <chr> \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"N...\n$ Dependents       <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"N...\n$ tenure           <int> 1, 34, 2, 45, 2, 8, 22, 10, 28, 62, 13, 16, 5...\n$ PhoneService     <chr> \"No\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"Yes\"...\n$ MultipleLines    <chr> \"No phone service\", \"No\", \"No\", \"No phone ser...\n$ InternetService  <chr> \"DSL\", \"DSL\", \"DSL\", \"DSL\", \"Fiber optic\", \"F...\n$ OnlineSecurity   <chr> \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"No\", \"No\", ...\n$ OnlineBackup     <chr> \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"Yes\", ...\n$ DeviceProtection <chr> \"No\", \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"No\", ...\n$ TechSupport      <chr> \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"N...\n$ StreamingTV      <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"...\n$ StreamingMovies  <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"N...\n$ Contract         <chr> \"Month-to-month\", \"One year\", \"Month-to-month...\n$ PaperlessBilling <chr> \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"Yes\"...\n$ PaymentMethod    <chr> \"Electronic check\", \"Mailed check\", \"Mailed c...\n$ MonthlyCharges   <dbl> 29.85, 56.95, 53.85, 42.30, 70.70, 99.65, 89....\n$ TotalCharges     <dbl> 29.85, 1889.50, 108.15, 1840.75, 151.65, 820..\nSplit Into Train/Test Sets\nWe have a new package, rsample, which is very useful for sampling methods. It has the initial_split() function for splitting data sets into training and testing sets. The return is a special rsplit object.\n\n\n# Split test/training sets\nset.seed(100)\ntrain_test_split <- initial_split(churn_data_tbl, prop = 0.8)\ntrain_test_split\n\n\n<5626/1406/7032>\nWe can retrieve our training and testing sets using training() and testing() functions.\n\n\n# Retrieve train and test sets\ntrain_tbl <- training(train_test_split)\ntest_tbl  <- testing(train_test_split) \n\nExploration: What Transformation Steps Are Needed For ML?\nThis phase of the analysis is often called exploratory analysis, but basically we are trying to answer the question, “What steps are needed to prepare for ML?” The key concept is knowing what transformations are needed to run the algorithm most effectively. Artificial Neural Networks are best when the data is one-hot encoded, scaled and centered. In addition, other transformations may be beneficial as well to make relationships easier for the algorithm to identify. A full exploratory analysis is not practical in this article. With that said we’ll cover a few tips on transformations that can help as they relate to this dataset. In the next section, we will implement the preprocessing techniques.\nDiscretize The “tenure” Feature\nNumeric features like age, years worked, length of time in a position can generalize a group (or cohort). We see this in marketing a lot (think “millennials”, which identifies a group born in a certain timeframe). The “tenure” feature falls into this category of numeric features that can be discretized into groups.\n\n\n\n\nWe can split into six cohorts that divide up the user base by tenure in roughly one year (12 month) increments. This should help the ML algorithm detect if a group is more/less susceptible to customer churn.\n\n\n\n\nTransform The “TotalCharges” Feature\nWhat we don’t like to see is when a lot of observations are bunched within a small part of the range.\n\n\n\n\nWe can use a log transformation to even out the data into more of a normal distribution. It’s not perfect, but it’s quick and easy to get our data spread out a bit more.\n\n\n\n\nPro Tip: A quick test is to see if the log transformation increases the magnitude of the correlation between “TotalCharges” and “Churn”. We’ll use a few dplyr operations along with the corrr package to perform a quick correlation.\ncorrelate(): Performs tidy correlations on numeric data\nfocus(): Similar to select(). Takes columns and focuses on only the rows/columns of importance.\nfashion(): Makes the formatting aesthetically easier to read.\n\n\n# Determine if log transformation improves correlation \n# between TotalCharges and Churn\ntrain_tbl %>%\n  select(Churn, TotalCharges) %>%\n  mutate(\n      Churn = Churn %>% as.factor() %>% as.numeric(),\n      LogTotalCharges = log(TotalCharges)\n      ) %>%\n  correlate() %>%\n  focus(Churn) %>%\n  fashion()\n\n\n          rowname Churn\n1    TotalCharges  -.20\n2 LogTotalCharges  -.25\nThe correlation between “Churn” and “LogTotalCharges” is greatest in magnitude indicating the log transformation should improve the accuracy of the ANN model we build. Therefore, we should perform the log transformation.\nOne-Hot Encoding\nOne-hot encoding is the process of converting categorical data to sparse data, which has columns of only zeros and ones (this is also called creating “dummy variables” or a “design matrix”). All non-numeric data will need to be converted to dummy variables. This is simple for binary Yes/No data because we can simply convert to 1’s and 0’s. It becomes slightly more complicated with multiple categories, which requires creating new columns of 1’s and 0`s for each category (actually one less). We have four features that are multi-category: Contract, Internet Service, Multiple Lines, and Payment Method.\n\n\n\n\nFeature Scaling\nANN’s typically perform faster and often times with higher accuracy when the features are scaled and/or normalized (aka centered and scaled, also known as standardizing). Because ANNs use gradient descent, weights tend to update faster. According to Sebastian Raschka, an expert in the field of Deep Learning, several examples when feature scaling is important are:\n\nk-nearest neighbors with an Euclidean distance measure if want all features to contribute equally\nk-means (see k-nearest neighbors)\nlogistic regression, SVMs, perceptrons, neural networks etc. if you are using gradient descent/ascent-based optimization, otherwise some weights will update much faster than others\nlinear discriminant analysis, principal component analysis, kernel principal component analysis since you want to find directions of maximizing the variance (under the constraints that those directions/eigenvectors/principal components are orthogonal); you want to have features on the same scale since you’d emphasize variables on “larger measurement scales” more. There are many more cases than I can possibly list here … I always recommend you to think about the algorithm and what it’s doing, and then it typically becomes obvious whether we want to scale your features or not.\n\nThe interested reader can read Sebastian Raschka’s article for a full discussion on the scaling/normalization topic. Pro Tip: When in doubt, standardize the data.\nPreprocessing With Recipes\nLet’s implement the preprocessing steps/transformations uncovered during our exploration. Max Kuhn (creator of caret) has been putting some work into Rlang ML tools lately, and the payoff is beginning to take shape. A new package, recipes, makes creating ML data preprocessing workflows a breeze! It takes a little getting used to, but I’ve found that it really helps manage the preprocessing steps. We’ll go over the nitty gritty as it applies to this problem.\nStep 1: Create A Recipe\nA “recipe” is nothing more than a series of steps you would like to perform on the training, testing and/or validation sets. Think of preprocessing data like baking a cake (I’m not a baker but stay with me). The recipe is our steps to make the cake. It doesn’t do anything other than create the playbook for baking.\nWe use the recipe() function to implement our preprocessing steps. The function takes a familiar object argument, which is a modeling function such as object = Churn ~ . meaning “Churn” is the outcome (aka response, predictor, target) and all other features are predictors. The function also takes the data argument, which gives the “recipe steps” perspective on how to apply during baking (next).\nA recipe is not very useful until we add “steps”, which are used to transform the data during baking. The package contains a number of useful “step functions” that can be applied. The entire list of Step Functions can be viewed here. For our model, we use:\nstep_discretize() with the option = list(cuts = 6) to cut the continuous variable for “tenure” (number of years as a customer) to group customers into cohorts.\nstep_log() to log transform “TotalCharges”.\nstep_dummy() to one-hot encode the categorical data. Note that this adds columns of one/zero for categorical data with three or more categories.\nstep_center() to mean-center the data.\nstep_scale() to scale the data.\nThe last step is to prepare the recipe with the prep() function. This step is used to “estimate the required parameters from a training set that can later be applied to other data sets”. This is important for centering and scaling and other functions that use parameters defined from the training set.\nHere’s how simple it is to implement the preprocessing steps that we went over!\n\n\n# Create recipe\nrec_obj <- recipe(Churn ~ ., data = train_tbl) %>%\n  step_discretize(tenure, options = list(cuts = 6)) %>%\n  step_log(TotalCharges) %>%\n  step_dummy(all_nominal(), -all_outcomes()) %>%\n  step_center(all_predictors(), -all_outcomes()) %>%\n  step_scale(all_predictors(), -all_outcomes()) %>%\n  prep(data = train_tbl)\n\nWe can print the recipe object if we ever forget what steps were used to prepare the data. Pro Tip: We can save the recipe object as an RDS file using saveRDS(), and then use it to bake() (discussed next) future raw data into ML-ready data in production!\n\n\n# Print the recipe object\nrec_obj\n\n\nData Recipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         19\n\nTraining data contained 5626 data points and no missing data.\n\nSteps:\n\nDummy variables from tenure [trained]\nLog transformation on TotalCharges [trained]\nDummy variables from ~gender, ~Partner, ... [trained]\nCentering for SeniorCitizen, ... [trained]\nScaling for SeniorCitizen, ... [trained]\nStep 2: Baking With Your Recipe\nNow for the fun part! We can apply the “recipe” to any data set with the bake() function, and it processes the data following our recipe steps. We’ll apply to our training and testing data to convert from raw data to a machine learning dataset. Check our training set out with glimpse(). Now that’s an ML-ready dataset prepared for ANN modeling!!\n\n\n# Predictors\nx_train_tbl <- bake(rec_obj, newdata = train_tbl) %>% select(-Churn)\nx_test_tbl  <- bake(rec_obj, newdata = test_tbl) %>% select(-Churn)\n\nglimpse(x_train_tbl)\n\n\nObservations: 5,626\nVariables: 35\n$ SeniorCitizen                         <dbl> -0.4351959, -0.4351...\n$ MonthlyCharges                        <dbl> -1.1575972, -0.2601...\n$ TotalCharges                          <dbl> -2.275819130, 0.389...\n$ gender_Male                           <dbl> -1.0016900, 0.99813...\n$ Partner_Yes                           <dbl> 1.0262054, -0.97429...\n$ Dependents_Yes                        <dbl> -0.6507747, -0.6507...\n$ tenure_bin1                           <dbl> 2.1677790, -0.46121...\n$ tenure_bin2                           <dbl> -0.4389453, -0.4389...\n$ tenure_bin3                           <dbl> -0.4481273, -0.4481...\n$ tenure_bin4                           <dbl> -0.4509837, 2.21698...\n$ tenure_bin5                           <dbl> -0.4498419, -0.4498...\n$ tenure_bin6                           <dbl> -0.4337508, -0.4337...\n$ PhoneService_Yes                      <dbl> -3.0407367, 0.32880...\n$ MultipleLines_No.phone.service        <dbl> 3.0407367, -0.32880...\n$ MultipleLines_Yes                     <dbl> -0.8571364, -0.8571...\n$ InternetService_Fiber.optic           <dbl> -0.8884255, -0.8884...\n$ InternetService_No                    <dbl> -0.5272627, -0.5272...\n$ OnlineSecurity_No.internet.service    <dbl> -0.5272627, -0.5272...\n$ OnlineSecurity_Yes                    <dbl> -0.6369654, 1.56966...\n$ OnlineBackup_No.internet.service      <dbl> -0.5272627, -0.5272...\n$ OnlineBackup_Yes                      <dbl> 1.3771987, -0.72598...\n$ DeviceProtection_No.internet.service  <dbl> -0.5272627, -0.5272...\n$ DeviceProtection_Yes                  <dbl> -0.7259826, 1.37719...\n$ TechSupport_No.internet.service       <dbl> -0.5272627, -0.5272...\n$ TechSupport_Yes                       <dbl> -0.6358628, -0.6358...\n$ StreamingTV_No.internet.service       <dbl> -0.5272627, -0.5272...\n$ StreamingTV_Yes                       <dbl> -0.7917326, -0.7917...\n$ StreamingMovies_No.internet.service   <dbl> -0.5272627, -0.5272...\n$ StreamingMovies_Yes                   <dbl> -0.797388, -0.79738...\n$ Contract_One.year                     <dbl> -0.5156834, 1.93882...\n$ Contract_Two.year                     <dbl> -0.5618358, -0.5618...\n$ PaperlessBilling_Yes                  <dbl> 0.8330334, -1.20021...\n$ PaymentMethod_Credit.card..automatic. <dbl> -0.5231315, -0.5231...\n$ PaymentMethod_Electronic.check        <dbl> 1.4154085, -0.70638...\n$ PaymentMethod_Mailed.check            <dbl> -0.5517013, 1.81225...\nStep 3: Don’t Forget The Target\nOne last step, we need to store the actual values (truth) as y_train_vec and y_test_vec, which are needed for modeling our ANN. We convert to a series of numeric ones and zeros which can be accepted by the Keras ANN modeling functions. We add “vec” to the name so we can easily remember the class of the object (it’s easy to get confused when working with tibbles, vectors, and matrix data types).\n\n\n# Response variables for training and testing sets\ny_train_vec <- ifelse(pull(train_tbl, Churn) == \"Yes\", 1, 0)\ny_test_vec  <- ifelse(pull(test_tbl, Churn) == \"Yes\", 1, 0)\n\nModel Customer Churn With Keras (Deep Learning)\nThis is super exciting!! Finally, Deep Learning with Keras in R! The team at RStudio has done fantastic work recently to create the keras package, which implements Keras in R. Very cool!\nBackground On Artifical Neural Networks\nFor those unfamiliar with Neural Networks (and those that need a refresher), read this article. It’s very comprehensive, and you’ll leave with a general understanding of the types of deep learning and how they work.\n\n\nSource: Xenon Stack\n\nDeep Learning has been available in R for some time, but the primary packages used in the wild have not (this includes Keras, Tensor Flow, Theano, etc, which are all Python libraries). It’s worth mentioning that a number of other Deep Learning packages exist in R including h2o, mxnet, and others. The interested reader can check out this blog post for a comparison of deep learning packages in R.\nBuilding A Deep Learning Model\nWe’re going to build a special class of ANN called a Multi-Layer Perceptron (MLP). MLPs are one of the simplest forms of deep learning, but they are both highly accurate and serve as a jumping-off point for more complex algorithms. MLPs are quite versatile as they can be used for regression, binary and multi classification (and are typically quite good at classification problems).\nWe’ll build a three layer MLP with Keras. Let’s walk-through the steps before we implement in R.\nInitialize a sequential model: The first step is to initialize a sequential model with keras_model_sequential(), which is the beginning of our Keras model. The sequential model is composed of a linear stack of layers.\nApply layers to the sequential model: Layers consist of the input layer, hidden layers and an output layer. The input layer is the data and provided it’s formatted correctly there’s nothing more to discuss. The hidden layers and output layers are what controls the ANN inner workings.\nHidden Layers: Hidden layers form the neural network nodes that enable non-linear activation using weights. The hidden layers are created using layer_dense(). We’ll add two hidden layers. We’ll apply units = 16, which is the number of nodes. We’ll select kernel_initializer = \"uniform\" and activation = \"relu\" for both layers. The first layer needs to have the input_shape = 35, which is the number of columns in the training set. Key Point: While we are arbitrarily selecting the number of hidden layers, units, kernel initializers and activation functions, these parameters can be optimized through a process called hyperparameter tuning that is discussed in Next Steps.\nDropout Layers: Dropout layers are used to control overfitting. This eliminates weights below a cutoff threshold to prevent low weights from overfitting the layers. We use the layer_dropout() function add two drop out layers with rate = 0.10 to remove weights below 10%.\nOutput Layer: The output layer specifies the shape of the output and the method of assimilating the learned information. The output layer is applied using the layer_dense(). For binary values, the shape should be units = 1. For multi-classification, the units should correspond to the number of classes. We set the kernel_initializer = \"uniform\" and the activation = \"sigmoid\" (common for binary classification).\n\nCompile the model: The last step is to compile the model with compile(). We’ll use optimizer = \"adam\", which is one of the most popular optimization algorithms. We select loss = \"binary_crossentropy\" since this is a binary classification problem. We’ll select metrics = c(\"accuracy\") to be evaluated during training and testing. Key Point: The optimizer is often included in the tuning process.\nLet’s codify the discussion above to build our Keras MLP-flavored ANN model.\n\n\n# Building our Artificial Neural Network\nmodel_keras <- keras_model_sequential()\n\nmodel_keras %>% \n  \n  # First hidden layer\n  layer_dense(\n    units              = 16, \n    kernel_initializer = \"uniform\", \n    activation         = \"relu\", \n    input_shape        = ncol(x_train_tbl)) %>% \n  \n  # Dropout to prevent overfitting\n  layer_dropout(rate = 0.1) %>%\n  \n  # Second hidden layer\n  layer_dense(\n    units              = 16, \n    kernel_initializer = \"uniform\", \n    activation         = \"relu\") %>% \n  \n  # Dropout to prevent overfitting\n  layer_dropout(rate = 0.1) %>%\n  \n  # Output layer\n  layer_dense(\n    units              = 1, \n    kernel_initializer = \"uniform\", \n    activation         = \"sigmoid\") %>% \n  \n  # Compile ANN\n  compile(\n    optimizer = 'adam',\n    loss      = 'binary_crossentropy',\n    metrics   = c('accuracy')\n  )\n\nkeras_model\n\n\nModel\n___________________________________________________________________________________________________\nLayer (type)                                Output Shape                            Param #        \n===================================================================================================\ndense_1 (Dense)                             (None, 16)                              576            \n___________________________________________________________________________________________________\ndropout_1 (Dropout)                         (None, 16)                              0              \n___________________________________________________________________________________________________\ndense_2 (Dense)                             (None, 16)                              272            \n___________________________________________________________________________________________________\ndropout_2 (Dropout)                         (None, 16)                              0              \n___________________________________________________________________________________________________\ndense_3 (Dense)                             (None, 1)                               17             \n===================================================================================================\nTotal params: 865\nTrainable params: 865\nNon-trainable params: 0\n___________________________________________________________________________________________________\nWe use the fit() function to run the ANN on our training data. The object is our model, and x and y are our training data in matrix and numeric vector forms, respectively. The batch_size = 50 sets the number samples per gradient update within each epoch. We set epochs = 35 to control the number training cycles. Typically we want to keep the batch size high since this decreases the error within each training cycle (epoch). We also want epochs to be large, which is important in visualizing the training history (discussed below). We set validation_split = 0.30 to include 30% of the data for model validation, which prevents overfitting. The training process should complete in 15 seconds or so.\n\n\n# Fit the keras model to the training data\nhistory <- fit(\n  object           = model_keras, \n  x                = as.matrix(x_train_tbl), \n  y                = y_train_vec,\n  batch_size       = 50, \n  epochs           = 35,\n  validation_split = 0.30\n)\n\nWe can inspect the training history. We want to make sure there is minimal difference between the validation accuracy and the training accuracy.\n\n\n# Print a summary of the training history\nprint(history)\n\n\nTrained on 3,938 samples, validated on 1,688 samples (batch_size=50, epochs=35)\nFinal epoch (plot to see history):\nval_loss: 0.4215\n val_acc: 0.8057\n    loss: 0.399\n     acc: 0.8101\nWe can visualize the Keras training history using the plot() function. What we want to see is the validation accuracy and loss leveling off, which means the model has completed training. We see that there is some divergence between training loss/accuracy and validation loss/accuracy. This model indicates we can possibly stop training at an earlier epoch. Pro Tip: Only use enough epochs to get a high validation accuracy. Once validation accuracy curve begins to flatten or decrease, it’s time to stop training.\n\n\n# Plot the training/validation history of our Keras model\nplot(history) \n\n\nMaking Predictions\nWe’ve got a good model based on the validation accuracy. Now let’s make some predictions from our keras model on the test data set, which was unseen during modeling (we use this for the true performance assessment). We have two functions to generate predictions:\npredict_classes(): Generates class values as a matrix of ones and zeros. Since we are dealing with binary classification, we’ll convert the output to a vector.\npredict_proba(): Generates the class probabilities as a numeric matrix indicating the probability of being a class. Again, we convert to a numeric vector because there is only one column output.\n\n\n# Predicted Class\nyhat_keras_class_vec <- predict_classes(object = model_keras, x = as.matrix(x_test_tbl)) %>%\n    as.vector()\n\n# Predicted Class Probability\nyhat_keras_prob_vec  <- predict_proba(object = model_keras, x = as.matrix(x_test_tbl)) %>%\n    as.vector()\n\nInspect Performance With Yardstick\nThe yardstick package has a collection of handy functions for measuring performance of machine learning models. We’ll overview some metrics we can use to understand the performance of our model.\nFirst, let’s get the data formatted for yardstick. We create a data frame with the truth (actual values as factors), estimate (predicted values as factors), and the class probability (probability of yes as numeric). We use the fct_recode() function from the forcats package to assist with recoding as Yes/No values.\n\n\n# Format test data and predictions for yardstick metrics\nestimates_keras_tbl <- tibble(\n  truth      = as.factor(y_test_vec) %>% fct_recode(yes = \"1\", no = \"0\"),\n  estimate   = as.factor(yhat_keras_class_vec) %>% fct_recode(yes = \"1\", no = \"0\"),\n  class_prob = yhat_keras_prob_vec\n)\n\nestimates_keras_tbl\n\n\n# A tibble: 1,406 x 3\n    truth estimate  class_prob\n   <fctr>   <fctr>       <dbl>\n 1    yes       no 0.328355074\n 2    yes      yes 0.633630514\n 3     no       no 0.004589651\n 4     no       no 0.007402068\n 5     no       no 0.049968336\n 6     no       no 0.116824441\n 7     no      yes 0.775479317\n 8     no       no 0.492996633\n 9     no       no 0.011550998\n10     no       no 0.004276015\n# ... with 1,396 more rows\nNow that we have the data formatted, we can take advantage of the yardstick package. The only other thing we need to do is to set options(yardstick.event_first = FALSE). As pointed out by ad1729 in GitHub Issue 13, the default is to classify 0 as the positive class instead of 1.\n\n\noptions(yardstick.event_first = FALSE)\n\nConfusion Table\nWe can use the conf_mat() function to get the confusion table. We see that the model was by no means perfect, but it did a decent job of identifying customers likely to churn.\n\n\n# Confusion Table\nestimates_keras_tbl %>% conf_mat(truth, estimate)\n\n\n          Truth\nPrediction  no yes\n       no  950 161\n       yes  99 196\nAccuracy\nWe can use the metrics() function to get an accuracy measurement from the test set. We are getting roughly 82% accuracy.\n\n\n# Accuracy\nestimates_keras_tbl %>% metrics(truth, estimate)\n\n\n# A tibble: 1 x 1\n   accuracy\n      <dbl>\n1 0.8150782\nAUC\nWe can also get the ROC Area Under the Curve (AUC) measurement. AUC is often a good metric used to compare different classifiers and to compare to randomly guessing (AUC_random = 0.50). Our model has AUC = 0.85, which is much better than randomly guessing. Tuning and testing different classification algorithms may yield even better results.\n\n\n# AUC\nestimates_keras_tbl %>% roc_auc(truth, class_prob)\n\n\n[1] 0.8523951\nPrecision And Recall\nPrecision is when the model predicts “yes”, how often is it actually “yes”. Recall (also true positive rate or specificity) is when the actual value is “yes” how often is the model correct. We can get precision() and recall() measurements using yardstick.\n\n\n# Precision\ntibble(\n  precision = estimates_keras_tbl %>% precision(truth, estimate),\n  recall    = estimates_keras_tbl %>% recall(truth, estimate)\n)\n\n\n# A tibble: 1 x 2\n  precision    recall\n      <dbl>     <dbl>\n1 0.6644068 0.5490196\nPrecision and recall are very important to the business case: The organization is concerned with balancing the cost of targeting and retaining customers at risk of leaving with the cost of inadvertently targeting customers that are not planning to leave (and potentially decreasing revenue from this group). The threshold above which to predict Churn = “Yes” can be adjusted to optimize for the business problem. This becomes an Customer Lifetime Value optimization problem that is discussed further in Next Steps.\nF1 Score\nWe can also get the F1-score, which is a weighted average between the precision and recall. Machine learning classifier thresholds are often adjusted to maximize the F1-score. However, this is often not the optimal solution to the business problem.\n\n\n# F1-Statistic\nestimates_keras_tbl %>% f_meas(truth, estimate, beta = 1)\n\n\n[1] 0.601227\nExplain The Model With LIME\nLIME stands for Local Interpretable Model-agnostic Explanations, and is a method for explaining black-box machine learning model classifiers. For those new to LIME, this YouTube video does a really nice job explaining how LIME helps to identify feature importance with black box machine learning models (e.g. deep learning, stacked ensembles, random forest).\n\n\n\n\nSetup\nThe lime package implements LIME in R. One thing to note is that it’s not setup out-of-the-box to work with keras. The good news is with a few functions we can get everything working properly. We’ll need to make two custom functions:\nmodel_type: Used to tell lime what type of model we are dealing with. It could be classification, regression, survival, etc.\npredict_model: Used to allow lime to perform predictions that its algorithm can interpret.\nThe first thing we need to do is identify the class of our model object. We do this with the class() function.\n\n\nclass(model_keras)\n\n\n[1] \"keras.models.Sequential\"        \n[2] \"keras.engine.training.Model\"    \n[3] \"keras.engine.topology.Container\"\n[4] \"keras.engine.topology.Layer\"    \n[5] \"python.builtin.object\"\nNext we create our model_type() function. It’s only input is x the keras model. The function simply returns “classification”, which tells LIME we are classifying.\n\n\n# Setup lime::model_type() function for keras\nmodel_type.keras.models.Sequential <- function(x, ...) {\n  \"classification\"\n}\n\nNow we can create our predict_model() function, which wraps keras::predict_proba(). The trick here is to realize that it’s inputs must be x a model, newdata a dataframe object (this is important), and type which is not used but can be use to switch the output type. The output is also a little tricky because it must be in the format of probabilities by classification (this is important; shown next).\n\n\n# Setup lime::predict_model() function for keras\npredict_model.keras.models.Sequential <- function(x, newdata, type, ...) {\n  pred <- predict_proba(object = x, x = as.matrix(newdata))\n  data.frame(Yes = pred, No = 1 - pred)\n}\n\nRun this next script to show you what the output looks like and to test our predict_model() function. See how it’s the probabilities by classification. It must be in this form for model_type = \"classification\".\n\n\n# Test our predict_model() function\npredict_model(x = model_keras, newdata = x_test_tbl, type = 'raw') %>%\n  tibble::as_tibble()\n\n\n# A tibble: 1,406 x 2\n           Yes        No\n         <dbl>     <dbl>\n 1 0.328355074 0.6716449\n 2 0.633630514 0.3663695\n 3 0.004589651 0.9954103\n 4 0.007402068 0.9925979\n 5 0.049968336 0.9500317\n 6 0.116824441 0.8831756\n 7 0.775479317 0.2245207\n 8 0.492996633 0.5070034\n 9 0.011550998 0.9884490\n10 0.004276015 0.9957240\n# ... with 1,396 more rows\nNow the fun part, we create an explainer using the lime() function. Just pass the training data set without the “Attribution column”. The form must be a data frame, which is OK since our predict_model function will switch it to an keras object. Set model = automl_leader our leader model, and bin_continuous = FALSE. We could tell the algorithm to bin continuous variables, but this may not make sense for categorical numeric data that we didn’t change to factors.\n\n\n# Run lime() on training set\nexplainer <- lime::lime(\n  x              = x_train_tbl, \n  model          = model_keras, \n  bin_continuous = FALSE\n)\n\nNow we run the explain() function, which returns our explanation. This can take a minute to run so we limit it to just the first ten rows of the test data set. We set n_labels = 1 because we care about explaining a single class. Setting n_features = 4 returns the top four features that are critical to each case. Finally, setting kernel_width = 0.5 allows us to increase the “model_r2” value by shrinking the localized evaluation.\n\n\n# Run explain() on explainer\nexplanation <- lime::explain(\n  x_test_tbl[1:10, ], \n  explainer    = explainer, \n  n_labels     = 1, \n  n_features   = 4,\n  kernel_width = 0.5\n)\n\nFeature Importance Visualization\nThe payoff for the work we put in using LIME is this feature importance plot. This allows us to visualize each of the first ten cases (observations) from the test data. The top four features for each case are shown. Note that they are not the same for each case. The green bars mean that the feature supports the model conclusion, and the red bars contradict. A few important features based on frequency in first ten cases:\nTenure (7 cases)\nSenior Citizen (5 cases)\nOnline Security (4 cases)\n\n\nplot_features(explanation) +\n  labs(title = \"LIME Feature Importance Visualization\",\n       subtitle = \"Hold Out (Test) Set, First 10 Cases Shown\")\n\n\nAnother excellent visualization can be performed using plot_explanations(), which produces a facetted heatmap of all case/label/feature combinations. It’s a more condensed version of plot_features(), but we need to be careful because it does not provide exact statistics and it makes it less easy to investigate binned features (Notice that “tenure” would not be identified as a contributor even though it shows up as a top feature in 7 of 10 cases).\n\n\nplot_explanations(explanation) +\n    labs(title = \"LIME Feature Importance Heatmap\",\n         subtitle = \"Hold Out (Test) Set, First 10 Cases Shown\")\n\n\n\n\nCheck Explanations With Correlation Analysis\nOne thing we need to be careful with the LIME visualization is that we are only doing a sample of the data, in our case the first 10 test observations. Therefore, we are gaining a very localized understanding of how the ANN works. However, we also want to know on from a global perspective what drives feature importance.\nWe can perform a correlation analysis on the training set as well to help glean what features correlate globally to “Churn”. We’ll use the corrr package, which performs tidy correlations with the function correlate(). We can get the correlations as follows.\n\n\n# Feature correlations to Churn\ncorrr_analysis <- x_train_tbl %>%\n  mutate(Churn = y_train_vec) %>%\n  correlate() %>%\n  focus(Churn) %>%\n  rename(feature = rowname) %>%\n  arrange(abs(Churn)) %>%\n  mutate(feature = as_factor(feature)) \ncorrr_analysis\n\n\n# A tibble: 35 x 2\n                          feature        Churn\n                           <fctr>        <dbl>\n 1                    gender_Male -0.006690899\n 2                    tenure_bin3 -0.009557165\n 3 MultipleLines_No.phone.service -0.016950072\n 4               PhoneService_Yes  0.016950072\n 5              MultipleLines_Yes  0.032103354\n 6                StreamingTV_Yes  0.066192594\n 7            StreamingMovies_Yes  0.067643871\n 8           DeviceProtection_Yes -0.073301197\n 9                    tenure_bin4 -0.073371838\n10     PaymentMethod_Mailed.check -0.080451164\n# ... with 25 more rows\nThe correlation visualization helps in distinguishing which features are relavant to Churn.\n\n\n# Correlation visualization\ncorrr_analysis %>%\n  ggplot(aes(x = Churn, y = fct_reorder(feature, desc(Churn)))) +\n  geom_point() +\n  # Positive Correlations - Contribute to churn\n  geom_segment(aes(xend = 0, yend = feature), \n               color = palette_light()[[2]], \n               data = corrr_analysis %>% filter(Churn > 0)) +\n  geom_point(color = palette_light()[[2]], \n             data = corrr_analysis %>% filter(Churn > 0)) +\n  # Negative Correlations - Prevent churn\n  geom_segment(aes(xend = 0, yend = feature), \n               color = palette_light()[[1]], \n               data = corrr_analysis %>% filter(Churn < 0)) +\n  geom_point(color = palette_light()[[1]], \n             data = corrr_analysis %>% filter(Churn < 0)) +\n  # Vertical lines\n  geom_vline(xintercept = 0, color = palette_light()[[5]], size = 1, linetype = 2) +\n  geom_vline(xintercept = -0.25, color = palette_light()[[5]], size = 1, linetype = 2) +\n  geom_vline(xintercept = 0.25, color = palette_light()[[5]], size = 1, linetype = 2) +\n  # Aesthetics\n  theme_tq() +\n  labs(title = \"Churn Correlation Analysis\",\n       subtitle = paste(\"Positive Correlations (contribute to churn),\",\n                        \"Negative Correlations (prevent churn)\")\n       y = \"Feature Importance\")\n\n\n\n\nThe correlation analysis helps us quickly disseminate which features that the LIME analysis may be excluding. We can see that the following features are highly correlated (magnitude > 0.25):\nIncreases Likelihood of Churn (Red): - Tenure = Bin 1 (<12 Months) - Internet Service = “Fiber Optic” - Payment Method = “Electronic Check”\nDecreases Likelihood of Churn (Blue): - Contract = “Two Year” - Total Charges (Note that this may be a biproduct of additional services such as Online Security)\nFeature Investigation\nWe can investigate features that are most frequent in the LIME feature importance visualization along with those that the correlation analysis shows an above normal magnitude. We’ll investigate:\nTenure (7/10 LIME Cases, Highly Correlated)\nContract (Highly Correlated)\nInternet Service (Highly Correlated)\nPayment Method (Highly Correlated)\nSenior Citizen (5/10 LIME Cases)\nOnline Security (4/10 LIME Cases)\nTenure (7/10 LIME Cases, Highly Correlated)\nLIME cases indicate that the ANN model is using this feature frequently and high correlation agrees that this is important. Investigating the feature distribution, it appears that customers with lower tenure (bin 1) are more likely to leave. Opportunity: Target customers with less than 12 month tenure.\n\n\n\n\n\n\nContract (Highly Correlated)\nWhile LIME did not indicate this as a primary feature in the first 10 cases, the feature is clearly correlated with those electing to stay. Customers with one and two year contracts are much less likely to churn. Opportunity: Offer promotion to switch to long term contracts.\n\n\n\n\n\n\nInternet Service (Highly Correlated)\nWhile LIME did not indicate this as a primary feature in the first 10 cases, the feature is clearly correlated with those electing to stay. Customers with fiber optic service are more likely to churn while those with no internet service are less likely to churn. Improvement Area: Customers may be dissatisfied with fiber optic service.\n\n\n\n\n\n\nPayment Method (Highly Correlated)\nWhile LIME did not indicate this as a primary feature in the first 10 cases, the feature is clearly correlated with those electing to stay. Customers with electronic check are more likely to leave. Opportunity: Offer customers a promotion to switch to automatic payments.\n\n\n\n\n\n\nSenior Citizen (5/10 LIME Cases)\nSenior citizen appeared in several of the LIME cases indicating it was important to the ANN for the 10 samples. However, it was not highly correlated to Churn, which may indicate that the ANN is using in an more sophisticated manner (e.g. as an interaction). It’s difficult to say that senior citizens are more likely to leave, but non-senior citizens appear less at risk of churning. Opportunity: Target users in the lower age demographic.\n\n\n\n\n\n\nOnline Security (4/10 LIME Cases)\nCustomers that did not sign up for online security were more likely to leave while customers with no internet service or online security were less likely to leave. Opportunity: Promote online security and other packages that increase retention rates.\n\n\n\n\n\n\nNext Steps: Business Science University\nWe’ve just scratched the surface with the solution to this problem, but unfortunately there’s only so much ground we can cover in an article. Here are a few next steps that I’m pleased to announce will be covered in a Business Science University course coming in 2018!\nCustomer Lifetime Value\nYour organization needs to see the financial benefit so always tie your analysis to sales, profitability or ROI. Customer Lifetime Value (CLV) is a methodology that ties the business profitability to the retention rate. While we did not implement the CLV methodology herein, a full customer churn analysis would tie the churn to an classification cutoff (threshold) optimization to maximize the CLV with the predictive ANN model.\nThe simplified CLV model is:\n\\[ \nCLV=GC*\\frac{1}{1+d-r} \n\\]\nWhere,\nGC is the gross contribution per customer\nd is the annual discount rate\nr is the retention rate\nANN Performance Evaluation and Improvement\nThe ANN model we built is good, but it could be better. How we understand our model accuracy and improve on it is through the combination of two techniques:\nK-Fold Cross-Fold Validation: Used to obtain bounds for accuracy estimates.\nHyper Parameter Tuning: Used to improve model performance by searching for the best parameters possible.\nWe need to implement K-Fold Cross Validation and Hyper Parameter Tuning if we want a best-in-class model.\nDistributing Analytics\nIt’s critical to communicate data science insights to decision makers in the organization. Most decision makers in organizations are not data scientists, but these individuals make important decisions on a day-to-day basis. The Shiny application below includes a Customer Scorecard to monitor customer health (risk of churn).\n\n\n\nBusiness Science University\nYou’re probably wondering why we are going into so much detail on next steps. We are happy to announce a new project for 2018: Business Science University, an online school dedicated to helping data science learners.\nBenefits to learners:\nBuild your own online GitHub portfolio of data science projects to market your skills to future employers!\nLearn real-world applications in People Analytics (HR), Customer Analytics, Marketing Analytics, Social Media Analytics, Text Mining and Natural Language Processing (NLP), Financial and Time Series Analytics, and more!\nUse advanced machine learning techniques for both high accuracy modeling and explaining features that have an effect on the outcome!\nCreate ML-powered web-applications that can be distributed throughout an organization, enabling non-data scientists to benefit from algorithms in a user-friendly way!\nEnrollment is open so please signup for special perks. Just go to Business Science University and select enroll.\nConclusions\nCustomer churn is a costly problem. The good news is that machine learning can solve churn problems, making the organization more profitable in the process. In this article, we saw how Deep Learning can be used to predict customer churn. We built an ANN model using the new keras package that achieved 82% predictive accuracy (without tuning)! We used three new machine learning packages to help with preprocessing and measuring performance: recipes, rsample and yardstick. Finally we used lime to explain the Deep Learning model, which traditionally was impossible! We checked the LIME results with a Correlation Analysis, which brought to light other features to investigate. For the IBM Telco dataset, tenure, contract type, internet service type, payment menthod, senior citizen status, and online security status were useful in diagnosing customer churn. We hope you enjoyed this article!\n\n\n",
    "preview": "posts/2018-01-11-keras-customer-churn/images/customer_churn_analysis_corrr.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 2696,
    "preview_height": 1696
  },
  {
    "path": "posts/2018-01-10-r-interface-to-cloudml/",
    "title": "R Interface to Google CloudML",
    "description": "We are excited to announce the availability of the cloudml package, which provides an R interface to Google Cloud Machine Learning Engine. CloudML provides a number of services including on-demand access to training on GPUs and hyperparameter tuning to optimize key attributes of model architectures.",
    "author": [
      {
        "name": "J.J. Allaire",
        "url": "https://github.com/jjallaire"
      }
    ],
    "date": "2018-01-10",
    "categories": [
      "Cloud",
      "Packages/Releases"
    ],
    "contents": "\nOverview\nWe are excited to announce the availability of the cloudml package, which provides an R interface to Google Cloud Machine Learning Engine. CloudML provides a number of services including:\nScalable training of models built with the keras, tfestimators, and tensorflow R packages.\nOn-demand access to training on GPUs, including the new Tesla P100 GPUs from NVIDIA®.\nHyperparameter tuning to optmize key attributes of model architectures in order to maximize predictive accuracy.\nDeployment of trained models to the Google global prediction platform that can support thousands of users and TBs of data.\nTraining with CloudML\nOnce you’ve configured your system to publish to CloudML, training a model is as straightforward as calling the cloudml_train() function:\n\n\nlibrary(cloudml)\ncloudml_train(\"train.R\")\n\nCloudML provides a variety of GPU configurations, which can be easily selected when calling cloudml_train(). For example, the following would train the same model as above but with a Tesla K80 GPU:\n\n\ncloudml_train(\"train.R\", master_type = \"standard_gpu\")\n\nTo train using a Tesla P100 GPU you would specify \"standard_p100\":\n\n\ncloudml_train(\"train.R\", master_type = \"standard_p100\")\n\nWhen training completes the job is collected and a training run report is displayed:\n\n\nLearning More\nCheck out the cloudml package documentation to get started with training and deploying models on CloudML.\nYou can also find out more about the various capabilities of CloudML in these articles:\nTraining with CloudML goes into additional depth on managing training jobs and their output.\nHyperparameter Tuning explores how you can improve the performance of your models by running many trials with distinct hyperparameters (e.g. number and size of layers) to determine their optimal values.\nGoogle Cloud Storage provides information on copying data between your local machine and Google Storage and also describes how to use data within Google Storage during training.\nDeploying Models describes how to deploy trained models and generate predictions from them.\n",
    "preview": "posts/2018-01-10-r-interface-to-cloudml/images/cloudml.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 394,
    "preview_height": 211
  },
  {
    "path": "posts/2018-01-09-keras-duplicate-questions-quora/",
    "title": "Classifying Duplicate Questions from Quora with Keras",
    "description": "In this post we will use Keras to classify duplicated questions from Quora. Our implementation is inspired by the Siamese Recurrent Architecture, with modifications to the similarity measure and the embedding layers (the original paper uses pre-trained word vectors)",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": "https://github.com/dfalbel"
      }
    ],
    "date": "2018-01-09",
    "categories": [
      "TensorFlow/Keras",
      "Natural Language Processing"
    ],
    "contents": "\nIntroduction\nIn this post we will use Keras to classify duplicated questions from Quora. The dataset first appeared in the Kaggle competition Quora Question Pairs and consists of approximately 400,000 pairs of questions along with a column indicating if the question pair is considered a duplicate.\nOur implementation is inspired by the Siamese Recurrent Architecture, with modifications to the similarity measure and the embedding layers (the original paper uses pre-trained word vectors). Using this kind of architecture dates back to 2005 with Le Cun et al and is useful for verification tasks. The idea is to learn a function that maps input patterns into a target space such that a similarity measure in the target space approximates the “semantic” distance in the input space.\nAfter the competition, Quora also described their approach to this problem in this blog post.\nDowloading data\nData can be downloaded from the Kaggle dataset webpage or from Quora’s release of the dataset:\n\n\nlibrary(keras)\nquora_data <- get_file(\n  \"quora_duplicate_questions.tsv\",\n  \"https://qim.ec.quoracdn.net/quora_duplicate_questions.tsv\"\n)\n\nWe are using the Keras get_file() function so that the file download is cached.\nReading and preprocessing\nWe will first load data into R and do some preprocessing to make it easier to include in the model. After downloading the data, you can read it using the readr read_tsv() function.\n\n\nlibrary(readr)\ndf <- read_tsv(quora_data)\n\nWe will create a Keras tokenizer to transform each word into an integer token. We will also specify a hyperparameter of our model: the vocabulary size. For now let’s use the 50,000 most common words (we’ll tune this parameter later). The tokenizer will be fit using all unique questions from the dataset.\n\n\ntokenizer <- text_tokenizer(num_words = 50000)\ntokenizer %>% fit_text_tokenizer(unique(c(df$question1, df$question2)))\n\nLet’s save the tokenizer to disk in order to use it for inference later.\n\n\nsave_text_tokenizer(tokenizer, \"tokenizer-question-pairs\")\n\nWe will now use the text tokenizer to transform each question into a list of integers.\n\n\nquestion1 <- texts_to_sequences(tokenizer, df$question1)\nquestion2 <- texts_to_sequences(tokenizer, df$question2)\n\nLet’s take a look at the number of words in each question. This will helps us to decide the padding length, another hyperparameter of our model. Padding the sequences normalizes them to the same size so that we can feed them to the Keras model.\n\n\nlibrary(purrr)\nquestions_length <- c(\n  map_int(question1, length),\n  map_int(question2, length)\n)\n\nquantile(questions_length, c(0.8, 0.9, 0.95, 0.99))\n\n\n80% 90% 95% 99% \n 14  18  23  31 \nWe can see that 99% of questions have at most length 31 so we’ll choose a padding length between 15 and 30. Let’s start with 20 (we’ll also tune this parameter later). The default padding value is 0, but we are already using this value for words that don’t appear within the 50,000 most frequent, so we’ll use 50,001 instead.\n\n\nquestion1_padded <- pad_sequences(question1, maxlen = 20, value = 50000 + 1)\nquestion2_padded <- pad_sequences(question2, maxlen = 20, value = 50000 + 1)\n\nWe have now finished the preprocessing steps. We will now run a simple benchmark model before moving on to the Keras model.\nSimple benchmark\nBefore creating a complicated model let’s take a simple approach. Let’s create two predictors: percentage of words from question1 that appear in the question2 and vice-versa. Then we will use a logistic regression to predict if the questions are duplicate.\n\n\nperc_words_question1 <- map2_dbl(question1, question2, ~mean(.x %in% .y))\nperc_words_question2 <- map2_dbl(question2, question1, ~mean(.x %in% .y))\n\ndf_model <- data.frame(\n  perc_words_question1 = perc_words_question1,\n  perc_words_question2 = perc_words_question2,\n  is_duplicate = df$is_duplicate\n) %>%\n  na.omit()\n\nNow that we have our predictors, let’s create the logistic model. We will take a small sample for validation.\n\n\nval_sample <- sample.int(nrow(df_model), 0.1*nrow(df_model))\nlogistic_regression <- glm(\n  is_duplicate ~ perc_words_question1 + perc_words_question2, \n  family = \"binomial\",\n  data = df_model[-val_sample,]\n)\nsummary(logistic_regression)\n\n\nCall:\nglm(formula = is_duplicate ~ perc_words_question1 + perc_words_question2, \n    family = \"binomial\", data = df_model[-val_sample, ])\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.5938  -0.9097  -0.6106   1.1452   2.0292  \n\nCoefficients:\n                      Estimate Std. Error z value Pr(>|z|)    \n(Intercept)          -2.259007   0.009668 -233.66   <2e-16 ***\nperc_words_question1  1.517990   0.023038   65.89   <2e-16 ***\nperc_words_question2  1.681410   0.022795   73.76   <2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 479158  on 363843  degrees of freedom\nResidual deviance: 431627  on 363841  degrees of freedom\n  (17 observations deleted due to missingness)\nAIC: 431633\n\nNumber of Fisher Scoring iterations: 3\nLet’s calculate the accuracy on our validation set.\n\n\npred <- predict(logistic_regression, df_model[val_sample,], type = \"response\")\npred <- pred > mean(df_model$is_duplicate[-val_sample])\naccuracy <- table(pred, df_model$is_duplicate[val_sample]) %>% \n  prop.table() %>% \n  diag() %>% \n  sum()\naccuracy\n\n\n[1] 0.6573577\nWe got an accuracy of 65.7%. Not all that much better than random guessing. Now let’s create our model in Keras.\nModel definition\nWe will use a Siamese network to predict whether the pairs are duplicated or not. The idea is to create a model that can embed the questions (sequence of words) into a vector. Then we can compare the vectors for each question using a similarity measure and tell if the questions are duplicated or not.\nFirst let’s define the inputs for the model.\n\n\ninput1 <- layer_input(shape = c(20), name = \"input_question1\")\ninput2 <- layer_input(shape = c(20), name = \"input_question2\")\n\nThen let’s the define the part of the model that will embed the questions in a vector.\n\n\nword_embedder <- layer_embedding( \n  input_dim = 50000 + 2, # vocab size + UNK token + padding value\n  output_dim = 128,      # hyperparameter - embedding size\n  input_length = 20,     # padding size,\n  embeddings_regularizer = regularizer_l2(0.0001) # hyperparameter - regularization \n)\n\nseq_embedder <- layer_lstm(\n  units = 128, # hyperparameter -- sequence embedding size\n  kernel_regularizer = regularizer_l2(0.0001) # hyperparameter - regularization \n)\n\nNow we will define the relationship between the input vectors and the embeddings layers. Note that we use the same layers and weights on both inputs. That’s why this is called a Siamese network. It makes sense, because we don’t want to have different outputs if question1 is switched with question2.\n\n\nvector1 <- input1 %>% word_embedder() %>% seq_embedder()\nvector2 <- input2 %>% word_embedder() %>% seq_embedder()\n\nWe then define the similarity measure we want to optimize. We want duplicated questions to have higher values of similarity. In this example we’ll use the cosine similarity, but any similarity measure could be used. Remember that the cosine similarity is the normalized dot product of the vectors, but for training it’s not necessary to normalize the results.\n\n\ncosine_similarity <- layer_dot(list(vector1, vector2), axes = 1)\n\nNext, we define a final sigmoid layer to output the probability of both questions being duplicated.\n\n\noutput <- cosine_similarity %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nNow that let’s define the Keras model in terms of it’s inputs and outputs and compile it. In the compilation phase we define our loss function and optimizer. Like in the Kaggle challenge, we will minimize the logloss (equivalent to minimizing the binary crossentropy). We will use the Adam optimizer.\n\n\nmodel <- keras_model(list(input1, input2), output)\nmodel %>% compile(\n  optimizer = \"adam\", \n  metrics = list(acc = metric_binary_accuracy), \n  loss = \"binary_crossentropy\"\n)\n\nWe can then take a look at out model with the summary function.\n\n\nsummary(model)\n\n\n_______________________________________________________________________________________\nLayer (type)                Output Shape       Param #    Connected to                 \n=======================================================================================\ninput_question1 (InputLayer (None, 20)         0                                       \n_______________________________________________________________________________________\ninput_question2 (InputLayer (None, 20)         0                                       \n_______________________________________________________________________________________\nembedding_1 (Embedding)     (None, 20, 128)    6400256    input_question1[0][0]        \n                                                          input_question2[0][0]        \n_______________________________________________________________________________________\nlstm_1 (LSTM)               (None, 128)        131584     embedding_1[0][0]            \n                                                          embedding_1[1][0]            \n_______________________________________________________________________________________\ndot_1 (Dot)                 (None, 1)          0          lstm_1[0][0]                 \n                                                          lstm_1[1][0]                 \n_______________________________________________________________________________________\ndense_1 (Dense)             (None, 1)          2          dot_1[0][0]                  \n=======================================================================================\nTotal params: 6,531,842\nTrainable params: 6,531,842\nNon-trainable params: 0\n_______________________________________________________________________________________\nModel fitting\nNow we will fit and tune our model. However before proceeding let’s take a sample for validation.\n\n\nset.seed(1817328)\nval_sample <- sample.int(nrow(question1_padded), size = 0.1*nrow(question1_padded))\n\ntrain_question1_padded <- question1_padded[-val_sample,]\ntrain_question2_padded <- question2_padded[-val_sample,]\ntrain_is_duplicate <- df$is_duplicate[-val_sample]\n\nval_question1_padded <- question1_padded[val_sample,]\nval_question2_padded <- question2_padded[val_sample,]\nval_is_duplicate <- df$is_duplicate[val_sample]\n\nNow we use the fit() function to train the model:\n\n\nmodel %>% fit(\n  list(train_question1_padded, train_question2_padded),\n  train_is_duplicate, \n  batch_size = 64, \n  epochs = 10, \n  validation_data = list(\n    list(val_question1_padded, val_question2_padded), \n    val_is_duplicate\n  )\n)\n\n\nTrain on 363861 samples, validate on 40429 samples\nEpoch 1/10\n363861/363861 [==============================] - 89s 245us/step - loss: 0.5860 - acc: 0.7248 - val_loss: 0.5590 - val_acc: 0.7449\nEpoch 2/10\n363861/363861 [==============================] - 88s 243us/step - loss: 0.5528 - acc: 0.7461 - val_loss: 0.5472 - val_acc: 0.7510\nEpoch 3/10\n363861/363861 [==============================] - 88s 242us/step - loss: 0.5428 - acc: 0.7536 - val_loss: 0.5439 - val_acc: 0.7515\nEpoch 4/10\n363861/363861 [==============================] - 88s 242us/step - loss: 0.5353 - acc: 0.7595 - val_loss: 0.5358 - val_acc: 0.7590\nEpoch 5/10\n363861/363861 [==============================] - 88s 242us/step - loss: 0.5299 - acc: 0.7633 - val_loss: 0.5358 - val_acc: 0.7592\nEpoch 6/10\n363861/363861 [==============================] - 88s 242us/step - loss: 0.5256 - acc: 0.7662 - val_loss: 0.5309 - val_acc: 0.7631\nEpoch 7/10\n363861/363861 [==============================] - 88s 242us/step - loss: 0.5211 - acc: 0.7701 - val_loss: 0.5349 - val_acc: 0.7586\nEpoch 8/10\n363861/363861 [==============================] - 88s 242us/step - loss: 0.5173 - acc: 0.7733 - val_loss: 0.5278 - val_acc: 0.7667\nEpoch 9/10\n363861/363861 [==============================] - 88s 242us/step - loss: 0.5138 - acc: 0.7762 - val_loss: 0.5292 - val_acc: 0.7667\nEpoch 10/10\n363861/363861 [==============================] - 88s 242us/step - loss: 0.5092 - acc: 0.7794 - val_loss: 0.5313 - val_acc: 0.7654\nAfter training completes, we can save our model for inference with the save_model_hdf5() function.\n\n\nsave_model_hdf5(model, \"model-question-pairs.hdf5\")\n\nModel tuning\nNow that we have a reasonable model, let’s tune the hyperparameters using the tfruns package. We’ll begin by adding FLAGS declarations to our script for all hyperparameters we want to tune (FLAGS allow us to vary hyperparmaeters without changing our source code):\n\n\nFLAGS <- flags(\n  flag_integer(\"vocab_size\", 50000),\n  flag_integer(\"max_len_padding\", 20),\n  flag_integer(\"embedding_size\", 256),\n  flag_numeric(\"regularization\", 0.0001),\n  flag_integer(\"seq_embedding_size\", 512)\n)\n\nWith this FLAGS definition we can now write our code in terms of the flags. For example:\n\n\ninput1 <- layer_input(shape = c(FLAGS$max_len_padding))\ninput2 <- layer_input(shape = c(FLAGS$max_len_padding))\n\nembedding <- layer_embedding(\n  input_dim = FLAGS$vocab_size + 2, \n  output_dim = FLAGS$embedding_size, \n  input_length = FLAGS$max_len_padding, \n  embeddings_regularizer = regularizer_l2(l = FLAGS$regularization)\n)\n\nThe full source code of the script with FLAGS can be found here.\nWe additionally added an early stopping callback in the training step in order to stop training if validation loss doesn’t decrease for 5 epochs in a row. This will hopefully reduce training time for bad models. We also added a learning rate reducer to reduce the learning rate by a factor of 10 when the loss doesn’t decrease for 3 epochs (this technique typically increases model accuracy).\n\n\nmodel %>% fit(\n  ...,\n  callbacks = list(\n    callback_early_stopping(patience = 5),\n    callback_reduce_lr_on_plateau(patience = 3)\n  )\n)\n\nWe can now execute a tuning run to probe for the optimal combination of hyperparameters. We call the tuning_run() function, passing a list with the possible values for each flag. The tuning_run() function will be responsible for executing the script for all combinations of hyperparameters. We also specify the sample parameter to train the model for only a random sample from all combinations (reducing training time significantly).\n\n\nlibrary(tfruns)\n\nruns <- tuning_run(\n  \"question-pairs.R\", \n  flags = list(\n    vocab_size = c(30000, 40000, 50000, 60000),\n    max_len_padding = c(15, 20, 25),\n    embedding_size = c(64, 128, 256),\n    regularization = c(0.00001, 0.0001, 0.001),\n    seq_embedding_size = c(128, 256, 512)\n  ), \n  runs_dir = \"tuning\", \n  sample = 0.2\n)\n\nThe tuning run will return a data.frame with results for all runs. We found that the best run attained 84.9% accuracy using the combination of hyperparameters shown below, so we modify our training script to use these values as the defaults:\n\n\nFLAGS <- flags(\n  flag_integer(\"vocab_size\", 50000),\n  flag_integer(\"max_len_padding\", 20),\n  flag_integer(\"embedding_size\", 256),\n  flag_numeric(\"regularization\", 1e-4),\n  flag_integer(\"seq_embedding_size\", 512)\n)\n\nMaking predictions\nNow that we have trained and tuned our model we can start making predictions. At prediction time we will load both the text tokenizer and the model we saved to disk earlier.\n\n\nlibrary(keras)\nmodel <- load_model_hdf5(\"model-question-pairs.hdf5\", compile = FALSE)\ntokenizer <- load_text_tokenizer(\"tokenizer-question-pairs\")\n\nSince we won’t continue training the model, we specified the compile = FALSE argument.\nNow let`s define a function to create predictions. In this function we we preprocess the input data in the same way we preprocessed the training data:\n\n\npredict_question_pairs <- function(model, tokenizer, q1, q2) {\n  q1 <- texts_to_sequences(tokenizer, list(q1))\n  q2 <- texts_to_sequences(tokenizer, list(q2))\n  \n  q1 <- pad_sequences(q1, 20)\n  q2 <- pad_sequences(q2, 20)\n  \n  as.numeric(predict(model, list(q1, q2)))\n}\n\nWe can now call it with new pairs of questions, for example:\n\n\npredict_question_pairs(\n  model,\n  tokenizer,\n  \"What's R programming?\",\n  \"What's R in programming?\"\n)\n\n\n[1] 0.9784008\nPrediction is quite fast (~40 milliseconds).\nDeploying the model\nTo demonstrate deployment of the trained model, we created a simple Shiny application, where you can paste 2 questions from Quora and find the probability of them being duplicated. Try changing the questions below or entering two entirely different questions.\n\n\nThe shiny application can be found at https://jjallaire.shinyapps.io/shiny-quora/ and it’s source code at https://github.com/dfalbel/shiny-quora-question-pairs.\nNote that when deploying a Keras model you only need to load the previously saved model file and tokenizer (no training data or model training steps are required).\nWrapping up\nWe trained a Siamese LSTM that gives us reasonable accuracy (84%). Quora’s state of the art is 87%.\nWe can improve our model by using pre-trained word embeddings on larger datasets. For example, try using what’s described in this example. Quora uses their own complete corpus to train the word embeddings.\nAfter training we deployed our model as a Shiny application which given two Quora questions calculates the probability of their being duplicates.\n",
    "preview": "posts/2018-01-09-keras-duplicate-questions-quora/keras-duplicate-questions-quora.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 1302,
    "preview_height": 788
  },
  {
    "path": "posts/2017-12-22-word-embeddings-with-keras/",
    "title": "Word Embeddings with Keras",
    "description": "Word embedding is a method used to map words of a vocabulary to dense vectors of real numbers where semantically similar words are mapped to nearby points. In this example we'll use Keras to generate word embeddings for the Amazon Fine Foods Reviews dataset.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": "https://github.com/dfalbel"
      }
    ],
    "date": "2017-12-22",
    "categories": [
      "TensorFlow/Keras",
      "Natural Language Processing"
    ],
    "contents": "\nIntroduction\nWord embedding is a method used to map words of a vocabulary to dense vectors of real numbers where semantically similar words are mapped to nearby points. Representing words in this vector space help algorithms achieve better performance in natural language processing tasks like syntactic parsing and sentiment analysis by grouping similar words. For example, we expect that in the embedding space “cats” and “dogs” are mapped to nearby points since they are both animals, mammals, pets, etc.\nIn this tutorial we will implement the skip-gram model created by Mikolov et al in R using the keras package. The skip-gram model is a flavor of word2vec, a class of computationally-efficient predictive models for learning word embeddings from raw text. We won’t address theoretical details about embeddings and the skip-gram model. If you want to get more details you can read the paper linked above. The TensorFlow Vector Representation of Words tutorial includes additional details as does the Deep Learning With R notebook about embeddings.\nThere are other ways to create vector representations of words. For example, GloVe Embeddings are implemented in the text2vec package by Dmitriy Selivanov. There’s also a tidy approach described in Julia Silge’s blog post Word Vectors with Tidy Data Principles.\nGetting the Data\nWe will use the Amazon Fine Foods Reviews dataset. This dataset consists of reviews of fine foods from Amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and narrative text.\nData can be downloaded (~116MB) by running:\n\n\ndownload.file(\"https://snap.stanford.edu/data/finefoods.txt.gz\", \"finefoods.txt.gz\")\n\nWe will now load the plain text reviews into R.\n\n\nlibrary(readr)\nlibrary(stringr)\nreviews <- read_lines(\"finefoods.txt.gz\") \nreviews <- reviews[str_sub(reviews, 1, 12) == \"review/text:\"]\nreviews <- str_sub(reviews, start = 14)\nreviews <- iconv(reviews, to = \"UTF-8\")\n\nLet’s take a look at some reviews we have in the dataset.\n\n\nhead(reviews, 2)\n\n\n[1] \"I have bought several of the Vitality canned dog food products ...\n[2] \"Product arrived labeled as Jumbo Salted Peanuts...the peanuts ... \nPreprocessing\nWe’ll begin with some text pre-processing using a keras text_tokenizer(). The tokenizer will be responsible for transforming each review into a sequence of integer tokens (which will subsequently be used as input into the skip-gram model).\n\n\nlibrary(keras)\ntokenizer <- text_tokenizer(num_words = 20000)\ntokenizer %>% fit_text_tokenizer(reviews)\n\nNote that the tokenizer object is modified in place by the call to fit_text_tokenizer(). An integer token will be assigned for each of the 20,000 most common words (the other words will be assigned to token 0).\nSkip-Gram Model\nIn the skip-gram model we will use each word as input to a log-linear classifier with a projection layer, then predict words within a certain range before and after this word. It would be very computationally expensive to output a probability distribution over all the vocabulary for each target word we input into the model. Instead, we are going to use negative sampling, meaning we will sample some words that don’t appear in the context and train a binary classifier to predict if the context word we passed is truly from the context or not.\nIn more practical terms, for the skip-gram model we will input a 1d integer vector of the target word tokens and a 1d integer vector of sampled context word tokens. We will generate a prediction of 1 if the sampled word really appeared in the context and 0 if it didn’t.\nWe will now define a generator function to yield batches for model training.\n\n\nlibrary(reticulate)\nlibrary(purrr)\nskipgrams_generator <- function(text, tokenizer, window_size, negative_samples) {\n  gen <- texts_to_sequences_generator(tokenizer, sample(text))\n  function() {\n    skip <- generator_next(gen) %>%\n      skipgrams(\n        vocabulary_size = tokenizer$num_words, \n        window_size = window_size, \n        negative_samples = 1\n      )\n    x <- transpose(skip$couples) %>% map(. %>% unlist %>% as.matrix(ncol = 1))\n    y <- skip$labels %>% as.matrix(ncol = 1)\n    list(x, y)\n  }\n}\n\nA generator function is a function that returns a different value each time it is called (generator functions are often used to provide streaming or dynamic data for training models). Our generator function will receive a vector of texts, a tokenizer and the arguments for the skip-gram (the size of the window around each target word we examine and how many negative samples we want to sample for each target word).\nNow let’s start defining the keras model. We will use the Keras functional API.\n\n\nembedding_size <- 128  # Dimension of the embedding vector.\nskip_window <- 5       # How many words to consider left and right.\nnum_sampled <- 1       # Number of negative examples to sample for each word.\n\nWe will first write placeholders for the inputs using the layer_input function.\n\n\ninput_target <- layer_input(shape = 1)\ninput_context <- layer_input(shape = 1)\n\nNow let’s define the embedding matrix. The embedding is a matrix with dimensions (vocabulary, embedding_size) that acts as lookup table for the word vectors.\n\n\nembedding <- layer_embedding(\n  input_dim = tokenizer$num_words + 1, \n  output_dim = embedding_size, \n  input_length = 1, \n  name = \"embedding\"\n)\n\ntarget_vector <- input_target %>% \n  embedding() %>% \n  layer_flatten()\n\ncontext_vector <- input_context %>%\n  embedding() %>%\n  layer_flatten()\n\nThe next step is to define how the target_vector will be related to the context_vector in order to make our network output 1 when the context word really appeared in the context and 0 otherwise. We want target_vector to be similar to the context_vector if they appeared in the same context. A typical measure of similarity is the cosine similarity. Give two vectors \\(A\\) and \\(B\\) the cosine similarity is defined by the Euclidean Dot product of \\(A\\) and \\(B\\) normalized by their magnitude. As we don’t need the similarity to be normalized inside the network, we will only calculate the dot product and then output a dense layer with sigmoid activation.\n\n\ndot_product <- layer_dot(list(target_vector, context_vector), axes = 1)\noutput <- layer_dense(dot_product, units = 1, activation = \"sigmoid\")\n\nNow we will create the model and compile it.\n\n\nmodel <- keras_model(list(input_target, input_context), output)\nmodel %>% compile(loss = \"binary_crossentropy\", optimizer = \"adam\")\n\nWe can see the full definition of the model by calling summary:\n\n\nsummary(model)\n\n\n_________________________________________________________________________________________\nLayer (type)                 Output Shape       Param #    Connected to                  \n=========================================================================================\ninput_1 (InputLayer)         (None, 1)          0                                        \n_________________________________________________________________________________________\ninput_2 (InputLayer)         (None, 1)          0                                        \n_________________________________________________________________________________________\nembedding (Embedding)        (None, 1, 128)     2560128    input_1[0][0]                 \n                                                           input_2[0][0]                 \n_________________________________________________________________________________________\nflatten_1 (Flatten)          (None, 128)        0          embedding[0][0]               \n_________________________________________________________________________________________\nflatten_2 (Flatten)          (None, 128)        0          embedding[1][0]               \n_________________________________________________________________________________________\ndot_1 (Dot)                  (None, 1)          0          flatten_1[0][0]               \n                                                           flatten_2[0][0]               \n_________________________________________________________________________________________\ndense_1 (Dense)              (None, 1)          2          dot_1[0][0]                   \n=========================================================================================\nTotal params: 2,560,130\nTrainable params: 2,560,130\nNon-trainable params: 0\n_________________________________________________________________________________________\nModel Training\nWe will fit the model using the fit_generator() function We need to specify the number of training steps as well as number of epochs we want to train. We will train for 100,000 steps for 5 epochs. This is quite slow (~1000 seconds per epoch on a modern GPU). Note that you may also get reasonable results with just one epoch of training.\n\n\nmodel %>%\n  fit_generator(\n    skipgrams_generator(reviews, tokenizer, skip_window, negative_samples), \n    steps_per_epoch = 100000, epochs = 5\n    )\n\n\nEpoch 1/1\n100000/100000 [==============================] - 1092s - loss: 0.3749      \nEpoch 2/5\n100000/100000 [==============================] - 1094s - loss: 0.3548     \nEpoch 3/5\n100000/100000 [==============================] - 1053s - loss: 0.3630     \nEpoch 4/5\n100000/100000 [==============================] - 1020s - loss: 0.3737     \nEpoch 5/5\n100000/100000 [==============================] - 1017s - loss: 0.3823 \nWe can now extract the embeddings matrix from the model by using the get_weights() function. We also added row.names to our embedding matrix so we can easily find where each word is.\n\n\nlibrary(dplyr)\n\nembedding_matrix <- get_weights(model)[[1]]\n\nwords <- data_frame(\n  word = names(tokenizer$word_index), \n  id = as.integer(unlist(tokenizer$word_index))\n)\n\nwords <- words %>%\n  filter(id <= tokenizer$num_words) %>%\n  arrange(id)\n\nrow.names(embedding_matrix) <- c(\"UNK\", words$word)\n\nUnderstanding the Embeddings\nWe can now find words that are close to each other in the embedding. We will use the cosine similarity, since this is what we trained the model to minimize.\n\n\nlibrary(text2vec)\n\nfind_similar_words <- function(word, embedding_matrix, n = 5) {\n  similarities <- embedding_matrix[word, , drop = FALSE] %>%\n    sim2(embedding_matrix, y = ., method = \"cosine\")\n  \n  similarities[,1] %>% sort(decreasing = TRUE) %>% head(n)\n}\n\n\n\nfind_similar_words(\"2\", embedding_matrix)\n\n\n        2         4         3       two         6 \n1.0000000 0.9830254 0.9777042 0.9765668 0.9722549 \n\n\nfind_similar_words(\"little\", embedding_matrix)\n\n\n   little       bit       few     small     treat \n1.0000000 0.9501037 0.9478287 0.9309829 0.9286966 \n\n\nfind_similar_words(\"delicious\", embedding_matrix)\n\n\ndelicious     tasty wonderful   amazing     yummy \n1.0000000 0.9632145 0.9619508 0.9617954 0.9529505 \n\n\nfind_similar_words(\"cats\", embedding_matrix)\n\n\n     cats      dogs      kids       cat       dog \n1.0000000 0.9844937 0.9743756 0.9676026 0.9624494 \nThe t-SNE algorithm can be used to visualize the embeddings. Because of time constraints we will only use it with the first 500 words. To understand more about the t-SNE method see the article How to Use t-SNE Effectively.\nThis plot may look like a mess, but if you zoom into the small groups you end up seeing some nice patterns. Try, for example, to find a group of web related words like http, href, etc. Another group that may be easy to pick out is the pronouns group: she, he, her, etc.\n\n\nlibrary(Rtsne)\nlibrary(ggplot2)\nlibrary(plotly)\n\ntsne <- Rtsne(embedding_matrix[2:500,], perplexity = 50, pca = FALSE)\n\ntsne_plot <- tsne$Y %>%\n  as.data.frame() %>%\n  mutate(word = row.names(embedding_matrix)[2:500]) %>%\n  ggplot(aes(x = V1, y = V2, label = word)) + \n  geom_text(size = 3)\ntsne_plot\n\n\n\n\n\n",
    "preview": "posts/2017-12-22-word-embeddings-with-keras/word-embeddings-with-keras.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 700,
    "preview_height": 450
  },
  {
    "path": "posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks/",
    "title": "Time Series Forecasting with Recurrent Neural Networks",
    "description": "In this post, we'll review three advanced techniques for improving the performance and generalization power of recurrent neural networks.  We'll demonstrate all three concepts on a temperature-forecasting problem, where you have access to a time series of data points coming from sensors installed on the roof of a building.",
    "author": [
      {
        "name": "François Chollet",
        "url": "https://github.com/fchollet"
      },
      {
        "name": "J.J. Allaire",
        "url": "https://github.com/jjallaire"
      }
    ],
    "date": "2017-12-20",
    "categories": [
      "TensorFlow/Keras",
      "Time Series"
    ],
    "contents": "\nOverview\nIn this post, we’ll review three advanced techniques for improving the performance and generalization power of recurrent neural networks. By the end of the section, you’ll know most of what there is to know about using recurrent networks with Keras. We’ll demonstrate all three concepts on a temperature-forecasting problem, where you have access to a time series of data points coming from sensors installed on the roof of a building, such as temperature, air pressure, and humidity, which you use to predict what the temperature will be 24 hours after the last data point. This is a fairly challenging problem that exemplifies many common difficulties encountered when working with time series.\nWe’ll cover the following techniques:\nRecurrent dropout — This is a specific, built-in way to use dropout to fight overfitting in recurrent layers.\nStacking recurrent layers — This increases the representational power of the network (at the cost of higher computational loads).\nBidirectional recurrent layers — These present the same information to a recurrent network in different ways, increasing accuracy and mitigating forgetting issues.\nA temperature-forecasting problem\nUntil now, the only sequence data we’ve covered has been text data, such as the IMDB dataset and the Reuters dataset. But sequence data is found in many more problems than just language processing. In all the examples in this section, you’ll play with a weather timeseries dataset recorded at the Weather Station at the Max Planck Institute for Biogeochemistry in Jena, Germany.\nIn this dataset, 14 different quantities (such air temperature, atmospheric pressure, humidity, wind direction, and so on) were recorded every 10 minutes, over several years. The original data goes back to 2003, but this example is limited to data from 2009–2016. This dataset is perfect for learning to work with numerical time series. You’ll use it to build a model that takes as input some data from the recent past (a few days’ worth of data points) and predicts the air temperature 24 hours in the future.\nDownload and uncompress the data as follows:\n\n\ndir.create(\"~/Downloads/jena_climate\", recursive = TRUE)\ndownload.file(\n  \"https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\",\n  \"~/Downloads/jena_climate/jena_climate_2009_2016.csv.zip\"\n)\nunzip(\n  \"~/Downloads/jena_climate/jena_climate_2009_2016.csv.zip\",\n  exdir = \"~/Downloads/jena_climate\"\n)\n\nLet’s look at the data.\n\n\nlibrary(tibble)\nlibrary(readr)\n\ndata_dir <- \"~/Downloads/jena_climate\"\nfname <- file.path(data_dir, \"jena_climate_2009_2016.csv\")\ndata <- read_csv(fname)\n\nglimpse(data)\n\n\nObservations: 420,551\nVariables: 15\n$ `Date Time`       <chr> \"01.01.2009 00:10:00\", \"01.01.2009 00:20:00\", \"...\n$ `p (mbar)`        <dbl> 996.52, 996.57, 996.53, 996.51, 996.51, 996.50,...\n$ `T (degC)`        <dbl> -8.02, -8.41, -8.51, -8.31, -8.27, -8.05, -7.62...\n$ `Tpot (K)`        <dbl> 265.40, 265.01, 264.91, 265.12, 265.15, 265.38,...\n$ `Tdew (degC)`     <dbl> -8.90, -9.28, -9.31, -9.07, -9.04, -8.78, -8.30...\n$ `rh (%)`          <dbl> 93.3, 93.4, 93.9, 94.2, 94.1, 94.4, 94.8, 94.4,...\n$ `VPmax (mbar)`    <dbl> 3.33, 3.23, 3.21, 3.26, 3.27, 3.33, 3.44, 3.44,...\n$ `VPact (mbar)`    <dbl> 3.11, 3.02, 3.01, 3.07, 3.08, 3.14, 3.26, 3.25,...\n$ `VPdef (mbar)`    <dbl> 0.22, 0.21, 0.20, 0.19, 0.19, 0.19, 0.18, 0.19,...\n$ `sh (g/kg)`       <dbl> 1.94, 1.89, 1.88, 1.92, 1.92, 1.96, 2.04, 2.03,...\n$ `H2OC (mmol/mol)` <dbl> 3.12, 3.03, 3.02, 3.08, 3.09, 3.15, 3.27, 3.26,...\n$ `rho (g/m**3)`    <dbl> 1307.75, 1309.80, 1310.24, 1309.19, 1309.00, 13...\n$ `wv (m/s)`        <dbl> 1.03, 0.72, 0.19, 0.34, 0.32, 0.21, 0.18, 0.19,...\n$ `max. wv (m/s)`   <dbl> 1.75, 1.50, 0.63, 0.50, 0.63, 0.63, 0.63, 0.50,...\n$ `wd (deg)`        <dbl> 152.3, 136.1, 171.6, 198.0, 214.3, 192.7, 166.5...\nHere is the plot of temperature (in degrees Celsius) over time. On this plot, you can clearly see the yearly periodicity of temperature.\n\n\nlibrary(ggplot2)\nggplot(data, aes(x = 1:nrow(data), y = `T (degC)`)) + geom_line()\n\n\nHere is a more narrow plot of the first 10 days of temperature data (see figure 6.15). Because the data is recorded every 10 minutes, you get 144 data points per day.\n\n\nggplot(data[1:1440,], aes(x = 1:1440, y = `T (degC)`)) + geom_line()\n\n\nOn this plot, you can see daily periodicity, especially evident for the last 4 days. Also note that this 10-day period must be coming from a fairly cold winter month.\nIf you were trying to predict average temperature for the next month given a few months of past data, the problem would be easy, due to the reliable year-scale periodicity of the data. But looking at the data over a scale of days, the temperature looks a lot more chaotic. Is this time series predictable at a daily scale? Let’s find out.\nPreparing the data\nThe exact formulation of the problem will be as follows: given data going as far back as lookback timesteps (a timestep is 10 minutes) and sampled every steps timesteps, can you predict the temperature in delay timesteps? You’ll use the following parameter values:\nlookback = 1440 — Observations will go back 10 days.\nsteps = 6 — Observations will be sampled at one data point per hour.\ndelay = 144 — Targets will be 24 hours in the future.\nTo get started, you need to do two things:\nPreprocess the data to a format a neural network can ingest. This is easy: the data is already numerical, so you don’t need to do any vectorization. But each time series in the data is on a different scale (for example, temperature is typically between -20 and +30, but atmospheric pressure, measured in mbar, is around 1,000). You’ll normalize each time series independently so that they all take small values on a similar scale.\nWrite a generator function that takes the current array of float data and yields batches of data from the recent past, along with a target temperature in the future. Because the samples in the dataset are highly redundant (sample N and sample N + 1 will have most of their timesteps in common), it would be wasteful to explicitly allocate every sample. Instead, you’ll generate the samples on the fly using the original data.\n\n\n.note {\n  border: solid 1px rgba(0, 0, 0, 0.3); \n  padding: 16px 25px; \n  margin-bottom: 20px;\n  background-color: rgb(253,253,253);\n}\n\n\nNOTE: Understanding generator functions A generator function is a special type of function that you call repeatedly to obtain a sequence of values from. Often generators need to maintain internal state, so they are typically constructed by calling another yet another function which returns the generator function (the environment of the function which returns the generator is then used to track state).\nFor example, the sequence_generator() function below returns a generator function that yields an infinite sequence of numbers:\n\n\nsequence_generator <- function(start) {\n  value <- start - 1\n  function() {\n    value <<- value + 1\n    value\n  }\n}\n\ngen <- sequence_generator(10)\ngen()\n\n\n[1] 10\n\n\ngen()\n\n\n[1] 11\nThe current state of the generator is the value variable that is defined outside of the function. Note that superassignment (<<-) is used to update this state from within the function.\nGenerator functions can signal completion by returning the value NULL. However, generator functions passed to Keras training methods (e.g. fit_generator()) should always return values infinitely (the number of calls to the generator function is controlled by the epochs and steps_per_epoch parameters).\n\nFirst, you’ll convert the R data frame which we read earlier into a matrix of floating point values (we’ll discard the first column which included a text timestamp):\n\n\ndata <- data.matrix(data[,-1])\n\nYou’ll then preprocess the data by subtracting the mean of each time series and dividing by the standard deviation. You’re going to use the first 200,000 timesteps as training data, so compute the mean and standard deviation for normalization only on this fraction of the data.\n\n\ntrain_data <- data[1:200000,]\nmean <- apply(train_data, 2, mean)\nstd <- apply(train_data, 2, sd)\ndata <- scale(data, center = mean, scale = std)\n\nThe code for the data generator you’ll use is below. It yields a list (samples, targets), where samples is one batch of input data and targets is the corresponding array of target temperatures. It takes the following arguments:\ndata — The original array of floating-point data, which you normalized in listing 6.32.\nlookback — How many timesteps back the input data should go.\ndelay — How many timesteps in the future the target should be.\nmin_index and max_index — Indices in the data array that delimit which timesteps to draw from. This is useful for keeping a segment of the data for validation and another for testing.\nshuffle — Whether to shuffle the samples or draw them in chronological order.\nbatch_size — The number of samples per batch.\nstep — The period, in timesteps, at which you sample data. You’ll set it 6 in order to draw one data point every hour.\n\n\ngenerator <- function(data, lookback, delay, min_index, max_index,\n                      shuffle = FALSE, batch_size = 128, step = 6) {\n  if (is.null(max_index))\n    max_index <- nrow(data) - delay - 1\n  i <- min_index + lookback\n  function() {\n    if (shuffle) {\n      rows <- sample(c((min_index+lookback):max_index), size = batch_size)\n    } else {\n      if (i + batch_size >= max_index)\n        i <<- min_index + lookback\n      rows <- c(i:min(i+batch_size-1, max_index))\n      i <<- i + length(rows)\n    }\n\n    samples <- array(0, dim = c(length(rows),\n                                lookback / step,\n                                dim(data)[[-1]]))\n    targets <- array(0, dim = c(length(rows)))\n                      \n    for (j in 1:length(rows)) {\n      indices <- seq(rows[[j]] - lookback, rows[[j]]-1,\n                     length.out = dim(samples)[[2]])\n      samples[j,,] <- data[indices,]\n      targets[[j]] <- data[rows[[j]] + delay,2]\n    }           \n    list(samples, targets)\n  }\n}\n\nThe i variable contains the state that tracks next window of data to return, so it is updated using superassignment (e.g. i <<- i + length(rows)).\nNow, let’s use the abstract generator function to instantiate three generators: one for training, one for validation, and one for testing. Each will look at different temporal segments of the original data: the training generator looks at the first 200,000 timesteps, the validation generator looks at the following 100,000, and the test generator looks at the remainder.\n\n\nlookback <- 1440\nstep <- 6\ndelay <- 144\nbatch_size <- 128\n\ntrain_gen <- generator(\n  data,\n  lookback = lookback,\n  delay = delay,\n  min_index = 1,\n  max_index = 200000,\n  shuffle = TRUE,\n  step = step, \n  batch_size = batch_size\n)\n\nval_gen = generator(\n  data,\n  lookback = lookback,\n  delay = delay,\n  min_index = 200001,\n  max_index = 300000,\n  step = step,\n  batch_size = batch_size\n)\n\ntest_gen <- generator(\n  data,\n  lookback = lookback,\n  delay = delay,\n  min_index = 300001,\n  max_index = NULL,\n  step = step,\n  batch_size = batch_size\n)\n\n# How many steps to draw from val_gen in order to see the entire validation set\nval_steps <- (300000 - 200001 - lookback) / batch_size\n\n# How many steps to draw from test_gen in order to see the entire test set\ntest_steps <- (nrow(data) - 300001 - lookback) / batch_size\n\nA common-sense, non-machine-learning baseline\nBefore you start using black-box deep-learning models to solve the temperature-prediction problem, let’s try a simple, common-sense approach. It will serve as a sanity check, and it will establish a baseline that you’ll have to beat in order to demonstrate the usefulness of more-advanced machine-learning models. Such common-sense baselines can be useful when you’re approaching a new problem for which there is no known solution (yet). A classic example is that of unbalanced classification tasks, where some classes are much more common than others. If your dataset contains 90% instances of class A and 10% instances of class B, then a common-sense approach to the classification task is to always predict “A” when presented with a new sample. Such a classifier is 90% accurate overall, and any learning-based approach should therefore beat this 90% score in order to demonstrate usefulness. Sometimes, such elementary baselines can prove surprisingly hard to beat.\nIn this case, the temperature time series can safely be assumed to be continuous (the temperatures tomorrow are likely to be close to the temperatures today) as well as periodical with a daily period. Thus a common-sense approach is to always predict that the temperature 24 hours from now will be equal to the temperature right now. Let’s evaluate this approach, using the mean absolute error (MAE) metric:\n\n\nmean(abs(preds - targets))\n\nHere’s the evaluation loop.\n\n\nlibrary(keras)\nevaluate_naive_method <- function() {\n  batch_maes <- c()\n  for (step in 1:val_steps) {\n    c(samples, targets) %<-% val_gen()\n    preds <- samples[,dim(samples)[[2]],2]\n    mae <- mean(abs(preds - targets))\n    batch_maes <- c(batch_maes, mae)\n  }\n  print(mean(batch_maes))\n}\n\nevaluate_naive_method()\n\nThis yields an MAE of 0.29. Because the temperature data has been normalized to be centered on 0 and have a standard deviation of 1, this number isn’t immediately interpretable. It translates to an average absolute error of 0.29 x temperature_std degrees Celsius: 2.57˚C.\n\n\ncelsius_mae <- 0.29 * std[[2]]\n\nThat’s a fairly large average absolute error. Now the game is to use your knowledge of deep learning to do better.\nA basic machine-learning approach\nIn the same way that it’s useful to establish a common-sense baseline before trying machine-learning approaches, it’s useful to try simple, cheap machine-learning models (such as small, densely connected networks) before looking into complicated and computationally expensive models such as RNNs. This is the best way to make sure any further complexity you throw at the problem is legitimate and delivers real benefits.\nThe following listing shows a fully connected model that starts by flattening the data and then runs it through two dense layers. Note the lack of activation function on the last dense layer, which is typical for a regression problem. You use MAE as the loss. Because you evaluate on the exact same data and with the exact same metric you did with the common-sense approach, the results will be directly comparable.\n\n\nlibrary(keras)\n\nmodel <- keras_model_sequential() %>% \n  layer_flatten(input_shape = c(lookback / step, dim(data)[-1])) %>% \n  layer_dense(units = 32, activation = \"relu\") %>% \n  layer_dense(units = 1)\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(),\n  loss = \"mae\"\n)\n\nhistory <- model %>% fit_generator(\n  train_gen,\n  steps_per_epoch = 500,\n  epochs = 20,\n  validation_data = val_gen,\n  validation_steps = val_steps\n)\n\nLet’s display the loss curves for validation and training.\n\nSome of the validation losses are close to the no-learning baseline, but not reliably. This goes to show the merit of having this baseline in the first place: it turns out to be not easy to outperform. Your common sense contains a lot of valuable information that a machine-learning model doesn’t have access to.\nYou may wonder, if a simple, well-performing model exists to go from the data to the targets (the common-sense baseline), why doesn’t the model you’re training find it and improve on it? Because this simple solution isn’t what your training setup is looking for. The space of models in which you’re searching for a solution – that is, your hypothesis space – is the space of all possible two-layer networks with the configuration you defined. These networks are already fairly complicated. When you’re looking for a solution with a space of complicated models, the simple, well-performing baseline may be unlearnable, even if it’s technically part of the hypothesis space. That is a pretty significant limitation of machine learning in general: unless the learning algorithm is hardcoded to look for a specific kind of simple model, parameter learning can sometimes fail to find a simple solution to a simple problem.\nA first recurrent baseline\nThe first fully connected approach didn’t do well, but that doesn’t mean machine learning isn’t applicable to this problem. The previous approach first flattened the time series, which removed the notion of time from the input data. Let’s instead look at the data as what it is: a sequence, where causality and order matter. You’ll try a recurrent-sequence processing model – it should be the perfect fit for such sequence data, precisely because it exploits the temporal ordering of data points, unlike the first approach.\nInstead of the LSTM layer introduced in the previous section, you’ll use the GRU layer, developed by Chung et al. in 2014. Gated recurrent unit (GRU) layers work using the same principle as LSTM, but they’re somewhat streamlined and thus cheaper to run (although they may not have as much representational power as LSTM). This trade-off between computational expensiveness and representational power is seen everywhere in machine learning.\n\n\nmodel <- keras_model_sequential() %>% \n  layer_gru(units = 32, input_shape = list(NULL, dim(data)[[-1]])) %>% \n  layer_dense(units = 1)\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(),\n  loss = \"mae\"\n)\n\nhistory <- model %>% fit_generator(\n  train_gen,\n  steps_per_epoch = 500,\n  epochs = 20,\n  validation_data = val_gen,\n  validation_steps = val_steps\n)\n\nThe results are plotted below. Much better! You can significantly beat the common-sense baseline, demonstrating the value of machine learning as well as the superiority of recurrent networks compared to sequence-flattening dense networks on this type of task.\n\nThe new validation MAE of ~0.265 (before you start significantly overfitting) translates to a mean absolute error of 2.35˚C after denormalization. That’s a solid gain on the initial error of 2.57˚C, but you probably still have a bit of a margin for improvement.\nUsing recurrent dropout to fight overfitting\nIt’s evident from the training and validation curves that the model is overfitting: the training and validation losses start to diverge considerably after a few epochs. You’re already familiar with a classic technique for fighting this phenomenon: dropout, which randomly zeros out input units of a layer in order to break happenstance correlations in the training data that the layer is exposed to. But how to correctly apply dropout in recurrent networks isn’t a trivial question. It has long been known that applying dropout before a recurrent layer hinders learning rather than helping with regularization. In 2015, Yarin Gal, as part of his PhD thesis on Bayesian deep learning, determined the proper way to use dropout with a recurrent network: the same dropout mask (the same pattern of dropped units) should be applied at every timestep, instead of a dropout mask that varies randomly from timestep to timestep. What’s more, in order to regularize the representations formed by the recurrent gates of layers such as layer_gru and layer_lstm, a temporally constant dropout mask should be applied to the inner recurrent activations of the layer (a recurrent dropout mask). Using the same dropout mask at every timestep allows the network to properly propagate its learning error through time; a temporally random dropout mask would disrupt this error signal and be harmful to the learning process.\nYarin Gal did his research using Keras and helped build this mechanism directly into Keras recurrent layers. Every recurrent layer in Keras has two dropout-related arguments: dropout, a float specifying the dropout rate for input units of the layer, and recurrent_dropout, specifying the dropout rate of the recurrent units. Let’s add dropout and recurrent dropout to the layer_gru and see how doing so impacts overfitting. Because networks being regularized with dropout always take longer to fully converge, you’ll train the network for twice as many epochs.\n\n\nmodel <- keras_model_sequential() %>% \n  layer_gru(units = 32, dropout = 0.2, recurrent_dropout = 0.2,\n            input_shape = list(NULL, dim(data)[[-1]])) %>% \n  layer_dense(units = 1)\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(),\n  loss = \"mae\"\n)\n\nhistory <- model %>% fit_generator(\n  train_gen,\n  steps_per_epoch = 500,\n  epochs = 40,\n  validation_data = val_gen,\n  validation_steps = val_steps\n)\n\nThe plot below shows the results. Success! You’re no longer overfitting during the first 20 epochs. But although you have more stable evaluation scores, your best scores aren’t much lower than they were previously.\n\nStacking recurrent layers\nBecause you’re no longer overfitting but seem to have hit a performance bottleneck, you should consider increasing the capacity of the network. Recall the description of the universal machine-learning workflow: it’s generally a good idea to increase the capacity of your network until overfitting becomes the primary obstacle (assuming you’re already taking basic steps to mitigate overfitting, such as using dropout). As long as you aren’t overfitting too badly, you’re likely under capacity.\nIncreasing network capacity is typically done by increasing the number of units in the layers or adding more layers. Recurrent layer stacking is a classic way to build more-powerful recurrent networks: for instance, what currently powers the Google Translate algorithm is a stack of seven large LSTM layers – that’s huge.\nTo stack recurrent layers on top of each other in Keras, all intermediate layers should return their full sequence of outputs (a 3D tensor) rather than their output at the last timestep. This is done by specifying return_sequences = TRUE.\n\n\nmodel <- keras_model_sequential() %>% \n  layer_gru(units = 32, \n            dropout = 0.1, \n            recurrent_dropout = 0.5,\n            return_sequences = TRUE,\n            input_shape = list(NULL, dim(data)[[-1]])) %>% \n  layer_gru(units = 64, activation = \"relu\",\n            dropout = 0.1,\n            recurrent_dropout = 0.5) %>% \n  layer_dense(units = 1)\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(),\n  loss = \"mae\"\n)\n\nhistory <- model %>% fit_generator(\n  train_gen,\n  steps_per_epoch = 500,\n  epochs = 40,\n  validation_data = val_gen,\n  validation_steps = val_steps\n)\n\nThe figure below shows the results. You can see that the added layer does improve the results a bit, though not significantly. You can draw two conclusions:\nBecause you’re still not overfitting too badly, you could safely increase the size of your layers in a quest for validation-loss improvement. This has a non-negligible computational cost, though.\nAdding a layer didn’t help by a significant factor, so you may be seeing diminishing returns from increasing network capacity at this point.\n\nUsing bidirectional RNNs\nThe last technique introduced in this section is called bidirectional RNNs. A bidirectional RNN is a common RNN variant that can offer greater performance than a regular RNN on certain tasks. It’s frequently used in natural-language processing – you could call it the Swiss Army knife of deep learning for natural-language processing.\nRNNs are notably order dependent, or time dependent: they process the timesteps of their input sequences in order, and shuffling or reversing the timesteps can completely change the representations the RNN extracts from the sequence. This is precisely the reason they perform well on problems where order is meaningful, such as the temperature-forecasting problem. A bidirectional RNN exploits the order sensitivity of RNNs: it consists of using two regular RNNs, such as the layer_gru and layer_lstm you’re already familiar with, each of which processes the input sequence in one direction (chronologically and antichronologically), and then merging their representations. By processing a sequence both ways, a bidirectional RNN can catch patterns that may be overlooked by a unidirectional RNN.\nRemarkably, the fact that the RNN layers in this section have processed sequences in chronological order (older timesteps first) may have been an arbitrary decision. At least, it’s a decision we made no attempt to question so far. Could the RNNs have performed well enough if they processed input sequences in antichronological order, for instance (newer timesteps first)? Let’s try this in practice and see what happens. All you need to do is write a variant of the data generator where the input sequences are reverted along the time dimension (replace the last line with list(samples[,ncol(samples):1,], targets)). Training the same one-GRU-layer network that you used in the first experiment in this section, you get the results shown below.\n\nThe reversed-order GRU underperforms even the common-sense baseline, indicating that in this case, chronological processing is important to the success of your approach. This makes perfect sense: the underlying GRU layer will typically be better at remembering the recent past than the distant past, and naturally the more recent weather data points are more predictive than older data points for the problem (that’s what makes the common-sense baseline fairly strong). Thus the chronological version of the layer is bound to outperform the reversed-order version. Importantly, this isn’t true for many other problems, including natural language: intuitively, the importance of a word in understanding a sentence isn’t usually dependent on its position in the sentence. Let’s try the same trick on the LSTM IMDB example from section 6.2.\n\n\nlibrary(keras)\n\n# Number of words to consider as features\nmax_features <- 10000  \n\n# Cuts off texts after this number of words\nmaxlen <- 500\n\nimdb <- dataset_imdb(num_words = max_features)\nc(c(x_train, y_train), c(x_test, y_test)) %<-% imdb\n\n# Reverses sequences\nx_train <- lapply(x_train, rev)\nx_test <- lapply(x_test, rev) \n\n# Pads sequences\nx_train <- pad_sequences(x_train, maxlen = maxlen)  <4>\nx_test <- pad_sequences(x_test, maxlen = maxlen)\n\nmodel <- keras_model_sequential() %>% \n  layer_embedding(input_dim = max_features, output_dim = 128) %>% \n  layer_lstm(units = 32) %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %>% compile(\n  optimizer = \"rmsprop\",\n  loss = \"binary_crossentropy\",\n  metrics = c(\"acc\")\n)\n  \nhistory <- model %>% fit(\n  x_train, y_train,\n  epochs = 10,\n  batch_size = 128,\n  validation_split = 0.2\n)\n\nYou get performance nearly identical to that of the chronological-order LSTM. Remarkably, on such a text dataset, reversed-order processing works just as well as chronological processing, confirming the hypothesis that, although word order does matter in understanding language, which order you use isn’t crucial. Importantly, an RNN trained on reversed sequences will learn different representations than one trained on the original sequences, much as you would have different mental models if time flowed backward in the real world – if you lived a life where you died on your first day and were born on your last day. In machine learning, representations that are different yet useful are always worth exploiting, and the more they differ, the better: they offer a new angle from which to look at your data, capturing aspects of the data that were missed by other approaches, and thus they can help boost performance on a task. This is the intuition behind ensembling, a concept we’ll explore in chapter 7.\nA bidirectional RNN exploits this idea to improve on the performance of chronological-order RNNs. It looks at its input sequence both ways, obtaining potentially richer representations and capturing patterns that may have been missed by the chronological-order version alone.\n\nTo instantiate a bidirectional RNN in Keras, you use the bidirectional() function, which takes a recurrent layer instance as an argument. The bidirectional() function creates a second, separate instance of this recurrent layer and uses one instance for processing the input sequences in chronological order and the other instance for processing the input sequences in reversed order. Let’s try it on the IMDB sentiment-analysis task.\n\n\nmodel <- keras_model_sequential() %>% \n  layer_embedding(input_dim = max_features, output_dim = 32) %>% \n  bidirectional(\n    layer_lstm(units = 32)\n  ) %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %>% compile(\n  optimizer = \"rmsprop\",\n  loss = \"binary_crossentropy\",\n  metrics = c(\"acc\")\n)\n\nhistory <- model %>% fit(\n  x_train, y_train,\n  epochs = 10,\n  batch_size = 128,\n  validation_split = 0.2\n)\n\nIt performs slightly better than the regular LSTM you tried in the previous section, achieving over 89% validation accuracy. It also seems to overfit more quickly, which is unsurprising because a bidirectional layer has twice as many parameters as a chronological LSTM. With some regularization, the bidirectional approach would likely be a strong performer on this task.\nNow let’s try the same approach on the temperature prediction task.\n\n\nmodel <- keras_model_sequential() %>% \n  bidirectional(\n    layer_gru(units = 32), input_shape = list(NULL, dim(data)[[-1]])\n  ) %>% \n  layer_dense(units = 1)\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(),\n  loss = \"mae\"\n)\n\nhistory <- model %>% fit_generator(\n  train_gen,\n  steps_per_epoch = 500,\n  epochs = 40,\n  validation_data = val_gen,\n  validation_steps = val_steps\n)\n\nThis performs about as well as the regular layer_gru. It’s easy to understand why: all the predictive capacity must come from the chronological half of the network, because the antichronological half is known to be severely underperforming on this task (again, because the recent past matters much more than the distant past in this case).\nGoing even further\nThere are many other things you could try, in order to improve performance on the temperature-forecasting problem:\nAdjust the number of units in each recurrent layer in the stacked setup. The current choices are largely arbitrary and thus probably suboptimal.\nAdjust the learning rate used by the RMSprop optimizer.\nTry using layer_lstm instead of layer_gru.\nTry using a bigger densely connected regressor on top of the recurrent layers: that is, a bigger dense layer or even a stack of dense layers.\nDon’t forget to eventually run the best-performing models (in terms of validation MAE) on the test set! Otherwise, you’ll develop architectures that are overfitting to the validation set.\nAs always, deep learning is more an art than a science. We can provide guidelines that suggest what is likely to work or not work on a given problem, but, ultimately, every problem is unique; you’ll have to evaluate different strategies empirically. There is currently no theory that will tell you in advance precisely what you should do to optimally solve a problem. You must iterate.\nWrapping up\nHere’s what you should take away from this section:\nAs you first learned in chapter 4, when approaching a new problem, it’s good to first establish common-sense baselines for your metric of choice. If you don’t have a baseline to beat, you can’t tell whether you’re making real progress.\nTry simple models before expensive ones, to justify the additional expense. Sometimes a simple model will turn out to be your best option.\nWhen you have data where temporal ordering matters, recurrent networks are a great fit and easily outperform models that first flatten the temporal data.\nTo use dropout with recurrent networks, you should use a time-constant dropout mask and recurrent dropout mask. These are built into Keras recurrent layers, so all you have to do is use the dropout and recurrent_dropout arguments of recurrent layers.\nStacked RNNs provide more representational power than a single RNN layer. They’re also much more expensive and thus not always worth it. Although they offer clear gains on complex problems (such as machine translation), they may not always be relevant to smaller, simpler problems.\nBidirectional RNNs, which look at a sequence both ways, are useful on natural-language processing problems. But they aren’t strong performers on sequence data where the recent past is much more informative than the beginning of the sequence.\n\nNOTE: Markets and machine learning Some readers are bound to want to take the techniques we’ve introduced here and try them on the problem of forecasting the future price of securities on the stock market (or currency exchange rates, and so on). Markets have very different statistical characteristics than natural phenomena such as weather patterns. Trying to use machine learning to beat markets, when you only have access to publicly available data, is a difficult endeavor, and you’re likely to waste your time and resources with nothing to show for it.\nAlways remember that when it comes to markets, past performance is not a good predictor of future returns – looking in the rear-view mirror is a bad way to drive. Machine learning, on the other hand, is applicable to datasets where the past is a good predictor of the future.\n\n\n\n",
    "preview": "posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks/images/jena_temp-r.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 6000,
    "preview_height": 4000
  },
  {
    "path": "posts/2017-12-14-image-classification-on-small-datasets/",
    "title": "Image Classification on Small Datasets with Keras",
    "description": "Having to train an image-classification model using very little data is a common situation, in this article we review three techniques for tackling this problem including feature extraction and fine tuning from a pretrained network.",
    "author": [
      {
        "name": "François Chollet",
        "url": "https://github.com/fchollet"
      },
      {
        "name": "J.J. Allaire",
        "url": "https://github.com/jjallaire"
      }
    ],
    "date": "2017-12-14",
    "categories": [
      "TensorFlow/Keras",
      "Image Recognition & Image Processing"
    ],
    "contents": "\nTraining a convnet with a small dataset\nHaving to train an image-classification model using very little data is a common situation, which you’ll likely encounter in practice if you ever do computer vision in a professional context. A “few” samples can mean anywhere from a few hundred to a few tens of thousands of images. As a practical example, we’ll focus on classifying images as dogs or cats, in a dataset containing 4,000 pictures of cats and dogs (2,000 cats, 2,000 dogs). We’ll use 2,000 pictures for training – 1,000 for validation, and 1,000 for testing.\nIn Chapter 5 of the Deep Learning with R book we review three techniques for tackling this problem. The first of these is training a small model from scratch on what little data you have (which achieves an accuracy of 82%). Subsequently we use feature extraction with a pretrained network (resulting in an accuracy of 90%) and fine-tuning a pretrained network (with a final accuracy of 97%). In this post we’ll cover only the second and third techniques.\nThe relevance of deep learning for small-data problems\nYou’ll sometimes hear that deep learning only works when lots of data is available. This is valid in part: one fundamental characteristic of deep learning is that it can find interesting features in the training data on its own, without any need for manual feature engineering, and this can only be achieved when lots of training examples are available. This is especially true for problems where the input samples are very high-dimensional, like images.\nBut what constitutes lots of samples is relative – relative to the size and depth of the network you’re trying to train, for starters. It isn’t possible to train a convnet to solve a complex problem with just a few tens of samples, but a few hundred can potentially suffice if the model is small and well regularized and the task is simple. Because convnets learn local, translation-invariant features, they’re highly data efficient on perceptual problems. Training a convnet from scratch on a very small image dataset will still yield reasonable results despite a relative lack of data, without the need for any custom feature engineering. You’ll see this in action in this section.\nWhat’s more, deep-learning models are by nature highly repurposable: you can take, say, an image-classification or speech-to-text model trained on a large-scale dataset and reuse it on a significantly different problem with only minor changes. Specifically, in the case of computer vision, many pretrained models (usually trained on the ImageNet dataset) are now publicly available for download and can be used to bootstrap powerful vision models out of very little data. That’s what you’ll do in the next section. Let’s start by getting your hands on the data.\nDownloading the data\nThe Dogs vs. Cats dataset that you’ll use isn’t packaged with Keras. It was made available by Kaggle as part of a computer-vision competition in late 2013, back when convnets weren’t mainstream. You can download the original dataset from https://www.kaggle.com/c/dogs-vs-cats/data (you’ll need to create a Kaggle account if you don’t already have one – don’t worry, the process is painless).\nThe pictures are medium-resolution color JPEGs. Here are some examples:\n\nUnsurprisingly, the dogs-versus-cats Kaggle competition in 2013 was won by entrants who used convnets. The best entries achieved up to 95% accuracy. Below you’ll end up with a 97% accuracy, even though you’ll train your models on less than 10% of the data that was available to the competitors.\nThis dataset contains 25,000 images of dogs and cats (12,500 from each class) and is 543 MB (compressed). After downloading and uncompressing it, you’ll create a new dataset containing three subsets: a training set with 1,000 samples of each class, a validation set with 500 samples of each class, and a test set with 500 samples of each class.\nFollowing is the code to do this:\n\n\noriginal_dataset_dir <- \"~/Downloads/kaggle_original_data\"\n\nbase_dir <- \"~/Downloads/cats_and_dogs_small\"\ndir.create(base_dir)\n\ntrain_dir <- file.path(base_dir, \"train\")\ndir.create(train_dir)\nvalidation_dir <- file.path(base_dir, \"validation\")\ndir.create(validation_dir)\ntest_dir <- file.path(base_dir, \"test\")\ndir.create(test_dir)\n\ntrain_cats_dir <- file.path(train_dir, \"cats\")\ndir.create(train_cats_dir)\n\ntrain_dogs_dir <- file.path(train_dir, \"dogs\")\ndir.create(train_dogs_dir)\n\nvalidation_cats_dir <- file.path(validation_dir, \"cats\")\ndir.create(validation_cats_dir)\n\nvalidation_dogs_dir <- file.path(validation_dir, \"dogs\")\ndir.create(validation_dogs_dir)\n\ntest_cats_dir <- file.path(test_dir, \"cats\")\ndir.create(test_cats_dir)\n\ntest_dogs_dir <- file.path(test_dir, \"dogs\")\ndir.create(test_dogs_dir)\n\nfnames <- paste0(\"cat.\", 1:1000, \".jpg\")\nfile.copy(file.path(original_dataset_dir, fnames), \n          file.path(train_cats_dir)) \n\nfnames <- paste0(\"cat.\", 1001:1500, \".jpg\")\nfile.copy(file.path(original_dataset_dir, fnames), \n          file.path(validation_cats_dir))\n\nfnames <- paste0(\"cat.\", 1501:2000, \".jpg\")\nfile.copy(file.path(original_dataset_dir, fnames),\n          file.path(test_cats_dir))\n\nfnames <- paste0(\"dog.\", 1:1000, \".jpg\")\nfile.copy(file.path(original_dataset_dir, fnames),\n          file.path(train_dogs_dir))\n\nfnames <- paste0(\"dog.\", 1001:1500, \".jpg\")\nfile.copy(file.path(original_dataset_dir, fnames),\n          file.path(validation_dogs_dir)) \n\nfnames <- paste0(\"dog.\", 1501:2000, \".jpg\")\nfile.copy(file.path(original_dataset_dir, fnames),\n          file.path(test_dogs_dir))\n\nUsing a pretrained convnet\nA common and highly effective approach to deep learning on small image datasets is to use a pretrained network. A pretrained network is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task. If this original dataset is large enough and general enough, then the spatial hierarchy of features learned by the pretrained network can effectively act as a generic model of the visual world, and hence its features can prove useful for many different computer-vision problems, even though these new problems may involve completely different classes than those of the original task. For instance, you might train a network on ImageNet (where classes are mostly animals and everyday objects) and then repurpose this trained network for something as remote as identifying furniture items in images. Such portability of learned features across different problems is a key advantage of deep learning compared to many older, shallow-learning approaches, and it makes deep learning very effective for small-data problems.\nIn this case, let’s consider a large convnet trained on the ImageNet dataset (1.4 million labeled images and 1,000 different classes). ImageNet contains many animal classes, including different species of cats and dogs, and you can thus expect to perform well on the dogs-versus-cats classification problem.\nYou’ll use the VGG16 architecture, developed by Karen Simonyan and Andrew Zisserman in 2014; it’s a simple and widely used convnet architecture for ImageNet. Although it’s an older model, far from the current state of the art and somewhat heavier than many other recent models, I chose it because its architecture is similar to what you’re already familiar with and is easy to understand without introducing any new concepts. This may be your first encounter with one of these cutesy model names – VGG, ResNet, Inception, Inception-ResNet, Xception, and so on; you’ll get used to them, because they will come up frequently if you keep doing deep learning for computer vision.\nThere are two ways to use a pretrained network: feature extraction and fine-tuning. We’ll cover both of them. Let’s start with feature extraction.\nFeature extraction\nFeature extraction consists of using the representations learned by a previous network to extract interesting features from new samples. These features are then run through a new classifier, which is trained from scratch.\nAs you saw previously, convnets used for image classification comprise two parts: they start with a series of pooling and convolution layers, and they end with a densely connected classifier. The first part is called the convolutional base of the model. In the case of convnets, feature extraction consists of taking the convolutional base of a previously trained network, running the new data through it, and training a new classifier on top of the output.\n\nWhy only reuse the convolutional base? Could you reuse the densely connected classifier as well? In general, doing so should be avoided. The reason is that the representations learned by the convolutional base are likely to be more generic and therefore more reusable: the feature maps of a convnet are presence maps of generic concepts over a picture, which is likely to be useful regardless of the computer-vision problem at hand. But the representations learned by the classifier will necessarily be specific to the set of classes on which the model was trained – they will only contain information about the presence probability of this or that class in the entire picture. Additionally, representations found in densely connected layers no longer contain any information about where objects are located in the input image: these layers get rid of the notion of space, whereas the object location is still described by convolutional feature maps. For problems where object location matters, densely connected features are largely useless.\nNote that the level of generality (and therefore reusability) of the representations extracted by specific convolution layers depends on the depth of the layer in the model. Layers that come earlier in the model extract local, highly generic feature maps (such as visual edges, colors, and textures), whereas layers that are higher up extract more-abstract concepts (such as “cat ear” or “dog eye”). So if your new dataset differs a lot from the dataset on which the original model was trained, you may be better off using only the first few layers of the model to do feature extraction, rather than using the entire convolutional base.\nIn this case, because the ImageNet class set contains multiple dog and cat classes, it’s likely to be beneficial to reuse the information contained in the densely connected layers of the original model. But we’ll choose not to, in order to cover the more general case where the class set of the new problem doesn’t overlap the class set of the original model.\nLet’s put this in practice by using the convolutional base of the VGG16 network, trained on ImageNet, to extract interesting features from cat and dog images, and then train a dogs-versus-cats classifier on top of these features.\nThe VGG16 model, among others, comes prepackaged with Keras. Here’s the list of image-classification models (all pretrained on the ImageNet dataset) that are available as part of Keras:\nXception\nInception V3\nResNet50\nVGG16\nVGG19\nMobileNet\nLet’s instantiate the VGG16 model.\n\n\nlibrary(keras)\n\nconv_base <- application_vgg16(\n  weights = \"imagenet\",\n  include_top = FALSE,\n  input_shape = c(150, 150, 3)\n)\n\nYou pass three arguments to the function:\nweights specifies the weight checkpoint from which to initialize the model.\ninclude_top refers to including (or not) the densely connected classifier on top of the network. By default, this densely connected classifier corresponds to the 1,000 classes from ImageNet. Because you intend to use your own densely connected classifier (with only two classes: cat and dog), you don’t need to include it.\ninput_shape is the shape of the image tensors that you’ll feed to the network. This argument is purely optional: if you don’t pass it, the network will be able to process inputs of any size.\nHere’s the detail of the architecture of the VGG16 convolutional base. It’s similar to the simple convnets you’re already familiar with:\n\n\nsummary(conv_base)\n\n\nLayer (type)                     Output Shape          Param #  \n================================================================\ninput_1 (InputLayer)             (None, 150, 150, 3)   0       \n________________________________________________________________\nblock1_conv1 (Convolution2D)     (None, 150, 150, 64)  1792     \n________________________________________________________________\nblock1_conv2 (Convolution2D)     (None, 150, 150, 64)  36928    \n________________________________________________________________\nblock1_pool (MaxPooling2D)       (None, 75, 75, 64)    0        \n________________________________________________________________\nblock2_conv1 (Convolution2D)     (None, 75, 75, 128)   73856    \n________________________________________________________________\nblock2_conv2 (Convolution2D)     (None, 75, 75, 128)   147584   \n________________________________________________________________\nblock2_pool (MaxPooling2D)       (None, 37, 37, 128)   0        \n________________________________________________________________\nblock3_conv1 (Convolution2D)     (None, 37, 37, 256)   295168   \n________________________________________________________________\nblock3_conv2 (Convolution2D)     (None, 37, 37, 256)   590080   \n________________________________________________________________\nblock3_conv3 (Convolution2D)     (None, 37, 37, 256)   590080   \n________________________________________________________________\nblock3_pool (MaxPooling2D)       (None, 18, 18, 256)   0        \n________________________________________________________________\nblock4_conv1 (Convolution2D)     (None, 18, 18, 512)   1180160  \n________________________________________________________________\nblock4_conv2 (Convolution2D)     (None, 18, 18, 512)   2359808  \n________________________________________________________________\nblock4_conv3 (Convolution2D)     (None, 18, 18, 512)   2359808  \n________________________________________________________________\nblock4_pool (MaxPooling2D)       (None, 9, 9, 512)     0        \n________________________________________________________________\nblock5_conv1 (Convolution2D)     (None, 9, 9, 512)     2359808  \n________________________________________________________________\nblock5_conv2 (Convolution2D)     (None, 9, 9, 512)     2359808  \n________________________________________________________________\nblock5_conv3 (Convolution2D)     (None, 9, 9, 512)     2359808  \n________________________________________________________________\nblock5_pool (MaxPooling2D)       (None, 4, 4, 512)     0        \n================================================================\nTotal params: 14,714,688\nTrainable params: 14,714,688\nNon-trainable params: 0\nThe final feature map has shape (4, 4, 512). That’s the feature on top of which you’ll stick a densely connected classifier.\nAt this point, there are two ways you could proceed:\nRunning the convolutional base over your dataset, recording its output to an array on disk, and then using this data as input to a standalone, densely connected classifier similar to those you saw in part 1 of this book. This solution is fast and cheap to run, because it only requires running the convolutional base once for every input image, and the convolutional base is by far the most expensive part of the pipeline. But for the same reason, this technique won’t allow you to use data augmentation.\nExtending the model you have (conv_base) by adding dense layers on top, and running the whole thing end to end on the input data. This will allow you to use data augmentation, because every input image goes through the convolutional base every time it’s seen by the model. But for the same reason, this technique is far more expensive than the first.\nIn this post we’ll cover the second technique in detail (in the book we cover both). Note that this technique is so expensive that you should only attempt it if you have access to a GPU – it’s absolutely intractable on a CPU.\nFeature extraction with data augmentation\nBecause models behave just like layers, you can add a model (like conv_base) to a sequential model just like you would add a layer.\n\n\nmodel <- keras_model_sequential() %>% \n  conv_base %>% \n  layer_flatten() %>% \n  layer_dense(units = 256, activation = \"relu\") %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nThis is what the model looks like now:\n\n\nsummary(model)\n\n\nLayer (type)                     Output Shape          Param #  \n================================================================\nvgg16 (Model)                    (None, 4, 4, 512)     14714688                                     \n________________________________________________________________\nflatten_1 (Flatten)              (None, 8192)          0        \n________________________________________________________________\ndense_1 (Dense)                  (None, 256)           2097408  \n________________________________________________________________\ndense_2 (Dense)                  (None, 1)             257      \n================================================================\nTotal params: 16,812,353\nTrainable params: 16,812,353\nNon-trainable params: 0\nAs you can see, the convolutional base of VGG16 has 14,714,688 parameters, which is very large. The classifier you’re adding on top has 2 million parameters.\nBefore you compile and train the model, it’s very important to freeze the convolutional base. Freezing a layer or set of layers means preventing their weights from being updated during training. If you don’t do this, then the representations that were previously learned by the convolutional base will be modified during training. Because the dense layers on top are randomly initialized, very large weight updates would be propagated through the network, effectively destroying the representations previously learned.\nIn Keras, you freeze a network using the freeze_weights() function:\n\n\nlength(model$trainable_weights)\n\n\n[1] 30\n\n\nfreeze_weights(conv_base)\nlength(model$trainable_weights)\n\n\n[1] 4\nWith this setup, only the weights from the two dense layers that you added will be trained. That’s a total of four weight tensors: two per layer (the main weight matrix and the bias vector). Note that in order for these changes to take effect, you must first compile the model. If you ever modify weight trainability after compilation, you should then recompile the model, or these changes will be ignored.\nUsing data augmentation\nOverfitting is caused by having too few samples to learn from, rendering you unable to train a model that can generalize to new data. Given infinite data, your model would be exposed to every possible aspect of the data distribution at hand: you would never overfit. Data augmentation takes the approach of generating more training data from existing training samples, by augmenting the samples via a number of random transformations that yield believable-looking images. The goal is that at training time, your model will never see the exact same picture twice. This helps expose the model to more aspects of the data and generalize better.\nIn Keras, this can be done by configuring a number of random transformations to be performed on the images read by an image_data_generator(). For example:\n\n\ntrain_datagen = image_data_generator(\n  rescale = 1/255,\n  rotation_range = 40,\n  width_shift_range = 0.2,\n  height_shift_range = 0.2,\n  shear_range = 0.2,\n  zoom_range = 0.2,\n  horizontal_flip = TRUE,\n  fill_mode = \"nearest\"\n)\n\nThese are just a few of the options available (for more, see the Keras documentation). Let’s quickly go over this code:\nrotation_range is a value in degrees (0–180), a range within which to randomly rotate pictures.\nwidth_shift and height_shift are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally.\nshear_range is for randomly applying shearing transformations.\nzoom_range is for randomly zooming inside pictures.\nhorizontal_flip is for randomly flipping half the images horizontally – relevant when there are no assumptions of horizontal asymmetry (for example, real-world pictures).\nfill_mode is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.\nNow we can train our model using the image data generator:\n\n\n# Note that the validation data shouldn't be augmented!\ntest_datagen <- image_data_generator(rescale = 1/255)  \n\ntrain_generator <- flow_images_from_directory(\n  train_dir,                  # Target directory  \n  train_datagen,              # Data generator\n  target_size = c(150, 150),  # Resizes all images to 150 × 150\n  batch_size = 20,\n  class_mode = \"binary\"       # binary_crossentropy loss for binary labels\n)\n\nvalidation_generator <- flow_images_from_directory(\n  validation_dir,\n  test_datagen,\n  target_size = c(150, 150),\n  batch_size = 20,\n  class_mode = \"binary\"\n)\n\nmodel %>% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = optimizer_rmsprop(lr = 2e-5),\n  metrics = c(\"accuracy\")\n)\n\nhistory <- model %>% fit_generator(\n  train_generator,\n  steps_per_epoch = 100,\n  epochs = 30,\n  validation_data = validation_generator,\n  validation_steps = 50\n)\n\nLet’s plot the results. As you can see, you reach a validation accuracy of about 90%.\n\nFine-tuning\nAnother widely used technique for model reuse, complementary to feature extraction, is fine-tuning Fine-tuning consists of unfreezing a few of the top layers of a frozen model base used for feature extraction, and jointly training both the newly added part of the model (in this case, the fully connected classifier) and these top layers. This is called fine-tuning because it slightly adjusts the more abstract representations of the model being reused, in order to make them more relevant for the problem at hand.\n\nI stated earlier that it’s necessary to freeze the convolution base of VGG16 in order to be able to train a randomly initialized classifier on top. For the same reason, it’s only possible to fine-tune the top layers of the convolutional base once the classifier on top has already been trained. If the classifier isn’t already trained, then the error signal propagating through the network during training will be too large, and the representations previously learned by the layers being fine-tuned will be destroyed. Thus the steps for fine-tuning a network are as follows:\nAdd your custom network on top of an already-trained base network.\nFreeze the base network.\nTrain the part you added.\nUnfreeze some layers in the base network.\nJointly train both these layers and the part you added.\nYou already completed the first three steps when doing feature extraction. Let’s proceed with step 4: you’ll unfreeze your conv_base and then freeze individual layers inside it.\nAs a reminder, this is what your convolutional base looks like:\n\n\nsummary(conv_base)\n\n\nLayer (type)                     Output Shape          Param #  \n================================================================\ninput_1 (InputLayer)             (None, 150, 150, 3)   0        \n________________________________________________________________\nblock1_conv1 (Convolution2D)     (None, 150, 150, 64)  1792     \n________________________________________________________________\nblock1_conv2 (Convolution2D)     (None, 150, 150, 64)  36928    \n________________________________________________________________\nblock1_pool (MaxPooling2D)       (None, 75, 75, 64)    0        \n________________________________________________________________\nblock2_conv1 (Convolution2D)     (None, 75, 75, 128)   73856    \n________________________________________________________________\nblock2_conv2 (Convolution2D)     (None, 75, 75, 128)   147584   \n________________________________________________________________\nblock2_pool (MaxPooling2D)       (None, 37, 37, 128)   0        \n________________________________________________________________\nblock3_conv1 (Convolution2D)     (None, 37, 37, 256)   295168   \n________________________________________________________________\nblock3_conv2 (Convolution2D)     (None, 37, 37, 256)   590080   \n________________________________________________________________\nblock3_conv3 (Convolution2D)     (None, 37, 37, 256)   590080   \n________________________________________________________________\nblock3_pool (MaxPooling2D)       (None, 18, 18, 256)   0        \n________________________________________________________________\nblock4_conv1 (Convolution2D)     (None, 18, 18, 512)   1180160  \n________________________________________________________________\nblock4_conv2 (Convolution2D)     (None, 18, 18, 512)   2359808  \n________________________________________________________________\nblock4_conv3 (Convolution2D)     (None, 18, 18, 512)   2359808  \n________________________________________________________________\nblock4_pool (MaxPooling2D)       (None, 9, 9, 512)     0        \n________________________________________________________________\nblock5_conv1 (Convolution2D)     (None, 9, 9, 512)     2359808  \n________________________________________________________________\nblock5_conv2 (Convolution2D)     (None, 9, 9, 512)     2359808  \n________________________________________________________________\nblock5_conv3 (Convolution2D)     (None, 9, 9, 512)     2359808  \n________________________________________________________________\nblock5_pool (MaxPooling2D)       (None, 4, 4, 512)     0        \n================================================================\nTotal params: 14714688\nYou’ll fine-tune all of the layers from block3_conv1 and on. Why not fine-tune the entire convolutional base? You could. But you need to consider the following:\nEarlier layers in the convolutional base encode more-generic, reusable features, whereas layers higher up encode more-specialized features. It’s more useful to fine-tune the more specialized features, because these are the ones that need to be repurposed on your new problem. There would be fast-decreasing returns in fine-tuning lower layers.\nThe more parameters you’re training, the more you’re at risk of overfitting. The convolutional base has 15 million parameters, so it would be risky to attempt to train it on your small dataset.\nThus, in this situation, it’s a good strategy to fine-tune only some of the layers in the convolutional base. Let’s set this up, starting from where you left off in the previous example.\n\n\nunfreeze_weights(conv_base, from = \"block3_conv1\")\n\nNow you can begin fine-tuning the network. You’ll do this with the RMSProp optimizer, using a very low learning rate. The reason for using a low learning rate is that you want to limit the magnitude of the modifications you make to the representations of the three layers you’re fine-tuning. Updates that are too large may harm these representations.\n\n\nmodel %>% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = optimizer_rmsprop(lr = 1e-5),\n  metrics = c(\"accuracy\")\n)\n\nhistory <- model %>% fit_generator(\n  train_generator,\n  steps_per_epoch = 100,\n  epochs = 100,\n  validation_data = validation_generator,\n  validation_steps = 50\n)\n\nLet’s plot our results:\n\nYou’re seeing a nice 6% absolute improvement in accuracy, from about 90% to above 96%.\nNote that the loss curve doesn’t show any real improvement (in fact, it’s deteriorating). You may wonder, how could accuracy stay stable or improve if the loss isn’t decreasing? The answer is simple: what you display is an average of pointwise loss values; but what matters for accuracy is the distribution of the loss values, not their average, because accuracy is the result of a binary thresholding of the class probability predicted by the model. The model may still be improving even if this isn’t reflected in the average loss.\nYou can now finally evaluate this model on the test data:\n\n\ntest_generator <- flow_images_from_directory(\n  test_dir,\n  test_datagen,\n  target_size = c(150, 150),\n  batch_size = 20,\n  class_mode = \"binary\"\n)\n\n\n\nmodel %>% evaluate_generator(test_generator, steps = 50)\n\n\n$loss\n[1] 0.2158171\n\n$acc\n[1] 0.965\nHere you get a test accuracy of 96.5%. In the original Kaggle competition around this dataset, this would have been one of the top results. But using modern deep-learning techniques, you managed to reach this result using only a small fraction of the training data available (about 10%). There is a huge difference between being able to train on 20,000 samples compared to 2,000 samples!\nTake-aways: using convnets with small datasets\nHere’s what you should take away from the exercises in the past two sections:\nConvnets are the best type of machine-learning models for computer-vision tasks. It’s possible to train one from scratch even on a very small dataset, with decent results.\nOn a small dataset, overfitting will be the main issue. Data augmentation is a powerful way to fight overfitting when you’re working with image data.\nIt’s easy to reuse an existing convnet on a new dataset via feature extraction. This is a valuable technique for working with small image datasets.\nAs a complement to feature extraction, you can use fine-tuning, which adapts to a new problem some of the representations previously learned by an existing model. This pushes performance a bit further.\nNow you have a solid set of tools for dealing with image-classification problems – in particular with small datasets.\n\n\n",
    "preview": "posts/2017-12-14-image-classification-on-small-datasets/images/swapping_fc_classifier.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 678,
    "preview_height": 453
  },
  {
    "path": "posts/2017-12-07-text-classification-with-keras/",
    "title": "Deep Learning for Text Classification with Keras",
    "description": "Two-class classification, or binary classification, may be the most widely applied kind of machine-learning problem. In this excerpt from the book Deep Learning with R, you'll learn to classify movie reviews as positive or negative, based on the text content of the reviews.",
    "author": [
      {
        "name": "François Chollet",
        "url": "https://github.com/fchollet"
      },
      {
        "name": "J.J. Allaire",
        "url": "https://github.com/jjallaire"
      }
    ],
    "date": "2017-12-07",
    "categories": [
      "TensorFlow/Keras",
      "Natural Language Processing"
    ],
    "contents": "\nThe IMDB dataset\nIn this example, we’ll work with the IMDB dataset: a set of 50,000 highly polarized reviews from the Internet Movie Database. They’re split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting of 50% negative and 50% positive reviews.\nWhy use separate training and test sets? Because you should never test a machine-learning model on the same data that you used to train it! Just because a model performs well on its training data doesn’t mean it will perform well on data it has never seen; and what you care about is your model’s performance on new data (because you already know the labels of your training data – obviously you don’t need your model to predict those). For instance, it’s possible that your model could end up merely memorizing a mapping between your training samples and their targets, which would be useless for the task of predicting targets for data the model has never seen before. We’ll go over this point in much more detail in the next chapter.\nJust like the MNIST dataset, the IMDB dataset comes packaged with Keras. It has already been preprocessed: the reviews (sequences of words) have been turned into sequences of integers, where each integer stands for a specific word in a dictionary.\nThe following code will load the dataset (when you run it the first time, about 80 MB of data will be downloaded to your machine).\n\n\nlibrary(keras)\nimdb <- dataset_imdb(num_words = 10000)\ntrain_data <- imdb$train$x\ntrain_labels <- imdb$train$y\ntest_data <- imdb$test$x\ntest_labels <- imdb$test$y\n\nThe argument num_words = 10000 means you’ll only keep the top 10,000 most frequently occurring words in the training data. Rare words will be discarded. This allows you to work with vector data of manageable size.\nThe variables train_data and test_data are lists of reviews; each review is a list of word indices (encoding a sequence of words). train_labels and test_labels are lists of 0s and 1s, where 0 stands for negative and 1 stands for positive:\n\n\nstr(train_data[[1]])\n\n\nint [1:218] 1 14 22 16 43 530 973 1622 1385 65 ...\n\n\ntrain_labels[[1]]\n\n\n[1] 1\nBecause you’re restricting yourself to the top 10,000 most frequent words, no word index will exceed 10,000:\n\n\nmax(sapply(train_data, max))\n\n\n[1] 9999\nFor kicks, here’s how you can quickly decode one of these reviews back to English words:\n\n\n# Named list mapping words to an integer index.\nword_index <- dataset_imdb_word_index()  \nreverse_word_index <- names(word_index)\nnames(reverse_word_index) <- word_index\n\n# Decodes the review. Note that the indices are offset by 3 because 0, 1, and \n# 2 are reserved indices for \"padding,\" \"start of sequence,\" and \"unknown.\"\ndecoded_review <- sapply(train_data[[1]], function(index) {\n  word <- if (index >= 3) reverse_word_index[[as.character(index - 3)]]\n  if (!is.null(word)) word else \"?\"\n})\ncat(decoded_review)\n\n\n? this film was just brilliant casting location scenery story direction\neveryone's really suited the part they played and you could just imagine\nbeing there robert ? is an amazing actor and now the same being director\n? father came from the same scottish island as myself so i loved the fact\nthere was a real connection with this film the witty remarks throughout\nthe film were great it was just brilliant so much that i bought the film\nas soon as it was released for ? and would recommend it to everyone to \nwatch and the fly fishing was amazing really cried at the end it was so\nsad and you know what they say if you cry at a film it must have been \ngood and this definitely was also ? to the two little boy's that played'\nthe ? of norman and paul they were just brilliant children are often left\nout of the ? list i think because the stars that play them all grown up\nare such a big profile for the whole film but these children are amazing\nand should be praised for what they have done don't you think the whole\nstory was so lovely because it was true and was someone's life after all\nthat was shared with us all\nPreparing the data\nYou can’t feed lists of integers into a neural network. You have to turn your lists into tensors. There are two ways to do that:\nPad your lists so that they all have the same length, turn them into an integer tensor of shape (samples, word_indices), and then use as the first layer in your network a layer capable of handling such integer tensors (the “embedding” layer, which we’ll cover in detail later in the book).\nOne-hot encode your lists to turn them into vectors of 0s and 1s. This would mean, for instance, turning the sequence [3, 5] into a 10,000-dimensional vector that would be all 0s except for indices 3 and 5, which would be 1s. Then you could use as the first layer in your network a dense layer, capable of handling floating-point vector data.\nLet’s go with the latter solution to vectorize the data, which you’ll do manually for maximum clarity.\n\n\nvectorize_sequences <- function(sequences, dimension = 10000) {\n  # Creates an all-zero matrix of shape (length(sequences), dimension)\n  results <- matrix(0, nrow = length(sequences), ncol = dimension) \n  for (i in 1:length(sequences))\n    # Sets specific indices of results[i] to 1s\n    results[i, sequences[[i]]] <- 1 \n  results\n}\n\nx_train <- vectorize_sequences(train_data)\nx_test <- vectorize_sequences(test_data)\n\nHere’s what the samples look like now:\n\n\nstr(x_train[1,])\n\n\n num [1:10000] 1 1 0 1 1 1 1 1 1 0 ...\nYou should also convert your labels from integer to numeric, which is straightforward:\n\n\ny_train <- as.numeric(train_labels)\ny_test <- as.numeric(test_labels)\n\nNow the data is ready to be fed into a neural network.\nBuilding your network\nThe input data is vectors, and the labels are scalars (1s and 0s): this is the easiest setup you’ll ever encounter. A type of network that performs well on such a problem is a simple stack of fully connected (“dense”) layers with relu activations: layer_dense(units = 16, activation = \"relu\").\nThe argument being passed to each dense layer (16) is the number of hidden units of the layer. A hidden unit is a dimension in the representation space of the layer. You may remember from chapter 2 that each such dense layer with a relu activation implements the following chain of tensor operations:\noutput = relu(dot(W, input) + b)\nHaving 16 hidden units means the weight matrix W will have shape (input_dimension, 16): the dot product with W will project the input data onto a 16-dimensional representation space (and then you’ll add the bias vector b and apply the relu operation). You can intuitively understand the dimensionality of your representation space as “how much freedom you’re allowing the network to have when learning internal representations.” Having more hidden units (a higher-dimensional representation space) allows your network to learn more-complex representations, but it makes the network more computationally expensive and may lead to learning unwanted patterns (patterns that will improve performance on the training data but not on the test data).\nThere are two key architecture decisions to be made about such stack of dense layers:\nHow many layers to use\nHow many hidden units to choose for each layer\nIn chapter 4, you’ll learn formal principles to guide you in making these choices. For the time being, you’ll have to trust me with the following architecture choice:\nTwo intermediate layers with 16 hidden units each\nA third layer that will output the scalar prediction regarding the sentiment of the current review\nThe intermediate layers will use relu as their activation function, and the final layer will use a sigmoid activation so as to output a probability (a score between 0 and 1, indicating how likely the sample is to have the target “1”: how likely the review is to be positive). A relu (rectified linear unit) is a function meant to zero out negative values.\n\nA sigmoid “squashes” arbitrary values into the [0, 1] interval, outputting something that can be interpreted as a probability.\n\nHere’s what the network looks like.\n\nHere’s the Keras implementation, similar to the MNIST example you saw previously.\n\n\nlibrary(keras)\n\nmodel <- keras_model_sequential() %>% \n  layer_dense(units = 16, activation = \"relu\", input_shape = c(10000)) %>% \n  layer_dense(units = 16, activation = \"relu\") %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nActivation Functions\nNote that without an activation function like relu (also called a non-linearity), the dense layer would consist of two linear operations – a dot product and an addition:\noutput = dot(W, input) + b\nSo the layer could only learn linear transformations (affine transformations) of the input data: the hypothesis space of the layer would be the set of all possible linear transformations of the input data into a 16-dimensional space. Such a hypothesis space is too restricted and wouldn’t benefit from multiple layers of representations, because a deep stack of linear layers would still implement a linear operation: adding more layers wouldn’t extend the hypothesis space.\nIn order to get access to a much richer hypothesis space that would benefit from deep representations, you need a non-linearity, or activation function. relu is the most popular activation function in deep learning, but there are many other candidates, which all come with similarly strange names: prelu, elu, and so on.\nLoss Function and Optimizer\nFinally, you need to choose a loss function and an optimizer. Because you’re facing a binary classification problem and the output of your network is a probability (you end your network with a single-unit layer with a sigmoid activation), it’s best to use the binary_crossentropy loss. It isn’t the only viable choice: you could use, for instance, mean_squared_error. But crossentropy is usually the best choice when you’re dealing with models that output probabilities. Crossentropy is a quantity from the field of Information Theory that measures the distance between probability distributions or, in this case, between the ground-truth distribution and your predictions.\nHere’s the step where you configure the model with the rmsprop optimizer and the binary_crossentropy loss function. Note that you’ll also monitor accuracy during training.\n\n\nmodel %>% compile(\n  optimizer = \"rmsprop\",\n  loss = \"binary_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\nYou’re passing your optimizer, loss function, and metrics as strings, which is possible because rmsprop, binary_crossentropy, and accuracy are packaged as part of Keras. Sometimes you may want to configure the parameters of your optimizer or pass a custom loss function or metric function. The former can be done by passing an optimizer instance as the optimizer argument:\n\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(lr=0.001),\n  loss = \"binary_crossentropy\",\n  metrics = c(\"accuracy\")\n) \n\nCustom loss and metrics functions can be provided by passing function objects as the loss and/or metrics arguments\n\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(lr = 0.001),\n  loss = loss_binary_crossentropy,\n  metrics = metric_binary_accuracy\n) \n\nValidating your approach\nIn order to monitor during training the accuracy of the model on data it has never seen before, you’ll create a validation set by setting apart 10,000 samples from the original training data.\n\n\nval_indices <- 1:10000\n\nx_val <- x_train[val_indices,]\npartial_x_train <- x_train[-val_indices,]\n\ny_val <- y_train[val_indices]\npartial_y_train <- y_train[-val_indices]\n\nYou’ll now train the model for 20 epochs (20 iterations over all samples in the x_train and y_train tensors), in mini-batches of 512 samples. At the same time, you’ll monitor loss and accuracy on the 10,000 samples that you set apart. You do so by passing the validation data as the validation_data argument.\n\n\nmodel %>% compile(\n  optimizer = \"rmsprop\",\n  loss = \"binary_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\nhistory <- model %>% fit(\n  partial_x_train,\n  partial_y_train,\n  epochs = 20,\n  batch_size = 512,\n  validation_data = list(x_val, y_val)\n)\n\nOn CPU, this will take less than 2 seconds per epoch – training is over in 20 seconds. At the end of every epoch, there is a slight pause as the model computes its loss and accuracy on the 10,000 samples of the validation data.\nNote that the call to fit() returns a history object. The history object has a plot() method that enables us to visualize the training and validation metrics by epoch:\n\n\nplot(history)\n\n\nThe accuracy is plotted on the top panel and the loss on the bottom panel. Note that your own results may vary slightly due to a different random initialization of your network.\nAs you can see, the training loss decreases with every epoch, and the training accuracy increases with every epoch. That’s what you would expect when running a gradient-descent optimization – the quantity you’re trying to minimize should be less with every iteration. But that isn’t the case for the validation loss and accuracy: they seem to peak at the fourth epoch. This is an example of what we warned against earlier: a model that performs better on the training data isn’t necessarily a model that will do better on data it has never seen before. In precise terms, what you’re seeing is overfitting: after the second epoch, you’re overoptimizing on the training data, and you end up learning representations that are specific to the training data and don’t generalize to data outside of the training set.\nIn this case, to prevent overfitting, you could stop training after three epochs. In general, you can use a range of techniques to mitigate overfitting,which we’ll cover in chapter 4.\nLet’s train a new network from scratch for four epochs and then evaluate it on the test data.\n\n\nmodel <- keras_model_sequential() %>% \n  layer_dense(units = 16, activation = \"relu\", input_shape = c(10000)) %>% \n  layer_dense(units = 16, activation = \"relu\") %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %>% compile(\n  optimizer = \"rmsprop\",\n  loss = \"binary_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\nmodel %>% fit(x_train, y_train, epochs = 4, batch_size = 512)\nresults <- model %>% evaluate(x_test, y_test)\n\n\n\nresults\n\n\n$loss\n[1] 0.2900235\n\n$acc\n[1] 0.88512\nThis fairly naive approach achieves an accuracy of 88%. With state-of-the-art approaches, you should be able to get close to 95%.\nGenerating predictions\nAfter having trained a network, you’ll want to use it in a practical setting. You can generate the likelihood of reviews being positive by using the predict method:\n\n\nmodel %>% predict(x_test[1:10,])\n\n\n [1,] 0.92306918\n [2,] 0.84061098\n [3,] 0.99952853\n [4,] 0.67913240\n [5,] 0.73874789\n [6,] 0.23108074\n [7,] 0.01230567\n [8,] 0.04898361\n [9,] 0.99017477\n[10,] 0.72034937\nAs you can see, the network is confident for some samples (0.99 or more, or 0.01 or less) but less confident for others (0.7, 0.2).\nFurther experiments\nThe following experiments will help convince you that the architecture choices you’ve made are all fairly reasonable, although there’s still room for improvement.\nYou used two hidden layers. Try using one or three hidden layers, and see how doing so affects validation and test accuracy.\nTry using layers with more hidden units or fewer hidden units: 32 units, 64 units, and so on.\nTry using the mse loss function instead of binary_crossentropy.\nTry using the tanh activation (an activation that was popular in the early days of neural networks) instead of relu.\nWrapping up\nHere’s what you should take away from this example:\nYou usually need to do quite a bit of preprocessing on your raw data in order to be able to feed it – as tensors – into a neural network. Sequences of words can be encoded as binary vectors, but there are other encoding options, too.\nStacks of dense layers with relu activations can solve a wide range of problems (including sentiment classification), and you’ll likely use them frequently.\nIn a binary classification problem (two output classes), your network should end with a dense layer with one unit and a sigmoid activation: the output of your network should be a scalar between 0 and 1, encoding a probability.\nWith such a scalar sigmoid output on a binary classification problem, the loss function you should use is binary_crossentropy.\nThe rmsprop optimizer is generally a good enough choice, whatever your problem. That’s one less thing for you to worry about.\nAs they get better on their training data, neural networks eventually start overfitting and end up obtaining increasingly worse results on data they’ve never seen before. Be sure to always monitor performance on data that is outside of the training set.\n",
    "preview": "posts/2017-12-07-text-classification-with-keras/images/training-history.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 1400,
    "preview_height": 865
  },
  {
    "path": "posts/2017-10-04-tfruns/",
    "title": "tfruns: Tools for TensorFlow Training Runs",
    "description": "The tfruns package provides a suite of tools for tracking, visualizing, and managing TensorFlow training runs and experiments from R.",
    "author": [
      {
        "name": "J.J. Allaire",
        "url": "https://github.com/jjallaire"
      }
    ],
    "date": "2017-10-04",
    "categories": [
      "Packages/Releases"
    ],
    "contents": "\nThe tfruns package provides a suite of tools for tracking, visualizing, and managing TensorFlow training runs and experiments from R. Use the tfruns package to:\nTrack the hyperparameters, metrics, output, and source code of every training run.\nCompare hyperparmaeters and metrics across runs to find the best performing model.\nAutomatically generate reports to visualize individual training runs or comparisons between runs. \nYou can install the tfruns package from GitHub as follows:\n\n\ndevtools::install_github(\"rstudio/tfruns\")\n\nComplete documentation for tfruns is available on the TensorFlow for R website.\ntfruns is intended to be used with the keras and/or the tfestimators packages, both of which provide higher level interfaces to TensorFlow from R. These packages can be installed with:\n\n\n# keras\ninstall.packages(\"keras\")\n\n# tfestimators\ndevtools::install_github(\"rstudio/tfestimators\")\n\nTraining\nIn the following sections we’ll describe the various capabilities of tfruns. Our example training script (mnist_mlp.R) trains a Keras model to recognize MNIST digits.\nTo train a model with tfruns, just use the training_run() function in place of the source() function to execute your R script. For example:\n\n\nlibrary(tfruns)\ntraining_run(\"mnist_mlp.R\")\n\nWhen training is completed, a summary of the run will automatically be displayed if you are within an interactive R session:\n\nThe metrics and output of each run are automatically captured within a run directory which is unique for each run that you initiate. Note that for Keras and TF Estimator models this data is captured automatically (no changes to your source code are required).\nYou can call the latest_run() function to view the results of the last run (including the path to the run directory which stores all of the run’s output):\n\n\nlatest_run()\n\n\n$ run_dir           : chr \"runs/2017-10-02T14-23-38Z\"\n$ eval_loss         : num 0.0956\n$ eval_acc          : num 0.98\n$ metric_loss       : num 0.0624\n$ metric_acc        : num 0.984\n$ metric_val_loss   : num 0.0962\n$ metric_val_acc    : num 0.98\n$ flag_dropout1     : num 0.4\n$ flag_dropout2     : num 0.3\n$ samples           : int 48000\n$ validation_samples: int 12000\n$ batch_size        : int 128\n$ epochs            : int 20\n$ epochs_completed  : int 20\n$ metrics           : chr \"(metrics data frame)\"\n$ model             : chr \"(model summary)\"\n$ loss_function     : chr \"categorical_crossentropy\"\n$ optimizer         : chr \"RMSprop\"\n$ learning_rate     : num 0.001\n$ script            : chr \"mnist_mlp.R\"\n$ start             : POSIXct[1:1], format: \"2017-10-02 14:23:38\"\n$ end               : POSIXct[1:1], format: \"2017-10-02 14:24:24\"\n$ completed         : logi TRUE\n$ output            : chr \"(script ouptut)\"\n$ source_code       : chr \"(source archive)\"\n$ context           : chr \"local\"\n$ type              : chr \"training\"\nThe run directory used in the example above is “runs/2017-10-02T14-23-38Z”. Run directories are by default generated within the “runs” subdirectory of the current working directory, and use a timestamp as the name of the run directory. You can view the report for any given run using the view_run() function:\n\n\nview_run(\"runs/2017-10-02T14-23-38Z\")\n\nComparing Runs\nLet’s make a couple of changes to our training script to see if we can improve model performance. We’ll change the number of units in our first dense layer to 128, change the learning_rate from 0.001 to 0.003 and run 30 rather than 20 epochs. After making these changes to the source code we re-run the script using training_run() as before:\n\n\ntraining_run(\"mnist_mlp.R\")\n\nThis will also show us a report summarizing the results of the run, but what we are really interested in is a comparison between this run and the previous one. We can view a comparison via the compare_runs() function:\n\n\ncompare_runs()\n\n\nThe comparison report shows the model attributes and metrics side-by-side, as well as differences in the source code and output of the training script.\nNote that compare_runs() will by default compare the last two runs, however you can pass any two run directories you like to be compared.\nUsing Flags\nTuning a model often requires exploring the impact of changes to many hyperparameters. The best way to approach this is generally not by changing the source code of the training script as we did above, but instead by defining flags for key parameters you may want to vary. In the example script you can see that we have done this for the dropout layers:\n\n\nFLAGS <- flags(\n  flag_numeric(\"dropout1\", 0.4),\n  flag_numeric(\"dropout2\", 0.3)\n)\n\nThese flags are then used in the definition of our model here:\n\n\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_dense(units = 128, activation = 'relu', input_shape = c(784)) %>%\n  layer_dropout(rate = FLAGS$dropout1) %>%\n  layer_dense(units = 128, activation = 'relu') %>%\n  layer_dropout(rate = FLAGS$dropout2) %>%\n  layer_dense(units = 10, activation = 'softmax')\n\nOnce we’ve defined flags, we can pass alternate flag values to training_run() as follows:\n\n\ntraining_run('mnist_mlp.R', flags = c(dropout1 = 0.2, dropout2 = 0.2))\n\nYou aren’t required to specify all of the flags (any flags excluded will simply use their default value).\nFlags make it very straightforward to systematically explore the impact of changes to hyperparameters on model performance, for example:\n\n\nfor (dropout1 in c(0.1, 0.2, 0.3))\n  training_run('mnist_mlp.R', flags = c(dropout1 = dropout1))\n\nFlag values are automatically included in run data with a “flag_” prefix (e.g. flag_dropout1, flag_dropout2).\nSee the article on training flags for additional documentation on using flags.\nAnalyzing Runs\nWe’ve demonstrated visualizing and comparing one or two runs, however as you accumulate more runs you’ll generally want to analyze and compare runs many runs. You can use the ls_runs() function to yield a data frame with summary information on all of the runs you’ve conducted within a given directory:\n\n\nls_runs()\n\n\n# A tibble: 6 x 27\n                    run_dir eval_loss eval_acc metric_loss metric_acc metric_val_loss\n                      <chr>     <dbl>    <dbl>       <dbl>      <dbl>           <dbl>\n1 runs/2017-10-02T14-56-57Z    0.1263   0.9784      0.0773     0.9807          0.1283\n2 runs/2017-10-02T14-56-04Z    0.1323   0.9783      0.0545     0.9860          0.1414\n3 runs/2017-10-02T14-55-11Z    0.1407   0.9804      0.0348     0.9914          0.1542\n4 runs/2017-10-02T14-51-44Z    0.1164   0.9801      0.0448     0.9882          0.1396\n5 runs/2017-10-02T14-37-00Z    0.1338   0.9750      0.1097     0.9732          0.1328\n6 runs/2017-10-02T14-23-38Z    0.0956   0.9796      0.0624     0.9835          0.0962\n# ... with 21 more variables: metric_val_acc <dbl>, flag_dropout1 <dbl>,\n#   flag_dropout2 <dbl>, samples <int>, validation_samples <int>, batch_size <int>,\n#   epochs <int>, epochs_completed <int>, metrics <chr>, model <chr>, loss_function <chr>,\n#   optimizer <chr>, learning_rate <dbl>, script <chr>, start <dttm>, end <dttm>,\n#   completed <lgl>, output <chr>, source_code <chr>, context <chr>, type <chr>\nSince ls_runs() returns a data frame you can also render a sortable, filterable version of it within RStudio using the View() function:\n\n\nView(ls_runs())\n\n\nThe ls_runs() function also supports subset and order arguments. For example, the following will yield all runs with an eval accuracy better than 0.98:\n\n\nls_runs(eval_acc > 0.98, order = eval_acc)\n\nYou can pass the results of ls_runs() to compare runs (which will always compare the first two runs passed). For example, this will compare the two runs that performed best in terms of evaluation accuracy:\n\n\ncompare_runs(ls_runs(eval_acc > 0.98, order = eval_acc))\n\n\nRStudio IDE\nIf you use RStudio with tfruns, it’s strongly recommended that you update to the current Preview Release of RStudio v1.1, as there are are a number of points of integration with the IDE that require this newer release.\nAddin\nThe tfruns package installs an RStudio IDE addin which provides quick access to frequently used functions from the Addins menu:\n\nNote that you can use Tools -> Modify Keyboard Shortcuts within RStudio to assign a keyboard shortcut to one or more of the addin commands.\nBackground Training\nRStudio v1.1 includes a Terminal pane alongside the Console pane. Since training runs can become quite lengthy, it’s often useful to run them in the background in order to keep the R console free for other work. You can do this from a Terminal as follows:\n\nIf you are not running within RStudio then you can of course use a system terminal window for background training.\nPublishing Reports\nTraining run views and comparisons are HTML documents which can be saved and shared with others. When viewing a report within RStudio v1.1 you can save a copy of the report or publish it to RPubs or RStudio Connect:\n\nIf you are not running within RStudio then you can use the save_run_view() and save_run_comparison() functions to create standalone HTML versions of run reports.\nManaging Runs\nThere are a variety of tools available for managing training run output, including:\nExporting run artifacts (e.g. saved models).\nCopying and purging run directories.\nUsing a custom run directory for an experiment or other set of related runs.\nThe Managing Runs article provides additional details on using these features.\n\n\n",
    "preview": "posts/2017-10-04-tfruns/preview.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 2006,
    "preview_height": 1116
  },
  {
    "path": "posts/2017-09-06-keras-for-r/",
    "title": "Keras for R",
    "description": "We are excited to announce that the keras package is now available on CRAN. The package provides an R interface to Keras, a high-level neural networks API developed with a focus on enabling fast experimentation.",
    "author": [
      {
        "name": "J.J. Allaire",
        "url": "https://github.com/jjallaire"
      }
    ],
    "date": "2017-09-05",
    "categories": [
      "TensorFlow/Keras",
      "Packages/Releases"
    ],
    "contents": "\nWe are excited to announce that the keras package is now available on CRAN. The package provides an R interface to Keras, a high-level neural networks API developed with a focus on enabling fast experimentation. Keras has the following key features:\nAllows the same code to run on CPU or on GPU, seamlessly.\nUser-friendly API which makes it easy to quickly prototype deep learning models.\nBuilt-in support for convolutional networks (for computer vision), recurrent networks (for sequence processing), and any combination of both.\nSupports arbitrary network architectures: multi-input or multi-output models, layer sharing, model sharing, etc. This means that Keras is appropriate for building essentially any deep learning model, from a memory network to a neural Turing machine.\nIs capable of running on top of multiple back-ends including TensorFlow, CNTK, or Theano.\nIf you are already familiar with Keras and want to jump right in, check out https://tensorflow.rstudio.com/keras which has everything you need to get started including over 20 complete examples to learn from.\nTo learn a bit more about Keras and why we’re so excited to announce the Keras interface for R, read on!\nKeras and Deep Learning\nInterest in deep learning has been accelerating rapidly over the past few years, and several deep learning frameworks have emerged over the same time frame. Of all the available frameworks, Keras has stood out for its productivity, flexibility and user-friendly API. At the same time, TensorFlow has emerged as a next-generation machine learning platform that is both extremely flexible and well-suited to production deployment.\nNot surprisingly, Keras and TensorFlow have of late been pulling away from other deep learning frameworks:\n\n\nGoogle web search interest around deep learning frameworks over time. If you remember Q4 2015 and Q1-2 2016 as confusing, you weren't alone. pic.twitter.com/1f1VQVGr8n\n\n— François Chollet (@fchollet) June 3, 2017\n\nThe good news about Keras and TensorFlow is that you don’t need to choose between them! The default backend for Keras is TensorFlow and Keras can be integrated seamlessly with TensorFlow workflows. There is also a pure-TensorFlow implementation of Keras with deeper integration on the roadmap for later this year.\nKeras and TensorFlow are the state of the art in deep learning tools and with the keras package you can now access both with a fluent R interface.\nGetting Started\nInstallation\nTo begin, install the keras R package from CRAN as follows:\n\n\ninstall.packages(\"keras\")\n\nThe Keras R interface uses the TensorFlow backend engine by default. To install both the core Keras library as well as the TensorFlow backend use the install_keras() function:\n\n\nlibrary(keras)\ninstall_keras()\n\nThis will provide you with default CPU-based installations of Keras and TensorFlow. If you want a more customized installation, e.g. if you want to take advantage of NVIDIA GPUs, see the documentation for install_keras().\nMNIST Example\nWe can learn the basics of Keras by walking through a simple example: recognizing handwritten digits from the MNIST dataset. MNIST consists of 28 x 28 grayscale images of handwritten digits like these:\n\nThe dataset also includes labels for each image, telling us which digit it is. For example, the labels for the above images are 5, 0, 4, and 1.\nPreparing the Data\nThe MNIST dataset is included with Keras and can be accessed using the dataset_mnist() function. Here we load the dataset then create variables for our test and training data:\n\n\nlibrary(keras)\nmnist <- dataset_mnist()\nx_train <- mnist$train$x\ny_train <- mnist$train$y\nx_test <- mnist$test$x\ny_test <- mnist$test$y\n\nThe x data is a 3-d array (images,width,height) of grayscale values. To prepare the data for training we convert the 3-d arrays into matrices by reshaping width and height into a single dimension (28x28 images are flattened into length 784 vectors). Then, we convert the grayscale values from integers ranging between 0 to 255 into floating point values ranging between 0 and 1:\n\n\n# reshape\ndim(x_train) <- c(nrow(x_train), 784)\ndim(x_test) <- c(nrow(x_test), 784)\n# rescale\nx_train <- x_train / 255\nx_test <- x_test / 255\n\nThe y data is an integer vector with values ranging from 0 to 9. To prepare this data for training we one-hot encode the vectors into binary class matrices using the Keras to_categorical() function:\n\n\ny_train <- to_categorical(y_train, 10)\ny_test <- to_categorical(y_test, 10)\n\nDefining the Model\nThe core data structure of Keras is a model, a way to organize layers. The simplest type of model is the sequential model, a linear stack of layers.\nWe begin by creating a sequential model and then adding layers using the pipe (%>%) operator:\n\n\nmodel <- keras_model_sequential() \nmodel %>% \n  layer_dense(units = 256, activation = \"relu\", input_shape = c(784)) %>% \n  layer_dropout(rate = 0.4) %>% \n  layer_dense(units = 128, activation = \"relu\") %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units = 10, activation = \"softmax\")\n\nThe input_shape argument to the first layer specifies the shape of the input data (a length 784 numeric vector representing a grayscale image). The final layer outputs a length 10 numeric vector (probabilities for each digit) using a softmax activation function.\nUse the summary() function to print the details of the model:\n\n\nsummary(model)\n\nModel\n________________________________________________________________________________\nLayer (type)                        Output Shape                    Param #     \n================================================================================\ndense_1 (Dense)                     (None, 256)                     200960      \n________________________________________________________________________________\ndropout_1 (Dropout)                 (None, 256)                     0           \n________________________________________________________________________________\ndense_2 (Dense)                     (None, 128)                     32896       \n________________________________________________________________________________\ndropout_2 (Dropout)                 (None, 128)                     0           \n________________________________________________________________________________\ndense_3 (Dense)                     (None, 10)                      1290        \n================================================================================\nTotal params: 235,146\nTrainable params: 235,146\nNon-trainable params: 0\n________________________________________________________________________________\nNext, compile the model with appropriate loss function, optimizer, and metrics:\n\n\nmodel %>% compile(\n  loss = \"categorical_crossentropy\",\n  optimizer = optimizer_rmsprop(),\n  metrics = c(\"accuracy\")\n)\n\nTraining and Evaluation\nUse the fit() function to train the model for 30 epochs using batches of 128 images:\n\n\nhistory <- model %>% fit(\n  x_train, y_train, \n  epochs = 30, batch_size = 128, \n  validation_split = 0.2\n)\n\nThe history object returned by fit() includes loss and accuracy metrics which we can plot:\n\n\nplot(history)\n\n\nEvaluate the model’s performance on the test data:\n\n\nmodel %>% evaluate(x_test, y_test,verbose = 0)\n\n\n$loss\n[1] 0.1149\n\n$acc\n[1] 0.9807\nGenerate predictions on new data:\n\n\nmodel %>% predict_classes(x_test)\n\n\n  [1] 7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7 2 7 1 2\n [40] 1 1 7 4 2 3 5 1 2 4 4 6 3 5 5 6 0 4 1 9 5 7 8 9 3 7 4 6 4 3 0 7 0 2 9 1 7 3 2\n [79] 9 7 7 6 2 7 8 4 7 3 6 1 3 6 9 3 1 4 1 7 6 9\n [ reached getOption(\"max.print\") -- omitted 9900 entries ]\nKeras provides a vocabulary for building deep learning models that is simple, elegant, and intuitive. Building a question answering system, an image classification model, a neural Turing machine, or any other model is just as straightforward.\nThe Guide to the Sequential Model article describes the basics of Keras sequential models in more depth.\nExamples\nOver 20 complete examples are available (special thanks to [@dfalbel](https://github.com/dfalbel) for his work on these!). The examples cover image classification, text generation with stacked LSTMs, question-answering with memory networks, transfer learning, variational encoding, and more.\nExample\nDescription\naddition_rnn\nImplementation of sequence to sequence learning for performing addition of two numbers (as strings).\nbabi_memnn\nTrains a memory network on the bAbI dataset for reading comprehension.\nbabi_rnn\nTrains a two-branch recurrent network on the bAbI dataset for reading comprehension.\ncifar10_cnn\nTrains a simple deep CNN on the CIFAR10 small images dataset.\nconv_lstm\nDemonstrates the use of a convolutional LSTM network.\ndeep_dream\nDeep Dreams in Keras.\nimdb_bidirectional_lstm\nTrains a Bidirectional LSTM on the IMDB sentiment classification task.\nimdb_cnn\nDemonstrates the use of Convolution1D for text classification.\nimdb_cnn_lstm\nTrains a convolutional stack followed by a recurrent stack network on the IMDB sentiment classification task.\nimdb_fasttext\nTrains a FastText model on the IMDB sentiment classification task.\nimdb_lstm\nTrains a LSTM on the IMDB sentiment classification task.\nlstm_text_generation\nGenerates text from Nietzsche’s writings.\nmnist_acgan\nImplementation of AC-GAN (Auxiliary Classifier GAN ) on the MNIST dataset\nmnist_antirectifier\nDemonstrates how to write custom layers for Keras\nmnist_cnn\nTrains a simple convnet on the MNIST dataset.\nmnist_irnn\nReproduction of the IRNN experiment with pixel-by-pixel sequential MNIST in “A Simple Way to Initialize Recurrent Networks of Rectified Linear Units” by Le et al.\nmnist_mlp\nTrains a simple deep multi-layer perceptron on the MNIST dataset.\nmnist_hierarchical_rnn\nTrains a Hierarchical RNN (HRNN) to classify MNIST digits.\nmnist_transfer_cnn\nTransfer learning toy example.\nneural_style_transfer\nNeural style transfer (generating an image with the same “content” as a base image, but with the “style” of a different picture).\nreuters_mlp\nTrains and evaluates a simple MLP on the Reuters newswire topic classification task.\nstateful_lstm\nDemonstrates how to use stateful RNNs to model long sequences efficiently.\nvariational_autoencoder\nDemonstrates how to build a variational autoencoder.\nvariational_autoencoder_deconv\nDemonstrates how to build a variational autoencoder with Keras using deconvolution layers.\nLearning More\nAfter you’ve become familiar with the basics, these articles are a good next step:\nGuide to the Sequential Model. The sequential model is a linear stack of layers and is the API most users should start with.\nGuide to the Functional API. The Keras functional API is the way to go for defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.\nTraining Visualization. There are a wide variety of tools available for visualizing training. These include plotting of training metrics, real time display of metrics within the RStudio IDE, and integration with the TensorBoard visualization tool included with TensorFlow.\nUsing Pre-Trained Models. Keras includes a number of deep learning models (Xception, VGG16, VGG19, ResNet50, InceptionVV3, and MobileNet) that are made available alongside pre-trained weights. These models can be used for prediction, feature extraction, and fine-tuning.\nFrequently Asked Questions. Covers many additional topics including streaming training data, saving models, training on GPUs, and more.\nKeras provides a productive, highly flexible framework for developing deep learning models. We can’t wait to see what the R community will do with these tools!\n\n\n",
    "preview": "posts/2017-09-06-keras-for-r/preview.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 669,
    "preview_height": 414
  },
  {
    "path": "posts/2017-08-31-tensorflow-estimators-for-r/",
    "title": "TensorFlow Estimators",
    "description": "The tfestimators package is an R interface to TensorFlow Estimators, a high-level API that provides implementations of many different model types including linear models and deep neural networks.",
    "author": [
      {
        "name": "Yuan Tang",
        "url": "https://orcid.org/0000-0001-5243-233X"
      }
    ],
    "date": "2017-08-31",
    "categories": [
      "Packages/Releases"
    ],
    "contents": "\nThe tfestimators package is an R interface to TensorFlow Estimators, a high-level API that provides implementations of many different model types including linear models and deep neural networks.\nMore models are coming soon such as state saving recurrent neural networks, dynamic recurrent neural networks, support vector machines, random forest, KMeans clustering, etc. TensorFlow estimators also provides a flexible framework for defining arbitrary new model types as custom estimators.\nThe framework balances the competing demands for flexibility and simplicity by offering APIs at different levels of abstraction, making common model architectures available out of the box, while providing a library of utilities designed to speed up experimentation with model architectures.\nThese abstractions guide developers to write models in ways conducive to productionization as well as making it possible to write downstream infrastructure for distributed training or parameter tuning independent of the model implementation.\nTo make out of the box models flexible and usable across a wide range of problems, tfestimators provides canned Estimators that are are parameterized not only over traditional hyperparameters, but also using feature columns, a declarative specification describing how to interpret input data.\nFor more details on the architecture and design of TensorFlow Estimators, please check out the KDD’17 paper: TensorFlow Estimators: Managing Simplicity vs. Flexibility in High-Level Machine Learning Frameworks.\nQuick Start\nInstallation\nTo use tfestimators, you need to install both the tfestimators R package as well as TensorFlow itself.\nFirst, install the tfestimators R package as follows:\n\n\ndevtools::install_github(\"rstudio/tfestimators\")\n\nThen, use the install_tensorflow() function to install TensorFlow (note that the current tfestimators package requires version 1.3.0 of TensorFlow so even if you already have TensorFlow installed you should update if you are running a previous version):\n\n\nlibrary(tfestimators)\ninstall_tensorflow()\n\nThis will provide you with a default installation of TensorFlow suitable for getting started. See the article on installation to learn about more advanced options, including installing a version of TensorFlow that takes advantage of NVIDIA GPUs if you have the correct CUDA libraries installed.\nLinear Regression\nLet’s create a simple linear regression model with the mtcars dataset to demonstrate the use of estimators. We’ll illustrate how input functions can be constructed and used to feed data to an estimator, how feature columns can be used to specify a set of transformations to apply to input data, and how these pieces come together in the Estimator interface.\nInput Function\nEstimators can receive data through input functions. Input functions take an arbitrary data source (in-memory data sets, streaming data, custom data format, and so on) and generate Tensors that can be supplied to TensorFlow models. The tfestimators package includes an input_fn() function that can create TensorFlow input functions from common R data sources (e.g. data frames and matrices). It’s also possible to write a fully custom input function.\nHere, we define a helper function that will return an input function for a subset of our mtcars data set.\n\n\nlibrary(tfestimators)\n\n# return an input_fn for a given subset of data\nmtcars_input_fn <- function(data) {\n  input_fn(data, \n           features = c(\"disp\", \"cyl\"), \n           response = \"mpg\")\n}\n\nFeature Columns\nNext, we define the feature columns for our model. Feature columns are used to specify how Tensors received from the input function should be combined and transformed before entering the model training, evaluation, and prediction steps. A feature column can be a plain mapping to some input column (e.g. column_numeric() for a column of numerical data), or a transformation of other feature columns (e.g. column_crossed() to define a new column as the cross of two other feature columns).\nHere, we create a list of feature columns containing two numeric variables - disp and cyl:\n\n\ncols <- feature_columns(\n  column_numeric(\"disp\"),\n  column_numeric(\"cyl\")\n)\n\nYou can also define multiple feature columns at once:\n\n\ncols <- feature_columns( \n  column_numeric(\"disp\", \"cyl\")\n)\n\nBy using the family of feature column functions we can define various transformations on the data before using it for modeling.\nEstimator\nNext, we create the estimator by calling the linear_regressor() function and passing it a set of feature columns:\n\n\nmodel <- linear_regressor(feature_columns = cols)\n\nTraining\nWe’re now ready to train our model, using the train() function. We’ll partition the mtcars data set into separate training and validation data sets, and feed the training data set into train(). We’ll hold 20% of the data aside for validation.\n\n\nindices <- sample(1:nrow(mtcars), size = 0.80 * nrow(mtcars))\ntrain <- mtcars[indices, ]\ntest  <- mtcars[-indices, ]\n\n# train the model\nmodel %>% train(mtcars_input_fn(train))\n\nEvaluation\nWe can evaluate the model’s accuracy using the evaluate() function, using our ‘test’ data set for validation.\n\n\nmodel %>% evaluate(mtcars_input_fn(test))\n\nPrediction\nAfter we’ve finished training out model, we can use it to generate predictions from new data.\n\n\nnew_obs <- mtcars[1:3, ]\nmodel %>% predict(mtcars_input_fn(new_obs))\n\nLearning More\nAfter you’ve become familiar with these concepts, these articles cover the basics of using TensorFlow Estimators and the main components in more detail:\nEstimator Basics\nInput Functions\nFeature Columns\nThese articles describe more advanced topics/usage:\nRun Hooks\nCustom Estimators\nTensorFlow Layers\nTensorBoard Visualization\nParsing Utilities\nOne of the best ways to learn is from reviewing and experimenting with examples. See the Examples page for a variety of examples to help you get started.\n\n\n",
    "preview": "posts/2017-08-31-tensorflow-estimators-for-r/tensorflow-architecture.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 1198,
    "preview_height": 796
  },
  {
    "path": "posts/2017-08-17-tensorflow-v13-released/",
    "title": "TensorFlow v1.3 Released",
    "description": "The final release of TensorFlow v1.3 is now available. This release marks the initial availability of several canned estimators including DNNClassifier and  DNNRegressor.",
    "author": [
      {
        "name": "J.J. Allaire",
        "url": "https://github.com/jjallaire"
      }
    ],
    "date": "2017-08-17",
    "categories": [
      "Packages/Releases"
    ],
    "contents": "\nThe final release of TensorFlow v1.3 is now available. This release of TensorFlow marks the initial availability of several canned estimators, including:\nDNNClassifier\nDNNRegressor\nLinearClassifier\nLinearRegressor\nDNNLinearCombinedClassifier\nDNNLinearCombinedRegressor.\nThe tfestimators package provides a high level R interface for these estimators.\nFull details on the release of TensorFlow v1.3 are available here: https://github.com/tensorflow/tensorflow/releases/tag/v1.3.0\nYou can update your R installation of TensorFlow using the install_tensorflow function:\n\nlibrary(tensorflow)\ninstall_tensorflow()\nNote that you should also provide any options used in your original installation (e.g. method = \"conda\", version = \"gpu\", etc. )\ncuDNN 6.0\nTensorFlow v1.3 is built against version 6.0 of the cuDNN library from NVIDIA. Previous versions were built against cuDNN v5.1, so for installations running the GPU version of TensorFlow this means that you will need to install an updated version of cuDNN along with TensorFlow v1.3.\nUpdated installation instructions are available here: https://tensorflow.rstudio.com/tensorflow/installation_gpu.html.\nVersion 1.4 of TensorFlow is expected to migrate again to version 7.0 of cuDNN.\n\n\n",
    "preview": "posts/2017-08-17-tensorflow-v13-released/tensorflow-logo.png",
    "last_modified": "2020-09-30T20:24:30+02:00",
    "input_file": {},
    "preview_width": 3876,
    "preview_height": 741
  }
]
