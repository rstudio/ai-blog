<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #545454; font-weight: bold; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #a1024a; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007faa; font-weight: bold; } /* ControlFlow */
code span.ch { color: #008000; } /* Char */
code span.cn { color: #d91e18; } /* Constant */
code span.co { color: #545454; } /* Comment */
code span.cv { color: #545454; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #aa5d00; } /* DataType */
code span.dv { color: #a1024a; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #a1024a; } /* Float */
code span.fu { color: #4254a7; } /* Function */
code span.im { } /* Import */
code span.in { color: #545454; font-weight: bold; } /* Information */
code span.kw { color: #007faa; font-weight: bold; } /* Keyword */
code span.op { color: #696969; } /* Operator */
code span.ot { color: #007faa; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #008000; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #008000; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #008000; } /* VerbatimString */
code span.wa { color: #545454; font-weight: bold; font-style: italic; } /* Warning */
</style>

  <!--radix_placeholder_meta_tags-->
<title>RStudio AI Blog: Representation learning with MMD-VAE</title>

<meta property="description" itemprop="description" content="Like GANs, variational autoencoders (VAEs) are often used to generate images. However, VAEs add an additional promise: namely, to model an underlying latent space. Here, we first look at a typical implementation that maximizes the evidence lower bound. Then, we compare it to one of the more recent competitors, MMD-VAE, from the Info-VAE (information maximizing VAE) family."/>

<link rel="canonical" href="https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae/"/>
<link rel="license" href="https://creativecommons.org/licenses/by/4.0/"/>
<link rel="icon" type="image/png" href="../../images/favicon.png"/>

<!--  https://schema.org/Article -->
<meta property="article:published" itemprop="datePublished" content="2018-10-22"/>
<meta property="article:created" itemprop="dateCreated" content="2018-10-22"/>
<meta name="article:author" content="Sigrid Keydana"/>

<!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
<meta property="og:title" content="RStudio AI Blog: Representation learning with MMD-VAE"/>
<meta property="og:type" content="article"/>
<meta property="og:description" content="Like GANs, variational autoencoders (VAEs) are often used to generate images. However, VAEs add an additional promise: namely, to model an underlying latent space. Here, we first look at a typical implementation that maximizes the evidence lower bound. Then, we compare it to one of the more recent competitors, MMD-VAE, from the Info-VAE (information maximizing VAE) family."/>
<meta property="og:url" content="https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae/"/>
<meta property="og:image" content="https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae/images/thumb.png"/>
<meta property="og:image:width" content="468"/>
<meta property="og:image:height" content="178"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:site_name" content="RStudio AI Blog"/>

<!--  https://dev.twitter.com/cards/types/summary -->
<meta property="twitter:card" content="summary_large_image"/>
<meta property="twitter:title" content="RStudio AI Blog: Representation learning with MMD-VAE"/>
<meta property="twitter:description" content="Like GANs, variational autoencoders (VAEs) are often used to generate images. However, VAEs add an additional promise: namely, to model an underlying latent space. Here, we first look at a typical implementation that maximizes the evidence lower bound. Then, we compare it to one of the more recent competitors, MMD-VAE, from the Info-VAE (information maximizing VAE) family."/>
<meta property="twitter:url" content="https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae/"/>
<meta property="twitter:image" content="https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae/images/thumb.png"/>
<meta property="twitter:image:width" content="468"/>
<meta property="twitter:image:height" content="178"/>

<!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
<meta name="citation_title" content="RStudio AI Blog: Representation learning with MMD-VAE"/>
<meta name="citation_fulltext_html_url" content="https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae/"/>
<meta name="citation_fulltext_world_readable" content=""/>
<meta name="citation_online_date" content="2018/10/22"/>
<meta name="citation_publication_date" content="2018/10/22"/>
<meta name="citation_author" content="Sigrid Keydana"/>
<meta name="citation_author_institution" content="RStudio"/>
<!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=InfoVAE: Information Maximizing Variational Autoencoders;citation_publication_date=2017;citation_volume=abs/1706.02262;citation_author=Shengjia Zhao;citation_author=Jiaming Song;citation_author=Stefano Ermon"/>
  <meta name="citation_reference" content="citation_title=Auto-Encoding Variational Bayes;citation_publication_date=2013;citation_volume=abs/1312.6114;citation_author=Diederik P. Kingma;citation_author=Max Welling"/>
  <meta name="citation_reference" content="citation_title=VAE with a VampPrior;citation_publication_date=2017;citation_volume=abs/1705.07120;citation_author=Jakub M. Tomczak;citation_author=Max Welling"/>
  <meta name="citation_reference" content="citation_title=Tutorial on Variational Autoencoders;citation_publication_date=2016;citation_author=C. Doersch"/>
  <meta name="citation_reference" content="citation_title=Understanding disentangling in beta-VAE;citation_publication_date=2018;citation_author=C. P. Burgess;citation_author=I. Higgins;citation_author=A. Pal;citation_author=L. Matthey;citation_author=N. Watters;citation_author=G. Desjardins;citation_author=A. Lerchner"/>
  <!--radix_placeholder_rmarkdown_metadata-->

<script type="text/json" id="radix-rmarkdown-metadata">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["creative_commons","title","description","author","bibliography","slug","date","categories","output","preview","citation_url","canonical_url"]}},"value":[{"type":"character","attributes":{},"value":["CC BY"]},{"type":"character","attributes":{},"value":["Representation learning with MMD-VAE"]},{"type":"character","attributes":{},"value":["Like GANs, variational autoencoders (VAEs) are often used to generate images. However, VAEs add an additional promise: namely, to model an underlying latent space. Here, we first look at a typical implementation that maximizes the evidence lower bound. Then, we compare it to one of the more recent competitors, MMD-VAE, from the Info-VAE (information maximizing VAE) family."]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","affiliation_url"]}},"value":[{"type":"character","attributes":{},"value":["Sigrid Keydana"]},{"type":"character","attributes":{},"value":["RStudio"]},{"type":"character","attributes":{},"value":["https://www.rstudio.com/"]}]}]},{"type":"character","attributes":{},"value":["bibliography.bib"]},{"type":"character","attributes":{},"value":["keydana2018mmdvae"]},{"type":"character","attributes":{},"value":["10-22-2018"]},{"type":"character","attributes":{},"value":["TensorFlow/Keras","Unsupervised Learning","Image Recognition & Image Processing"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"character","attributes":{},"value":["images/thumb.png"]},{"type":"character","attributes":{},"value":["https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae/"]},{"type":"character","attributes":{},"value":["https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae/"]}]}
</script>
<!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["bibliography.bib","images/c_epoch_0.png","images/comp_digits.png","images/comp_grid.png","images/comp_lat.png","images/cvae_clothes_epoch_50.png","images/cvae_grid_epoch_50.png","images/cvae_latentspace_epoch_50.png","images/mmd_clothes_epoch_50.png","images/mmd_grid_epoch_50.png","images/mmd_latentspace_epoch_50.png","images/thumb.png","mmd_vae_files/bowser-1.9.3/bowser.min.js","mmd_vae_files/distill-2.2.21/template.v2.js","mmd_vae_files/jquery-1.11.3/jquery.min.js","mmd_vae_files/webcomponents-2.0.0/webcomponents.js"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
<meta name="distill:offset" content="../.."/>

<script type="application/javascript">

  window.headroom_prevent_pin = false;

  window.document.addEventListener("DOMContentLoaded", function (event) {

    // initialize headroom for banner
    var header = $('header').get(0);
    var headerHeight = header.offsetHeight;
    var headroom = new Headroom(header, {
      onPin : function() {
        if (window.headroom_prevent_pin) {
          window.headroom_prevent_pin = false;
          headroom.unpin();
        }
      }
    });
    headroom.init();
    if(window.location.hash)
      headroom.unpin();
    $(header).addClass('headroom--transition');

    // offset scroll location for banner on hash change
    // (see: https://github.com/WickyNilliams/headroom.js/issues/38)
    window.addEventListener("hashchange", function(event) {
      window.scrollTo(0, window.pageYOffset - (headerHeight + 25));
    });

    // responsive menu
    $('.distill-site-header').each(function(i, val) {
      var topnav = $(this);
      var toggle = topnav.find('.nav-toggle');
      toggle.on('click', function() {
        topnav.toggleClass('responsive');
      });
    });

    // nav dropdowns
    $('.nav-dropbtn').click(function(e) {
      $(this).next('.nav-dropdown-content').toggleClass('nav-dropdown-active');
      $(this).parent().siblings('.nav-dropdown')
         .children('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $("body").click(function(e){
      $('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $(".nav-dropdown").click(function(e){
      e.stopPropagation();
    });
  });
</script>

<style type="text/css">

/* Theme (user-documented overrideables for nav appearance) */

.distill-site-nav {
  color: rgba(255, 255, 255, 0.8);
  background-color: #0F2E3D;
  font-size: 15px;
  font-weight: 300;
}

.distill-site-nav a {
  color: inherit;
  text-decoration: none;
}

.distill-site-nav a:hover {
  color: white;
}

@media print {
  .distill-site-nav {
    display: none;
  }
}

.distill-site-header {

}

.distill-site-footer {

}


/* Site Header */

.distill-site-header {
  width: 100%;
  box-sizing: border-box;
  z-index: 3;
}

.distill-site-header .nav-left {
  display: inline-block;
  margin-left: 8px;
}

@media screen and (max-width: 768px) {
  .distill-site-header .nav-left {
    margin-left: 0;
  }
}


.distill-site-header .nav-right {
  float: right;
  margin-right: 8px;
}

.distill-site-header a,
.distill-site-header .title {
  display: inline-block;
  text-align: center;
  padding: 14px 10px 14px 10px;
}

.distill-site-header .title {
  font-size: 18px;
  min-width: 150px;
}

.distill-site-header .logo {
  padding: 0;
}

.distill-site-header .logo img {
  display: none;
  max-height: 20px;
  width: auto;
  margin-bottom: -4px;
}

.distill-site-header .nav-image img {
  max-height: 18px;
  width: auto;
  display: inline-block;
  margin-bottom: -3px;
}



@media screen and (min-width: 1000px) {
  .distill-site-header .logo img {
    display: inline-block;
  }
  .distill-site-header .nav-left {
    margin-left: 20px;
  }
  .distill-site-header .nav-right {
    margin-right: 20px;
  }
  .distill-site-header .title {
    padding-left: 12px;
  }
}


.distill-site-header .nav-toggle {
  display: none;
}

.nav-dropdown {
  display: inline-block;
  position: relative;
}

.nav-dropdown .nav-dropbtn {
  border: none;
  outline: none;
  color: rgba(255, 255, 255, 0.8);
  padding: 16px 10px;
  background-color: transparent;
  font-family: inherit;
  font-size: inherit;
  font-weight: inherit;
  margin: 0;
  margin-top: 1px;
  z-index: 2;
}

.nav-dropdown-content {
  display: none;
  position: absolute;
  background-color: white;
  min-width: 200px;
  border: 1px solid rgba(0,0,0,0.15);
  border-radius: 4px;
  box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.1);
  z-index: 1;
  margin-top: 2px;
  white-space: nowrap;
  padding-top: 4px;
  padding-bottom: 4px;
}

.nav-dropdown-content hr {
  margin-top: 4px;
  margin-bottom: 4px;
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.nav-dropdown-active {
  display: block;
}

.nav-dropdown-content a, .nav-dropdown-content .nav-dropdown-header {
  color: black;
  padding: 6px 24px;
  text-decoration: none;
  display: block;
  text-align: left;
}

.nav-dropdown-content .nav-dropdown-header {
  display: block;
  padding: 5px 24px;
  padding-bottom: 0;
  text-transform: uppercase;
  font-size: 14px;
  color: #999999;
  white-space: nowrap;
}

.nav-dropdown:hover .nav-dropbtn {
  color: white;
}

.nav-dropdown-content a:hover {
  background-color: #ddd;
  color: black;
}

.nav-right .nav-dropdown-content {
  margin-left: -45%;
  right: 0;
}

@media screen and (max-width: 768px) {
  .distill-site-header a, .distill-site-header .nav-dropdown  {display: none;}
  .distill-site-header a.nav-toggle {
    float: right;
    display: block;
  }
  .distill-site-header .title {
    margin-left: 0;
  }
  .distill-site-header .nav-right {
    margin-right: 0;
  }
  .distill-site-header {
    overflow: hidden;
  }
  .nav-right .nav-dropdown-content {
    margin-left: 0;
  }
}


@media screen and (max-width: 768px) {
  .distill-site-header.responsive {position: relative; min-height: 500px; }
  .distill-site-header.responsive a.nav-toggle {
    position: absolute;
    right: 0;
    top: 0;
  }
  .distill-site-header.responsive a,
  .distill-site-header.responsive .nav-dropdown {
    display: block;
    text-align: left;
  }
  .distill-site-header.responsive .nav-left,
  .distill-site-header.responsive .nav-right {
    width: 100%;
  }
  .distill-site-header.responsive .nav-dropdown {float: none;}
  .distill-site-header.responsive .nav-dropdown-content {position: relative;}
  .distill-site-header.responsive .nav-dropdown .nav-dropbtn {
    display: block;
    width: 100%;
    text-align: left;
  }
}

/* Site Footer */

.distill-site-footer {
  width: 100%;
  overflow: hidden;
  box-sizing: border-box;
  z-index: 3;
  margin-top: 30px;
  padding-top: 30px;
  padding-bottom: 30px;
  text-align: center;
}

/* Headroom */

d-title {
  padding-top: 6rem;
}

@media print {
  d-title {
    padding-top: 4rem;
  }
}

.headroom {
  z-index: 1000;
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
}

.headroom--transition {
  transition: all .4s ease-in-out;
}

.headroom--unpinned {
  top: -100px;
}

.headroom--pinned {
  top: 0;
}

</style>

<link href="../../site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet"/>
<link href="../../site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet"/>
<script src="../../site_libs/headroom-0.9.4/headroom.min.js"></script>
<script src="../../site_libs/autocomplete-0.37.1/autocomplete.min.js"></script>
<script src="../../site_libs/fuse-6.4.1/fuse.min.js"></script>

<script type="application/javascript">

function getMeta(metaName) {
  var metas = document.getElementsByTagName('meta');
  for (let i = 0; i < metas.length; i++) {
    if (metas[i].getAttribute('name') === metaName) {
      return metas[i].getAttribute('content');
    }
  }
  return '';
}

function offsetURL(url) {
  var offset = getMeta('distill:offset');
  return offset ? offset + '/' + url : url;
}

function createIndex() {
  var options = {
    keys: [
      "title",
      "categories",
      "description",
      "contents"
    ]
  };
  return new window.Fuse([],options);
}

function createFuseIndex() {

  // create fuse index
  var options = { keys: ["title", "description", "contents"] };
  var fuse = new window.Fuse([], options);

  // fetch the main search.json
  return fetch(offsetURL('search.json'))
    .then(function(response) {
      if (response.status == 200) {
        return response.json().then(function(json) {
          // index main articles
          json.articles.forEach(function(article) {
            fuse.add(article);
          });
          // download collections and index their articles
          return Promise.all(json.collections.map(function(collection) {
            return fetch(offsetURL(collection)).then(function(response) {
              if (response.status === 200) {
                return response.json().then(function(articles) {
                  articles.forEach(function(article) {
                    fuse.add(article);
                  });
                })
              } else {
                return Promise.reject(
                  new Error('Unexpected status from search index request: ' +
                            response.status)
                );
              }
            });
          })).then(function() {
            return fuse;
          });
        });

      } else {
        return Promise.reject(
          new Error('Unexpected status from search index request: ' +
                      response.status)
        );
      }
    });
}

window.document.addEventListener("DOMContentLoaded", function (event) {

  // get search element (bail if we don't have one)
  var searchEl = window.document.getElementById('distill-search');
  if (!searchEl)
    return;

  createFuseIndex()
    .then(function(fuse) {

      // make search box visible
      searchEl.classList.remove('hidden');

      // initialize autocomplete
      var options = {
        autoselect: true,
        hint: false,
        minLength: 2,
      };
      window.autocomplete(searchEl, options, [{
        source: function(query, callback) {
          const searchOptions = {
            isCaseSensitive: false,
            shouldSort: true,
            minMatchCharLength: 2,
            limit: 10,
            keys: [
              { name: 'title', weight: 20 },
              { name: 'categories', weight: 15 },
              { name: 'description', weight: 10 },
              { name: 'contents', weight: 5 },
            ],
          };
          var results = fuse.search(query, searchOptions);
          callback(results
            .map(function(result) { return result.item; })
            .filter(function(item) { return !!item.description; })
          );
        },
        templates: {
          suggestion: function(suggestion) {
            var html = `
              <div class="search-item">
                <h3>${suggestion.title}</h3>
                <div class="search-item-description">
                  ${suggestion.description}
                </div>
                <div class="search-item-preview">
                  <img src="${suggestion.preview ? offsetURL(suggestion.preview) : ''}"</img>
                </div>
              </div>
            `;
            return html;
          }
        }
      }]).on('autocomplete:selected', function(event, suggestion) {
        window.location.href = offsetURL(suggestion.path);
      });
      // remove inline display style on autocompleter (we want to
      // manage responsive display via css)
      $('.algolia-autocomplete').css("display", "");
    })
    .catch(function(error) {
      console.log(error);
    });

});

</script>

<style type="text/css">

/* Algolioa Autocomplete */

.algolia-autocomplete {
  display: inline-block;
  margin-left: 10px;
  vertical-align: sub;
  background-color: white;
  color: black;
  padding: 6px;
  padding-top: 8px;
  padding-bottom: 0;
  border-radius: 6px;
  border: 1px #0F2E3D solid;
  width: 180px;
}


@media screen and (max-width: 768px) {
  .distill-site-nav .algolia-autocomplete {
    display: none;
    visibility: hidden;
  }
  .distill-site-nav.responsive .algolia-autocomplete {
    display: inline-block;
    visibility: visible;
  }
  .distill-site-nav.responsive .algolia-autocomplete .aa-dropdown-menu {
    margin-left: 0;
    width: 400px;
    max-height: 400px;
  }
}

.algolia-autocomplete .aa-input, .algolia-autocomplete .aa-hint {
  width: 90%;
  outline: none;
  border: none;
}

.algolia-autocomplete .aa-hint {
  color: #999;
}
.algolia-autocomplete .aa-dropdown-menu {
  width: 550px;
  max-height: 70vh;
  overflow-x: visible;
  overflow-y: scroll;
  padding: 5px;
  margin-top: 3px;
  margin-left: -150px;
  background-color: #fff;
  border-radius: 5px;
  border: 1px solid #999;
  border-top: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion {
  cursor: pointer;
  padding: 5px 4px;
  border-bottom: 1px solid #eee;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion:last-of-type {
  border-bottom: none;
  margin-bottom: 2px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item {
  overflow: hidden;
  font-size: 0.8em;
  line-height: 1.4em;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item h3 {
  margin-block-start: 0;
  margin-block-end: 5px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-description {
  display: inline-block;
  overflow: hidden;
  height: 2.8em;
  width: 80%;
  margin-right: 4%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview {
  display: inline-block;
  width: 15%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img {
  height: 3em;
  width: auto;
  display: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img[src] {
  display: initial;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion.aa-cursor {
  background-color: #eee;
}
.algolia-autocomplete .aa-dropdown-menu .aa-suggestion em {
  font-weight: bold;
  font-style: normal;
}

</style>


<!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

<style type="text/css">

body {
  background-color: white;
}

.pandoc-table {
  width: 100%;
}

.pandoc-table>caption {
  margin-bottom: 10px;
}

.pandoc-table th:not([align]) {
  text-align: left;
}

.pagedtable-footer {
  font-size: 15px;
}

d-byline .byline {
  grid-template-columns: 2fr 2fr;
}

d-byline .byline h3 {
  margin-block-start: 1.5em;
}

d-byline .byline .authors-affiliations h3 {
  margin-block-start: 0.5em;
}

.authors-affiliations .orcid-id {
  width: 16px;
  height:16px;
  margin-left: 4px;
  margin-right: 4px;
  vertical-align: middle;
  padding-bottom: 2px;
}

d-title .dt-tags {
  margin-top: 1em;
  grid-column: text;
}

.dt-tags .dt-tag {
  text-decoration: none;
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0em 0.4em;
  margin-right: 0.5em;
  margin-bottom: 0.4em;
  font-size: 70%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

d-article table.gt_table td,
d-article table.gt_table th {
  border-bottom: none;
}

.html-widget {
  margin-bottom: 2.0em;
}

.l-screen-inset {
  padding-right: 16px;
}

.l-screen .caption {
  margin-left: 10px;
}

.shaded {
  background: rgb(247, 247, 247);
  padding-top: 20px;
  padding-bottom: 20px;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .html-widget {
  margin-bottom: 0;
  border: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .shaded-content {
  background: white;
}

.text-output {
  margin-top: 0;
  line-height: 1.5em;
}

.hidden {
  display: none !important;
}

d-article {
  padding-top: 2.5rem;
  padding-bottom: 30px;
}

d-appendix {
  padding-top: 30px;
}

d-article>p>img {
  width: 100%;
}

d-article h2 {
  margin: 1rem 0 1.5rem 0;
}

d-article h3 {
  margin-top: 1.5rem;
}

d-article iframe {
  border: 1px solid rgba(0, 0, 0, 0.1);
  margin-bottom: 2.0em;
  width: 100%;
}

/* Tweak code blocks */

d-article div.sourceCode code,
d-article pre code {
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
}

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: hidden;
}

d-article div.sourceCode {
  background-color: white;
}

d-article div.sourceCode pre {
  padding-left: 10px;
  font-size: 12px;
  border-left: 2px solid rgba(0,0,0,0.1);
}

d-article pre {
  font-size: 12px;
  color: black;
  background: none;
  margin-top: 0;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

d-article pre a {
  border-bottom: none;
}

d-article pre a:hover {
  border-bottom: none;
  text-decoration: underline;
}

@media(min-width: 768px) {

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: visible !important;
}

d-article div.sourceCode pre {
  padding-left: 18px;
  font-size: 14px;
}

d-article pre {
  font-size: 14px;
}

}

figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

/* CSS for d-contents */

.d-contents {
  grid-column: text;
  color: rgba(0,0,0,0.8);
  font-size: 0.9em;
  padding-bottom: 1em;
  margin-bottom: 1em;
  padding-bottom: 0.5em;
  margin-bottom: 1em;
  padding-left: 0.25em;
  justify-self: start;
}

@media(min-width: 1000px) {
  .d-contents.d-contents-float {
    height: 0;
    grid-column-start: 1;
    grid-column-end: 4;
    justify-self: center;
    padding-right: 3em;
    padding-left: 2em;
  }
}

.d-contents nav h3 {
  font-size: 18px;
  margin-top: 0;
  margin-bottom: 1em;
}

.d-contents li {
  list-style-type: none
}

.d-contents nav > ul {
  padding-left: 0;
}

.d-contents ul {
  padding-left: 1em
}

.d-contents nav ul li {
  margin-top: 0.6em;
  margin-bottom: 0.2em;
}

.d-contents nav a {
  font-size: 13px;
  border-bottom: none;
  text-decoration: none
  color: rgba(0, 0, 0, 0.8);
}

.d-contents nav a:hover {
  text-decoration: underline solid rgba(0, 0, 0, 0.6)
}

.d-contents nav > ul > li > a {
  font-weight: 600;
}

.d-contents nav > ul > li > ul {
  font-weight: inherit;
}

.d-contents nav > ul > li > ul > li {
  margin-top: 0.2em;
}


.d-contents nav ul {
  margin-top: 0;
  margin-bottom: 0.25em;
}

.d-article-with-toc h2:nth-child(2) {
  margin-top: 0;
}


/* Figure */

.figure {
  position: relative;
  margin-bottom: 2.5em;
  margin-top: 1.5em;
}

.figure img {
  width: 100%;
}

.figure .caption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

.figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

.figure .caption a {
  color: rgba(0, 0, 0, 0.6);
}

.figure .caption b,
.figure .caption strong, {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

/* Citations */

d-article .citation {
  color: inherit;
  cursor: inherit;
}

div.hanging-indent{
  margin-left: 1em; text-indent: -1em;
}


/* Tweak 1000px media break to show more text */

@media(min-width: 1000px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }

  d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
  }
  figure .caption, .figure .caption, figure figcaption {
    font-size: 13px;
  }
}

@media(min-width: 1180px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 32px;
  }

  .grid {
    grid-column-gap: 32px;
  }
}


/* Get the citation styles for the appendix (not auto-injected on render since
   we do our own rendering of the citation appendix) */

d-appendix .citation-appendix,
.d-appendix .citation-appendix {
  font-size: 11px;
  line-height: 15px;
  border-left: 1px solid rgba(0, 0, 0, 0.1);
  padding-left: 18px;
  border: 1px solid rgba(0,0,0,0.1);
  background: rgba(0, 0, 0, 0.02);
  padding: 10px 18px;
  border-radius: 3px;
  color: rgba(150, 150, 150, 1);
  overflow: hidden;
  margin-top: -12px;
  white-space: pre-wrap;
  word-wrap: break-word;
}

/* Include appendix styles here so they can be overridden */

d-appendix {
  contain: layout style;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-top: 60px;
  margin-bottom: 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  color: rgba(0,0,0,0.5);
  padding-top: 60px;
  padding-bottom: 48px;
}

d-appendix h3 {
  grid-column: page-start / text-start;
  font-size: 15px;
  font-weight: 500;
  margin-top: 1em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.65);
}

d-appendix h3 + * {
  margin-top: 1em;
}

d-appendix ol {
  padding: 0 0 0 15px;
}

@media (min-width: 768px) {
  d-appendix ol {
    padding: 0 0 0 30px;
    margin-left: -30px;
  }
}

d-appendix li {
  margin-bottom: 1em;
}

d-appendix a {
  color: rgba(0, 0, 0, 0.6);
}

d-appendix > * {
  grid-column: text;
}

d-appendix > d-footnote-list,
d-appendix > d-citation-list,
d-appendix > distill-appendix {
  grid-column: screen;
}

/* Include footnote styles here so they can be overridden */

d-footnote-list {
  contain: layout style;
}

d-footnote-list > * {
  grid-column: text;
}

d-footnote-list a.footnote-backlink {
  color: rgba(0,0,0,0.3);
  padding-left: 0.5em;
}



/* Anchor.js */

.anchorjs-link {
  /*transition: all .25s linear; */
  text-decoration: none;
  border-bottom: none;
}
*:hover > .anchorjs-link {
  margin-left: -1.125em !important;
  text-decoration: none;
  border-bottom: none;
}

/* Social footer */

.social_footer {
  margin-top: 30px;
  margin-bottom: 0;
  color: rgba(0,0,0,0.67);
}

.disqus-comments {
  margin-right: 30px;
}

.disqus-comment-count {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  cursor: pointer;
}

#disqus_thread {
  margin-top: 30px;
}

.article-sharing a {
  border-bottom: none;
  margin-right: 8px;
}

.article-sharing a:hover {
  border-bottom: none;
}

.sidebar-section.subscribe {
  font-size: 12px;
  line-height: 1.6em;
}

.subscribe p {
  margin-bottom: 0.5em;
}


.article-footer .subscribe {
  font-size: 15px;
  margin-top: 45px;
}


.sidebar-section.custom {
  font-size: 12px;
  line-height: 1.6em;
}

.custom p {
  margin-bottom: 0.5em;
}

/* Styles for listing layout (hide title) */
.layout-listing d-title, .layout-listing .d-title {
  display: none;
}

/* Styles for posts lists (not auto-injected) */


.posts-with-sidebar {
  padding-left: 45px;
  padding-right: 45px;
}

.posts-list .description h2,
.posts-list .description p {
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
}

.posts-list .description h2 {
  font-weight: 700;
  border-bottom: none;
  padding-bottom: 0;
}

.posts-list h2.post-tag {
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
  padding-bottom: 12px;
}
.posts-list {
  margin-top: 60px;
  margin-bottom: 24px;
}

.posts-list .post-preview {
  text-decoration: none;
  overflow: hidden;
  display: block;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  padding: 24px 0;
}

.post-preview-last {
  border-bottom: none !important;
}

.posts-list .posts-list-caption {
  grid-column: screen;
  font-weight: 400;
}

.posts-list .post-preview h2 {
  margin: 0 0 6px 0;
  line-height: 1.2em;
  font-style: normal;
  font-size: 24px;
}

.posts-list .post-preview p {
  margin: 0 0 12px 0;
  line-height: 1.4em;
  font-size: 16px;
}

.posts-list .post-preview .thumbnail {
  box-sizing: border-box;
  margin-bottom: 24px;
  position: relative;
  max-width: 500px;
}
.posts-list .post-preview img {
  width: 100%;
  display: block;
}

.posts-list .metadata {
  font-size: 12px;
  line-height: 1.4em;
  margin-bottom: 18px;
}

.posts-list .metadata > * {
  display: inline-block;
}

.posts-list .metadata .publishedDate {
  margin-right: 2em;
}

.posts-list .metadata .dt-authors {
  display: block;
  margin-top: 0.3em;
  margin-right: 2em;
}

.posts-list .dt-tags {
  display: block;
  line-height: 1em;
}

.posts-list .dt-tags .dt-tag {
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0.3em 0.4em;
  margin-right: 0.2em;
  margin-bottom: 0.4em;
  font-size: 60%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

.posts-list img {
  opacity: 1;
}

.posts-list img[data-src] {
  opacity: 0;
}

.posts-more {
  clear: both;
}


.posts-sidebar {
  font-size: 16px;
}

.posts-sidebar h3 {
  font-size: 16px;
  margin-top: 0;
  margin-bottom: 0.5em;
  font-weight: 400;
  text-transform: uppercase;
}

.sidebar-section {
  margin-bottom: 30px;
}

.categories ul {
  list-style-type: none;
  margin: 0;
  padding: 0;
}

.categories li {
  color: rgba(0, 0, 0, 0.8);
  margin-bottom: 0;
}

.categories li>a {
  border-bottom: none;
}

.categories li>a:hover {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
}

.categories .active {
  font-weight: 600;
}

.categories .category-count {
  color: rgba(0, 0, 0, 0.4);
}


@media(min-width: 768px) {
  .posts-list .post-preview h2 {
    font-size: 26px;
  }
  .posts-list .post-preview .thumbnail {
    float: right;
    width: 30%;
    margin-bottom: 0;
  }
  .posts-list .post-preview .description {
    float: left;
    width: 45%;
  }
  .posts-list .post-preview .metadata {
    float: left;
    width: 20%;
    margin-top: 8px;
  }
  .posts-list .post-preview p {
    margin: 0 0 12px 0;
    line-height: 1.5em;
    font-size: 16px;
  }
  .posts-with-sidebar .posts-list {
    float: left;
    width: 75%;
  }
  .posts-with-sidebar .posts-sidebar {
    float: right;
    width: 20%;
    margin-top: 60px;
    padding-top: 24px;
    padding-bottom: 24px;
  }
}


/* Improve display for browsers without grid (IE/Edge <= 15) */

.downlevel {
  line-height: 1.6em;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  margin: 0;
}

.downlevel .d-title {
  padding-top: 6rem;
  padding-bottom: 1.5rem;
}

.downlevel .d-title h1 {
  font-size: 50px;
  font-weight: 700;
  line-height: 1.1em;
  margin: 0 0 0.5rem;
}

.downlevel .d-title p {
  font-weight: 300;
  font-size: 1.2rem;
  line-height: 1.55em;
  margin-top: 0;
}

.downlevel .d-byline {
  padding-top: 0.8em;
  padding-bottom: 0.8em;
  font-size: 0.8rem;
  line-height: 1.8em;
}

.downlevel .section-separator {
  border: none;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
}

.downlevel .d-article {
  font-size: 1.06rem;
  line-height: 1.7em;
  padding-top: 1rem;
  padding-bottom: 2rem;
}


.downlevel .d-appendix {
  padding-left: 0;
  padding-right: 0;
  max-width: none;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.5);
  padding-top: 40px;
  padding-bottom: 48px;
}

.downlevel .footnotes ol {
  padding-left: 13px;
}

.downlevel .base-grid,
.downlevel .distill-header,
.downlevel .d-title,
.downlevel .d-abstract,
.downlevel .d-article,
.downlevel .d-appendix,
.downlevel .distill-appendix,
.downlevel .d-byline,
.downlevel .d-footnote-list,
.downlevel .d-citation-list,
.downlevel .distill-footer,
.downlevel .appendix-bottom,
.downlevel .posts-container {
  padding-left: 40px;
  padding-right: 40px;
}

@media(min-width: 768px) {
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
  padding-left: 150px;
  padding-right: 150px;
  max-width: 900px;
}
}

.downlevel pre code {
  display: block;
  border-left: 2px solid rgba(0, 0, 0, .1);
  padding: 0 0 0 20px;
  font-size: 14px;
}

.downlevel code, .downlevel pre {
  color: black;
  background: none;
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

.downlevel .posts-list .post-preview {
  color: inherit;
}



</style>

<script type="application/javascript">

function is_downlevel_browser() {
  if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                 window.navigator.userAgent)) {
    return true;
  } else {
    return window.load_distill_framework === undefined;
  }
}

// show body when load is complete
function on_load_complete() {

  // add anchors
  if (window.anchors) {
    window.anchors.options.placement = 'left';
    window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
  }


  // set body to visible
  document.body.style.visibility = 'visible';

  // force redraw for leaflet widgets
  if (window.HTMLWidgets) {
    var maps = window.HTMLWidgets.findAll(".leaflet");
    $.each(maps, function(i, el) {
      var map = this.getMap();
      map.invalidateSize();
      map.eachLayer(function(layer) {
        if (layer instanceof L.TileLayer)
          layer.redraw();
      });
    });
  }

  // trigger 'shown' so htmlwidgets resize
  $('d-article').trigger('shown');
}

function init_distill() {

  init_common();

  // create front matter
  var front_matter = $('<d-front-matter></d-front-matter>');
  $('#distill-front-matter').wrap(front_matter);

  // create d-title
  $('.d-title').changeElementType('d-title');

  // create d-byline
  var byline = $('<d-byline></d-byline>');
  $('.d-byline').replaceWith(byline);

  // create d-article
  var article = $('<d-article></d-article>');
  $('.d-article').wrap(article).children().unwrap();

  // move posts container into article
  $('.posts-container').appendTo($('d-article'));

  // create d-appendix
  $('.d-appendix').changeElementType('d-appendix');

  // flag indicating that we have appendix items
  var appendix = $('.appendix-bottom').children('h3').length > 0;

  // replace footnotes with <d-footnote>
  $('.footnote-ref').each(function(i, val) {
    appendix = true;
    var href = $(this).attr('href');
    var id = href.replace('#', '');
    var fn = $('#' + id);
    var fn_p = $('#' + id + '>p');
    fn_p.find('.footnote-back').remove();
    var text = fn_p.html();
    var dtfn = $('<d-footnote></d-footnote>');
    dtfn.html(text);
    $(this).replaceWith(dtfn);
  });
  // remove footnotes
  $('.footnotes').remove();

  // move refs into #references-listing
  $('#references-listing').replaceWith($('#refs'));

  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    var id = $(this).attr('id');
    $('.d-contents a[href="#' + id + '"]').parent().remove();
    appendix = true;
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
  });

  // show d-appendix if we have appendix content
  $("d-appendix").css('display', appendix ? 'grid' : 'none');

  // localize layout chunks to just output
  $('.layout-chunk').each(function(i, val) {

    // capture layout
    var layout = $(this).attr('data-layout');

    // apply layout to markdown level block elements
    var elements = $(this).children().not('div.sourceCode, pre, script');
    elements.each(function(i, el) {
      var layout_div = $('<div class="' + layout + '"></div>');
      if (layout_div.hasClass('shaded')) {
        var shaded_content = $('<div class="shaded-content"></div>');
        $(this).wrap(shaded_content);
        $(this).parent().wrap(layout_div);
      } else {
        $(this).wrap(layout_div);
      }
    });


    // unwrap the layout-chunk div
    $(this).children().unwrap();
  });

  // remove code block used to force  highlighting css
  $('.distill-force-highlighting-css').parent().remove();

  // remove empty line numbers inserted by pandoc when using a
  // custom syntax highlighting theme
  $('code.sourceCode a:empty').remove();

  // load distill framework
  load_distill_framework();

  // wait for window.distillRunlevel == 4 to do post processing
  function distill_post_process() {

    if (!window.distillRunlevel || window.distillRunlevel < 4)
      return;

    // hide author/affiliations entirely if we have no authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
    if (!have_authors)
      $('d-byline').addClass('hidden');

    // article with toc class
    $('.d-contents').parent().addClass('d-article-with-toc');

    // strip links that point to #
    $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

    // add orcid ids
    $('.authors-affiliations').find('.author').each(function(i, el) {
      var orcid_id = front_matter.authors[i].orcidID;
      if (orcid_id) {
        var a = $('<a></a>');
        a.attr('href', 'https://orcid.org/' + orcid_id);
        var img = $('<img></img>');
        img.addClass('orcid-id');
        img.attr('alt', 'ORCID ID');
        img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
        a.append(img);
        $(this).append(a);
      }
    });

    // hide elements of author/affiliations grid that have no value
    function hide_byline_column(caption) {
      $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
    }

    // affiliations
    var have_affiliations = false;
    for (var i = 0; i<front_matter.authors.length; ++i) {
      var author = front_matter.authors[i];
      if (author.affiliation !== "&nbsp;") {
        have_affiliations = true;
        break;
      }
    }
    if (!have_affiliations)
      $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

    // published date
    if (!front_matter.publishedDate)
      hide_byline_column("Published");

    // document object identifier
    var doi = $('d-byline').find('h3:contains("DOI")');
    var doi_p = doi.next().empty();
    if (!front_matter.doi) {
      // if we have a citation and valid citationText then link to that
      if ($('#citation').length > 0 && front_matter.citationText) {
        doi.html('Citation');
        $('<a href="#citation"></a>')
          .text(front_matter.citationText)
          .appendTo(doi_p);
      } else {
        hide_byline_column("DOI");
      }
    } else {
      $('<a></a>')
         .attr('href', "https://doi.org/" + front_matter.doi)
         .html(front_matter.doi)
         .appendTo(doi_p);
    }

     // change plural form of authors/affiliations
    if (front_matter.authors.length === 1) {
      var grid = $('.authors-affiliations');
      grid.children('h3:contains("Authors")').text('Author');
      grid.children('h3:contains("Affiliations")').text('Affiliation');
    }

    // remove d-appendix and d-footnote-list local styles
    $('d-appendix > style:first-child').remove();
    $('d-footnote-list > style:first-child').remove();

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // clear polling timer
    clearInterval(tid);

    // show body now that everything is ready
    on_load_complete();
  }

  var tid = setInterval(distill_post_process, 50);
  distill_post_process();

}

function init_downlevel() {

  init_common();

   // insert hr after d-title
  $('.d-title').after($('<hr class="section-separator"/>'));

  // check if we have authors
  var front_matter = JSON.parse($("#distill-front-matter").html());
  var have_authors = front_matter.authors && front_matter.authors.length > 0;

  // manage byline/border
  if (!have_authors)
    $('.d-byline').remove();
  $('.d-byline').after($('<hr class="section-separator"/>'));
  $('.d-byline a').remove();

  // remove toc
  $('.d-contents').remove();

  // move appendix elements
  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
  });


  // inject headers into references and footnotes
  var refs_header = $('<h3></h3>');
  refs_header.text('References');
  $('#refs').prepend(refs_header);

  var footnotes_header = $('<h3></h3');
  footnotes_header.text('Footnotes');
  $('.footnotes').children('hr').first().replaceWith(footnotes_header);

  // move appendix-bottom entries to the bottom
  $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
  $('.appendix-bottom').remove();

  // remove appendix if it's empty
  if ($('.d-appendix').children().length === 0)
    $('.d-appendix').remove();

  // prepend separator above appendix
  $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

  // trim code
  $('pre>code').each(function(i, val) {
    $(this).html($.trim($(this).html()));
  });

  // move posts-container right before article
  $('.posts-container').insertBefore($('.d-article'));

  $('body').addClass('downlevel');

  on_load_complete();
}


function init_common() {

  // jquery plugin to change element types
  (function($) {
    $.fn.changeElementType = function(newType) {
      var attrs = {};

      $.each(this[0].attributes, function(idx, attr) {
        attrs[attr.nodeName] = attr.nodeValue;
      });

      this.replaceWith(function() {
        return $("<" + newType + "/>", attrs).append($(this).contents());
      });
    };
  })(jQuery);

  // prevent underline for linked images
  $('a > img').parent().css({'border-bottom' : 'none'});

  // mark non-body figures created by knitr chunks as 100% width
  $('.layout-chunk').each(function(i, val) {
    var figures = $(this).find('img, .html-widget');
    if ($(this).attr('data-layout') !== "l-body") {
      figures.css('width', '100%');
    } else {
      figures.css('max-width', '100%');
      figures.filter("[width]").each(function(i, val) {
        var fig = $(this);
        fig.css('width', fig.attr('width') + 'px');
      });

    }
  });

  // auto-append index.html to post-preview links in file: protocol
  // and in rstudio ide preview
  $('.post-preview').each(function(i, val) {
    if (window.location.protocol === "file:")
      $(this).attr('href', $(this).attr('href') + "index.html");
  });

  // get rid of index.html references in header
  if (window.location.protocol !== "file:") {
    $('.distill-site-header a[href]').each(function(i,val) {
      $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
    });
  }

  // add class to pandoc style tables
  $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
  $('.kable-table').children('table').addClass('pandoc-table');

  // add figcaption style to table captions
  $('caption').parent('table').addClass("figcaption");

  // initialize posts list
  if (window.init_posts_list)
    window.init_posts_list();

  // implmement disqus comment link
  $('.disqus-comment-count').click(function() {
    window.headroom_prevent_pin = true;
    $('#disqus_thread').toggleClass('hidden');
    if (!$('#disqus_thread').hasClass('hidden')) {
      var offset = $(this).offset();
      $(window).resize();
      $('html, body').animate({
        scrollTop: offset.top - 35
      });
    }
  });
}

document.addEventListener('DOMContentLoaded', function() {
  if (is_downlevel_browser())
    init_downlevel();
  else
    window.addEventListener('WebComponentsReady', init_distill);
});

</script>

<!--/radix_placeholder_distill-->
  <script src="../../site_libs/header-attrs-2.5/header-attrs.js"></script>
  <script src="../../site_libs/jquery-1.11.3/jquery.min.js"></script>
  <script src="../../site_libs/anchor-4.2.2/anchor.min.js"></script>
  <script src="../../site_libs/bowser-1.9.3/bowser.min.js"></script>
  <script src="../../site_libs/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="../../site_libs/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-20375833-3"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-20375833-3');
</script>
<!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Representation learning with MMD-VAE","description":"Like GANs, variational autoencoders (VAEs) are often used to generate images. However, VAEs add an additional promise: namely, to model an underlying latent space. Here, we first look at a typical implementation that maximizes the evidence lower bound. Then, we compare it to one of the more recent competitors, MMD-VAE, from the Info-VAE (information maximizing VAE) family.","authors":[{"author":"Sigrid Keydana","authorURL":"#","affiliation":"RStudio","affiliationURL":"https://www.rstudio.com/","orcidID":""}],"publishedDate":"2018-10-22T00:00:00.000+00:00","citationText":"Keydana, 2018"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<header class="header header--fixed" role="banner">
<nav class="distill-site-nav distill-site-header">
<div class="nav-left">
<span class="logo">
<img src="../../images/rstudio.png"/>
</span>
<a href="../../index.html" class="title">AI Blog</a>
<input id="distill-search" class="nav-search hidden" type="text" placeholder="Search..."/>
</div>
<div class="nav-right">
<a href="../../index.html">Home</a>
<a href="../../gallery.html">Gallery</a>
<a href="../../about.html">About</a>
<a href="../../contributing.html">Contributing</a>
<a href="../../index.xml">
<i class="fa fa-rss" aria-hidden="true"></i>
</a>
<a href="javascript:void(0);" class="nav-toggle">&#9776;</a>
</div>
</nav>
</header>
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Representation learning with MMD-VAE</h1>
<!--radix_placeholder_categories-->
<div class="dt-tags">
  <a href="../../index.html#category:TensorFlow/Keras" class="dt-tag">TensorFlow/Keras</a>
  <a href="../../index.html#category:Unsupervised_Learning" class="dt-tag">Unsupervised Learning</a>
  <a href="../../index.html#category:Image_Recognition_&amp;_Image_Processing" class="dt-tag">Image Recognition &amp; Image Processing</a>
</div>
<!--/radix_placeholder_categories-->
<p><p>Like GANs, variational autoencoders (VAEs) are often used to generate images. However, VAEs add an additional promise: namely, to model an underlying latent space. Here, we first look at a typical implementation that maximizes the evidence lower bound. Then, we compare it to one of the more recent competitors, MMD-VAE, from the Info-VAE (information maximizing VAE) family.</p></p>
</div>

<div class="d-byline">
  Sigrid Keydana  (RStudio)<a href="https://www.rstudio.com/" class="uri">https://www.rstudio.com/</a>
  
<br/>10-22-2018
</div>

<div class="d-article">
<p>Recently, we showed how to <a href="https://blogs.rstudio.com/tensorflow/posts/2018-08-26-eager-dcgan/">generate images</a> using generative adversarial networks (GANs). GANs may yield amazing results, but the contract there basically is: what you see is what you get. Sometimes this may be all we want. In other cases, we may be more interested in actually modelling a domain. We dont just want to generate realistic-looking samples - we want our samples to be located at specific coordinates in domain space.</p>
<p>For example, imagine our domain to be the space of facial expressions. Then our latent space might be conceived as two-dimensional: In accordance with underlying emotional states, expressions vary on a positive-negative scale. At the same time, they vary in intensity. Now if we trained a VAE on a set of facial expressions adequately covering the ranges, and it did in fact discover our hypothesized dimensions, we could then use it to generate previously-nonexisting incarnations of points (faces, that is) in latent space.</p>
<p>Variational autoencoders are similar to probabilistic graphical models in that they assume a latent space that is responsible for the observations, but unobservable. They are similar to plain autoencoders in that they compress, and then decompress again, the input domain. In contrast to plain autoencoders though, the crucial point here is to devise a loss function that allows to obtain informative representations in latent space.</p>
<h2 id="in-a-nutshell">In a nutshell</h2>
<p>In standard VAEs <span class="citation" data-cites="KingmaW13">(Kingma and Welling <a href="#ref-KingmaW13" role="doc-biblioref">2013</a>)</span>, the objective is to maximize the evidence lower bound (ELBO):</p>
<p><span class="math display">\[ELBO\ = \ E[log\ p(x|z)]\ -\ KL(q(z)||p(z))\]</span></p>
<p>In plain words and expressed in terms of how we use it in practice, the first component is the <em>reconstruction loss</em> we also see in plain (non-variational) autoencoders. The second is the Kullback-Leibler divergence between a prior imposed on the latent space (typically, a standard normal distribution) and the representation of latent space as learned from the data.</p>
<aside>
For a well-written and intuitive introduction to VAEs, including the why and how of their optimization, see this <a href="https://arxiv.org/abs/1606.05908">Tutorial on variational autoencoders</a> <span class="citation" data-cites="2016arXiv160605908D">(Doersch <a href="#ref-2016arXiv160605908D" role="doc-biblioref">2016</a>)</span>.
</aside>
<p>A major criticism regarding the traditional VAE loss is that it results in uninformative latent space. Alternatives include <span class="math inline">\(\beta\)</span>-VAE<span class="citation" data-cites="2018arXiv180403599B">(Burgess et al. <a href="#ref-2018arXiv180403599B" role="doc-biblioref">2018</a>)</span>, Info-VAE <span class="citation" data-cites="ZhaoSE17b">(Zhao, Song, and Ermon <a href="#ref-ZhaoSE17b" role="doc-biblioref">2017</a>)</span>, and more. The MMD-VAE<span class="citation" data-cites="ZhaoSE17b">(Zhao, Song, and Ermon <a href="#ref-ZhaoSE17b" role="doc-biblioref">2017</a>)</span> implemented below is a subtype of Info-VAE that instead of making each representation in latent space as similar as possible to the prior, coerces the respective <em>distributions</em> to be as close as possible. Here MMD stands for <em>maximum mean discrepancy</em>, a similarity measure for distributions based on matching their respective moments. We explain this in more detail below.</p>
<aside>
The main author of the paper<span class="citation" data-cites="ZhaoSE17b">(Zhao, Song, and Ermon <a href="#ref-ZhaoSE17b" role="doc-biblioref">2017</a>)</span> has a <a href="http://szhao.me/2017/06/10/a-tutorial-on-mmd-variational-autoencoders.html">tutorial</a> on his website explaining the reasons behind this choice of cost function in a very accessible way.
</aside>
<h2 id="our-objective-today">Our objective today</h2>
<p>In this post, we are first going to implement a standard VAE that strives to maximize the ELBO. Then, we compare its performance to that of an Info-VAE using the MMD loss.</p>
<p>Our focus will be on inspecting the latent spaces and see if, and how, they differ as a consequence of the optimization criteria used.</p>
<p>The domain were going to model will be glamorous (fashion!), but for the sake of manageability, confined to size 28 x 28: Well compress and reconstruct images from the <a href="https://github.com/zalandoresearch/fashion-mnist">Fashion MNIST</a> dataset that has been developed as a drop-in to MNIST.</p>
<h2 id="a-standard-variational-autoencoder">A standard variational autoencoder</h2>
<p>Seeing we havent used TensorFlow eager execution for some weeks, well do the model in an eager way. If youre new to eager execution, dont worry: As every new technique, it needs some getting accustomed to, but youll quickly find that many tasks are made easier if you use it. A simple yet complete, template-like example is available as part of the <a href="https://tensorflow.rstudio.com/tutorials/advanced/">Keras documentation</a><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<aside>
For interesting applications using eager execution in combination with Keras, ranging from machine translation to neural style transfer, see the recent posts in the <a href="https://blogs.rstudio.com/tensorflow/#Eager">Eager category</a> on this blog.
</aside>
<h4 id="setup-and-data-preparation">Setup and data preparation</h4>
<p>As usual, we start by making sure were using the TensorFlow implementation of Keras and enabling eager execution. Besides <code>tensorflow</code> and <code>keras</code>, we also load <code>tfdatasets</code> for use in data streaming.</p>
<p>By the way: No need to copy-paste any of the below code snippets. The two approaches are available among our Keras examples, namely, as <a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/eager_cvae.R">eager_cvae.R</a> and <a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/mmd_cvae.R">mmd_cvae.R</a>.</p>
<aside>
You might find it interesting to compare non-eager Keras code implementing a variational autoencoder: see <a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/variational_autoencoder_deconv.R">variational_autoencoder_deconv.R</a>.
</aside>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='co'># the following 5 lines have to be executed in this order</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'>keras</span><span class='op'>)</span>
<span class='fu'>use_implementation</span><span class='op'>(</span><span class='st'>"tensorflow"</span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'>tensorflow</span><span class='op'>)</span>
<span class='fu'>tfe_enable_eager_execution</span><span class='op'>(</span>device_policy <span class='op'>=</span> <span class='st'>"silent"</span><span class='op'>)</span>

<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'>tfdatasets</span><span class='op'>)</span>

<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://dplyr.tidyverse.org'>dplyr</a></span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'>ggplot2</span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://github.com/tidyverse/glue'>glue</a></span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>The data comes conveniently with <code>keras</code>, all we need to do is the usual normalization and reshaping.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>fashion</span> <span class='op'>&lt;-</span> <span class='fu'>dataset_fashion_mnist</span><span class='op'>(</span><span class='op'>)</span>

<span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='va'>train_images</span>, <span class='va'>train_labels</span><span class='op'>)</span> <span class='op'>%&lt;-%</span> <span class='va'>fashion</span><span class='op'>$</span><span class='va'>train</span>
<span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='va'>test_images</span>, <span class='va'>test_labels</span><span class='op'>)</span> <span class='op'>%&lt;-%</span> <span class='va'>fashion</span><span class='op'>$</span><span class='va'>test</span>

<span class='va'>train_x</span> <span class='op'>&lt;-</span> <span class='va'>train_images</span> <span class='op'>%&gt;%</span>
  <span class='fu'>`/`</span><span class='op'>(</span><span class='fl'>255</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>k_reshape</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>60000</span>, <span class='fl'>28</span>, <span class='fl'>28</span>, <span class='fl'>1</span><span class='op'>)</span><span class='op'>)</span>

<span class='va'>test_x</span> <span class='op'>&lt;-</span> <span class='va'>test_images</span> <span class='op'>%&gt;%</span> <span class='fu'>`/`</span><span class='op'>(</span><span class='fl'>255</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>k_reshape</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>10000</span>, <span class='fl'>28</span>, <span class='fl'>28</span>, <span class='fl'>1</span><span class='op'>)</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>What do we need the test set for, given we are going to train an unsupervised (a better term being: <em>semi-supervised</em>) model? Well use it to see how (previously unknown) data points cluster together in latent space.</p>
<p>Now prepare for streaming the data to <code>keras</code>:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>buffer_size</span> <span class='op'>&lt;-</span> <span class='fl'>60000</span>
<span class='va'>batch_size</span> <span class='op'>&lt;-</span> <span class='fl'>100</span>
<span class='va'>batches_per_epoch</span> <span class='op'>&lt;-</span> <span class='va'>buffer_size</span> <span class='op'>/</span> <span class='va'>batch_size</span>

<span class='va'>train_dataset</span> <span class='op'>&lt;-</span> <span class='fu'>tensor_slices_dataset</span><span class='op'>(</span><span class='va'>train_x</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>dataset_shuffle</span><span class='op'>(</span><span class='va'>buffer_size</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>dataset_batch</span><span class='op'>(</span><span class='va'>batch_size</span><span class='op'>)</span>

<span class='va'>test_dataset</span> <span class='op'>&lt;-</span> <span class='fu'>tensor_slices_dataset</span><span class='op'>(</span><span class='va'>test_x</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>dataset_batch</span><span class='op'>(</span><span class='fl'>10000</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>Next up is defining the model.</p>
<h4 id="encoder-decoder-model">Encoder-decoder model</h4>
<p><em>The model</em> really is two models: the encoder and the decoder. As well see shortly, in the standard version of the VAE there is a third component in between, performing the so-called <em>reparameterization trick</em>.</p>
<p>The encoder is a <a href="https://tensorflow.rstudio.com/keras/articles/custom_models.html">custom model</a>, comprised of two convolutional layers and a dense layer. It returns the output of the dense layer split into two parts, one storing the mean of the latent variables, the other their variance.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>latent_dim</span> <span class='op'>&lt;-</span> <span class='fl'>2</span>

<span class='va'>encoder_model</span> <span class='op'>&lt;-</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>name</span> <span class='op'>=</span> <span class='cn'>NULL</span><span class='op'>)</span> <span class='op'>{</span>
  
  <span class='fu'>keras_model_custom</span><span class='op'>(</span>name <span class='op'>=</span> <span class='va'>name</span>, <span class='kw'>function</span><span class='op'>(</span><span class='va'>self</span><span class='op'>)</span> <span class='op'>{</span>
    <span class='va'>self</span><span class='op'>$</span><span class='va'>conv1</span> <span class='op'>&lt;-</span>
      <span class='fu'>layer_conv_2d</span><span class='op'>(</span>
        filters <span class='op'>=</span> <span class='fl'>32</span>,
        kernel_size <span class='op'>=</span> <span class='fl'>3</span>,
        strides <span class='op'>=</span> <span class='fl'>2</span>,
        activation <span class='op'>=</span> <span class='st'>"relu"</span>
      <span class='op'>)</span>
    <span class='va'>self</span><span class='op'>$</span><span class='va'>conv2</span> <span class='op'>&lt;-</span>
      <span class='fu'>layer_conv_2d</span><span class='op'>(</span>
        filters <span class='op'>=</span> <span class='fl'>64</span>,
        kernel_size <span class='op'>=</span> <span class='fl'>3</span>,
        strides <span class='op'>=</span> <span class='fl'>2</span>,
        activation <span class='op'>=</span> <span class='st'>"relu"</span>
      <span class='op'>)</span>
    <span class='va'>self</span><span class='op'>$</span><span class='va'>flatten</span> <span class='op'>&lt;-</span> <span class='fu'>layer_flatten</span><span class='op'>(</span><span class='op'>)</span>
    <span class='va'>self</span><span class='op'>$</span><span class='va'>dense</span> <span class='op'>&lt;-</span> <span class='fu'>layer_dense</span><span class='op'>(</span>units <span class='op'>=</span> <span class='fl'>2</span> <span class='op'>*</span> <span class='va'>latent_dim</span><span class='op'>)</span>
    
    <span class='kw'>function</span> <span class='op'>(</span><span class='va'>x</span>, <span class='va'>mask</span> <span class='op'>=</span> <span class='cn'>NULL</span><span class='op'>)</span> <span class='op'>{</span>
      <span class='va'>x</span> <span class='op'>%&gt;%</span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>conv1</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>conv2</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>flatten</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>dense</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
        <span class='va'>tf</span><span class='op'>$</span><span class='fu'>split</span><span class='op'>(</span>num_or_size_splits <span class='op'>=</span> <span class='fl'>2L</span>, axis <span class='op'>=</span> <span class='fl'>1L</span><span class='op'>)</span> 
    <span class='op'>}</span>
  <span class='op'>}</span><span class='op'>)</span>
<span class='op'>}</span>
</code></pre>
</div>
</div>
<p>We choose the latent space to be of dimension 2 - just because that makes visualization easy. With more complex data, you will probably benefit from choosing a higher dimensionality here.</p>
<p>So the encoder compresses real data into estimates of mean and variance of the latent space. We then indirectly sample from this distribution (the so-called <em>reparameterization trick</em>):</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>reparameterize</span> <span class='op'>&lt;-</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>mean</span>, <span class='va'>logvar</span><span class='op'>)</span> <span class='op'>{</span>
  <span class='va'>eps</span> <span class='op'>&lt;-</span> <span class='fu'>k_random_normal</span><span class='op'>(</span>shape <span class='op'>=</span> <span class='va'>mean</span><span class='op'>$</span><span class='va'>shape</span>, dtype <span class='op'>=</span> <span class='va'>tf</span><span class='op'>$</span><span class='va'>float64</span><span class='op'>)</span>
  <span class='va'>eps</span> <span class='op'>*</span> <span class='fu'>k_exp</span><span class='op'>(</span><span class='va'>logvar</span> <span class='op'>*</span> <span class='fl'>0.5</span><span class='op'>)</span> <span class='op'>+</span> <span class='va'>mean</span>
<span class='op'>}</span>
</code></pre>
</div>
</div>
<p>The sampled values will serve as input to the decoder, who will attempt to map them back to the original space. The decoder is basically a sequence of transposed convolutions, upsampling until we reach a resolution of 28x28.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>decoder_model</span> <span class='op'>&lt;-</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>name</span> <span class='op'>=</span> <span class='cn'>NULL</span><span class='op'>)</span> <span class='op'>{</span>
  
  <span class='fu'>keras_model_custom</span><span class='op'>(</span>name <span class='op'>=</span> <span class='va'>name</span>, <span class='kw'>function</span><span class='op'>(</span><span class='va'>self</span><span class='op'>)</span> <span class='op'>{</span>
    
    <span class='va'>self</span><span class='op'>$</span><span class='va'>dense</span> <span class='op'>&lt;-</span> <span class='fu'>layer_dense</span><span class='op'>(</span>units <span class='op'>=</span> <span class='fl'>7</span> <span class='op'>*</span> <span class='fl'>7</span> <span class='op'>*</span> <span class='fl'>32</span>, activation <span class='op'>=</span> <span class='st'>"relu"</span><span class='op'>)</span>
    <span class='va'>self</span><span class='op'>$</span><span class='va'>reshape</span> <span class='op'>&lt;-</span> <span class='fu'>layer_reshape</span><span class='op'>(</span>target_shape <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>7</span>, <span class='fl'>7</span>, <span class='fl'>32</span><span class='op'>)</span><span class='op'>)</span>
    <span class='va'>self</span><span class='op'>$</span><span class='va'>deconv1</span> <span class='op'>&lt;-</span>
      <span class='fu'>layer_conv_2d_transpose</span><span class='op'>(</span>
        filters <span class='op'>=</span> <span class='fl'>64</span>,
        kernel_size <span class='op'>=</span> <span class='fl'>3</span>,
        strides <span class='op'>=</span> <span class='fl'>2</span>,
        padding <span class='op'>=</span> <span class='st'>"same"</span>,
        activation <span class='op'>=</span> <span class='st'>"relu"</span>
      <span class='op'>)</span>
    <span class='va'>self</span><span class='op'>$</span><span class='va'>deconv2</span> <span class='op'>&lt;-</span>
      <span class='fu'>layer_conv_2d_transpose</span><span class='op'>(</span>
        filters <span class='op'>=</span> <span class='fl'>32</span>,
        kernel_size <span class='op'>=</span> <span class='fl'>3</span>,
        strides <span class='op'>=</span> <span class='fl'>2</span>,
        padding <span class='op'>=</span> <span class='st'>"same"</span>,
        activation <span class='op'>=</span> <span class='st'>"relu"</span>
      <span class='op'>)</span>
    <span class='va'>self</span><span class='op'>$</span><span class='va'>deconv3</span> <span class='op'>&lt;-</span>
      <span class='fu'>layer_conv_2d_transpose</span><span class='op'>(</span>
        filters <span class='op'>=</span> <span class='fl'>1</span>,
        kernel_size <span class='op'>=</span> <span class='fl'>3</span>,
        strides <span class='op'>=</span> <span class='fl'>1</span>,
        padding <span class='op'>=</span> <span class='st'>"same"</span>
      <span class='op'>)</span>
    
    <span class='kw'>function</span> <span class='op'>(</span><span class='va'>x</span>, <span class='va'>mask</span> <span class='op'>=</span> <span class='cn'>NULL</span><span class='op'>)</span> <span class='op'>{</span>
      <span class='va'>x</span> <span class='op'>%&gt;%</span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>dense</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>reshape</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>deconv1</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>deconv2</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>deconv3</span><span class='op'>(</span><span class='op'>)</span>
    <span class='op'>}</span>
  <span class='op'>}</span><span class='op'>)</span>
<span class='op'>}</span>
</code></pre>
</div>
</div>
<p>Note how the final deconvolution does not have the sigmoid activation you might have expected. This is because we will be using <code>tf$nn$sigmoid_cross_entropy_with_logits</code> when calculating the loss.</p>
<p>Speaking of losses, lets inspect them now.</p>
<h4 id="loss-calculations">Loss calculations</h4>
<p>One way to implement the VAE loss is combining reconstruction loss (cross entropy, in the present case) and Kullback-Leibler divergence. In Keras, the latter is available directly as <code>loss_kullback_leibler_divergence</code>.</p>
<p>Here, we follow a recent <a href="https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/cvae.ipynb">Google Colaboratory notebook</a> in batch-estimating the complete ELBO instead (instead of just estimating reconstruction loss and computing the KL-divergence analytically):</p>
<p><span class="math display">\[ELBO \ batch \ estimate = log\ p(x_{batch}|z_{sampled})+log\ p(z)log\ q(z_{sampled}|x_{batch})\]</span></p>
<p>Calculation of the normal loglikelihood is packaged into a function so we can reuse it during the training loop.</p>
<aside>
Note that were calculating with the log of the variance, instead of the variance, for reasons of numerical stability.
</aside>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>normal_loglik</span> <span class='op'>&lt;-</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>sample</span>, <span class='va'>mean</span>, <span class='va'>logvar</span>, <span class='va'>reduce_axis</span> <span class='op'>=</span> <span class='fl'>2</span><span class='op'>)</span> <span class='op'>{</span>
  <span class='va'>loglik</span> <span class='op'>&lt;-</span> <span class='fu'>k_constant</span><span class='op'>(</span><span class='fl'>0.5</span>, dtype <span class='op'>=</span> <span class='va'>tf</span><span class='op'>$</span><span class='va'>float64</span><span class='op'>)</span> <span class='op'>*</span>
    <span class='op'>(</span><span class='fu'>k_log</span><span class='op'>(</span><span class='fl'>2</span> <span class='op'>*</span> <span class='fu'>k_constant</span><span class='op'>(</span><span class='va'>pi</span>, dtype <span class='op'>=</span> <span class='va'>tf</span><span class='op'>$</span><span class='va'>float64</span><span class='op'>)</span><span class='op'>)</span> <span class='op'>+</span>
    <span class='va'>logvar</span> <span class='op'>+</span>
    <span class='fu'>k_exp</span><span class='op'>(</span><span class='op'>-</span><span class='va'>logvar</span><span class='op'>)</span> <span class='op'>*</span> <span class='op'>(</span><span class='va'>sample</span> <span class='op'>-</span> <span class='va'>mean</span><span class='op'>)</span> <span class='op'>^</span> <span class='fl'>2</span><span class='op'>)</span>
  <span class='op'>-</span> <span class='fu'>k_sum</span><span class='op'>(</span><span class='va'>loglik</span>, axis <span class='op'>=</span> <span class='va'>reduce_axis</span><span class='op'>)</span>
<span class='op'>}</span>
</code></pre>
</div>
</div>
<p>Peeking ahead some, during training we will compute the above as follows.</p>
<p>First,</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>crossentropy_loss</span> <span class='op'>&lt;-</span> <span class='va'>tf</span><span class='op'>$</span><span class='va'>nn</span><span class='op'>$</span><span class='fu'>sigmoid_cross_entropy_with_logits</span><span class='op'>(</span>
  logits <span class='op'>=</span> <span class='va'>preds</span>,
  labels <span class='op'>=</span> <span class='va'>x</span>
<span class='op'>)</span>
<span class='va'>logpx_z</span> <span class='op'>&lt;-</span> <span class='op'>-</span> <span class='fu'>k_sum</span><span class='op'>(</span><span class='va'>crossentropy_loss</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>yields <span class="math inline">\(log \ p(x|z)\)</span>, the loglikelihood of the reconstructed samples given values sampled from latent space (a.k.a. reconstruction loss).</p>
<p>Then,</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>logpz</span> <span class='op'>&lt;-</span> <span class='fu'>normal_loglik</span><span class='op'>(</span>
  <span class='va'>z</span>,
  <span class='fu'>k_constant</span><span class='op'>(</span><span class='fl'>0</span>, dtype <span class='op'>=</span> <span class='va'>tf</span><span class='op'>$</span><span class='va'>float64</span><span class='op'>)</span>,
  <span class='fu'>k_constant</span><span class='op'>(</span><span class='fl'>0</span>, dtype <span class='op'>=</span> <span class='va'>tf</span><span class='op'>$</span><span class='va'>float64</span><span class='op'>)</span>
<span class='op'>)</span>
</code></pre>
</div>
</div>
<p>gives <span class="math inline">\(log \ p(z)\)</span>, the prior loglikelihood of <span class="math inline">\(z\)</span>. The prior is assumed to be standard normal, as is most often the case with VAEs.</p>
<p>Finally,</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>logqz_x</span> <span class='op'>&lt;-</span> <span class='fu'>normal_loglik</span><span class='op'>(</span><span class='va'>z</span>, <span class='va'>mean</span>, <span class='va'>logvar</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>vields <span class="math inline">\(log \ q(z|x)\)</span>, the loglikelihood of the samples <span class="math inline">\(z\)</span> given mean and variance computed from the observed samples <span class="math inline">\(x\)</span>.</p>
<p>From these three components, we will compute the final loss as</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>loss</span> <span class='op'>&lt;-</span> <span class='op'>-</span><span class='fu'>k_mean</span><span class='op'>(</span><span class='va'>logpx_z</span> <span class='op'>+</span> <span class='va'>logpz</span> <span class='op'>-</span> <span class='va'>logqz_x</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>After this peaking ahead, lets quickly finish the setup so we get ready for training.</p>
<h4 id="final-setup">Final setup</h4>
<p>Besides the loss, we need an optimizer that will strive to diminish it.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>optimizer</span> <span class='op'>&lt;-</span> <span class='va'>tf</span><span class='op'>$</span><span class='va'>train</span><span class='op'>$</span><span class='fu'>AdamOptimizer</span><span class='op'>(</span><span class='fl'>1e-4</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>We instantiate our models </p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>encoder</span> <span class='op'>&lt;-</span> <span class='fu'>encoder_model</span><span class='op'>(</span><span class='op'>)</span>
<span class='va'>decoder</span> <span class='op'>&lt;-</span> <span class='fu'>decoder_model</span><span class='op'>(</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>and set up checkpointing, so we can later restore trained weights.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>checkpoint_dir</span> <span class='op'>&lt;-</span> <span class='st'>"./checkpoints_cvae"</span>
<span class='va'>checkpoint_prefix</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/base/file.path.html'>file.path</a></span><span class='op'>(</span><span class='va'>checkpoint_dir</span>, <span class='st'>"ckpt"</span><span class='op'>)</span>
<span class='va'>checkpoint</span> <span class='op'>&lt;-</span> <span class='va'>tf</span><span class='op'>$</span><span class='va'>train</span><span class='op'>$</span><span class='fu'>Checkpoint</span><span class='op'>(</span>
  optimizer <span class='op'>=</span> <span class='va'>optimizer</span>,
  encoder <span class='op'>=</span> <span class='va'>encoder</span>,
  decoder <span class='op'>=</span> <span class='va'>decoder</span>
<span class='op'>)</span>
</code></pre>
</div>
</div>
<p>From the training loop, we will, in certain intervals, also call three functions not reproduced here (but available in the <a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/eager_vae.R">code example</a>): <code>generate_random_clothes</code>, used to generate clothes from random samples from the latent space; <code>show_latent_space</code>, that displays the complete test set in latent (2-dimensional, thus easily visualizable) space; and <code>show_grid</code>, that generates clothes according to input values systematically spaced out in a grid.</p>
<p>Lets start training! Actually, before we do that, lets have a look at what these functions display <em>before</em> any training: Instead of clothes, we see random pixels. Latent space has no structure. And different types of clothes do not cluster together in latent space.</p>
<p><img src="images/c_epoch_0.png" style="width:100.0%" /></p>
<h4 id="training-loop">Training loop</h4>
<p>Were training for 50 epochs here. For each epoch, we loop over the training set in batches. For each batch, we follow the usual eager execution flow: Inside the context of a <code>GradientTape</code>, apply the model and calculate the current loss; then outside this context calculate the gradients and let the optimizer perform backprop.</p>
<p>Whats special here is that we have two models that both need their gradients calculated and weights adjusted. This can be taken care of by a single gradient tape, provided we create it <code>persistent</code>.</p>
<p>After each epoch, we save current weights and every ten epochs, we also save plots for later inspection.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>num_epochs</span> <span class='op'>&lt;-</span> <span class='fl'>50</span>

<span class='kw'>for</span> <span class='op'>(</span><span class='va'>epoch</span> <span class='kw'>in</span> <span class='fu'><a href='https://rdrr.io/r/base/seq.html'>seq_len</a></span><span class='op'>(</span><span class='va'>num_epochs</span><span class='op'>)</span><span class='op'>)</span> <span class='op'>{</span>
  <span class='va'>iter</span> <span class='op'>&lt;-</span> <span class='fu'>make_iterator_one_shot</span><span class='op'>(</span><span class='va'>train_dataset</span><span class='op'>)</span>
  
  <span class='va'>total_loss</span> <span class='op'>&lt;-</span> <span class='fl'>0</span>
  <span class='va'>logpx_z_total</span> <span class='op'>&lt;-</span> <span class='fl'>0</span>
  <span class='va'>logpz_total</span> <span class='op'>&lt;-</span> <span class='fl'>0</span>
  <span class='va'>logqz_x_total</span> <span class='op'>&lt;-</span> <span class='fl'>0</span>
  
  <span class='fu'>until_out_of_range</span><span class='op'>(</span><span class='op'>{</span>
    <span class='va'>x</span> <span class='op'>&lt;-</span>  <span class='fu'>iterator_get_next</span><span class='op'>(</span><span class='va'>iter</span><span class='op'>)</span>
    
    <span class='fu'><a href='https://rdrr.io/r/base/with.html'>with</a></span><span class='op'>(</span><span class='va'>tf</span><span class='op'>$</span><span class='fu'>GradientTape</span><span class='op'>(</span>persistent <span class='op'>=</span> <span class='cn'>TRUE</span><span class='op'>)</span> <span class='op'>%as%</span> <span class='va'>tape</span>, <span class='op'>{</span>
      
      <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='va'>mean</span>, <span class='va'>logvar</span><span class='op'>)</span> <span class='op'>%&lt;-%</span> <span class='fu'>encoder</span><span class='op'>(</span><span class='va'>x</span><span class='op'>)</span>
      <span class='va'>z</span> <span class='op'>&lt;-</span> <span class='fu'>reparameterize</span><span class='op'>(</span><span class='va'>mean</span>, <span class='va'>logvar</span><span class='op'>)</span>
      <span class='va'>preds</span> <span class='op'>&lt;-</span> <span class='fu'>decoder</span><span class='op'>(</span><span class='va'>z</span><span class='op'>)</span>
      
      <span class='va'>crossentropy_loss</span> <span class='op'>&lt;-</span>
        <span class='va'>tf</span><span class='op'>$</span><span class='va'>nn</span><span class='op'>$</span><span class='fu'>sigmoid_cross_entropy_with_logits</span><span class='op'>(</span>logits <span class='op'>=</span> <span class='va'>preds</span>, labels <span class='op'>=</span> <span class='va'>x</span><span class='op'>)</span>
      <span class='va'>logpx_z</span> <span class='op'>&lt;-</span>
        <span class='op'>-</span> <span class='fu'>k_sum</span><span class='op'>(</span><span class='va'>crossentropy_loss</span><span class='op'>)</span>
      <span class='va'>logpz</span> <span class='op'>&lt;-</span>
        <span class='fu'>normal_loglik</span><span class='op'>(</span><span class='va'>z</span>,
                      <span class='fu'>k_constant</span><span class='op'>(</span><span class='fl'>0</span>, dtype <span class='op'>=</span> <span class='va'>tf</span><span class='op'>$</span><span class='va'>float64</span><span class='op'>)</span>,
                      <span class='fu'>k_constant</span><span class='op'>(</span><span class='fl'>0</span>, dtype <span class='op'>=</span> <span class='va'>tf</span><span class='op'>$</span><span class='va'>float64</span><span class='op'>)</span>
        <span class='op'>)</span>
      <span class='va'>logqz_x</span> <span class='op'>&lt;-</span> <span class='fu'>normal_loglik</span><span class='op'>(</span><span class='va'>z</span>, <span class='va'>mean</span>, <span class='va'>logvar</span><span class='op'>)</span>
      <span class='va'>loss</span> <span class='op'>&lt;-</span> <span class='op'>-</span><span class='fu'>k_mean</span><span class='op'>(</span><span class='va'>logpx_z</span> <span class='op'>+</span> <span class='va'>logpz</span> <span class='op'>-</span> <span class='va'>logqz_x</span><span class='op'>)</span>
      
    <span class='op'>}</span><span class='op'>)</span>

    <span class='va'>total_loss</span> <span class='op'>&lt;-</span> <span class='va'>total_loss</span> <span class='op'>+</span> <span class='va'>loss</span>
    <span class='va'>logpx_z_total</span> <span class='op'>&lt;-</span> <span class='va'>tf</span><span class='op'>$</span><span class='fu'>reduce_mean</span><span class='op'>(</span><span class='va'>logpx_z</span><span class='op'>)</span> <span class='op'>+</span> <span class='va'>logpx_z_total</span>
    <span class='va'>logpz_total</span> <span class='op'>&lt;-</span> <span class='va'>tf</span><span class='op'>$</span><span class='fu'>reduce_mean</span><span class='op'>(</span><span class='va'>logpz</span><span class='op'>)</span> <span class='op'>+</span> <span class='va'>logpz_total</span>
    <span class='va'>logqz_x_total</span> <span class='op'>&lt;-</span> <span class='va'>tf</span><span class='op'>$</span><span class='fu'>reduce_mean</span><span class='op'>(</span><span class='va'>logqz_x</span><span class='op'>)</span> <span class='op'>+</span> <span class='va'>logqz_x_total</span>
    
    <span class='va'>encoder_gradients</span> <span class='op'>&lt;-</span> <span class='va'>tape</span><span class='op'>$</span><span class='fu'>gradient</span><span class='op'>(</span><span class='va'>loss</span>, <span class='va'>encoder</span><span class='op'>$</span><span class='va'>variables</span><span class='op'>)</span>
    <span class='va'>decoder_gradients</span> <span class='op'>&lt;-</span> <span class='va'>tape</span><span class='op'>$</span><span class='fu'>gradient</span><span class='op'>(</span><span class='va'>loss</span>, <span class='va'>decoder</span><span class='op'>$</span><span class='va'>variables</span><span class='op'>)</span>
    
    <span class='va'>optimizer</span><span class='op'>$</span><span class='fu'>apply_gradients</span><span class='op'>(</span>
      <span class='fu'>purrr</span><span class='fu'>::</span><span class='fu'><a href='https://purrr.tidyverse.org/reference/transpose.html'>transpose</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span><span class='va'>encoder_gradients</span>, <span class='va'>encoder</span><span class='op'>$</span><span class='va'>variables</span><span class='op'>)</span><span class='op'>)</span>,
      global_step <span class='op'>=</span> <span class='va'>tf</span><span class='op'>$</span><span class='va'>train</span><span class='op'>$</span><span class='fu'>get_or_create_global_step</span><span class='op'>(</span><span class='op'>)</span>
    <span class='op'>)</span>
    <span class='va'>optimizer</span><span class='op'>$</span><span class='fu'>apply_gradients</span><span class='op'>(</span>
      <span class='fu'>purrr</span><span class='fu'>::</span><span class='fu'><a href='https://purrr.tidyverse.org/reference/transpose.html'>transpose</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span><span class='va'>decoder_gradients</span>, <span class='va'>decoder</span><span class='op'>$</span><span class='va'>variables</span><span class='op'>)</span><span class='op'>)</span>,
      global_step <span class='op'>=</span> <span class='va'>tf</span><span class='op'>$</span><span class='va'>train</span><span class='op'>$</span><span class='fu'>get_or_create_global_step</span><span class='op'>(</span><span class='op'>)</span>
    <span class='op'>)</span>
    
  <span class='op'>}</span><span class='op'>)</span>
  
  <span class='va'>checkpoint</span><span class='op'>$</span><span class='fu'>save</span><span class='op'>(</span>file_prefix <span class='op'>=</span> <span class='va'>checkpoint_prefix</span><span class='op'>)</span>
  
  <span class='fu'><a href='https://rdrr.io/r/base/cat.html'>cat</a></span><span class='op'>(</span>
    <span class='fu'><a href='https://glue.tidyverse.org/reference/glue.html'>glue</a></span><span class='op'>(</span>
      <span class='st'>"Losses (epoch): {epoch}:"</span>,
      <span class='st'>"  {(as.numeric(logpx_z_total)/batches_per_epoch) %&gt;% round(2)} logpx_z_total,"</span>,
      <span class='st'>"  {(as.numeric(logpz_total)/batches_per_epoch) %&gt;% round(2)} logpz_total,"</span>,
      <span class='st'>"  {(as.numeric(logqz_x_total)/batches_per_epoch) %&gt;% round(2)} logqz_x_total,"</span>,
      <span class='st'>"  {(as.numeric(total_loss)/batches_per_epoch) %&gt;% round(2)} total"</span>
    <span class='op'>)</span>,
    <span class='st'>"\n"</span>
  <span class='op'>)</span>
  
  <span class='kw'>if</span> <span class='op'>(</span><span class='va'>epoch</span> <span class='op'>%%</span> <span class='fl'>10</span> <span class='op'>==</span> <span class='fl'>0</span><span class='op'>)</span> <span class='op'>{</span>
    <span class='fu'>generate_random_clothes</span><span class='op'>(</span><span class='va'>epoch</span><span class='op'>)</span>
    <span class='fu'>show_latent_space</span><span class='op'>(</span><span class='va'>epoch</span><span class='op'>)</span>
    <span class='fu'>show_grid</span><span class='op'>(</span><span class='va'>epoch</span><span class='op'>)</span>
  <span class='op'>}</span>
<span class='op'>}</span>
</code></pre>
</div>
</div>
<h4 id="results">Results</h4>
<p>How well did that work? Lets see the kinds of clothes generated after 50 epochs.</p>
<p><img src="images/cvae_clothes_epoch_50.png" style="width:66.0%" /></p>
<p>Also, how disentangled (or not) are the different classes in latent space?</p>
<p><img src="images/cvae_latentspace_epoch_50.png" style="width:66.0%" /></p>
<p>And now watch different clothes morph into one another.</p>
<p><img src="images/cvae_grid_epoch_50.png" style="width:66.0%" /></p>
<p>How good are these representations? This is hard to say when there is nothing to compare with.</p>
<p>So lets dive into MMD-VAE and see how it does on the same dataset.</p>
<h2 id="mmd-vae">MMD-VAE</h2>
<p>MMD-VAE promises to generate more informative latent features, so we would hope to see different behavior especially in the clustering and morphing plots.</p>
<p>Data setup is the same, and there are only very slight differences in the model. Please check out the complete code for this example, <a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/mmd_vae.R">mmd_vae.R</a>, as here well just highlight the differences.</p>
<h4 id="differences-in-the-models">Differences in the model(s)</h4>
<p>There are three differences as regards model architecture.</p>
<p>One, the encoder does not have to return the variance, so there is no need for <code>tf$split</code>. The encoders <code>call</code> method now just is</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='kw'>function</span> <span class='op'>(</span><span class='va'>x</span>, <span class='va'>mask</span> <span class='op'>=</span> <span class='cn'>NULL</span><span class='op'>)</span> <span class='op'>{</span>
  <span class='va'>x</span> <span class='op'>%&gt;%</span>
    <span class='va'>self</span><span class='op'>$</span><span class='fu'>conv1</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
    <span class='va'>self</span><span class='op'>$</span><span class='fu'>conv2</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
    <span class='va'>self</span><span class='op'>$</span><span class='fu'>flatten</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
    <span class='va'>self</span><span class='op'>$</span><span class='fu'>dense</span><span class='op'>(</span><span class='op'>)</span> 
<span class='op'>}</span>
</code></pre>
</div>
</div>
<p>Between the encoder and the decoder, we dont need the sampling step anymore, so there is no <em>reparameterization</em>. And since we wont use <code>tf$nn$sigmoid_cross_entropy_with_logits</code> to compute the loss, we let the decoder apply the sigmoid in the last deconvolution layer:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>self</span><span class='op'>$</span><span class='va'>deconv3</span> <span class='op'>&lt;-</span> <span class='fu'>layer_conv_2d_transpose</span><span class='op'>(</span>
  filters <span class='op'>=</span> <span class='fl'>1</span>,
  kernel_size <span class='op'>=</span> <span class='fl'>3</span>,
  strides <span class='op'>=</span> <span class='fl'>1</span>,
  padding <span class='op'>=</span> <span class='st'>"same"</span>,
  activation <span class='op'>=</span> <span class='st'>"sigmoid"</span>
<span class='op'>)</span>
</code></pre>
</div>
</div>
<h4 id="loss-calculations-1">Loss calculations</h4>
<p>Now, as expected, the big novelty is in the loss function.</p>
<p>The loss, <em>maximum mean discrepancy</em> (MMD), is based on the idea that two distributions are identical if and only if all moments are identical. Concretely, MMD is estimated using a <em>kernel</em>, such as the Gaussian kernel</p>
<p><span class="math display">\[k(z,z&#39;)=\frac{e^{||z-z&#39;||}}{2\sigma^2}\]</span></p>
<p>to assess similarity between distributions.</p>
<p>The idea then is that if two distributions are identical, the average similarity between samples from each distribution should be identical to the average similarity between mixed samples from both distributions:</p>
<p><span class="math display">\[MMD(p(z)||q(z))=E_{p(z),p(z&#39;)}[k(z,z&#39;)]+E_{q(z),q(z&#39;)}[k(z,z&#39;)]2E_{p(z),q(z&#39;)}[k(z,z&#39;)]\]</span> The following code is a direct port of the authors <a href="https://github.com/ShengjiaZhao/MMD-Variational-Autoencoder">original TensorFlow code</a>:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>compute_kernel</span> <span class='op'>&lt;-</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>x</span>, <span class='va'>y</span><span class='op'>)</span> <span class='op'>{</span>
  <span class='va'>x_size</span> <span class='op'>&lt;-</span> <span class='fu'>k_shape</span><span class='op'>(</span><span class='va'>x</span><span class='op'>)</span><span class='op'>[</span><span class='fl'>1</span><span class='op'>]</span>
  <span class='va'>y_size</span> <span class='op'>&lt;-</span> <span class='fu'>k_shape</span><span class='op'>(</span><span class='va'>y</span><span class='op'>)</span><span class='op'>[</span><span class='fl'>1</span><span class='op'>]</span>
  <span class='va'>dim</span> <span class='op'>&lt;-</span> <span class='fu'>k_shape</span><span class='op'>(</span><span class='va'>x</span><span class='op'>)</span><span class='op'>[</span><span class='fl'>2</span><span class='op'>]</span>
  <span class='va'>tiled_x</span> <span class='op'>&lt;-</span> <span class='fu'>k_tile</span><span class='op'>(</span>
    <span class='fu'>k_reshape</span><span class='op'>(</span><span class='va'>x</span>, <span class='fu'>k_stack</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span><span class='va'>x_size</span>, <span class='fl'>1</span>, <span class='va'>dim</span><span class='op'>)</span><span class='op'>)</span><span class='op'>)</span>,
    <span class='fu'>k_stack</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span><span class='fl'>1</span>, <span class='va'>y_size</span>, <span class='fl'>1</span><span class='op'>)</span><span class='op'>)</span>
  <span class='op'>)</span>
  <span class='va'>tiled_y</span> <span class='op'>&lt;-</span> <span class='fu'>k_tile</span><span class='op'>(</span>
    <span class='fu'>k_reshape</span><span class='op'>(</span><span class='va'>y</span>, <span class='fu'>k_stack</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span><span class='fl'>1</span>, <span class='va'>y_size</span>, <span class='va'>dim</span><span class='op'>)</span><span class='op'>)</span><span class='op'>)</span>,
    <span class='fu'>k_stack</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span><span class='va'>x_size</span>, <span class='fl'>1</span>, <span class='fl'>1</span><span class='op'>)</span><span class='op'>)</span>
  <span class='op'>)</span>
  <span class='fu'>k_exp</span><span class='op'>(</span><span class='op'>-</span><span class='fu'>k_mean</span><span class='op'>(</span><span class='fu'>k_square</span><span class='op'>(</span><span class='va'>tiled_x</span> <span class='op'>-</span> <span class='va'>tiled_y</span><span class='op'>)</span>, axis <span class='op'>=</span> <span class='fl'>3</span><span class='op'>)</span> <span class='op'>/</span>
          <span class='fu'>k_cast</span><span class='op'>(</span><span class='va'>dim</span>, <span class='va'>tf</span><span class='op'>$</span><span class='va'>float64</span><span class='op'>)</span><span class='op'>)</span>
<span class='op'>}</span>

<span class='va'>compute_mmd</span> <span class='op'>&lt;-</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>x</span>, <span class='va'>y</span>, <span class='va'>sigma_sqr</span> <span class='op'>=</span> <span class='fl'>1</span><span class='op'>)</span> <span class='op'>{</span>
  <span class='va'>x_kernel</span> <span class='op'>&lt;-</span> <span class='fu'>compute_kernel</span><span class='op'>(</span><span class='va'>x</span>, <span class='va'>x</span><span class='op'>)</span>
  <span class='va'>y_kernel</span> <span class='op'>&lt;-</span> <span class='fu'>compute_kernel</span><span class='op'>(</span><span class='va'>y</span>, <span class='va'>y</span><span class='op'>)</span>
  <span class='va'>xy_kernel</span> <span class='op'>&lt;-</span> <span class='fu'>compute_kernel</span><span class='op'>(</span><span class='va'>x</span>, <span class='va'>y</span><span class='op'>)</span>
  <span class='fu'>k_mean</span><span class='op'>(</span><span class='va'>x_kernel</span><span class='op'>)</span> <span class='op'>+</span> <span class='fu'>k_mean</span><span class='op'>(</span><span class='va'>y_kernel</span><span class='op'>)</span> <span class='op'>-</span> <span class='fl'>2</span> <span class='op'>*</span> <span class='fu'>k_mean</span><span class='op'>(</span><span class='va'>xy_kernel</span><span class='op'>)</span>
<span class='op'>}</span>
</code></pre>
</div>
</div>
<h4 id="training-loop-1">Training loop</h4>
<p>The training loop differs from the standard VAE example only in the loss calculations. Here are the respective lines:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code> <span class='fu'><a href='https://rdrr.io/r/base/with.html'>with</a></span><span class='op'>(</span><span class='va'>tf</span><span class='op'>$</span><span class='fu'>GradientTape</span><span class='op'>(</span>persistent <span class='op'>=</span> <span class='cn'>TRUE</span><span class='op'>)</span> <span class='op'>%as%</span> <span class='va'>tape</span>, <span class='op'>{</span>
      
      <span class='va'>mean</span> <span class='op'>&lt;-</span> <span class='fu'>encoder</span><span class='op'>(</span><span class='va'>x</span><span class='op'>)</span>
      <span class='va'>preds</span> <span class='op'>&lt;-</span> <span class='fu'>decoder</span><span class='op'>(</span><span class='va'>mean</span><span class='op'>)</span>
      
      <span class='va'>true_samples</span> <span class='op'>&lt;-</span> <span class='fu'>k_random_normal</span><span class='op'>(</span>
        shape <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='va'>batch_size</span>, <span class='va'>latent_dim</span><span class='op'>)</span>,
        dtype <span class='op'>=</span> <span class='va'>tf</span><span class='op'>$</span><span class='va'>float64</span>
      <span class='op'>)</span>
      <span class='va'>loss_mmd</span> <span class='op'>&lt;-</span> <span class='fu'>compute_mmd</span><span class='op'>(</span><span class='va'>true_samples</span>, <span class='va'>mean</span><span class='op'>)</span>
      <span class='va'>loss_nll</span> <span class='op'>&lt;-</span> <span class='fu'>k_mean</span><span class='op'>(</span><span class='fu'>k_square</span><span class='op'>(</span><span class='va'>x</span> <span class='op'>-</span> <span class='va'>preds</span><span class='op'>)</span><span class='op'>)</span>
      <span class='va'>loss</span> <span class='op'>&lt;-</span> <span class='va'>loss_nll</span> <span class='op'>+</span> <span class='va'>loss_mmd</span>
      
    <span class='op'>}</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>So we simply compute MMD loss as well as reconstruction loss, and add them up. No sampling is involved in this version. Of course, we are curious to see how well that worked!</p>
<h4 id="results-1">Results</h4>
<p>Again, lets look at some generated clothes first. It seems like edges are much sharper here.</p>
<p><img src="images/mmd_clothes_epoch_50.png" style="width:66.0%" /></p>
<p>The clusters too look more nicely spread out in the two dimensions. And, they are centered at (0,0), as we would have hoped for.</p>
<p><img src="images/mmd_latentspace_epoch_50.png" style="width:66.0%" /></p>
<p>Finally, lets see clothes morph into one another. Here, the smooth, continuous evolutions are impressive! Also, nearly all space is filled with meaningful objects, which hasnt been the case above.</p>
<p><img src="images/mmd_grid_epoch_50.png" style="width:66.0%" /></p>
<h2 id="mnist">MNIST</h2>
<p>For curiositys sake, we generated the same kinds of plots after training on original MNIST. Here, there are hardly any differences visible in generated random digits after 50 epochs of training.</p>
<figure>
<img src="images/comp_digits.png" style="width:100.0%" alt="Left: random digits as generated after training with ELBO loss. Right: MMD loss." /><figcaption aria-hidden="true">Left: random digits as generated after training with ELBO loss. Right: MMD loss.</figcaption>
</figure>
<p>Also the differences in clustering are not <em>that</em> big.</p>
<figure>
<img src="images/comp_lat.png" style="width:100.0%" alt="Left: latent space as observed after training with ELBO loss. Right: MMD loss." /><figcaption aria-hidden="true">Left: latent space as observed after training with ELBO loss. Right: MMD loss.</figcaption>
</figure>
<p>But here too, the morphing looks much more organic with MMD-VAE.</p>
<figure>
<img src="images/comp_grid.png" style="width:100.0%" alt="Left: Morphing as observed after training with ELBO loss. Right: MMD loss." /><figcaption aria-hidden="true">Left: Morphing as observed after training with ELBO loss. Right: MMD loss.</figcaption>
</figure>
<h2 id="conclusion">Conclusion</h2>
<p>To us, this demonstrates impressively what big a difference the cost function can make when working with VAEs. Another component open to experimentation may be the prior used for the latent space - see <a href="https://www.ics.uci.edu/~enalisni/nalisnick_openAI_talk.pdf">this talk</a> for an overview of alternative priors and the Variational Mixture of Posteriors paper <span class="citation" data-cites="TomczakW17">(Tomczak and Welling <a href="#ref-TomczakW17" role="doc-biblioref">2017</a>)</span> for a popular recent approach.</p>
<p>For both cost functions and priors, we expect effective differences to become way bigger still when we leave the controlled environment of (Fashion) MNIST and work with real-world datasets.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r distill-force-highlighting-css"><code class="sourceCode r"></code></pre></div>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-2018arXiv180403599B">
<p>Burgess, C. P., I. Higgins, A. Pal, L. Matthey, N. Watters, G. Desjardins, and A. Lerchner. 2018. Understanding Disentangling in Beta-Vae. <em>ArXiv E-Prints</em>, April. <a href="http://arxiv.org/abs/1804.03599">http://arxiv.org/abs/1804.03599</a>.</p>
</div>
<div id="ref-2016arXiv160605908D">
<p>Doersch, C. 2016. Tutorial on Variational Autoencoders. <em>ArXiv E-Prints</em>, June. <a href="http://arxiv.org/abs/1606.05908">http://arxiv.org/abs/1606.05908</a>.</p>
</div>
<div id="ref-KingmaW13">
<p>Kingma, Diederik P., and Max Welling. 2013. Auto-Encoding Variational Bayes. <em>CoRR</em> abs/1312.6114.</p>
</div>
<div id="ref-TomczakW17">
<p>Tomczak, Jakub M., and Max Welling. 2017. VAE with a Vampprior. <em>CoRR</em> abs/1705.07120.</p>
</div>
<div id="ref-ZhaoSE17b">
<p>Zhao, Shengjia, Jiaming Song, and Stefano Ermon. 2017. InfoVAE: Information Maximizing Variational Autoencoders. <em>CoRR</em> abs/1706.02262. <a href="http://arxiv.org/abs/1706.02262">http://arxiv.org/abs/1706.02262</a>.</p>
</div>
</div>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Note: this link was updated as of November 29th, 2019, to point to the most up-to-date version.<a href="#fnref1" class="footnote-back" role="doc-backlink"></a></p></li>
</ol>
</section>
<!--radix_placeholder_article_footer-->
<div class="article-footer">
  <p class="social_footer">
    <span class="disqus-comments">
      <i class="fas fa-comments"></i>
      &nbsp;
      <span class="disqus-comment-count" data-disqus-identifier="posts/2018-10-22-mmd-vae/">Comment on this article</span>
    </span>
    <span class="article-sharing">
      Share: &nbsp;
      <a href="https://twitter.com/share?text=Representation%20learning%20with%20MMD-VAE&amp;url=https%3A%2F%2Fblogs.rstudio.com%2Ftensorflow%2Fposts%2F2018-10-22-mmd-vae%2F">
        <i class="fab fa-twitter"></i>
      </a>
      <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3A%2F%2Fblogs.rstudio.com%2Ftensorflow%2Fposts%2F2018-10-22-mmd-vae%2F&amp;title=Representation%20learning%20with%20MMD-VAE">
        <i class="fab fa-linkedin"></i>
      </a>
    </span>
  </p>
  <script id="dsq-count-scr" src="https://tensorflow-for-r-blog.disqus.com/count.js" async></script>
  <div id="disqus_thread" class="hidden"></div>
  <script>
var disqus_config = function () {
  this.page.url = 'https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae/';
  this.page.identifier = 'posts/2018-10-22-mmd-vae/';
};
(function() {
  var d = document, s = d.createElement('script');
  s.src = 'https://tensorflow-for-r-blog.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
})();
</script>
  <p>
    <div class="subscribe">
<div id="subscribe-caption" style="line-height: 1.2; margin-bottom: 2px;">Enjoy this blog? Get notified of new posts by email:</div>

<script src="https://app-ab02.marketo.com/js/forms2/js/forms2.min.js"></script>
<form class="mtktoBlogEmailForm" id="mktoForm_2224"></form>
<script>

// establish metrics based on where the form is located
var in_sidebar = $('#subscribe-caption').parents('.sidebar-section').length;
if (in_sidebar) {
  var form_width = 190;
  var base_width = form_width - 23;
  var email_width = base_width + 'px';
  var label_width = (base_width - 20) + 'px';
  var button_width = email_width;
  var button_padding = '30px';
  var button_margin = '';
  var font_size = '12px';
} else {
  var form_width = 400;
  var base_width = form_width - 23;
  var email_width = base_width + 'px';
  var label_width = (base_width - 20) + 'px';
  var button_width = email_width;
  var button_padding = '30px';
  var button_margin = '12px';
  var font_size = '15px';
}

$('#subscribe-caption')
  .css('width', base_width)
  .css('font-size', font_size);


MktoForms2.loadForm("https://app-ab02.marketo.com", "709-NXN-706", 2224, function(form) {

  // get jquery reference to form
  form = $(form.getFormElem().get(0));

  $(form).css('width', form_width + 'px');
  $(form).find('.mktoOffset').remove();
  $(form).find('.mktoGutter').remove();
  $(form).find('.mktoEmailField').css('width', email_width);
  $(form).find('.mktoLabel').children().attr('style', 'font-weight: 400');
  $(form).find('.mktoLabel')
      .css('width', label_width)
      .css('font-size', font_size);
  $(form).find('.mktoButtonRow')
      .css('width', button_width)
      .css('text-align', 'center');
  $(form).find('.mktoButtonWrap').attr('style', '');
  $(form).find('.mktoButton')
      .css('margin-top', '10px')
      .css('padding-left', button_padding)
      .css('padding-right', button_padding)
      .css('font-size', font_size)
      .css('margin-top', button_margin);
});
</script>
Posts also available at <a href="https://www.r-bloggers.com">r-bloggers</a>

<script>
  document.addEventListener("DOMContentLoaded", function(){
    var menulink = document.querySelector("a[href='#category:R']");
    if (menulink) menulink.parentNode.style.display = "None";
    
    for (var e of document.querySelectorAll(".dt-tag")) {
      if (e.innerHTML == 'R') e.style.display = "None";
    }
  });
</script>
</div>
  </p>
</div>
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
  <h3 id="references">References</h3>
  <div id="references-listing"></div>
  <h3 id="reuse">Reuse</h3>
  <p>Text and figures are licensed under Creative Commons Attribution <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>. The figures that have been reused from other sources don't fall under this license and can be recognized by a note in their caption: "Figure from ...".</p>
  <h3 id="citation">Citation</h3>
  <p>For attribution, please cite this work as</p>
  <pre class="citation-appendix short">Keydana (2018, Oct. 22). RStudio AI Blog: Representation learning with MMD-VAE. Retrieved from https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae/</pre>
  <p>BibTeX citation</p>
  <pre class="citation-appendix long">@misc{keydana2018mmdvae,
  author = {Keydana, Sigrid},
  title = {RStudio AI Blog: Representation learning with MMD-VAE},
  url = {https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae/},
  year = {2018}
}</pre>
</div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
