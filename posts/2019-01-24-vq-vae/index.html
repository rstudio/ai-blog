<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #545454; font-weight: bold; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #a1024a; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007faa; font-weight: bold; } /* ControlFlow */
code span.ch { color: #008000; } /* Char */
code span.cn { color: #d91e18; } /* Constant */
code span.co { color: #545454; } /* Comment */
code span.cv { color: #545454; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #aa5d00; } /* DataType */
code span.dv { color: #a1024a; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #a1024a; } /* Float */
code span.fu { color: #4254a7; } /* Function */
code span.im { } /* Import */
code span.in { color: #545454; font-weight: bold; } /* Information */
code span.kw { color: #007faa; font-weight: bold; } /* Keyword */
code span.op { color: #696969; } /* Operator */
code span.ot { color: #007faa; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #008000; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #008000; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #008000; } /* VerbatimString */
code span.wa { color: #545454; font-weight: bold; font-style: italic; } /* Warning */
</style>

  <!--radix_placeholder_meta_tags-->
<title>RStudio AI Blog: Discrete Representation Learning with VQ-VAE and TensorFlow Probability</title>

<meta property="description" itemprop="description" content="Mostly when thinking of Variational Autoencoders (VAEs), we picture the prior as an isotropic Gaussian. But this is by no means a necessity. The Vector Quantised Variational Autoencoder (VQ-VAE) described in van den Oord et al&#39;s &quot;Neural Discrete Representation Learning&quot; features a discrete latent space that allows to learn impressively concise latent representations. In this post, we combine elements of Keras, TensorFlow, and TensorFlow Probability to see if we can generate convincing letters resembling those in Kuzushiji-MNIST."/>

<link rel="canonical" href="https://blogs.rstudio.com/tensorflow/posts/2019-01-24-vq-vae/"/>
<link rel="license" href="https://creativecommons.org/licenses/by/4.0/"/>
<link rel="icon" type="image/png" href="../../images/favicon.png"/>

<!--  https://schema.org/Article -->
<meta property="article:published" itemprop="datePublished" content="2019-01-24"/>
<meta property="article:created" itemprop="dateCreated" content="2019-01-24"/>
<meta name="article:author" content="Sigrid Keydana"/>

<!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
<meta property="og:title" content="RStudio AI Blog: Discrete Representation Learning with VQ-VAE and TensorFlow Probability"/>
<meta property="og:type" content="article"/>
<meta property="og:description" content="Mostly when thinking of Variational Autoencoders (VAEs), we picture the prior as an isotropic Gaussian. But this is by no means a necessity. The Vector Quantised Variational Autoencoder (VQ-VAE) described in van den Oord et al&#39;s &quot;Neural Discrete Representation Learning&quot; features a discrete latent space that allows to learn impressively concise latent representations. In this post, we combine elements of Keras, TensorFlow, and TensorFlow Probability to see if we can generate convincing letters resembling those in Kuzushiji-MNIST."/>
<meta property="og:url" content="https://blogs.rstudio.com/tensorflow/posts/2019-01-24-vq-vae/"/>
<meta property="og:image" content="https://blogs.rstudio.com/tensorflow/posts/2019-01-24-vq-vae/images/thumb1.png"/>
<meta property="og:image:width" content="510"/>
<meta property="og:image:height" content="287"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:site_name" content="RStudio AI Blog"/>

<!--  https://dev.twitter.com/cards/types/summary -->
<meta property="twitter:card" content="summary_large_image"/>
<meta property="twitter:title" content="RStudio AI Blog: Discrete Representation Learning with VQ-VAE and TensorFlow Probability"/>
<meta property="twitter:description" content="Mostly when thinking of Variational Autoencoders (VAEs), we picture the prior as an isotropic Gaussian. But this is by no means a necessity. The Vector Quantised Variational Autoencoder (VQ-VAE) described in van den Oord et al&#39;s &quot;Neural Discrete Representation Learning&quot; features a discrete latent space that allows to learn impressively concise latent representations. In this post, we combine elements of Keras, TensorFlow, and TensorFlow Probability to see if we can generate convincing letters resembling those in Kuzushiji-MNIST."/>
<meta property="twitter:url" content="https://blogs.rstudio.com/tensorflow/posts/2019-01-24-vq-vae/"/>
<meta property="twitter:image" content="https://blogs.rstudio.com/tensorflow/posts/2019-01-24-vq-vae/images/thumb1.png"/>
<meta property="twitter:image:width" content="510"/>
<meta property="twitter:image:height" content="287"/>

<!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
<meta name="citation_title" content="RStudio AI Blog: Discrete Representation Learning with VQ-VAE and TensorFlow Probability"/>
<meta name="citation_fulltext_html_url" content="https://blogs.rstudio.com/tensorflow/posts/2019-01-24-vq-vae/"/>
<meta name="citation_fulltext_world_readable" content=""/>
<meta name="citation_online_date" content="2019/01/24"/>
<meta name="citation_publication_date" content="2019/01/24"/>
<meta name="citation_author" content="Sigrid Keydana"/>
<meta name="citation_author_institution" content="RStudio"/>
<!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=Neural Discrete Representation Learning;citation_publication_date=2017;citation_volume=abs/1711.00937;citation_author=Aaron Oord;citation_author=Oriol Vinyals;citation_author=Koray Kavukcuoglu"/>
  <meta name="citation_reference" content="citation_title=Deep Learning for Classical Japanese Literature;citation_publication_date=2018;citation_author=Tarin Clanuwat;citation_author=Mikel Bober-Irizar;citation_author=Asanobu Kitamoto;citation_author=Alex Lamb;citation_author=Kazuaki Yamamoto;citation_author=David Ha"/>
  <!--radix_placeholder_rmarkdown_metadata-->

<script type="text/json" id="radix-rmarkdown-metadata">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["creative_commons","title","description","author","slug","date","categories","bibliography","output","preview","citation_url","canonical_url"]}},"value":[{"type":"character","attributes":{},"value":["CC BY"]},{"type":"character","attributes":{},"value":["Discrete Representation Learning with VQ-VAE and TensorFlow Probability"]},{"type":"character","attributes":{},"value":["Mostly when thinking of Variational Autoencoders (VAEs), we picture the prior as an isotropic Gaussian. But this is by no means a necessity. The Vector Quantised Variational Autoencoder (VQ-VAE) described in van den Oord et al's \"Neural Discrete Representation Learning\" features a discrete latent space that allows to learn impressively concise latent representations. In this post, we combine elements of Keras, TensorFlow, and TensorFlow Probability to see if we can generate convincing letters resembling those in Kuzushiji-MNIST."]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","affiliation_url"]}},"value":[{"type":"character","attributes":{},"value":["Sigrid Keydana"]},{"type":"character","attributes":{},"value":["RStudio"]},{"type":"character","attributes":{},"value":["https://www.rstudio.com/"]}]}]},{"type":"character","attributes":{},"value":["keydana2019vqvae"]},{"type":"character","attributes":{},"value":["01-24-2019"]},{"type":"character","attributes":{},"value":["TensorFlow/Keras","Probabilistic ML/DL","Unsupervised Learning"]},{"type":"character","attributes":{},"value":["bibliography.bib"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"character","attributes":{},"value":["images/thumb1.png"]},{"type":"character","attributes":{},"value":["https://blogs.rstudio.com/tensorflow/posts/2019-01-24-vq-vae/"]},{"type":"character","attributes":{},"value":["https://blogs.rstudio.com/tensorflow/posts/2019-01-24-vq-vae/"]}]}
</script>
<!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["bibliography.bib","images/epoch_9.png","images/thumb1.png","vq_vae_files/bowser-1.9.3/bowser.min.js","vq_vae_files/distill-2.2.21/template.v2.js","vq_vae_files/jquery-1.11.3/jquery.min.js","vq_vae_files/webcomponents-2.0.0/webcomponents.js"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
<meta name="distill:offset" content="../.."/>

<script type="application/javascript">

  window.headroom_prevent_pin = false;

  window.document.addEventListener("DOMContentLoaded", function (event) {

    // initialize headroom for banner
    var header = $('header').get(0);
    var headerHeight = header.offsetHeight;
    var headroom = new Headroom(header, {
      onPin : function() {
        if (window.headroom_prevent_pin) {
          window.headroom_prevent_pin = false;
          headroom.unpin();
        }
      }
    });
    headroom.init();
    if(window.location.hash)
      headroom.unpin();
    $(header).addClass('headroom--transition');

    // offset scroll location for banner on hash change
    // (see: https://github.com/WickyNilliams/headroom.js/issues/38)
    window.addEventListener("hashchange", function(event) {
      window.scrollTo(0, window.pageYOffset - (headerHeight + 25));
    });

    // responsive menu
    $('.distill-site-header').each(function(i, val) {
      var topnav = $(this);
      var toggle = topnav.find('.nav-toggle');
      toggle.on('click', function() {
        topnav.toggleClass('responsive');
      });
    });

    // nav dropdowns
    $('.nav-dropbtn').click(function(e) {
      $(this).next('.nav-dropdown-content').toggleClass('nav-dropdown-active');
      $(this).parent().siblings('.nav-dropdown')
         .children('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $("body").click(function(e){
      $('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $(".nav-dropdown").click(function(e){
      e.stopPropagation();
    });
  });
</script>

<style type="text/css">

/* Theme (user-documented overrideables for nav appearance) */

.distill-site-nav {
  color: rgba(255, 255, 255, 0.8);
  background-color: #0F2E3D;
  font-size: 15px;
  font-weight: 300;
}

.distill-site-nav a {
  color: inherit;
  text-decoration: none;
}

.distill-site-nav a:hover {
  color: white;
}

@media print {
  .distill-site-nav {
    display: none;
  }
}

.distill-site-header {

}

.distill-site-footer {

}


/* Site Header */

.distill-site-header {
  width: 100%;
  box-sizing: border-box;
  z-index: 3;
}

.distill-site-header .nav-left {
  display: inline-block;
  margin-left: 8px;
}

@media screen and (max-width: 768px) {
  .distill-site-header .nav-left {
    margin-left: 0;
  }
}


.distill-site-header .nav-right {
  float: right;
  margin-right: 8px;
}

.distill-site-header a,
.distill-site-header .title {
  display: inline-block;
  text-align: center;
  padding: 14px 10px 14px 10px;
}

.distill-site-header .title {
  font-size: 18px;
  min-width: 150px;
}

.distill-site-header .logo {
  padding: 0;
}

.distill-site-header .logo img {
  display: none;
  max-height: 20px;
  width: auto;
  margin-bottom: -4px;
}

.distill-site-header .nav-image img {
  max-height: 18px;
  width: auto;
  display: inline-block;
  margin-bottom: -3px;
}



@media screen and (min-width: 1000px) {
  .distill-site-header .logo img {
    display: inline-block;
  }
  .distill-site-header .nav-left {
    margin-left: 20px;
  }
  .distill-site-header .nav-right {
    margin-right: 20px;
  }
  .distill-site-header .title {
    padding-left: 12px;
  }
}


.distill-site-header .nav-toggle {
  display: none;
}

.nav-dropdown {
  display: inline-block;
  position: relative;
}

.nav-dropdown .nav-dropbtn {
  border: none;
  outline: none;
  color: rgba(255, 255, 255, 0.8);
  padding: 16px 10px;
  background-color: transparent;
  font-family: inherit;
  font-size: inherit;
  font-weight: inherit;
  margin: 0;
  margin-top: 1px;
  z-index: 2;
}

.nav-dropdown-content {
  display: none;
  position: absolute;
  background-color: white;
  min-width: 200px;
  border: 1px solid rgba(0,0,0,0.15);
  border-radius: 4px;
  box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.1);
  z-index: 1;
  margin-top: 2px;
  white-space: nowrap;
  padding-top: 4px;
  padding-bottom: 4px;
}

.nav-dropdown-content hr {
  margin-top: 4px;
  margin-bottom: 4px;
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.nav-dropdown-active {
  display: block;
}

.nav-dropdown-content a, .nav-dropdown-content .nav-dropdown-header {
  color: black;
  padding: 6px 24px;
  text-decoration: none;
  display: block;
  text-align: left;
}

.nav-dropdown-content .nav-dropdown-header {
  display: block;
  padding: 5px 24px;
  padding-bottom: 0;
  text-transform: uppercase;
  font-size: 14px;
  color: #999999;
  white-space: nowrap;
}

.nav-dropdown:hover .nav-dropbtn {
  color: white;
}

.nav-dropdown-content a:hover {
  background-color: #ddd;
  color: black;
}

.nav-right .nav-dropdown-content {
  margin-left: -45%;
  right: 0;
}

@media screen and (max-width: 768px) {
  .distill-site-header a, .distill-site-header .nav-dropdown  {display: none;}
  .distill-site-header a.nav-toggle {
    float: right;
    display: block;
  }
  .distill-site-header .title {
    margin-left: 0;
  }
  .distill-site-header .nav-right {
    margin-right: 0;
  }
  .distill-site-header {
    overflow: hidden;
  }
  .nav-right .nav-dropdown-content {
    margin-left: 0;
  }
}


@media screen and (max-width: 768px) {
  .distill-site-header.responsive {position: relative; min-height: 500px; }
  .distill-site-header.responsive a.nav-toggle {
    position: absolute;
    right: 0;
    top: 0;
  }
  .distill-site-header.responsive a,
  .distill-site-header.responsive .nav-dropdown {
    display: block;
    text-align: left;
  }
  .distill-site-header.responsive .nav-left,
  .distill-site-header.responsive .nav-right {
    width: 100%;
  }
  .distill-site-header.responsive .nav-dropdown {float: none;}
  .distill-site-header.responsive .nav-dropdown-content {position: relative;}
  .distill-site-header.responsive .nav-dropdown .nav-dropbtn {
    display: block;
    width: 100%;
    text-align: left;
  }
}

/* Site Footer */

.distill-site-footer {
  width: 100%;
  overflow: hidden;
  box-sizing: border-box;
  z-index: 3;
  margin-top: 30px;
  padding-top: 30px;
  padding-bottom: 30px;
  text-align: center;
}

/* Headroom */

d-title {
  padding-top: 6rem;
}

@media print {
  d-title {
    padding-top: 4rem;
  }
}

.headroom {
  z-index: 1000;
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
}

.headroom--transition {
  transition: all .4s ease-in-out;
}

.headroom--unpinned {
  top: -100px;
}

.headroom--pinned {
  top: 0;
}

</style>

<link href="../../site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet"/>
<link href="../../site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet"/>
<script src="../../site_libs/headroom-0.9.4/headroom.min.js"></script>
<script src="../../site_libs/autocomplete-0.37.1/autocomplete.min.js"></script>
<script src="../../site_libs/fuse-6.4.1/fuse.min.js"></script>

<script type="application/javascript">

function getMeta(metaName) {
  var metas = document.getElementsByTagName('meta');
  for (let i = 0; i < metas.length; i++) {
    if (metas[i].getAttribute('name') === metaName) {
      return metas[i].getAttribute('content');
    }
  }
  return '';
}

function offsetURL(url) {
  var offset = getMeta('distill:offset');
  return offset ? offset + '/' + url : url;
}

function createIndex() {
  var options = {
    keys: [
      "title",
      "categories",
      "description",
      "contents"
    ]
  };
  return new window.Fuse([],options);
}

function createFuseIndex() {

  // create fuse index
  var options = { keys: ["title", "description", "contents"] };
  var fuse = new window.Fuse([], options);

  // fetch the main search.json
  return fetch(offsetURL('search.json'))
    .then(function(response) {
      if (response.status == 200) {
        return response.json().then(function(json) {
          // index main articles
          json.articles.forEach(function(article) {
            fuse.add(article);
          });
          // download collections and index their articles
          return Promise.all(json.collections.map(function(collection) {
            return fetch(offsetURL(collection)).then(function(response) {
              if (response.status === 200) {
                return response.json().then(function(articles) {
                  articles.forEach(function(article) {
                    fuse.add(article);
                  });
                })
              } else {
                return Promise.reject(
                  new Error('Unexpected status from search index request: ' +
                            response.status)
                );
              }
            });
          })).then(function() {
            return fuse;
          });
        });

      } else {
        return Promise.reject(
          new Error('Unexpected status from search index request: ' +
                      response.status)
        );
      }
    });
}

window.document.addEventListener("DOMContentLoaded", function (event) {

  // get search element (bail if we don't have one)
  var searchEl = window.document.getElementById('distill-search');
  if (!searchEl)
    return;

  createFuseIndex()
    .then(function(fuse) {

      // make search box visible
      searchEl.classList.remove('hidden');

      // initialize autocomplete
      var options = {
        autoselect: true,
        hint: false,
        minLength: 2,
      };
      window.autocomplete(searchEl, options, [{
        source: function(query, callback) {
          const searchOptions = {
            isCaseSensitive: false,
            shouldSort: true,
            minMatchCharLength: 2,
            limit: 10,
            keys: [
              { name: 'title', weight: 20 },
              { name: 'categories', weight: 15 },
              { name: 'description', weight: 10 },
              { name: 'contents', weight: 5 },
            ],
          };
          var results = fuse.search(query, searchOptions);
          callback(results
            .map(function(result) { return result.item; })
            .filter(function(item) { return !!item.description; })
          );
        },
        templates: {
          suggestion: function(suggestion) {
            var html = `
              <div class="search-item">
                <h3>${suggestion.title}</h3>
                <div class="search-item-description">
                  ${suggestion.description}
                </div>
                <div class="search-item-preview">
                  <img src="${suggestion.preview ? offsetURL(suggestion.preview) : ''}"</img>
                </div>
              </div>
            `;
            return html;
          }
        }
      }]).on('autocomplete:selected', function(event, suggestion) {
        window.location.href = offsetURL(suggestion.path);
      });
      // remove inline display style on autocompleter (we want to
      // manage responsive display via css)
      $('.algolia-autocomplete').css("display", "");
    })
    .catch(function(error) {
      console.log(error);
    });

});

</script>

<style type="text/css">

/* Algolioa Autocomplete */

.algolia-autocomplete {
  display: inline-block;
  margin-left: 10px;
  vertical-align: sub;
  background-color: white;
  color: black;
  padding: 6px;
  padding-top: 8px;
  padding-bottom: 0;
  border-radius: 6px;
  border: 1px #0F2E3D solid;
  width: 180px;
}


@media screen and (max-width: 768px) {
  .distill-site-nav .algolia-autocomplete {
    display: none;
    visibility: hidden;
  }
  .distill-site-nav.responsive .algolia-autocomplete {
    display: inline-block;
    visibility: visible;
  }
  .distill-site-nav.responsive .algolia-autocomplete .aa-dropdown-menu {
    margin-left: 0;
    width: 400px;
    max-height: 400px;
  }
}

.algolia-autocomplete .aa-input, .algolia-autocomplete .aa-hint {
  width: 90%;
  outline: none;
  border: none;
}

.algolia-autocomplete .aa-hint {
  color: #999;
}
.algolia-autocomplete .aa-dropdown-menu {
  width: 550px;
  max-height: 70vh;
  overflow-x: visible;
  overflow-y: scroll;
  padding: 5px;
  margin-top: 3px;
  margin-left: -150px;
  background-color: #fff;
  border-radius: 5px;
  border: 1px solid #999;
  border-top: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion {
  cursor: pointer;
  padding: 5px 4px;
  border-bottom: 1px solid #eee;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion:last-of-type {
  border-bottom: none;
  margin-bottom: 2px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item {
  overflow: hidden;
  font-size: 0.8em;
  line-height: 1.4em;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item h3 {
  margin-block-start: 0;
  margin-block-end: 5px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-description {
  display: inline-block;
  overflow: hidden;
  height: 2.8em;
  width: 80%;
  margin-right: 4%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview {
  display: inline-block;
  width: 15%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img {
  height: 3em;
  width: auto;
  display: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img[src] {
  display: initial;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion.aa-cursor {
  background-color: #eee;
}
.algolia-autocomplete .aa-dropdown-menu .aa-suggestion em {
  font-weight: bold;
  font-style: normal;
}

</style>


<!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

<style type="text/css">

body {
  background-color: white;
}

.pandoc-table {
  width: 100%;
}

.pandoc-table>caption {
  margin-bottom: 10px;
}

.pandoc-table th:not([align]) {
  text-align: left;
}

.pagedtable-footer {
  font-size: 15px;
}

d-byline .byline {
  grid-template-columns: 2fr 2fr;
}

d-byline .byline h3 {
  margin-block-start: 1.5em;
}

d-byline .byline .authors-affiliations h3 {
  margin-block-start: 0.5em;
}

.authors-affiliations .orcid-id {
  width: 16px;
  height:16px;
  margin-left: 4px;
  margin-right: 4px;
  vertical-align: middle;
  padding-bottom: 2px;
}

d-title .dt-tags {
  margin-top: 1em;
  grid-column: text;
}

.dt-tags .dt-tag {
  text-decoration: none;
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0em 0.4em;
  margin-right: 0.5em;
  margin-bottom: 0.4em;
  font-size: 70%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

d-article table.gt_table td,
d-article table.gt_table th {
  border-bottom: none;
}

.html-widget {
  margin-bottom: 2.0em;
}

.l-screen-inset {
  padding-right: 16px;
}

.l-screen .caption {
  margin-left: 10px;
}

.shaded {
  background: rgb(247, 247, 247);
  padding-top: 20px;
  padding-bottom: 20px;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .html-widget {
  margin-bottom: 0;
  border: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .shaded-content {
  background: white;
}

.text-output {
  margin-top: 0;
  line-height: 1.5em;
}

.hidden {
  display: none !important;
}

d-article {
  padding-top: 2.5rem;
  padding-bottom: 30px;
}

d-appendix {
  padding-top: 30px;
}

d-article>p>img {
  width: 100%;
}

d-article h2 {
  margin: 1rem 0 1.5rem 0;
}

d-article h3 {
  margin-top: 1.5rem;
}

d-article iframe {
  border: 1px solid rgba(0, 0, 0, 0.1);
  margin-bottom: 2.0em;
  width: 100%;
}

/* Tweak code blocks */

d-article div.sourceCode code,
d-article pre code {
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
}

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: hidden;
}

d-article div.sourceCode {
  background-color: white;
}

d-article div.sourceCode pre {
  padding-left: 10px;
  font-size: 12px;
  border-left: 2px solid rgba(0,0,0,0.1);
}

d-article pre {
  font-size: 12px;
  color: black;
  background: none;
  margin-top: 0;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

d-article pre a {
  border-bottom: none;
}

d-article pre a:hover {
  border-bottom: none;
  text-decoration: underline;
}

@media(min-width: 768px) {

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: visible !important;
}

d-article div.sourceCode pre {
  padding-left: 18px;
  font-size: 14px;
}

d-article pre {
  font-size: 14px;
}

}

figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

/* CSS for d-contents */

.d-contents {
  grid-column: text;
  color: rgba(0,0,0,0.8);
  font-size: 0.9em;
  padding-bottom: 1em;
  margin-bottom: 1em;
  padding-bottom: 0.5em;
  margin-bottom: 1em;
  padding-left: 0.25em;
  justify-self: start;
}

@media(min-width: 1000px) {
  .d-contents.d-contents-float {
    height: 0;
    grid-column-start: 1;
    grid-column-end: 4;
    justify-self: center;
    padding-right: 3em;
    padding-left: 2em;
  }
}

.d-contents nav h3 {
  font-size: 18px;
  margin-top: 0;
  margin-bottom: 1em;
}

.d-contents li {
  list-style-type: none
}

.d-contents nav > ul {
  padding-left: 0;
}

.d-contents ul {
  padding-left: 1em
}

.d-contents nav ul li {
  margin-top: 0.6em;
  margin-bottom: 0.2em;
}

.d-contents nav a {
  font-size: 13px;
  border-bottom: none;
  text-decoration: none
  color: rgba(0, 0, 0, 0.8);
}

.d-contents nav a:hover {
  text-decoration: underline solid rgba(0, 0, 0, 0.6)
}

.d-contents nav > ul > li > a {
  font-weight: 600;
}

.d-contents nav > ul > li > ul {
  font-weight: inherit;
}

.d-contents nav > ul > li > ul > li {
  margin-top: 0.2em;
}


.d-contents nav ul {
  margin-top: 0;
  margin-bottom: 0.25em;
}

.d-article-with-toc h2:nth-child(2) {
  margin-top: 0;
}


/* Figure */

.figure {
  position: relative;
  margin-bottom: 2.5em;
  margin-top: 1.5em;
}

.figure img {
  width: 100%;
}

.figure .caption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

.figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

.figure .caption a {
  color: rgba(0, 0, 0, 0.6);
}

.figure .caption b,
.figure .caption strong, {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

/* Citations */

d-article .citation {
  color: inherit;
  cursor: inherit;
}

div.hanging-indent{
  margin-left: 1em; text-indent: -1em;
}


/* Tweak 1000px media break to show more text */

@media(min-width: 1000px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }

  d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
  }
  figure .caption, .figure .caption, figure figcaption {
    font-size: 13px;
  }
}

@media(min-width: 1180px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 32px;
  }

  .grid {
    grid-column-gap: 32px;
  }
}


/* Get the citation styles for the appendix (not auto-injected on render since
   we do our own rendering of the citation appendix) */

d-appendix .citation-appendix,
.d-appendix .citation-appendix {
  font-size: 11px;
  line-height: 15px;
  border-left: 1px solid rgba(0, 0, 0, 0.1);
  padding-left: 18px;
  border: 1px solid rgba(0,0,0,0.1);
  background: rgba(0, 0, 0, 0.02);
  padding: 10px 18px;
  border-radius: 3px;
  color: rgba(150, 150, 150, 1);
  overflow: hidden;
  margin-top: -12px;
  white-space: pre-wrap;
  word-wrap: break-word;
}

/* Include appendix styles here so they can be overridden */

d-appendix {
  contain: layout style;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-top: 60px;
  margin-bottom: 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  color: rgba(0,0,0,0.5);
  padding-top: 60px;
  padding-bottom: 48px;
}

d-appendix h3 {
  grid-column: page-start / text-start;
  font-size: 15px;
  font-weight: 500;
  margin-top: 1em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.65);
}

d-appendix h3 + * {
  margin-top: 1em;
}

d-appendix ol {
  padding: 0 0 0 15px;
}

@media (min-width: 768px) {
  d-appendix ol {
    padding: 0 0 0 30px;
    margin-left: -30px;
  }
}

d-appendix li {
  margin-bottom: 1em;
}

d-appendix a {
  color: rgba(0, 0, 0, 0.6);
}

d-appendix > * {
  grid-column: text;
}

d-appendix > d-footnote-list,
d-appendix > d-citation-list,
d-appendix > distill-appendix {
  grid-column: screen;
}

/* Include footnote styles here so they can be overridden */

d-footnote-list {
  contain: layout style;
}

d-footnote-list > * {
  grid-column: text;
}

d-footnote-list a.footnote-backlink {
  color: rgba(0,0,0,0.3);
  padding-left: 0.5em;
}



/* Anchor.js */

.anchorjs-link {
  /*transition: all .25s linear; */
  text-decoration: none;
  border-bottom: none;
}
*:hover > .anchorjs-link {
  margin-left: -1.125em !important;
  text-decoration: none;
  border-bottom: none;
}

/* Social footer */

.social_footer {
  margin-top: 30px;
  margin-bottom: 0;
  color: rgba(0,0,0,0.67);
}

.disqus-comments {
  margin-right: 30px;
}

.disqus-comment-count {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  cursor: pointer;
}

#disqus_thread {
  margin-top: 30px;
}

.article-sharing a {
  border-bottom: none;
  margin-right: 8px;
}

.article-sharing a:hover {
  border-bottom: none;
}

.sidebar-section.subscribe {
  font-size: 12px;
  line-height: 1.6em;
}

.subscribe p {
  margin-bottom: 0.5em;
}


.article-footer .subscribe {
  font-size: 15px;
  margin-top: 45px;
}


.sidebar-section.custom {
  font-size: 12px;
  line-height: 1.6em;
}

.custom p {
  margin-bottom: 0.5em;
}

/* Styles for listing layout (hide title) */
.layout-listing d-title, .layout-listing .d-title {
  display: none;
}

/* Styles for posts lists (not auto-injected) */


.posts-with-sidebar {
  padding-left: 45px;
  padding-right: 45px;
}

.posts-list .description h2,
.posts-list .description p {
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
}

.posts-list .description h2 {
  font-weight: 700;
  border-bottom: none;
  padding-bottom: 0;
}

.posts-list h2.post-tag {
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
  padding-bottom: 12px;
}
.posts-list {
  margin-top: 60px;
  margin-bottom: 24px;
}

.posts-list .post-preview {
  text-decoration: none;
  overflow: hidden;
  display: block;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  padding: 24px 0;
}

.post-preview-last {
  border-bottom: none !important;
}

.posts-list .posts-list-caption {
  grid-column: screen;
  font-weight: 400;
}

.posts-list .post-preview h2 {
  margin: 0 0 6px 0;
  line-height: 1.2em;
  font-style: normal;
  font-size: 24px;
}

.posts-list .post-preview p {
  margin: 0 0 12px 0;
  line-height: 1.4em;
  font-size: 16px;
}

.posts-list .post-preview .thumbnail {
  box-sizing: border-box;
  margin-bottom: 24px;
  position: relative;
  max-width: 500px;
}
.posts-list .post-preview img {
  width: 100%;
  display: block;
}

.posts-list .metadata {
  font-size: 12px;
  line-height: 1.4em;
  margin-bottom: 18px;
}

.posts-list .metadata > * {
  display: inline-block;
}

.posts-list .metadata .publishedDate {
  margin-right: 2em;
}

.posts-list .metadata .dt-authors {
  display: block;
  margin-top: 0.3em;
  margin-right: 2em;
}

.posts-list .dt-tags {
  display: block;
  line-height: 1em;
}

.posts-list .dt-tags .dt-tag {
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0.3em 0.4em;
  margin-right: 0.2em;
  margin-bottom: 0.4em;
  font-size: 60%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

.posts-list img {
  opacity: 1;
}

.posts-list img[data-src] {
  opacity: 0;
}

.posts-more {
  clear: both;
}


.posts-sidebar {
  font-size: 16px;
}

.posts-sidebar h3 {
  font-size: 16px;
  margin-top: 0;
  margin-bottom: 0.5em;
  font-weight: 400;
  text-transform: uppercase;
}

.sidebar-section {
  margin-bottom: 30px;
}

.categories ul {
  list-style-type: none;
  margin: 0;
  padding: 0;
}

.categories li {
  color: rgba(0, 0, 0, 0.8);
  margin-bottom: 0;
}

.categories li>a {
  border-bottom: none;
}

.categories li>a:hover {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
}

.categories .active {
  font-weight: 600;
}

.categories .category-count {
  color: rgba(0, 0, 0, 0.4);
}


@media(min-width: 768px) {
  .posts-list .post-preview h2 {
    font-size: 26px;
  }
  .posts-list .post-preview .thumbnail {
    float: right;
    width: 30%;
    margin-bottom: 0;
  }
  .posts-list .post-preview .description {
    float: left;
    width: 45%;
  }
  .posts-list .post-preview .metadata {
    float: left;
    width: 20%;
    margin-top: 8px;
  }
  .posts-list .post-preview p {
    margin: 0 0 12px 0;
    line-height: 1.5em;
    font-size: 16px;
  }
  .posts-with-sidebar .posts-list {
    float: left;
    width: 75%;
  }
  .posts-with-sidebar .posts-sidebar {
    float: right;
    width: 20%;
    margin-top: 60px;
    padding-top: 24px;
    padding-bottom: 24px;
  }
}


/* Improve display for browsers without grid (IE/Edge <= 15) */

.downlevel {
  line-height: 1.6em;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  margin: 0;
}

.downlevel .d-title {
  padding-top: 6rem;
  padding-bottom: 1.5rem;
}

.downlevel .d-title h1 {
  font-size: 50px;
  font-weight: 700;
  line-height: 1.1em;
  margin: 0 0 0.5rem;
}

.downlevel .d-title p {
  font-weight: 300;
  font-size: 1.2rem;
  line-height: 1.55em;
  margin-top: 0;
}

.downlevel .d-byline {
  padding-top: 0.8em;
  padding-bottom: 0.8em;
  font-size: 0.8rem;
  line-height: 1.8em;
}

.downlevel .section-separator {
  border: none;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
}

.downlevel .d-article {
  font-size: 1.06rem;
  line-height: 1.7em;
  padding-top: 1rem;
  padding-bottom: 2rem;
}


.downlevel .d-appendix {
  padding-left: 0;
  padding-right: 0;
  max-width: none;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.5);
  padding-top: 40px;
  padding-bottom: 48px;
}

.downlevel .footnotes ol {
  padding-left: 13px;
}

.downlevel .base-grid,
.downlevel .distill-header,
.downlevel .d-title,
.downlevel .d-abstract,
.downlevel .d-article,
.downlevel .d-appendix,
.downlevel .distill-appendix,
.downlevel .d-byline,
.downlevel .d-footnote-list,
.downlevel .d-citation-list,
.downlevel .distill-footer,
.downlevel .appendix-bottom,
.downlevel .posts-container {
  padding-left: 40px;
  padding-right: 40px;
}

@media(min-width: 768px) {
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
  padding-left: 150px;
  padding-right: 150px;
  max-width: 900px;
}
}

.downlevel pre code {
  display: block;
  border-left: 2px solid rgba(0, 0, 0, .1);
  padding: 0 0 0 20px;
  font-size: 14px;
}

.downlevel code, .downlevel pre {
  color: black;
  background: none;
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

.downlevel .posts-list .post-preview {
  color: inherit;
}



</style>

<script type="application/javascript">

function is_downlevel_browser() {
  if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                 window.navigator.userAgent)) {
    return true;
  } else {
    return window.load_distill_framework === undefined;
  }
}

// show body when load is complete
function on_load_complete() {

  // add anchors
  if (window.anchors) {
    window.anchors.options.placement = 'left';
    window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
  }


  // set body to visible
  document.body.style.visibility = 'visible';

  // force redraw for leaflet widgets
  if (window.HTMLWidgets) {
    var maps = window.HTMLWidgets.findAll(".leaflet");
    $.each(maps, function(i, el) {
      var map = this.getMap();
      map.invalidateSize();
      map.eachLayer(function(layer) {
        if (layer instanceof L.TileLayer)
          layer.redraw();
      });
    });
  }

  // trigger 'shown' so htmlwidgets resize
  $('d-article').trigger('shown');
}

function init_distill() {

  init_common();

  // create front matter
  var front_matter = $('<d-front-matter></d-front-matter>');
  $('#distill-front-matter').wrap(front_matter);

  // create d-title
  $('.d-title').changeElementType('d-title');

  // create d-byline
  var byline = $('<d-byline></d-byline>');
  $('.d-byline').replaceWith(byline);

  // create d-article
  var article = $('<d-article></d-article>');
  $('.d-article').wrap(article).children().unwrap();

  // move posts container into article
  $('.posts-container').appendTo($('d-article'));

  // create d-appendix
  $('.d-appendix').changeElementType('d-appendix');

  // flag indicating that we have appendix items
  var appendix = $('.appendix-bottom').children('h3').length > 0;

  // replace footnotes with <d-footnote>
  $('.footnote-ref').each(function(i, val) {
    appendix = true;
    var href = $(this).attr('href');
    var id = href.replace('#', '');
    var fn = $('#' + id);
    var fn_p = $('#' + id + '>p');
    fn_p.find('.footnote-back').remove();
    var text = fn_p.html();
    var dtfn = $('<d-footnote></d-footnote>');
    dtfn.html(text);
    $(this).replaceWith(dtfn);
  });
  // remove footnotes
  $('.footnotes').remove();

  // move refs into #references-listing
  $('#references-listing').replaceWith($('#refs'));

  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    var id = $(this).attr('id');
    $('.d-contents a[href="#' + id + '"]').parent().remove();
    appendix = true;
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
  });

  // show d-appendix if we have appendix content
  $("d-appendix").css('display', appendix ? 'grid' : 'none');

  // localize layout chunks to just output
  $('.layout-chunk').each(function(i, val) {

    // capture layout
    var layout = $(this).attr('data-layout');

    // apply layout to markdown level block elements
    var elements = $(this).children().not('div.sourceCode, pre, script');
    elements.each(function(i, el) {
      var layout_div = $('<div class="' + layout + '"></div>');
      if (layout_div.hasClass('shaded')) {
        var shaded_content = $('<div class="shaded-content"></div>');
        $(this).wrap(shaded_content);
        $(this).parent().wrap(layout_div);
      } else {
        $(this).wrap(layout_div);
      }
    });


    // unwrap the layout-chunk div
    $(this).children().unwrap();
  });

  // remove code block used to force  highlighting css
  $('.distill-force-highlighting-css').parent().remove();

  // remove empty line numbers inserted by pandoc when using a
  // custom syntax highlighting theme
  $('code.sourceCode a:empty').remove();

  // load distill framework
  load_distill_framework();

  // wait for window.distillRunlevel == 4 to do post processing
  function distill_post_process() {

    if (!window.distillRunlevel || window.distillRunlevel < 4)
      return;

    // hide author/affiliations entirely if we have no authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
    if (!have_authors)
      $('d-byline').addClass('hidden');

    // article with toc class
    $('.d-contents').parent().addClass('d-article-with-toc');

    // strip links that point to #
    $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

    // add orcid ids
    $('.authors-affiliations').find('.author').each(function(i, el) {
      var orcid_id = front_matter.authors[i].orcidID;
      if (orcid_id) {
        var a = $('<a></a>');
        a.attr('href', 'https://orcid.org/' + orcid_id);
        var img = $('<img></img>');
        img.addClass('orcid-id');
        img.attr('alt', 'ORCID ID');
        img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
        a.append(img);
        $(this).append(a);
      }
    });

    // hide elements of author/affiliations grid that have no value
    function hide_byline_column(caption) {
      $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
    }

    // affiliations
    var have_affiliations = false;
    for (var i = 0; i<front_matter.authors.length; ++i) {
      var author = front_matter.authors[i];
      if (author.affiliation !== "&nbsp;") {
        have_affiliations = true;
        break;
      }
    }
    if (!have_affiliations)
      $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

    // published date
    if (!front_matter.publishedDate)
      hide_byline_column("Published");

    // document object identifier
    var doi = $('d-byline').find('h3:contains("DOI")');
    var doi_p = doi.next().empty();
    if (!front_matter.doi) {
      // if we have a citation and valid citationText then link to that
      if ($('#citation').length > 0 && front_matter.citationText) {
        doi.html('Citation');
        $('<a href="#citation"></a>')
          .text(front_matter.citationText)
          .appendTo(doi_p);
      } else {
        hide_byline_column("DOI");
      }
    } else {
      $('<a></a>')
         .attr('href', "https://doi.org/" + front_matter.doi)
         .html(front_matter.doi)
         .appendTo(doi_p);
    }

     // change plural form of authors/affiliations
    if (front_matter.authors.length === 1) {
      var grid = $('.authors-affiliations');
      grid.children('h3:contains("Authors")').text('Author');
      grid.children('h3:contains("Affiliations")').text('Affiliation');
    }

    // remove d-appendix and d-footnote-list local styles
    $('d-appendix > style:first-child').remove();
    $('d-footnote-list > style:first-child').remove();

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // clear polling timer
    clearInterval(tid);

    // show body now that everything is ready
    on_load_complete();
  }

  var tid = setInterval(distill_post_process, 50);
  distill_post_process();

}

function init_downlevel() {

  init_common();

   // insert hr after d-title
  $('.d-title').after($('<hr class="section-separator"/>'));

  // check if we have authors
  var front_matter = JSON.parse($("#distill-front-matter").html());
  var have_authors = front_matter.authors && front_matter.authors.length > 0;

  // manage byline/border
  if (!have_authors)
    $('.d-byline').remove();
  $('.d-byline').after($('<hr class="section-separator"/>'));
  $('.d-byline a').remove();

  // remove toc
  $('.d-contents').remove();

  // move appendix elements
  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
  });


  // inject headers into references and footnotes
  var refs_header = $('<h3></h3>');
  refs_header.text('References');
  $('#refs').prepend(refs_header);

  var footnotes_header = $('<h3></h3');
  footnotes_header.text('Footnotes');
  $('.footnotes').children('hr').first().replaceWith(footnotes_header);

  // move appendix-bottom entries to the bottom
  $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
  $('.appendix-bottom').remove();

  // remove appendix if it's empty
  if ($('.d-appendix').children().length === 0)
    $('.d-appendix').remove();

  // prepend separator above appendix
  $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

  // trim code
  $('pre>code').each(function(i, val) {
    $(this).html($.trim($(this).html()));
  });

  // move posts-container right before article
  $('.posts-container').insertBefore($('.d-article'));

  $('body').addClass('downlevel');

  on_load_complete();
}


function init_common() {

  // jquery plugin to change element types
  (function($) {
    $.fn.changeElementType = function(newType) {
      var attrs = {};

      $.each(this[0].attributes, function(idx, attr) {
        attrs[attr.nodeName] = attr.nodeValue;
      });

      this.replaceWith(function() {
        return $("<" + newType + "/>", attrs).append($(this).contents());
      });
    };
  })(jQuery);

  // prevent underline for linked images
  $('a > img').parent().css({'border-bottom' : 'none'});

  // mark non-body figures created by knitr chunks as 100% width
  $('.layout-chunk').each(function(i, val) {
    var figures = $(this).find('img, .html-widget');
    if ($(this).attr('data-layout') !== "l-body") {
      figures.css('width', '100%');
    } else {
      figures.css('max-width', '100%');
      figures.filter("[width]").each(function(i, val) {
        var fig = $(this);
        fig.css('width', fig.attr('width') + 'px');
      });

    }
  });

  // auto-append index.html to post-preview links in file: protocol
  // and in rstudio ide preview
  $('.post-preview').each(function(i, val) {
    if (window.location.protocol === "file:")
      $(this).attr('href', $(this).attr('href') + "index.html");
  });

  // get rid of index.html references in header
  if (window.location.protocol !== "file:") {
    $('.distill-site-header a[href]').each(function(i,val) {
      $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
    });
  }

  // add class to pandoc style tables
  $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
  $('.kable-table').children('table').addClass('pandoc-table');

  // add figcaption style to table captions
  $('caption').parent('table').addClass("figcaption");

  // initialize posts list
  if (window.init_posts_list)
    window.init_posts_list();

  // implmement disqus comment link
  $('.disqus-comment-count').click(function() {
    window.headroom_prevent_pin = true;
    $('#disqus_thread').toggleClass('hidden');
    if (!$('#disqus_thread').hasClass('hidden')) {
      var offset = $(this).offset();
      $(window).resize();
      $('html, body').animate({
        scrollTop: offset.top - 35
      });
    }
  });
}

document.addEventListener('DOMContentLoaded', function() {
  if (is_downlevel_browser())
    init_downlevel();
  else
    window.addEventListener('WebComponentsReady', init_distill);
});

</script>

<!--/radix_placeholder_distill-->
  <script src="../../site_libs/header-attrs-2.5/header-attrs.js"></script>
  <script src="../../site_libs/jquery-1.11.3/jquery.min.js"></script>
  <script src="../../site_libs/anchor-4.2.2/anchor.min.js"></script>
  <script src="../../site_libs/bowser-1.9.3/bowser.min.js"></script>
  <script src="../../site_libs/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="../../site_libs/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-20375833-3"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-20375833-3');
</script>
<!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Discrete Representation Learning with VQ-VAE and TensorFlow Probability","description":"Mostly when thinking of Variational Autoencoders (VAEs), we picture the prior as an isotropic Gaussian. But this is by no means a necessity. The Vector Quantised Variational Autoencoder (VQ-VAE) described in van den Oord et al's \"Neural Discrete Representation Learning\" features a discrete latent space that allows to learn impressively concise latent representations. In this post, we combine elements of Keras, TensorFlow, and TensorFlow Probability to see if we can generate convincing letters resembling those in Kuzushiji-MNIST.","authors":[{"author":"Sigrid Keydana","authorURL":"#","affiliation":"RStudio","affiliationURL":"https://www.rstudio.com/","orcidID":""}],"publishedDate":"2019-01-24T00:00:00.000+00:00","citationText":"Keydana, 2019"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<header class="header header--fixed" role="banner">
<nav class="distill-site-nav distill-site-header">
<div class="nav-left">
<span class="logo">
<img src="../../images/rstudio.png"/>
</span>
<a href="../../index.html" class="title">AI Blog</a>
<input id="distill-search" class="nav-search hidden" type="text" placeholder="Search..."/>
</div>
<div class="nav-right">
<a href="../../index.html">Home</a>
<a href="../../gallery.html">Gallery</a>
<a href="../../about.html">About</a>
<a href="../../contributing.html">Contributing</a>
<a href="../../index.xml">
<i class="fa fa-rss" aria-hidden="true"></i>
</a>
<a href="javascript:void(0);" class="nav-toggle">&#9776;</a>
</div>
</nav>
</header>
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Discrete Representation Learning with VQ-VAE and TensorFlow Probability</h1>
<!--radix_placeholder_categories-->
<div class="dt-tags">
  <a href="../../index.html#category:TensorFlow/Keras" class="dt-tag">TensorFlow/Keras</a>
  <a href="../../index.html#category:Probabilistic_ML/DL" class="dt-tag">Probabilistic ML/DL</a>
  <a href="../../index.html#category:Unsupervised_Learning" class="dt-tag">Unsupervised Learning</a>
</div>
<!--/radix_placeholder_categories-->
<p><p>Mostly when thinking of Variational Autoencoders (VAEs), we picture the prior as an isotropic Gaussian. But this is by no means a necessity. The Vector Quantised Variational Autoencoder (VQ-VAE) described in van den Oord et als Neural Discrete Representation Learning features a discrete latent space that allows to learn impressively concise latent representations. In this post, we combine elements of Keras, TensorFlow, and TensorFlow Probability to see if we can generate convincing letters resembling those in Kuzushiji-MNIST.</p></p>
</div>

<div class="d-byline">
  Sigrid Keydana  (RStudio)<a href="https://www.rstudio.com/" class="uri">https://www.rstudio.com/</a>
  
<br/>01-24-2019
</div>

<div class="d-article">
<p>About two weeks ago, we <a href="https://blogs.rstudio.com/tensorflow/posts/2019-01-08-getting-started-with-tf-probability/">introduced TensorFlow Probability (TFP)</a>, showing how to create and sample from <em>distributions</em> and put them to use in a Variational Autoencoder (VAE) that learns its prior. Today, we move on to a different specimen in the VAE model zoo: the Vector Quantised Variational Autoencoder (VQ-VAE) described in <em>Neural Discrete Representation Learning</em> <span class="citation" data-cites="abs-1711-00937">(Oord, Vinyals, and Kavukcuoglu <a href="#ref-abs-1711-00937" role="doc-biblioref">2017</a>)</span>. This model differs from most VAEs in that its approximate posterior is not continuous, but discrete - hence the quantised in the articles title. Well quickly look at what this means, and then dive directly into the code, combining Keras layers, eager execution, and TFP.</p>
<h1 id="discrete-codes">Discrete codes</h1>
<p>Many phenomena are best thought of, and modeled, as discrete. This holds for phonemes and lexemes in language, higher-level structures in images (think objects instead of pixels),and tasks that necessitate reasoning and planning. The latent code used in most VAEs, however, is continuous - usually its a multivariate Gaussian. Continuous-space VAEs have been found very successful in reconstructing their input, but often they suffer from something called <em>posterior collapse</em>: The decoder is so powerful that it may create realistic output given just <em>any</em> input. This means there is no incentive to learn an expressive latent space.</p>
<p>In VQ-VAE, however, each input sample gets mapped deterministically to one of a set of <em>embedding vectors</em>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Together, these embedding vectors constitute the prior for the latent space. As such, an embedding vector contains a lot more information than a mean and a variance, and thus, is much harder to ignore by the decoder.</p>
<p>The question then is: Where is that magical hat, for us to pull out meaningful embeddings?</p>
<h1 id="learning-a-discrete-embedding-space">Learning a discrete embedding space</h1>
<p>From the above conceptual description, we now have two questions to answer. First, by what mechanism do we assign input samples (that went through the encoder) to appropriate embedding vectors? And second: How can we learn embedding vectors that actually are useful representations - that when fed to a decoder, will result in entities perceived as belonging to the same species?</p>
<p>As regards assignment, a tensor emitted from the encoder is simply mapped to its nearest neighbor in embedding space, using Euclidean distance. The embedding vectors are then updated using exponential moving averages.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> As well see soon, this means that they are actually not being learned using gradient descent - a feature worth pointing out as we dont come across it every day in deep learning.</p>
<p>Concretely, how then should the loss function and training process look? This will probably easiest be seen in code.</p>
<h1 id="coding-the-vq-vae">Coding the VQ-VAE</h1>
<p>The complete code for this example, including utilities for model saving and image visualization, is <a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/vq_vae.R">available on github</a> as part of the Keras examples. Order of presentation here may differ from actual execution order for expository purposes, so please to actually run the code consider making use of the example on github.</p>
<h1 id="setup-and-data-loading">Setup and data loading</h1>
<p>As in all our prior posts on VAEs, we use eager execution, which presupposes the TensorFlow implementation of Keras.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'>keras</span><span class='op'>)</span>
<span class='fu'>use_implementation</span><span class='op'>(</span><span class='st'>"tensorflow"</span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'>tensorflow</span><span class='op'>)</span>
<span class='fu'>tfe_enable_eager_execution</span><span class='op'>(</span>device_policy <span class='op'>=</span> <span class='st'>"silent"</span><span class='op'>)</span>

<span class='va'>tfp</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/pkg/reticulate/man/import.html'>import</a></span><span class='op'>(</span><span class='st'>"tensorflow_probability"</span><span class='op'>)</span>
<span class='va'>tfd</span> <span class='op'>&lt;-</span> <span class='va'>tfp</span><span class='op'>$</span><span class='va'>distributions</span>

<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'>tfdatasets</span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://dplyr.tidyverse.org'>dplyr</a></span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://github.com/tidyverse/glue'>glue</a></span><span class='op'>)</span>
<span class='co'># used for set_defaults; please get the development version:</span>
<span class='co'># devtools::install_github("thomasp85/curry")</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'>curry</span><span class='op'>)</span> 
</code></pre>
</div>
</div>
<p>As in our previous post on doing VAE with TFP, well use <a href="https://github.com/rois-codh/kmnist">Kuzushiji-MNIST</a><span class="citation" data-cites="clanuwat2018deep">(Clanuwat et al. <a href="#ref-clanuwat2018deep" role="doc-biblioref">2018</a>)</span> as input. Now is the time to look at <a href="https://blogs.rstudio.com/tensorflow/posts/2019-01-08-getting-started-with-tf-probability/images/results.png">what we ended up generating that time</a> and place your bet: How will that compare against the discrete latent space of VQ-VAE?</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>np</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/pkg/reticulate/man/import.html'>import</a></span><span class='op'>(</span><span class='st'>"numpy"</span><span class='op'>)</span>
 
<span class='va'>kuzushiji</span> <span class='op'>&lt;-</span> <span class='va'>np</span><span class='op'>$</span><span class='fu'>load</span><span class='op'>(</span><span class='st'>"kmnist-train-imgs.npz"</span><span class='op'>)</span>
<span class='va'>kuzushiji</span> <span class='op'>&lt;-</span> <span class='va'>kuzushiji</span><span class='op'>$</span><span class='fu'>get</span><span class='op'>(</span><span class='st'>"arr_0"</span><span class='op'>)</span>

<span class='va'>train_images</span> <span class='op'>&lt;-</span> <span class='va'>kuzushiji</span> <span class='op'>%&gt;%</span>
  <span class='fu'>k_expand_dims</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>k_cast</span><span class='op'>(</span>dtype <span class='op'>=</span> <span class='st'>"float32"</span><span class='op'>)</span>

<span class='va'>train_images</span> <span class='op'>&lt;-</span> <span class='va'>train_images</span> <span class='op'>%&gt;%</span> <span class='fu'>`/`</span><span class='op'>(</span><span class='fl'>255</span><span class='op'>)</span>

<span class='va'>buffer_size</span> <span class='op'>&lt;-</span> <span class='fl'>60000</span>
<span class='va'>batch_size</span> <span class='op'>&lt;-</span> <span class='fl'>64</span>
<span class='va'>num_examples_to_generate</span> <span class='op'>&lt;-</span> <span class='va'>batch_size</span>

<span class='va'>batches_per_epoch</span> <span class='op'>&lt;-</span> <span class='va'>buffer_size</span> <span class='op'>/</span> <span class='va'>batch_size</span>

<span class='va'>train_dataset</span> <span class='op'>&lt;-</span> <span class='fu'>tensor_slices_dataset</span><span class='op'>(</span><span class='va'>train_images</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>dataset_shuffle</span><span class='op'>(</span><span class='va'>buffer_size</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>dataset_batch</span><span class='op'>(</span><span class='va'>batch_size</span>, drop_remainder <span class='op'>=</span> <span class='cn'>TRUE</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<h2 id="hyperparameters">Hyperparameters</h2>
<p>In addition to the usual hyperparameters we have in deep learning, the VQ-VAE infrastructure introduces a few model-specific ones. First of all, the embedding space is of dimensionality <em>number of embedding vectors</em> times <em>embedding vector size</em>:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='co'># number of embedding vectors</span>
<span class='va'>num_codes</span> <span class='op'>&lt;-</span> <span class='fl'>64L</span>
<span class='co'># dimensionality of the embedding vectors</span>
<span class='va'>code_size</span> <span class='op'>&lt;-</span> <span class='fl'>16L</span>
</code></pre>
</div>
</div>
<p>The latent space in our example will be of size one, that is, we have a single embedding vector representing the latent code for each input sample. This will be fine for our dataset, but it should be noted that van den Oord et al.used far higher-dimensional latent spaces on e.g.ImageNet and Cifar-10.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>latent_size</span> <span class='op'>&lt;-</span> <span class='fl'>1</span>
</code></pre>
</div>
</div>
<h2 id="encoder-model">Encoder model</h2>
<p>The encoder uses convolutional layers to extract image features. Its output is a 3-d tensor of shape <em>batchsize</em> * 1 * <em>code_size</em>.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>activation</span> <span class='op'>&lt;-</span> <span class='st'>"elu"</span>
<span class='co'># modularizing the code just a little bit</span>
<span class='va'>default_conv</span> <span class='op'>&lt;-</span> <span class='fu'>set_defaults</span><span class='op'>(</span><span class='va'>layer_conv_2d</span>, <span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span>padding <span class='op'>=</span> <span class='st'>"same"</span>, activation <span class='op'>=</span> <span class='va'>activation</span><span class='op'>)</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>base_depth</span> <span class='op'>&lt;-</span> <span class='fl'>32</span>

<span class='va'>encoder_model</span> <span class='op'>&lt;-</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>name</span> <span class='op'>=</span> <span class='cn'>NULL</span>,
                          <span class='va'>code_size</span><span class='op'>)</span> <span class='op'>{</span>
  
  <span class='fu'>keras_model_custom</span><span class='op'>(</span>name <span class='op'>=</span> <span class='va'>name</span>, <span class='kw'>function</span><span class='op'>(</span><span class='va'>self</span><span class='op'>)</span> <span class='op'>{</span>
    
    <span class='va'>self</span><span class='op'>$</span><span class='va'>conv1</span> <span class='op'>&lt;-</span> <span class='fu'>default_conv</span><span class='op'>(</span>filters <span class='op'>=</span> <span class='va'>base_depth</span>, kernel_size <span class='op'>=</span> <span class='fl'>5</span><span class='op'>)</span>
    <span class='va'>self</span><span class='op'>$</span><span class='va'>conv2</span> <span class='op'>&lt;-</span> <span class='fu'>default_conv</span><span class='op'>(</span>filters <span class='op'>=</span> <span class='va'>base_depth</span>, kernel_size <span class='op'>=</span> <span class='fl'>5</span>, strides <span class='op'>=</span> <span class='fl'>2</span><span class='op'>)</span>
    <span class='va'>self</span><span class='op'>$</span><span class='va'>conv3</span> <span class='op'>&lt;-</span> <span class='fu'>default_conv</span><span class='op'>(</span>filters <span class='op'>=</span> <span class='fl'>2</span> <span class='op'>*</span> <span class='va'>base_depth</span>, kernel_size <span class='op'>=</span> <span class='fl'>5</span><span class='op'>)</span>
    <span class='va'>self</span><span class='op'>$</span><span class='va'>conv4</span> <span class='op'>&lt;-</span> <span class='fu'>default_conv</span><span class='op'>(</span>filters <span class='op'>=</span> <span class='fl'>2</span> <span class='op'>*</span> <span class='va'>base_depth</span>, kernel_size <span class='op'>=</span> <span class='fl'>5</span>, strides <span class='op'>=</span> <span class='fl'>2</span><span class='op'>)</span>
    <span class='va'>self</span><span class='op'>$</span><span class='va'>conv5</span> <span class='op'>&lt;-</span> <span class='fu'>default_conv</span><span class='op'>(</span>filters <span class='op'>=</span> <span class='fl'>4</span> <span class='op'>*</span> <span class='va'>latent_size</span>, kernel_size <span class='op'>=</span> <span class='fl'>7</span>, padding <span class='op'>=</span> <span class='st'>"valid"</span><span class='op'>)</span>
    <span class='va'>self</span><span class='op'>$</span><span class='va'>flatten</span> <span class='op'>&lt;-</span> <span class='fu'>layer_flatten</span><span class='op'>(</span><span class='op'>)</span>
    <span class='va'>self</span><span class='op'>$</span><span class='va'>dense</span> <span class='op'>&lt;-</span> <span class='fu'>layer_dense</span><span class='op'>(</span>units <span class='op'>=</span> <span class='va'>latent_size</span> <span class='op'>*</span> <span class='va'>code_size</span><span class='op'>)</span>
    <span class='va'>self</span><span class='op'>$</span><span class='va'>reshape</span> <span class='op'>&lt;-</span> <span class='fu'>layer_reshape</span><span class='op'>(</span>target_shape <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='va'>latent_size</span>, <span class='va'>code_size</span><span class='op'>)</span><span class='op'>)</span>
    
    <span class='kw'>function</span> <span class='op'>(</span><span class='va'>x</span>, <span class='va'>mask</span> <span class='op'>=</span> <span class='cn'>NULL</span><span class='op'>)</span> <span class='op'>{</span>
      <span class='va'>x</span> <span class='op'>%&gt;%</span> 
        <span class='co'># output shape:  7 28 28 32 </span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>conv1</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span> 
        <span class='co'># output shape:  7 14 14 32 </span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>conv2</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span> 
        <span class='co'># output shape:  7 14 14 64 </span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>conv3</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span> 
        <span class='co'># output shape:  7 7 7 64 </span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>conv4</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span> 
        <span class='co'># output shape:  7 1 1 4 </span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>conv5</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span> 
        <span class='co'># output shape:  7 4 </span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>flatten</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span> 
        <span class='co'># output shape:  7 16 </span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>dense</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span> 
        <span class='co'># output shape:  7 1 16</span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>reshape</span><span class='op'>(</span><span class='op'>)</span>
    <span class='op'>}</span>
  <span class='op'>}</span><span class='op'>)</span>
<span class='op'>}</span>
</code></pre>
</div>
</div>
<p>As always, lets make use of the fact that were using eager execution, and see a few example outputs.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>iter</span> <span class='op'>&lt;-</span> <span class='fu'>make_iterator_one_shot</span><span class='op'>(</span><span class='va'>train_dataset</span><span class='op'>)</span>
<span class='va'>batch</span> <span class='op'>&lt;-</span>  <span class='fu'>iterator_get_next</span><span class='op'>(</span><span class='va'>iter</span><span class='op'>)</span>

<span class='va'>encoder</span> <span class='op'>&lt;-</span> <span class='fu'>encoder_model</span><span class='op'>(</span>code_size <span class='op'>=</span> <span class='va'>code_size</span><span class='op'>)</span>
<span class='va'>encoded</span>  <span class='op'>&lt;-</span> <span class='fu'>encoder</span><span class='op'>(</span><span class='va'>batch</span><span class='op'>)</span>
<span class='va'>encoded</span>
</code></pre>
</div>
</div>
<pre><code>tf.Tensor(
[[[ 0.00516277 -0.00746826  0.0268365  ... -0.012577   -0.07752544
   -0.02947626]]
...

 [[-0.04757921 -0.07282603 -0.06814402 ... -0.10861694 -0.01237121
    0.11455103]]], shape=(64, 1, 16), dtype=float32)</code></pre>
<p>Now, each of these 16d vectors needs to be mapped to the embedding vector it is closest to. This mapping is taken care of by another model: <code>vector_quantizer</code>.</p>
<h2 id="vector-quantizer-model">Vector quantizer model</h2>
<p>This is how we will instantiate the vector quantizer:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>vector_quantizer</span> <span class='op'>&lt;-</span> <span class='fu'>vector_quantizer_model</span><span class='op'>(</span>num_codes <span class='op'>=</span> <span class='va'>num_codes</span>, code_size <span class='op'>=</span> <span class='va'>code_size</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>This model serves two purposes: First, it acts as a store for the embedding vectors. Second, it matches encoder output to available embeddings.</p>
<p>Here, the current state of embeddings is stored in <code>codebook</code>. <code>ema_means</code> and <code>ema_count</code> are for bookkeeping purposes only (note how they are set to be non-trainable). Well see them in use shortly.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>vector_quantizer_model</span> <span class='op'>&lt;-</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>name</span> <span class='op'>=</span> <span class='cn'>NULL</span>, <span class='va'>num_codes</span>, <span class='va'>code_size</span><span class='op'>)</span> <span class='op'>{</span>
  
    <span class='fu'>keras_model_custom</span><span class='op'>(</span>name <span class='op'>=</span> <span class='va'>name</span>, <span class='kw'>function</span><span class='op'>(</span><span class='va'>self</span><span class='op'>)</span> <span class='op'>{</span>
      
      <span class='va'>self</span><span class='op'>$</span><span class='va'>num_codes</span> <span class='op'>&lt;-</span> <span class='va'>num_codes</span>
      <span class='va'>self</span><span class='op'>$</span><span class='va'>code_size</span> <span class='op'>&lt;-</span> <span class='va'>code_size</span>
      <span class='va'>self</span><span class='op'>$</span><span class='va'>codebook</span> <span class='op'>&lt;-</span> <span class='va'>tf</span><span class='op'>$</span><span class='fu'>get_variable</span><span class='op'>(</span>
        <span class='st'>"codebook"</span>,
        shape <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='va'>num_codes</span>, <span class='va'>code_size</span><span class='op'>)</span>, 
        dtype <span class='op'>=</span> <span class='va'>tf</span><span class='op'>$</span><span class='va'>float32</span>
        <span class='op'>)</span>
      <span class='va'>self</span><span class='op'>$</span><span class='va'>ema_count</span> <span class='op'>&lt;-</span> <span class='va'>tf</span><span class='op'>$</span><span class='fu'>get_variable</span><span class='op'>(</span>
        name <span class='op'>=</span> <span class='st'>"ema_count"</span>, shape <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='va'>num_codes</span><span class='op'>)</span>,
        initializer <span class='op'>=</span> <span class='va'>tf</span><span class='op'>$</span><span class='fu'>constant_initializer</span><span class='op'>(</span><span class='fl'>0</span><span class='op'>)</span>,
        trainable <span class='op'>=</span> <span class='cn'>FALSE</span>
        <span class='op'>)</span>
      <span class='va'>self</span><span class='op'>$</span><span class='va'>ema_means</span> <span class='op'>=</span> <span class='va'>tf</span><span class='op'>$</span><span class='fu'>get_variable</span><span class='op'>(</span>
        name <span class='op'>=</span> <span class='st'>"ema_means"</span>,
        initializer <span class='op'>=</span> <span class='va'>self</span><span class='op'>$</span><span class='va'>codebook</span><span class='op'>$</span><span class='fu'>initialized_value</span><span class='op'>(</span><span class='op'>)</span>,
        trainable <span class='op'>=</span> <span class='cn'>FALSE</span>
        <span class='op'>)</span>
      
      <span class='kw'>function</span> <span class='op'>(</span><span class='va'>x</span>, <span class='va'>mask</span> <span class='op'>=</span> <span class='cn'>NULL</span><span class='op'>)</span> <span class='op'>{</span> 
        
        <span class='co'># to be filled in shortly ...</span>
        
      <span class='op'>}</span>
    <span class='op'>}</span><span class='op'>)</span>
<span class='op'>}</span>
</code></pre>
</div>
</div>
<p>In addition to the actual embeddings, in its <code>call</code> method <code>vector_quantizer</code> holds the assignment logic. First, we compute the Euclidean distance of each encoding to the vectors in the codebook (<code>tf$norm</code>). We assign each encoding to the closest as by that distance embedding (<code>tf$argmin</code>) and one-hot-encode the assignments (<code>tf$one_hot</code>). Finally, we isolate the corresponding vector by masking out all others and summing up whats left over (multiplication followed by <code>tf$reduce_sum</code>).</p>
<p>Regarding the <code>axis</code> argument used with many TensorFlow functions, please take into consideration that in contrast to their <code>k_*</code> siblings, raw TensorFlow (<code>tf$*</code>) functions expect axis numbering to be 0-based. We also have to add the <code>L</code>s after the numbers to conform to TensorFlows datatype requirements.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>vector_quantizer_model</span> <span class='op'>&lt;-</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>name</span> <span class='op'>=</span> <span class='cn'>NULL</span>, <span class='va'>num_codes</span>, <span class='va'>code_size</span><span class='op'>)</span> <span class='op'>{</span>
  
    <span class='fu'>keras_model_custom</span><span class='op'>(</span>name <span class='op'>=</span> <span class='va'>name</span>, <span class='kw'>function</span><span class='op'>(</span><span class='va'>self</span><span class='op'>)</span> <span class='op'>{</span>
      
      <span class='co'># here we have the above instance fields</span>
      
      <span class='kw'>function</span> <span class='op'>(</span><span class='va'>x</span>, <span class='va'>mask</span> <span class='op'>=</span> <span class='cn'>NULL</span><span class='op'>)</span> <span class='op'>{</span>
    
        <span class='co'># shape: bs * 1 * num_codes</span>
         <span class='va'>distances</span> <span class='op'>&lt;-</span> <span class='va'>tf</span><span class='op'>$</span><span class='fu'>norm</span><span class='op'>(</span>
          <span class='va'>tf</span><span class='op'>$</span><span class='fu'>expand_dims</span><span class='op'>(</span><span class='va'>x</span>, axis <span class='op'>=</span> <span class='fl'>2L</span><span class='op'>)</span> <span class='op'>-</span>
            <span class='va'>tf</span><span class='op'>$</span><span class='fu'>reshape</span><span class='op'>(</span><span class='va'>self</span><span class='op'>$</span><span class='va'>codebook</span>, 
                       <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>1L</span>, <span class='fl'>1L</span>, <span class='va'>self</span><span class='op'>$</span><span class='va'>num_codes</span>, <span class='va'>self</span><span class='op'>$</span><span class='va'>code_size</span><span class='op'>)</span><span class='op'>)</span>,
                       axis <span class='op'>=</span> <span class='fl'>3L</span> 
        <span class='op'>)</span>
        
        <span class='co'># bs * 1</span>
        <span class='va'>assignments</span> <span class='op'>&lt;-</span> <span class='va'>tf</span><span class='op'>$</span><span class='fu'>argmin</span><span class='op'>(</span><span class='va'>distances</span>, axis <span class='op'>=</span> <span class='fl'>2L</span><span class='op'>)</span>
        
        <span class='co'># bs * 1 * num_codes</span>
        <span class='va'>one_hot_assignments</span> <span class='op'>&lt;-</span> <span class='va'>tf</span><span class='op'>$</span><span class='fu'>one_hot</span><span class='op'>(</span><span class='va'>assignments</span>, depth <span class='op'>=</span> <span class='va'>self</span><span class='op'>$</span><span class='va'>num_codes</span><span class='op'>)</span>
        
        <span class='co'># bs * 1 * code_size</span>
        <span class='va'>nearest_codebook_entries</span> <span class='op'>&lt;-</span> <span class='va'>tf</span><span class='op'>$</span><span class='fu'>reduce_sum</span><span class='op'>(</span>
          <span class='va'>tf</span><span class='op'>$</span><span class='fu'>expand_dims</span><span class='op'>(</span>
            <span class='va'>one_hot_assignments</span>, <span class='op'>-</span><span class='fl'>1L</span><span class='op'>)</span> <span class='op'>*</span> 
            <span class='va'>tf</span><span class='op'>$</span><span class='fu'>reshape</span><span class='op'>(</span><span class='va'>self</span><span class='op'>$</span><span class='va'>codebook</span>, <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>1L</span>, <span class='fl'>1L</span>, <span class='va'>self</span><span class='op'>$</span><span class='va'>num_codes</span>, <span class='va'>self</span><span class='op'>$</span><span class='va'>code_size</span><span class='op'>)</span><span class='op'>)</span>,
                       axis <span class='op'>=</span> <span class='fl'>2L</span> 
                       <span class='op'>)</span>
        <span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span><span class='va'>nearest_codebook_entries</span>, <span class='va'>one_hot_assignments</span><span class='op'>)</span>
      <span class='op'>}</span>
    <span class='op'>}</span><span class='op'>)</span>
  <span class='op'>}</span>
</code></pre>
</div>
</div>
<p>Now that weve seen how the codes are stored, lets add functionality for updating them. As we said above, they are not learned via gradient descent. Instead, they are exponential moving averages, continually updated by whatever new class member they get assigned.</p>
<p>So here is a function <code>update_ema</code> that will take care of this.</p>
<p><code>update_ema</code> uses TensorFlow <a href="https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage">moving_averages</a> to</p>
<ul>
<li>first, keep track of the number of currently assigned samples per code (<code>updated_ema_count</code>), and</li>
<li>second, compute and assign the current exponential moving average (<code>updated_ema_means</code>).</li>
</ul>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>moving_averages</span> <span class='op'>&lt;-</span> <span class='va'>tf</span><span class='op'>$</span><span class='va'>python</span><span class='op'>$</span><span class='va'>training</span><span class='op'>$</span><span class='va'>moving_averages</span>

<span class='co'># decay to use in computing exponential moving average</span>
<span class='va'>decay</span> <span class='op'>&lt;-</span> <span class='fl'>0.99</span>

<span class='va'>update_ema</span> <span class='op'>&lt;-</span> <span class='kw'>function</span><span class='op'>(</span>
  <span class='va'>vector_quantizer</span>,
  <span class='va'>one_hot_assignments</span>,
  <span class='va'>codes</span>,
  <span class='va'>decay</span><span class='op'>)</span> <span class='op'>{</span>
 
  <span class='va'>updated_ema_count</span> <span class='op'>&lt;-</span> <span class='va'>moving_averages</span><span class='op'>$</span><span class='fu'>assign_moving_average</span><span class='op'>(</span>
    <span class='va'>vector_quantizer</span><span class='op'>$</span><span class='va'>ema_count</span>,
    <span class='va'>tf</span><span class='op'>$</span><span class='fu'>reduce_sum</span><span class='op'>(</span><span class='va'>one_hot_assignments</span>, axis <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>0L</span>, <span class='fl'>1L</span><span class='op'>)</span><span class='op'>)</span>,
    <span class='va'>decay</span>,
    zero_debias <span class='op'>=</span> <span class='cn'>FALSE</span>
  <span class='op'>)</span>

  <span class='va'>updated_ema_means</span> <span class='op'>&lt;-</span> <span class='va'>moving_averages</span><span class='op'>$</span><span class='fu'>assign_moving_average</span><span class='op'>(</span>
    <span class='va'>vector_quantizer</span><span class='op'>$</span><span class='va'>ema_means</span>,
    <span class='co'># selects all assigned values (masking out the others) and sums them up over the batch</span>
    <span class='co'># (will be divided by count later, so we get an average)</span>
    <span class='va'>tf</span><span class='op'>$</span><span class='fu'>reduce_sum</span><span class='op'>(</span>
      <span class='va'>tf</span><span class='op'>$</span><span class='fu'>expand_dims</span><span class='op'>(</span><span class='va'>codes</span>, <span class='fl'>2L</span><span class='op'>)</span> <span class='op'>*</span>
        <span class='va'>tf</span><span class='op'>$</span><span class='fu'>expand_dims</span><span class='op'>(</span><span class='va'>one_hot_assignments</span>, <span class='fl'>3L</span><span class='op'>)</span>, axis <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>0L</span>, <span class='fl'>1L</span><span class='op'>)</span><span class='op'>)</span>,
    <span class='va'>decay</span>,
    zero_debias <span class='op'>=</span> <span class='cn'>FALSE</span>
  <span class='op'>)</span>

  <span class='va'>updated_ema_count</span> <span class='op'>&lt;-</span> <span class='va'>updated_ema_count</span> <span class='op'>+</span> <span class='fl'>1e-5</span>
  <span class='va'>updated_ema_means</span> <span class='op'>&lt;-</span>  <span class='va'>updated_ema_means</span> <span class='op'>/</span> <span class='va'>tf</span><span class='op'>$</span><span class='fu'>expand_dims</span><span class='op'>(</span><span class='va'>updated_ema_count</span>, axis <span class='op'>=</span> <span class='op'>-</span><span class='fl'>1L</span><span class='op'>)</span>
  
  <span class='va'>tf</span><span class='op'>$</span><span class='fu'>assign</span><span class='op'>(</span><span class='va'>vector_quantizer</span><span class='op'>$</span><span class='va'>codebook</span>, <span class='va'>updated_ema_means</span><span class='op'>)</span>
<span class='op'>}</span>
</code></pre>
</div>
</div>
<p>Before we look at the training loop, lets quickly complete the scene adding in the last actor, the decoder.</p>
<h2 id="decoder-model">Decoder model</h2>
<p>The decoder is pretty standard, performing a series of deconvolutions and finally, returning a probability for each image pixel.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>default_deconv</span> <span class='op'>&lt;-</span> <span class='fu'>set_defaults</span><span class='op'>(</span>
  <span class='va'>layer_conv_2d_transpose</span>,
  <span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span>padding <span class='op'>=</span> <span class='st'>"same"</span>, activation <span class='op'>=</span> <span class='va'>activation</span><span class='op'>)</span>
<span class='op'>)</span>

<span class='va'>decoder_model</span> <span class='op'>&lt;-</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>name</span> <span class='op'>=</span> <span class='cn'>NULL</span>,
                          <span class='va'>input_size</span>,
                          <span class='va'>output_shape</span><span class='op'>)</span> <span class='op'>{</span>
  
  <span class='fu'>keras_model_custom</span><span class='op'>(</span>name <span class='op'>=</span> <span class='va'>name</span>, <span class='kw'>function</span><span class='op'>(</span><span class='va'>self</span><span class='op'>)</span> <span class='op'>{</span>
    
    <span class='va'>self</span><span class='op'>$</span><span class='va'>reshape1</span> <span class='op'>&lt;-</span> <span class='fu'>layer_reshape</span><span class='op'>(</span>target_shape <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>1</span>, <span class='fl'>1</span>, <span class='va'>input_size</span><span class='op'>)</span><span class='op'>)</span>
    <span class='va'>self</span><span class='op'>$</span><span class='va'>deconv1</span> <span class='op'>&lt;-</span>
      <span class='fu'>default_deconv</span><span class='op'>(</span>
        filters <span class='op'>=</span> <span class='fl'>2</span> <span class='op'>*</span> <span class='va'>base_depth</span>,
        kernel_size <span class='op'>=</span> <span class='fl'>7</span>,
        padding <span class='op'>=</span> <span class='st'>"valid"</span>
      <span class='op'>)</span>
    <span class='va'>self</span><span class='op'>$</span><span class='va'>deconv2</span> <span class='op'>&lt;-</span>
      <span class='fu'>default_deconv</span><span class='op'>(</span>filters <span class='op'>=</span> <span class='fl'>2</span> <span class='op'>*</span> <span class='va'>base_depth</span>, kernel_size <span class='op'>=</span> <span class='fl'>5</span><span class='op'>)</span>
    <span class='va'>self</span><span class='op'>$</span><span class='va'>deconv3</span> <span class='op'>&lt;-</span>
      <span class='fu'>default_deconv</span><span class='op'>(</span>
        filters <span class='op'>=</span> <span class='fl'>2</span> <span class='op'>*</span> <span class='va'>base_depth</span>,
        kernel_size <span class='op'>=</span> <span class='fl'>5</span>,
        strides <span class='op'>=</span> <span class='fl'>2</span>
      <span class='op'>)</span>
    <span class='va'>self</span><span class='op'>$</span><span class='va'>deconv4</span> <span class='op'>&lt;-</span>
      <span class='fu'>default_deconv</span><span class='op'>(</span>filters <span class='op'>=</span> <span class='va'>base_depth</span>, kernel_size <span class='op'>=</span> <span class='fl'>5</span><span class='op'>)</span>
    <span class='va'>self</span><span class='op'>$</span><span class='va'>deconv5</span> <span class='op'>&lt;-</span>
      <span class='fu'>default_deconv</span><span class='op'>(</span>filters <span class='op'>=</span> <span class='va'>base_depth</span>,
                     kernel_size <span class='op'>=</span> <span class='fl'>5</span>,
                     strides <span class='op'>=</span> <span class='fl'>2</span><span class='op'>)</span>
    <span class='va'>self</span><span class='op'>$</span><span class='va'>deconv6</span> <span class='op'>&lt;-</span>
      <span class='fu'>default_deconv</span><span class='op'>(</span>filters <span class='op'>=</span> <span class='va'>base_depth</span>, kernel_size <span class='op'>=</span> <span class='fl'>5</span><span class='op'>)</span>
    <span class='va'>self</span><span class='op'>$</span><span class='va'>conv1</span> <span class='op'>&lt;-</span>
      <span class='fu'>default_conv</span><span class='op'>(</span>filters <span class='op'>=</span> <span class='va'>output_shape</span><span class='op'>[</span><span class='fl'>3</span><span class='op'>]</span>,
                   kernel_size <span class='op'>=</span> <span class='fl'>5</span>,
                   activation <span class='op'>=</span> <span class='st'>"linear"</span><span class='op'>)</span>
    
    <span class='kw'>function</span> <span class='op'>(</span><span class='va'>x</span>, <span class='va'>mask</span> <span class='op'>=</span> <span class='cn'>NULL</span><span class='op'>)</span> <span class='op'>{</span>
      
      <span class='va'>x</span> <span class='op'>&lt;-</span> <span class='va'>x</span> <span class='op'>%&gt;%</span>
        <span class='co'># output shape:  7 1 1 16</span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>reshape1</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
        <span class='co'># output shape:  7 7 7 64</span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>deconv1</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
        <span class='co'># output shape:  7 7 7 64</span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>deconv2</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
        <span class='co'># output shape:  7 14 14 64</span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>deconv3</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
        <span class='co'># output shape:  7 14 14 32</span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>deconv4</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
        <span class='co'># output shape:  7 28 28 32</span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>deconv5</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
        <span class='co'># output shape:  7 28 28 32</span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>deconv6</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
        <span class='co'># output shape:  7 28 28 1</span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>conv1</span><span class='op'>(</span><span class='op'>)</span>
      
      <span class='va'>tfd</span><span class='op'>$</span><span class='fu'>Independent</span><span class='op'>(</span><span class='va'>tfd</span><span class='op'>$</span><span class='fu'>Bernoulli</span><span class='op'>(</span>logits <span class='op'>=</span> <span class='va'>x</span><span class='op'>)</span>,
                      reinterpreted_batch_ndims <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/length.html'>length</a></span><span class='op'>(</span><span class='va'>output_shape</span><span class='op'>)</span><span class='op'>)</span>
    <span class='op'>}</span>
  <span class='op'>}</span><span class='op'>)</span>
<span class='op'>}</span>

<span class='va'>input_shape</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>28</span>, <span class='fl'>28</span>, <span class='fl'>1</span><span class='op'>)</span>
<span class='va'>decoder</span> <span class='op'>&lt;-</span> <span class='fu'>decoder_model</span><span class='op'>(</span>input_size <span class='op'>=</span> <span class='va'>latent_size</span> <span class='op'>*</span> <span class='va'>code_size</span>,
                         output_shape <span class='op'>=</span> <span class='va'>input_shape</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>Now were ready to train. One thing we havent really talked about yet is the cost function: Given the differences in architecture (compared to standard VAEs), will the losses still look as expected (the usual add-up of reconstruction loss and KL divergence)? Well see that in a second.</p>
<h2 id="training-loop">Training loop</h2>
<p>Heres the optimizer well use. Losses will be calculated inline.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>optimizer</span> <span class='op'>&lt;-</span> <span class='va'>tf</span><span class='op'>$</span><span class='va'>train</span><span class='op'>$</span><span class='fu'>AdamOptimizer</span><span class='op'>(</span>learning_rate <span class='op'>=</span> <span class='va'>learning_rate</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>The training loop, as usual, is a loop over epochs, where each iteration is a loop over batches obtained from the dataset. For each batch, we have a forward pass, recorded by a <code>gradientTape</code>, based on which we calculate the loss. The tape will then determine the gradients of all trainable weights throughout the model, and the optimizer will use those gradients to update the weights.</p>
<p>So far, all of this conforms to a scheme weve oftentimes seen before. One point to note though: In this same loop, we also call <code>update_ema</code> to recalculate the moving averages, as those are not operated on during backprop. Here is the essential functionality:<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>num_epochs</span> <span class='op'>&lt;-</span> <span class='fl'>20</span>

<span class='kw'>for</span> <span class='op'>(</span><span class='va'>epoch</span> <span class='kw'>in</span> <span class='fu'><a href='https://rdrr.io/r/base/seq.html'>seq_len</a></span><span class='op'>(</span><span class='va'>num_epochs</span><span class='op'>)</span><span class='op'>)</span> <span class='op'>{</span>
  
  <span class='va'>iter</span> <span class='op'>&lt;-</span> <span class='fu'>make_iterator_one_shot</span><span class='op'>(</span><span class='va'>train_dataset</span><span class='op'>)</span>
  
  <span class='fu'>until_out_of_range</span><span class='op'>(</span><span class='op'>{</span>
    
    <span class='va'>x</span> <span class='op'>&lt;-</span>  <span class='fu'>iterator_get_next</span><span class='op'>(</span><span class='va'>iter</span><span class='op'>)</span>
    <span class='fu'><a href='https://rdrr.io/r/base/with.html'>with</a></span><span class='op'>(</span><span class='va'>tf</span><span class='op'>$</span><span class='fu'>GradientTape</span><span class='op'>(</span>persistent <span class='op'>=</span> <span class='cn'>TRUE</span><span class='op'>)</span> <span class='op'>%as%</span> <span class='va'>tape</span>, <span class='op'>{</span>
      
      <span class='co'># do forward pass</span>
      <span class='co'># calculate losses</span>
      
    <span class='op'>}</span><span class='op'>)</span>
    
    <span class='va'>encoder_gradients</span> <span class='op'>&lt;-</span> <span class='va'>tape</span><span class='op'>$</span><span class='fu'>gradient</span><span class='op'>(</span><span class='va'>loss</span>, <span class='va'>encoder</span><span class='op'>$</span><span class='va'>variables</span><span class='op'>)</span>
    <span class='va'>decoder_gradients</span> <span class='op'>&lt;-</span> <span class='va'>tape</span><span class='op'>$</span><span class='fu'>gradient</span><span class='op'>(</span><span class='va'>loss</span>, <span class='va'>decoder</span><span class='op'>$</span><span class='va'>variables</span><span class='op'>)</span>
    
    <span class='va'>optimizer</span><span class='op'>$</span><span class='fu'>apply_gradients</span><span class='op'>(</span><span class='fu'>purrr</span><span class='fu'>::</span><span class='fu'><a href='https://purrr.tidyverse.org/reference/transpose.html'>transpose</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span>
      <span class='va'>encoder_gradients</span>, <span class='va'>encoder</span><span class='op'>$</span><span class='va'>variables</span>
    <span class='op'>)</span><span class='op'>)</span>,
    global_step <span class='op'>=</span> <span class='va'>tf</span><span class='op'>$</span><span class='va'>train</span><span class='op'>$</span><span class='fu'>get_or_create_global_step</span><span class='op'>(</span><span class='op'>)</span><span class='op'>)</span>
    
    <span class='va'>optimizer</span><span class='op'>$</span><span class='fu'>apply_gradients</span><span class='op'>(</span><span class='fu'>purrr</span><span class='fu'>::</span><span class='fu'><a href='https://purrr.tidyverse.org/reference/transpose.html'>transpose</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span>
      <span class='va'>decoder_gradients</span>, <span class='va'>decoder</span><span class='op'>$</span><span class='va'>variables</span>
    <span class='op'>)</span><span class='op'>)</span>,
    global_step <span class='op'>=</span> <span class='va'>tf</span><span class='op'>$</span><span class='va'>train</span><span class='op'>$</span><span class='fu'>get_or_create_global_step</span><span class='op'>(</span><span class='op'>)</span><span class='op'>)</span>
    
    <span class='fu'>update_ema</span><span class='op'>(</span><span class='va'>vector_quantizer</span>,
               <span class='va'>one_hot_assignments</span>,
               <span class='va'>codes</span>,
               <span class='va'>decay</span><span class='op'>)</span>

    <span class='co'># periodically display some generated images</span>
    <span class='co'># see code on github </span>
    <span class='co'># visualize_images("kuzushiji", epoch, reconstructed_images, random_images)</span>
  <span class='op'>}</span><span class='op'>)</span>
<span class='op'>}</span>
</code></pre>
</div>
</div>
<p>Now, for the actual action. Inside the context of the gradient tape, we first determine which encoded input sample gets assigned to which embedding vector.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>codes</span> <span class='op'>&lt;-</span> <span class='fu'>encoder</span><span class='op'>(</span><span class='va'>x</span><span class='op'>)</span>
<span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='va'>nearest_codebook_entries</span>, <span class='va'>one_hot_assignments</span><span class='op'>)</span> <span class='op'>%&lt;-%</span> <span class='fu'>vector_quantizer</span><span class='op'>(</span><span class='va'>codes</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>Now, for this assignment operation there is no gradient. Instead what we can do is pass the gradients from decoder input straight through to encoder output. Here <code>tf$stop_gradient</code> exempts <code>nearest_codebook_entries</code> from the chain of gradients, so encoder and decoder are linked by <code>codes</code>:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>codes_straight_through</span> <span class='op'>&lt;-</span> <span class='va'>codes</span> <span class='op'>+</span> <span class='va'>tf</span><span class='op'>$</span><span class='fu'>stop_gradient</span><span class='op'>(</span><span class='va'>nearest_codebook_entries</span> <span class='op'>-</span> <span class='va'>codes</span><span class='op'>)</span>
<span class='va'>decoder_distribution</span> <span class='op'>&lt;-</span> <span class='fu'>decoder</span><span class='op'>(</span><span class='va'>codes_straight_through</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>In sum, backprop will take care of the decoders as well as the encoders weights, whereas the latent embeddings are updated using moving averages, as weve seen already.</p>
<p>Now were ready to tackle the losses. There are three components:</p>
<ul>
<li>First, the reconstruction loss, which is just the log probability of the actual input under the distribution learned by the decoder.</li>
</ul>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>reconstruction_loss</span> <span class='op'>&lt;-</span> <span class='op'>-</span><span class='va'>tf</span><span class='op'>$</span><span class='fu'>reduce_mean</span><span class='op'>(</span><span class='va'>decoder_distribution</span><span class='op'>$</span><span class='fu'>log_prob</span><span class='op'>(</span><span class='va'>x</span><span class='op'>)</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<ul>
<li>Second, we have the <em>commitment loss</em>, defined as the mean squared deviation of the encoded input samples from the nearest neighbors theyve been assigned to: We want the network to commit to a concise set of latent codes!</li>
</ul>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>commitment_loss</span> <span class='op'>&lt;-</span> <span class='va'>tf</span><span class='op'>$</span><span class='fu'>reduce_mean</span><span class='op'>(</span><span class='va'>tf</span><span class='op'>$</span><span class='fu'>square</span><span class='op'>(</span><span class='va'>codes</span> <span class='op'>-</span> <span class='va'>tf</span><span class='op'>$</span><span class='fu'>stop_gradient</span><span class='op'>(</span><span class='va'>nearest_codebook_entries</span><span class='op'>)</span><span class='op'>)</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<ul>
<li>Finally, we have the usual KL diverge to a prior. As, a priori, all assignments are equally probable, this component of the loss is constant and can oftentimes be dispensed of. Were adding it here mainly for illustrative purposes.</li>
</ul>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>prior_dist</span> <span class='op'>&lt;-</span> <span class='va'>tfd</span><span class='op'>$</span><span class='fu'>Multinomial</span><span class='op'>(</span>
  total_count <span class='op'>=</span> <span class='fl'>1</span>,
  logits <span class='op'>=</span> <span class='va'>tf</span><span class='op'>$</span><span class='fu'>zeros</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='va'>latent_size</span>, <span class='va'>num_codes</span><span class='op'>)</span><span class='op'>)</span>
  <span class='op'>)</span>
<span class='va'>prior_loss</span> <span class='op'>&lt;-</span> <span class='op'>-</span><span class='va'>tf</span><span class='op'>$</span><span class='fu'>reduce_mean</span><span class='op'>(</span>
  <span class='va'>tf</span><span class='op'>$</span><span class='fu'>reduce_sum</span><span class='op'>(</span><span class='va'>prior_dist</span><span class='op'>$</span><span class='fu'>log_prob</span><span class='op'>(</span><span class='va'>one_hot_assignments</span><span class='op'>)</span>, <span class='fl'>1L</span><span class='op'>)</span>
  <span class='op'>)</span>
</code></pre>
</div>
</div>
<p>Summing up all three components, we arrive at the overall loss:<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='va'>beta</span> <span class='op'>&lt;-</span> <span class='fl'>0.25</span>
<span class='va'>loss</span> <span class='op'>&lt;-</span> <span class='va'>reconstruction_loss</span> <span class='op'>+</span> <span class='va'>beta</span> <span class='op'>*</span> <span class='va'>commitment_loss</span> <span class='op'>+</span> <span class='va'>prior_loss</span>
</code></pre>
</div>
</div>
<p>Before we look at the results, lets see what happens inside <code>gradientTape</code> at a single glance:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre><code><span class='fu'><a href='https://rdrr.io/r/base/with.html'>with</a></span><span class='op'>(</span><span class='va'>tf</span><span class='op'>$</span><span class='fu'>GradientTape</span><span class='op'>(</span>persistent <span class='op'>=</span> <span class='cn'>TRUE</span><span class='op'>)</span> <span class='op'>%as%</span> <span class='va'>tape</span>, <span class='op'>{</span>
      
  <span class='va'>codes</span> <span class='op'>&lt;-</span> <span class='fu'>encoder</span><span class='op'>(</span><span class='va'>x</span><span class='op'>)</span>
  <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='va'>nearest_codebook_entries</span>, <span class='va'>one_hot_assignments</span><span class='op'>)</span> <span class='op'>%&lt;-%</span> <span class='fu'>vector_quantizer</span><span class='op'>(</span><span class='va'>codes</span><span class='op'>)</span>
  <span class='va'>codes_straight_through</span> <span class='op'>&lt;-</span> <span class='va'>codes</span> <span class='op'>+</span> <span class='va'>tf</span><span class='op'>$</span><span class='fu'>stop_gradient</span><span class='op'>(</span><span class='va'>nearest_codebook_entries</span> <span class='op'>-</span> <span class='va'>codes</span><span class='op'>)</span>
  <span class='va'>decoder_distribution</span> <span class='op'>&lt;-</span> <span class='fu'>decoder</span><span class='op'>(</span><span class='va'>codes_straight_through</span><span class='op'>)</span>
      
  <span class='va'>reconstruction_loss</span> <span class='op'>&lt;-</span> <span class='op'>-</span><span class='va'>tf</span><span class='op'>$</span><span class='fu'>reduce_mean</span><span class='op'>(</span><span class='va'>decoder_distribution</span><span class='op'>$</span><span class='fu'>log_prob</span><span class='op'>(</span><span class='va'>x</span><span class='op'>)</span><span class='op'>)</span>
  <span class='va'>commitment_loss</span> <span class='op'>&lt;-</span> <span class='va'>tf</span><span class='op'>$</span><span class='fu'>reduce_mean</span><span class='op'>(</span><span class='va'>tf</span><span class='op'>$</span><span class='fu'>square</span><span class='op'>(</span><span class='va'>codes</span> <span class='op'>-</span> <span class='va'>tf</span><span class='op'>$</span><span class='fu'>stop_gradient</span><span class='op'>(</span><span class='va'>nearest_codebook_entries</span><span class='op'>)</span><span class='op'>)</span><span class='op'>)</span>
  <span class='va'>prior_dist</span> <span class='op'>&lt;-</span> <span class='va'>tfd</span><span class='op'>$</span><span class='fu'>Multinomial</span><span class='op'>(</span>
    total_count <span class='op'>=</span> <span class='fl'>1</span>,
    logits <span class='op'>=</span> <span class='va'>tf</span><span class='op'>$</span><span class='fu'>zeros</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='va'>latent_size</span>, <span class='va'>num_codes</span><span class='op'>)</span><span class='op'>)</span>
  <span class='op'>)</span>
  <span class='va'>prior_loss</span> <span class='op'>&lt;-</span> <span class='op'>-</span><span class='va'>tf</span><span class='op'>$</span><span class='fu'>reduce_mean</span><span class='op'>(</span><span class='va'>tf</span><span class='op'>$</span><span class='fu'>reduce_sum</span><span class='op'>(</span><span class='va'>prior_dist</span><span class='op'>$</span><span class='fu'>log_prob</span><span class='op'>(</span><span class='va'>one_hot_assignments</span><span class='op'>)</span>, <span class='fl'>1L</span><span class='op'>)</span><span class='op'>)</span>
  
  <span class='va'>loss</span> <span class='op'>&lt;-</span> <span class='va'>reconstruction_loss</span> <span class='op'>+</span> <span class='va'>beta</span> <span class='op'>*</span> <span class='va'>commitment_loss</span> <span class='op'>+</span> <span class='va'>prior_loss</span>
<span class='op'>}</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<h2 id="results">Results</h2>
<p>And here we go. This time, we cant have the 2d morphing view one generally likes to display with VAEs (there just is no 2d latent space). Instead, the two images below are (1) letters generated from random input and (2) reconstructed <em>actual</em> letters, each saved after training for nine epochs.</p>
<figure>
<img src="images/epoch_9.png" alt="Left: letters generated from random input. Right: reconstructed input letters." /><figcaption aria-hidden="true">Left: letters generated from random input. Right: reconstructed input letters.</figcaption>
</figure>
<p>Two things jump to the eye: First, the generated letters are significantly sharper than their continuous-prior counterparts (from the previous post). And second, would you have been able to tell the random image from the reconstruction image?</p>
<h1 id="conclusion">Conclusion</h1>
<p>At this point, weve hopefully convinced you of the power and effectiveness of this discrete-latents approach. However, you might secretly have hoped wed apply this to more complex data, such as the elements of speech we mentioned in the introduction, or higher-resolution images as found in ImageNet.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<p>The truth is that theres a continuous tradeoff between the number of new and exciting techniques we can show, and the time we can spend on iterations to successfully apply these techniques to complex datasets. In the end its you, our readers, who will put these techniques to meaningful use on relevant, real world data.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r distill-force-highlighting-css"><code class="sourceCode r"></code></pre></div>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-clanuwat2018deep">
<p>Clanuwat, Tarin, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. 2018. Deep Learning for Classical Japanese Literature. December 3, 2018. <a href="http://arxiv.org/abs/cs.CV/1812.01718">http://arxiv.org/abs/cs.CV/1812.01718</a>.</p>
</div>
<div id="ref-abs-1711-00937">
<p>Oord, Aaron van den, Oriol Vinyals, and Koray Kavukcuoglu. 2017. Neural Discrete Representation Learning. <em>CoRR</em> abs/1711.00937. <a href="http://arxiv.org/abs/1711.00937">http://arxiv.org/abs/1711.00937</a>.</p>
</div>
</div>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Assuming a 1d latent space, that is. The authors actually used 1d, 2d and 3d spaces in their experiments.<a href="#fnref1" class="footnote-back" role="doc-backlink"></a></p></li>
<li id="fn2" role="doc-endnote"><p>In the paper, the authors actually mention this as one of two ways to learn the prior, the other one being vector quantisation.<a href="#fnref2" class="footnote-back" role="doc-backlink"></a></p></li>
<li id="fn3" role="doc-endnote"><p>To be specific, the authors indicate that they used a field of 32 x 32 latents for ImageNet, and 8 x 8 x 10 for CIFAR10.<a href="#fnref3" class="footnote-back" role="doc-backlink"></a></p></li>
<li id="fn4" role="doc-endnote"><p>The code on github additionally contains functionality to display generated images, output the losses, and save checkpoints.<a href="#fnref4" class="footnote-back" role="doc-backlink"></a></p></li>
<li id="fn5" role="doc-endnote"><p>Here beta is a scaling parameter found surprisingly unimportant by the paper authors.<a href="#fnref5" class="footnote-back" role="doc-backlink"></a></p></li>
<li id="fn6" role="doc-endnote"><p>Although we have to say we find that Kuzushiji-MNIST beats MNIST by far, in complexity and aesthetics!<a href="#fnref6" class="footnote-back" role="doc-backlink"></a></p></li>
</ol>
</section>
<!--radix_placeholder_article_footer-->
<div class="article-footer">
  <p class="social_footer">
    <span class="disqus-comments">
      <i class="fas fa-comments"></i>
      &nbsp;
      <span class="disqus-comment-count" data-disqus-identifier="posts/2019-01-24-vq-vae/">Comment on this article</span>
    </span>
    <span class="article-sharing">
      Share: &nbsp;
      <a href="https://twitter.com/share?text=Discrete%20Representation%20Learning%20with%20VQ-VAE%20and%20TensorFlow%20Probability&amp;url=https%3A%2F%2Fblogs.rstudio.com%2Ftensorflow%2Fposts%2F2019-01-24-vq-vae%2F">
        <i class="fab fa-twitter"></i>
      </a>
      <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3A%2F%2Fblogs.rstudio.com%2Ftensorflow%2Fposts%2F2019-01-24-vq-vae%2F&amp;title=Discrete%20Representation%20Learning%20with%20VQ-VAE%20and%20TensorFlow%20Probability">
        <i class="fab fa-linkedin"></i>
      </a>
    </span>
  </p>
  <script id="dsq-count-scr" src="https://tensorflow-for-r-blog.disqus.com/count.js" async></script>
  <div id="disqus_thread" class="hidden"></div>
  <script>
var disqus_config = function () {
  this.page.url = 'https://blogs.rstudio.com/tensorflow/posts/2019-01-24-vq-vae/';
  this.page.identifier = 'posts/2019-01-24-vq-vae/';
};
(function() {
  var d = document, s = d.createElement('script');
  s.src = 'https://tensorflow-for-r-blog.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
})();
</script>
  <p>
    <div class="subscribe">
<div id="subscribe-caption" style="line-height: 1.2; margin-bottom: 2px;">Enjoy this blog? Get notified of new posts by email:</div>

<script src="https://app-ab02.marketo.com/js/forms2/js/forms2.min.js"></script>
<form class="mtktoBlogEmailForm" id="mktoForm_2224"></form>
<script>

// establish metrics based on where the form is located
var in_sidebar = $('#subscribe-caption').parents('.sidebar-section').length;
if (in_sidebar) {
  var form_width = 190;
  var base_width = form_width - 23;
  var email_width = base_width + 'px';
  var label_width = (base_width - 20) + 'px';
  var button_width = email_width;
  var button_padding = '30px';
  var button_margin = '';
  var font_size = '12px';
} else {
  var form_width = 400;
  var base_width = form_width - 23;
  var email_width = base_width + 'px';
  var label_width = (base_width - 20) + 'px';
  var button_width = email_width;
  var button_padding = '30px';
  var button_margin = '12px';
  var font_size = '15px';
}

$('#subscribe-caption')
  .css('width', base_width)
  .css('font-size', font_size);


MktoForms2.loadForm("https://app-ab02.marketo.com", "709-NXN-706", 2224, function(form) {

  // get jquery reference to form
  form = $(form.getFormElem().get(0));

  $(form).css('width', form_width + 'px');
  $(form).find('.mktoOffset').remove();
  $(form).find('.mktoGutter').remove();
  $(form).find('.mktoEmailField').css('width', email_width);
  $(form).find('.mktoLabel').children().attr('style', 'font-weight: 400');
  $(form).find('.mktoLabel')
      .css('width', label_width)
      .css('font-size', font_size);
  $(form).find('.mktoButtonRow')
      .css('width', button_width)
      .css('text-align', 'center');
  $(form).find('.mktoButtonWrap').attr('style', '');
  $(form).find('.mktoButton')
      .css('margin-top', '10px')
      .css('padding-left', button_padding)
      .css('padding-right', button_padding)
      .css('font-size', font_size)
      .css('margin-top', button_margin);
});
</script>
Posts also available at <a href="https://www.r-bloggers.com">r-bloggers</a>

<script>
  document.addEventListener("DOMContentLoaded", function(){
    var menulink = document.querySelector("a[href='#category:R']");
    if (menulink) menulink.parentNode.style.display = "None";
    
    for (var e of document.querySelectorAll(".dt-tag")) {
      if (e.innerHTML == 'R') e.style.display = "None";
    }
  });
</script>
</div>
  </p>
</div>
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>


<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
  <h3 id="references">References</h3>
  <div id="references-listing"></div>
  <h3 id="reuse">Reuse</h3>
  <p>Text and figures are licensed under Creative Commons Attribution <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>. The figures that have been reused from other sources don't fall under this license and can be recognized by a note in their caption: "Figure from ...".</p>
  <h3 id="citation">Citation</h3>
  <p>For attribution, please cite this work as</p>
  <pre class="citation-appendix short">Keydana (2019, Jan. 24). RStudio AI Blog: Discrete Representation Learning with VQ-VAE and TensorFlow Probability. Retrieved from https://blogs.rstudio.com/tensorflow/posts/2019-01-24-vq-vae/</pre>
  <p>BibTeX citation</p>
  <pre class="citation-appendix long">@misc{keydana2019vqvae,
  author = {Keydana, Sigrid},
  title = {RStudio AI Blog: Discrete Representation Learning with VQ-VAE and TensorFlow Probability},
  url = {https://blogs.rstudio.com/tensorflow/posts/2019-01-24-vq-vae/},
  year = {2019}
}</pre>
</div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
