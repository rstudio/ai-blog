<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ad0000; } /* Alert */
code span.an { color: #5e5e5e; } /* Annotation */
code span.at { color: #20794d; } /* Attribute */
code span.bn { color: #ad0000; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007ba5; } /* ControlFlow */
code span.ch { color: #20794d; } /* Char */
code span.cn { color: #8f5902; } /* Constant */
code span.co { color: #5e5e5e; } /* Comment */
code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
code span.dt { color: #ad0000; } /* DataType */
code span.dv { color: #ad0000; } /* DecVal */
code span.er { color: #ad0000; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #ad0000; } /* Float */
code span.fu { color: #4758ab; } /* Function */
code span.im { } /* Import */
code span.in { color: #5e5e5e; } /* Information */
code span.kw { color: #007ba5; } /* Keyword */
code span.op { color: #5e5e5e; } /* Operator */
code span.ot { color: #007ba5; } /* Other */
code span.pp { color: #ad0000; } /* Preprocessor */
code span.sc { color: #20794d; } /* SpecialChar */
code span.ss { color: #20794d; } /* SpecialString */
code span.st { color: #20794d; } /* String */
code span.va { color: #111111; } /* Variable */
code span.vs { color: #20794d; } /* VerbatimString */
code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
</style>

  <!--radix_placeholder_meta_tags-->
<title>RStudio AI Blog: Hacking deep learning: model inversion attack by example</title>

<meta property="description" itemprop="description" content="Compared to other applications, deep learning models might not seem too likely as victims of privacy attacks. However, methods exist to determine whether an entity was used in the training set (an adversarial attack called member inference), and techniques subsumed under &quot;model inversion&quot; allow to reconstruct raw data input given just model output (and sometimes, context information). This post shows an end-to-end example of model inversion, and explores mitigation strategies using TensorFlow Privacy."/>

<link rel="canonical" href="https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks/"/>
<link rel="license" href="https://creativecommons.org/licenses/by/4.0/"/>
<link rel="icon" type="image/png" href="../../images/favicon.png"/>

<!--  https://schema.org/Article -->
<meta property="article:published" itemprop="datePublished" content="2020-05-15"/>
<meta property="article:created" itemprop="dateCreated" content="2020-05-15"/>
<meta name="article:author" content="Sigrid Keydana"/>

<!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
<meta property="og:title" content="RStudio AI Blog: Hacking deep learning: model inversion attack by example"/>
<meta property="og:type" content="article"/>
<meta property="og:description" content="Compared to other applications, deep learning models might not seem too likely as victims of privacy attacks. However, methods exist to determine whether an entity was used in the training set (an adversarial attack called member inference), and techniques subsumed under &quot;model inversion&quot; allow to reconstruct raw data input given just model output (and sometimes, context information). This post shows an end-to-end example of model inversion, and explores mitigation strategies using TensorFlow Privacy."/>
<meta property="og:url" content="https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks/"/>
<meta property="og:image" content="https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks/images/results.png"/>
<meta property="og:image:width" content="600"/>
<meta property="og:image:height" content="394"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:site_name" content="RStudio AI Blog"/>

<!--  https://dev.twitter.com/cards/types/summary -->
<meta property="twitter:card" content="summary_large_image"/>
<meta property="twitter:title" content="RStudio AI Blog: Hacking deep learning: model inversion attack by example"/>
<meta property="twitter:description" content="Compared to other applications, deep learning models might not seem too likely as victims of privacy attacks. However, methods exist to determine whether an entity was used in the training set (an adversarial attack called member inference), and techniques subsumed under &quot;model inversion&quot; allow to reconstruct raw data input given just model output (and sometimes, context information). This post shows an end-to-end example of model inversion, and explores mitigation strategies using TensorFlow Privacy."/>
<meta property="twitter:url" content="https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks/"/>
<meta property="twitter:image" content="https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks/images/results.png"/>
<meta property="twitter:image:width" content="600"/>
<meta property="twitter:image:height" content="394"/>

<!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
<meta name="citation_title" content="RStudio AI Blog: Hacking deep learning: model inversion attack by example"/>
<meta name="citation_fulltext_html_url" content="https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks/"/>
<meta name="citation_fulltext_world_readable" content=""/>
<meta name="citation_online_date" content="2020/05/15"/>
<meta name="citation_publication_date" content="2020/05/15"/>
<meta name="citation_author" content="Sigrid Keydana"/>
<meta name="citation_author_institution" content="RStudio"/>
<!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=Human-level concept learning through probabilistic program induction;citation_publication_date=2015;citation_publisher=American Association for the Advancement of Science;citation_volume=350;citation_doi=10.1126/science.aab3050;citation_issn=0036-8075;citation_author=Brenden M. Lake;citation_author=Ruslan Salakhutdinov;citation_author=Joshua B. Tenenbaum"/>
  <meta name="citation_reference" content="citation_title=Federated Learning of Deep Networks using Model Averaging;citation_publication_date=2016;citation_volume=abs/1602.05629;citation_author=H. Brendan McMahan;citation_author=Eider Moore;citation_author=Daniel Ramage;citation_author=Blaise AgÃ¼era Arcas"/>
  <meta name="citation_reference" content="citation_title=Calibrating Noise to Sensitivity in Private Data Analysis;citation_publication_date=2006;citation_publisher=Springer-Verlag;citation_doi=10.1007/11681878_14;citation_author=Cynthia Dwork;citation_author=Frank McSherry;citation_author=Kobbi Nissim;citation_author=Adam Smith"/>
  <meta name="citation_reference" content="citation_title=A Methodology for Formalizing Model-Inversion Attacks;citation_publication_date=2016;citation_author=X. Wu;citation_author=M. Fredrikson;citation_author=S. Jha;citation_author=J. F. Naughton"/>
  <meta name="citation_reference" content="citation_title=Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing;citation_publication_date=2014;citation_publisher=USENIX Association;citation_author=Matthew Fredrikson;citation_author=Eric Lantz;citation_author=Somesh Jha;citation_author=Simon Lin;citation_author=David Page;citation_author=Thomas Ristenpart"/>
  <!--radix_placeholder_rmarkdown_metadata-->

<script type="text/json" id="radix-rmarkdown-metadata">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["creative_commons","title","description","author","slug","bibliography","date","categories","output","preview","citation_url","canonical_url"]}},"value":[{"type":"character","attributes":{},"value":["CC BY"]},{"type":"character","attributes":{},"value":["Hacking deep learning: model inversion attack by example"]},{"type":"character","attributes":{},"value":["Compared to other applications, deep learning models might not seem too likely as victims of privacy attacks. However, methods exist to determine whether an entity was used in the training set (an adversarial attack called member inference), and techniques subsumed under \"model inversion\" allow to reconstruct raw data input given just model output (and sometimes, context information). This post shows an end-to-end example of model inversion, and explores mitigation strategies using TensorFlow Privacy."]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","affiliation_url"]}},"value":[{"type":"character","attributes":{},"value":["Sigrid Keydana"]},{"type":"character","attributes":{},"value":["RStudio"]},{"type":"character","attributes":{},"value":["https://www.rstudio.com/"]}]}]},{"type":"character","attributes":{},"value":["keydanamodelinversion"]},{"type":"character","attributes":{},"value":["bibliography.bib"]},{"type":"character","attributes":{},"value":["05-15-2020"]},{"type":"character","attributes":{},"value":["R","Privacy & Security","TensorFlow/Keras"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"character","attributes":{},"value":["images/results.png"]},{"type":"character","attributes":{},"value":["https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks/"]},{"type":"character","attributes":{},"value":["https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks/"]}]}
</script>
<!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["bibliography.bib","images/actual_test_images.png","images/attacker_ds.png","images/intercepted_eps_12.5.png","images/intercepted_eps_4.0.png","images/intercepted_eps_84.7.png","images/recon_noeps_dropout.png","images/results.png","images/training_set_images.png","model_inversion_differential_privacy_files/bowser-1.9.3/bowser.min.js","model_inversion_differential_privacy_files/distill-2.2.21/template.v2.js","model_inversion_differential_privacy_files/header-attrs-2.1/header-attrs.js","model_inversion_differential_privacy_files/jquery-1.11.3/jquery.min.js","model_inversion_differential_privacy_files/webcomponents-2.0.0/webcomponents.js"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
<meta name="distill:offset" content="../.."/>

<script type="application/javascript">

  window.headroom_prevent_pin = false;

  window.document.addEventListener("DOMContentLoaded", function (event) {

    // initialize headroom for banner
    var header = $('header').get(0);
    var headerHeight = header.offsetHeight;
    var headroom = new Headroom(header, {
      tolerance: 5,
      onPin : function() {
        if (window.headroom_prevent_pin) {
          window.headroom_prevent_pin = false;
          headroom.unpin();
        }
      }
    });
    headroom.init();
    if(window.location.hash)
      headroom.unpin();
    $(header).addClass('headroom--transition');

    // offset scroll location for banner on hash change
    // (see: https://github.com/WickyNilliams/headroom.js/issues/38)
    window.addEventListener("hashchange", function(event) {
      window.scrollTo(0, window.pageYOffset - (headerHeight + 25));
    });

    // responsive menu
    $('.distill-site-header').each(function(i, val) {
      var topnav = $(this);
      var toggle = topnav.find('.nav-toggle');
      toggle.on('click', function() {
        topnav.toggleClass('responsive');
      });
    });

    // nav dropdowns
    $('.nav-dropbtn').click(function(e) {
      $(this).next('.nav-dropdown-content').toggleClass('nav-dropdown-active');
      $(this).parent().siblings('.nav-dropdown')
         .children('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $("body").click(function(e){
      $('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $(".nav-dropdown").click(function(e){
      e.stopPropagation();
    });
  });
</script>

<style type="text/css">

/* Theme (user-documented overrideables for nav appearance) */

.distill-site-nav {
  color: rgba(255, 255, 255, 0.8);
  background-color: #0F2E3D;
  font-size: 15px;
  font-weight: 300;
}

.distill-site-nav a {
  color: inherit;
  text-decoration: none;
}

.distill-site-nav a:hover {
  color: white;
}

@media print {
  .distill-site-nav {
    display: none;
  }
}

.distill-site-header {

}

.distill-site-footer {

}


/* Site Header */

.distill-site-header {
  width: 100%;
  box-sizing: border-box;
  z-index: 3;
}

.distill-site-header .nav-left {
  display: inline-block;
  margin-left: 8px;
}

@media screen and (max-width: 768px) {
  .distill-site-header .nav-left {
    margin-left: 0;
  }
}


.distill-site-header .nav-right {
  float: right;
  margin-right: 8px;
}

.distill-site-header a,
.distill-site-header .title {
  display: inline-block;
  text-align: center;
  padding: 14px 10px 14px 10px;
}

.distill-site-header .title {
  font-size: 18px;
  min-width: 150px;
}

.distill-site-header .logo {
  padding: 0;
}

.distill-site-header .logo img {
  display: none;
  max-height: 20px;
  width: auto;
  margin-bottom: -4px;
}

.distill-site-header .nav-image img {
  max-height: 18px;
  width: auto;
  display: inline-block;
  margin-bottom: -3px;
}



@media screen and (min-width: 1000px) {
  .distill-site-header .logo img {
    display: inline-block;
  }
  .distill-site-header .nav-left {
    margin-left: 20px;
  }
  .distill-site-header .nav-right {
    margin-right: 20px;
  }
  .distill-site-header .title {
    padding-left: 12px;
  }
}


.distill-site-header .nav-toggle {
  display: none;
}

.nav-dropdown {
  display: inline-block;
  position: relative;
}

.nav-dropdown .nav-dropbtn {
  border: none;
  outline: none;
  color: rgba(255, 255, 255, 0.8);
  padding: 16px 10px;
  background-color: transparent;
  font-family: inherit;
  font-size: inherit;
  font-weight: inherit;
  margin: 0;
  margin-top: 1px;
  z-index: 2;
}

.nav-dropdown-content {
  display: none;
  position: absolute;
  background-color: white;
  min-width: 200px;
  border: 1px solid rgba(0,0,0,0.15);
  border-radius: 4px;
  box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.1);
  z-index: 1;
  margin-top: 2px;
  white-space: nowrap;
  padding-top: 4px;
  padding-bottom: 4px;
}

.nav-dropdown-content hr {
  margin-top: 4px;
  margin-bottom: 4px;
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.nav-dropdown-active {
  display: block;
}

.nav-dropdown-content a, .nav-dropdown-content .nav-dropdown-header {
  color: black;
  padding: 6px 24px;
  text-decoration: none;
  display: block;
  text-align: left;
}

.nav-dropdown-content .nav-dropdown-header {
  display: block;
  padding: 5px 24px;
  padding-bottom: 0;
  text-transform: uppercase;
  font-size: 14px;
  color: #999999;
  white-space: nowrap;
}

.nav-dropdown:hover .nav-dropbtn {
  color: white;
}

.nav-dropdown-content a:hover {
  background-color: #ddd;
  color: black;
}

.nav-right .nav-dropdown-content {
  margin-left: -45%;
  right: 0;
}

@media screen and (max-width: 768px) {
  .distill-site-header a, .distill-site-header .nav-dropdown  {display: none;}
  .distill-site-header a.nav-toggle {
    float: right;
    display: block;
  }
  .distill-site-header .title {
    margin-left: 0;
  }
  .distill-site-header .nav-right {
    margin-right: 0;
  }
  .distill-site-header {
    overflow: hidden;
  }
  .nav-right .nav-dropdown-content {
    margin-left: 0;
  }
}


@media screen and (max-width: 768px) {
  .distill-site-header.responsive {position: relative; min-height: 500px; }
  .distill-site-header.responsive a.nav-toggle {
    position: absolute;
    right: 0;
    top: 0;
  }
  .distill-site-header.responsive a,
  .distill-site-header.responsive .nav-dropdown {
    display: block;
    text-align: left;
  }
  .distill-site-header.responsive .nav-left,
  .distill-site-header.responsive .nav-right {
    width: 100%;
  }
  .distill-site-header.responsive .nav-dropdown {float: none;}
  .distill-site-header.responsive .nav-dropdown-content {position: relative;}
  .distill-site-header.responsive .nav-dropdown .nav-dropbtn {
    display: block;
    width: 100%;
    text-align: left;
  }
}

/* Site Footer */

.distill-site-footer {
  width: 100%;
  overflow: hidden;
  box-sizing: border-box;
  z-index: 3;
  margin-top: 30px;
  padding-top: 30px;
  padding-bottom: 30px;
  text-align: center;
}

/* Headroom */

d-title {
  padding-top: 6rem;
}

@media print {
  d-title {
    padding-top: 4rem;
  }
}

.headroom {
  z-index: 1000;
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
}

.headroom--transition {
  transition: all .4s ease-in-out;
}

.headroom--unpinned {
  top: -100px;
}

.headroom--pinned {
  top: 0;
}

/* adjust viewport for navbar height */
/* helps vertically center bootstrap (non-distill) content */
.min-vh-100 {
  min-height: calc(100vh - 100px) !important;
}

</style>

<script src="../../site_libs/jquery-1.11.3/jquery.min.js"></script>
<link href="../../site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet"/>
<link href="../../site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet"/>
<script src="../../site_libs/headroom-0.9.4/headroom.min.js"></script>
<script src="../../site_libs/autocomplete-0.37.1/autocomplete.min.js"></script>
<script src="../../site_libs/fuse-6.4.1/fuse.min.js"></script>

<script type="application/javascript">

function getMeta(metaName) {
  var metas = document.getElementsByTagName('meta');
  for (let i = 0; i < metas.length; i++) {
    if (metas[i].getAttribute('name') === metaName) {
      return metas[i].getAttribute('content');
    }
  }
  return '';
}

function offsetURL(url) {
  var offset = getMeta('distill:offset');
  return offset ? offset + '/' + url : url;
}

function createFuseIndex() {

  // create fuse index
  var options = {
    keys: [
      { name: 'title', weight: 20 },
      { name: 'categories', weight: 15 },
      { name: 'description', weight: 10 },
      { name: 'contents', weight: 5 },
    ],
    ignoreLocation: true,
    threshold: 0
  };
  var fuse = new window.Fuse([], options);

  // fetch the main search.json
  return fetch(offsetURL('search.json'))
    .then(function(response) {
      if (response.status == 200) {
        return response.json().then(function(json) {
          // index main articles
          json.articles.forEach(function(article) {
            fuse.add(article);
          });
          // download collections and index their articles
          return Promise.all(json.collections.map(function(collection) {
            return fetch(offsetURL(collection)).then(function(response) {
              if (response.status === 200) {
                return response.json().then(function(articles) {
                  articles.forEach(function(article) {
                    fuse.add(article);
                  });
                })
              } else {
                return Promise.reject(
                  new Error('Unexpected status from search index request: ' +
                            response.status)
                );
              }
            });
          })).then(function() {
            return fuse;
          });
        });

      } else {
        return Promise.reject(
          new Error('Unexpected status from search index request: ' +
                      response.status)
        );
      }
    });
}

window.document.addEventListener("DOMContentLoaded", function (event) {

  // get search element (bail if we don't have one)
  var searchEl = window.document.getElementById('distill-search');
  if (!searchEl)
    return;

  createFuseIndex()
    .then(function(fuse) {

      // make search box visible
      searchEl.classList.remove('hidden');

      // initialize autocomplete
      var options = {
        autoselect: true,
        hint: false,
        minLength: 2,
      };
      window.autocomplete(searchEl, options, [{
        source: function(query, callback) {
          const searchOptions = {
            isCaseSensitive: false,
            shouldSort: true,
            minMatchCharLength: 2,
            limit: 10,
          };
          var results = fuse.search(query, searchOptions);
          callback(results
            .map(function(result) { return result.item; })
          );
        },
        templates: {
          suggestion: function(suggestion) {
            var img = suggestion.preview && Object.keys(suggestion.preview).length > 0
              ? `<img src="${offsetURL(suggestion.preview)}"</img>`
              : '';
            var html = `
              <div class="search-item">
                <h3>${suggestion.title}</h3>
                <div class="search-item-description">
                  ${suggestion.description || ''}
                </div>
                <div class="search-item-preview">
                  ${img}
                </div>
              </div>
            `;
            return html;
          }
        }
      }]).on('autocomplete:selected', function(event, suggestion) {
        window.location.href = offsetURL(suggestion.path);
      });
      // remove inline display style on autocompleter (we want to
      // manage responsive display via css)
      $('.algolia-autocomplete').css("display", "");
    })
    .catch(function(error) {
      console.log(error);
    });

});

</script>

<style type="text/css">

.nav-search {
  font-size: x-small;
}

/* Algolioa Autocomplete */

.algolia-autocomplete {
  display: inline-block;
  margin-left: 10px;
  vertical-align: sub;
  background-color: white;
  color: black;
  padding: 6px;
  padding-top: 8px;
  padding-bottom: 0;
  border-radius: 6px;
  border: 1px #0F2E3D solid;
  width: 180px;
}


@media screen and (max-width: 768px) {
  .distill-site-nav .algolia-autocomplete {
    display: none;
    visibility: hidden;
  }
  .distill-site-nav.responsive .algolia-autocomplete {
    display: inline-block;
    visibility: visible;
  }
  .distill-site-nav.responsive .algolia-autocomplete .aa-dropdown-menu {
    margin-left: 0;
    width: 400px;
    max-height: 400px;
  }
}

.algolia-autocomplete .aa-input, .algolia-autocomplete .aa-hint {
  width: 90%;
  outline: none;
  border: none;
}

.algolia-autocomplete .aa-hint {
  color: #999;
}
.algolia-autocomplete .aa-dropdown-menu {
  width: 550px;
  max-height: 70vh;
  overflow-x: visible;
  overflow-y: scroll;
  padding: 5px;
  margin-top: 3px;
  margin-left: -150px;
  background-color: #fff;
  border-radius: 5px;
  border: 1px solid #999;
  border-top: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion {
  cursor: pointer;
  padding: 5px 4px;
  border-bottom: 1px solid #eee;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion:last-of-type {
  border-bottom: none;
  margin-bottom: 2px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item {
  overflow: hidden;
  font-size: 0.8em;
  line-height: 1.4em;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item h3 {
  font-size: 1rem;
  margin-block-start: 0;
  margin-block-end: 5px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-description {
  display: inline-block;
  overflow: hidden;
  height: 2.8em;
  width: 80%;
  margin-right: 4%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview {
  display: inline-block;
  width: 15%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img {
  height: 3em;
  width: auto;
  display: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img[src] {
  display: initial;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion.aa-cursor {
  background-color: #eee;
}
.algolia-autocomplete .aa-dropdown-menu .aa-suggestion em {
  font-weight: bold;
  font-style: normal;
}

</style>


<!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

<style type="text/css">

body {
  background-color: white;
}

.pandoc-table {
  width: 100%;
}

.pandoc-table>caption {
  margin-bottom: 10px;
}

.pandoc-table th:not([align]) {
  text-align: left;
}

.pagedtable-footer {
  font-size: 15px;
}

d-byline .byline {
  grid-template-columns: 2fr 2fr;
}

d-byline .byline h3 {
  margin-block-start: 1.5em;
}

d-byline .byline .authors-affiliations h3 {
  margin-block-start: 0.5em;
}

.authors-affiliations .orcid-id {
  width: 16px;
  height:16px;
  margin-left: 4px;
  margin-right: 4px;
  vertical-align: middle;
  padding-bottom: 2px;
}

d-title .dt-tags {
  margin-top: 1em;
  grid-column: text;
}

.dt-tags .dt-tag {
  text-decoration: none;
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0em 0.4em;
  margin-right: 0.5em;
  margin-bottom: 0.4em;
  font-size: 70%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

d-article table.gt_table td,
d-article table.gt_table th {
  border-bottom: none;
}

.html-widget {
  margin-bottom: 2.0em;
}

.l-screen-inset {
  padding-right: 16px;
}

.l-screen .caption {
  margin-left: 10px;
}

.shaded {
  background: rgb(247, 247, 247);
  padding-top: 20px;
  padding-bottom: 20px;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .html-widget {
  margin-bottom: 0;
  border: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .shaded-content {
  background: white;
}

.text-output {
  margin-top: 0;
  line-height: 1.5em;
}

.hidden {
  display: none !important;
}

d-article {
  padding-top: 2.5rem;
  padding-bottom: 30px;
}

d-appendix {
  padding-top: 30px;
}

d-article>p>img {
  width: 100%;
}

d-article h2 {
  margin: 1rem 0 1.5rem 0;
}

d-article h3 {
  margin-top: 1.5rem;
}

d-article iframe {
  border: 1px solid rgba(0, 0, 0, 0.1);
  margin-bottom: 2.0em;
  width: 100%;
}

/* Tweak code blocks */

d-article div.sourceCode code,
d-article pre code {
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
}

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: auto;
}

d-article div.sourceCode {
  background-color: white;
}

d-article div.sourceCode pre {
  padding-left: 10px;
  font-size: 12px;
  border-left: 2px solid rgba(0,0,0,0.1);
}

d-article pre {
  font-size: 12px;
  color: black;
  background: none;
  margin-top: 0;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

d-article pre a {
  border-bottom: none;
}

d-article pre a:hover {
  border-bottom: none;
  text-decoration: underline;
}

d-article details {
  grid-column: text;
  margin-bottom: 0.8em;
}

@media(min-width: 768px) {

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: visible !important;
}

d-article div.sourceCode pre {
  padding-left: 18px;
  font-size: 14px;
}

d-article pre {
  font-size: 14px;
}

}

figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

/* CSS for d-contents */

.d-contents {
  grid-column: text;
  color: rgba(0,0,0,0.8);
  font-size: 0.9em;
  padding-bottom: 1em;
  margin-bottom: 1em;
  padding-bottom: 0.5em;
  margin-bottom: 1em;
  padding-left: 0.25em;
  justify-self: start;
}

@media(min-width: 1000px) {
  .d-contents.d-contents-float {
    height: 0;
    grid-column-start: 1;
    grid-column-end: 4;
    justify-self: center;
    padding-right: 3em;
    padding-left: 2em;
  }
}

.d-contents nav h3 {
  font-size: 18px;
  margin-top: 0;
  margin-bottom: 1em;
}

.d-contents li {
  list-style-type: none
}

.d-contents nav > ul {
  padding-left: 0;
}

.d-contents ul {
  padding-left: 1em
}

.d-contents nav ul li {
  margin-top: 0.6em;
  margin-bottom: 0.2em;
}

.d-contents nav a {
  font-size: 13px;
  border-bottom: none;
  text-decoration: none
  color: rgba(0, 0, 0, 0.8);
}

.d-contents nav a:hover {
  text-decoration: underline solid rgba(0, 0, 0, 0.6)
}

.d-contents nav > ul > li > a {
  font-weight: 600;
}

.d-contents nav > ul > li > ul {
  font-weight: inherit;
}

.d-contents nav > ul > li > ul > li {
  margin-top: 0.2em;
}


.d-contents nav ul {
  margin-top: 0;
  margin-bottom: 0.25em;
}

.d-article-with-toc h2:nth-child(2) {
  margin-top: 0;
}


/* Figure */

.figure {
  position: relative;
  margin-bottom: 2.5em;
  margin-top: 1.5em;
}

.figure img {
  width: 100%;
}

.figure .caption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

.figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

.figure .caption a {
  color: rgba(0, 0, 0, 0.6);
}

.figure .caption b,
.figure .caption strong, {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

/* Citations */

d-article .citation {
  color: inherit;
  cursor: inherit;
}

div.hanging-indent{
  margin-left: 1em; text-indent: -1em;
}

/* Citation hover box */

.tippy-box[data-theme~=light-border] {
  background-color: rgba(250, 250, 250, 0.95);
}

.tippy-content > p {
  margin-bottom: 0;
  padding: 2px;
}


/* Tweak 1000px media break to show more text */

@media(min-width: 1000px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }

  d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
  }
  figure .caption, .figure .caption, figure figcaption {
    font-size: 13px;
  }
}

@media(min-width: 1180px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 32px;
  }

  .grid {
    grid-column-gap: 32px;
  }
}


/* Get the citation styles for the appendix (not auto-injected on render since
   we do our own rendering of the citation appendix) */

d-appendix .citation-appendix,
.d-appendix .citation-appendix {
  font-size: 11px;
  line-height: 15px;
  border-left: 1px solid rgba(0, 0, 0, 0.1);
  padding-left: 18px;
  border: 1px solid rgba(0,0,0,0.1);
  background: rgba(0, 0, 0, 0.02);
  padding: 10px 18px;
  border-radius: 3px;
  color: rgba(150, 150, 150, 1);
  overflow: hidden;
  margin-top: -12px;
  white-space: pre-wrap;
  word-wrap: break-word;
}

/* Include appendix styles here so they can be overridden */

d-appendix {
  contain: layout style;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-top: 60px;
  margin-bottom: 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  color: rgba(0,0,0,0.5);
  padding-top: 60px;
  padding-bottom: 48px;
}

d-appendix h3 {
  grid-column: page-start / text-start;
  font-size: 15px;
  font-weight: 500;
  margin-top: 1em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.65);
}

d-appendix h3 + * {
  margin-top: 1em;
}

d-appendix ol {
  padding: 0 0 0 15px;
}

@media (min-width: 768px) {
  d-appendix ol {
    padding: 0 0 0 30px;
    margin-left: -30px;
  }
}

d-appendix li {
  margin-bottom: 1em;
}

d-appendix a {
  color: rgba(0, 0, 0, 0.6);
}

d-appendix > * {
  grid-column: text;
}

d-appendix > d-footnote-list,
d-appendix > d-citation-list,
d-appendix > distill-appendix {
  grid-column: screen;
}

/* Include footnote styles here so they can be overridden */

d-footnote-list {
  contain: layout style;
}

d-footnote-list > * {
  grid-column: text;
}

d-footnote-list a.footnote-backlink {
  color: rgba(0,0,0,0.3);
  padding-left: 0.5em;
}



/* Anchor.js */

.anchorjs-link {
  /*transition: all .25s linear; */
  text-decoration: none;
  border-bottom: none;
}
*:hover > .anchorjs-link {
  margin-left: -1.125em !important;
  text-decoration: none;
  border-bottom: none;
}

/* Social footer */

.social_footer {
  margin-top: 30px;
  margin-bottom: 0;
  color: rgba(0,0,0,0.67);
}

.disqus-comments {
  margin-right: 30px;
}

.disqus-comment-count {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  cursor: pointer;
}

#disqus_thread {
  margin-top: 30px;
}

.article-sharing a {
  border-bottom: none;
  margin-right: 8px;
}

.article-sharing a:hover {
  border-bottom: none;
}

.sidebar-section.subscribe {
  font-size: 12px;
  line-height: 1.6em;
}

.subscribe p {
  margin-bottom: 0.5em;
}


.article-footer .subscribe {
  font-size: 15px;
  margin-top: 45px;
}


.sidebar-section.custom {
  font-size: 12px;
  line-height: 1.6em;
}

.custom p {
  margin-bottom: 0.5em;
}

/* Styles for listing layout (hide title) */
.layout-listing d-title, .layout-listing .d-title {
  display: none;
}

/* Styles for posts lists (not auto-injected) */


.posts-with-sidebar {
  padding-left: 45px;
  padding-right: 45px;
}

.posts-list .description h2,
.posts-list .description p {
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
}

.posts-list .description h2 {
  font-weight: 700;
  border-bottom: none;
  padding-bottom: 0;
}

.posts-list h2.post-tag {
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
  padding-bottom: 12px;
}
.posts-list {
  margin-top: 60px;
  margin-bottom: 24px;
}

.posts-list .post-preview {
  text-decoration: none;
  overflow: hidden;
  display: block;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  padding: 24px 0;
}

.post-preview-last {
  border-bottom: none !important;
}

.posts-list .posts-list-caption {
  grid-column: screen;
  font-weight: 400;
}

.posts-list .post-preview h2 {
  margin: 0 0 6px 0;
  line-height: 1.2em;
  font-style: normal;
  font-size: 24px;
}

.posts-list .post-preview p {
  margin: 0 0 12px 0;
  line-height: 1.4em;
  font-size: 16px;
}

.posts-list .post-preview .thumbnail {
  box-sizing: border-box;
  margin-bottom: 24px;
  position: relative;
  max-width: 500px;
}
.posts-list .post-preview img {
  width: 100%;
  display: block;
}

.posts-list .metadata {
  font-size: 12px;
  line-height: 1.4em;
  margin-bottom: 18px;
}

.posts-list .metadata > * {
  display: inline-block;
}

.posts-list .metadata .publishedDate {
  margin-right: 2em;
}

.posts-list .metadata .dt-authors {
  display: block;
  margin-top: 0.3em;
  margin-right: 2em;
}

.posts-list .dt-tags {
  display: block;
  line-height: 1em;
}

.posts-list .dt-tags .dt-tag {
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0.3em 0.4em;
  margin-right: 0.2em;
  margin-bottom: 0.4em;
  font-size: 60%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

.posts-list img {
  opacity: 1;
}

.posts-list img[data-src] {
  opacity: 0;
}

.posts-more {
  clear: both;
}


.posts-sidebar {
  font-size: 16px;
}

.posts-sidebar h3 {
  font-size: 16px;
  margin-top: 0;
  margin-bottom: 0.5em;
  font-weight: 400;
  text-transform: uppercase;
}

.sidebar-section {
  margin-bottom: 30px;
}

.categories ul {
  list-style-type: none;
  margin: 0;
  padding: 0;
}

.categories li {
  color: rgba(0, 0, 0, 0.8);
  margin-bottom: 0;
}

.categories li>a {
  border-bottom: none;
}

.categories li>a:hover {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
}

.categories .active {
  font-weight: 600;
}

.categories .category-count {
  color: rgba(0, 0, 0, 0.4);
}


@media(min-width: 768px) {
  .posts-list .post-preview h2 {
    font-size: 26px;
  }
  .posts-list .post-preview .thumbnail {
    float: right;
    width: 30%;
    margin-bottom: 0;
  }
  .posts-list .post-preview .description {
    float: left;
    width: 45%;
  }
  .posts-list .post-preview .metadata {
    float: left;
    width: 20%;
    margin-top: 8px;
  }
  .posts-list .post-preview p {
    margin: 0 0 12px 0;
    line-height: 1.5em;
    font-size: 16px;
  }
  .posts-with-sidebar .posts-list {
    float: left;
    width: 75%;
  }
  .posts-with-sidebar .posts-sidebar {
    float: right;
    width: 20%;
    margin-top: 60px;
    padding-top: 24px;
    padding-bottom: 24px;
  }
}


/* Improve display for browsers without grid (IE/Edge <= 15) */

.downlevel {
  line-height: 1.6em;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  margin: 0;
}

.downlevel .d-title {
  padding-top: 6rem;
  padding-bottom: 1.5rem;
}

.downlevel .d-title h1 {
  font-size: 50px;
  font-weight: 700;
  line-height: 1.1em;
  margin: 0 0 0.5rem;
}

.downlevel .d-title p {
  font-weight: 300;
  font-size: 1.2rem;
  line-height: 1.55em;
  margin-top: 0;
}

.downlevel .d-byline {
  padding-top: 0.8em;
  padding-bottom: 0.8em;
  font-size: 0.8rem;
  line-height: 1.8em;
}

.downlevel .section-separator {
  border: none;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
}

.downlevel .d-article {
  font-size: 1.06rem;
  line-height: 1.7em;
  padding-top: 1rem;
  padding-bottom: 2rem;
}


.downlevel .d-appendix {
  padding-left: 0;
  padding-right: 0;
  max-width: none;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.5);
  padding-top: 40px;
  padding-bottom: 48px;
}

.downlevel .footnotes ol {
  padding-left: 13px;
}

.downlevel .base-grid,
.downlevel .distill-header,
.downlevel .d-title,
.downlevel .d-abstract,
.downlevel .d-article,
.downlevel .d-appendix,
.downlevel .distill-appendix,
.downlevel .d-byline,
.downlevel .d-footnote-list,
.downlevel .d-citation-list,
.downlevel .distill-footer,
.downlevel .appendix-bottom,
.downlevel .posts-container {
  padding-left: 40px;
  padding-right: 40px;
}

@media(min-width: 768px) {
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
  padding-left: 150px;
  padding-right: 150px;
  max-width: 900px;
}
}

.downlevel pre code {
  display: block;
  border-left: 2px solid rgba(0, 0, 0, .1);
  padding: 0 0 0 20px;
  font-size: 14px;
}

.downlevel code, .downlevel pre {
  color: black;
  background: none;
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

.downlevel .posts-list .post-preview {
  color: inherit;
}



</style>

<script type="application/javascript">

function is_downlevel_browser() {
  if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                 window.navigator.userAgent)) {
    return true;
  } else {
    return window.load_distill_framework === undefined;
  }
}

// show body when load is complete
function on_load_complete() {

  // add anchors
  if (window.anchors) {
    window.anchors.options.placement = 'left';
    window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
  }


  // set body to visible
  document.body.style.visibility = 'visible';

  // force redraw for leaflet widgets
  if (window.HTMLWidgets) {
    var maps = window.HTMLWidgets.findAll(".leaflet");
    $.each(maps, function(i, el) {
      var map = this.getMap();
      map.invalidateSize();
      map.eachLayer(function(layer) {
        if (layer instanceof L.TileLayer)
          layer.redraw();
      });
    });
  }

  // trigger 'shown' so htmlwidgets resize
  $('d-article').trigger('shown');
}

function init_distill() {

  init_common();

  // create front matter
  var front_matter = $('<d-front-matter></d-front-matter>');
  $('#distill-front-matter').wrap(front_matter);

  // create d-title
  $('.d-title').changeElementType('d-title');

  // create d-byline
  var byline = $('<d-byline></d-byline>');
  $('.d-byline').replaceWith(byline);

  // create d-article
  var article = $('<d-article></d-article>');
  $('.d-article').wrap(article).children().unwrap();

  // move posts container into article
  $('.posts-container').appendTo($('d-article'));

  // create d-appendix
  $('.d-appendix').changeElementType('d-appendix');

  // flag indicating that we have appendix items
  var appendix = $('.appendix-bottom').children('h3').length > 0;

  // replace footnotes with <d-footnote>
  $('.footnote-ref').each(function(i, val) {
    appendix = true;
    var href = $(this).attr('href');
    var id = href.replace('#', '');
    var fn = $('#' + id);
    var fn_p = $('#' + id + '>p');
    fn_p.find('.footnote-back').remove();
    var text = fn_p.html();
    var dtfn = $('<d-footnote></d-footnote>');
    dtfn.html(text);
    $(this).replaceWith(dtfn);
  });
  // remove footnotes
  $('.footnotes').remove();

  // move refs into #references-listing
  $('#references-listing').replaceWith($('#refs'));

  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    var id = $(this).attr('id');
    $('.d-contents a[href="#' + id + '"]').parent().remove();
    appendix = true;
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
  });

  // show d-appendix if we have appendix content
  $("d-appendix").css('display', appendix ? 'grid' : 'none');

  // localize layout chunks to just output
  $('.layout-chunk').each(function(i, val) {

    // capture layout
    var layout = $(this).attr('data-layout');

    // apply layout to markdown level block elements
    var elements = $(this).children().not('details, div.sourceCode, pre, script');
    elements.each(function(i, el) {
      var layout_div = $('<div class="' + layout + '"></div>');
      if (layout_div.hasClass('shaded')) {
        var shaded_content = $('<div class="shaded-content"></div>');
        $(this).wrap(shaded_content);
        $(this).parent().wrap(layout_div);
      } else {
        $(this).wrap(layout_div);
      }
    });


    // unwrap the layout-chunk div
    $(this).children().unwrap();
  });

  // remove code block used to force  highlighting css
  $('.distill-force-highlighting-css').parent().remove();

  // remove empty line numbers inserted by pandoc when using a
  // custom syntax highlighting theme
  $('code.sourceCode a:empty').remove();

  // load distill framework
  load_distill_framework();

  // wait for window.distillRunlevel == 4 to do post processing
  function distill_post_process() {

    if (!window.distillRunlevel || window.distillRunlevel < 4)
      return;

    // hide author/affiliations entirely if we have no authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
    if (!have_authors)
      $('d-byline').addClass('hidden');

    // article with toc class
    $('.d-contents').parent().addClass('d-article-with-toc');

    // strip links that point to #
    $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

    // add orcid ids
    $('.authors-affiliations').find('.author').each(function(i, el) {
      var orcid_id = front_matter.authors[i].orcidID;
      if (orcid_id) {
        var a = $('<a></a>');
        a.attr('href', 'https://orcid.org/' + orcid_id);
        var img = $('<img></img>');
        img.addClass('orcid-id');
        img.attr('alt', 'ORCID ID');
        img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
        a.append(img);
        $(this).append(a);
      }
    });

    // hide elements of author/affiliations grid that have no value
    function hide_byline_column(caption) {
      $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
    }

    // affiliations
    var have_affiliations = false;
    for (var i = 0; i<front_matter.authors.length; ++i) {
      var author = front_matter.authors[i];
      if (author.affiliation !== "&nbsp;") {
        have_affiliations = true;
        break;
      }
    }
    if (!have_affiliations)
      $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

    // published date
    if (!front_matter.publishedDate)
      hide_byline_column("Published");

    // document object identifier
    var doi = $('d-byline').find('h3:contains("DOI")');
    var doi_p = doi.next().empty();
    if (!front_matter.doi) {
      // if we have a citation and valid citationText then link to that
      if ($('#citation').length > 0 && front_matter.citationText) {
        doi.html('Citation');
        $('<a href="#citation"></a>')
          .text(front_matter.citationText)
          .appendTo(doi_p);
      } else {
        hide_byline_column("DOI");
      }
    } else {
      $('<a></a>')
         .attr('href', "https://doi.org/" + front_matter.doi)
         .html(front_matter.doi)
         .appendTo(doi_p);
    }

     // change plural form of authors/affiliations
    if (front_matter.authors.length === 1) {
      var grid = $('.authors-affiliations');
      grid.children('h3:contains("Authors")').text('Author');
      grid.children('h3:contains("Affiliations")').text('Affiliation');
    }

    // remove d-appendix and d-footnote-list local styles
    $('d-appendix > style:first-child').remove();
    $('d-footnote-list > style:first-child').remove();

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // hoverable references
    $('span.citation[data-cites]').each(function() {
      var refHtml = $('#ref-' + $(this).attr('data-cites')).html();
      window.tippy(this, {
        allowHTML: true,
        content: refHtml,
        maxWidth: 500,
        interactive: true,
        interactiveBorder: 10,
        theme: 'light-border',
        placement: 'bottom-start'
      });
    });

    // clear polling timer
    clearInterval(tid);

    // show body now that everything is ready
    on_load_complete();
  }

  var tid = setInterval(distill_post_process, 50);
  distill_post_process();

}

function init_downlevel() {

  init_common();

   // insert hr after d-title
  $('.d-title').after($('<hr class="section-separator"/>'));

  // check if we have authors
  var front_matter = JSON.parse($("#distill-front-matter").html());
  var have_authors = front_matter.authors && front_matter.authors.length > 0;

  // manage byline/border
  if (!have_authors)
    $('.d-byline').remove();
  $('.d-byline').after($('<hr class="section-separator"/>'));
  $('.d-byline a').remove();

  // remove toc
  $('.d-contents').remove();

  // move appendix elements
  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
  });


  // inject headers into references and footnotes
  var refs_header = $('<h3></h3>');
  refs_header.text('References');
  $('#refs').prepend(refs_header);

  var footnotes_header = $('<h3></h3');
  footnotes_header.text('Footnotes');
  $('.footnotes').children('hr').first().replaceWith(footnotes_header);

  // move appendix-bottom entries to the bottom
  $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
  $('.appendix-bottom').remove();

  // remove appendix if it's empty
  if ($('.d-appendix').children().length === 0)
    $('.d-appendix').remove();

  // prepend separator above appendix
  $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

  // trim code
  $('pre>code').each(function(i, val) {
    $(this).html($.trim($(this).html()));
  });

  // move posts-container right before article
  $('.posts-container').insertBefore($('.d-article'));

  $('body').addClass('downlevel');

  on_load_complete();
}


function init_common() {

  // jquery plugin to change element types
  (function($) {
    $.fn.changeElementType = function(newType) {
      var attrs = {};

      $.each(this[0].attributes, function(idx, attr) {
        attrs[attr.nodeName] = attr.nodeValue;
      });

      this.replaceWith(function() {
        return $("<" + newType + "/>", attrs).append($(this).contents());
      });
    };
  })(jQuery);

  // prevent underline for linked images
  $('a > img').parent().css({'border-bottom' : 'none'});

  // mark non-body figures created by knitr chunks as 100% width
  $('.layout-chunk').each(function(i, val) {
    var figures = $(this).find('img, .html-widget');
    if ($(this).attr('data-layout') !== "l-body") {
      figures.css('width', '100%');
    } else {
      figures.css('max-width', '100%');
      figures.filter("[width]").each(function(i, val) {
        var fig = $(this);
        fig.css('width', fig.attr('width') + 'px');
      });

    }
  });

  // auto-append index.html to post-preview links in file: protocol
  // and in rstudio ide preview
  $('.post-preview').each(function(i, val) {
    if (window.location.protocol === "file:")
      $(this).attr('href', $(this).attr('href') + "index.html");
  });

  // get rid of index.html references in header
  if (window.location.protocol !== "file:") {
    $('.distill-site-header a[href]').each(function(i,val) {
      $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
    });
  }

  // add class to pandoc style tables
  $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
  $('.kable-table').children('table').addClass('pandoc-table');

  // add figcaption style to table captions
  $('caption').parent('table').addClass("figcaption");

  // initialize posts list
  if (window.init_posts_list)
    window.init_posts_list();

  // implmement disqus comment link
  $('.disqus-comment-count').click(function() {
    window.headroom_prevent_pin = true;
    $('#disqus_thread').toggleClass('hidden');
    if (!$('#disqus_thread').hasClass('hidden')) {
      var offset = $(this).offset();
      $(window).resize();
      $('html, body').animate({
        scrollTop: offset.top - 35
      });
    }
  });
}

document.addEventListener('DOMContentLoaded', function() {
  if (is_downlevel_browser())
    init_downlevel();
  else
    window.addEventListener('WebComponentsReady', init_distill);
});

</script>

<!--/radix_placeholder_distill-->
  <script src="../../site_libs/header-attrs-2.6/header-attrs.js"></script>
  <script src="../../site_libs/jquery-1.11.3/jquery.min.js"></script>
  <script src="../../site_libs/popper-2.6.0/popper.min.js"></script>
  <link href="../../site_libs/tippy-6.2.7/tippy.css" rel="stylesheet" />
  <link href="../../site_libs/tippy-6.2.7/tippy-light-border.css" rel="stylesheet" />
  <script src="../../site_libs/tippy-6.2.7/tippy.umd.min.js"></script>
  <script src="../../site_libs/anchor-4.2.2/anchor.min.js"></script>
  <script src="../../site_libs/bowser-1.9.3/bowser.min.js"></script>
  <script src="../../site_libs/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="../../site_libs/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
<script type="text/javascript" cookie-consent="tracking" async src="https://www.googletagmanager.com/gtag/js?id=UA-20375833-3"></script>
<script type="text/javascript" cookie-consent="tracking">
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-20375833-3');
</script>
<!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Hacking deep learning: model inversion attack by example","description":"Compared to other applications, deep learning models might not seem too likely as victims of privacy attacks. However, methods exist to determine whether an entity was used in the training set (an adversarial attack called member inference), and techniques subsumed under \"model inversion\" allow to reconstruct raw data input given just model output (and sometimes, context information). This post shows an end-to-end example of model inversion, and explores mitigation strategies using TensorFlow Privacy.","authors":[{"author":"Sigrid Keydana","authorURL":"#","affiliation":"RStudio","affiliationURL":"https://www.rstudio.com/","orcidID":""}],"publishedDate":"2020-05-15T00:00:00.000+00:00","citationText":"Keydana, 2020"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<header class="header header--fixed" role="banner">
<nav class="distill-site-nav distill-site-header">
<div class="nav-left">
<span class="logo">
<img src="../../images/rstudio.png"/>
</span>
<a href="../../index.html" class="title">AI Blog</a>
<input id="distill-search" class="nav-search hidden" type="text" placeholder="Search..."/>
</div>
<div class="nav-right">
<a href="../../index.html">Home</a>
<a href="../../gallery.html">Gallery</a>
<a href="../../about.html">About</a>
<a href="../../contributing.html">Contributing</a>
<a href="../../index.xml">
<i class="fa fa-rss" aria-hidden="true"></i>
</a>
<a href="javascript:void(0);" class="nav-toggle">&#9776;</a>
</div>
</nav>
</header>
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Hacking deep learning: model inversion attack by example</h1>
<!--radix_placeholder_categories-->
<div class="dt-tags">
  <a href="../../index.html#category:R" class="dt-tag">R</a>
  <a href="../../index.html#category:Privacy_&amp;_Security" class="dt-tag">Privacy &amp; Security</a>
  <a href="../../index.html#category:TensorFlow/Keras" class="dt-tag">TensorFlow/Keras</a>
</div>
<!--/radix_placeholder_categories-->
<p><p>Compared to other applications, deep learning models might not seem too likely as victims of privacy attacks. However, methods exist to determine whether an entity was used in the training set (an adversarial attack called member inference), and techniques subsumed under âmodel inversionâ allow to reconstruct raw data input given just model output (and sometimes, context information). This post shows an end-to-end example of model inversion, and explores mitigation strategies using TensorFlow Privacy.</p></p>
</div>

<div class="d-byline">
  Sigrid Keydana  (RStudio)<a href="https://www.rstudio.com/" class="uri">https://www.rstudio.com/</a>
  
<br/>05-15-2020
</div>

<div class="d-article">
<p>How private are individual data in the context of machine learning models? The data used to train the model, say. There are types of models where the answer is simple. Take k-nearest-neighbors, for example. There <em>is not</em> even a model without the complete dataset. Or support vector machines. There is no model without the support vectors. But neural networks? Theyâre just some composition of functions, â no data included.</p>
<p>The same is true for data fed to a deployed deep-learning model. Itâs pretty unlikely one could invert the final softmax output from a big ResNet and get back the raw input data.</p>
<p>In theory, then, âhackingâ a standard neural net to spy on input data sounds illusory. In practice, however, there is always some real-world <em>context</em>. The context may be other datasets, publicly available, that can be linked to the âprivateâ data in question. This is a popular showcase used in advocating for differential privacy<span class="citation" data-cites="Dwork2006">(Dwork et al. <a href="#ref-Dwork2006" role="doc-biblioref">2006</a>)</span>: Take an âanonymizedâ dataset, dig up complementary information from public sources, and de-anonymize records ad libitum. Some context in that sense will often be used in âblack-boxâ attacks, ones that presuppose no insider information about the model to be hacked.</p>
<p>But context can also be structural, such as in the scenario demonstrated in this post. For example, assume a distributed model, where sets of layers run on different devices â embedded devices or mobile phones, for example. (A scenario like that is sometimes seen as âwhite-boxâ<span class="citation" data-cites="7536387">(Wu et al. <a href="#ref-7536387" role="doc-biblioref">2016</a>)</span>, but in common understanding, white-box attacks probably presuppose some more insider knowledge, such as access to model architecture or even, weights. Iâd therefore prefer calling this white-ish at most.) â Now assume that in this context, it is possible to intercept, and interact with, a system that executes the deeper layers of the model. Based on that systemâs intermediate-level output, it is possible to perform <em>model inversion</em><span class="citation" data-cites="Fred">(Fredrikson et al. <a href="#ref-Fred" role="doc-biblioref">2014</a>)</span>, that is, to reconstruct the input data fed into the system.</p>
<p>In this post, weâll demonstrate such a model inversion attack, basically porting the approach given in a <a href="https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/advanced/privacy_attacks/Tutorial%201%20-%20Black%20box%20model%20inversion.ipynb%20Sy">notebook</a> found in the <a href="https://github.com/OpenMined/PySyft">PySyft</a> repository. We then experiment with different levels of <span class="math inline">\(\epsilon\)</span>-privacy, exploring impact on reconstruction success. This second part will make use of TensorFlow Privacy, introduced in a <a href="https://blogs.rstudio.com/ai/posts/2019-12-20-differential-privacy/">previous blog post</a>.</p>
<h2 id="part-1-model-inversion-in-action">Part 1: Model inversion in action</h2>
<h3 id="example-dataset-all-the-worlds-letters1">Example dataset: All the worldâs letters<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></h3>
<p>The overall process of model inversion used here is the following. With no, or scarcely any, insider knowledge about a model, â but given opportunities to repeatedly query it â, I want to learn how to reconstruct unknown inputs based on just model outputs . Independently of original model training, this, too, is a training process; however, in general it will not involve the original data, as those wonât be publicly available. Still, for best success, the attacker model is trained with data as similar as possible to the original training data assumed. Thinking of images, for example, and presupposing the popular view of successive layers representing successively coarse-grained features, we want that the surrogate data to share as many representation spaces with the real data as possible â up to the very highest layers before final classification, ideally.</p>
<p>If we wanted to use classical MNIST as an example, one thing we could do is to only use some of the digits for training the ârealâ model; and the rest, for training the adversary. Letâs try something different though, something that might make the undertaking harder as well as easier at the same time. Harder, because the dataset features exemplars more complex than MNIST digits; easier because of the same reason: More could possibly be learned, by the adversary, from a complex task.</p>
<p>Originally designed to develop a machine model of concept learning and generalization <span class="citation" data-cites="Lake1332">(Lake, Salakhutdinov, and Tenenbaum <a href="#ref-Lake1332" role="doc-biblioref">2015</a>)</span>, the <a href="https://github.com/brendenlake/omniglot/">OmniGlot</a> dataset incorporates characters from fifty alphabets, split into two disjoint groups of thirty and twenty alphabets each. Weâll use the group of twenty to train our target model. Here is a sample:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span id="fig:unnamed-chunk-1"></span>
<img src="images/training_set_images.png" alt="Sample from the twenty-alphabet set used to train the target model (originally: 'evaluation set')" width="398" />
<p class="caption">
Figure 1: Sample from the twenty-alphabet set used to train the target model (originally: âevaluation setâ)
</p>
</div>
</div>
<p>The group of thirty we donât use; instead, weâll employ two small five-alphabet collections to train the adversary and to test reconstruction, respectively. (These small subsets of the original âbigâ thirty-alphabet set are again disjoint.)</p>
<p>Here first is a sample from the set used to train the adversary.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span id="fig:unnamed-chunk-2"></span>
<img src="images/attacker_ds.png" alt="Sample from the five-alphabet set used to train the adversary (originally: 'background small 1')" width="404" />
<p class="caption">
Figure 2: Sample from the five-alphabet set used to train the adversary (originally: âbackground small 1â)
</p>
</div>
</div>
<p>The other small subset will be used to test the adversaryâs spying capabilities after training. Letâs peek at this one, too:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span id="fig:unnamed-chunk-3"></span>
<img src="images/actual_test_images.png" alt="Sample from the five-alphabet set used to test the adversary after training(originally: 'background small 2')" width="386" />
<p class="caption">
Figure 3: Sample from the five-alphabet set used to test the adversary after training(originally: âbackground small 2â)
</p>
</div>
</div>
<p>Conveniently, we can use <a href="https://github.com/rstudio/tfds">tfds</a>, the R wrapper to TensorFlow Datasets, to load those subsets:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://github.com/rstudio/reticulate'>reticulate</a></span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'>keras</span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'>tfdatasets</span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'>tfautograph</span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'>tfds</span><span class='op'>)</span>

<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='http://purrr.tidyverse.org'>purrr</a></span><span class='op'>)</span>

<span class='co'># we'll use this to train the target model</span>
<span class='co'># n = 13180</span>
<span class='va'>omni_train</span> <span class='op'>&lt;-</span> <span class='va'>tfds</span><span class='op'>$</span><span class='fu'>load</span><span class='op'>(</span><span class='st'>"omniglot"</span>, split <span class='op'>=</span> <span class='st'>"test"</span><span class='op'>)</span>

<span class='co'># this is used to train the adversary</span>
<span class='co'># n = 2720</span>
<span class='va'>omni_spy</span> <span class='op'>&lt;-</span> <span class='va'>tfds</span><span class='op'>$</span><span class='fu'>load</span><span class='op'>(</span><span class='st'>"omniglot"</span>, split <span class='op'>=</span> <span class='st'>"small1"</span><span class='op'>)</span>

<span class='co'># this we'll use for testing</span>
<span class='co'># n = 3120</span>
<span class='va'>omni_test</span> <span class='op'>&lt;-</span> <span class='va'>tfds</span><span class='op'>$</span><span class='fu'>load</span><span class='op'>(</span><span class='st'>"omniglot"</span>, split <span class='op'>=</span> <span class='st'>"small2"</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>Now first, we train the target model.</p>
<h3 id="train-target-model">Train target model</h3>
<p>The dataset originally has four columns: the image, of size 105 x 105; an alphabet id and a within-dataset character id; and a label. For our use case, weâre not really interested in the task the target model was/is used for; we just want to get at the data. Basically, whatever task we choose, it is not much more than a dummy task. So, letâs just say we train the target to classify characters <em>by alphabet</em>.</p>
<p>We thus throw out all unneeded features, keeping just the alphabet id and the image itself:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='co'># normalize and work with a single channel (images are black-and-white anyway)</span>
<span class='va'>preprocess_image</span> <span class='op'>&lt;-</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>image</span><span class='op'>)</span> <span class='op'>{</span>
  <span class='va'>image</span> <span class='op'>%&gt;%</span>
    <span class='va'>tf</span><span class='op'>$</span><span class='fu'>cast</span><span class='op'>(</span>dtype <span class='op'>=</span> <span class='va'>tf</span><span class='op'>$</span><span class='va'>float32</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
    <span class='va'>tf</span><span class='op'>$</span><span class='fu'>truediv</span><span class='op'>(</span>y <span class='op'>=</span> <span class='fl'>255</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
    <span class='va'>tf</span><span class='op'>$</span><span class='va'>image</span><span class='op'>$</span><span class='fu'>rgb_to_grayscale</span><span class='op'>(</span><span class='op'>)</span>
<span class='op'>}</span>

<span class='co'># use the first 11000 images for training</span>
<span class='va'>train_ds</span> <span class='op'>&lt;-</span> <span class='va'>omni_train</span> <span class='op'>%&gt;%</span> 
  <span class='fu'>dataset_take</span><span class='op'>(</span><span class='fl'>11000</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>dataset_map</span><span class='op'>(</span><span class='kw'>function</span><span class='op'>(</span><span class='va'>record</span><span class='op'>)</span> <span class='op'>{</span>
    <span class='va'>record</span><span class='op'>$</span><span class='va'>image</span> <span class='op'>&lt;-</span> <span class='fu'>preprocess_image</span><span class='op'>(</span><span class='va'>record</span><span class='op'>$</span><span class='va'>image</span><span class='op'>)</span>
    <span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span><span class='va'>record</span><span class='op'>$</span><span class='va'>image</span>, <span class='va'>record</span><span class='op'>$</span><span class='va'>alphabet</span><span class='op'>)</span><span class='op'>}</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>dataset_shuffle</span><span class='op'>(</span><span class='fl'>1000</span><span class='op'>)</span> <span class='op'>%&gt;%</span> 
  <span class='fu'>dataset_batch</span><span class='op'>(</span><span class='fl'>32</span><span class='op'>)</span>

<span class='co'># use the remaining 2180 records for validation</span>
<span class='va'>val_ds</span> <span class='op'>&lt;-</span> <span class='va'>omni_train</span> <span class='op'>%&gt;%</span> 
  <span class='fu'>dataset_skip</span><span class='op'>(</span><span class='fl'>11000</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>dataset_map</span><span class='op'>(</span><span class='kw'>function</span><span class='op'>(</span><span class='va'>record</span><span class='op'>)</span> <span class='op'>{</span>
    <span class='va'>record</span><span class='op'>$</span><span class='va'>image</span> <span class='op'>&lt;-</span> <span class='fu'>preprocess_image</span><span class='op'>(</span><span class='va'>record</span><span class='op'>$</span><span class='va'>image</span><span class='op'>)</span>
    <span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span><span class='va'>record</span><span class='op'>$</span><span class='va'>image</span>, <span class='va'>record</span><span class='op'>$</span><span class='va'>alphabet</span><span class='op'>)</span><span class='op'>}</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>dataset_batch</span><span class='op'>(</span><span class='fl'>32</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>The model consists of two parts. The first is imagined to run in a distributed fashion; for example, on mobile devices (stage one). These devices then send model outputs to a central server, where final results are computed (stage two). Sure, you will be thinking, this is a convenient setup for our scenario: If we intercept stage one results, we â most probably â gain access to richer information than what is contained in a modelâs final output layer. â That is correct, but the scenario is less contrived than one might assume. Just like federated learning <span class="citation" data-cites="McMahanMRA16">(McMahan et al. <a href="#ref-McMahanMRA16" role="doc-biblioref">2016</a>)</span>, it fulfills important desiderata: Actual training data never leaves the devices, thus staying (in theory!) private; at the same time, ingoing traffic to the server is significantly reduced.</p>
<p>In our example setup, the on-device model is a convnet, while the server model is a simple feedforward network.</p>
<p>We link both together as a <em>TargetModel</em> that when called normally, will run both steps in succession. However, weâll be able to call <code>target_model$mobile_step()</code> separately, thereby intercepting intermediate results.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>on_device_model</span> <span class='op'>&lt;-</span> <span class='fu'>keras_model_sequential</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_conv_2d</span><span class='op'>(</span>filters <span class='op'>=</span> <span class='fl'>32</span>, kernel_size <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>7</span>, <span class='fl'>7</span><span class='op'>)</span>,
                input_shape <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>105</span>, <span class='fl'>105</span>, <span class='fl'>1</span><span class='op'>)</span>, activation <span class='op'>=</span> <span class='st'>"relu"</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_batch_normalization</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_max_pooling_2d</span><span class='op'>(</span>pool_size <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>3</span>, <span class='fl'>3</span><span class='op'>)</span>, strides <span class='op'>=</span> <span class='fl'>3</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_dropout</span><span class='op'>(</span><span class='fl'>0.2</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_conv_2d</span><span class='op'>(</span>filters <span class='op'>=</span> <span class='fl'>32</span>, kernel_size <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>7</span>, <span class='fl'>7</span><span class='op'>)</span>, activation <span class='op'>=</span> <span class='st'>"relu"</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_batch_normalization</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_max_pooling_2d</span><span class='op'>(</span>pool_size <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>3</span>, <span class='fl'>3</span><span class='op'>)</span>, strides <span class='op'>=</span> <span class='fl'>2</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_dropout</span><span class='op'>(</span><span class='fl'>0.2</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_conv_2d</span><span class='op'>(</span>filters <span class='op'>=</span> <span class='fl'>32</span>, kernel_size <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>5</span>, <span class='fl'>5</span><span class='op'>)</span>, activation <span class='op'>=</span> <span class='st'>"relu"</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_batch_normalization</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_max_pooling_2d</span><span class='op'>(</span>pool_size <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>2</span>, <span class='fl'>2</span><span class='op'>)</span>, strides <span class='op'>=</span> <span class='fl'>2</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_dropout</span><span class='op'>(</span><span class='fl'>0.2</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_conv_2d</span><span class='op'>(</span>filters <span class='op'>=</span> <span class='fl'>32</span>, kernel_size <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>3</span>, <span class='fl'>3</span><span class='op'>)</span>, activation <span class='op'>=</span> <span class='st'>"relu"</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_batch_normalization</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_max_pooling_2d</span><span class='op'>(</span>pool_size <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>2</span>, <span class='fl'>2</span><span class='op'>)</span>, strides <span class='op'>=</span> <span class='fl'>2</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_dropout</span><span class='op'>(</span><span class='fl'>0.2</span><span class='op'>)</span> 

<span class='va'>server_model</span> <span class='op'>&lt;-</span> <span class='fu'>keras_model_sequential</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_dense</span><span class='op'>(</span>units <span class='op'>=</span> <span class='fl'>256</span>, activation <span class='op'>=</span> <span class='st'>"relu"</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_flatten</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_dropout</span><span class='op'>(</span><span class='fl'>0.2</span><span class='op'>)</span> <span class='op'>%&gt;%</span> 
  <span class='co'># we have just 20 different ids, but they are not in lexicographic order</span>
  <span class='fu'>layer_dense</span><span class='op'>(</span>units <span class='op'>=</span> <span class='fl'>50</span>, activation <span class='op'>=</span> <span class='st'>"softmax"</span><span class='op'>)</span>

<span class='va'>target_model</span> <span class='op'>&lt;-</span> <span class='kw'>function</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>{</span>
  <span class='fu'>keras_model_custom</span><span class='op'>(</span>name <span class='op'>=</span> <span class='st'>"TargetModel"</span>, <span class='kw'>function</span><span class='op'>(</span><span class='va'>self</span><span class='op'>)</span> <span class='op'>{</span>
    
    <span class='va'>self</span><span class='op'>$</span><span class='va'>on_device_model</span> <span class='op'>&lt;-</span><span class='va'>on_device_model</span>
    <span class='va'>self</span><span class='op'>$</span><span class='va'>server_model</span> <span class='op'>&lt;-</span> <span class='va'>server_model</span>
    <span class='va'>self</span><span class='op'>$</span><span class='va'>mobile_step</span> <span class='op'>&lt;-</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>inputs</span><span class='op'>)</span> 
      <span class='va'>self</span><span class='op'>$</span><span class='fu'>on_device_model</span><span class='op'>(</span><span class='va'>inputs</span><span class='op'>)</span>
    <span class='va'>self</span><span class='op'>$</span><span class='va'>server_step</span> <span class='op'>&lt;-</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>inputs</span><span class='op'>)</span>
      <span class='va'>self</span><span class='op'>$</span><span class='fu'>server_model</span><span class='op'>(</span><span class='va'>inputs</span><span class='op'>)</span>

    <span class='kw'>function</span><span class='op'>(</span><span class='va'>inputs</span>, <span class='va'>mask</span> <span class='op'>=</span> <span class='cn'>NULL</span><span class='op'>)</span> <span class='op'>{</span>
      <span class='va'>inputs</span> <span class='op'>%&gt;%</span> 
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>mobile_step</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>server_step</span><span class='op'>(</span><span class='op'>)</span>
    <span class='op'>}</span>
  <span class='op'>}</span><span class='op'>)</span>
  
<span class='op'>}</span>

<span class='va'>model</span> <span class='op'>&lt;-</span> <span class='fu'>target_model</span><span class='op'>(</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>The overall model is a Keras custom model, so we train it <a href="https://tensorflow.rstudio.com/tutorials/advanced/">TensorFlow 2.x - style</a>. After ten epochs, training and validation accuracy are at ~0.84 and ~0.73, respectively â not bad at all for a 20-class discrimination task.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>loss</span> <span class='op'>&lt;-</span> <span class='va'>loss_sparse_categorical_crossentropy</span>
<span class='va'>optimizer</span> <span class='op'>&lt;-</span> <span class='fu'>optimizer_adam</span><span class='op'>(</span><span class='op'>)</span>

<span class='va'>train_loss</span> <span class='op'>&lt;-</span> <span class='va'>tf</span><span class='op'>$</span><span class='va'>keras</span><span class='op'>$</span><span class='va'>metrics</span><span class='op'>$</span><span class='fu'>Mean</span><span class='op'>(</span>name<span class='op'>=</span><span class='st'>'train_loss'</span><span class='op'>)</span>
<span class='va'>train_accuracy</span> <span class='op'>&lt;-</span>  <span class='va'>tf</span><span class='op'>$</span><span class='va'>keras</span><span class='op'>$</span><span class='va'>metrics</span><span class='op'>$</span><span class='fu'>SparseCategoricalAccuracy</span><span class='op'>(</span>name<span class='op'>=</span><span class='st'>'train_accuracy'</span><span class='op'>)</span>

<span class='va'>val_loss</span> <span class='op'>&lt;-</span> <span class='va'>tf</span><span class='op'>$</span><span class='va'>keras</span><span class='op'>$</span><span class='va'>metrics</span><span class='op'>$</span><span class='fu'>Mean</span><span class='op'>(</span>name<span class='op'>=</span><span class='st'>'val_loss'</span><span class='op'>)</span>
<span class='va'>val_accuracy</span> <span class='op'>&lt;-</span>  <span class='va'>tf</span><span class='op'>$</span><span class='va'>keras</span><span class='op'>$</span><span class='va'>metrics</span><span class='op'>$</span><span class='fu'>SparseCategoricalAccuracy</span><span class='op'>(</span>name<span class='op'>=</span><span class='st'>'val_accuracy'</span><span class='op'>)</span>

<span class='va'>train_step</span> <span class='op'>&lt;-</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>images</span>, <span class='va'>labels</span><span class='op'>)</span> <span class='op'>{</span>
  <span class='fu'><a href='https://rdrr.io/r/base/with.html'>with</a></span> <span class='op'>(</span><span class='va'>tf</span><span class='op'>$</span><span class='fu'>GradientTape</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%as%</span> <span class='va'>tape</span>, <span class='op'>{</span>
    <span class='va'>predictions</span> <span class='op'>&lt;-</span> <span class='fu'>model</span><span class='op'>(</span><span class='va'>images</span><span class='op'>)</span>
    <span class='va'>l</span> <span class='op'>&lt;-</span> <span class='fu'>loss</span><span class='op'>(</span><span class='va'>labels</span>, <span class='va'>predictions</span><span class='op'>)</span>
  <span class='op'>}</span><span class='op'>)</span>
  <span class='va'>gradients</span> <span class='op'>&lt;-</span> <span class='va'>tape</span><span class='op'>$</span><span class='fu'>gradient</span><span class='op'>(</span><span class='va'>l</span>, <span class='va'>model</span><span class='op'>$</span><span class='va'>trainable_variables</span><span class='op'>)</span>
  <span class='va'>optimizer</span><span class='op'>$</span><span class='fu'>apply_gradients</span><span class='op'>(</span><span class='fu'>purrr</span><span class='fu'>::</span><span class='fu'><a href='https://purrr.tidyverse.org/reference/transpose.html'>transpose</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span>
    <span class='va'>gradients</span>, <span class='va'>model</span><span class='op'>$</span><span class='va'>trainable_variables</span>
  <span class='op'>)</span><span class='op'>)</span><span class='op'>)</span>
  <span class='fu'>train_loss</span><span class='op'>(</span><span class='va'>l</span><span class='op'>)</span>
  <span class='fu'>train_accuracy</span><span class='op'>(</span><span class='va'>labels</span>, <span class='va'>predictions</span><span class='op'>)</span>
<span class='op'>}</span>

<span class='va'>val_step</span> <span class='op'>&lt;-</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>images</span>, <span class='va'>labels</span><span class='op'>)</span> <span class='op'>{</span>
  <span class='va'>predictions</span> <span class='op'>&lt;-</span> <span class='fu'>model</span><span class='op'>(</span><span class='va'>images</span><span class='op'>)</span>
  <span class='va'>l</span> <span class='op'>&lt;-</span> <span class='fu'>loss</span><span class='op'>(</span><span class='va'>labels</span>, <span class='va'>predictions</span><span class='op'>)</span>
  <span class='fu'>val_loss</span><span class='op'>(</span><span class='va'>l</span><span class='op'>)</span>
  <span class='fu'>val_accuracy</span><span class='op'>(</span><span class='va'>labels</span>, <span class='va'>predictions</span><span class='op'>)</span>
<span class='op'>}</span>


<span class='va'>training_loop</span> <span class='op'>&lt;-</span> <span class='fu'>tf_function</span><span class='op'>(</span><span class='fu'>autograph</span><span class='op'>(</span><span class='kw'>function</span><span class='op'>(</span><span class='va'>train_ds</span>, <span class='va'>val_ds</span><span class='op'>)</span> <span class='op'>{</span>
  <span class='kw'>for</span> <span class='op'>(</span><span class='va'>b1</span> <span class='kw'>in</span> <span class='va'>train_ds</span><span class='op'>)</span> <span class='op'>{</span>
    <span class='fu'>train_step</span><span class='op'>(</span><span class='va'>b1</span><span class='op'>[[</span><span class='fl'>1</span><span class='op'>]</span><span class='op'>]</span>, <span class='va'>b1</span><span class='op'>[[</span><span class='fl'>2</span><span class='op'>]</span><span class='op'>]</span><span class='op'>)</span>
  <span class='op'>}</span>
  <span class='kw'>for</span> <span class='op'>(</span><span class='va'>b2</span> <span class='kw'>in</span> <span class='va'>val_ds</span><span class='op'>)</span> <span class='op'>{</span>
    <span class='fu'>val_step</span><span class='op'>(</span><span class='va'>b2</span><span class='op'>[[</span><span class='fl'>1</span><span class='op'>]</span><span class='op'>]</span>, <span class='va'>b2</span><span class='op'>[[</span><span class='fl'>2</span><span class='op'>]</span><span class='op'>]</span><span class='op'>)</span>
  <span class='op'>}</span>
  
  <span class='va'>tf</span><span class='op'>$</span><span class='fu'>print</span><span class='op'>(</span><span class='st'>"Train accuracy"</span>, <span class='va'>train_accuracy</span><span class='op'>$</span><span class='fu'>result</span><span class='op'>(</span><span class='op'>)</span>,
           <span class='st'>"    Validation Accuracy"</span>, <span class='va'>val_accuracy</span><span class='op'>$</span><span class='fu'>result</span><span class='op'>(</span><span class='op'>)</span><span class='op'>)</span>
  
  <span class='va'>train_loss</span><span class='op'>$</span><span class='fu'>reset_states</span><span class='op'>(</span><span class='op'>)</span>
  <span class='va'>train_accuracy</span><span class='op'>$</span><span class='fu'>reset_states</span><span class='op'>(</span><span class='op'>)</span>
  <span class='va'>val_loss</span><span class='op'>$</span><span class='fu'>reset_states</span><span class='op'>(</span><span class='op'>)</span>
  <span class='va'>val_accuracy</span><span class='op'>$</span><span class='fu'>reset_states</span><span class='op'>(</span><span class='op'>)</span>
<span class='op'>}</span><span class='op'>)</span><span class='op'>)</span>


<span class='kw'>for</span> <span class='op'>(</span><span class='va'>epoch</span> <span class='kw'>in</span> <span class='fl'>1</span><span class='op'>:</span><span class='fl'>10</span><span class='op'>)</span> <span class='op'>{</span>
  <span class='fu'><a href='https://rdrr.io/r/base/cat.html'>cat</a></span><span class='op'>(</span><span class='st'>"Epoch: "</span>, <span class='va'>epoch</span>, <span class='st'>" -----------\n"</span><span class='op'>)</span>
  <span class='fu'>training_loop</span><span class='op'>(</span><span class='va'>train_ds</span>, <span class='va'>val_ds</span><span class='op'>)</span>  
<span class='op'>}</span>
</code></pre>
</div>
</div>
<pre><code>Epoch:  1  -----------
Train accuracy 0.195090905     Validation Accuracy 0.376605511
Epoch:  2  -----------
Train accuracy 0.472272724     Validation Accuracy 0.5243119
...
...
Epoch:  9  -----------
Train accuracy 0.821454525     Validation Accuracy 0.720183492
Epoch:  10  -----------
Train accuracy 0.840454519     Validation Accuracy 0.726605475</code></pre>
<p>Now, we train the adversary.</p>
<h3 id="train-adversary">Train adversary</h3>
<p>The adversaryâs general strategy will be:</p>
<ul>
<li><strong>Feed its small, surrogate dataset to the on-device model.</strong> The output received can be regarded as a (highly) <em>compressed</em> version of the original images.</li>
<li>P<strong>ass that âcompressedâ version as input to its own model,</strong> which tries to reconstruct the original images from the sparse code.</li>
<li><strong>Compare original images (those from the surrogate dataset) to the reconstruction pixel-wise.</strong> The goal is to minimize the mean (squared, say) error.</li>
</ul>
<p>Doesnât this sound a lot like the decoding side of an autoencoder? No wonder the attacker model is a deconvolutional network. Its input â equivalently, the on-device modelâs output â is of size <code>batch_size x 1 x 1 x 32</code>. That is, the information is encoded in 32 channels, but the spatial resolution is 1. Just like in an autoencoder operating on images, we need to <em>upsample</em> until we arrive at the original resolution of 105 x 105.</p>
<p>This is exactly whatâs happening in the attacker model:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>attack_model</span> <span class='op'>&lt;-</span> <span class='kw'>function</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>{</span>
  
  <span class='fu'>keras_model_custom</span><span class='op'>(</span>name <span class='op'>=</span> <span class='st'>"AttackModel"</span>, <span class='kw'>function</span><span class='op'>(</span><span class='va'>self</span><span class='op'>)</span> <span class='op'>{</span>
    
    <span class='va'>self</span><span class='op'>$</span><span class='va'>conv1</span> <span class='op'>&lt;-</span><span class='fu'>layer_conv_2d_transpose</span><span class='op'>(</span>filters <span class='op'>=</span> <span class='fl'>32</span>, kernel_size <span class='op'>=</span> <span class='fl'>9</span>,
                                         padding <span class='op'>=</span> <span class='st'>"valid"</span>,
                                         strides <span class='op'>=</span> <span class='fl'>1</span>, activation <span class='op'>=</span> <span class='st'>"relu"</span><span class='op'>)</span>
    <span class='va'>self</span><span class='op'>$</span><span class='va'>conv2</span> <span class='op'>&lt;-</span> <span class='fu'>layer_conv_2d_transpose</span><span class='op'>(</span>filters <span class='op'>=</span> <span class='fl'>32</span>, kernel_size <span class='op'>=</span> <span class='fl'>7</span>,
                                          padding <span class='op'>=</span> <span class='st'>"valid"</span>,
                                          strides <span class='op'>=</span> <span class='fl'>2</span>, activation <span class='op'>=</span> <span class='st'>"relu"</span><span class='op'>)</span> 
    <span class='va'>self</span><span class='op'>$</span><span class='va'>conv3</span> <span class='op'>&lt;-</span> <span class='fu'>layer_conv_2d_transpose</span><span class='op'>(</span>filters <span class='op'>=</span> <span class='fl'>1</span>, kernel_size <span class='op'>=</span> <span class='fl'>7</span>,
                                          padding <span class='op'>=</span> <span class='st'>"valid"</span>,
                                          strides <span class='op'>=</span> <span class='fl'>2</span>, activation <span class='op'>=</span> <span class='st'>"relu"</span><span class='op'>)</span>  
    <span class='va'>self</span><span class='op'>$</span><span class='va'>conv4</span> <span class='op'>&lt;-</span> <span class='fu'>layer_conv_2d_transpose</span><span class='op'>(</span>filters <span class='op'>=</span> <span class='fl'>1</span>, kernel_size <span class='op'>=</span> <span class='fl'>5</span>,
                                          padding <span class='op'>=</span> <span class='st'>"valid"</span>,
                                          strides <span class='op'>=</span> <span class='fl'>2</span>, activation <span class='op'>=</span> <span class='st'>"relu"</span><span class='op'>)</span>
    
    <span class='kw'>function</span><span class='op'>(</span><span class='va'>inputs</span>, <span class='va'>mask</span> <span class='op'>=</span> <span class='cn'>NULL</span><span class='op'>)</span> <span class='op'>{</span>
      <span class='va'>inputs</span> <span class='op'>%&gt;%</span> 
        <span class='co'># bs * 9 * 9 * 32</span>
        <span class='co'># output = strides * (input - 1) + kernel_size - 2 * padding</span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>conv1</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
        <span class='co'># bs * 23 * 23 * 32</span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>conv2</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
        <span class='co'># bs * 51 * 51 * 1</span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>conv3</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
        <span class='co'># bs * 105 * 105 * 1</span>
        <span class='va'>self</span><span class='op'>$</span><span class='fu'>conv4</span><span class='op'>(</span><span class='op'>)</span>
    <span class='op'>}</span>
  <span class='op'>}</span><span class='op'>)</span>
  
<span class='op'>}</span>

<span class='va'>attacker</span> <span class='op'>=</span> <span class='fu'>attack_model</span><span class='op'>(</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>To train the adversary, we use one of the small (five-alphabet) subsets. To reiterate what was said above, there is no overlap with the data used to train the target model.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>attacker_ds</span> <span class='op'>&lt;-</span> <span class='va'>omni_spy</span> <span class='op'>%&gt;%</span> 
<span class='fu'>dataset_map</span><span class='op'>(</span><span class='kw'>function</span><span class='op'>(</span><span class='va'>record</span><span class='op'>)</span> <span class='op'>{</span>
    <span class='va'>record</span><span class='op'>$</span><span class='va'>image</span> <span class='op'>&lt;-</span> <span class='fu'>preprocess_image</span><span class='op'>(</span><span class='va'>record</span><span class='op'>$</span><span class='va'>image</span><span class='op'>)</span>
    <span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span><span class='va'>record</span><span class='op'>$</span><span class='va'>image</span>, <span class='va'>record</span><span class='op'>$</span><span class='va'>alphabet</span><span class='op'>)</span><span class='op'>}</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>dataset_batch</span><span class='op'>(</span><span class='fl'>32</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>Here, then, is the attacker training loop, striving to refine the decoding process over a hundred â short â epochs:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>attacker_criterion</span> <span class='op'>&lt;-</span> <span class='va'>loss_mean_squared_error</span>
<span class='va'>attacker_optimizer</span> <span class='op'>&lt;-</span> <span class='fu'>optimizer_adam</span><span class='op'>(</span><span class='op'>)</span>
<span class='va'>attacker_loss</span> <span class='op'>&lt;-</span> <span class='va'>tf</span><span class='op'>$</span><span class='va'>keras</span><span class='op'>$</span><span class='va'>metrics</span><span class='op'>$</span><span class='fu'>Mean</span><span class='op'>(</span>name<span class='op'>=</span><span class='st'>'attacker_loss'</span><span class='op'>)</span>
<span class='va'>attacker_mse</span> <span class='op'>&lt;-</span>  <span class='va'>tf</span><span class='op'>$</span><span class='va'>keras</span><span class='op'>$</span><span class='va'>metrics</span><span class='op'>$</span><span class='fu'>MeanSquaredError</span><span class='op'>(</span>name<span class='op'>=</span><span class='st'>'attacker_mse'</span><span class='op'>)</span>

<span class='va'>attacker_step</span> <span class='op'>&lt;-</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>images</span><span class='op'>)</span> <span class='op'>{</span>
  
  <span class='va'>attack_input</span> <span class='op'>&lt;-</span> <span class='va'>model</span><span class='op'>$</span><span class='fu'>mobile_step</span><span class='op'>(</span><span class='va'>images</span><span class='op'>)</span>
  
  <span class='fu'><a href='https://rdrr.io/r/base/with.html'>with</a></span> <span class='op'>(</span><span class='va'>tf</span><span class='op'>$</span><span class='fu'>GradientTape</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%as%</span> <span class='va'>tape</span>, <span class='op'>{</span>
    <span class='va'>generated</span> <span class='op'>&lt;-</span> <span class='fu'>attacker</span><span class='op'>(</span><span class='va'>attack_input</span><span class='op'>)</span>
    <span class='va'>l</span> <span class='op'>&lt;-</span> <span class='fu'>attacker_criterion</span><span class='op'>(</span><span class='va'>images</span>, <span class='va'>generated</span><span class='op'>)</span>
  <span class='op'>}</span><span class='op'>)</span>
  <span class='va'>gradients</span> <span class='op'>&lt;-</span> <span class='va'>tape</span><span class='op'>$</span><span class='fu'>gradient</span><span class='op'>(</span><span class='va'>l</span>, <span class='va'>attacker</span><span class='op'>$</span><span class='va'>trainable_variables</span><span class='op'>)</span>
  <span class='va'>attacker_optimizer</span><span class='op'>$</span><span class='fu'>apply_gradients</span><span class='op'>(</span><span class='fu'>purrr</span><span class='fu'>::</span><span class='fu'><a href='https://purrr.tidyverse.org/reference/transpose.html'>transpose</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span>
    <span class='va'>gradients</span>, <span class='va'>attacker</span><span class='op'>$</span><span class='va'>trainable_variables</span>
  <span class='op'>)</span><span class='op'>)</span><span class='op'>)</span>
  <span class='fu'>attacker_loss</span><span class='op'>(</span><span class='va'>l</span><span class='op'>)</span>
  <span class='fu'>attacker_mse</span><span class='op'>(</span><span class='va'>images</span>, <span class='va'>generated</span><span class='op'>)</span>
<span class='op'>}</span>


<span class='va'>attacker_training_loop</span> <span class='op'>&lt;-</span> <span class='fu'>tf_function</span><span class='op'>(</span><span class='fu'>autograph</span><span class='op'>(</span><span class='kw'>function</span><span class='op'>(</span><span class='va'>attacker_ds</span><span class='op'>)</span> <span class='op'>{</span>
  <span class='kw'>for</span> <span class='op'>(</span><span class='va'>b</span> <span class='kw'>in</span> <span class='va'>attacker_ds</span><span class='op'>)</span> <span class='op'>{</span>
    <span class='fu'>attacker_step</span><span class='op'>(</span><span class='va'>b</span><span class='op'>[[</span><span class='fl'>1</span><span class='op'>]</span><span class='op'>]</span><span class='op'>)</span>
  <span class='op'>}</span>
  
  <span class='va'>tf</span><span class='op'>$</span><span class='fu'>print</span><span class='op'>(</span><span class='st'>"mse: "</span>, <span class='va'>attacker_mse</span><span class='op'>$</span><span class='fu'>result</span><span class='op'>(</span><span class='op'>)</span><span class='op'>)</span>
  
  <span class='va'>attacker_loss</span><span class='op'>$</span><span class='fu'>reset_states</span><span class='op'>(</span><span class='op'>)</span>
  <span class='va'>attacker_mse</span><span class='op'>$</span><span class='fu'>reset_states</span><span class='op'>(</span><span class='op'>)</span>
<span class='op'>}</span><span class='op'>)</span><span class='op'>)</span>

<span class='kw'>for</span> <span class='op'>(</span><span class='va'>epoch</span> <span class='kw'>in</span> <span class='fl'>1</span><span class='op'>:</span><span class='fl'>100</span><span class='op'>)</span> <span class='op'>{</span>
  <span class='fu'><a href='https://rdrr.io/r/base/cat.html'>cat</a></span><span class='op'>(</span><span class='st'>"Epoch: "</span>, <span class='va'>epoch</span>, <span class='st'>" -----------\n"</span><span class='op'>)</span>
  <span class='fu'>attacker_training_loop</span><span class='op'>(</span><span class='va'>attacker_ds</span><span class='op'>)</span>  
<span class='op'>}</span>
</code></pre>
</div>
</div>
<pre><code>Epoch:  1  -----------
  mse:  0.530902684
Epoch:  2  -----------
  mse:  0.201351956
...
...
Epoch:  99  -----------
  mse:  0.0413453057
Epoch:  100  -----------
  mse:  0.0413028933</code></pre>
<p>The question now is, â does it work? Has the attacker really learned to infer actual data from (stage one) model output?</p>
<h3 id="test-adversary">Test adversary</h3>
<p>To test the adversary, we use the third dataset we downloaded, containing images from five yet-unseen alphabets. For display, we select just the first sixteen records â a completely arbitrary decision, of course.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>test_ds</span> <span class='op'>&lt;-</span> <span class='va'>omni_test</span> <span class='op'>%&gt;%</span> 
  <span class='fu'>dataset_map</span><span class='op'>(</span><span class='kw'>function</span><span class='op'>(</span><span class='va'>record</span><span class='op'>)</span> <span class='op'>{</span>
    <span class='va'>record</span><span class='op'>$</span><span class='va'>image</span> <span class='op'>&lt;-</span> <span class='fu'>preprocess_image</span><span class='op'>(</span><span class='va'>record</span><span class='op'>$</span><span class='va'>image</span><span class='op'>)</span>
    <span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span><span class='va'>record</span><span class='op'>$</span><span class='va'>image</span>, <span class='va'>record</span><span class='op'>$</span><span class='va'>alphabet</span><span class='op'>)</span><span class='op'>}</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>dataset_take</span><span class='op'>(</span><span class='fl'>16</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>dataset_batch</span><span class='op'>(</span><span class='fl'>16</span><span class='op'>)</span>

<span class='va'>batch</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/pkg/reticulate/man/iterate.html'>as_iterator</a></span><span class='op'>(</span><span class='va'>test_ds</span><span class='op'>)</span> <span class='op'>%&gt;%</span> <span class='fu'>iterator_get_next</span><span class='op'>(</span><span class='op'>)</span>
<span class='va'>images</span> <span class='op'>&lt;-</span> <span class='va'>batch</span><span class='op'>[[</span><span class='fl'>1</span><span class='op'>]</span><span class='op'>]</span>

<span class='va'>attack_input</span> <span class='op'>&lt;-</span> <span class='va'>model</span><span class='op'>$</span><span class='fu'>mobile_step</span><span class='op'>(</span><span class='va'>images</span><span class='op'>)</span>
<span class='va'>generated</span> <span class='op'>&lt;-</span> <span class='fu'>attacker</span><span class='op'>(</span><span class='va'>attack_input</span><span class='op'>)</span> <span class='op'>%&gt;%</span> <span class='fu'><a href='https://rdrr.io/r/base/array.html'>as.array</a></span><span class='op'>(</span><span class='op'>)</span>

<span class='va'>generated</span><span class='op'>[</span><span class='va'>generated</span> <span class='op'>&gt;</span> <span class='fl'>1</span><span class='op'>]</span> <span class='op'>&lt;-</span> <span class='fl'>1</span>
<span class='va'>generated</span> <span class='op'>&lt;-</span> <span class='va'>generated</span><span class='op'>[</span> , , , <span class='fl'>1</span><span class='op'>]</span>
<span class='va'>generated</span> <span class='op'>%&gt;%</span>
  <span class='fu'>purrr</span><span class='fu'>::</span><span class='fu'><a href='https://purrr.tidyverse.org/reference/array-coercion.html'>array_tree</a></span><span class='op'>(</span><span class='fl'>1</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>purrr</span><span class='fu'>::</span><span class='fu'><a href='https://purrr.tidyverse.org/reference/map.html'>map</a></span><span class='op'>(</span><span class='va'>as.raster</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>purrr</span><span class='fu'>::</span><span class='fu'><a href='https://purrr.tidyverse.org/reference/imap.html'>iwalk</a></span><span class='op'>(</span><span class='op'>~</span><span class='op'>{</span><span class='fu'><a href='https://rdrr.io/r/graphics/plot.default.html'>plot</a></span><span class='op'>(</span><span class='va'>.x</span><span class='op'>)</span><span class='op'>}</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>Just like during the training process, the adversary queries the target model (stage one), obtains the compressed representation, and attempts to reconstruct the original image. (Of course, in the real world, the setup would be different in that the attacker would <em>not</em> be able to simply inspect the images, as is the case here. There would thus have to be some way to intercept, and make sense of, network traffic.)</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>attack_input</span> <span class='op'>&lt;-</span> <span class='va'>model</span><span class='op'>$</span><span class='fu'>mobile_step</span><span class='op'>(</span><span class='va'>images</span><span class='op'>)</span>
<span class='va'>generated</span> <span class='op'>&lt;-</span> <span class='fu'>attacker</span><span class='op'>(</span><span class='va'>attack_input</span><span class='op'>)</span> <span class='op'>%&gt;%</span> <span class='fu'><a href='https://rdrr.io/r/base/array.html'>as.array</a></span><span class='op'>(</span><span class='op'>)</span>

<span class='va'>generated</span><span class='op'>[</span><span class='va'>generated</span> <span class='op'>&gt;</span> <span class='fl'>1</span><span class='op'>]</span> <span class='op'>&lt;-</span> <span class='fl'>1</span>
<span class='va'>generated</span> <span class='op'>&lt;-</span> <span class='va'>generated</span><span class='op'>[</span> , , , <span class='fl'>1</span><span class='op'>]</span>
<span class='va'>generated</span> <span class='op'>%&gt;%</span>
  <span class='fu'>purrr</span><span class='fu'>::</span><span class='fu'><a href='https://purrr.tidyverse.org/reference/array-coercion.html'>array_tree</a></span><span class='op'>(</span><span class='fl'>1</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>purrr</span><span class='fu'>::</span><span class='fu'><a href='https://purrr.tidyverse.org/reference/map.html'>map</a></span><span class='op'>(</span><span class='va'>as.raster</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>purrr</span><span class='fu'>::</span><span class='fu'><a href='https://purrr.tidyverse.org/reference/imap.html'>iwalk</a></span><span class='op'>(</span><span class='op'>~</span><span class='op'>{</span><span class='fu'><a href='https://rdrr.io/r/graphics/plot.default.html'>plot</a></span><span class='op'>(</span><span class='va'>.x</span><span class='op'>)</span><span class='op'>}</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>To allow for easier comparison (and increase suspense â¦!), here again are the actual images, which we displayed already when introducing the dataset:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span id="fig:unnamed-chunk-13"></span>
<img src="images/actual_test_images.png" alt="First images from the test set, the way they really look." width="386" />
<p class="caption">
Figure 4: First images from the test set, the way they really look.
</p>
</div>
</div>
<p>And here is the reconstruction:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span id="fig:unnamed-chunk-14"></span>
<img src="images/recon_noeps_dropout.png" alt="First images from the test set, as reconstructed by the adversary." width="386" />
<p class="caption">
Figure 5: First images from the test set, as reconstructed by the adversary.
</p>
</div>
</div>
<p>Of course, it is hard to say how revealing these âguessesâ are. There definitely seems to be a connection to character complexity; overall, it seems like the Greek and Roman letters, which are the least complex, are also the ones most easily reconstructed. Still, in the end, how much privacy is lost will very much depend on contextual factors.</p>
<p>First and foremost, do the exemplars in the dataset represent <em>individuals</em> or <em>classes</em> of individuals? If â as in reality â the character <code>X</code> represents a class, it might not be so grave if we were able to reconstruct âsome Xâ here: There are many <code>X</code>s in the dataset, all pretty similar to each other; weâre unlikely to exactly to have reconstructed one special, individual <code>X</code>. If, however, this was a dataset of individual people, with all <code>X</code>s being photographs of Alex, then in reconstructing an <code>X</code> we have effectively reconstructed Alex.</p>
<p>Second, in less obvious scenarios, evaluating the degree of privacy breach will likely surpass computation of quantitative metrics, and involve the judgment of domain experts.</p>
<p>Speaking of quantitative metrics though â our example seems like a perfect use case to experiment with <em>differential privacy.</em> Differential privacy is measured by <span class="math inline">\(\epsilon\)</span> (lower is better), the main idea being that answers to queries to a system should depend as little as possible on the presence or absence of a single (<em>any</em> single) datapoint.</p>
<p>So, we will repeat the above experiment, using TensorFlow Privacy (TFP) to add noise, as well as clip gradients, during optimization of the target model. Weâll try three different conditions, resulting in three different values for <span class="math inline">\(\epsilon\)</span>s, and for each condition, inspect the images reconstructed by the adversary.</p>
<h2 id="part-2-differential-privacy-to-the-rescue">Part 2: Differential privacy to the rescue</h2>
<p>Unfortunately, the setup for this part of the experiment requires a little workaround. Making use of the flexibility afforded by TensorFlow 2.x, our target model has been a custom model, joining two distinct stages (âmobileâ and âserverâ) that could be called independently.</p>
<p>TFP, however, does still not work with TensorFlow 2.x, meaning we have to use old-style, non-eager model definitions and training. Luckily, the workaround will be easy.</p>
<p>First, load (and possibly, install) libraries, taking care to disable TensorFlow V2 behavior.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'>keras</span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'>tensorflow</span><span class='op'>)</span>
<span class='co'># still necessary when working with TensorFlow Privacy, as of this writing</span>
<span class='va'>tf</span><span class='op'>$</span><span class='va'>compat</span><span class='op'>$</span><span class='va'>v1</span><span class='op'>$</span><span class='fu'>disable_v2_behavior</span><span class='op'>(</span><span class='op'>)</span>

<span class='co'># if you don't have it installed:</span>
<span class='co'># reticulate::py_install("tensorflow_privacy")</span>
<span class='va'>tfp</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/pkg/reticulate/man/import.html'>import</a></span><span class='op'>(</span><span class='st'>"tensorflow_privacy"</span><span class='op'>)</span>

<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'>tfdatasets</span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'>tfds</span><span class='op'>)</span>

<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='http://purrr.tidyverse.org'>purrr</a></span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>The training set is loaded, preprocessed and batched (nearly) as before.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>omni_train</span> <span class='op'>&lt;-</span> <span class='va'>tfds</span><span class='op'>$</span><span class='fu'>load</span><span class='op'>(</span><span class='st'>"omniglot"</span>, split <span class='op'>=</span> <span class='st'>"test"</span><span class='op'>)</span>

<span class='va'>batch_size</span> <span class='op'>&lt;-</span> <span class='fl'>32</span>

<span class='va'>train_ds</span> <span class='op'>&lt;-</span> <span class='va'>omni_train</span> <span class='op'>%&gt;%</span>
  <span class='fu'>dataset_take</span><span class='op'>(</span><span class='fl'>11000</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>dataset_map</span><span class='op'>(</span><span class='kw'>function</span><span class='op'>(</span><span class='va'>record</span><span class='op'>)</span> <span class='op'>{</span>
    <span class='va'>record</span><span class='op'>$</span><span class='va'>image</span> <span class='op'>&lt;-</span> <span class='fu'>preprocess_image</span><span class='op'>(</span><span class='va'>record</span><span class='op'>$</span><span class='va'>image</span><span class='op'>)</span>
    <span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span><span class='va'>record</span><span class='op'>$</span><span class='va'>image</span>, <span class='va'>record</span><span class='op'>$</span><span class='va'>alphabet</span><span class='op'>)</span><span class='op'>}</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>dataset_shuffle</span><span class='op'>(</span><span class='fl'>1000</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='co'># need dataset_repeat() when not eager</span>
  <span class='fu'>dataset_repeat</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>dataset_batch</span><span class='op'>(</span><span class='va'>batch_size</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<h3 id="train-target-model-with-tensorflow-privacy">Train target model â with TensorFlow Privacy</h3>
<p>To train the target, we put the layers from both stages â âmobileâ and âserverâ â into one sequential model. Note how we remove the dropout. This is because noise will be added during optimization anyway.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>complete_model</span> <span class='op'>&lt;-</span> <span class='fu'>keras_model_sequential</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_conv_2d</span><span class='op'>(</span>filters <span class='op'>=</span> <span class='fl'>32</span>, kernel_size <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>7</span>, <span class='fl'>7</span><span class='op'>)</span>,
                input_shape <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>105</span>, <span class='fl'>105</span>, <span class='fl'>1</span><span class='op'>)</span>,
                activation <span class='op'>=</span> <span class='st'>"relu"</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_batch_normalization</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_max_pooling_2d</span><span class='op'>(</span>pool_size <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>3</span>, <span class='fl'>3</span><span class='op'>)</span>, strides <span class='op'>=</span> <span class='fl'>3</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='co'>#layer_dropout(0.2) %&gt;%</span>
  <span class='fu'>layer_conv_2d</span><span class='op'>(</span>filters <span class='op'>=</span> <span class='fl'>32</span>, kernel_size <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>7</span>, <span class='fl'>7</span><span class='op'>)</span>, activation <span class='op'>=</span> <span class='st'>"relu"</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_batch_normalization</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_max_pooling_2d</span><span class='op'>(</span>pool_size <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>3</span>, <span class='fl'>3</span><span class='op'>)</span>, strides <span class='op'>=</span> <span class='fl'>2</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='co'>#layer_dropout(0.2) %&gt;%</span>
  <span class='fu'>layer_conv_2d</span><span class='op'>(</span>filters <span class='op'>=</span> <span class='fl'>32</span>, kernel_size <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>5</span>, <span class='fl'>5</span><span class='op'>)</span>, activation <span class='op'>=</span> <span class='st'>"relu"</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_batch_normalization</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_max_pooling_2d</span><span class='op'>(</span>pool_size <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>2</span>, <span class='fl'>2</span><span class='op'>)</span>, strides <span class='op'>=</span> <span class='fl'>2</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='co'>#layer_dropout(0.2) %&gt;%</span>
  <span class='fu'>layer_conv_2d</span><span class='op'>(</span>filters <span class='op'>=</span> <span class='fl'>32</span>, kernel_size <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>3</span>, <span class='fl'>3</span><span class='op'>)</span>, activation <span class='op'>=</span> <span class='st'>"relu"</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_batch_normalization</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_max_pooling_2d</span><span class='op'>(</span>pool_size <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>2</span>, <span class='fl'>2</span><span class='op'>)</span>, strides <span class='op'>=</span> <span class='fl'>2</span>, name <span class='op'>=</span> <span class='st'>"mobile_output"</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='co'>#layer_dropout(0.2) %&gt;%</span>
  <span class='fu'>layer_dense</span><span class='op'>(</span>units <span class='op'>=</span> <span class='fl'>256</span>, activation <span class='op'>=</span> <span class='st'>"relu"</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='fu'>layer_flatten</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span>
  <span class='co'>#layer_dropout(0.2) %&gt;%</span>
  <span class='fu'>layer_dense</span><span class='op'>(</span>units <span class='op'>=</span> <span class='fl'>50</span>, activation <span class='op'>=</span> <span class='st'>"softmax"</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>Using TFP mainly means using a TFP optimizer, one that clips gradients according to some defined magnitude and adds noise of defined size. <code>noise_multiplier</code> is the parameter we are going to vary to arrive at different <span class="math inline">\(\epsilon\)</span>s:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>l2_norm_clip</span> <span class='op'>&lt;-</span> <span class='fl'>1</span>

<span class='co'># ratio of the standard deviation to the clipping norm</span>
<span class='co'># we run training for each of the three values</span>
<span class='va'>noise_multiplier</span> <span class='op'>&lt;-</span> <span class='fl'>0.7</span>
<span class='va'>noise_multiplier</span> <span class='op'>&lt;-</span> <span class='fl'>0.5</span>
<span class='va'>noise_multiplier</span> <span class='op'>&lt;-</span> <span class='fl'>0.3</span>

<span class='co'># same as batch size</span>
<span class='va'>num_microbatches</span> <span class='op'>&lt;-</span> <span class='fu'>k_cast</span><span class='op'>(</span><span class='va'>batch_size</span>, <span class='st'>"int32"</span><span class='op'>)</span>
<span class='va'>learning_rate</span> <span class='op'>&lt;-</span> <span class='fl'>0.005</span>

<span class='va'>optimizer</span> <span class='op'>&lt;-</span> <span class='va'>tfp</span><span class='op'>$</span><span class='fu'>DPAdamGaussianOptimizer</span><span class='op'>(</span>
  l2_norm_clip <span class='op'>=</span> <span class='va'>l2_norm_clip</span>,
  noise_multiplier <span class='op'>=</span> <span class='va'>noise_multiplier</span>,
  num_microbatches <span class='op'>=</span> <span class='va'>num_microbatches</span>,
  learning_rate <span class='op'>=</span> <span class='va'>learning_rate</span>
<span class='op'>)</span>
</code></pre>
</div>
</div>
<p>In training the model, the second important change for TFP we need to make is to have loss and gradients computed on the individual level.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='co'># need to add noise to every individual contribution</span>
<span class='va'>loss</span> <span class='op'>&lt;-</span> <span class='va'>tf</span><span class='op'>$</span><span class='va'>keras</span><span class='op'>$</span><span class='va'>losses</span><span class='op'>$</span><span class='fu'>SparseCategoricalCrossentropy</span><span class='op'>(</span>reduction <span class='op'>=</span>   <span class='va'>tf</span><span class='op'>$</span><span class='va'>keras</span><span class='op'>$</span><span class='va'>losses</span><span class='op'>$</span><span class='va'>Reduction</span><span class='op'>$</span><span class='va'>NONE</span><span class='op'>)</span>

<span class='va'>complete_model</span> <span class='op'>%&gt;%</span> <span class='fu'>compile</span><span class='op'>(</span>loss <span class='op'>=</span> <span class='va'>loss</span>, optimizer <span class='op'>=</span> <span class='va'>optimizer</span>, metrics <span class='op'>=</span> <span class='st'>"sparse_categorical_accuracy"</span><span class='op'>)</span>

<span class='va'>num_epochs</span> <span class='op'>&lt;-</span> <span class='fl'>20</span>

<span class='va'>n_train</span> <span class='op'>&lt;-</span> <span class='fl'>13180</span>

<span class='va'>history</span> <span class='op'>&lt;-</span> <span class='va'>complete_model</span> <span class='op'>%&gt;%</span> <span class='fu'>fit</span><span class='op'>(</span>
  <span class='va'>train_ds</span>,
  <span class='co'># need steps_per_epoch when not in eager mode</span>
  steps_per_epoch <span class='op'>=</span> <span class='va'>n_train</span><span class='op'>/</span><span class='va'>batch_size</span>,
  epochs <span class='op'>=</span> <span class='va'>num_epochs</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>To test three different <span class="math inline">\(\epsilon\)</span>s, we run this thrice, each time with a different <code>noise_multiplier</code>. Each time we arrive at a different final accuracy.</p>
<p>Here is a synopsis, where <span class="math inline">\(\epsilon\)</span> was computed like so:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>compute_priv</span> <span class='op'>&lt;-</span> <span class='va'>tfp</span><span class='op'>$</span><span class='va'>privacy</span><span class='op'>$</span><span class='va'>analysis</span><span class='op'>$</span><span class='va'>compute_dp_sgd_privacy</span>

<span class='va'>compute_priv</span><span class='op'>$</span><span class='fu'>compute_dp_sgd_privacy</span><span class='op'>(</span>
  <span class='co'># number of records in training set</span>
  <span class='va'>n_train</span>,
  <span class='va'>batch_size</span>,
  <span class='co'># noise_multiplier</span>
  <span class='fl'>0.7</span>, <span class='co'># or 0.5, or 0.3</span>
  <span class='co'># number of epochs</span>
  <span class='fl'>20</span>,
  <span class='co'># delta - should not exceed 1/number of examples in training set</span>
  <span class='fl'>1e-5</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<table>
<thead>
<tr class="header">
<th>noise multiplier</th>
<th>epsilon</th>
<th>final acc. (training set)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.7</td>
<td>4.0</td>
<td>0.37</td>
</tr>
<tr class="even">
<td>0.5</td>
<td>12.5</td>
<td>0.45</td>
</tr>
<tr class="odd">
<td>0.3</td>
<td>84.7</td>
<td>0.56</td>
</tr>
</tbody>
</table>
<p>Now, as the adversary wonât call the complete model, we need to âcut offâ the second-stage layers. This leaves us with a model that executes stage-one logic only. We save its weights, so we can later call it from the adversary:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>intercepted</span> <span class='op'>&lt;-</span> <span class='fu'>keras_model</span><span class='op'>(</span>
  <span class='va'>complete_model</span><span class='op'>$</span><span class='va'>input</span>,
  <span class='va'>complete_model</span><span class='op'>$</span><span class='fu'>get_layer</span><span class='op'>(</span><span class='st'>"mobile_output"</span><span class='op'>)</span><span class='op'>$</span><span class='va'>output</span>
<span class='op'>)</span>

<span class='va'>intercepted</span> <span class='op'>%&gt;%</span> <span class='fu'>save_model_hdf5</span><span class='op'>(</span><span class='st'>"./intercepted.hdf5"</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<h3 id="train-adversary-against-differentially-private-target">Train adversary (against differentially private target)</h3>
<p>In training the adversary, we can keep most of the original code â meaning, weâre back to TF-2 style. Even the definition of the target model is the same as before:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a>on_device_model &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>() <span class="op">%&gt;%</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a><span class="st">  </span>[...]</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true"></a>server_model &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>() <span class="op">%&gt;%</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true"></a><span class="st">  </span>[...]</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true"></a>target_model &lt;-<span class="st"> </span><span class="cf">function</span>() {</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true"></a>  <span class="kw">keras_model_custom</span>(<span class="dt">name =</span> <span class="st">&quot;TargetModel&quot;</span>, <span class="cf">function</span>(self) {</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true"></a>    </span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true"></a>    self<span class="op">$</span>on_device_model &lt;-on_device_model</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true"></a>    self<span class="op">$</span>server_model &lt;-<span class="st"> </span>server_model</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true"></a>    self<span class="op">$</span>mobile_step &lt;-<span class="st"> </span><span class="cf">function</span>(inputs) </span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true"></a>      self<span class="op">$</span><span class="kw">on_device_model</span>(inputs)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true"></a>    self<span class="op">$</span>server_step &lt;-<span class="st"> </span><span class="cf">function</span>(inputs)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true"></a>      self<span class="op">$</span><span class="kw">server_model</span>(inputs)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true"></a>    </span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true"></a>    <span class="cf">function</span>(inputs, <span class="dt">mask =</span> <span class="ot">NULL</span>) {</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true"></a>      inputs <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true"></a><span class="st">        </span>self<span class="op">$</span><span class="kw">mobile_step</span>() <span class="op">%&gt;%</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true"></a><span class="st">        </span>self<span class="op">$</span><span class="kw">server_step</span>()</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true"></a>    }</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true"></a>  })</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true"></a>}</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true"></a></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true"></a>intercepted &lt;-<span class="st"> </span><span class="kw">target_model</span>()</span></code></pre></div>
</div>
<p>But now, we load the trained targetâs weights into the freshly defined modelâs âmobile stageâ:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>intercepted</span><span class='op'>$</span><span class='va'>on_device_model</span><span class='op'>$</span><span class='fu'>load_weights</span><span class='op'>(</span><span class='st'>"intercepted.hdf5"</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>And now, weâre back to the old training routine. Testing setup is the same as before, as well.</p>
<p>So how well does the adversary perform with differential privacy added to the picture?</p>
<h3 id="test-adversary-against-differentially-private-target">Test adversary (against differentially private target)</h3>
<p>Here, ordered by decreasing <span class="math inline">\(\epsilon\)</span>, are the reconstructions. Again, we refrain from judging the results, for the same reasons as before: In real-world applications, whether privacy is preserved âwell enoughâ will depend on the context.</p>
<p>Here, first, are reconstructions from the run where the least noise was added.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span id="fig:unnamed-chunk-24"></span>
<img src="images/intercepted_eps_84.7.png" alt="Reconstruction attempts from a setup where the target model was trained with an epsilon of 84.7." width="384" />
<p class="caption">
Figure 6: Reconstruction attempts from a setup where the target model was trained with an epsilon of 84.7.
</p>
</div>
</div>
<p>On to the next level of privacy protection:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span id="fig:unnamed-chunk-25"></span>
<img src="images/intercepted_eps_12.5.png" alt="Reconstruction attempts from a setup where the target model was trained with an epsilon of 12.5." width="385" />
<p class="caption">
Figure 7: Reconstruction attempts from a setup where the target model was trained with an epsilon of 12.5.
</p>
</div>
</div>
<p>And the highest-<span class="math inline">\(\epsilon\)</span> one:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span id="fig:unnamed-chunk-26"></span>
<img src="images/intercepted_eps_4.0.png" alt="Reconstruction attempts from a setup where the target model was trained with an epsilon of 4.0." width="386" />
<p class="caption">
Figure 8: Reconstruction attempts from a setup where the target model was trained with an epsilon of 4.0.
</p>
</div>
</div>
<h2 id="conclusion">Conclusion</h2>
<p>Throughout this post, weâve refrained from âover-commentingâ on results, and focused on the why-and-how instead. This is because in an artificial setup, chosen to facilitate exposition of concepts and methods, there really is no objective frame of reference. What is a good reconstruction? What is a good <span class="math inline">\(\epsilon\)</span>? What constitutes a data breach? No-one knows.</p>
<p>In the real world, there is a context to everything â there are people involved, the people whose data weâre talking about. There are organizations, regulations, laws. There are abstract principles, and there are implementations; different implementations of the same âideaâ can differ.</p>
<p>As in machine learning overall, research papers on privacy-, ethics- or otherwise society-related topics are full of LaTeX formulae. Amid the math, letâs not forget the people.</p>
<p>Thanks for reading!</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r distill-force-highlighting-css"><code class="sourceCode r"></code></pre></div>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Dwork2006">
<p>Dwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. âCalibrating Noise to Sensitivity in Private Data Analysis.â In <em>Proceedings of the Third Conference on Theory of Cryptography</em>, 265â84. TCCâ06. Berlin, Heidelberg: Springer-Verlag. <a href="https://doi.org/10.1007/11681878_14">https://doi.org/10.1007/11681878_14</a>.</p>
</div>
<div id="ref-Fred">
<p>Fredrikson, Matthew, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas Ristenpart. 2014. âPrivacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing.â In <em>Proceedings of the 23rd Usenix Conference on Security Symposium</em>, 17â32. SECâ14. USA: USENIX Association.</p>
</div>
<div id="ref-Lake1332">
<p>Lake, Brenden M., Ruslan Salakhutdinov, and Joshua B. Tenenbaum. 2015. âHuman-Level Concept Learning Through Probabilistic Program Induction.â <em>Science</em> 350 (6266): 1332â8. <a href="https://doi.org/10.1126/science.aab3050">https://doi.org/10.1126/science.aab3050</a>.</p>
</div>
<div id="ref-McMahanMRA16">
<p>McMahan, H. Brendan, Eider Moore, Daniel Ramage, and Blaise AgÃ¼era y Arcas. 2016. âFederated Learning of Deep Networks Using Model Averaging.â <em>CoRR</em> abs/1602.05629. <a href="http://arxiv.org/abs/1602.05629">http://arxiv.org/abs/1602.05629</a>.</p>
</div>
<div id="ref-7536387">
<p>Wu, X., M. Fredrikson, S. Jha, and J. F. Naughton. 2016. âA Methodology for Formalizing Model-Inversion Attacks.â In <em>2016 Ieee 29th Computer Security Foundations Symposium (Csf)</em>, 355â70.</p>
</div>
</div>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Donât take <em>all</em> literally please; itâs just a nice phrase.<a href="#fnref1" class="footnote-back" role="doc-backlink">â©ï¸</a></p></li>
</ol>
</section>
<!--radix_placeholder_article_footer-->
<div class="article-footer">
  <p class="social_footer">
    <span class="disqus-comments">
      <i class="fas fa-comments"></i>
      &nbsp;
      <span class="disqus-comment-count" data-disqus-identifier="posts/2020-05-15-model-inversion-attacks/">Comment on this article</span>
    </span>
    <span class="article-sharing">
      Share: &nbsp;
      <a href="https://twitter.com/share?text=Hacking%20deep%20learning%3A%20model%20inversion%20attack%20by%20example&amp;url=https%3A%2F%2Fblogs.rstudio.com%2Ftensorflow%2Fposts%2F2020-05-15-model-inversion-attacks%2F">
        <i class="fab fa-twitter"></i>
      </a>
      <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3A%2F%2Fblogs.rstudio.com%2Ftensorflow%2Fposts%2F2020-05-15-model-inversion-attacks%2F&amp;title=Hacking%20deep%20learning%3A%20model%20inversion%20attack%20by%20example">
        <i class="fab fa-linkedin"></i>
      </a>
    </span>
  </p>
  <script id="dsq-count-scr" src="https://tensorflow-for-r-blog.disqus.com/count.js" async></script>
  <div id="disqus_thread" class="hidden"></div>
  <script type="text/javascript" cookie-consent="functionality">
var disqus_config = function () {
  this.page.url = 'https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks/';
  this.page.identifier = 'posts/2020-05-15-model-inversion-attacks/';
};
(function() {
  var d = document, s = d.createElement('script');
  s.src = 'https://tensorflow-for-r-blog.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
})();
</script>
  <p>
    <div class="subscribe">
<div id="subscribe-caption" style="line-height: 1.2; margin-bottom: 2px;">Enjoy this blog? Get notified of new posts by email:</div>

<script src="https://app-ab02.marketo.com/js/forms2/js/forms2.min.js"></script>
<form class="mtktoBlogEmailForm" id="mktoForm_2224"></form>
<script>

// establish metrics based on where the form is located
var in_sidebar = $('#subscribe-caption').parents('.sidebar-section').length;
if (in_sidebar) {
  var form_width = 190;
  var base_width = form_width - 23;
  var email_width = base_width + 'px';
  var label_width = (base_width - 20) + 'px';
  var button_width = email_width;
  var button_padding = '30px';
  var button_margin = '';
  var font_size = '12px';
} else {
  var form_width = 400;
  var base_width = form_width - 23;
  var email_width = base_width + 'px';
  var label_width = (base_width - 20) + 'px';
  var button_width = email_width;
  var button_padding = '30px';
  var button_margin = '12px';
  var font_size = '15px';
}

$('#subscribe-caption')
  .css('width', base_width)
  .css('font-size', font_size);


MktoForms2.loadForm("https://app-ab02.marketo.com", "709-NXN-706", 2224, function(form) {

  // get jquery reference to form
  form = $(form.getFormElem().get(0));

  $(form).css('width', form_width + 'px');
  $(form).find('.mktoOffset').remove();
  $(form).find('.mktoGutter').remove();
  $(form).find('.mktoEmailField').css('width', email_width);
  $(form).find('.mktoLabel').children().attr('style', 'font-weight: 400');
  $(form).find('.mktoLabel')
      .css('width', label_width)
      .css('font-size', font_size);
  $(form).find('.mktoButtonRow')
      .css('width', button_width)
      .css('text-align', 'center');
  $(form).find('.mktoButtonWrap').attr('style', '');
  $(form).find('.mktoButton')
      .css('margin-top', '10px')
      .css('padding-left', button_padding)
      .css('padding-right', button_padding)
      .css('font-size', font_size)
      .css('margin-top', button_margin);
});
</script>
Posts also available at <a href="https://www.r-bloggers.com">r-bloggers</a>

<script>
  document.addEventListener("DOMContentLoaded", function(){
    var menulink = document.querySelector("a[href='#category:R']");
    if (menulink) menulink.parentNode.style.display = "None";
    
    for (var e of document.querySelectorAll(".dt-tag")) {
      if (e.innerHTML == 'R') e.style.display = "None";
    }
  });
</script>
</div>
  </p>
</div>
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
  <h3 id="references">References</h3>
  <div id="references-listing"></div>
  <h3 id="reuse">Reuse</h3>
  <p>Text and figures are licensed under Creative Commons Attribution <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>. The figures that have been reused from other sources don't fall under this license and can be recognized by a note in their caption: "Figure from ...".</p>
  <h3 id="citation">Citation</h3>
  <p>For attribution, please cite this work as</p>
  <pre class="citation-appendix short">Keydana (2020, May 15). RStudio AI Blog: Hacking deep learning: model inversion attack by example. Retrieved from https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks/</pre>
  <p>BibTeX citation</p>
  <pre class="citation-appendix long">@misc{keydanamodelinversion,
  author = {Keydana, Sigrid},
  title = {RStudio AI Blog: Hacking deep learning: model inversion attack by example},
  url = {https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks/},
  year = {2020}
}</pre>
</div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
