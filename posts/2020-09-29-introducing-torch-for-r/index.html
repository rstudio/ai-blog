<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { color: #00769e; background-color: #f1f3f5; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span { color: #00769e; } /* Normal */
code span.al { color: #ad0000; } /* Alert */
code span.an { color: #5e5e5e; } /* Annotation */
code span.at { color: #657422; } /* Attribute */
code span.bn { color: #ad0000; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #00769e; } /* ControlFlow */
code span.ch { color: #20794d; } /* Char */
code span.cn { color: #8f5902; } /* Constant */
code span.co { color: #5e5e5e; } /* Comment */
code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
code span.dt { color: #ad0000; } /* DataType */
code span.dv { color: #ad0000; } /* DecVal */
code span.er { color: #ad0000; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #ad0000; } /* Float */
code span.fu { color: #4758ab; } /* Function */
code span.im { } /* Import */
code span.in { color: #5e5e5e; } /* Information */
code span.kw { color: #00769e; } /* Keyword */
code span.op { color: #5e5e5e; } /* Operator */
code span.ot { color: #00769e; } /* Other */
code span.pp { color: #ad0000; } /* Preprocessor */
code span.sc { color: #5e5e5e; } /* SpecialChar */
code span.ss { color: #20794d; } /* SpecialString */
code span.st { color: #20794d; } /* String */
code span.va { color: #111111; } /* Variable */
code span.vs { color: #20794d; } /* VerbatimString */
code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
</style>

<style>
  div.csl-bib-body { }
  div.csl-entry {
    clear: both;
    }
  .hanging div.csl-entry {
    margin-left:2em;
    text-indent:-2em;
  }
  div.csl-left-margin {
    min-width:2em;
    float:left;
  }
  div.csl-right-inline {
    margin-left:2em;
    padding-left:1em;
  }
  div.csl-indent {
    margin-left: 2em;
  }
</style>

  <!--radix_placeholder_meta_tags-->
<title>Posit AI Blog: Please allow me to introduce myself: Torch for R</title>

<meta property="description" itemprop="description" content="Today, we are excited to introduce torch, an R package that allows you to use PyTorch-like functionality natively from R. No Python installation is required: torch is built directly on top of libtorch, a C++ library that provides the tensor-computation and automatic-differentiation capabilities essential to building neural networks."/>

<link rel="canonical" href="https://blogs.rstudio.com/tensorflow/posts/2020-09-29-introducing-torch-for-r/"/>
<link rel="license" href="https://creativecommons.org/licenses/by/4.0/"/>
<link rel="icon" type="image/png" href="../../images/favicon.png"/>

<!--  https://schema.org/Article -->
<meta property="article:published" itemprop="datePublished" content="2020-09-29"/>
<meta property="article:created" itemprop="dateCreated" content="2020-09-29"/>
<meta name="article:author" content="Sigrid Keydana"/>

<!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
<meta property="og:title" content="Posit AI Blog: Please allow me to introduce myself: Torch for R"/>
<meta property="og:type" content="article"/>
<meta property="og:description" content="Today, we are excited to introduce torch, an R package that allows you to use PyTorch-like functionality natively from R. No Python installation is required: torch is built directly on top of libtorch, a C++ library that provides the tensor-computation and automatic-differentiation capabilities essential to building neural networks."/>
<meta property="og:url" content="https://blogs.rstudio.com/tensorflow/posts/2020-09-29-introducing-torch-for-r/"/>
<meta property="og:image" content="https://blogs.rstudio.com/tensorflow/posts/2020-09-29-introducing-torch-for-r/images/pt.png"/>
<meta property="og:image:width" content="919"/>
<meta property="og:image:height" content="264"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:site_name" content="Posit AI Blog"/>

<!--  https://dev.twitter.com/cards/types/summary -->
<meta property="twitter:card" content="summary_large_image"/>
<meta property="twitter:title" content="Posit AI Blog: Please allow me to introduce myself: Torch for R"/>
<meta property="twitter:description" content="Today, we are excited to introduce torch, an R package that allows you to use PyTorch-like functionality natively from R. No Python installation is required: torch is built directly on top of libtorch, a C++ library that provides the tensor-computation and automatic-differentiation capabilities essential to building neural networks."/>
<meta property="twitter:url" content="https://blogs.rstudio.com/tensorflow/posts/2020-09-29-introducing-torch-for-r/"/>
<meta property="twitter:image" content="https://blogs.rstudio.com/tensorflow/posts/2020-09-29-introducing-torch-for-r/images/pt.png"/>
<meta property="twitter:image:width" content="919"/>
<meta property="twitter:image:height" content="264"/>

<!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
<meta name="citation_title" content="Posit AI Blog: Please allow me to introduce myself: Torch for R"/>
<meta name="citation_fulltext_html_url" content="https://blogs.rstudio.com/tensorflow/posts/2020-09-29-introducing-torch-for-r/"/>
<meta name="citation_fulltext_world_readable" content=""/>
<meta name="citation_online_date" content="2020/09/29"/>
<meta name="citation_publication_date" content="2020/09/29"/>
<meta name="citation_author" content="Sigrid Keydana"/>
<meta name="citation_author_institution" content="RStudio"/>
<!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=Deep learning for classical japanese literature;citation_author=Tarin Clanuwat;citation_author=Mikel Bober-Irizar;citation_author=Asanobu Kitamoto;citation_author=Alex Lamb;citation_author=Kazuaki Yamamoto;citation_author=David Ha"/>
  <!--radix_placeholder_rmarkdown_metadata-->

<script type="text/json" id="radix-rmarkdown-metadata">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["creative_commons","title","description","author","slug","bibliography","date","categories","output","preview","citation_url","canonical_url"]}},"value":[{"type":"character","attributes":{},"value":["CC BY"]},{"type":"character","attributes":{},"value":["Please allow me to introduce myself: Torch for R"]},{"type":"character","attributes":{},"value":["Today, we are excited to introduce torch, an R package that allows you to use PyTorch-like functionality natively from R. No Python installation is required: torch is built directly on top of libtorch, a C++ library that provides the tensor-computation and automatic-differentiation capabilities essential to building neural networks."]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","affiliation_url"]}},"value":[{"type":"character","attributes":{},"value":["Sigrid Keydana"]},{"type":"character","attributes":{},"value":["RStudio"]},{"type":"character","attributes":{},"value":["https://www.rstudio.com/"]}]}]},{"type":"character","attributes":{},"value":["keydanaintroducingtorchforR"]},{"type":"character","attributes":{},"value":["bibliography.bib"]},{"type":"character","attributes":{},"value":["09-29-2020"]},{"type":"character","attributes":{},"value":["Packages/Releases","Torch","R"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained","toc"]}},"value":[{"type":"logical","attributes":{},"value":[false]},{"type":"logical","attributes":{},"value":[true]}]}]},{"type":"character","attributes":{},"value":["images/pt.png"]},{"type":"character","attributes":{},"value":["https://blogs.rstudio.com/tensorflow/posts/2020-09-29-introducing-torch-for-r/"]},{"type":"character","attributes":{},"value":["https://blogs.rstudio.com/tensorflow/posts/2020-09-29-introducing-torch-for-r/"]}]}
</script>
<!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["bibliography.bib","images/kmnist.png","images/pt.png","introducing_torch_for_R_files/anchor-4.2.2/anchor.min.js","introducing_torch_for_R_files/bowser-1.9.3/bowser.min.js","introducing_torch_for_R_files/distill-2.2.21/template.v2.js","introducing_torch_for_R_files/header-attrs-2.3/header-attrs.js","introducing_torch_for_R_files/jquery-1.11.3/jquery.min.js","introducing_torch_for_R_files/webcomponents-2.0.0/webcomponents.js"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
<meta name="distill:offset" content="../.."/>

<script type="application/javascript">

  window.headroom_prevent_pin = false;

  window.document.addEventListener("DOMContentLoaded", function (event) {

    // initialize headroom for banner
    var header = $('header').get(0);
    var headerHeight = header.offsetHeight;
    var headroom = new Headroom(header, {
      tolerance: 5,
      onPin : function() {
        if (window.headroom_prevent_pin) {
          window.headroom_prevent_pin = false;
          headroom.unpin();
        }
      }
    });
    headroom.init();
    if(window.location.hash)
      headroom.unpin();
    $(header).addClass('headroom--transition');

    // offset scroll location for banner on hash change
    // (see: https://github.com/WickyNilliams/headroom.js/issues/38)
    window.addEventListener("hashchange", function(event) {
      window.scrollTo(0, window.pageYOffset - (headerHeight + 25));
    });

    // responsive menu
    $('.distill-site-header').each(function(i, val) {
      var topnav = $(this);
      var toggle = topnav.find('.nav-toggle');
      toggle.on('click', function() {
        topnav.toggleClass('responsive');
      });
    });

    // nav dropdowns
    $('.nav-dropbtn').click(function(e) {
      $(this).next('.nav-dropdown-content').toggleClass('nav-dropdown-active');
      $(this).parent().siblings('.nav-dropdown')
         .children('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $("body").click(function(e){
      $('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $(".nav-dropdown").click(function(e){
      e.stopPropagation();
    });
  });
</script>

<style type="text/css">

/* Theme (user-documented overrideables for nav appearance) */

.distill-site-nav {
  color: rgba(255, 255, 255, 0.8);
  background-color: #0F2E3D;
  font-size: 15px;
  font-weight: 300;
}

.distill-site-nav a {
  color: inherit;
  text-decoration: none;
}

.distill-site-nav a:hover {
  color: white;
}

@media print {
  .distill-site-nav {
    display: none;
  }
}

.distill-site-header {

}

.distill-site-footer {

}


/* Site Header */

.distill-site-header {
  width: 100%;
  box-sizing: border-box;
  z-index: 3;
}

.distill-site-header .nav-left {
  display: inline-block;
  margin-left: 8px;
}

@media screen and (max-width: 768px) {
  .distill-site-header .nav-left {
    margin-left: 0;
  }
}


.distill-site-header .nav-right {
  float: right;
  margin-right: 8px;
}

.distill-site-header a,
.distill-site-header .title {
  display: inline-block;
  text-align: center;
  padding: 14px 10px 14px 10px;
}

.distill-site-header .title {
  font-size: 18px;
  min-width: 150px;
}

.distill-site-header .logo {
  padding: 0;
}

.distill-site-header .logo img {
  display: none;
  max-height: 20px;
  width: auto;
  margin-bottom: -4px;
}

.distill-site-header .nav-image img {
  max-height: 18px;
  width: auto;
  display: inline-block;
  margin-bottom: -3px;
}



@media screen and (min-width: 1000px) {
  .distill-site-header .logo img {
    display: inline-block;
  }
  .distill-site-header .nav-left {
    margin-left: 20px;
  }
  .distill-site-header .nav-right {
    margin-right: 20px;
  }
  .distill-site-header .title {
    padding-left: 12px;
  }
}


.distill-site-header .nav-toggle {
  display: none;
}

.nav-dropdown {
  display: inline-block;
  position: relative;
}

.nav-dropdown .nav-dropbtn {
  border: none;
  outline: none;
  color: rgba(255, 255, 255, 0.8);
  padding: 16px 10px;
  background-color: transparent;
  font-family: inherit;
  font-size: inherit;
  font-weight: inherit;
  margin: 0;
  margin-top: 1px;
  z-index: 2;
}

.nav-dropdown-content {
  display: none;
  position: absolute;
  background-color: white;
  min-width: 200px;
  border: 1px solid rgba(0,0,0,0.15);
  border-radius: 4px;
  box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.1);
  z-index: 1;
  margin-top: 2px;
  white-space: nowrap;
  padding-top: 4px;
  padding-bottom: 4px;
}

.nav-dropdown-content hr {
  margin-top: 4px;
  margin-bottom: 4px;
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.nav-dropdown-active {
  display: block;
}

.nav-dropdown-content a, .nav-dropdown-content .nav-dropdown-header {
  color: black;
  padding: 6px 24px;
  text-decoration: none;
  display: block;
  text-align: left;
}

.nav-dropdown-content .nav-dropdown-header {
  display: block;
  padding: 5px 24px;
  padding-bottom: 0;
  text-transform: uppercase;
  font-size: 14px;
  color: #999999;
  white-space: nowrap;
}

.nav-dropdown:hover .nav-dropbtn {
  color: white;
}

.nav-dropdown-content a:hover {
  background-color: #ddd;
  color: black;
}

.nav-right .nav-dropdown-content {
  margin-left: -45%;
  right: 0;
}

@media screen and (max-width: 768px) {
  .distill-site-header a, .distill-site-header .nav-dropdown  {display: none;}
  .distill-site-header a.nav-toggle {
    float: right;
    display: block;
  }
  .distill-site-header .title {
    margin-left: 0;
  }
  .distill-site-header .nav-right {
    margin-right: 0;
  }
  .distill-site-header {
    overflow: hidden;
  }
  .nav-right .nav-dropdown-content {
    margin-left: 0;
  }
}


@media screen and (max-width: 768px) {
  .distill-site-header.responsive {position: relative; min-height: 500px; }
  .distill-site-header.responsive a.nav-toggle {
    position: absolute;
    right: 0;
    top: 0;
  }
  .distill-site-header.responsive a,
  .distill-site-header.responsive .nav-dropdown {
    display: block;
    text-align: left;
  }
  .distill-site-header.responsive .nav-left,
  .distill-site-header.responsive .nav-right {
    width: 100%;
  }
  .distill-site-header.responsive .nav-dropdown {float: none;}
  .distill-site-header.responsive .nav-dropdown-content {position: relative;}
  .distill-site-header.responsive .nav-dropdown .nav-dropbtn {
    display: block;
    width: 100%;
    text-align: left;
  }
}

/* Site Footer */

.distill-site-footer {
  width: 100%;
  overflow: hidden;
  box-sizing: border-box;
  z-index: 3;
  margin-top: 30px;
  padding-top: 30px;
  padding-bottom: 30px;
  text-align: center;
}

/* Headroom */

d-title {
  padding-top: 6rem;
}

@media print {
  d-title {
    padding-top: 4rem;
  }
}

.headroom {
  z-index: 1000;
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
}

.headroom--transition {
  transition: all .4s ease-in-out;
}

.headroom--unpinned {
  top: -100px;
}

.headroom--pinned {
  top: 0;
}

/* adjust viewport for navbar height */
/* helps vertically center bootstrap (non-distill) content */
.min-vh-100 {
  min-height: calc(100vh - 100px) !important;
}

</style>

<script src="../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="../../site_libs/font-awesome-6.4.2/css/all.min.css" rel="stylesheet"/>
<link href="../../site_libs/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet"/>
<script src="../../site_libs/headroom-0.9.4/headroom.min.js"></script>
<script src="../../site_libs/autocomplete-0.37.1/autocomplete.min.js"></script>
<script src="../../site_libs/fuse-6.4.1/fuse.min.js"></script>

<script type="application/javascript">

function getMeta(metaName) {
  var metas = document.getElementsByTagName('meta');
  for (let i = 0; i < metas.length; i++) {
    if (metas[i].getAttribute('name') === metaName) {
      return metas[i].getAttribute('content');
    }
  }
  return '';
}

function offsetURL(url) {
  var offset = getMeta('distill:offset');
  return offset ? offset + '/' + url : url;
}

function createFuseIndex() {

  // create fuse index
  var options = {
    keys: [
      { name: 'title', weight: 20 },
      { name: 'categories', weight: 15 },
      { name: 'description', weight: 10 },
      { name: 'contents', weight: 5 },
    ],
    ignoreLocation: true,
    threshold: 0
  };
  var fuse = new window.Fuse([], options);

  // fetch the main search.json
  return fetch(offsetURL('search.json'))
    .then(function(response) {
      if (response.status == 200) {
        return response.json().then(function(json) {
          // index main articles
          json.articles.forEach(function(article) {
            fuse.add(article);
          });
          // download collections and index their articles
          return Promise.all(json.collections.map(function(collection) {
            return fetch(offsetURL(collection)).then(function(response) {
              if (response.status === 200) {
                return response.json().then(function(articles) {
                  articles.forEach(function(article) {
                    fuse.add(article);
                  });
                })
              } else {
                return Promise.reject(
                  new Error('Unexpected status from search index request: ' +
                            response.status)
                );
              }
            });
          })).then(function() {
            return fuse;
          });
        });

      } else {
        return Promise.reject(
          new Error('Unexpected status from search index request: ' +
                      response.status)
        );
      }
    });
}

window.document.addEventListener("DOMContentLoaded", function (event) {

  // get search element (bail if we don't have one)
  var searchEl = window.document.getElementById('distill-search');
  if (!searchEl)
    return;

  createFuseIndex()
    .then(function(fuse) {

      // make search box visible
      searchEl.classList.remove('hidden');

      // initialize autocomplete
      var options = {
        autoselect: true,
        hint: false,
        minLength: 2,
      };
      window.autocomplete(searchEl, options, [{
        source: function(query, callback) {
          const searchOptions = {
            isCaseSensitive: false,
            shouldSort: true,
            minMatchCharLength: 2,
            limit: 10,
          };
          var results = fuse.search(query, searchOptions);
          callback(results
            .map(function(result) { return result.item; })
          );
        },
        templates: {
          suggestion: function(suggestion) {
            var img = suggestion.preview && Object.keys(suggestion.preview).length > 0
              ? `<img src="${offsetURL(suggestion.preview)}"</img>`
              : '';
            var html = `
              <div class="search-item">
                <h3>${suggestion.title}</h3>
                <div class="search-item-description">
                  ${suggestion.description || ''}
                </div>
                <div class="search-item-preview">
                  ${img}
                </div>
              </div>
            `;
            return html;
          }
        }
      }]).on('autocomplete:selected', function(event, suggestion) {
        window.location.href = offsetURL(suggestion.path);
      });
      // remove inline display style on autocompleter (we want to
      // manage responsive display via css)
      $('.algolia-autocomplete').css("display", "");
    })
    .catch(function(error) {
      console.log(error);
    });

});

</script>

<style type="text/css">

.nav-search {
  font-size: x-small;
}

/* Algolioa Autocomplete */

.algolia-autocomplete {
  display: inline-block;
  margin-left: 10px;
  vertical-align: sub;
  background-color: white;
  color: black;
  padding: 6px;
  padding-top: 8px;
  padding-bottom: 0;
  border-radius: 6px;
  border: 1px #0F2E3D solid;
  width: 180px;
}


@media screen and (max-width: 768px) {
  .distill-site-nav .algolia-autocomplete {
    display: none;
    visibility: hidden;
  }
  .distill-site-nav.responsive .algolia-autocomplete {
    display: inline-block;
    visibility: visible;
  }
  .distill-site-nav.responsive .algolia-autocomplete .aa-dropdown-menu {
    margin-left: 0;
    width: 400px;
    max-height: 400px;
  }
}

.algolia-autocomplete .aa-input, .algolia-autocomplete .aa-hint {
  width: 90%;
  outline: none;
  border: none;
}

.algolia-autocomplete .aa-hint {
  color: #999;
}
.algolia-autocomplete .aa-dropdown-menu {
  width: 550px;
  max-height: 70vh;
  overflow-x: visible;
  overflow-y: scroll;
  padding: 5px;
  margin-top: 3px;
  margin-left: -150px;
  background-color: #fff;
  border-radius: 5px;
  border: 1px solid #999;
  border-top: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion {
  cursor: pointer;
  padding: 5px 4px;
  border-bottom: 1px solid #eee;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion:last-of-type {
  border-bottom: none;
  margin-bottom: 2px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item {
  overflow: hidden;
  font-size: 0.8em;
  line-height: 1.4em;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item h3 {
  font-size: 1rem;
  margin-block-start: 0;
  margin-block-end: 5px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-description {
  display: inline-block;
  overflow: hidden;
  height: 2.8em;
  width: 80%;
  margin-right: 4%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview {
  display: inline-block;
  width: 15%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img {
  height: 3em;
  width: auto;
  display: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img[src] {
  display: initial;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion.aa-cursor {
  background-color: #eee;
}
.algolia-autocomplete .aa-dropdown-menu .aa-suggestion em {
  font-weight: bold;
  font-style: normal;
}

</style>


<!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

<style type="text/css">

body {
  background-color: white;
}

.pandoc-table {
  width: 100%;
}

.pandoc-table>caption {
  margin-bottom: 10px;
}

.pandoc-table th:not([align]) {
  text-align: left;
}

.pagedtable-footer {
  font-size: 15px;
}

d-byline .byline {
  grid-template-columns: 2fr 2fr;
}

d-byline .byline h3 {
  margin-block-start: 1.5em;
}

d-byline .byline .authors-affiliations h3 {
  margin-block-start: 0.5em;
}

.authors-affiliations .orcid-id {
  width: 16px;
  height:16px;
  margin-left: 4px;
  margin-right: 4px;
  vertical-align: middle;
  padding-bottom: 2px;
}

d-title .dt-tags {
  margin-top: 1em;
  grid-column: text;
}

.dt-tags .dt-tag {
  text-decoration: none;
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0em 0.4em;
  margin-right: 0.5em;
  margin-bottom: 0.4em;
  font-size: 70%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

d-article table.gt_table td,
d-article table.gt_table th {
  border-bottom: none;
  font-size: 100%;
}

.html-widget {
  margin-bottom: 2.0em;
}

.l-screen-inset {
  padding-right: 16px;
}

.l-screen .caption {
  margin-left: 10px;
}

.shaded {
  background: rgb(247, 247, 247);
  padding-top: 20px;
  padding-bottom: 20px;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .html-widget {
  margin-bottom: 0;
  border: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .shaded-content {
  background: white;
}

.text-output {
  margin-top: 0;
  line-height: 1.5em;
}

.hidden {
  display: none !important;
}

hr.section-separator {
  border: none;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  margin: 0px;
}


d-byline {
  border-top: none;
}

d-article {
  padding-top: 2.5rem;
  padding-bottom: 30px;
  border-top: none;
}

d-appendix {
  padding-top: 30px;
}

d-article>p>img {
  width: 100%;
}

d-article h2 {
  margin: 1rem 0 1.5rem 0;
}

d-article h3 {
  margin-top: 1.5rem;
}

d-article iframe {
  border: 1px solid rgba(0, 0, 0, 0.1);
  margin-bottom: 2.0em;
  width: 100%;
}

/* Tweak code blocks */

d-article div.sourceCode code,
d-article pre code {
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
}

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: auto;
}

d-article div.sourceCode {
  background-color: white;
}

d-article div.sourceCode pre {
  padding-left: 10px;
  font-size: 12px;
  border-left: 2px solid rgba(0,0,0,0.1);
}

d-article pre {
  font-size: 12px;
  color: black;
  background: none;
  margin-top: 0;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

d-article pre a {
  border-bottom: none;
}

d-article pre a:hover {
  border-bottom: none;
  text-decoration: underline;
}

d-article details {
  grid-column: text;
  margin-bottom: 0.8em;
}

@media(min-width: 768px) {

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: visible !important;
}

d-article div.sourceCode pre {
  padding-left: 18px;
  font-size: 14px;
}

/* tweak for Pandoc numbered line within distill */
d-article pre.numberSource code > span {
    left: -2em;
}

d-article pre {
  font-size: 14px;
}

}

figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

/* CSS for d-contents */

.d-contents {
  grid-column: text;
  color: rgba(0,0,0,0.8);
  font-size: 0.9em;
  padding-bottom: 1em;
  margin-bottom: 1em;
  padding-bottom: 0.5em;
  margin-bottom: 1em;
  padding-left: 0.25em;
  justify-self: start;
}

@media(min-width: 1000px) {
  .d-contents.d-contents-float {
    height: 0;
    grid-column-start: 1;
    grid-column-end: 4;
    justify-self: center;
    padding-right: 3em;
    padding-left: 2em;
  }
}

.d-contents nav h3 {
  font-size: 18px;
  margin-top: 0;
  margin-bottom: 1em;
}

.d-contents li {
  list-style-type: none
}

.d-contents nav > ul {
  padding-left: 0;
}

.d-contents ul {
  padding-left: 1em
}

.d-contents nav ul li {
  margin-top: 0.6em;
  margin-bottom: 0.2em;
}

.d-contents nav a {
  font-size: 13px;
  border-bottom: none;
  text-decoration: none
  color: rgba(0, 0, 0, 0.8);
}

.d-contents nav a:hover {
  text-decoration: underline solid rgba(0, 0, 0, 0.6)
}

.d-contents nav > ul > li > a {
  font-weight: 600;
}

.d-contents nav > ul > li > ul {
  font-weight: inherit;
}

.d-contents nav > ul > li > ul > li {
  margin-top: 0.2em;
}


.d-contents nav ul {
  margin-top: 0;
  margin-bottom: 0.25em;
}

.d-article-with-toc h2:nth-child(2) {
  margin-top: 0;
}


/* Figure */

.figure {
  position: relative;
  margin-bottom: 2.5em;
  margin-top: 1.5em;
}

.figure .caption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

.figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

.figure .caption a {
  color: rgba(0, 0, 0, 0.6);
}

.figure .caption b,
.figure .caption strong, {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

/* Citations */

d-article .citation {
  color: inherit;
  cursor: inherit;
}

div.hanging-indent{
  margin-left: 1em; text-indent: -1em;
}

/* Citation hover box */

.tippy-box[data-theme~=light-border] {
  background-color: rgba(250, 250, 250, 0.95);
}

.tippy-content > p {
  margin-bottom: 0;
  padding: 2px;
}


/* Tweak 1000px media break to show more text */

@media(min-width: 1000px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }

  d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
  }
  figure .caption, .figure .caption, figure figcaption {
    font-size: 13px;
  }
}

@media(min-width: 1180px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 32px;
  }

  .grid {
    grid-column-gap: 32px;
  }
}


/* Get the citation styles for the appendix (not auto-injected on render since
   we do our own rendering of the citation appendix) */

d-appendix .citation-appendix,
.d-appendix .citation-appendix {
  font-size: 11px;
  line-height: 15px;
  border-left: 1px solid rgba(0, 0, 0, 0.1);
  padding-left: 18px;
  border: 1px solid rgba(0,0,0,0.1);
  background: rgba(0, 0, 0, 0.02);
  padding: 10px 18px;
  border-radius: 3px;
  color: rgba(150, 150, 150, 1);
  overflow: hidden;
  margin-top: -12px;
  white-space: pre-wrap;
  word-wrap: break-word;
}

/* Include appendix styles here so they can be overridden */

d-appendix {
  contain: layout style;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-top: 60px;
  margin-bottom: 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  color: rgba(0,0,0,0.5);
  padding-top: 60px;
  padding-bottom: 48px;
}

d-appendix h3 {
  grid-column: page-start / text-start;
  font-size: 15px;
  font-weight: 500;
  margin-top: 1em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.65);
}

d-appendix h3 + * {
  margin-top: 1em;
}

d-appendix ol {
  padding: 0 0 0 15px;
}

@media (min-width: 768px) {
  d-appendix ol {
    padding: 0 0 0 30px;
    margin-left: -30px;
  }
}

d-appendix li {
  margin-bottom: 1em;
}

d-appendix a {
  color: rgba(0, 0, 0, 0.6);
}

d-appendix > * {
  grid-column: text;
}

d-appendix > d-footnote-list,
d-appendix > d-citation-list,
d-appendix > distill-appendix {
  grid-column: screen;
}

/* Include footnote styles here so they can be overridden */

d-footnote-list {
  contain: layout style;
}

d-footnote-list > * {
  grid-column: text;
}

d-footnote-list a.footnote-backlink {
  color: rgba(0,0,0,0.3);
  padding-left: 0.5em;
}



/* Anchor.js */

.anchorjs-link {
  /*transition: all .25s linear; */
  text-decoration: none;
  border-bottom: none;
}
*:hover > .anchorjs-link {
  margin-left: -1.125em !important;
  text-decoration: none;
  border-bottom: none;
}

/* Social footer */

.social_footer {
  margin-top: 30px;
  margin-bottom: 0;
  color: rgba(0,0,0,0.67);
}

.disqus-comments {
  margin-right: 30px;
}

.disqus-comment-count {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  cursor: pointer;
}

#disqus_thread {
  margin-top: 30px;
}

.article-sharing a {
  border-bottom: none;
  margin-right: 8px;
}

.article-sharing a:hover {
  border-bottom: none;
}

.sidebar-section.subscribe {
  font-size: 12px;
  line-height: 1.6em;
}

.subscribe p {
  margin-bottom: 0.5em;
}


.article-footer .subscribe {
  font-size: 15px;
  margin-top: 45px;
}


.sidebar-section.custom {
  font-size: 12px;
  line-height: 1.6em;
}

.custom p {
  margin-bottom: 0.5em;
}

/* Styles for listing layout (hide title) */
.layout-listing d-title, .layout-listing .d-title {
  display: none;
}

/* Styles for posts lists (not auto-injected) */


.posts-with-sidebar {
  padding-left: 45px;
  padding-right: 45px;
}

.posts-list .description h2,
.posts-list .description p {
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
}

.posts-list .description h2 {
  font-weight: 700;
  border-bottom: none;
  padding-bottom: 0;
}

.posts-list h2.post-tag {
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
  padding-bottom: 12px;
}
.posts-list {
  margin-top: 60px;
  margin-bottom: 24px;
}

.posts-list .post-preview {
  text-decoration: none;
  overflow: hidden;
  display: block;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  padding: 24px 0;
}

.post-preview-last {
  border-bottom: none !important;
}

.posts-list .posts-list-caption {
  grid-column: screen;
  font-weight: 400;
}

.posts-list .post-preview h2 {
  margin: 0 0 6px 0;
  line-height: 1.2em;
  font-style: normal;
  font-size: 24px;
}

.posts-list .post-preview p {
  margin: 0 0 12px 0;
  line-height: 1.4em;
  font-size: 16px;
}

.posts-list .post-preview .thumbnail {
  box-sizing: border-box;
  margin-bottom: 24px;
  position: relative;
  max-width: 500px;
}
.posts-list .post-preview img {
  width: 100%;
  display: block;
}

.posts-list .metadata {
  font-size: 12px;
  line-height: 1.4em;
  margin-bottom: 18px;
}

.posts-list .metadata > * {
  display: inline-block;
}

.posts-list .metadata .publishedDate {
  margin-right: 2em;
}

.posts-list .metadata .dt-authors {
  display: block;
  margin-top: 0.3em;
  margin-right: 2em;
}

.posts-list .dt-tags {
  display: block;
  line-height: 1em;
}

.posts-list .dt-tags .dt-tag {
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0.3em 0.4em;
  margin-right: 0.2em;
  margin-bottom: 0.4em;
  font-size: 60%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

.posts-list img {
  opacity: 1;
}

.posts-list img[data-src] {
  opacity: 0;
}

.posts-more {
  clear: both;
}


.posts-sidebar {
  font-size: 16px;
}

.posts-sidebar h3 {
  font-size: 16px;
  margin-top: 0;
  margin-bottom: 0.5em;
  font-weight: 400;
  text-transform: uppercase;
}

.sidebar-section {
  margin-bottom: 30px;
}

.categories ul {
  list-style-type: none;
  margin: 0;
  padding: 0;
}

.categories li {
  color: rgba(0, 0, 0, 0.8);
  margin-bottom: 0;
}

.categories li>a {
  border-bottom: none;
}

.categories li>a:hover {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
}

.categories .active {
  font-weight: 600;
}

.categories .category-count {
  color: rgba(0, 0, 0, 0.4);
}


@media(min-width: 768px) {
  .posts-list .post-preview h2 {
    font-size: 26px;
  }
  .posts-list .post-preview .thumbnail {
    float: right;
    width: 30%;
    margin-bottom: 0;
  }
  .posts-list .post-preview .description {
    float: left;
    width: 45%;
  }
  .posts-list .post-preview .metadata {
    float: left;
    width: 20%;
    margin-top: 8px;
  }
  .posts-list .post-preview p {
    margin: 0 0 12px 0;
    line-height: 1.5em;
    font-size: 16px;
  }
  .posts-with-sidebar .posts-list {
    float: left;
    width: 75%;
  }
  .posts-with-sidebar .posts-sidebar {
    float: right;
    width: 20%;
    margin-top: 60px;
    padding-top: 24px;
    padding-bottom: 24px;
  }
}


/* Improve display for browsers without grid (IE/Edge <= 15) */

.downlevel {
  line-height: 1.6em;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  margin: 0;
}

.downlevel .d-title {
  padding-top: 6rem;
  padding-bottom: 1.5rem;
}

.downlevel .d-title h1 {
  font-size: 50px;
  font-weight: 700;
  line-height: 1.1em;
  margin: 0 0 0.5rem;
}

.downlevel .d-title p {
  font-weight: 300;
  font-size: 1.2rem;
  line-height: 1.55em;
  margin-top: 0;
}

.downlevel .d-byline {
  padding-top: 0.8em;
  padding-bottom: 0.8em;
  font-size: 0.8rem;
  line-height: 1.8em;
}

.downlevel .section-separator {
  border: none;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
}

.downlevel .d-article {
  font-size: 1.06rem;
  line-height: 1.7em;
  padding-top: 1rem;
  padding-bottom: 2rem;
}


.downlevel .d-appendix {
  padding-left: 0;
  padding-right: 0;
  max-width: none;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.5);
  padding-top: 40px;
  padding-bottom: 48px;
}

.downlevel .footnotes ol {
  padding-left: 13px;
}

.downlevel .base-grid,
.downlevel .distill-header,
.downlevel .d-title,
.downlevel .d-abstract,
.downlevel .d-article,
.downlevel .d-appendix,
.downlevel .distill-appendix,
.downlevel .d-byline,
.downlevel .d-footnote-list,
.downlevel .d-citation-list,
.downlevel .distill-footer,
.downlevel .appendix-bottom,
.downlevel .posts-container {
  padding-left: 40px;
  padding-right: 40px;
}

@media(min-width: 768px) {
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
  padding-left: 150px;
  padding-right: 150px;
  max-width: 900px;
}
}

.downlevel pre code {
  display: block;
  border-left: 2px solid rgba(0, 0, 0, .1);
  padding: 0 0 0 20px;
  font-size: 14px;
}

.downlevel code, .downlevel pre {
  color: black;
  background: none;
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

.downlevel .posts-list .post-preview {
  color: inherit;
}



</style>

<script type="application/javascript">

function is_downlevel_browser() {
  if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                 window.navigator.userAgent)) {
    return true;
  } else {
    return window.load_distill_framework === undefined;
  }
}

// show body when load is complete
function on_load_complete() {

  // add anchors
  if (window.anchors) {
    window.anchors.options.placement = 'left';
    window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
  }


  // set body to visible
  document.body.style.visibility = 'visible';

  // force redraw for leaflet widgets
  if (window.HTMLWidgets) {
    var maps = window.HTMLWidgets.findAll(".leaflet");
    $.each(maps, function(i, el) {
      var map = this.getMap();
      map.invalidateSize();
      map.eachLayer(function(layer) {
        if (layer instanceof L.TileLayer)
          layer.redraw();
      });
    });
  }

  // trigger 'shown' so htmlwidgets resize
  $('d-article').trigger('shown');
}

function init_distill() {

  init_common();

  // create front matter
  var front_matter = $('<d-front-matter></d-front-matter>');
  $('#distill-front-matter').wrap(front_matter);

  // create d-title
  $('.d-title').changeElementType('d-title');

  // separator
  var separator = '<hr class="section-separator" style="clear: both"/>';
  // prepend separator above appendix
  $('.d-byline').before(separator);
  $('.d-article').before(separator);

  // create d-byline
  var byline = $('<d-byline></d-byline>');
  $('.d-byline').replaceWith(byline);

  // create d-article
  var article = $('<d-article></d-article>');
  $('.d-article').wrap(article).children().unwrap();

  // move posts container into article
  $('.posts-container').appendTo($('d-article'));

  // create d-appendix
  $('.d-appendix').changeElementType('d-appendix');

  // flag indicating that we have appendix items
  var appendix = $('.appendix-bottom').children('h3').length > 0;

  // replace footnotes with <d-footnote>
  $('.footnote-ref').each(function(i, val) {
    appendix = true;
    var href = $(this).attr('href');
    var id = href.replace('#', '');
    var fn = $('#' + id);
    var fn_p = $('#' + id + '>p');
    fn_p.find('.footnote-back').remove();
    var text = fn_p.html();
    var dtfn = $('<d-footnote></d-footnote>');
    dtfn.html(text);
    $(this).replaceWith(dtfn);
  });
  // remove footnotes
  $('.footnotes').remove();

  // move refs into #references-listing
  $('#references-listing').replaceWith($('#refs'));

  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    var id = $(this).attr('id');
    $('.d-contents a[href="#' + id + '"]').parent().remove();
    appendix = true;
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
  });

  // show d-appendix if we have appendix content
  $("d-appendix").css('display', appendix ? 'grid' : 'none');

  // localize layout chunks to just output
  $('.layout-chunk').each(function(i, val) {

    // capture layout
    var layout = $(this).attr('data-layout');

    // apply layout to markdown level block elements
    var elements = $(this).children().not('details, div.sourceCode, pre, script');
    elements.each(function(i, el) {
      var layout_div = $('<div class="' + layout + '"></div>');
      if (layout_div.hasClass('shaded')) {
        var shaded_content = $('<div class="shaded-content"></div>');
        $(this).wrap(shaded_content);
        $(this).parent().wrap(layout_div);
      } else {
        $(this).wrap(layout_div);
      }
    });


    // unwrap the layout-chunk div
    $(this).children().unwrap();
  });

  // remove code block used to force  highlighting css
  $('.distill-force-highlighting-css').parent().remove();

  // remove empty line numbers inserted by pandoc when using a
  // custom syntax highlighting theme, except when numbering line
  // in code chunk
  $('pre:not(.numberLines) code.sourceCode a:empty').remove();

  // load distill framework
  load_distill_framework();

  // wait for window.distillRunlevel == 4 to do post processing
  function distill_post_process() {

    if (!window.distillRunlevel || window.distillRunlevel < 4)
      return;

    // hide author/affiliations entirely if we have no authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
    if (!have_authors)
      $('d-byline').addClass('hidden');

    // article with toc class
    $('.d-contents').parent().addClass('d-article-with-toc');

    // strip links that point to #
    $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

    // add orcid ids
    $('.authors-affiliations').find('.author').each(function(i, el) {
      var orcid_id = front_matter.authors[i].orcidID;
      var author_name = front_matter.authors[i].author
      if (orcid_id) {
        var a = $('<a></a>');
        a.attr('href', 'https://orcid.org/' + orcid_id);
        var img = $('<img></img>');
        img.addClass('orcid-id');
        img.attr('alt', author_name ? 'ORCID ID for ' + author_name : 'ORCID ID');
        img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
        a.append(img);
        $(this).append(a);
      }
    });

    // hide elements of author/affiliations grid that have no value
    function hide_byline_column(caption) {
      $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
    }

    // affiliations
    var have_affiliations = false;
    for (var i = 0; i<front_matter.authors.length; ++i) {
      var author = front_matter.authors[i];
      if (author.affiliation !== "&nbsp;") {
        have_affiliations = true;
        break;
      }
    }
    if (!have_affiliations)
      $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

    // published date
    if (!front_matter.publishedDate)
      hide_byline_column("Published");

    // document object identifier
    var doi = $('d-byline').find('h3:contains("DOI")');
    var doi_p = doi.next().empty();
    if (!front_matter.doi) {
      // if we have a citation and valid citationText then link to that
      if ($('#citation').length > 0 && front_matter.citationText) {
        doi.html('Citation');
        $('<a href="#citation"></a>')
          .text(front_matter.citationText)
          .appendTo(doi_p);
      } else {
        hide_byline_column("DOI");
      }
    } else {
      $('<a></a>')
         .attr('href', "https://doi.org/" + front_matter.doi)
         .html(front_matter.doi)
         .appendTo(doi_p);
    }

     // change plural form of authors/affiliations
    if (front_matter.authors.length === 1) {
      var grid = $('.authors-affiliations');
      grid.children('h3:contains("Authors")').text('Author');
      grid.children('h3:contains("Affiliations")').text('Affiliation');
    }

    // remove d-appendix and d-footnote-list local styles
    $('d-appendix > style:first-child').remove();
    $('d-footnote-list > style:first-child').remove();

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // hoverable references
    $('span.citation[data-cites]').each(function() {
      const citeChild = $(this).children()[0]
      // Do not process if @xyz has been used without escaping and without bibliography activated
      // https://github.com/rstudio/distill/issues/466
      if (citeChild === undefined) return true

      if (citeChild.nodeName == "D-FOOTNOTE") {
        var fn = citeChild
        $(this).html(fn.shadowRoot.querySelector("sup"))
        $(this).id = fn.id
        fn.remove()
      }
      var refs = $(this).attr('data-cites').split(" ");
      var refHtml = refs.map(function(ref) {
        // Could use CSS.escape too here, we insure backward compatibility in navigator
        return "<p>" + $('div[id="ref-' + ref + '"]').html() + "</p>";
      }).join("\n");
      window.tippy(this, {
        allowHTML: true,
        content: refHtml,
        maxWidth: 500,
        interactive: true,
        interactiveBorder: 10,
        theme: 'light-border',
        placement: 'bottom-start'
      });
    });

    // fix footnotes in tables (#411)
    // replacing broken distill.pub feature
    $('table d-footnote').each(function() {
      // we replace internal showAtNode methode which is triggered when hovering a footnote
      this.hoverBox.showAtNode = function(node) {
        // ported from https://github.com/distillpub/template/pull/105/files
        calcOffset = function(elem) {
            let x = elem.offsetLeft;
            let y = elem.offsetTop;
            // Traverse upwards until an `absolute` element is found or `elem`
            // becomes null.
            while (elem = elem.offsetParent && elem.style.position != 'absolute') {
                x += elem.offsetLeft;
                y += elem.offsetTop;
            }

            return { left: x, top: y };
        }
        // https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/offsetTop
        const bbox = node.getBoundingClientRect();
        const offset = calcOffset(node);
        this.show([offset.left + bbox.width, offset.top + bbox.height]);
      }
    })

    // clear polling timer
    clearInterval(tid);

    // show body now that everything is ready
    on_load_complete();
  }

  var tid = setInterval(distill_post_process, 50);
  distill_post_process();

}

function init_downlevel() {

  init_common();

   // insert hr after d-title
  $('.d-title').after($('<hr class="section-separator"/>'));

  // check if we have authors
  var front_matter = JSON.parse($("#distill-front-matter").html());
  var have_authors = front_matter.authors && front_matter.authors.length > 0;

  // manage byline/border
  if (!have_authors)
    $('.d-byline').remove();
  $('.d-byline').after($('<hr class="section-separator"/>'));
  $('.d-byline a').remove();

  // remove toc
  $('.d-contents').remove();

  // move appendix elements
  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
  });


  // inject headers into references and footnotes
  var refs_header = $('<h3></h3>');
  refs_header.text('References');
  $('#refs').prepend(refs_header);

  var footnotes_header = $('<h3></h3');
  footnotes_header.text('Footnotes');
  $('.footnotes').children('hr').first().replaceWith(footnotes_header);

  // move appendix-bottom entries to the bottom
  $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
  $('.appendix-bottom').remove();

  // remove appendix if it's empty
  if ($('.d-appendix').children().length === 0)
    $('.d-appendix').remove();

  // prepend separator above appendix
  $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

  // trim code
  $('pre>code').each(function(i, val) {
    $(this).html($.trim($(this).html()));
  });

  // move posts-container right before article
  $('.posts-container').insertBefore($('.d-article'));

  $('body').addClass('downlevel');

  on_load_complete();
}


function init_common() {

  // jquery plugin to change element types
  (function($) {
    $.fn.changeElementType = function(newType) {
      var attrs = {};

      $.each(this[0].attributes, function(idx, attr) {
        attrs[attr.nodeName] = attr.nodeValue;
      });

      this.replaceWith(function() {
        return $("<" + newType + "/>", attrs).append($(this).contents());
      });
    };
  })(jQuery);

  // prevent underline for linked images
  $('a > img').parent().css({'border-bottom' : 'none'});

  // mark non-body figures created by knitr chunks as 100% width
  $('.layout-chunk').each(function(i, val) {
    var figures = $(this).find('img, .html-widget');
    // ignore leaflet img layers (#106)
    figures = figures.filter(':not(img[class*="leaflet"])')
    if ($(this).attr('data-layout') !== "l-body") {
      figures.css('width', '100%');
    } else {
      figures.css('max-width', '100%');
      figures.filter("[width]").each(function(i, val) {
        var fig = $(this);
        fig.css('width', fig.attr('width') + 'px');
      });

    }
  });

  // auto-append index.html to post-preview links in file: protocol
  // and in rstudio ide preview
  $('.post-preview').each(function(i, val) {
    if (window.location.protocol === "file:")
      $(this).attr('href', $(this).attr('href') + "index.html");
  });

  // get rid of index.html references in header
  if (window.location.protocol !== "file:") {
    $('.distill-site-header a[href]').each(function(i,val) {
      $(this).attr('href', $(this).attr('href').replace(/^index[.]html/, "./"));
    });
  }

  // add class to pandoc style tables
  $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
  $('.kable-table').children('table').addClass('pandoc-table');

  // add figcaption style to table captions
  $('caption').parent('table').addClass("figcaption");

  // initialize posts list
  if (window.init_posts_list)
    window.init_posts_list();

  // implmement disqus comment link
  $('.disqus-comment-count').click(function() {
    window.headroom_prevent_pin = true;
    $('#disqus_thread').toggleClass('hidden');
    if (!$('#disqus_thread').hasClass('hidden')) {
      var offset = $(this).offset();
      $(window).resize();
      $('html, body').animate({
        scrollTop: offset.top - 35
      });
    }
  });
}

document.addEventListener('DOMContentLoaded', function() {
  if (is_downlevel_browser())
    init_downlevel();
  else
    window.addEventListener('WebComponentsReady', init_distill);
});

</script>

<!--/radix_placeholder_distill-->
  <script src="../../site_libs/header-attrs-2.26/header-attrs.js"></script>
  <script src="../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <script src="../../site_libs/popper-2.6.0/popper.min.js"></script>
  <link href="../../site_libs/tippy-6.2.7/tippy.css" rel="stylesheet" />
  <link href="../../site_libs/tippy-6.2.7/tippy-light-border.css" rel="stylesheet" />
  <script src="../../site_libs/tippy-6.2.7/tippy.umd.min.js"></script>
  <script src="../../site_libs/anchor-4.2.2/anchor.min.js"></script>
  <script src="../../site_libs/bowser-1.9.3/bowser.min.js"></script>
  <script src="../../site_libs/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="../../site_libs/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
<script defer data-domain="blogs.rstudio.com" src="https://plausible.io/js/plausible.js"></script>
<!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Please allow me to introduce myself: Torch for R","description":"Today, we are excited to introduce torch, an R package that allows you to use PyTorch-like functionality natively from R. No Python installation is required: torch is built directly on top of libtorch, a C++ library that provides the tensor-computation and automatic-differentiation capabilities essential to building neural networks.","authors":[{"author":"Sigrid Keydana","authorURL":"#","affiliation":"RStudio","affiliationURL":"https://www.rstudio.com/","orcidID":""}],"publishedDate":"2020-09-29T00:00:00.000+00:00","citationText":"Keydana, 2020"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<header class="header header--fixed" role="banner">
<nav class="distill-site-nav distill-site-header">
<div class="nav-left">
<span class="logo">
<img src="../../images/posit.png"/>
</span>
<a href="../../index.html" class="title">AI Blog</a>
<input id="distill-search" class="nav-search hidden" type="text" placeholder="Search..."/>
</div>
<div class="nav-right">
<a href="../../index.html">Home</a>
<a href="../../gallery.html">Gallery</a>
<a href="../../about.html">About</a>
<a href="../../contributing.html">Contributing</a>
<a href="../../index.xml">
<i class="fa fa-rss" aria-hidden="true"></i>
</a>
<a href="javascript:void(0);" class="nav-toggle">&#9776;</a>
</div>
</nav>
</header>
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Please allow me to introduce myself: Torch for R</h1>

<!--radix_placeholder_categories-->
<div class="dt-tags">
  <a href="../../index.html#category:Packages/Releases" class="dt-tag">Packages/Releases</a>
  <a href="../../index.html#category:Torch" class="dt-tag">Torch</a>
  <a href="../../index.html#category:R" class="dt-tag">R</a>
</div>
<!--/radix_placeholder_categories-->
<p><p>Today, we are excited to introduce torch, an R package that allows you to use PyTorch-like functionality natively from R. No Python installation is required: torch is built directly on top of libtorch, a C++ library that provides the tensor-computation and automatic-differentiation capabilities essential to building neural networks.</p></p>
</div>

<div class="d-byline">
  Sigrid Keydana  (RStudio)<a href="https://www.rstudio.com/" class="uri">https://www.rstudio.com/</a>
  
<br/>09-29-2020
</div>

<div class="d-article">
<div class="d-contents d-contents-float">
<nav class="l-text toc figcaption" id="TOC">
<h3>Contents</h3>
<ul>
<li><a href="#installation">Installation</a></li>
<li><a href="#data-loading-and-pre-processing">Data loading and pre-processing</a></li>
<li><a href="#network">Network</a></li>
<li><a href="#training">Training</a></li>
<li><a href="#evaluation">Evaluation</a></li>
<li><a href="#learn">Learn</a></li>
<li><a href="#we-need-you">We need you</a></li>
</ul>
</nav>
</div>
<p>Last January at <a href="https://rstudio.com/conference/">rstudio::conf</a>, in that distant past when conferences still used to take place at some physical location, my colleague <a href="https://twitter.com/dfalbel">Daniel</a> gave a talk introducing new features and ongoing development in the <code>tensorflow</code> ecosystem. In the Q&amp;A part, he was asked something unexpected: Were we going to build support for <a href="https://pytorch.org/">PyTorch</a>? He hesitated; that was in fact the plan, and he had already played around with natively implementing <code>torch</code> tensors at a prior time, but he was not completely certain how well “it” would work.</p>
<p>“It,” that is an implementation which does not bind to Python Torch, meaning, we don’t install the PyTorch wheel and import it via <code>reticulate</code>. Instead, we delegate to the underlying C++ library <code>libtorch</code> for tensor computations and automatic differentiation, while neural network features – layers, activations, optimizers – are implemented directly in R. Removing the intermediary has at least two benefits: For one, the leaner software stack means fewer possible problems in installation and fewer places to look when troubleshooting. Secondly, through its non-dependence on Python, <code>torch</code> does not require users to install and maintain a suitable Python environment. Depending on operating system and context, this can make an enormous difference: For example, in many organizations employees are not allowed to manipulate privileged software installations on their laptops.</p>
<p>So why did Daniel hesitate, and, if I recall correctly, give a not-too-conclusive answer? On the one hand, it was not clear whether compilation against <code>libtorch</code> would, on some operating systems, pose severe difficulties. (It did, but difficulties turned out to be surmountable.)<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> On the other, the sheer amount of work involved in re-implementing – not all, but a big amount of – PyTorch in R seemed intimidating. Today, there is still lots of work to be done (we’ll pick up that thread at the end), but the main obstacles have been ovecome, and enough components are available that <code>torch</code> can be useful to the R community. Thus, without further ado, let’s train a neural network.</p>
<p>You’re not at your laptop now? Just follow along in the <a href="https://colab.research.google.com/drive/1NdiN9n_a7NEvFpvjPDvxKTshrSWgxZK5?usp=sharing">companion notebook on Colaboratory</a>.</p>
<h2 id="installation">Installation</h2>
<h4 id="torch"><code>torch</code></h4>
<p>Installing <code>torch</code> is as straightforward as typing</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='fu'><a href='https://rdrr.io/r/utils/install.packages.html'>install.packages</a></span><span class='op'>(</span><span class='st'>"torch"</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>This will detect whether you have CUDA installed, and either download the CPU or the GPU version of <code>libtorch</code>. Then, it will install the R package from CRAN. To make use of the very newest features, you can install the development version from GitHub:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='fu'>devtools</span><span class='fu'>::</span><span class='fu'>install_github</span><span class='op'>(</span><span class='st'>"mlverse/torch"</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>To quickly check the installation, and whether GPU support works fine (assuming that there <em>is</em> a CUDA-capable NVidia GPU), create a tensor <em>on the CUDA device</em>:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='fu'>torch_tensor</span><span class='op'>(</span><span class='fl'>1</span>, device <span class='op'>=</span> <span class='st'>"cuda"</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<pre><code>torch_tensor 
 1
[ CUDAFloatType{1} ]</code></pre>
<p>If all our <em>hello torch</em> example did was run a network on, say, simulated data, we could stop here. As we’ll do image classification, however, we need to install another package: <code>torchvision</code>.</p>
<h4 id="torchvision"><code>torchvision</code></h4>
<div class="layout-chunk" data-layout="l-body">

</div>
<p>Whereas <code>torch</code> is where tensors, network modules, and generic data loading functionality live, datatype-specific capabilities are – or will be – provided by dedicated packages. In general, these capabilities comprise three types of things: datasets, tools for pre-processing and data loading, and pre-trained models.</p>
<p>As of this writing, PyTorch has dedicated libraries for three domain areas: vision, text, and audio. In R, we plan to proceed analogously – “plan,” because <code>torchtext</code> and <code>torchaudio</code> are yet to be created. Right now, <code>torchvision</code> is all we need:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='fu'>devtools</span><span class='fu'>::</span><span class='fu'>install_github</span><span class='op'>(</span><span class='st'>"mlverse/torchvision"</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>And we’re ready to load the data.</p>
<h2 id="data-loading-and-pre-processing">Data loading and pre-processing</h2>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://torch.mlverse.org/docs'>torch</a></span><span class='op'>)</span></span>
<span><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://torchvision.mlverse.org'>torchvision</a></span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>The list of vision datasets bundled with PyTorch is long, and they’re continually being added to <code>torchvision</code>.</p>
<p>The one we need right now is available already, and it’s – MNIST? … not quite: It’s my favorite “MNIST dropin,” <a href="https://github.com/rois-codh/kmnist">Kuzushiji-MNIST</a> <span class="citation" data-cites="clanuwat2018deep">(<a href="#ref-clanuwat2018deep" role="doc-biblioref">Clanuwat et al. 2018</a>)</span>. Like other datasets explicitly created to replace MNIST, it has ten classes – characters, in this case, depicted as grayscale images of resolution <code>28x28</code>.</p>
<p>Here are the first 32 characters:</p>
<div class="layout-chunk" data-layout="l-body-outset">
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-7"></span>
<img src="images/kmnist.png" alt="Kuzushiji MNIST." width="768" />
<p class="caption">
Figure 1: Kuzushiji MNIST.
</p>
</div>
</div>
<h4 id="dataset">Dataset</h4>
<p>The following code will download the data separately for training and test sets.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>train_ds</span> <span class='op'>&lt;-</span> <span class='fu'>kmnist_dataset</span><span class='op'>(</span></span>
<span>  <span class='st'>"."</span>,</span>
<span>  download <span class='op'>=</span> <span class='cn'>TRUE</span>,</span>
<span>  train <span class='op'>=</span> <span class='cn'>TRUE</span>,</span>
<span>  transform <span class='op'>=</span> <span class='va'>transform_to_tensor</span></span>
<span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>test_ds</span> <span class='op'>&lt;-</span> <span class='fu'>kmnist_dataset</span><span class='op'>(</span></span>
<span>  <span class='st'>"."</span>,</span>
<span>  download <span class='op'>=</span> <span class='cn'>TRUE</span>,</span>
<span>  train <span class='op'>=</span> <span class='cn'>FALSE</span>,</span>
<span>  transform <span class='op'>=</span> <span class='va'>transform_to_tensor</span></span>
<span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>Note the <code>transform</code> argument. <code>transform_to_tensor</code> takes an image and applies two transformations: First, it normalizes the pixels to the range between 0 and 1. Then, it adds another dimension in front. Why?</p>
<p>Contrary to what you might expect – if until now, you’ve been using <code>keras</code> – the additional dimension is <em>not</em> the batch dimension. Batching will be taken care of by the <code>dataloader</code>, to be introduced next. Instead, this is the <em>channels</em> dimension that in <code>torch</code>, is found <em>before</em> the width and height dimensions by default.</p>
<p>One thing I’ve found to be extremely useful about <code>torch</code> is how easy it is to inspect objects. Even though we’re dealing with a <code>dataset</code>, a custom object, and not an R array or even a <code>torch</code> tensor, we can easily peek at what’s inside. Indexing in <code>torch</code> is 1-based, conforming to the R user’s intuitions. Consequently,</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>train_ds</span><span class='op'>[</span><span class='fl'>1</span><span class='op'>]</span></span></code></pre>
</div>
</div>
<p>gives us the first element in the dataset, an R <em>list</em> of two tensors corresponding to input and target, respectively. (We don’t reproduce the output here, but you can see for yourself in the notebook.)</p>
<p>Let’s inspect the shape of the input tensor:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>train_ds</span><span class='op'>[</span><span class='fl'>1</span><span class='op'>]</span><span class='op'>[[</span><span class='fl'>1</span><span class='op'>]</span><span class='op'>]</span><span class='op'>$</span><span class='fu'>size</span><span class='op'>(</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<pre><code>[1]  1 28 28</code></pre>
<p>Now that we have the data, we need someone to feed them to a deep learning model, nicely batched and all. In <code>torch</code>, this is the task of data loaders.</p>
<h4 id="data-loader">Data loader</h4>
<p>Each of the training and test sets gets their own data loader:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>train_dl</span> <span class='op'>&lt;-</span> <span class='fu'>dataloader</span><span class='op'>(</span><span class='va'>train_ds</span>, batch_size <span class='op'>=</span> <span class='fl'>32</span>, shuffle <span class='op'>=</span> <span class='cn'>TRUE</span><span class='op'>)</span></span>
<span><span class='va'>test_dl</span> <span class='op'>&lt;-</span> <span class='fu'>dataloader</span><span class='op'>(</span><span class='va'>test_ds</span>, batch_size <span class='op'>=</span> <span class='fl'>32</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>Again, <code>torch</code> makes it easy to verify we did the correct thing. To take a look at the content of the first batch, do</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>train_iter</span> <span class='op'>&lt;-</span> <span class='va'>train_dl</span><span class='op'>$</span><span class='fu'>.iter</span><span class='op'>(</span><span class='op'>)</span></span>
<span><span class='va'>train_iter</span><span class='op'>$</span><span class='fu'>.next</span><span class='op'>(</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>Functionality like this may not seem indispensable when working with a well-known dataset, but it will turn out to be very useful when a lot of domain-specific pre-processing is required.</p>
<p>Now that we’ve seen how to load data, all prerequisites are fulfilled for visualizing them. Here is the code that was used to display the first batch of characters, above:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='fu'><a href='https://rdrr.io/r/graphics/par.html'>par</a></span><span class='op'>(</span>mfrow <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>4</span>,<span class='fl'>8</span><span class='op'>)</span>, mar <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/rep.html'>rep</a></span><span class='op'>(</span><span class='fl'>0</span>, <span class='fl'>4</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='va'>images</span> <span class='op'>&lt;-</span> <span class='va'>train_dl</span><span class='op'>$</span><span class='fu'>.iter</span><span class='op'>(</span><span class='op'>)</span><span class='op'>$</span><span class='fu'>.next</span><span class='op'>(</span><span class='op'>)</span><span class='op'>[[</span><span class='fl'>1</span><span class='op'>]</span><span class='op'>]</span><span class='op'>[</span><span class='fl'>1</span><span class='op'>:</span><span class='fl'>32</span>, <span class='fl'>1</span>, , <span class='op'>]</span> </span>
<span><span class='va'>images</span> <span class='op'>%&gt;%</span></span>
<span>  <span class='fu'>purrr</span><span class='fu'>::</span><span class='fu'><a href='https://purrr.tidyverse.org/reference/array-coercion.html'>array_tree</a></span><span class='op'>(</span><span class='fl'>1</span><span class='op'>)</span> <span class='op'>%&gt;%</span></span>
<span>  <span class='fu'>purrr</span><span class='fu'>::</span><span class='fu'><a href='https://purrr.tidyverse.org/reference/map.html'>map</a></span><span class='op'>(</span><span class='va'>as.raster</span><span class='op'>)</span> <span class='op'>%&gt;%</span></span>
<span>  <span class='fu'>purrr</span><span class='fu'>::</span><span class='fu'><a href='https://purrr.tidyverse.org/reference/imap.html'>iwalk</a></span><span class='op'>(</span><span class='op'>~</span><span class='op'>{</span><span class='fu'><a href='https://rdrr.io/r/graphics/plot.default.html'>plot</a></span><span class='op'>(</span><span class='va'>.x</span><span class='op'>)</span><span class='op'>}</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>We’re ready to define our network – a simple convnet.</p>
<h2 id="network">Network</h2>
<p>If you’ve been using <code>keras</code> <em>custom models</em> (or have some experience with <em>Py</em>Torch), the following way of defining a network may not look too surprising.</p>
<p>You use <code>nn_module()</code> to define an R6 class that will hold the network’s components. Its layers are created in <code>initialize()</code>; <code>forward()</code> describes what happens during the network’s forward pass. One thing on terminology: In <code>torch</code>, layers are called <em>modules</em>, as are networks. This makes sense: The design is truly <em>modular</em> in that any module can be used as a component in a larger one.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>net</span> <span class='op'>&lt;-</span> <span class='fu'>nn_module</span><span class='op'>(</span></span>
<span>  </span>
<span>  <span class='st'>"KMNIST-CNN"</span>,</span>
<span>  </span>
<span>  initialize <span class='op'>=</span> <span class='kw'>function</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>{</span></span>
<span>    <span class='co'># in_channels, out_channels, kernel_size, stride = 1, padding = 0</span></span>
<span>    <span class='va'>self</span><span class='op'>$</span><span class='va'>conv1</span> <span class='op'>&lt;-</span> <span class='fu'>nn_conv2d</span><span class='op'>(</span><span class='fl'>1</span>, <span class='fl'>32</span>, <span class='fl'>3</span><span class='op'>)</span></span>
<span>    <span class='va'>self</span><span class='op'>$</span><span class='va'>conv2</span> <span class='op'>&lt;-</span> <span class='fu'>nn_conv2d</span><span class='op'>(</span><span class='fl'>32</span>, <span class='fl'>64</span>, <span class='fl'>3</span><span class='op'>)</span></span>
<span>    <span class='va'>self</span><span class='op'>$</span><span class='va'>dropout1</span> <span class='op'>&lt;-</span> <span class='fu'>nn_dropout2d</span><span class='op'>(</span><span class='fl'>0.25</span><span class='op'>)</span></span>
<span>    <span class='va'>self</span><span class='op'>$</span><span class='va'>dropout2</span> <span class='op'>&lt;-</span> <span class='fu'>nn_dropout2d</span><span class='op'>(</span><span class='fl'>0.5</span><span class='op'>)</span></span>
<span>    <span class='va'>self</span><span class='op'>$</span><span class='va'>fc1</span> <span class='op'>&lt;-</span> <span class='fu'>nn_linear</span><span class='op'>(</span><span class='fl'>9216</span>, <span class='fl'>128</span><span class='op'>)</span></span>
<span>    <span class='va'>self</span><span class='op'>$</span><span class='va'>fc2</span> <span class='op'>&lt;-</span> <span class='fu'>nn_linear</span><span class='op'>(</span><span class='fl'>128</span>, <span class='fl'>10</span><span class='op'>)</span></span>
<span>  <span class='op'>}</span>,</span>
<span>  </span>
<span>  forward <span class='op'>=</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>x</span><span class='op'>)</span> <span class='op'>{</span></span>
<span>    <span class='va'>x</span> <span class='op'>%&gt;%</span> </span>
<span>      <span class='va'>self</span><span class='op'>$</span><span class='fu'>conv1</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span></span>
<span>      <span class='fu'>nnf_relu</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span></span>
<span>      <span class='va'>self</span><span class='op'>$</span><span class='fu'>conv2</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span></span>
<span>      <span class='fu'>nnf_relu</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span></span>
<span>      <span class='fu'>nnf_max_pool2d</span><span class='op'>(</span><span class='fl'>2</span><span class='op'>)</span> <span class='op'>%&gt;%</span></span>
<span>      <span class='va'>self</span><span class='op'>$</span><span class='fu'>dropout1</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span></span>
<span>      <span class='fu'>torch_flatten</span><span class='op'>(</span>start_dim <span class='op'>=</span> <span class='fl'>2</span><span class='op'>)</span> <span class='op'>%&gt;%</span></span>
<span>      <span class='va'>self</span><span class='op'>$</span><span class='fu'>fc1</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span></span>
<span>      <span class='fu'>nnf_relu</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span></span>
<span>      <span class='va'>self</span><span class='op'>$</span><span class='fu'>dropout2</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span></span>
<span>      <span class='va'>self</span><span class='op'>$</span><span class='fu'>fc2</span><span class='op'>(</span><span class='op'>)</span></span>
<span>  <span class='op'>}</span></span>
<span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>The layers – apologies: modules – themselves may look familiar. Unsurprisingly, <code>nn_conv2d()</code> performs two-dimensional convolution; <code>nn_linear()</code> multiplies by a weight matrix and adds a vector of biases. But what are those numbers: <code>nn_linear(128, 10)</code>, say?</p>
<p>In <code>torch</code>, instead of the number of units in a layer, you specify input and output dimensionalities of the “data” that run through it. Thus, <code>nn_linear(128, 10)</code> has 128 input connections and outputs 10 values – one for every class. In some cases, such as this one, specifying dimensions is easy – we know how many input edges there are (namely, the same as the number of output edges from the previous layer), and we know how many output values we need. But how about the previous module? How do we arrive at <code>9216</code> input connections?</p>
<p>Here, a bit of calculation is necessary. We go through all actions that happen in <code>forward()</code> – if they affect shapes, we keep track of the transformation; if they don’t, we ignore them.</p>
<p>So, we start with input tensors of shape <code>batch_size x 1 x 28 x 28</code>. Then,</p>
<ul>
<li><p><code>nn_conv2d(1, 32, 3)</code> , or equivalently, <code>nn_conv2d(in_channels = 1, out_channels = 32, kernel_size = 3),</code>applies a convolution with kernel size 3, stride 1 (the default), and no padding (the default). We can consult the <a href="https://mlverse.github.io/torch/reference/nn_conv2d.html">documentation</a> to look up the resulting output size, or just intuitively reason that with a kernel of size 3 and no padding, the image will shrink by one pixel in each direction, resulting in a spatial resolution of <code>26 x 26</code>. <em>Per channel</em>, that is. Thus, the actual output shape is <code>batch_size x 32 x 26 x 26</code> . Next,</p></li>
<li><p><code>nnf_relu()</code> applies ReLU activation, in no way touching the shape. Next is</p></li>
<li><p><code>nn_conv2d(32, 64, 3)</code>, another convolution with zero padding and kernel size 3. Output size now is <code>batch_size x 64 x 24 x 24</code> . Now, the second</p></li>
<li><p><code>nnf_relu()</code> again does nothing to the output shape, but</p></li>
<li><p><code>nnf_max_pool2d(2)</code> (equivalently: <code>nnf_max_pool2d(kernel_size = 2)</code>) does: It applies max pooling over regions of extension <code>2 x 2</code>, thus downsizing the output to a format of <code>batch_size x 64 x 12 x 12</code> . Now,</p></li>
<li><p><code>nn_dropout2d(0.25)</code> is a no-op, shape-wise, but if we want to apply a linear layer later, we need to merge all of the <em>channels</em>, <em>height</em> and <em>width</em> axes into a single dimension. This is done in</p></li>
<li><p><code>torch_flatten(start_dim = 2)</code>. Output shape is now <code>batch_size * 9216</code> , since <code>64 * 12 * 12 = 9216</code> . Thus here we have the <code>9216</code> input connections fed into the</p></li>
<li><p><code>nn_linear(9216, 128)</code> discussed above. Again,</p></li>
<li><p><code>nnf_relu()</code> and <code>nn_dropout2d(0.5)</code> leave dimensions as they are, and finally,</p></li>
<li><p><code>nn_linear(128, 10)</code> gives us the desired output scores, one for each of the ten classes.</p></li>
</ul>
<p>Now you’ll be thinking, – what if my network is more complicated? Calculations could become pretty cumbersome. Luckily, with <code>torch</code>’s flexibility, there is another way. Since every layer is callable <em>in isolation</em>, we can just … create some sample data and see what happens!</p>
<p>Here is a sample “image” – or more precisely, a one-item batch containing it:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>x</span> <span class='op'>&lt;-</span> <span class='fu'>torch_randn</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>1</span>, <span class='fl'>1</span>, <span class='fl'>28</span>, <span class='fl'>28</span><span class='op'>)</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>What if we call the first <em>conv2d</em> module on it?</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>conv1</span> <span class='op'>&lt;-</span> <span class='fu'>nn_conv2d</span><span class='op'>(</span><span class='fl'>1</span>, <span class='fl'>32</span>, <span class='fl'>3</span><span class='op'>)</span></span>
<span><span class='fu'>conv1</span><span class='op'>(</span><span class='va'>x</span><span class='op'>)</span><span class='op'>$</span><span class='fu'>size</span><span class='op'>(</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<pre><code>[1]  1 32 26 26</code></pre>
<p>Or both <em>conv2d</em> modules?</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>conv2</span> <span class='op'>&lt;-</span> <span class='fu'>nn_conv2d</span><span class='op'>(</span><span class='fl'>32</span>, <span class='fl'>64</span>, <span class='fl'>3</span><span class='op'>)</span></span>
<span><span class='op'>(</span><span class='fu'>conv1</span><span class='op'>(</span><span class='va'>x</span><span class='op'>)</span> <span class='op'>%&gt;%</span> <span class='fu'>conv2</span><span class='op'>(</span><span class='op'>)</span><span class='op'>)</span><span class='op'>$</span><span class='fu'>size</span><span class='op'>(</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<pre><code>[1]  1 64 24 24</code></pre>
<p>And so on. This is just one example illustrating how <code>torch</code>s flexibility makes developing neural nets easier.</p>
<p>Back to the main thread. We instantiate the model, and we ask <code>torch</code> to allocate its weights (parameters) on the GPU:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>model</span> <span class='op'>&lt;-</span> <span class='fu'>net</span><span class='op'>(</span><span class='op'>)</span></span>
<span><span class='va'>model</span><span class='op'>$</span><span class='fu'>to</span><span class='op'>(</span>device <span class='op'>=</span> <span class='st'>"cuda"</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>We’ll do the same for the input and output data – that is, we’ll move them to the GPU. This is done in the training loop, which we’ll inspect next.</p>
<h2 id="training">Training</h2>
<p>In <code>torch</code>, when creating an optimizer, we tell it what to operate on, namely, the model’s parameters:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>optimizer</span> <span class='op'>&lt;-</span> <span class='fu'>optim_adam</span><span class='op'>(</span><span class='va'>model</span><span class='op'>$</span><span class='va'>parameters</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>What about the loss function? For classification with more than two classes, we use <em>cross entropy</em>, in <code>torch</code>: <code>nnf_cross_entropy(prediction, ground_truth)</code>:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='co'># this will be called for every batch, see training loop below</span></span>
<span><span class='va'>loss</span> <span class='op'>&lt;-</span> <span class='fu'>nnf_cross_entropy</span><span class='op'>(</span><span class='va'>output</span>, <span class='va'>b</span><span class='op'>[[</span><span class='fl'>2</span><span class='op'>]</span><span class='op'>]</span><span class='op'>$</span><span class='fu'>to</span><span class='op'>(</span>device <span class='op'>=</span> <span class='st'>"cuda"</span><span class='op'>)</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>Unlike categorical cross entropy in <code>keras</code> , which would expect <code>prediction</code> to contain probabilities, as obtained by applying a <em>softmax</em> activation, <code>torch</code>’s <code>nnf_cross_entropy()</code> works with the raw outputs (the <em>logits</em>). This is why the network’s last linear layer was not followed by any activation.</p>
<p>The training loop, in fact, is a double one: It loops over epochs and batches. For every batch, it calls the model on the input, calculates the loss, and has the optimizer update the weights:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='kw'>for</span> <span class='op'>(</span><span class='va'>epoch</span> <span class='kw'>in</span> <span class='fl'>1</span><span class='op'>:</span><span class='fl'>5</span><span class='op'>)</span> <span class='op'>{</span></span>
<span></span>
<span>  <span class='va'>l</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='op'>)</span></span>
<span></span>
<span>  <span class='fu'>coro</span><span class='fu'>::</span><span class='fu'>loop</span><span class='op'>(</span><span class='kw'>for</span> <span class='op'>(</span><span class='va'>b</span> <span class='kw'>in</span> <span class='va'>train_dl</span><span class='op'>)</span> <span class='op'>{</span></span>
<span>    <span class='co'># make sure each batch's gradient updates are calculated from a fresh start</span></span>
<span>    <span class='va'>optimizer</span><span class='op'>$</span><span class='fu'>zero_grad</span><span class='op'>(</span><span class='op'>)</span></span>
<span>    <span class='co'># get model predictions</span></span>
<span>    <span class='va'>output</span> <span class='op'>&lt;-</span> <span class='fu'>model</span><span class='op'>(</span><span class='va'>b</span><span class='op'>[[</span><span class='fl'>1</span><span class='op'>]</span><span class='op'>]</span><span class='op'>$</span><span class='fu'>to</span><span class='op'>(</span>device <span class='op'>=</span> <span class='st'>"cuda"</span><span class='op'>)</span><span class='op'>)</span></span>
<span>    <span class='co'># calculate loss</span></span>
<span>    <span class='va'>loss</span> <span class='op'>&lt;-</span> <span class='fu'>nnf_cross_entropy</span><span class='op'>(</span><span class='va'>output</span>, <span class='va'>b</span><span class='op'>[[</span><span class='fl'>2</span><span class='op'>]</span><span class='op'>]</span><span class='op'>$</span><span class='fu'>to</span><span class='op'>(</span>device <span class='op'>=</span> <span class='st'>"cuda"</span><span class='op'>)</span><span class='op'>)</span></span>
<span>    <span class='co'># calculate gradient</span></span>
<span>    <span class='va'>loss</span><span class='op'>$</span><span class='fu'>backward</span><span class='op'>(</span><span class='op'>)</span></span>
<span>    <span class='co'># apply weight updates</span></span>
<span>    <span class='va'>optimizer</span><span class='op'>$</span><span class='fu'>step</span><span class='op'>(</span><span class='op'>)</span></span>
<span>    <span class='co'># track losses</span></span>
<span>    <span class='va'>l</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='va'>l</span>, <span class='va'>loss</span><span class='op'>$</span><span class='fu'>item</span><span class='op'>(</span><span class='op'>)</span><span class='op'>)</span></span>
<span>  <span class='op'>}</span><span class='op'>)</span></span>
<span></span>
<span>  <span class='fu'><a href='https://rdrr.io/r/base/cat.html'>cat</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/sprintf.html'>sprintf</a></span><span class='op'>(</span><span class='st'>"Loss at epoch %d: %3f\n"</span>, <span class='va'>epoch</span>, <span class='fu'><a href='https://rdrr.io/r/base/mean.html'>mean</a></span><span class='op'>(</span><span class='va'>l</span><span class='op'>)</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='op'>}</span></span></code></pre>
</div>
</div>
<pre><code>Loss at epoch 1: 1.795564
Loss at epoch 2: 1.540063
Loss at epoch 3: 1.495343
Loss at epoch 4: 1.461649
Loss at epoch 5: 1.446628</code></pre>
<p>Although there is a lot more that <em>could</em> be done – calculate metrics or evaluate performance on a validation set, for example – the above is a typical (if simple) template for a <code>torch</code> training loop.</p>
<p>The optimizer-related idioms in particular</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>optimizer</span><span class='op'>$</span><span class='fu'>zero_grad</span><span class='op'>(</span><span class='op'>)</span></span>
<span><span class='co'># ...</span></span>
<span><span class='va'>loss</span><span class='op'>$</span><span class='fu'>backward</span><span class='op'>(</span><span class='op'>)</span></span>
<span><span class='co'># ...</span></span>
<span><span class='va'>optimizer</span><span class='op'>$</span><span class='fu'>step</span><span class='op'>(</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>you’ll keep encountering over and over.</p>
<p>Finally, let’s evaluate model performance on the test set.</p>
<h2 id="evaluation">Evaluation</h2>
<p>Putting a model in <code>eval</code> mode tells <code>torch</code> <em>not</em> to calculate gradients and perform backprop during the operations that follow:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>model</span><span class='op'>$</span><span class='fu'>eval</span><span class='op'>(</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>We iterate over the test set, keeping track of losses and accuracies obtained on the batches.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>test_losses</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='op'>)</span></span>
<span><span class='va'>total</span> <span class='op'>&lt;-</span> <span class='fl'>0</span></span>
<span><span class='va'>correct</span> <span class='op'>&lt;-</span> <span class='fl'>0</span></span>
<span></span>
<span><span class='fu'>coro</span><span class='fu'>::</span><span class='fu'>loop</span><span class='op'>(</span><span class='kw'>for</span> <span class='op'>(</span><span class='va'>b</span> <span class='kw'>in</span> <span class='va'>test_dl</span><span class='op'>)</span> <span class='op'>{</span></span>
<span>  <span class='va'>output</span> <span class='op'>&lt;-</span> <span class='fu'>model</span><span class='op'>(</span><span class='va'>b</span><span class='op'>[[</span><span class='fl'>1</span><span class='op'>]</span><span class='op'>]</span><span class='op'>$</span><span class='fu'>to</span><span class='op'>(</span>device <span class='op'>=</span> <span class='st'>"cuda"</span><span class='op'>)</span><span class='op'>)</span></span>
<span>  <span class='va'>labels</span> <span class='op'>&lt;-</span> <span class='va'>b</span><span class='op'>[[</span><span class='fl'>2</span><span class='op'>]</span><span class='op'>]</span><span class='op'>$</span><span class='fu'>to</span><span class='op'>(</span>device <span class='op'>=</span> <span class='st'>"cuda"</span><span class='op'>)</span></span>
<span>  <span class='va'>loss</span> <span class='op'>&lt;-</span> <span class='fu'>nnf_cross_entropy</span><span class='op'>(</span><span class='va'>output</span>, <span class='va'>labels</span><span class='op'>)</span></span>
<span>  <span class='va'>test_losses</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='va'>test_losses</span>, <span class='va'>loss</span><span class='op'>$</span><span class='fu'>item</span><span class='op'>(</span><span class='op'>)</span><span class='op'>)</span></span>
<span>  <span class='co'># torch_max returns a list, with position 1 containing the values </span></span>
<span>  <span class='co'># and position 2 containing the respective indices</span></span>
<span>  <span class='va'>predicted</span> <span class='op'>&lt;-</span> <span class='fu'>torch_max</span><span class='op'>(</span><span class='va'>output</span><span class='op'>$</span><span class='fu'>data</span><span class='op'>(</span><span class='op'>)</span>, dim <span class='op'>=</span> <span class='fl'>2</span><span class='op'>)</span><span class='op'>[[</span><span class='fl'>2</span><span class='op'>]</span><span class='op'>]</span></span>
<span>  <span class='va'>total</span> <span class='op'>&lt;-</span> <span class='va'>total</span> <span class='op'>+</span> <span class='va'>labels</span><span class='op'>$</span><span class='fu'>size</span><span class='op'>(</span><span class='fl'>1</span><span class='op'>)</span></span>
<span>  <span class='co'># add number of correct classifications in this batch to the aggregate</span></span>
<span>  <span class='va'>correct</span> <span class='op'>&lt;-</span> <span class='va'>correct</span> <span class='op'>+</span> <span class='op'>(</span><span class='va'>predicted</span> <span class='op'>==</span> <span class='va'>labels</span><span class='op'>)</span><span class='op'>$</span><span class='fu'>sum</span><span class='op'>(</span><span class='op'>)</span><span class='op'>$</span><span class='fu'>item</span><span class='op'>(</span><span class='op'>)</span></span>
<span><span class='op'>}</span><span class='op'>)</span></span>
<span></span>
<span><span class='fu'><a href='https://rdrr.io/r/base/mean.html'>mean</a></span><span class='op'>(</span><span class='va'>test_losses</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<pre><code>[1] 1.53784480643349</code></pre>
<p>Here is mean accuracy, computed as proportion of correct classifications:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>test_accuracy</span> <span class='op'>&lt;-</span>  <span class='va'>correct</span><span class='op'>/</span><span class='va'>total</span></span>
<span><span class='va'>test_accuracy</span></span></code></pre>
</div>
</div>
<pre><code>[1] 0.9449</code></pre>
<p>That’s it for our first <code>torch</code> example. Where to from here?</p>
<h2 id="learn">Learn</h2>
<p>To learn more, check out our vignettes on the <a href="https://mlverse.github.io/torch"><code>torch</code> website</a>. To begin, you may want to check out these in particular:</p>
<ul>
<li>“Getting started” series: Build a simple neural network from scratch, starting from <a href="https://mlverse.github.io/tohttps://mlverse.github.io/torch/articles/getting-started/tensors.html">low-level tensor manipulation</a> and gradually adding in higher-level features like <a href="https://mlverse.github.io/torch/articles/getting-started/tensors-and-autograd.html">automatic differentiation</a> and <a href="https://mlverse.github.io/torch/articles/getting-started/nn.html">network modules</a>.</li>
<li>More on tensors: <a href="https://mlverse.github.io/torch/articles/tensor-creation.html">Tensor creation</a> and <a href="https://mlverse.github.io/torch/articles/indexing.html">indexing</a></li>
<li>Backpropagation in <code>torch</code>: <a href="https://mlverse.github.io/torch/articles/using-autograd.html">autograd</a></li>
</ul>
<p>If you have questions, or run into problems, please feel free to ask on <a href="https://github.com/mlverse/torch">GitHub</a> or on the <a href="https://community.rstudio.com/c/ml/15">RStudio community forum</a>.</p>
<h2 id="we-need-you">We need you</h2>
<p>We very much hope that the R community will find the new functionality useful. But that’s not all. We hope that you, many of you, will take part in the journey.</p>
<p>There is not just a whole framework to be built, including many specialized modules, activation functions, optimizers and schedulers, with more of each being added continuously, on the Python side.</p>
<p>There is not just that whole “bag of data types” to be taken care of (images, text, audio…), each of which demand their own pre-processing and data-loading functionality. As everyone knows from experience, ease of data preparation is a, perhaps <em>the</em> essential factor in how usable a framework is.</p>
<p>Then, there is the ever-expanding ecosystem of libraries built on top of PyTorch: <a href="https://github.com/OpenMined/PySyft">PySyft</a> and <a href="https://github.com/facebookresearch/CrypTen">CrypTen</a> for privacy-preserving machine learning, <a href="https://github.com/rusty1s/pytorch_geometric">PyTorch Geometric</a> for deep learning on manifolds, and <a href="http://pyro.ai/">Pyro</a> for probabilistic programming, to name just a few.</p>
<p>All this is much more than can be done by one or two people: We need your help! Contributions are greatly welcomed at absolutely <em>any</em> scale:</p>
<ul>
<li><p>Add or improve documentation, add introductory examples</p></li>
<li><p>Implement missing layers (modules), activations, helper functions…</p></li>
<li><p>Implement model architectures</p></li>
<li><p>Port some of the PyTorch ecosystem</p></li>
</ul>
<p>One component that should be of special interest to the R community is <a href="https://pytorch.org/docs/stable/distributions.html">Torch distributions</a>, the basis for probabilistic computation. This package is built upon by e.g. the aforementioned <a href="http://pyro.ai/">Pyro</a>; at the same time, the distributions that live there are used in probabilistic neural networks or normalizing flows.</p>
<p>To reiterate, participation from the R community is greatly encouraged (more than that – fervently hoped for!). Have fun with <code>torch</code>, and thanks for reading!</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r distill-force-highlighting-css"><code class="sourceCode r"></code></pre></div>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-clanuwat2018deep" class="csl-entry" role="doc-biblioentry">
Clanuwat, Tarin, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. 2018. <span>“Deep Learning for Classical Japanese Literature.”</span> December 3, 2018. <a href="https://arxiv.org/abs/cs.CV/1812.01718">https://arxiv.org/abs/cs.CV/1812.01718</a>.
</div>
</div>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>In a nutshell, <a href="https://twitter.com/javierluraschi">Javier</a> had the idea of wrapping <code>libtorch</code> into <a href="https://github.com/mlverse/lantern">lantern</a>, a C interface to <code>libtorch</code>, thus avoiding cross-compiler issues between MinGW and Visual Studio.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
<!--radix_placeholder_article_footer-->
<div class="article-footer">
  <p class="social_footer">
    <span class="disqus-comments">
      <i class="fas fa-comments"></i>
      &nbsp;
      <span class="disqus-comment-count" data-disqus-identifier="posts/2020-09-29-introducing-torch-for-r/">Comment on this article</span>
    </span>
    <span class="article-sharing">
      Share: &nbsp;
      <a href="https://twitter.com/share?text=Please%20allow%20me%20to%20introduce%20myself%3A%20Torch%20for%20R&amp;url=https%3A%2F%2Fblogs.rstudio.com%2Ftensorflow%2Fposts%2F2020-09-29-introducing-torch-for-r%2F" aria-label="share on twitter">
        <i class="fab fa-twitter" aria-hidden="true"></i>
      </a>
      <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3A%2F%2Fblogs.rstudio.com%2Ftensorflow%2Fposts%2F2020-09-29-introducing-torch-for-r%2F&amp;title=Please%20allow%20me%20to%20introduce%20myself%3A%20Torch%20for%20R" aria-label="share on linkedin">
        <i class="fab fa-linkedin" aria-hidden="true"></i>
      </a>
    </span>
  </p>
  <script id="dsq-count-scr" src="https://tensorflow-for-r-blog.disqus.com/count.js" async></script>
  <div id="disqus_thread" class="hidden"></div>
  <script type="text/javascript" cookie-consent="functionality">
var disqus_config = function () {
  this.page.url = 'https://blogs.rstudio.com/tensorflow/posts/2020-09-29-introducing-torch-for-r/';
  this.page.identifier = 'posts/2020-09-29-introducing-torch-for-r/';
};
(function() {
  var d = document, s = d.createElement('script');
  s.src = 'https://tensorflow-for-r-blog.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
})();
</script>
  <p>
    <div class="subscribe">
<div id="subscribe-caption" style="line-height: 1.2; margin-bottom: 2px;">Enjoy this blog? Get notified of new posts by email:</div>

<script src="https://app-ab02.marketo.com/js/forms2/js/forms2.min.js"></script>
<form class="mtktoBlogEmailForm" id="mktoForm_2224"></form>
<script>

// establish metrics based on where the form is located
var in_sidebar = $('#subscribe-caption').parents('.sidebar-section').length;
if (in_sidebar) {
  var form_width = 190;
  var base_width = form_width - 23;
  var email_width = base_width + 'px';
  var label_width = (base_width - 20) + 'px';
  var button_width = email_width;
  var button_padding = '30px';
  var button_margin = '';
  var font_size = '12px';
} else {
  var form_width = 400;
  var base_width = form_width - 23;
  var email_width = base_width + 'px';
  var label_width = (base_width - 20) + 'px';
  var button_width = email_width;
  var button_padding = '30px';
  var button_margin = '12px';
  var font_size = '15px';
}

$('#subscribe-caption')
  .css('width', base_width)
  .css('font-size', font_size);


MktoForms2.loadForm("https://app-ab02.marketo.com", "709-NXN-706", 2224, function(form) {

  // get jquery reference to form
  form = $(form.getFormElem().get(0));

  $(form).css('width', form_width + 'px');
  $(form).find('.mktoOffset').remove();
  $(form).find('.mktoGutter').remove();
  $(form).find('.mktoEmailField').css('width', email_width);
  $(form).find('.mktoLabel').children().attr('style', 'font-weight: 400');
  $(form).find('.mktoLabel')
      .css('width', label_width)
      .css('font-size', font_size);
  $(form).find('.mktoButtonRow')
      .css('width', button_width)
      .css('text-align', 'center');
  $(form).find('.mktoButtonWrap').attr('style', '');
  $(form).find('.mktoButton')
      .css('margin-top', '10px')
      .css('padding-left', button_padding)
      .css('padding-right', button_padding)
      .css('font-size', font_size)
      .css('margin-top', button_margin);
});
</script>
Posts also available at <a href="https://www.r-bloggers.com">r-bloggers</a>

<script>
  document.addEventListener("DOMContentLoaded", function(){
    var menulink = document.querySelector("a[href='#category:R']");
    if (menulink) menulink.parentNode.style.display = "None";
    
    for (var e of document.querySelectorAll(".dt-tag")) {
      if (e.innerHTML == 'R') e.style.display = "None";
    }
  });
</script>
</div>
  </p>
</div>
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>


<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
  <h3 id="references">References</h3>
  <div id="references-listing"></div>
  <h3 id="reuse">Reuse</h3>
  <p>Text and figures are licensed under Creative Commons Attribution <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>. The figures that have been reused from other sources don't fall under this license and can be recognized by a note in their caption: "Figure from ...".</p>
  <h3 id="citation">Citation</h3>
  <p>For attribution, please cite this work as</p>
  <pre class="citation-appendix short">Keydana (2020, Sept. 29). Posit AI Blog: Please allow me to introduce myself: Torch for R. Retrieved from https://blogs.rstudio.com/tensorflow/posts/2020-09-29-introducing-torch-for-r/</pre>
  <p>BibTeX citation</p>
  <pre class="citation-appendix long">@misc{keydanaintroducingtorchforR,
  author = {Keydana, Sigrid},
  title = {Posit AI Blog: Please allow me to introduce myself: Torch for R},
  url = {https://blogs.rstudio.com/tensorflow/posts/2020-09-29-introducing-torch-for-r/},
  year = {2020}
}</pre>
</div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
