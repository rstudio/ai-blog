<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { color: #00769e; background-color: #f1f3f5; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span { color: #00769e; } /* Normal */
code span.al { color: #ad0000; } /* Alert */
code span.an { color: #5e5e5e; } /* Annotation */
code span.at { color: #657422; } /* Attribute */
code span.bn { color: #ad0000; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #00769e; } /* ControlFlow */
code span.ch { color: #20794d; } /* Char */
code span.cn { color: #8f5902; } /* Constant */
code span.co { color: #5e5e5e; } /* Comment */
code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
code span.dt { color: #ad0000; } /* DataType */
code span.dv { color: #ad0000; } /* DecVal */
code span.er { color: #ad0000; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #ad0000; } /* Float */
code span.fu { color: #4758ab; } /* Function */
code span.im { } /* Import */
code span.in { color: #5e5e5e; } /* Information */
code span.kw { color: #00769e; } /* Keyword */
code span.op { color: #5e5e5e; } /* Operator */
code span.ot { color: #00769e; } /* Other */
code span.pp { color: #ad0000; } /* Preprocessor */
code span.sc { color: #5e5e5e; } /* SpecialChar */
code span.ss { color: #20794d; } /* SpecialString */
code span.st { color: #20794d; } /* String */
code span.va { color: #111111; } /* Variable */
code span.vs { color: #20794d; } /* VerbatimString */
code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
</style>


  <!--radix_placeholder_meta_tags-->
<title>Posit AI Blog: Getting familiar with torch tensors</title>

<meta property="description" itemprop="description" content="In this first installment of a four-part miniseries, we present the main things you will want to know about torch tensors. As an illustrative example, we&#39;ll code a simple neural network from scratch."/>

<link rel="canonical" href="https://blogs.rstudio.com/tensorflow/posts/2020-10-01-torch-network-from-scratch/"/>
<link rel="license" href="https://creativecommons.org/licenses/by/4.0/"/>
<link rel="icon" type="image/png" href="../../images/favicon.png"/>

<!--  https://schema.org/Article -->
<meta property="article:published" itemprop="datePublished" content="2020-10-01"/>
<meta property="article:created" itemprop="dateCreated" content="2020-10-01"/>
<meta name="article:author" content="Sigrid Keydana"/>

<!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
<meta property="og:title" content="Posit AI Blog: Getting familiar with torch tensors"/>
<meta property="og:type" content="article"/>
<meta property="og:description" content="In this first installment of a four-part miniseries, we present the main things you will want to know about torch tensors. As an illustrative example, we&#39;ll code a simple neural network from scratch."/>
<meta property="og:url" content="https://blogs.rstudio.com/tensorflow/posts/2020-10-01-torch-network-from-scratch/"/>
<meta property="og:image" content="https://blogs.rstudio.com/tensorflow/posts/2020-10-01-torch-network-from-scratch/images/pic.jpg"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:site_name" content="Posit AI Blog"/>

<!--  https://dev.twitter.com/cards/types/summary -->
<meta property="twitter:card" content="summary_large_image"/>
<meta property="twitter:title" content="Posit AI Blog: Getting familiar with torch tensors"/>
<meta property="twitter:description" content="In this first installment of a four-part miniseries, we present the main things you will want to know about torch tensors. As an illustrative example, we&#39;ll code a simple neural network from scratch."/>
<meta property="twitter:url" content="https://blogs.rstudio.com/tensorflow/posts/2020-10-01-torch-network-from-scratch/"/>
<meta property="twitter:image" content="https://blogs.rstudio.com/tensorflow/posts/2020-10-01-torch-network-from-scratch/images/pic.jpg"/>

<!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
<meta name="citation_title" content="Posit AI Blog: Getting familiar with torch tensors"/>
<meta name="citation_fulltext_html_url" content="https://blogs.rstudio.com/tensorflow/posts/2020-10-01-torch-network-from-scratch/"/>
<meta name="citation_fulltext_world_readable" content=""/>
<meta name="citation_online_date" content="2020/10/01"/>
<meta name="citation_publication_date" content="2020/10/01"/>
<meta name="citation_author" content="Sigrid Keydana"/>
<meta name="citation_author_institution" content="RStudio"/>
<!--/radix_placeholder_meta_tags-->
  <!--radix_placeholder_rmarkdown_metadata-->

<script type="text/json" id="radix-rmarkdown-metadata">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["creative_commons","title","description","author","slug","date","categories","output","preview","citation_url","canonical_url"]}},"value":[{"type":"character","attributes":{},"value":["CC BY"]},{"type":"character","attributes":{},"value":["Getting familiar with torch tensors"]},{"type":"character","attributes":{},"value":["In this first installment of a four-part miniseries, we present the main things you will want to know about torch tensors. As an illustrative example, we'll code a simple neural network from scratch."]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","affiliation_url"]}},"value":[{"type":"character","attributes":{},"value":["Sigrid Keydana"]},{"type":"character","attributes":{},"value":["RStudio"]},{"type":"character","attributes":{},"value":["https://www.rstudio.com/"]}]}]},{"type":"character","attributes":{},"value":["keydanatorchnetworkfromscratch"]},{"type":"character","attributes":{},"value":["10-01-2020"]},{"type":"character","attributes":{},"value":["Torch","R"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained","toc"]}},"value":[{"type":"logical","attributes":{},"value":[false]},{"type":"logical","attributes":{},"value":[true]}]}]},{"type":"character","attributes":{},"value":["images/pic.jpg"]},{"type":"character","attributes":{},"value":["https://blogs.rstudio.com/tensorflow/posts/2020-10-01-torch-network-from-scratch/"]},{"type":"character","attributes":{},"value":["https://blogs.rstudio.com/tensorflow/posts/2020-10-01-torch-network-from-scratch/"]}]}
</script>
<!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["images/pic.jpg","torch_network_from_scratch_files/anchor-4.2.2/anchor.min.js","torch_network_from_scratch_files/bowser-1.9.3/bowser.min.js","torch_network_from_scratch_files/distill-2.2.21/template.v2.js","torch_network_from_scratch_files/header-attrs-2.3/header-attrs.js","torch_network_from_scratch_files/jquery-1.11.3/jquery.min.js","torch_network_from_scratch_files/webcomponents-2.0.0/webcomponents.js"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
<meta name="distill:offset" content="../.."/>

<script type="application/javascript">

  window.headroom_prevent_pin = false;

  window.document.addEventListener("DOMContentLoaded", function (event) {

    // initialize headroom for banner
    var header = $('header').get(0);
    var headerHeight = header.offsetHeight;
    var headroom = new Headroom(header, {
      tolerance: 5,
      onPin : function() {
        if (window.headroom_prevent_pin) {
          window.headroom_prevent_pin = false;
          headroom.unpin();
        }
      }
    });
    headroom.init();
    if(window.location.hash)
      headroom.unpin();
    $(header).addClass('headroom--transition');

    // offset scroll location for banner on hash change
    // (see: https://github.com/WickyNilliams/headroom.js/issues/38)
    window.addEventListener("hashchange", function(event) {
      window.scrollTo(0, window.pageYOffset - (headerHeight + 25));
    });

    // responsive menu
    $('.distill-site-header').each(function(i, val) {
      var topnav = $(this);
      var toggle = topnav.find('.nav-toggle');
      toggle.on('click', function() {
        topnav.toggleClass('responsive');
      });
    });

    // nav dropdowns
    $('.nav-dropbtn').click(function(e) {
      $(this).next('.nav-dropdown-content').toggleClass('nav-dropdown-active');
      $(this).parent().siblings('.nav-dropdown')
         .children('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $("body").click(function(e){
      $('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $(".nav-dropdown").click(function(e){
      e.stopPropagation();
    });
  });
</script>

<style type="text/css">

/* Theme (user-documented overrideables for nav appearance) */

.distill-site-nav {
  color: rgba(255, 255, 255, 0.8);
  background-color: #0F2E3D;
  font-size: 15px;
  font-weight: 300;
}

.distill-site-nav a {
  color: inherit;
  text-decoration: none;
}

.distill-site-nav a:hover {
  color: white;
}

@media print {
  .distill-site-nav {
    display: none;
  }
}

.distill-site-header {

}

.distill-site-footer {

}


/* Site Header */

.distill-site-header {
  width: 100%;
  box-sizing: border-box;
  z-index: 3;
}

.distill-site-header .nav-left {
  display: inline-block;
  margin-left: 8px;
}

@media screen and (max-width: 768px) {
  .distill-site-header .nav-left {
    margin-left: 0;
  }
}


.distill-site-header .nav-right {
  float: right;
  margin-right: 8px;
}

.distill-site-header a,
.distill-site-header .title {
  display: inline-block;
  text-align: center;
  padding: 14px 10px 14px 10px;
}

.distill-site-header .title {
  font-size: 18px;
  min-width: 150px;
}

.distill-site-header .logo {
  padding: 0;
}

.distill-site-header .logo img {
  display: none;
  max-height: 20px;
  width: auto;
  margin-bottom: -4px;
}

.distill-site-header .nav-image img {
  max-height: 18px;
  width: auto;
  display: inline-block;
  margin-bottom: -3px;
}



@media screen and (min-width: 1000px) {
  .distill-site-header .logo img {
    display: inline-block;
  }
  .distill-site-header .nav-left {
    margin-left: 20px;
  }
  .distill-site-header .nav-right {
    margin-right: 20px;
  }
  .distill-site-header .title {
    padding-left: 12px;
  }
}


.distill-site-header .nav-toggle {
  display: none;
}

.nav-dropdown {
  display: inline-block;
  position: relative;
}

.nav-dropdown .nav-dropbtn {
  border: none;
  outline: none;
  color: rgba(255, 255, 255, 0.8);
  padding: 16px 10px;
  background-color: transparent;
  font-family: inherit;
  font-size: inherit;
  font-weight: inherit;
  margin: 0;
  margin-top: 1px;
  z-index: 2;
}

.nav-dropdown-content {
  display: none;
  position: absolute;
  background-color: white;
  min-width: 200px;
  border: 1px solid rgba(0,0,0,0.15);
  border-radius: 4px;
  box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.1);
  z-index: 1;
  margin-top: 2px;
  white-space: nowrap;
  padding-top: 4px;
  padding-bottom: 4px;
}

.nav-dropdown-content hr {
  margin-top: 4px;
  margin-bottom: 4px;
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.nav-dropdown-active {
  display: block;
}

.nav-dropdown-content a, .nav-dropdown-content .nav-dropdown-header {
  color: black;
  padding: 6px 24px;
  text-decoration: none;
  display: block;
  text-align: left;
}

.nav-dropdown-content .nav-dropdown-header {
  display: block;
  padding: 5px 24px;
  padding-bottom: 0;
  text-transform: uppercase;
  font-size: 14px;
  color: #999999;
  white-space: nowrap;
}

.nav-dropdown:hover .nav-dropbtn {
  color: white;
}

.nav-dropdown-content a:hover {
  background-color: #ddd;
  color: black;
}

.nav-right .nav-dropdown-content {
  margin-left: -45%;
  right: 0;
}

@media screen and (max-width: 768px) {
  .distill-site-header a, .distill-site-header .nav-dropdown  {display: none;}
  .distill-site-header a.nav-toggle {
    float: right;
    display: block;
  }
  .distill-site-header .title {
    margin-left: 0;
  }
  .distill-site-header .nav-right {
    margin-right: 0;
  }
  .distill-site-header {
    overflow: hidden;
  }
  .nav-right .nav-dropdown-content {
    margin-left: 0;
  }
}


@media screen and (max-width: 768px) {
  .distill-site-header.responsive {position: relative; min-height: 500px; }
  .distill-site-header.responsive a.nav-toggle {
    position: absolute;
    right: 0;
    top: 0;
  }
  .distill-site-header.responsive a,
  .distill-site-header.responsive .nav-dropdown {
    display: block;
    text-align: left;
  }
  .distill-site-header.responsive .nav-left,
  .distill-site-header.responsive .nav-right {
    width: 100%;
  }
  .distill-site-header.responsive .nav-dropdown {float: none;}
  .distill-site-header.responsive .nav-dropdown-content {position: relative;}
  .distill-site-header.responsive .nav-dropdown .nav-dropbtn {
    display: block;
    width: 100%;
    text-align: left;
  }
}

/* Site Footer */

.distill-site-footer {
  width: 100%;
  overflow: hidden;
  box-sizing: border-box;
  z-index: 3;
  margin-top: 30px;
  padding-top: 30px;
  padding-bottom: 30px;
  text-align: center;
}

/* Headroom */

d-title {
  padding-top: 6rem;
}

@media print {
  d-title {
    padding-top: 4rem;
  }
}

.headroom {
  z-index: 1000;
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
}

.headroom--transition {
  transition: all .4s ease-in-out;
}

.headroom--unpinned {
  top: -100px;
}

.headroom--pinned {
  top: 0;
}

/* adjust viewport for navbar height */
/* helps vertically center bootstrap (non-distill) content */
.min-vh-100 {
  min-height: calc(100vh - 100px) !important;
}

</style>

<script src="../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="../../site_libs/font-awesome-6.4.2/css/all.min.css" rel="stylesheet"/>
<link href="../../site_libs/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet"/>
<script src="../../site_libs/headroom-0.9.4/headroom.min.js"></script>
<script src="../../site_libs/autocomplete-0.37.1/autocomplete.min.js"></script>
<script src="../../site_libs/fuse-6.4.1/fuse.min.js"></script>

<script type="application/javascript">

function getMeta(metaName) {
  var metas = document.getElementsByTagName('meta');
  for (let i = 0; i < metas.length; i++) {
    if (metas[i].getAttribute('name') === metaName) {
      return metas[i].getAttribute('content');
    }
  }
  return '';
}

function offsetURL(url) {
  var offset = getMeta('distill:offset');
  return offset ? offset + '/' + url : url;
}

function createFuseIndex() {

  // create fuse index
  var options = {
    keys: [
      { name: 'title', weight: 20 },
      { name: 'categories', weight: 15 },
      { name: 'description', weight: 10 },
      { name: 'contents', weight: 5 },
    ],
    ignoreLocation: true,
    threshold: 0
  };
  var fuse = new window.Fuse([], options);

  // fetch the main search.json
  return fetch(offsetURL('search.json'))
    .then(function(response) {
      if (response.status == 200) {
        return response.json().then(function(json) {
          // index main articles
          json.articles.forEach(function(article) {
            fuse.add(article);
          });
          // download collections and index their articles
          return Promise.all(json.collections.map(function(collection) {
            return fetch(offsetURL(collection)).then(function(response) {
              if (response.status === 200) {
                return response.json().then(function(articles) {
                  articles.forEach(function(article) {
                    fuse.add(article);
                  });
                })
              } else {
                return Promise.reject(
                  new Error('Unexpected status from search index request: ' +
                            response.status)
                );
              }
            });
          })).then(function() {
            return fuse;
          });
        });

      } else {
        return Promise.reject(
          new Error('Unexpected status from search index request: ' +
                      response.status)
        );
      }
    });
}

window.document.addEventListener("DOMContentLoaded", function (event) {

  // get search element (bail if we don't have one)
  var searchEl = window.document.getElementById('distill-search');
  if (!searchEl)
    return;

  createFuseIndex()
    .then(function(fuse) {

      // make search box visible
      searchEl.classList.remove('hidden');

      // initialize autocomplete
      var options = {
        autoselect: true,
        hint: false,
        minLength: 2,
      };
      window.autocomplete(searchEl, options, [{
        source: function(query, callback) {
          const searchOptions = {
            isCaseSensitive: false,
            shouldSort: true,
            minMatchCharLength: 2,
            limit: 10,
          };
          var results = fuse.search(query, searchOptions);
          callback(results
            .map(function(result) { return result.item; })
          );
        },
        templates: {
          suggestion: function(suggestion) {
            var img = suggestion.preview && Object.keys(suggestion.preview).length > 0
              ? `<img src="${offsetURL(suggestion.preview)}"</img>`
              : '';
            var html = `
              <div class="search-item">
                <h3>${suggestion.title}</h3>
                <div class="search-item-description">
                  ${suggestion.description || ''}
                </div>
                <div class="search-item-preview">
                  ${img}
                </div>
              </div>
            `;
            return html;
          }
        }
      }]).on('autocomplete:selected', function(event, suggestion) {
        window.location.href = offsetURL(suggestion.path);
      });
      // remove inline display style on autocompleter (we want to
      // manage responsive display via css)
      $('.algolia-autocomplete').css("display", "");
    })
    .catch(function(error) {
      console.log(error);
    });

});

</script>

<style type="text/css">

.nav-search {
  font-size: x-small;
}

/* Algolioa Autocomplete */

.algolia-autocomplete {
  display: inline-block;
  margin-left: 10px;
  vertical-align: sub;
  background-color: white;
  color: black;
  padding: 6px;
  padding-top: 8px;
  padding-bottom: 0;
  border-radius: 6px;
  border: 1px #0F2E3D solid;
  width: 180px;
}


@media screen and (max-width: 768px) {
  .distill-site-nav .algolia-autocomplete {
    display: none;
    visibility: hidden;
  }
  .distill-site-nav.responsive .algolia-autocomplete {
    display: inline-block;
    visibility: visible;
  }
  .distill-site-nav.responsive .algolia-autocomplete .aa-dropdown-menu {
    margin-left: 0;
    width: 400px;
    max-height: 400px;
  }
}

.algolia-autocomplete .aa-input, .algolia-autocomplete .aa-hint {
  width: 90%;
  outline: none;
  border: none;
}

.algolia-autocomplete .aa-hint {
  color: #999;
}
.algolia-autocomplete .aa-dropdown-menu {
  width: 550px;
  max-height: 70vh;
  overflow-x: visible;
  overflow-y: scroll;
  padding: 5px;
  margin-top: 3px;
  margin-left: -150px;
  background-color: #fff;
  border-radius: 5px;
  border: 1px solid #999;
  border-top: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion {
  cursor: pointer;
  padding: 5px 4px;
  border-bottom: 1px solid #eee;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion:last-of-type {
  border-bottom: none;
  margin-bottom: 2px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item {
  overflow: hidden;
  font-size: 0.8em;
  line-height: 1.4em;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item h3 {
  font-size: 1rem;
  margin-block-start: 0;
  margin-block-end: 5px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-description {
  display: inline-block;
  overflow: hidden;
  height: 2.8em;
  width: 80%;
  margin-right: 4%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview {
  display: inline-block;
  width: 15%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img {
  height: 3em;
  width: auto;
  display: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img[src] {
  display: initial;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion.aa-cursor {
  background-color: #eee;
}
.algolia-autocomplete .aa-dropdown-menu .aa-suggestion em {
  font-weight: bold;
  font-style: normal;
}

</style>


<!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

<style type="text/css">

body {
  background-color: white;
}

.pandoc-table {
  width: 100%;
}

.pandoc-table>caption {
  margin-bottom: 10px;
}

.pandoc-table th:not([align]) {
  text-align: left;
}

.pagedtable-footer {
  font-size: 15px;
}

d-byline .byline {
  grid-template-columns: 2fr 2fr;
}

d-byline .byline h3 {
  margin-block-start: 1.5em;
}

d-byline .byline .authors-affiliations h3 {
  margin-block-start: 0.5em;
}

.authors-affiliations .orcid-id {
  width: 16px;
  height:16px;
  margin-left: 4px;
  margin-right: 4px;
  vertical-align: middle;
  padding-bottom: 2px;
}

d-title .dt-tags {
  margin-top: 1em;
  grid-column: text;
}

.dt-tags .dt-tag {
  text-decoration: none;
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0em 0.4em;
  margin-right: 0.5em;
  margin-bottom: 0.4em;
  font-size: 70%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

d-article table.gt_table td,
d-article table.gt_table th {
  border-bottom: none;
  font-size: 100%;
}

.html-widget {
  margin-bottom: 2.0em;
}

.l-screen-inset {
  padding-right: 16px;
}

.l-screen .caption {
  margin-left: 10px;
}

.shaded {
  background: rgb(247, 247, 247);
  padding-top: 20px;
  padding-bottom: 20px;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .html-widget {
  margin-bottom: 0;
  border: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .shaded-content {
  background: white;
}

.text-output {
  margin-top: 0;
  line-height: 1.5em;
}

.hidden {
  display: none !important;
}

hr.section-separator {
  border: none;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  margin: 0px;
}


d-byline {
  border-top: none;
}

d-article {
  padding-top: 2.5rem;
  padding-bottom: 30px;
  border-top: none;
}

d-appendix {
  padding-top: 30px;
}

d-article>p>img {
  width: 100%;
}

d-article h2 {
  margin: 1rem 0 1.5rem 0;
}

d-article h3 {
  margin-top: 1.5rem;
}

d-article iframe {
  border: 1px solid rgba(0, 0, 0, 0.1);
  margin-bottom: 2.0em;
  width: 100%;
}

/* Tweak code blocks */

d-article div.sourceCode code,
d-article pre code {
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
}

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: auto;
}

d-article div.sourceCode {
  background-color: white;
}

d-article div.sourceCode pre {
  padding-left: 10px;
  font-size: 12px;
  border-left: 2px solid rgba(0,0,0,0.1);
}

d-article pre {
  font-size: 12px;
  color: black;
  background: none;
  margin-top: 0;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

d-article pre a {
  border-bottom: none;
}

d-article pre a:hover {
  border-bottom: none;
  text-decoration: underline;
}

d-article details {
  grid-column: text;
  margin-bottom: 0.8em;
}

@media(min-width: 768px) {

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: visible !important;
}

d-article div.sourceCode pre {
  padding-left: 18px;
  font-size: 14px;
}

/* tweak for Pandoc numbered line within distill */
d-article pre.numberSource code > span {
    left: -2em;
}

d-article pre {
  font-size: 14px;
}

}

figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

/* CSS for d-contents */

.d-contents {
  grid-column: text;
  color: rgba(0,0,0,0.8);
  font-size: 0.9em;
  padding-bottom: 1em;
  margin-bottom: 1em;
  padding-bottom: 0.5em;
  margin-bottom: 1em;
  padding-left: 0.25em;
  justify-self: start;
}

@media(min-width: 1000px) {
  .d-contents.d-contents-float {
    height: 0;
    grid-column-start: 1;
    grid-column-end: 4;
    justify-self: center;
    padding-right: 3em;
    padding-left: 2em;
  }
}

.d-contents nav h3 {
  font-size: 18px;
  margin-top: 0;
  margin-bottom: 1em;
}

.d-contents li {
  list-style-type: none
}

.d-contents nav > ul {
  padding-left: 0;
}

.d-contents ul {
  padding-left: 1em
}

.d-contents nav ul li {
  margin-top: 0.6em;
  margin-bottom: 0.2em;
}

.d-contents nav a {
  font-size: 13px;
  border-bottom: none;
  text-decoration: none
  color: rgba(0, 0, 0, 0.8);
}

.d-contents nav a:hover {
  text-decoration: underline solid rgba(0, 0, 0, 0.6)
}

.d-contents nav > ul > li > a {
  font-weight: 600;
}

.d-contents nav > ul > li > ul {
  font-weight: inherit;
}

.d-contents nav > ul > li > ul > li {
  margin-top: 0.2em;
}


.d-contents nav ul {
  margin-top: 0;
  margin-bottom: 0.25em;
}

.d-article-with-toc h2:nth-child(2) {
  margin-top: 0;
}


/* Figure */

.figure {
  position: relative;
  margin-bottom: 2.5em;
  margin-top: 1.5em;
}

.figure .caption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

.figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

.figure .caption a {
  color: rgba(0, 0, 0, 0.6);
}

.figure .caption b,
.figure .caption strong, {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

/* Citations */

d-article .citation {
  color: inherit;
  cursor: inherit;
}

div.hanging-indent{
  margin-left: 1em; text-indent: -1em;
}

/* Citation hover box */

.tippy-box[data-theme~=light-border] {
  background-color: rgba(250, 250, 250, 0.95);
}

.tippy-content > p {
  margin-bottom: 0;
  padding: 2px;
}


/* Tweak 1000px media break to show more text */

@media(min-width: 1000px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }

  d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
  }
  figure .caption, .figure .caption, figure figcaption {
    font-size: 13px;
  }
}

@media(min-width: 1180px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 32px;
  }

  .grid {
    grid-column-gap: 32px;
  }
}


/* Get the citation styles for the appendix (not auto-injected on render since
   we do our own rendering of the citation appendix) */

d-appendix .citation-appendix,
.d-appendix .citation-appendix {
  font-size: 11px;
  line-height: 15px;
  border-left: 1px solid rgba(0, 0, 0, 0.1);
  padding-left: 18px;
  border: 1px solid rgba(0,0,0,0.1);
  background: rgba(0, 0, 0, 0.02);
  padding: 10px 18px;
  border-radius: 3px;
  color: rgba(150, 150, 150, 1);
  overflow: hidden;
  margin-top: -12px;
  white-space: pre-wrap;
  word-wrap: break-word;
}

/* Include appendix styles here so they can be overridden */

d-appendix {
  contain: layout style;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-top: 60px;
  margin-bottom: 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  color: rgba(0,0,0,0.5);
  padding-top: 60px;
  padding-bottom: 48px;
}

d-appendix h3 {
  grid-column: page-start / text-start;
  font-size: 15px;
  font-weight: 500;
  margin-top: 1em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.65);
}

d-appendix h3 + * {
  margin-top: 1em;
}

d-appendix ol {
  padding: 0 0 0 15px;
}

@media (min-width: 768px) {
  d-appendix ol {
    padding: 0 0 0 30px;
    margin-left: -30px;
  }
}

d-appendix li {
  margin-bottom: 1em;
}

d-appendix a {
  color: rgba(0, 0, 0, 0.6);
}

d-appendix > * {
  grid-column: text;
}

d-appendix > d-footnote-list,
d-appendix > d-citation-list,
d-appendix > distill-appendix {
  grid-column: screen;
}

/* Include footnote styles here so they can be overridden */

d-footnote-list {
  contain: layout style;
}

d-footnote-list > * {
  grid-column: text;
}

d-footnote-list a.footnote-backlink {
  color: rgba(0,0,0,0.3);
  padding-left: 0.5em;
}



/* Anchor.js */

.anchorjs-link {
  /*transition: all .25s linear; */
  text-decoration: none;
  border-bottom: none;
}
*:hover > .anchorjs-link {
  margin-left: -1.125em !important;
  text-decoration: none;
  border-bottom: none;
}

/* Social footer */

.social_footer {
  margin-top: 30px;
  margin-bottom: 0;
  color: rgba(0,0,0,0.67);
}

.disqus-comments {
  margin-right: 30px;
}

.disqus-comment-count {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  cursor: pointer;
}

#disqus_thread {
  margin-top: 30px;
}

.article-sharing a {
  border-bottom: none;
  margin-right: 8px;
}

.article-sharing a:hover {
  border-bottom: none;
}

.sidebar-section.subscribe {
  font-size: 12px;
  line-height: 1.6em;
}

.subscribe p {
  margin-bottom: 0.5em;
}


.article-footer .subscribe {
  font-size: 15px;
  margin-top: 45px;
}


.sidebar-section.custom {
  font-size: 12px;
  line-height: 1.6em;
}

.custom p {
  margin-bottom: 0.5em;
}

/* Styles for listing layout (hide title) */
.layout-listing d-title, .layout-listing .d-title {
  display: none;
}

/* Styles for posts lists (not auto-injected) */


.posts-with-sidebar {
  padding-left: 45px;
  padding-right: 45px;
}

.posts-list .description h2,
.posts-list .description p {
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
}

.posts-list .description h2 {
  font-weight: 700;
  border-bottom: none;
  padding-bottom: 0;
}

.posts-list h2.post-tag {
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
  padding-bottom: 12px;
}
.posts-list {
  margin-top: 60px;
  margin-bottom: 24px;
}

.posts-list .post-preview {
  text-decoration: none;
  overflow: hidden;
  display: block;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  padding: 24px 0;
}

.post-preview-last {
  border-bottom: none !important;
}

.posts-list .posts-list-caption {
  grid-column: screen;
  font-weight: 400;
}

.posts-list .post-preview h2 {
  margin: 0 0 6px 0;
  line-height: 1.2em;
  font-style: normal;
  font-size: 24px;
}

.posts-list .post-preview p {
  margin: 0 0 12px 0;
  line-height: 1.4em;
  font-size: 16px;
}

.posts-list .post-preview .thumbnail {
  box-sizing: border-box;
  margin-bottom: 24px;
  position: relative;
  max-width: 500px;
}
.posts-list .post-preview img {
  width: 100%;
  display: block;
}

.posts-list .metadata {
  font-size: 12px;
  line-height: 1.4em;
  margin-bottom: 18px;
}

.posts-list .metadata > * {
  display: inline-block;
}

.posts-list .metadata .publishedDate {
  margin-right: 2em;
}

.posts-list .metadata .dt-authors {
  display: block;
  margin-top: 0.3em;
  margin-right: 2em;
}

.posts-list .dt-tags {
  display: block;
  line-height: 1em;
}

.posts-list .dt-tags .dt-tag {
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0.3em 0.4em;
  margin-right: 0.2em;
  margin-bottom: 0.4em;
  font-size: 60%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

.posts-list img {
  opacity: 1;
}

.posts-list img[data-src] {
  opacity: 0;
}

.posts-more {
  clear: both;
}


.posts-sidebar {
  font-size: 16px;
}

.posts-sidebar h3 {
  font-size: 16px;
  margin-top: 0;
  margin-bottom: 0.5em;
  font-weight: 400;
  text-transform: uppercase;
}

.sidebar-section {
  margin-bottom: 30px;
}

.categories ul {
  list-style-type: none;
  margin: 0;
  padding: 0;
}

.categories li {
  color: rgba(0, 0, 0, 0.8);
  margin-bottom: 0;
}

.categories li>a {
  border-bottom: none;
}

.categories li>a:hover {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
}

.categories .active {
  font-weight: 600;
}

.categories .category-count {
  color: rgba(0, 0, 0, 0.4);
}


@media(min-width: 768px) {
  .posts-list .post-preview h2 {
    font-size: 26px;
  }
  .posts-list .post-preview .thumbnail {
    float: right;
    width: 30%;
    margin-bottom: 0;
  }
  .posts-list .post-preview .description {
    float: left;
    width: 45%;
  }
  .posts-list .post-preview .metadata {
    float: left;
    width: 20%;
    margin-top: 8px;
  }
  .posts-list .post-preview p {
    margin: 0 0 12px 0;
    line-height: 1.5em;
    font-size: 16px;
  }
  .posts-with-sidebar .posts-list {
    float: left;
    width: 75%;
  }
  .posts-with-sidebar .posts-sidebar {
    float: right;
    width: 20%;
    margin-top: 60px;
    padding-top: 24px;
    padding-bottom: 24px;
  }
}


/* Improve display for browsers without grid (IE/Edge <= 15) */

.downlevel {
  line-height: 1.6em;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  margin: 0;
}

.downlevel .d-title {
  padding-top: 6rem;
  padding-bottom: 1.5rem;
}

.downlevel .d-title h1 {
  font-size: 50px;
  font-weight: 700;
  line-height: 1.1em;
  margin: 0 0 0.5rem;
}

.downlevel .d-title p {
  font-weight: 300;
  font-size: 1.2rem;
  line-height: 1.55em;
  margin-top: 0;
}

.downlevel .d-byline {
  padding-top: 0.8em;
  padding-bottom: 0.8em;
  font-size: 0.8rem;
  line-height: 1.8em;
}

.downlevel .section-separator {
  border: none;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
}

.downlevel .d-article {
  font-size: 1.06rem;
  line-height: 1.7em;
  padding-top: 1rem;
  padding-bottom: 2rem;
}


.downlevel .d-appendix {
  padding-left: 0;
  padding-right: 0;
  max-width: none;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.5);
  padding-top: 40px;
  padding-bottom: 48px;
}

.downlevel .footnotes ol {
  padding-left: 13px;
}

.downlevel .base-grid,
.downlevel .distill-header,
.downlevel .d-title,
.downlevel .d-abstract,
.downlevel .d-article,
.downlevel .d-appendix,
.downlevel .distill-appendix,
.downlevel .d-byline,
.downlevel .d-footnote-list,
.downlevel .d-citation-list,
.downlevel .distill-footer,
.downlevel .appendix-bottom,
.downlevel .posts-container {
  padding-left: 40px;
  padding-right: 40px;
}

@media(min-width: 768px) {
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
  padding-left: 150px;
  padding-right: 150px;
  max-width: 900px;
}
}

.downlevel pre code {
  display: block;
  border-left: 2px solid rgba(0, 0, 0, .1);
  padding: 0 0 0 20px;
  font-size: 14px;
}

.downlevel code, .downlevel pre {
  color: black;
  background: none;
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

.downlevel .posts-list .post-preview {
  color: inherit;
}



</style>

<script type="application/javascript">

function is_downlevel_browser() {
  if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                 window.navigator.userAgent)) {
    return true;
  } else {
    return window.load_distill_framework === undefined;
  }
}

// show body when load is complete
function on_load_complete() {

  // add anchors
  if (window.anchors) {
    window.anchors.options.placement = 'left';
    window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
  }


  // set body to visible
  document.body.style.visibility = 'visible';

  // force redraw for leaflet widgets
  if (window.HTMLWidgets) {
    var maps = window.HTMLWidgets.findAll(".leaflet");
    $.each(maps, function(i, el) {
      var map = this.getMap();
      map.invalidateSize();
      map.eachLayer(function(layer) {
        if (layer instanceof L.TileLayer)
          layer.redraw();
      });
    });
  }

  // trigger 'shown' so htmlwidgets resize
  $('d-article').trigger('shown');
}

function init_distill() {

  init_common();

  // create front matter
  var front_matter = $('<d-front-matter></d-front-matter>');
  $('#distill-front-matter').wrap(front_matter);

  // create d-title
  $('.d-title').changeElementType('d-title');

  // separator
  var separator = '<hr class="section-separator" style="clear: both"/>';
  // prepend separator above appendix
  $('.d-byline').before(separator);
  $('.d-article').before(separator);

  // create d-byline
  var byline = $('<d-byline></d-byline>');
  $('.d-byline').replaceWith(byline);

  // create d-article
  var article = $('<d-article></d-article>');
  $('.d-article').wrap(article).children().unwrap();

  // move posts container into article
  $('.posts-container').appendTo($('d-article'));

  // create d-appendix
  $('.d-appendix').changeElementType('d-appendix');

  // flag indicating that we have appendix items
  var appendix = $('.appendix-bottom').children('h3').length > 0;

  // replace footnotes with <d-footnote>
  $('.footnote-ref').each(function(i, val) {
    appendix = true;
    var href = $(this).attr('href');
    var id = href.replace('#', '');
    var fn = $('#' + id);
    var fn_p = $('#' + id + '>p');
    fn_p.find('.footnote-back').remove();
    var text = fn_p.html();
    var dtfn = $('<d-footnote></d-footnote>');
    dtfn.html(text);
    $(this).replaceWith(dtfn);
  });
  // remove footnotes
  $('.footnotes').remove();

  // move refs into #references-listing
  $('#references-listing').replaceWith($('#refs'));

  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    var id = $(this).attr('id');
    $('.d-contents a[href="#' + id + '"]').parent().remove();
    appendix = true;
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
  });

  // show d-appendix if we have appendix content
  $("d-appendix").css('display', appendix ? 'grid' : 'none');

  // localize layout chunks to just output
  $('.layout-chunk').each(function(i, val) {

    // capture layout
    var layout = $(this).attr('data-layout');

    // apply layout to markdown level block elements
    var elements = $(this).children().not('details, div.sourceCode, pre, script');
    elements.each(function(i, el) {
      var layout_div = $('<div class="' + layout + '"></div>');
      if (layout_div.hasClass('shaded')) {
        var shaded_content = $('<div class="shaded-content"></div>');
        $(this).wrap(shaded_content);
        $(this).parent().wrap(layout_div);
      } else {
        $(this).wrap(layout_div);
      }
    });


    // unwrap the layout-chunk div
    $(this).children().unwrap();
  });

  // remove code block used to force  highlighting css
  $('.distill-force-highlighting-css').parent().remove();

  // remove empty line numbers inserted by pandoc when using a
  // custom syntax highlighting theme, except when numbering line
  // in code chunk
  $('pre:not(.numberLines) code.sourceCode a:empty').remove();

  // load distill framework
  load_distill_framework();

  // wait for window.distillRunlevel == 4 to do post processing
  function distill_post_process() {

    if (!window.distillRunlevel || window.distillRunlevel < 4)
      return;

    // hide author/affiliations entirely if we have no authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
    if (!have_authors)
      $('d-byline').addClass('hidden');

    // article with toc class
    $('.d-contents').parent().addClass('d-article-with-toc');

    // strip links that point to #
    $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

    // add orcid ids
    $('.authors-affiliations').find('.author').each(function(i, el) {
      var orcid_id = front_matter.authors[i].orcidID;
      var author_name = front_matter.authors[i].author
      if (orcid_id) {
        var a = $('<a></a>');
        a.attr('href', 'https://orcid.org/' + orcid_id);
        var img = $('<img></img>');
        img.addClass('orcid-id');
        img.attr('alt', author_name ? 'ORCID ID for ' + author_name : 'ORCID ID');
        img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
        a.append(img);
        $(this).append(a);
      }
    });

    // hide elements of author/affiliations grid that have no value
    function hide_byline_column(caption) {
      $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
    }

    // affiliations
    var have_affiliations = false;
    for (var i = 0; i<front_matter.authors.length; ++i) {
      var author = front_matter.authors[i];
      if (author.affiliation !== "&nbsp;") {
        have_affiliations = true;
        break;
      }
    }
    if (!have_affiliations)
      $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

    // published date
    if (!front_matter.publishedDate)
      hide_byline_column("Published");

    // document object identifier
    var doi = $('d-byline').find('h3:contains("DOI")');
    var doi_p = doi.next().empty();
    if (!front_matter.doi) {
      // if we have a citation and valid citationText then link to that
      if ($('#citation').length > 0 && front_matter.citationText) {
        doi.html('Citation');
        $('<a href="#citation"></a>')
          .text(front_matter.citationText)
          .appendTo(doi_p);
      } else {
        hide_byline_column("DOI");
      }
    } else {
      $('<a></a>')
         .attr('href', "https://doi.org/" + front_matter.doi)
         .html(front_matter.doi)
         .appendTo(doi_p);
    }

     // change plural form of authors/affiliations
    if (front_matter.authors.length === 1) {
      var grid = $('.authors-affiliations');
      grid.children('h3:contains("Authors")').text('Author');
      grid.children('h3:contains("Affiliations")').text('Affiliation');
    }

    // remove d-appendix and d-footnote-list local styles
    $('d-appendix > style:first-child').remove();
    $('d-footnote-list > style:first-child').remove();

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // hoverable references
    $('span.citation[data-cites]').each(function() {
      const citeChild = $(this).children()[0]
      // Do not process if @xyz has been used without escaping and without bibliography activated
      // https://github.com/rstudio/distill/issues/466
      if (citeChild === undefined) return true

      if (citeChild.nodeName == "D-FOOTNOTE") {
        var fn = citeChild
        $(this).html(fn.shadowRoot.querySelector("sup"))
        $(this).id = fn.id
        fn.remove()
      }
      var refs = $(this).attr('data-cites').split(" ");
      var refHtml = refs.map(function(ref) {
        // Could use CSS.escape too here, we insure backward compatibility in navigator
        return "<p>" + $('div[id="ref-' + ref + '"]').html() + "</p>";
      }).join("\n");
      window.tippy(this, {
        allowHTML: true,
        content: refHtml,
        maxWidth: 500,
        interactive: true,
        interactiveBorder: 10,
        theme: 'light-border',
        placement: 'bottom-start'
      });
    });

    // fix footnotes in tables (#411)
    // replacing broken distill.pub feature
    $('table d-footnote').each(function() {
      // we replace internal showAtNode methode which is triggered when hovering a footnote
      this.hoverBox.showAtNode = function(node) {
        // ported from https://github.com/distillpub/template/pull/105/files
        calcOffset = function(elem) {
            let x = elem.offsetLeft;
            let y = elem.offsetTop;
            // Traverse upwards until an `absolute` element is found or `elem`
            // becomes null.
            while (elem = elem.offsetParent && elem.style.position != 'absolute') {
                x += elem.offsetLeft;
                y += elem.offsetTop;
            }

            return { left: x, top: y };
        }
        // https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/offsetTop
        const bbox = node.getBoundingClientRect();
        const offset = calcOffset(node);
        this.show([offset.left + bbox.width, offset.top + bbox.height]);
      }
    })

    // clear polling timer
    clearInterval(tid);

    // show body now that everything is ready
    on_load_complete();
  }

  var tid = setInterval(distill_post_process, 50);
  distill_post_process();

}

function init_downlevel() {

  init_common();

   // insert hr after d-title
  $('.d-title').after($('<hr class="section-separator"/>'));

  // check if we have authors
  var front_matter = JSON.parse($("#distill-front-matter").html());
  var have_authors = front_matter.authors && front_matter.authors.length > 0;

  // manage byline/border
  if (!have_authors)
    $('.d-byline').remove();
  $('.d-byline').after($('<hr class="section-separator"/>'));
  $('.d-byline a').remove();

  // remove toc
  $('.d-contents').remove();

  // move appendix elements
  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
  });


  // inject headers into references and footnotes
  var refs_header = $('<h3></h3>');
  refs_header.text('References');
  $('#refs').prepend(refs_header);

  var footnotes_header = $('<h3></h3');
  footnotes_header.text('Footnotes');
  $('.footnotes').children('hr').first().replaceWith(footnotes_header);

  // move appendix-bottom entries to the bottom
  $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
  $('.appendix-bottom').remove();

  // remove appendix if it's empty
  if ($('.d-appendix').children().length === 0)
    $('.d-appendix').remove();

  // prepend separator above appendix
  $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

  // trim code
  $('pre>code').each(function(i, val) {
    $(this).html($.trim($(this).html()));
  });

  // move posts-container right before article
  $('.posts-container').insertBefore($('.d-article'));

  $('body').addClass('downlevel');

  on_load_complete();
}


function init_common() {

  // jquery plugin to change element types
  (function($) {
    $.fn.changeElementType = function(newType) {
      var attrs = {};

      $.each(this[0].attributes, function(idx, attr) {
        attrs[attr.nodeName] = attr.nodeValue;
      });

      this.replaceWith(function() {
        return $("<" + newType + "/>", attrs).append($(this).contents());
      });
    };
  })(jQuery);

  // prevent underline for linked images
  $('a > img').parent().css({'border-bottom' : 'none'});

  // mark non-body figures created by knitr chunks as 100% width
  $('.layout-chunk').each(function(i, val) {
    var figures = $(this).find('img, .html-widget');
    // ignore leaflet img layers (#106)
    figures = figures.filter(':not(img[class*="leaflet"])')
    if ($(this).attr('data-layout') !== "l-body") {
      figures.css('width', '100%');
    } else {
      figures.css('max-width', '100%');
      figures.filter("[width]").each(function(i, val) {
        var fig = $(this);
        fig.css('width', fig.attr('width') + 'px');
      });

    }
  });

  // auto-append index.html to post-preview links in file: protocol
  // and in rstudio ide preview
  $('.post-preview').each(function(i, val) {
    if (window.location.protocol === "file:")
      $(this).attr('href', $(this).attr('href') + "index.html");
  });

  // get rid of index.html references in header
  if (window.location.protocol !== "file:") {
    $('.distill-site-header a[href]').each(function(i,val) {
      $(this).attr('href', $(this).attr('href').replace(/^index[.]html/, "./"));
    });
  }

  // add class to pandoc style tables
  $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
  $('.kable-table').children('table').addClass('pandoc-table');

  // add figcaption style to table captions
  $('caption').parent('table').addClass("figcaption");

  // initialize posts list
  if (window.init_posts_list)
    window.init_posts_list();

  // implmement disqus comment link
  $('.disqus-comment-count').click(function() {
    window.headroom_prevent_pin = true;
    $('#disqus_thread').toggleClass('hidden');
    if (!$('#disqus_thread').hasClass('hidden')) {
      var offset = $(this).offset();
      $(window).resize();
      $('html, body').animate({
        scrollTop: offset.top - 35
      });
    }
  });
}

document.addEventListener('DOMContentLoaded', function() {
  if (is_downlevel_browser())
    init_downlevel();
  else
    window.addEventListener('WebComponentsReady', init_distill);
});

</script>

<!--/radix_placeholder_distill-->
  <script src="../../site_libs/header-attrs-2.26/header-attrs.js"></script>
  <script src="../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <script src="../../site_libs/popper-2.6.0/popper.min.js"></script>
  <link href="../../site_libs/tippy-6.2.7/tippy.css" rel="stylesheet" />
  <link href="../../site_libs/tippy-6.2.7/tippy-light-border.css" rel="stylesheet" />
  <script src="../../site_libs/tippy-6.2.7/tippy.umd.min.js"></script>
  <script src="../../site_libs/anchor-4.2.2/anchor.min.js"></script>
  <script src="../../site_libs/bowser-1.9.3/bowser.min.js"></script>
  <script src="../../site_libs/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="../../site_libs/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
<script defer data-domain="blogs.rstudio.com" src="https://plausible.io/js/plausible.js"></script>
<!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Getting familiar with torch tensors","description":"In this first installment of a four-part miniseries, we present the main things you will want to know about torch tensors. As an illustrative example, we'll code a simple neural network from scratch.","authors":[{"author":"Sigrid Keydana","authorURL":"#","affiliation":"RStudio","affiliationURL":"https://www.rstudio.com/","orcidID":""}],"publishedDate":"2020-10-01T00:00:00.000+00:00","citationText":"Keydana, 2020"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<header class="header header--fixed" role="banner">
<nav class="distill-site-nav distill-site-header">
<div class="nav-left">
<span class="logo">
<img src="../../images/posit.png"/>
</span>
<a href="../../index.html" class="title">AI Blog</a>
<input id="distill-search" class="nav-search hidden" type="text" placeholder="Search..."/>
</div>
<div class="nav-right">
<a href="../../index.html">Home</a>
<a href="../../gallery.html">Gallery</a>
<a href="../../about.html">About</a>
<a href="../../contributing.html">Contributing</a>
<a href="../../index.xml">
<i class="fa fa-rss" aria-hidden="true"></i>
</a>
<a href="javascript:void(0);" class="nav-toggle">&#9776;</a>
</div>
</nav>
</header>
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Getting familiar with torch tensors</h1>

<!--radix_placeholder_categories-->
<div class="dt-tags">
  <a href="../../index.html#category:Torch" class="dt-tag">Torch</a>
  <a href="../../index.html#category:R" class="dt-tag">R</a>
</div>
<!--/radix_placeholder_categories-->
<p><p>In this first installment of a four-part miniseries, we present the main things you will want to know about torch tensors. As an illustrative example, we’ll code a simple neural network from scratch.</p></p>
</div>

<div class="d-byline">
  Sigrid Keydana  (RStudio)<a href="https://www.rstudio.com/" class="uri">https://www.rstudio.com/</a>
  
<br/>10-01-2020
</div>

<div class="d-article">
<div class="d-contents d-contents-float">
<nav class="l-text toc figcaption" id="TOC">
<h3>Contents</h3>
<ul>
<li><a href="#tensors">Tensors</a>
<ul>
<li><a href="#creation">Creation</a></li>
<li><a href="#conversion-to-built-in-r-data-types">Conversion to built-in R data types</a></li>
<li><a href="#indexing-and-slicing-tensors">Indexing and slicing tensors</a></li>
<li><a href="#reshaping-tensors">Reshaping tensors</a></li>
<li><a href="#operations-on-tensors">Operations on tensors</a></li>
</ul></li>
<li><a href="#running-on-gpu">Running on GPU</a></li>
<li><a href="#broadcasting">Broadcasting</a></li>
<li><a href="#a-simple-neural-network-using-torch-tensors">A simple neural network using <code>torch</code> tensors</a></li>
</ul>
</nav>
</div>
<p>Two days ago, I introduced <a href="https://github.com/mlverse/torch"><code>torch</code></a>, an R package that provides the native functionality that is brought to Python users by <a href="https://pytorch.org/">PyTorch</a>. In that post, I assumed basic familiarity with TensorFlow/Keras. Consequently, I portrayed <code>torch</code> in a way I figured would be helpful to someone who “grew up” with the Keras way of training a model: Aiming to focus on differences, yet not lose sight of the overall process.</p>
<p>This post now changes perspective. We code a simple neural network “from scratch”, making use of just one of <code>torch</code>’s building blocks: <em>tensors</em>. This network will be as “raw” (low-level) as can be. (For the less math-inclined people among us, it may serve as a refresher of what’s actually going on beneath all those convenience tools they built for us. But the real purpose is to illustrate what can be done with tensors alone.)</p>
<p>Subsequently, three posts will progressively show how to reduce the effort – noticeably right from the start, enormously once we finish. At the end of this mini-series, you will have seen how automatic differentiation works in <code>torch</code>, how to use <code>module</code>s (layers, in <code>keras</code> speak, and compositions thereof), and optimizers. By then, you’ll have a lot of the background desirable when applying <code>torch</code> to real-world tasks.</p>
<p>This post will be the longest, since there is a lot to learn about tensors: How to create them; how to manipulate their contents and/or modify their shapes; how to convert them to R arrays, matrices or vectors; and of course, given the omnipresent need for speed: how to get all those operations executed on the GPU. Once we’ve cleared that agenda, we code the aforementioned little network, seeing all those aspects in action.</p>
<h2 id="tensors">Tensors</h2>
<h3 id="creation">Creation</h3>
<p>Tensors may be created by specifying individual values. Here we create two one-dimensional tensors (vectors), of types <code>float</code> and <code>bool</code>, respectively:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://torch.mlverse.org/docs'>torch</a></span><span class='op'>)</span></span>
<span><span class='co'># a 1d vector of length 2</span></span>
<span><span class='va'>t</span> <span class='op'>&lt;-</span> <span class='fu'>torch_tensor</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>1</span>, <span class='fl'>2</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='va'>t</span></span>
<span></span>
<span><span class='co'># also 1d, but of type boolean</span></span>
<span><span class='va'>t</span> <span class='op'>&lt;-</span> <span class='fu'>torch_tensor</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='cn'>TRUE</span>, <span class='cn'>FALSE</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='va'>t</span></span></code></pre>
</div>
</div>
<pre><code>torch_tensor 
 1
 2
[ CPUFloatType{2} ]

torch_tensor 
 1
 0
[ CPUBoolType{2} ]</code></pre>
<p>And here are two ways to create two-dimensional tensors (matrices). Note how in the second approach, you need to specify <code>byrow = TRUE</code> in the call to <code>matrix()</code> to get values arranged in row-major order.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='co'># a 3x3 tensor (matrix)</span></span>
<span><span class='va'>t</span> <span class='op'>&lt;-</span> <span class='fu'>torch_tensor</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/cbind.html'>rbind</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>1</span>,<span class='fl'>2</span>,<span class='fl'>0</span><span class='op'>)</span>, <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>3</span>,<span class='fl'>0</span>,<span class='fl'>0</span><span class='op'>)</span>, <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>4</span>,<span class='fl'>5</span>,<span class='fl'>6</span><span class='op'>)</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='va'>t</span></span>
<span></span>
<span><span class='co'># also 3x3</span></span>
<span><span class='va'>t</span> <span class='op'>&lt;-</span> <span class='fu'>torch_tensor</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/matrix.html'>matrix</a></span><span class='op'>(</span><span class='fl'>1</span><span class='op'>:</span><span class='fl'>9</span>, ncol <span class='op'>=</span> <span class='fl'>3</span>, byrow <span class='op'>=</span> <span class='cn'>TRUE</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='va'>t</span></span></code></pre>
</div>
</div>
<pre><code>torch_tensor 
 1  2  0
 3  0  0
 4  5  6
[ CPUFloatType{3,3} ]

torch_tensor 
 1  2  3
 4  5  6
 7  8  9
[ CPULongType{3,3} ]</code></pre>
<p>In higher dimensions especially, it can be easier to specify the type of tensor abstractly, as in: “give me a tensor of &lt;…&gt; of shape n1 x n2”, where &lt;…&gt; could be “zeros”; or “ones”; or, say, “values drawn from a standard normal distribution”:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='co'># a 3x3 tensor of standard-normally distributed values</span></span>
<span><span class='va'>t</span> <span class='op'>&lt;-</span> <span class='fu'>torch_randn</span><span class='op'>(</span><span class='fl'>3</span>, <span class='fl'>3</span><span class='op'>)</span></span>
<span><span class='va'>t</span></span>
<span></span>
<span><span class='co'># a 4x2x2 (3d) tensor of zeroes</span></span>
<span><span class='va'>t</span> <span class='op'>&lt;-</span> <span class='fu'>torch_zeros</span><span class='op'>(</span><span class='fl'>4</span>, <span class='fl'>2</span>, <span class='fl'>2</span><span class='op'>)</span></span>
<span><span class='va'>t</span></span></code></pre>
</div>
</div>
<pre><code>torch_tensor 
-2.1563  1.7085  0.5245
 0.8955 -0.6854  0.2418
 0.4193 -0.7742 -1.0399
[ CPUFloatType{3,3} ]

torch_tensor 
(1,.,.) = 
  0  0
  0  0

(2,.,.) = 
  0  0
  0  0

(3,.,.) = 
  0  0
  0  0

(4,.,.) = 
  0  0
  0  0
[ CPUFloatType{4,2,2} ]</code></pre>
<p>Many similar functions exist, including, e.g., <code>torch_arange()</code> to create a tensor holding a sequence of evenly spaced values, <code>torch_eye()</code> which returns an identity matrix, and <code>torch_logspace()</code> which fills a specified range with a list of values spaced logarithmically.</p>
<p>If no <code>dtype</code> argument is specified, <code>torch</code> will infer the data type from the passed-in value(s). For example:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t</span> <span class='op'>&lt;-</span> <span class='fu'>torch_tensor</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>3</span>, <span class='fl'>5</span>, <span class='fl'>7</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='va'>t</span><span class='op'>$</span><span class='va'>dtype</span></span>
<span></span>
<span><span class='va'>t</span> <span class='op'>&lt;-</span> <span class='fu'>torch_tensor</span><span class='op'>(</span><span class='fl'>1L</span><span class='op'>)</span></span>
<span><span class='va'>t</span><span class='op'>$</span><span class='va'>dtype</span></span></code></pre>
</div>
</div>
<pre><code>torch_Float
torch_Long</code></pre>
<p>But we can explicitly request a different <code>dtype</code> if we want:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t</span> <span class='op'>&lt;-</span> <span class='fu'>torch_tensor</span><span class='op'>(</span><span class='fl'>2</span>, dtype <span class='op'>=</span> <span class='fu'>torch_double</span><span class='op'>(</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='va'>t</span><span class='op'>$</span><span class='va'>dtype</span></span></code></pre>
</div>
</div>
<pre><code>torch_Double</code></pre>
<p><code>torch</code> tensors live on a <em>device</em>. By default, this will be the CPU:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t</span><span class='op'>$</span><span class='va'>device</span></span></code></pre>
</div>
</div>
<pre><code>torch_device(type=&#39;cpu&#39;)</code></pre>
<p>But we could also define a tensor to live on the GPU:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t</span> <span class='op'>&lt;-</span> <span class='fu'>torch_tensor</span><span class='op'>(</span><span class='fl'>2</span>, device <span class='op'>=</span> <span class='st'>"cuda"</span><span class='op'>)</span></span>
<span><span class='va'>t</span><span class='op'>$</span><span class='va'>device</span></span></code></pre>
</div>
</div>
<pre><code>torch_device(type=&#39;cuda&#39;, index=0)</code></pre>
<p>We’ll talk more about devices below.</p>
<p>There is another very important parameter to the tensor-creation functions: <code>requires_grad</code>. Here though, I need to ask for your patience: This one will prominently figure in the follow-up post.</p>
<h3 id="conversion-to-built-in-r-data-types">Conversion to built-in R data types</h3>
<p>To convert <code>torch</code> tensors to R, use <code>as_array()</code>:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t</span> <span class='op'>&lt;-</span> <span class='fu'>torch_tensor</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/matrix.html'>matrix</a></span><span class='op'>(</span><span class='fl'>1</span><span class='op'>:</span><span class='fl'>9</span>, ncol <span class='op'>=</span> <span class='fl'>3</span>, byrow <span class='op'>=</span> <span class='cn'>TRUE</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='fu'>as_array</span><span class='op'>(</span><span class='va'>t</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<pre><code>     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    4    5    6
[3,]    7    8    9</code></pre>
<p>Depending on whether the tensor is one-, two-, or three-dimensional, the resulting R object will be a vector, a matrix, or an array:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t</span> <span class='op'>&lt;-</span> <span class='fu'>torch_tensor</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>1</span>, <span class='fl'>2</span>, <span class='fl'>3</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='fu'>as_array</span><span class='op'>(</span><span class='va'>t</span><span class='op'>)</span> <span class='op'>%&gt;%</span> <span class='fu'><a href='https://rdrr.io/r/base/class.html'>class</a></span><span class='op'>(</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t</span> <span class='op'>&lt;-</span> <span class='fu'>torch_ones</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>2</span>, <span class='fl'>2</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='fu'>as_array</span><span class='op'>(</span><span class='va'>t</span><span class='op'>)</span> <span class='op'>%&gt;%</span> <span class='fu'><a href='https://rdrr.io/r/base/class.html'>class</a></span><span class='op'>(</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t</span> <span class='op'>&lt;-</span> <span class='fu'>torch_ones</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>2</span>, <span class='fl'>2</span>, <span class='fl'>2</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='fu'>as_array</span><span class='op'>(</span><span class='va'>t</span><span class='op'>)</span> <span class='op'>%&gt;%</span> <span class='fu'><a href='https://rdrr.io/r/base/class.html'>class</a></span><span class='op'>(</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<pre><code>[1] &quot;numeric&quot;

[1] &quot;matrix&quot; &quot;array&quot; 

[1] &quot;array&quot;</code></pre>
<p>For one-dimensional and two-dimensional tensors, it is also possible to use <code>as.integer()</code> / <code>as.matrix()</code>. (One reason you might want to do this is to have more self-documenting code.)</p>
<p>If a tensor currently lives on the GPU, you need to move it to the CPU first:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t</span> <span class='op'>&lt;-</span> <span class='fu'>torch_tensor</span><span class='op'>(</span><span class='fl'>2</span>, device <span class='op'>=</span> <span class='st'>"cuda"</span><span class='op'>)</span></span>
<span><span class='fu'><a href='https://rdrr.io/r/base/integer.html'>as.integer</a></span><span class='op'>(</span><span class='va'>t</span><span class='op'>$</span><span class='fu'>cpu</span><span class='op'>(</span><span class='op'>)</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<pre><code>[1] 2</code></pre>
<h3 id="indexing-and-slicing-tensors">Indexing and slicing tensors</h3>
<p>Often, we want to retrieve not a complete tensor, but only some of the values it holds, or even just a single value. In these cases, we talk about <em>slicing</em> and <em>indexing</em>, respectively.</p>
<p>In R, these operations are 1-based, meaning that when we specify offsets, we assume for the very first element in an array to reside at offset <code>1</code>. The same behavior was implemented for <code>torch</code>. Thus, a lot of the functionality described in this section should feel intuitive.</p>
<p>The way I’m organizing this section is the following. We’ll inspect the intuitive parts first, where by intuitive I mean: intuitive to the R user who has not yet worked with Python’s <a href="https://numpy.org/">NumPy</a>. Then come things which, to this user, may look more surprising, but will turn out to be pretty useful.</p>
<h4 id="indexing-and-slicing-the-r-like-part">Indexing and slicing: the R-like part</h4>
<p>None of these should be overly surprising:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t</span> <span class='op'>&lt;-</span> <span class='fu'>torch_tensor</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/cbind.html'>rbind</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>1</span>,<span class='fl'>2</span>,<span class='fl'>3</span><span class='op'>)</span>, <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>4</span>,<span class='fl'>5</span>,<span class='fl'>6</span><span class='op'>)</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='va'>t</span></span>
<span></span>
<span><span class='co'># a single value</span></span>
<span><span class='va'>t</span><span class='op'>[</span><span class='fl'>1</span>, <span class='fl'>1</span><span class='op'>]</span></span>
<span></span>
<span><span class='co'># first row, all columns</span></span>
<span><span class='va'>t</span><span class='op'>[</span><span class='fl'>1</span>, <span class='op'>]</span></span>
<span></span>
<span><span class='co'># first row, a subset of columns</span></span>
<span><span class='va'>t</span><span class='op'>[</span><span class='fl'>1</span>, <span class='fl'>1</span><span class='op'>:</span><span class='fl'>2</span><span class='op'>]</span></span></code></pre>
</div>
</div>
<pre><code>torch_tensor 
 1  2  3
 4  5  6
[ CPUFloatType{2,3} ]

torch_tensor 
1
[ CPUFloatType{} ]

torch_tensor 
 1
 2
 3
[ CPUFloatType{3} ]

torch_tensor 
 1
 2
[ CPUFloatType{2} ]</code></pre>
<p>Note how, just as in R, singleton dimensions are dropped:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t</span> <span class='op'>&lt;-</span> <span class='fu'>torch_tensor</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/cbind.html'>rbind</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>1</span>,<span class='fl'>2</span>,<span class='fl'>3</span><span class='op'>)</span>, <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>4</span>,<span class='fl'>5</span>,<span class='fl'>6</span><span class='op'>)</span><span class='op'>)</span><span class='op'>)</span></span>
<span></span>
<span><span class='co'># 2x3</span></span>
<span><span class='va'>t</span><span class='op'>$</span><span class='fu'>size</span><span class='op'>(</span><span class='op'>)</span> </span>
<span></span>
<span><span class='co'># just a single row: will be returned as a vector</span></span>
<span><span class='va'>t</span><span class='op'>[</span><span class='fl'>1</span>, <span class='fl'>1</span><span class='op'>:</span><span class='fl'>2</span><span class='op'>]</span><span class='op'>$</span><span class='fu'>size</span><span class='op'>(</span><span class='op'>)</span> </span>
<span></span>
<span><span class='co'># a single element</span></span>
<span><span class='va'>t</span><span class='op'>[</span><span class='fl'>1</span>, <span class='fl'>1</span><span class='op'>]</span><span class='op'>$</span><span class='fu'>size</span><span class='op'>(</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<pre><code>[1] 2 3

[1] 2

integer(0)</code></pre>
<p>And just like in R, you can specify <code>drop = FALSE</code> to keep those dimensions:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t</span><span class='op'>[</span><span class='fl'>1</span>, <span class='fl'>1</span><span class='op'>:</span><span class='fl'>2</span>, drop <span class='op'>=</span> <span class='cn'>FALSE</span><span class='op'>]</span><span class='op'>$</span><span class='fu'>size</span><span class='op'>(</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t</span><span class='op'>[</span><span class='fl'>1</span>, <span class='fl'>1</span>, drop <span class='op'>=</span> <span class='cn'>FALSE</span><span class='op'>]</span><span class='op'>$</span><span class='fu'>size</span><span class='op'>(</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<pre><code>[1] 1 2

[1] 1 1</code></pre>
<h4 id="indexing-and-slicing-what-to-look-out-for">Indexing and slicing: What to look out for</h4>
<p>Whereas R uses negative numbers to remove elements at specified positions, in <code>torch</code> negative values indicate that we start counting from the end of a tensor – with <code>-1</code> pointing to its last element:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t</span> <span class='op'>&lt;-</span> <span class='fu'>torch_tensor</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/cbind.html'>rbind</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>1</span>,<span class='fl'>2</span>,<span class='fl'>3</span><span class='op'>)</span>, <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>4</span>,<span class='fl'>5</span>,<span class='fl'>6</span><span class='op'>)</span><span class='op'>)</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t</span><span class='op'>[</span><span class='fl'>1</span>, <span class='op'>-</span><span class='fl'>1</span><span class='op'>]</span></span>
<span></span>
<span><span class='va'>t</span><span class='op'>[</span> , <span class='op'>-</span><span class='fl'>2</span><span class='op'>:</span><span class='op'>-</span><span class='fl'>1</span><span class='op'>]</span> </span></code></pre>
</div>
</div>
<pre><code>torch_tensor 
3
[ CPUFloatType{} ]

torch_tensor 
 2  3
 5  6
[ CPUFloatType{2,2} ]</code></pre>
<p>This is a feature you might know from NumPy. Same with the following.</p>
<p>When the slicing expression <code>m:n</code> is augmented by another colon and a third number – <code>m:n:o</code> –, we will take every <code>o</code>th item from the range specified by <code>m</code> and <code>n</code>:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t</span> <span class='op'>&lt;-</span> <span class='fu'>torch_tensor</span><span class='op'>(</span><span class='fl'>1</span><span class='op'>:</span><span class='fl'>10</span><span class='op'>)</span></span>
<span><span class='va'>t</span><span class='op'>[</span><span class='fl'>2</span><span class='op'>:</span><span class='fl'>10</span><span class='op'>:</span><span class='fl'>2</span><span class='op'>]</span></span></code></pre>
</div>
</div>
<pre><code>torch_tensor 
  2
  4
  6
  8
 10
[ CPULongType{5} ]</code></pre>
<p>Sometimes we don’t know how many dimensions a tensor has, but we do know what to do with the final dimension, or the first one. To subsume all others, we can use <code>..</code>:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t</span> <span class='op'>&lt;-</span> <span class='fu'>torch_randint</span><span class='op'>(</span><span class='op'>-</span><span class='fl'>7</span>, <span class='fl'>7</span>, size <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>2</span>, <span class='fl'>2</span>, <span class='fl'>2</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='va'>t</span></span>
<span></span>
<span><span class='va'>t</span><span class='op'>[</span><span class='va'>..</span>, <span class='fl'>1</span><span class='op'>]</span></span>
<span></span>
<span><span class='va'>t</span><span class='op'>[</span><span class='fl'>2</span>, <span class='va'>..</span><span class='op'>]</span></span></code></pre>
</div>
</div>
<pre><code>torch_tensor 
(1,.,.) = 
  2 -2
 -5  4

(2,.,.) = 
  0  4
 -3 -1
[ CPUFloatType{2,2,2} ]

torch_tensor 
 2 -5
 0 -3
[ CPUFloatType{2,2} ]

torch_tensor 
 0  4
-3 -1
[ CPUFloatType{2,2} ]</code></pre>
<p>Now we move on to a topic that, in practice, is just as indispensable as slicing: changing tensor <em>shapes</em>.</p>
<h3 id="reshaping-tensors">Reshaping tensors</h3>
<p>Changes in shape can occur in two fundamentally different ways. Seeing how “reshape” really means: <em>keep the values but modify their layout</em>, we could either alter how they’re arranged physically, or keep the physical structure as-is and just change the “mapping” (a semantic change, as it were).</p>
<p>In the first case, storage will have to be allocated for two tensors, source and target, and elements will be copied from the latter to the former. In the second, physically there will be just a single tensor, referenced by two logical entities with distinct metadata.</p>
<p>Not surprisingly, for performance reasons, the second operation is preferred.</p>
<h4 id="zero-copy-reshaping">Zero-copy reshaping</h4>
<p>We start with zero-copy methods, as we’ll want to use them whenever we can.</p>
<p>A special case often seen in practice is adding or removing a singleton dimension.</p>
<p><code>unsqueeze()</code> adds a dimension of size <code>1</code> at a position specified by <code>dim</code>:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t1</span> <span class='op'>&lt;-</span> <span class='fu'>torch_randint</span><span class='op'>(</span>low <span class='op'>=</span> <span class='fl'>3</span>, high <span class='op'>=</span> <span class='fl'>7</span>, size <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>3</span>, <span class='fl'>3</span>, <span class='fl'>3</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='va'>t1</span><span class='op'>$</span><span class='fu'>size</span><span class='op'>(</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t2</span> <span class='op'>&lt;-</span> <span class='va'>t1</span><span class='op'>$</span><span class='fu'>unsqueeze</span><span class='op'>(</span>dim <span class='op'>=</span> <span class='fl'>1</span><span class='op'>)</span></span>
<span><span class='va'>t2</span><span class='op'>$</span><span class='fu'>size</span><span class='op'>(</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t3</span> <span class='op'>&lt;-</span> <span class='va'>t1</span><span class='op'>$</span><span class='fu'>unsqueeze</span><span class='op'>(</span>dim <span class='op'>=</span> <span class='fl'>2</span><span class='op'>)</span></span>
<span><span class='va'>t3</span><span class='op'>$</span><span class='fu'>size</span><span class='op'>(</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<pre><code>[1] 3 3 3

[1] 1 3 3 3

[1] 3 1 3 3</code></pre>
<p>Conversely, <code>squeeze()</code> removes singleton dimensions:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t4</span> <span class='op'>&lt;-</span> <span class='va'>t3</span><span class='op'>$</span><span class='fu'>squeeze</span><span class='op'>(</span><span class='op'>)</span></span>
<span><span class='va'>t4</span><span class='op'>$</span><span class='fu'>size</span><span class='op'>(</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<pre><code>[1] 3 3 3</code></pre>
<p>The same could be accomplished with <code>view()</code>. <code>view()</code>, however, is much more general, in that it allows you to reshape the data to any valid dimensionality. (Valid meaning: The number of elements stays the same.)</p>
<p>Here we have a <code>3x2</code> tensor that is reshaped to size <code>2x3</code>:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t1</span> <span class='op'>&lt;-</span> <span class='fu'>torch_tensor</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/cbind.html'>rbind</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>1</span>, <span class='fl'>2</span><span class='op'>)</span>, <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>3</span>, <span class='fl'>4</span><span class='op'>)</span>, <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>5</span>, <span class='fl'>6</span><span class='op'>)</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='va'>t1</span></span>
<span></span>
<span><span class='va'>t2</span> <span class='op'>&lt;-</span> <span class='va'>t1</span><span class='op'>$</span><span class='fu'>view</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>2</span>, <span class='fl'>3</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='va'>t2</span></span></code></pre>
</div>
</div>
<pre><code>torch_tensor 
 1  2
 3  4
 5  6
[ CPUFloatType{3,2} ]

torch_tensor 
 1  2  3
 4  5  6
[ CPUFloatType{2,3} ]</code></pre>
<p>(Note how this is different from matrix transposition.)</p>
<p>Instead of going from two to three dimensions, we can flatten the matrix to a vector.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t4</span> <span class='op'>&lt;-</span> <span class='va'>t1</span><span class='op'>$</span><span class='fu'>view</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='op'>-</span><span class='fl'>1</span>, <span class='fl'>6</span><span class='op'>)</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t4</span><span class='op'>$</span><span class='fu'>size</span><span class='op'>(</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t4</span></span></code></pre>
</div>
</div>
<pre><code>[1] 1 6

torch_tensor 
 1  2  3  4  5  6
[ CPUFloatType{1,6} ]</code></pre>
<p>In contrast to indexing operations, this does not drop dimensions.</p>
<p>Like we said above, operations like <code>squeeze()</code> or <code>view()</code> do not make copies. Or, put differently: The output tensor shares storage with the input tensor. We can in fact verify this ourselves:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t1</span><span class='op'>$</span><span class='fu'>storage</span><span class='op'>(</span><span class='op'>)</span><span class='op'>$</span><span class='fu'>data_ptr</span><span class='op'>(</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t2</span><span class='op'>$</span><span class='fu'>storage</span><span class='op'>(</span><span class='op'>)</span><span class='op'>$</span><span class='fu'>data_ptr</span><span class='op'>(</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<pre><code>[1] &quot;0x5648d02ac800&quot;

[1] &quot;0x5648d02ac800&quot;</code></pre>
<p>What’s different is the storage <em>metadata</em> <code>torch</code> keeps about both tensors. Here, the relevant information is the <em>stride</em>:</p>
<p>A tensor’s <code>stride()</code> method tracks, <em>for every dimension</em>, how many elements have to be traversed to arrive at its next element (row or column, in two dimensions). For <code>t1</code> above, of shape <code>3x2</code>, we have to skip over 2 items to arrive at the next row. To arrive at the next column though, in every row we just have to skip a single entry:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t1</span><span class='op'>$</span><span class='fu'>stride</span><span class='op'>(</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<pre><code>[1] 2 1</code></pre>
<p>For <code>t2</code>, of shape <code>3x2</code>, the distance between column elements is the same, but the distance between rows is now 3:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t2</span><span class='op'>$</span><span class='fu'>stride</span><span class='op'>(</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<pre><code>[1] 3 1</code></pre>
<p>While zero-copy operations are optimal, there are cases where they won’t work.</p>
<p>With <code>view()</code>, this can happen when a tensor was obtained via an operation – other than <code>view()</code> itself – that itself has already modified the <em>stride</em>. One example would be <code>transpose()</code>:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t1</span> <span class='op'>&lt;-</span> <span class='fu'>torch_tensor</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/cbind.html'>rbind</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>1</span>, <span class='fl'>2</span><span class='op'>)</span>, <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>3</span>, <span class='fl'>4</span><span class='op'>)</span>, <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>5</span>, <span class='fl'>6</span><span class='op'>)</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='va'>t1</span></span>
<span><span class='va'>t1</span><span class='op'>$</span><span class='fu'>stride</span><span class='op'>(</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t2</span> <span class='op'>&lt;-</span> <span class='va'>t1</span><span class='op'>$</span><span class='fu'>t</span><span class='op'>(</span><span class='op'>)</span></span>
<span><span class='va'>t2</span></span>
<span><span class='va'>t2</span><span class='op'>$</span><span class='fu'>stride</span><span class='op'>(</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<pre><code>torch_tensor 
 1  2
 3  4
 5  6
[ CPUFloatType{3,2} ]

[1] 2 1

torch_tensor 
 1  3  5
 2  4  6
[ CPUFloatType{2,3} ]

[1] 1 2</code></pre>
<p>In <code>torch</code> lingo, tensors – like <code>t2</code> – that re-use existing storage (and just read it differently), are said not to be “contiguous”<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. One way to reshape them is to use <code>contiguous()</code> on them before. We’ll see this in the next subsection.</p>
<h4 id="reshape-with-copy">Reshape with copy</h4>
<p>In the following snippet, trying to reshape <code>t2</code> using <code>view()</code> fails, as it already carries information indicating that the underlying data should not be read in physical order.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t1</span> <span class='op'>&lt;-</span> <span class='fu'>torch_tensor</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/cbind.html'>rbind</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>1</span>, <span class='fl'>2</span><span class='op'>)</span>, <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>3</span>, <span class='fl'>4</span><span class='op'>)</span>, <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>5</span>, <span class='fl'>6</span><span class='op'>)</span><span class='op'>)</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t2</span> <span class='op'>&lt;-</span> <span class='va'>t1</span><span class='op'>$</span><span class='fu'>t</span><span class='op'>(</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t2</span><span class='op'>$</span><span class='fu'>view</span><span class='op'>(</span><span class='fl'>6</span><span class='op'>)</span> <span class='co'># error!</span></span></code></pre>
</div>
</div>
<pre><code>Error in (function (self, size)  : 
  view size is not compatible with input tensor&#39;s size and stride (at least one dimension spans across two contiguous subspaces).
  Use .reshape(...) instead. (view at ../aten/src/ATen/native/TensorShape.cpp:1364)</code></pre>
<p>However, if we first call <code>contiguous()</code> on it, a <em>new tensor</em> is created, which may then be (virtually) reshaped using <code>view()</code>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t3</span> <span class='op'>&lt;-</span> <span class='va'>t2</span><span class='op'>$</span><span class='fu'>contiguous</span><span class='op'>(</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t3</span><span class='op'>$</span><span class='fu'>view</span><span class='op'>(</span><span class='fl'>6</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<pre><code>torch_tensor 
 1
 3
 5
 2
 4
 6
[ CPUFloatType{6} ]</code></pre>
<p>Alternatively, we can use <code>reshape()</code>. <code>reshape()</code> defaults to <code>view()</code>-like behavior if possible; otherwise it will create a physical copy.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t2</span><span class='op'>$</span><span class='fu'>storage</span><span class='op'>(</span><span class='op'>)</span><span class='op'>$</span><span class='fu'>data_ptr</span><span class='op'>(</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t4</span> <span class='op'>&lt;-</span> <span class='va'>t2</span><span class='op'>$</span><span class='fu'>reshape</span><span class='op'>(</span><span class='fl'>6</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t4</span><span class='op'>$</span><span class='fu'>storage</span><span class='op'>(</span><span class='op'>)</span><span class='op'>$</span><span class='fu'>data_ptr</span><span class='op'>(</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<pre><code>[1] &quot;0x5648d49b4f40&quot;

[1] &quot;0x5648d2752980&quot;</code></pre>
<h3 id="operations-on-tensors">Operations on tensors</h3>
<p>Unsurprisingly, <code>torch</code> provides a bunch of mathematical operations on tensors; we’ll see some of them in the network code below, and you’ll encounter lots more when you continue your <code>torch</code> journey. Here, we quickly take a look at the overall tensor method semantics.</p>
<p>Tensor methods normally return references to new objects. Here, we add to <code>t1</code> a clone of itself:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t1</span> <span class='op'>&lt;-</span> <span class='fu'>torch_tensor</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/cbind.html'>rbind</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>1</span>, <span class='fl'>2</span><span class='op'>)</span>, <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>3</span>, <span class='fl'>4</span><span class='op'>)</span>, <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>5</span>, <span class='fl'>6</span><span class='op'>)</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='va'>t2</span> <span class='op'>&lt;-</span> <span class='va'>t1</span><span class='op'>$</span><span class='fu'>clone</span><span class='op'>(</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t1</span><span class='op'>$</span><span class='fu'>add</span><span class='op'>(</span><span class='va'>t2</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<pre><code>torch_tensor 
  2   4
  6   8
 10  12
[ CPUFloatType{3,2} ]</code></pre>
<p>In this process, <code>t1</code> has not been modified:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t1</span></span></code></pre>
</div>
</div>
<pre><code>torch_tensor 
 1  2
 3  4
 5  6
[ CPUFloatType{3,2} ]</code></pre>
<p>Many tensor methods have variants for mutating operations. These all carry a trailing underscore:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t1</span><span class='op'>$</span><span class='fu'>add_</span><span class='op'>(</span><span class='va'>t1</span><span class='op'>)</span></span>
<span></span>
<span><span class='co'># now t1 has been modified</span></span>
<span><span class='va'>t1</span></span></code></pre>
</div>
</div>
<pre><code>torch_tensor 
  4   8
 12  16
 20  24
[ CPUFloatType{3,2} ]

torch_tensor 
  4   8
 12  16
 20  24
[ CPUFloatType{3,2} ]</code></pre>
<p>Alternatively, you can of course assign the new object to a new reference variable:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t3</span> <span class='op'>&lt;-</span> <span class='va'>t1</span><span class='op'>$</span><span class='fu'>add</span><span class='op'>(</span><span class='va'>t1</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t3</span></span></code></pre>
</div>
</div>
<pre><code>torch_tensor 
  8  16
 24  32
 40  48
[ CPUFloatType{3,2} ]</code></pre>
<p>There is one thing we need to discuss before we wrap up our introduction to tensors: How can we have all those operations executed on the GPU?</p>
<h2 id="running-on-gpu">Running on GPU</h2>
<p>To check if your GPU(s) is/are visible to torch, run</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='fu'>cuda_is_available</span><span class='op'>(</span><span class='op'>)</span></span>
<span></span>
<span><span class='fu'>cuda_device_count</span><span class='op'>(</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<pre><code>[1] TRUE

[1] 1</code></pre>
<p>Tensors may be requested to live on the GPU right at creation:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>device</span> <span class='op'>&lt;-</span> <span class='fu'>torch_device</span><span class='op'>(</span><span class='st'>"cuda"</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t</span> <span class='op'>&lt;-</span> <span class='fu'>torch_ones</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>2</span>, <span class='fl'>2</span><span class='op'>)</span>, device <span class='op'>=</span> <span class='va'>device</span><span class='op'>)</span> </span></code></pre>
</div>
</div>
<p>Alternatively, they can be moved between devices at any time:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t2</span> <span class='op'>&lt;-</span> <span class='va'>t</span><span class='op'>$</span><span class='fu'>cuda</span><span class='op'>(</span><span class='op'>)</span></span>
<span><span class='va'>t2</span><span class='op'>$</span><span class='va'>device</span></span></code></pre>
</div>
</div>
<pre><code>torch_device(type=&#39;cuda&#39;, index=0)</code></pre>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t3</span> <span class='op'>&lt;-</span> <span class='va'>t2</span><span class='op'>$</span><span class='fu'>cpu</span><span class='op'>(</span><span class='op'>)</span></span>
<span><span class='va'>t3</span><span class='op'>$</span><span class='va'>device</span></span></code></pre>
</div>
</div>
<pre><code>torch_device(type=&#39;cpu&#39;)</code></pre>
<p>That’s it for our discussion on tensors — almost. There is one <code>torch</code> feature that, although related to tensor operations, deserves special mention. It is called broadcasting, and “bilingual” (R + Python) users will know it from NumPy.</p>
<h2 id="broadcasting">Broadcasting</h2>
<p>We often have to perform operations on tensors with shapes that don’t match exactly.</p>
<p>Unsurprisingly, we can add a scalar to a tensor:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t1</span> <span class='op'>&lt;-</span> <span class='fu'>torch_randn</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>3</span>,<span class='fl'>5</span><span class='op'>)</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t1</span> <span class='op'>+</span> <span class='fl'>22</span></span></code></pre>
</div>
</div>
<pre><code>torch_tensor 
 23.1097  21.4425  22.7732  22.2973  21.4128
 22.6936  21.8829  21.1463  21.6781  21.0827
 22.5672  21.2210  21.2344  23.1154  20.5004
[ CPUFloatType{3,5} ]</code></pre>
<p>The same will work if we add tensor of size <code>1</code>:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t1</span> <span class='op'>&lt;-</span> <span class='fu'>torch_randn</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>3</span>,<span class='fl'>5</span><span class='op'>)</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t1</span> <span class='op'>+</span> <span class='fu'>torch_tensor</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>22</span><span class='op'>)</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>Adding tensors of different sizes normally won’t work:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t1</span> <span class='op'>&lt;-</span> <span class='fu'>torch_randn</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>3</span>,<span class='fl'>5</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='va'>t2</span> <span class='op'>&lt;-</span> <span class='fu'>torch_randn</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>5</span>,<span class='fl'>5</span><span class='op'>)</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t1</span><span class='op'>$</span><span class='fu'>add</span><span class='op'>(</span><span class='va'>t2</span><span class='op'>)</span> <span class='co'># error</span></span></code></pre>
</div>
</div>
<pre><code>Error in (function (self, other, alpha)  : 
  The size of tensor a (2) must match the size of tensor b (5) at non-singleton dimension 1 (infer_size at ../aten/src/ATen/ExpandUtils.cpp:24)</code></pre>
<p>However, under certain conditions, one or both tensors may be virtually expanded so both tensors line up. This behavior is what is meant by <em>broadcasting</em>. The way it works in <code>torch</code> is not just inspired by, but actually identical to that of NumPy.</p>
<p>The rules are:</p>
<ol type="1">
<li><p>We align array shapes, <em>starting from the right</em>.</p>
<p>Say we have two tensors, one of size <code>8x1x6x1</code>, the other of size <code>7x1x5</code>.</p>
<p>Here they are, right-aligned:</p></li>
</ol>
<!-- -->
<pre><code># t1, shape:     8  1  6  1
# t2, shape:        7  1  5</code></pre>
<ol start="2" type="1">
<li><p><em>Starting to look from the right</em>, the sizes along aligned axes either have to match exactly, or one of them has to be equal to <code>1</code>: in which case the latter is <em>broadcast</em> to the larger one.</p>
<p>In the above example, this is the case for the second-from-last dimension. This now gives</p></li>
</ol>
<!-- -->
<pre><code># t1, shape:     8  1  6  1
# t2, shape:        7  6  5</code></pre>
<p>, with broadcasting happening in <code>t2</code>.</p>
<ol start="3" type="1">
<li><p>If on the left, one of the arrays has an additional axis (or more than one), the other is virtually expanded to have a size of <code>1</code> in that place, in which case broadcasting will happen as stated in (2).</p>
<p>This is the case with <code>t1</code>’s leftmost dimension. First, there is a virtual expansion</p></li>
</ol>
<!-- -->
<pre><code># t1, shape:     8  1  6  1
# t2, shape:     1  7  1  5</code></pre>
<p>and then, broadcasting happens:</p>
<pre><code># t1, shape:     8  1  6  1
# t2, shape:     8  7  1  5</code></pre>
<p>According to these rules, our above example</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t1</span> <span class='op'>&lt;-</span> <span class='fu'>torch_randn</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>3</span>,<span class='fl'>5</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='va'>t2</span> <span class='op'>&lt;-</span> <span class='fu'>torch_randn</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>5</span>,<span class='fl'>5</span><span class='op'>)</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t1</span><span class='op'>$</span><span class='fu'>add</span><span class='op'>(</span><span class='va'>t2</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>could be modified in various ways that would allow for adding two tensors.</p>
<p>For example, if <code>t2</code> were <code>1x5</code>, it would only need to get broadcast to size <code>3x5</code> before the addition operation:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t1</span> <span class='op'>&lt;-</span> <span class='fu'>torch_randn</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>3</span>,<span class='fl'>5</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='va'>t2</span> <span class='op'>&lt;-</span> <span class='fu'>torch_randn</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>1</span>,<span class='fl'>5</span><span class='op'>)</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t1</span><span class='op'>$</span><span class='fu'>add</span><span class='op'>(</span><span class='va'>t2</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<pre><code>torch_tensor 
-1.0505  1.5811  1.1956 -0.0445  0.5373
 0.0779  2.4273  2.1518 -0.6136  2.6295
 0.1386 -0.6107 -1.2527 -1.3256 -0.1009
[ CPUFloatType{3,5} ]</code></pre>
<p>If it were of size <code>5</code>, a virtual leading dimension would be added, and then, the same broadcasting would take place as in the previous case.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t1</span> <span class='op'>&lt;-</span> <span class='fu'>torch_randn</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>3</span>,<span class='fl'>5</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='va'>t2</span> <span class='op'>&lt;-</span> <span class='fu'>torch_randn</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>5</span><span class='op'>)</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t1</span><span class='op'>$</span><span class='fu'>add</span><span class='op'>(</span><span class='va'>t2</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<pre><code>torch_tensor 
-1.4123  2.1392 -0.9891  1.1636 -1.4960
 0.8147  1.0368 -2.6144  0.6075 -2.0776
-2.3502  1.4165  0.4651 -0.8816 -1.0685
[ CPUFloatType{3,5} ]</code></pre>
<p>Here is a more complex example. Broadcasting how happens both in <code>t1</code> and in <code>t2</code>:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t1</span> <span class='op'>&lt;-</span> <span class='fu'>torch_randn</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>1</span>,<span class='fl'>5</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='va'>t2</span> <span class='op'>&lt;-</span> <span class='fu'>torch_randn</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>3</span>,<span class='fl'>1</span><span class='op'>)</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t1</span><span class='op'>$</span><span class='fu'>add</span><span class='op'>(</span><span class='va'>t2</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<pre><code>torch_tensor 
 1.2274  1.1880  0.8531  1.8511 -0.0627
 0.2639  0.2246 -0.1103  0.8877 -1.0262
-1.5951 -1.6344 -1.9693 -0.9713 -2.8852
[ CPUFloatType{3,5} ]</code></pre>
<p>As a nice concluding example, through broadcasting an outer product can be computed like so:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>t1</span> <span class='op'>&lt;-</span> <span class='fu'>torch_tensor</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>0</span>, <span class='fl'>10</span>, <span class='fl'>20</span>, <span class='fl'>30</span><span class='op'>)</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t2</span> <span class='op'>&lt;-</span> <span class='fu'>torch_tensor</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>1</span>, <span class='fl'>2</span>, <span class='fl'>3</span><span class='op'>)</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>t1</span><span class='op'>$</span><span class='fu'>view</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>4</span>,<span class='fl'>1</span><span class='op'>)</span><span class='op'>)</span> <span class='op'>*</span> <span class='va'>t2</span></span></code></pre>
</div>
</div>
<pre><code>torch_tensor 
  0   0   0
 10  20  30
 20  40  60
 30  60  90
[ CPUFloatType{4,3} ]</code></pre>
<p>And now, we really get to implementing that neural network!</p>
<h2 id="a-simple-neural-network-using-torch-tensors">A simple neural network using <code>torch</code> tensors</h2>
<p>Our task, which we approach in a low-level way today but considerably simplify in upcoming installments, consists of regressing a single target datum based on three input variables.</p>
<p>We directly use <code>torch</code> to simulate some data.</p>
<h4 id="toy-data">Toy data</h4>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://torch.mlverse.org/docs'>torch</a></span><span class='op'>)</span></span>
<span></span>
<span><span class='co'># input dimensionality (number of input features)</span></span>
<span><span class='va'>d_in</span> <span class='op'>&lt;-</span> <span class='fl'>3</span></span>
<span><span class='co'># output dimensionality (number of predicted features)</span></span>
<span><span class='va'>d_out</span> <span class='op'>&lt;-</span> <span class='fl'>1</span></span>
<span><span class='co'># number of observations in training set</span></span>
<span><span class='va'>n</span> <span class='op'>&lt;-</span> <span class='fl'>100</span></span>
<span></span>
<span></span>
<span><span class='co'># create random data</span></span>
<span><span class='co'># input</span></span>
<span><span class='va'>x</span> <span class='op'>&lt;-</span> <span class='fu'>torch_randn</span><span class='op'>(</span><span class='va'>n</span>, <span class='va'>d_in</span><span class='op'>)</span></span>
<span><span class='co'># target</span></span>
<span><span class='va'>y</span> <span class='op'>&lt;-</span> <span class='va'>x</span><span class='op'>[</span>, <span class='fl'>1</span>, drop <span class='op'>=</span> <span class='cn'>FALSE</span><span class='op'>]</span> <span class='op'>*</span> <span class='fl'>0.2</span> <span class='op'>-</span></span>
<span>  <span class='va'>x</span><span class='op'>[</span>, <span class='fl'>2</span>, drop <span class='op'>=</span> <span class='cn'>FALSE</span><span class='op'>]</span> <span class='op'>*</span> <span class='fl'>1.3</span> <span class='op'>-</span></span>
<span>  <span class='va'>x</span><span class='op'>[</span>, <span class='fl'>3</span>, drop <span class='op'>=</span> <span class='cn'>FALSE</span><span class='op'>]</span> <span class='op'>*</span> <span class='fl'>0.5</span> <span class='op'>+</span></span>
<span>  <span class='fu'>torch_randn</span><span class='op'>(</span><span class='va'>n</span>, <span class='fl'>1</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>Next, we need to initialize the network’s weights. We’ll have one hidden layer, with <code>32</code> units. The output layer’s size, being determined by the task, is equal to <code>1</code>.</p>
<h4 id="initialize-weights">Initialize weights</h4>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='co'># dimensionality of hidden layer</span></span>
<span><span class='va'>d_hidden</span> <span class='op'>&lt;-</span> <span class='fl'>32</span></span>
<span></span>
<span><span class='co'># weights connecting input to hidden layer</span></span>
<span><span class='va'>w1</span> <span class='op'>&lt;-</span> <span class='fu'>torch_randn</span><span class='op'>(</span><span class='va'>d_in</span>, <span class='va'>d_hidden</span><span class='op'>)</span></span>
<span><span class='co'># weights connecting hidden to output layer</span></span>
<span><span class='va'>w2</span> <span class='op'>&lt;-</span> <span class='fu'>torch_randn</span><span class='op'>(</span><span class='va'>d_hidden</span>, <span class='va'>d_out</span><span class='op'>)</span></span>
<span></span>
<span><span class='co'># hidden layer bias</span></span>
<span><span class='va'>b1</span> <span class='op'>&lt;-</span> <span class='fu'>torch_zeros</span><span class='op'>(</span><span class='fl'>1</span>, <span class='va'>d_hidden</span><span class='op'>)</span></span>
<span><span class='co'># output layer bias</span></span>
<span><span class='va'>b2</span> <span class='op'>&lt;-</span> <span class='fu'>torch_zeros</span><span class='op'>(</span><span class='fl'>1</span>, <span class='va'>d_out</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>Now for the training loop proper. The training loop here really <em>is</em> the network.</p>
<h4 id="training-loop">Training loop</h4>
<p>In each iteration (“epoch”), the training loop does four things:</p>
<ul>
<li><p>runs through the network, computing predictions (<em>forward pass)</em></p></li>
<li><p>compares those predictions to the ground truth and quantify the loss</p></li>
<li><p>runs backwards through the network, computing the gradients that indicate how the weights should be changed</p></li>
<li><p>updates the weights, making use of the requested learning rate.</p></li>
</ul>
<p>Here is the template we’re going to fill:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='kw'>for</span> <span class='op'>(</span><span class='va'>t</span> <span class='kw'>in</span> <span class='fl'>1</span><span class='op'>:</span><span class='fl'>200</span><span class='op'>)</span> <span class='op'>{</span></span>
<span>    </span>
<span>    <span class='co'>### -------- Forward pass -------- </span></span>
<span>    </span>
<span>    <span class='co'># here we'll compute the prediction</span></span>
<span>    </span>
<span>    </span>
<span>    <span class='co'>### -------- compute loss -------- </span></span>
<span>    </span>
<span>    <span class='co'># here we'll compute the sum of squared errors</span></span>
<span>    </span>
<span></span>
<span>    <span class='co'>### -------- Backpropagation -------- </span></span>
<span>    </span>
<span>    <span class='co'># here we'll pass through the network, calculating the required gradients</span></span>
<span>    </span>
<span></span>
<span>    <span class='co'>### -------- Update weights -------- </span></span>
<span>    </span>
<span>    <span class='co'># here we'll update the weights, subtracting portion of the gradients </span></span>
<span><span class='op'>}</span></span></code></pre>
</div>
</div>
<p>The forward pass effectuates two affine transformations, one each for the hidden and output layers. In-between, ReLU activation is applied:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span>  <span class='co'># compute pre-activations of hidden layers (dim: 100 x 32)</span></span>
<span>  <span class='co'># torch_mm does matrix multiplication</span></span>
<span>  <span class='va'>h</span> <span class='op'>&lt;-</span> <span class='va'>x</span><span class='op'>$</span><span class='fu'>mm</span><span class='op'>(</span><span class='va'>w1</span><span class='op'>)</span> <span class='op'>+</span> <span class='va'>b1</span></span>
<span>  </span>
<span>  <span class='co'># apply activation function (dim: 100 x 32)</span></span>
<span>  <span class='co'># torch_clamp cuts off values below/above given thresholds</span></span>
<span>  <span class='va'>h_relu</span> <span class='op'>&lt;-</span> <span class='va'>h</span><span class='op'>$</span><span class='fu'>clamp</span><span class='op'>(</span>min <span class='op'>=</span> <span class='fl'>0</span><span class='op'>)</span></span>
<span>  </span>
<span>  <span class='co'># compute output (dim: 100 x 1)</span></span>
<span>  <span class='va'>y_pred</span> <span class='op'>&lt;-</span> <span class='va'>h_relu</span><span class='op'>$</span><span class='fu'>mm</span><span class='op'>(</span><span class='va'>w2</span><span class='op'>)</span> <span class='op'>+</span> <span class='va'>b2</span></span></code></pre>
</div>
</div>
<p>Our loss here is mean squared error:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span>  <span class='va'>loss</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/base/numeric.html'>as.numeric</a></span><span class='op'>(</span><span class='op'>(</span><span class='va'>y_pred</span> <span class='op'>-</span> <span class='va'>y</span><span class='op'>)</span><span class='op'>$</span><span class='fu'>pow</span><span class='op'>(</span><span class='fl'>2</span><span class='op'>)</span><span class='op'>$</span><span class='fu'>sum</span><span class='op'>(</span><span class='op'>)</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>Calculating gradients the manual way is a bit tedious<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>, but it can be done:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span>  <span class='co'># gradient of loss w.r.t. prediction (dim: 100 x 1)</span></span>
<span>  <span class='va'>grad_y_pred</span> <span class='op'>&lt;-</span> <span class='fl'>2</span> <span class='op'>*</span> <span class='op'>(</span><span class='va'>y_pred</span> <span class='op'>-</span> <span class='va'>y</span><span class='op'>)</span></span>
<span>  <span class='co'># gradient of loss w.r.t. w2 (dim: 32 x 1)</span></span>
<span>  <span class='va'>grad_w2</span> <span class='op'>&lt;-</span> <span class='va'>h_relu</span><span class='op'>$</span><span class='fu'>t</span><span class='op'>(</span><span class='op'>)</span><span class='op'>$</span><span class='fu'>mm</span><span class='op'>(</span><span class='va'>grad_y_pred</span><span class='op'>)</span></span>
<span>  <span class='co'># gradient of loss w.r.t. hidden activation (dim: 100 x 32)</span></span>
<span>  <span class='va'>grad_h_relu</span> <span class='op'>&lt;-</span> <span class='va'>grad_y_pred</span><span class='op'>$</span><span class='fu'>mm</span><span class='op'>(</span><span class='va'>w2</span><span class='op'>$</span><span class='fu'>t</span><span class='op'>(</span><span class='op'>)</span><span class='op'>)</span></span>
<span>  <span class='co'># gradient of loss w.r.t. hidden pre-activation (dim: 100 x 32)</span></span>
<span>  <span class='va'>grad_h</span> <span class='op'>&lt;-</span> <span class='va'>grad_h_relu</span><span class='op'>$</span><span class='fu'>clone</span><span class='op'>(</span><span class='op'>)</span></span>
<span>  </span>
<span>  <span class='va'>grad_h</span><span class='op'>[</span><span class='va'>h</span> <span class='op'>&lt;</span> <span class='fl'>0</span><span class='op'>]</span> <span class='op'>&lt;-</span> <span class='fl'>0</span></span>
<span>  </span>
<span>  <span class='co'># gradient of loss w.r.t. b2 (shape: ())</span></span>
<span>  <span class='va'>grad_b2</span> <span class='op'>&lt;-</span> <span class='va'>grad_y_pred</span><span class='op'>$</span><span class='fu'>sum</span><span class='op'>(</span><span class='op'>)</span></span>
<span>  </span>
<span>  <span class='co'># gradient of loss w.r.t. w1 (dim: 3 x 32)</span></span>
<span>  <span class='va'>grad_w1</span> <span class='op'>&lt;-</span> <span class='va'>x</span><span class='op'>$</span><span class='fu'>t</span><span class='op'>(</span><span class='op'>)</span><span class='op'>$</span><span class='fu'>mm</span><span class='op'>(</span><span class='va'>grad_h</span><span class='op'>)</span></span>
<span>  <span class='co'># gradient of loss w.r.t. b1 (shape: (32, ))</span></span>
<span>  <span class='va'>grad_b1</span> <span class='op'>&lt;-</span> <span class='va'>grad_h</span><span class='op'>$</span><span class='fu'>sum</span><span class='op'>(</span>dim <span class='op'>=</span> <span class='fl'>1</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>The final step then uses the calculated gradients to update the weights:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span>  <span class='va'>learning_rate</span> <span class='op'>&lt;-</span> <span class='fl'>1e-4</span></span>
<span>  </span>
<span>  <span class='va'>w2</span> <span class='op'>&lt;-</span> <span class='va'>w2</span> <span class='op'>-</span> <span class='va'>learning_rate</span> <span class='op'>*</span> <span class='va'>grad_w2</span></span>
<span>  <span class='va'>b2</span> <span class='op'>&lt;-</span> <span class='va'>b2</span> <span class='op'>-</span> <span class='va'>learning_rate</span> <span class='op'>*</span> <span class='va'>grad_b2</span></span>
<span>  <span class='va'>w1</span> <span class='op'>&lt;-</span> <span class='va'>w1</span> <span class='op'>-</span> <span class='va'>learning_rate</span> <span class='op'>*</span> <span class='va'>grad_w1</span></span>
<span>  <span class='va'>b1</span> <span class='op'>&lt;-</span> <span class='va'>b1</span> <span class='op'>-</span> <span class='va'>learning_rate</span> <span class='op'>*</span> <span class='va'>grad_b1</span></span></code></pre>
</div>
</div>
<p>Let’s use these snippets to fill in the gaps in the above template, and give it a try!</p>
<h4 id="putting-it-all-together">Putting it all together</h4>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://torch.mlverse.org/docs'>torch</a></span><span class='op'>)</span></span>
<span></span>
<span><span class='co'>### generate training data -----------------------------------------------------</span></span>
<span></span>
<span><span class='co'># input dimensionality (number of input features)</span></span>
<span><span class='va'>d_in</span> <span class='op'>&lt;-</span> <span class='fl'>3</span></span>
<span><span class='co'># output dimensionality (number of predicted features)</span></span>
<span><span class='va'>d_out</span> <span class='op'>&lt;-</span> <span class='fl'>1</span></span>
<span><span class='co'># number of observations in training set</span></span>
<span><span class='va'>n</span> <span class='op'>&lt;-</span> <span class='fl'>100</span></span>
<span></span>
<span></span>
<span><span class='co'># create random data</span></span>
<span><span class='va'>x</span> <span class='op'>&lt;-</span> <span class='fu'>torch_randn</span><span class='op'>(</span><span class='va'>n</span>, <span class='va'>d_in</span><span class='op'>)</span></span>
<span><span class='va'>y</span> <span class='op'>&lt;-</span></span>
<span>  <span class='va'>x</span><span class='op'>[</span>, <span class='fl'>1</span>, <span class='cn'>NULL</span><span class='op'>]</span> <span class='op'>*</span> <span class='fl'>0.2</span> <span class='op'>-</span> <span class='va'>x</span><span class='op'>[</span>, <span class='fl'>2</span>, <span class='cn'>NULL</span><span class='op'>]</span> <span class='op'>*</span> <span class='fl'>1.3</span> <span class='op'>-</span> <span class='va'>x</span><span class='op'>[</span>, <span class='fl'>3</span>, <span class='cn'>NULL</span><span class='op'>]</span> <span class='op'>*</span> <span class='fl'>0.5</span> <span class='op'>+</span> <span class='fu'>torch_randn</span><span class='op'>(</span><span class='va'>n</span>, <span class='fl'>1</span><span class='op'>)</span></span>
<span></span>
<span></span>
<span><span class='co'>### initialize weights ---------------------------------------------------------</span></span>
<span></span>
<span><span class='co'># dimensionality of hidden layer</span></span>
<span><span class='va'>d_hidden</span> <span class='op'>&lt;-</span> <span class='fl'>32</span></span>
<span><span class='co'># weights connecting input to hidden layer</span></span>
<span><span class='va'>w1</span> <span class='op'>&lt;-</span> <span class='fu'>torch_randn</span><span class='op'>(</span><span class='va'>d_in</span>, <span class='va'>d_hidden</span><span class='op'>)</span></span>
<span><span class='co'># weights connecting hidden to output layer</span></span>
<span><span class='va'>w2</span> <span class='op'>&lt;-</span> <span class='fu'>torch_randn</span><span class='op'>(</span><span class='va'>d_hidden</span>, <span class='va'>d_out</span><span class='op'>)</span></span>
<span></span>
<span><span class='co'># hidden layer bias</span></span>
<span><span class='va'>b1</span> <span class='op'>&lt;-</span> <span class='fu'>torch_zeros</span><span class='op'>(</span><span class='fl'>1</span>, <span class='va'>d_hidden</span><span class='op'>)</span></span>
<span><span class='co'># output layer bias</span></span>
<span><span class='va'>b2</span> <span class='op'>&lt;-</span> <span class='fu'>torch_zeros</span><span class='op'>(</span><span class='fl'>1</span>, <span class='va'>d_out</span><span class='op'>)</span></span>
<span></span>
<span><span class='co'>### network parameters ---------------------------------------------------------</span></span>
<span></span>
<span><span class='va'>learning_rate</span> <span class='op'>&lt;-</span> <span class='fl'>1e-4</span></span>
<span></span>
<span><span class='co'>### training loop --------------------------------------------------------------</span></span>
<span></span>
<span><span class='kw'>for</span> <span class='op'>(</span><span class='va'>t</span> <span class='kw'>in</span> <span class='fl'>1</span><span class='op'>:</span><span class='fl'>200</span><span class='op'>)</span> <span class='op'>{</span></span>
<span>  <span class='co'>### -------- Forward pass --------</span></span>
<span>  </span>
<span>  <span class='co'># compute pre-activations of hidden layers (dim: 100 x 32)</span></span>
<span>  <span class='va'>h</span> <span class='op'>&lt;-</span> <span class='va'>x</span><span class='op'>$</span><span class='fu'>mm</span><span class='op'>(</span><span class='va'>w1</span><span class='op'>)</span> <span class='op'>+</span> <span class='va'>b1</span></span>
<span>  <span class='co'># apply activation function (dim: 100 x 32)</span></span>
<span>  <span class='va'>h_relu</span> <span class='op'>&lt;-</span> <span class='va'>h</span><span class='op'>$</span><span class='fu'>clamp</span><span class='op'>(</span>min <span class='op'>=</span> <span class='fl'>0</span><span class='op'>)</span></span>
<span>  <span class='co'># compute output (dim: 100 x 1)</span></span>
<span>  <span class='va'>y_pred</span> <span class='op'>&lt;-</span> <span class='va'>h_relu</span><span class='op'>$</span><span class='fu'>mm</span><span class='op'>(</span><span class='va'>w2</span><span class='op'>)</span> <span class='op'>+</span> <span class='va'>b2</span></span>
<span>  </span>
<span>  <span class='co'>### -------- compute loss --------</span></span>
<span></span>
<span>  <span class='va'>loss</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/base/numeric.html'>as.numeric</a></span><span class='op'>(</span><span class='op'>(</span><span class='va'>y_pred</span> <span class='op'>-</span> <span class='va'>y</span><span class='op'>)</span><span class='op'>$</span><span class='fu'>pow</span><span class='op'>(</span><span class='fl'>2</span><span class='op'>)</span><span class='op'>$</span><span class='fu'>sum</span><span class='op'>(</span><span class='op'>)</span><span class='op'>)</span></span>
<span>  </span>
<span>  <span class='kw'>if</span> <span class='op'>(</span><span class='va'>t</span> <span class='op'><a href='https://rdrr.io/r/base/Arithmetic.html'>%%</a></span> <span class='fl'>10</span> <span class='op'>==</span> <span class='fl'>0</span><span class='op'>)</span></span>
<span>    <span class='fu'><a href='https://rdrr.io/r/base/cat.html'>cat</a></span><span class='op'>(</span><span class='st'>"Epoch: "</span>, <span class='va'>t</span>, <span class='st'>"   Loss: "</span>, <span class='va'>loss</span>, <span class='st'>"\n"</span><span class='op'>)</span></span>
<span>  </span>
<span>  <span class='co'>### -------- Backpropagation --------</span></span>
<span>  </span>
<span>  <span class='co'># gradient of loss w.r.t. prediction (dim: 100 x 1)</span></span>
<span>  <span class='va'>grad_y_pred</span> <span class='op'>&lt;-</span> <span class='fl'>2</span> <span class='op'>*</span> <span class='op'>(</span><span class='va'>y_pred</span> <span class='op'>-</span> <span class='va'>y</span><span class='op'>)</span></span>
<span>  <span class='co'># gradient of loss w.r.t. w2 (dim: 32 x 1)</span></span>
<span>  <span class='va'>grad_w2</span> <span class='op'>&lt;-</span> <span class='va'>h_relu</span><span class='op'>$</span><span class='fu'>t</span><span class='op'>(</span><span class='op'>)</span><span class='op'>$</span><span class='fu'>mm</span><span class='op'>(</span><span class='va'>grad_y_pred</span><span class='op'>)</span></span>
<span>  <span class='co'># gradient of loss w.r.t. hidden activation (dim: 100 x 32)</span></span>
<span>  <span class='va'>grad_h_relu</span> <span class='op'>&lt;-</span> <span class='va'>grad_y_pred</span><span class='op'>$</span><span class='fu'>mm</span><span class='op'>(</span></span>
<span>    <span class='va'>w2</span><span class='op'>$</span><span class='fu'>t</span><span class='op'>(</span><span class='op'>)</span><span class='op'>)</span></span>
<span>  <span class='co'># gradient of loss w.r.t. hidden pre-activation (dim: 100 x 32)</span></span>
<span>  <span class='va'>grad_h</span> <span class='op'>&lt;-</span> <span class='va'>grad_h_relu</span><span class='op'>$</span><span class='fu'>clone</span><span class='op'>(</span><span class='op'>)</span></span>
<span>  </span>
<span>  <span class='va'>grad_h</span><span class='op'>[</span><span class='va'>h</span> <span class='op'>&lt;</span> <span class='fl'>0</span><span class='op'>]</span> <span class='op'>&lt;-</span> <span class='fl'>0</span></span>
<span>  </span>
<span>  <span class='co'># gradient of loss w.r.t. b2 (shape: ())</span></span>
<span>  <span class='va'>grad_b2</span> <span class='op'>&lt;-</span> <span class='va'>grad_y_pred</span><span class='op'>$</span><span class='fu'>sum</span><span class='op'>(</span><span class='op'>)</span></span>
<span>  </span>
<span>  <span class='co'># gradient of loss w.r.t. w1 (dim: 3 x 32)</span></span>
<span>  <span class='va'>grad_w1</span> <span class='op'>&lt;-</span> <span class='va'>x</span><span class='op'>$</span><span class='fu'>t</span><span class='op'>(</span><span class='op'>)</span><span class='op'>$</span><span class='fu'>mm</span><span class='op'>(</span><span class='va'>grad_h</span><span class='op'>)</span></span>
<span>  <span class='co'># gradient of loss w.r.t. b1 (shape: (32, ))</span></span>
<span>  <span class='va'>grad_b1</span> <span class='op'>&lt;-</span> <span class='va'>grad_h</span><span class='op'>$</span><span class='fu'>sum</span><span class='op'>(</span>dim <span class='op'>=</span> <span class='fl'>1</span><span class='op'>)</span></span>
<span>  </span>
<span>  <span class='co'>### -------- Update weights --------</span></span>
<span>  </span>
<span>  <span class='va'>w2</span> <span class='op'>&lt;-</span> <span class='va'>w2</span> <span class='op'>-</span> <span class='va'>learning_rate</span> <span class='op'>*</span> <span class='va'>grad_w2</span></span>
<span>  <span class='va'>b2</span> <span class='op'>&lt;-</span> <span class='va'>b2</span> <span class='op'>-</span> <span class='va'>learning_rate</span> <span class='op'>*</span> <span class='va'>grad_b2</span></span>
<span>  <span class='va'>w1</span> <span class='op'>&lt;-</span> <span class='va'>w1</span> <span class='op'>-</span> <span class='va'>learning_rate</span> <span class='op'>*</span> <span class='va'>grad_w1</span></span>
<span>  <span class='va'>b1</span> <span class='op'>&lt;-</span> <span class='va'>b1</span> <span class='op'>-</span> <span class='va'>learning_rate</span> <span class='op'>*</span> <span class='va'>grad_b1</span></span>
<span>  </span>
<span><span class='op'>}</span></span></code></pre>
</div>
</div>
<pre><code>Epoch:  10     Loss:  352.3585 
Epoch:  20     Loss:  219.3624 
Epoch:  30     Loss:  155.2307 
Epoch:  40     Loss:  124.5716 
Epoch:  50     Loss:  109.2687 
Epoch:  60     Loss:  100.1543 
Epoch:  70     Loss:  94.77817 
Epoch:  80     Loss:  91.57003 
Epoch:  90     Loss:  89.37974 
Epoch:  100    Loss:  87.64617 
Epoch:  110    Loss:  86.3077 
Epoch:  120    Loss:  85.25118 
Epoch:  130    Loss:  84.37959 
Epoch:  140    Loss:  83.44133 
Epoch:  150    Loss:  82.60386 
Epoch:  160    Loss:  81.85324 
Epoch:  170    Loss:  81.23454 
Epoch:  180    Loss:  80.68679 
Epoch:  190    Loss:  80.16555 
Epoch:  200    Loss:  79.67953 </code></pre>
<p>This looks like it worked pretty well! It also should have fulfilled its purpose: Showing what you can achieve using <code>torch</code> tensors alone. In case you didn’t feel like going through the backprop logic with too much enthusiasm, don’t worry: In the next installment, this will get significantly less cumbersome. See you then!</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r distill-force-highlighting-css"><code class="sourceCode r"></code></pre></div>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Although the assumption may be tempting, “contiguous” does not correspond to what we’d call “contiguous in memory” in casual language.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>For correctness’ sake, <code>contiguous()</code> will only make a copy if the tensor it is called on is <em>not contiguous already.</em><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>Just to avoid any misunderstandings: In the next installment, this will be very first thing rendered obsolete by <code>torch</code>’s automatic differentiation capabilities.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
<!--radix_placeholder_article_footer-->
<div class="article-footer">
  <p class="social_footer">
    <span class="disqus-comments">
      <i class="fas fa-comments"></i>
      &nbsp;
      <span class="disqus-comment-count" data-disqus-identifier="posts/2020-10-01-torch-network-from-scratch/">Comment on this article</span>
    </span>
    <span class="article-sharing">
      Share: &nbsp;
      <a href="https://twitter.com/share?text=Getting%20familiar%20with%20torch%20tensors&amp;url=https%3A%2F%2Fblogs.rstudio.com%2Ftensorflow%2Fposts%2F2020-10-01-torch-network-from-scratch%2F" aria-label="share on twitter">
        <i class="fab fa-twitter" aria-hidden="true"></i>
      </a>
      <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3A%2F%2Fblogs.rstudio.com%2Ftensorflow%2Fposts%2F2020-10-01-torch-network-from-scratch%2F&amp;title=Getting%20familiar%20with%20torch%20tensors" aria-label="share on linkedin">
        <i class="fab fa-linkedin" aria-hidden="true"></i>
      </a>
    </span>
  </p>
  <script id="dsq-count-scr" src="https://tensorflow-for-r-blog.disqus.com/count.js" async></script>
  <div id="disqus_thread" class="hidden"></div>
  <script type="text/javascript" cookie-consent="functionality">
var disqus_config = function () {
  this.page.url = 'https://blogs.rstudio.com/tensorflow/posts/2020-10-01-torch-network-from-scratch/';
  this.page.identifier = 'posts/2020-10-01-torch-network-from-scratch/';
};
(function() {
  var d = document, s = d.createElement('script');
  s.src = 'https://tensorflow-for-r-blog.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
})();
</script>
  <p>
    <div class="subscribe">
<div id="subscribe-caption" style="line-height: 1.2; margin-bottom: 2px;">Enjoy this blog? Get notified of new posts by email:</div>

<script src="https://app-ab02.marketo.com/js/forms2/js/forms2.min.js"></script>
<form class="mtktoBlogEmailForm" id="mktoForm_2224"></form>
<script>

// establish metrics based on where the form is located
var in_sidebar = $('#subscribe-caption').parents('.sidebar-section').length;
if (in_sidebar) {
  var form_width = 190;
  var base_width = form_width - 23;
  var email_width = base_width + 'px';
  var label_width = (base_width - 20) + 'px';
  var button_width = email_width;
  var button_padding = '30px';
  var button_margin = '';
  var font_size = '12px';
} else {
  var form_width = 400;
  var base_width = form_width - 23;
  var email_width = base_width + 'px';
  var label_width = (base_width - 20) + 'px';
  var button_width = email_width;
  var button_padding = '30px';
  var button_margin = '12px';
  var font_size = '15px';
}

$('#subscribe-caption')
  .css('width', base_width)
  .css('font-size', font_size);


MktoForms2.loadForm("https://app-ab02.marketo.com", "709-NXN-706", 2224, function(form) {

  // get jquery reference to form
  form = $(form.getFormElem().get(0));

  $(form).css('width', form_width + 'px');
  $(form).find('.mktoOffset').remove();
  $(form).find('.mktoGutter').remove();
  $(form).find('.mktoEmailField').css('width', email_width);
  $(form).find('.mktoLabel').children().attr('style', 'font-weight: 400');
  $(form).find('.mktoLabel')
      .css('width', label_width)
      .css('font-size', font_size);
  $(form).find('.mktoButtonRow')
      .css('width', button_width)
      .css('text-align', 'center');
  $(form).find('.mktoButtonWrap').attr('style', '');
  $(form).find('.mktoButton')
      .css('margin-top', '10px')
      .css('padding-left', button_padding)
      .css('padding-right', button_padding)
      .css('font-size', font_size)
      .css('margin-top', button_margin);
});
</script>
Posts also available at <a href="https://www.r-bloggers.com">r-bloggers</a>

<script>
  document.addEventListener("DOMContentLoaded", function(){
    var menulink = document.querySelector("a[href='#category:R']");
    if (menulink) menulink.parentNode.style.display = "None";
    
    for (var e of document.querySelectorAll(".dt-tag")) {
      if (e.innerHTML == 'R') e.style.display = "None";
    }
  });
</script>
</div>
  </p>
</div>
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>


<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
  <h3 id="reuse">Reuse</h3>
  <p>Text and figures are licensed under Creative Commons Attribution <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>. The figures that have been reused from other sources don't fall under this license and can be recognized by a note in their caption: "Figure from ...".</p>
  <h3 id="citation">Citation</h3>
  <p>For attribution, please cite this work as</p>
  <pre class="citation-appendix short">Keydana (2020, Oct. 1). Posit AI Blog: Getting familiar with torch tensors. Retrieved from https://blogs.rstudio.com/tensorflow/posts/2020-10-01-torch-network-from-scratch/</pre>
  <p>BibTeX citation</p>
  <pre class="citation-appendix long">@misc{keydanatorchnetworkfromscratch,
  author = {Keydana, Sigrid},
  title = {Posit AI Blog: Getting familiar with torch tensors},
  url = {https://blogs.rstudio.com/tensorflow/posts/2020-10-01-torch-network-from-scratch/},
  year = {2020}
}</pre>
</div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
