<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { color: #00769e; background-color: #f1f3f5; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span { color: #00769e; } /* Normal */
code span.al { color: #ad0000; } /* Alert */
code span.an { color: #5e5e5e; } /* Annotation */
code span.at { color: #657422; } /* Attribute */
code span.bn { color: #ad0000; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #00769e; } /* ControlFlow */
code span.ch { color: #20794d; } /* Char */
code span.cn { color: #8f5902; } /* Constant */
code span.co { color: #5e5e5e; } /* Comment */
code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
code span.dt { color: #ad0000; } /* DataType */
code span.dv { color: #ad0000; } /* DecVal */
code span.er { color: #ad0000; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #ad0000; } /* Float */
code span.fu { color: #4758ab; } /* Function */
code span.im { } /* Import */
code span.in { color: #5e5e5e; } /* Information */
code span.kw { color: #00769e; } /* Keyword */
code span.op { color: #5e5e5e; } /* Operator */
code span.ot { color: #00769e; } /* Other */
code span.pp { color: #ad0000; } /* Preprocessor */
code span.sc { color: #5e5e5e; } /* SpecialChar */
code span.ss { color: #20794d; } /* SpecialString */
code span.st { color: #20794d; } /* String */
code span.va { color: #111111; } /* Variable */
code span.vs { color: #20794d; } /* VerbatimString */
code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
</style>

<style>
  div.csl-bib-body { }
  div.csl-entry {
    clear: both;
    }
  .hanging div.csl-entry {
    margin-left:2em;
    text-indent:-2em;
  }
  div.csl-left-margin {
    min-width:2em;
    float:left;
  }
  div.csl-right-inline {
    margin-left:2em;
    padding-left:1em;
  }
  div.csl-indent {
    margin-left: 2em;
  }
</style>

  <!--radix_placeholder_meta_tags-->
<title>Posit AI Blog: LLaMA in R with Keras and TensorFlow</title>

<meta property="description" itemprop="description" content="Implementation and walk-through of LLaMA, a Large Language Model, in R, with TensorFlow and Keras."/>

<link rel="canonical" href="https://blogs.rstudio.com/tensorflow/posts/2023-05-25-llama-tensorflow-keras/"/>
<link rel="license" href="https://creativecommons.org/licenses/by/4.0/"/>
<link rel="icon" type="image/png" href="../../images/favicon.png"/>

<!--  https://schema.org/Article -->
<meta property="article:published" itemprop="datePublished" content="2023-05-25"/>
<meta property="article:created" itemprop="dateCreated" content="2023-05-25"/>
<meta name="article:author" content="Tomasz Kalinowski"/>

<!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
<meta property="og:title" content="Posit AI Blog: LLaMA in R with Keras and TensorFlow"/>
<meta property="og:type" content="article"/>
<meta property="og:description" content="Implementation and walk-through of LLaMA, a Large Language Model, in R, with TensorFlow and Keras."/>
<meta property="og:url" content="https://blogs.rstudio.com/tensorflow/posts/2023-05-25-llama-tensorflow-keras/"/>
<meta property="og:image" content="https://blogs.rstudio.com/tensorflow/posts/2023-05-25-llama-tensorflow-keras/images/preview.jpg"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:site_name" content="Posit AI Blog"/>

<!--  https://dev.twitter.com/cards/types/summary -->
<meta property="twitter:card" content="summary_large_image"/>
<meta property="twitter:title" content="Posit AI Blog: LLaMA in R with Keras and TensorFlow"/>
<meta property="twitter:description" content="Implementation and walk-through of LLaMA, a Large Language Model, in R, with TensorFlow and Keras."/>
<meta property="twitter:url" content="https://blogs.rstudio.com/tensorflow/posts/2023-05-25-llama-tensorflow-keras/"/>
<meta property="twitter:image" content="https://blogs.rstudio.com/tensorflow/posts/2023-05-25-llama-tensorflow-keras/images/preview.jpg"/>

<!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
<meta name="citation_title" content="Posit AI Blog: LLaMA in R with Keras and TensorFlow"/>
<meta name="citation_fulltext_html_url" content="https://blogs.rstudio.com/tensorflow/posts/2023-05-25-llama-tensorflow-keras/"/>
<meta name="citation_fulltext_world_readable" content=""/>
<meta name="citation_online_date" content="2023/05/25"/>
<meta name="citation_publication_date" content="2023/05/25"/>
<meta name="citation_author" content="Tomasz Kalinowski"/>
<meta name="citation_author_institution" content="Posit"/>
<!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=LLaMA: Open and efficient foundation language models;citation_doi=10.48550/ARXIV.2302.13971;citation_author=Hugo Touvron;citation_author=Thibaut Lavril;citation_author=Gautier Izacard;citation_author=Xavier Martinet;citation_author=Marie-Anne Lachaux;citation_author=Timothée Lacroix;citation_author=Baptiste Rozière;citation_author=Naman Goyal;citation_author=Eric Hambro;citation_author=Faisal Azhar;citation_author=Aurelien Rodriguez;citation_author=Armand Joulin;citation_author=Edouard Grave;citation_author=Guillaume Lample"/>
  <meta name="citation_reference" content="citation_title=Attention is all you need;citation_author=Ashish Vaswani;citation_author=Noam Shazeer;citation_author=Niki Parmar;citation_author=Jakob Uszkoreit;citation_author=Llion Jones;citation_author=Aidan N. Gomez;citation_author=Lukasz Kaiser;citation_author=Illia Polosukhin"/>
  <meta name="citation_reference" content="citation_title=Training compute-optimal large language models;citation_author=Jordan Hoffmann;citation_author=Sebastian Borgeaud;citation_author=Arthur Mensch;citation_author=Elena Buchatskaya;citation_author=Trevor Cai;citation_author=Eliza Rutherford;citation_author=Diego Las Casas;citation_author=Lisa Anne Hendricks;citation_author=Johannes Welbl;citation_author=Aidan Clark;citation_author=Tom Hennigan;citation_author=Eric Noland;citation_author=Katie Millican;citation_author=George Driessche;citation_author=Bogdan Damoc;citation_author=Aurelia Guy;citation_author=Simon Osindero;citation_author=Karen Simonyan;citation_author=Erich Elsen;citation_author=Jack W. Rae;citation_author=Oriol Vinyals;citation_author=Laurent Sifre"/>
  <meta name="citation_reference" content="citation_title=GLU variants improve transformer;citation_author=Noam Shazeer"/>
  <meta name="citation_reference" content="citation_title=RoFormer: Enhanced transformer with rotary position embedding;citation_author=Jianlin Su;citation_author=Yu Lu;citation_author=Shengfeng Pan;citation_author=Ahmed Murtadha;citation_author=Bo Wen;citation_author=Yunfeng Liu"/>
  <meta name="citation_reference" content="citation_title=Posit AI blog: De-noising diffusion with torch;citation_author=Daniel Falbel;citation_author=Sigrid Keydana"/>
  <meta name="citation_reference" content="citation_title=Rotary embeddings: A relative revolution;citation_publisher=blog.eleuther.ai/rotary-embeddings/;citation_author=Stella Biderman;citation_author=Sid Black;citation_author=Charles Foster;citation_author=Leo Gao;citation_author=Eric Hallahan;citation_author=Horace He;citation_author=Ben Wang;citation_author=Phil Wang"/>
  <!--radix_placeholder_rmarkdown_metadata-->

<script type="text/json" id="radix-rmarkdown-metadata">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["creative_commons","title","description","author","slug","date","output","categories","bibliography","preview","citation_url","canonical_url"]}},"value":[{"type":"character","attributes":{},"value":["CC BY"]},{"type":"character","attributes":{},"value":["LLaMA in R with Keras and TensorFlow"]},{"type":"character","attributes":{},"value":["Implementation and walk-through of LLaMA, a Large Language Model, in R, with TensorFlow and Keras."]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","affiliation_url"]}},"value":[{"type":"character","attributes":{},"value":["Tomasz Kalinowski"]},{"type":"character","attributes":{},"value":["Posit"]},{"type":"character","attributes":{},"value":["https://www.posit.co/"]}]}]},{"type":"character","attributes":{},"value":["kalinowskillama"]},{"type":"character","attributes":{},"value":["2023-05-25"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained","toc"]}},"value":[{"type":"logical","attributes":{},"value":[false]},{"type":"logical","attributes":{},"value":[true]}]}]},{"type":"character","attributes":{},"value":["TensorFlow/Keras","R","Generative Models","Natural Language Processing"]},{"type":"character","attributes":{},"value":["references.bib"]},{"type":"character","attributes":{},"value":["images/preview.jpg"]},{"type":"character","attributes":{},"value":["https://blogs.rstudio.com/tensorflow/posts/2023-05-25-llama-tensorflow-keras/"]},{"type":"character","attributes":{},"value":["https://blogs.rstudio.com/tensorflow/posts/2023-05-25-llama-tensorflow-keras/"]}]}
</script>
<!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["images/preview.jpg","references.bib"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
<meta name="distill:offset" content="../.."/>

<script type="application/javascript">

  window.headroom_prevent_pin = false;

  window.document.addEventListener("DOMContentLoaded", function (event) {

    // initialize headroom for banner
    var header = $('header').get(0);
    var headerHeight = header.offsetHeight;
    var headroom = new Headroom(header, {
      tolerance: 5,
      onPin : function() {
        if (window.headroom_prevent_pin) {
          window.headroom_prevent_pin = false;
          headroom.unpin();
        }
      }
    });
    headroom.init();
    if(window.location.hash)
      headroom.unpin();
    $(header).addClass('headroom--transition');

    // offset scroll location for banner on hash change
    // (see: https://github.com/WickyNilliams/headroom.js/issues/38)
    window.addEventListener("hashchange", function(event) {
      window.scrollTo(0, window.pageYOffset - (headerHeight + 25));
    });

    // responsive menu
    $('.distill-site-header').each(function(i, val) {
      var topnav = $(this);
      var toggle = topnav.find('.nav-toggle');
      toggle.on('click', function() {
        topnav.toggleClass('responsive');
      });
    });

    // nav dropdowns
    $('.nav-dropbtn').click(function(e) {
      $(this).next('.nav-dropdown-content').toggleClass('nav-dropdown-active');
      $(this).parent().siblings('.nav-dropdown')
         .children('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $("body").click(function(e){
      $('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $(".nav-dropdown").click(function(e){
      e.stopPropagation();
    });
  });
</script>

<style type="text/css">

/* Theme (user-documented overrideables for nav appearance) */

.distill-site-nav {
  color: rgba(255, 255, 255, 0.8);
  background-color: #0F2E3D;
  font-size: 15px;
  font-weight: 300;
}

.distill-site-nav a {
  color: inherit;
  text-decoration: none;
}

.distill-site-nav a:hover {
  color: white;
}

@media print {
  .distill-site-nav {
    display: none;
  }
}

.distill-site-header {

}

.distill-site-footer {

}


/* Site Header */

.distill-site-header {
  width: 100%;
  box-sizing: border-box;
  z-index: 3;
}

.distill-site-header .nav-left {
  display: inline-block;
  margin-left: 8px;
}

@media screen and (max-width: 768px) {
  .distill-site-header .nav-left {
    margin-left: 0;
  }
}


.distill-site-header .nav-right {
  float: right;
  margin-right: 8px;
}

.distill-site-header a,
.distill-site-header .title {
  display: inline-block;
  text-align: center;
  padding: 14px 10px 14px 10px;
}

.distill-site-header .title {
  font-size: 18px;
  min-width: 150px;
}

.distill-site-header .logo {
  padding: 0;
}

.distill-site-header .logo img {
  display: none;
  max-height: 20px;
  width: auto;
  margin-bottom: -4px;
}

.distill-site-header .nav-image img {
  max-height: 18px;
  width: auto;
  display: inline-block;
  margin-bottom: -3px;
}



@media screen and (min-width: 1000px) {
  .distill-site-header .logo img {
    display: inline-block;
  }
  .distill-site-header .nav-left {
    margin-left: 20px;
  }
  .distill-site-header .nav-right {
    margin-right: 20px;
  }
  .distill-site-header .title {
    padding-left: 12px;
  }
}


.distill-site-header .nav-toggle {
  display: none;
}

.nav-dropdown {
  display: inline-block;
  position: relative;
}

.nav-dropdown .nav-dropbtn {
  border: none;
  outline: none;
  color: rgba(255, 255, 255, 0.8);
  padding: 16px 10px;
  background-color: transparent;
  font-family: inherit;
  font-size: inherit;
  font-weight: inherit;
  margin: 0;
  margin-top: 1px;
  z-index: 2;
}

.nav-dropdown-content {
  display: none;
  position: absolute;
  background-color: white;
  min-width: 200px;
  border: 1px solid rgba(0,0,0,0.15);
  border-radius: 4px;
  box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.1);
  z-index: 1;
  margin-top: 2px;
  white-space: nowrap;
  padding-top: 4px;
  padding-bottom: 4px;
}

.nav-dropdown-content hr {
  margin-top: 4px;
  margin-bottom: 4px;
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.nav-dropdown-active {
  display: block;
}

.nav-dropdown-content a, .nav-dropdown-content .nav-dropdown-header {
  color: black;
  padding: 6px 24px;
  text-decoration: none;
  display: block;
  text-align: left;
}

.nav-dropdown-content .nav-dropdown-header {
  display: block;
  padding: 5px 24px;
  padding-bottom: 0;
  text-transform: uppercase;
  font-size: 14px;
  color: #999999;
  white-space: nowrap;
}

.nav-dropdown:hover .nav-dropbtn {
  color: white;
}

.nav-dropdown-content a:hover {
  background-color: #ddd;
  color: black;
}

.nav-right .nav-dropdown-content {
  margin-left: -45%;
  right: 0;
}

@media screen and (max-width: 768px) {
  .distill-site-header a, .distill-site-header .nav-dropdown  {display: none;}
  .distill-site-header a.nav-toggle {
    float: right;
    display: block;
  }
  .distill-site-header .title {
    margin-left: 0;
  }
  .distill-site-header .nav-right {
    margin-right: 0;
  }
  .distill-site-header {
    overflow: hidden;
  }
  .nav-right .nav-dropdown-content {
    margin-left: 0;
  }
}


@media screen and (max-width: 768px) {
  .distill-site-header.responsive {position: relative; min-height: 500px; }
  .distill-site-header.responsive a.nav-toggle {
    position: absolute;
    right: 0;
    top: 0;
  }
  .distill-site-header.responsive a,
  .distill-site-header.responsive .nav-dropdown {
    display: block;
    text-align: left;
  }
  .distill-site-header.responsive .nav-left,
  .distill-site-header.responsive .nav-right {
    width: 100%;
  }
  .distill-site-header.responsive .nav-dropdown {float: none;}
  .distill-site-header.responsive .nav-dropdown-content {position: relative;}
  .distill-site-header.responsive .nav-dropdown .nav-dropbtn {
    display: block;
    width: 100%;
    text-align: left;
  }
}

/* Site Footer */

.distill-site-footer {
  width: 100%;
  overflow: hidden;
  box-sizing: border-box;
  z-index: 3;
  margin-top: 30px;
  padding-top: 30px;
  padding-bottom: 30px;
  text-align: center;
}

/* Headroom */

d-title {
  padding-top: 6rem;
}

@media print {
  d-title {
    padding-top: 4rem;
  }
}

.headroom {
  z-index: 1000;
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
}

.headroom--transition {
  transition: all .4s ease-in-out;
}

.headroom--unpinned {
  top: -100px;
}

.headroom--pinned {
  top: 0;
}

/* adjust viewport for navbar height */
/* helps vertically center bootstrap (non-distill) content */
.min-vh-100 {
  min-height: calc(100vh - 100px) !important;
}

</style>

<script src="../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="../../site_libs/font-awesome-6.4.0/css/all.min.css" rel="stylesheet"/>
<link href="../../site_libs/font-awesome-6.4.0/css/v4-shims.min.css" rel="stylesheet"/>
<script src="../../site_libs/headroom-0.9.4/headroom.min.js"></script>
<script src="../../site_libs/autocomplete-0.37.1/autocomplete.min.js"></script>
<script src="../../site_libs/fuse-6.4.1/fuse.min.js"></script>

<script type="application/javascript">

function getMeta(metaName) {
  var metas = document.getElementsByTagName('meta');
  for (let i = 0; i < metas.length; i++) {
    if (metas[i].getAttribute('name') === metaName) {
      return metas[i].getAttribute('content');
    }
  }
  return '';
}

function offsetURL(url) {
  var offset = getMeta('distill:offset');
  return offset ? offset + '/' + url : url;
}

function createFuseIndex() {

  // create fuse index
  var options = {
    keys: [
      { name: 'title', weight: 20 },
      { name: 'categories', weight: 15 },
      { name: 'description', weight: 10 },
      { name: 'contents', weight: 5 },
    ],
    ignoreLocation: true,
    threshold: 0
  };
  var fuse = new window.Fuse([], options);

  // fetch the main search.json
  return fetch(offsetURL('search.json'))
    .then(function(response) {
      if (response.status == 200) {
        return response.json().then(function(json) {
          // index main articles
          json.articles.forEach(function(article) {
            fuse.add(article);
          });
          // download collections and index their articles
          return Promise.all(json.collections.map(function(collection) {
            return fetch(offsetURL(collection)).then(function(response) {
              if (response.status === 200) {
                return response.json().then(function(articles) {
                  articles.forEach(function(article) {
                    fuse.add(article);
                  });
                })
              } else {
                return Promise.reject(
                  new Error('Unexpected status from search index request: ' +
                            response.status)
                );
              }
            });
          })).then(function() {
            return fuse;
          });
        });

      } else {
        return Promise.reject(
          new Error('Unexpected status from search index request: ' +
                      response.status)
        );
      }
    });
}

window.document.addEventListener("DOMContentLoaded", function (event) {

  // get search element (bail if we don't have one)
  var searchEl = window.document.getElementById('distill-search');
  if (!searchEl)
    return;

  createFuseIndex()
    .then(function(fuse) {

      // make search box visible
      searchEl.classList.remove('hidden');

      // initialize autocomplete
      var options = {
        autoselect: true,
        hint: false,
        minLength: 2,
      };
      window.autocomplete(searchEl, options, [{
        source: function(query, callback) {
          const searchOptions = {
            isCaseSensitive: false,
            shouldSort: true,
            minMatchCharLength: 2,
            limit: 10,
          };
          var results = fuse.search(query, searchOptions);
          callback(results
            .map(function(result) { return result.item; })
          );
        },
        templates: {
          suggestion: function(suggestion) {
            var img = suggestion.preview && Object.keys(suggestion.preview).length > 0
              ? `<img src="${offsetURL(suggestion.preview)}"</img>`
              : '';
            var html = `
              <div class="search-item">
                <h3>${suggestion.title}</h3>
                <div class="search-item-description">
                  ${suggestion.description || ''}
                </div>
                <div class="search-item-preview">
                  ${img}
                </div>
              </div>
            `;
            return html;
          }
        }
      }]).on('autocomplete:selected', function(event, suggestion) {
        window.location.href = offsetURL(suggestion.path);
      });
      // remove inline display style on autocompleter (we want to
      // manage responsive display via css)
      $('.algolia-autocomplete').css("display", "");
    })
    .catch(function(error) {
      console.log(error);
    });

});

</script>

<style type="text/css">

.nav-search {
  font-size: x-small;
}

/* Algolioa Autocomplete */

.algolia-autocomplete {
  display: inline-block;
  margin-left: 10px;
  vertical-align: sub;
  background-color: white;
  color: black;
  padding: 6px;
  padding-top: 8px;
  padding-bottom: 0;
  border-radius: 6px;
  border: 1px #0F2E3D solid;
  width: 180px;
}


@media screen and (max-width: 768px) {
  .distill-site-nav .algolia-autocomplete {
    display: none;
    visibility: hidden;
  }
  .distill-site-nav.responsive .algolia-autocomplete {
    display: inline-block;
    visibility: visible;
  }
  .distill-site-nav.responsive .algolia-autocomplete .aa-dropdown-menu {
    margin-left: 0;
    width: 400px;
    max-height: 400px;
  }
}

.algolia-autocomplete .aa-input, .algolia-autocomplete .aa-hint {
  width: 90%;
  outline: none;
  border: none;
}

.algolia-autocomplete .aa-hint {
  color: #999;
}
.algolia-autocomplete .aa-dropdown-menu {
  width: 550px;
  max-height: 70vh;
  overflow-x: visible;
  overflow-y: scroll;
  padding: 5px;
  margin-top: 3px;
  margin-left: -150px;
  background-color: #fff;
  border-radius: 5px;
  border: 1px solid #999;
  border-top: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion {
  cursor: pointer;
  padding: 5px 4px;
  border-bottom: 1px solid #eee;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion:last-of-type {
  border-bottom: none;
  margin-bottom: 2px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item {
  overflow: hidden;
  font-size: 0.8em;
  line-height: 1.4em;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item h3 {
  font-size: 1rem;
  margin-block-start: 0;
  margin-block-end: 5px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-description {
  display: inline-block;
  overflow: hidden;
  height: 2.8em;
  width: 80%;
  margin-right: 4%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview {
  display: inline-block;
  width: 15%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img {
  height: 3em;
  width: auto;
  display: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img[src] {
  display: initial;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion.aa-cursor {
  background-color: #eee;
}
.algolia-autocomplete .aa-dropdown-menu .aa-suggestion em {
  font-weight: bold;
  font-style: normal;
}

</style>


<!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

<style type="text/css">

body {
  background-color: white;
}

.pandoc-table {
  width: 100%;
}

.pandoc-table>caption {
  margin-bottom: 10px;
}

.pandoc-table th:not([align]) {
  text-align: left;
}

.pagedtable-footer {
  font-size: 15px;
}

d-byline .byline {
  grid-template-columns: 2fr 2fr;
}

d-byline .byline h3 {
  margin-block-start: 1.5em;
}

d-byline .byline .authors-affiliations h3 {
  margin-block-start: 0.5em;
}

.authors-affiliations .orcid-id {
  width: 16px;
  height:16px;
  margin-left: 4px;
  margin-right: 4px;
  vertical-align: middle;
  padding-bottom: 2px;
}

d-title .dt-tags {
  margin-top: 1em;
  grid-column: text;
}

.dt-tags .dt-tag {
  text-decoration: none;
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0em 0.4em;
  margin-right: 0.5em;
  margin-bottom: 0.4em;
  font-size: 70%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

d-article table.gt_table td,
d-article table.gt_table th {
  border-bottom: none;
  font-size: 100%;
}

.html-widget {
  margin-bottom: 2.0em;
}

.l-screen-inset {
  padding-right: 16px;
}

.l-screen .caption {
  margin-left: 10px;
}

.shaded {
  background: rgb(247, 247, 247);
  padding-top: 20px;
  padding-bottom: 20px;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .html-widget {
  margin-bottom: 0;
  border: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .shaded-content {
  background: white;
}

.text-output {
  margin-top: 0;
  line-height: 1.5em;
}

.hidden {
  display: none !important;
}

d-article {
  padding-top: 2.5rem;
  padding-bottom: 30px;
}

d-appendix {
  padding-top: 30px;
}

d-article>p>img {
  width: 100%;
}

d-article h2 {
  margin: 1rem 0 1.5rem 0;
}

d-article h3 {
  margin-top: 1.5rem;
}

d-article iframe {
  border: 1px solid rgba(0, 0, 0, 0.1);
  margin-bottom: 2.0em;
  width: 100%;
}

/* Tweak code blocks */

d-article div.sourceCode code,
d-article pre code {
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
}

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: auto;
}

d-article div.sourceCode {
  background-color: white;
}

d-article div.sourceCode pre {
  padding-left: 10px;
  font-size: 12px;
  border-left: 2px solid rgba(0,0,0,0.1);
}

d-article pre {
  font-size: 12px;
  color: black;
  background: none;
  margin-top: 0;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

d-article pre a {
  border-bottom: none;
}

d-article pre a:hover {
  border-bottom: none;
  text-decoration: underline;
}

d-article details {
  grid-column: text;
  margin-bottom: 0.8em;
}

@media(min-width: 768px) {

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: visible !important;
}

d-article div.sourceCode pre {
  padding-left: 18px;
  font-size: 14px;
}

d-article pre {
  font-size: 14px;
}

}

figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

/* CSS for d-contents */

.d-contents {
  grid-column: text;
  color: rgba(0,0,0,0.8);
  font-size: 0.9em;
  padding-bottom: 1em;
  margin-bottom: 1em;
  padding-bottom: 0.5em;
  margin-bottom: 1em;
  padding-left: 0.25em;
  justify-self: start;
}

@media(min-width: 1000px) {
  .d-contents.d-contents-float {
    height: 0;
    grid-column-start: 1;
    grid-column-end: 4;
    justify-self: center;
    padding-right: 3em;
    padding-left: 2em;
  }
}

.d-contents nav h3 {
  font-size: 18px;
  margin-top: 0;
  margin-bottom: 1em;
}

.d-contents li {
  list-style-type: none
}

.d-contents nav > ul {
  padding-left: 0;
}

.d-contents ul {
  padding-left: 1em
}

.d-contents nav ul li {
  margin-top: 0.6em;
  margin-bottom: 0.2em;
}

.d-contents nav a {
  font-size: 13px;
  border-bottom: none;
  text-decoration: none
  color: rgba(0, 0, 0, 0.8);
}

.d-contents nav a:hover {
  text-decoration: underline solid rgba(0, 0, 0, 0.6)
}

.d-contents nav > ul > li > a {
  font-weight: 600;
}

.d-contents nav > ul > li > ul {
  font-weight: inherit;
}

.d-contents nav > ul > li > ul > li {
  margin-top: 0.2em;
}


.d-contents nav ul {
  margin-top: 0;
  margin-bottom: 0.25em;
}

.d-article-with-toc h2:nth-child(2) {
  margin-top: 0;
}


/* Figure */

.figure {
  position: relative;
  margin-bottom: 2.5em;
  margin-top: 1.5em;
}

.figure .caption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

.figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

.figure .caption a {
  color: rgba(0, 0, 0, 0.6);
}

.figure .caption b,
.figure .caption strong, {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

/* Citations */

d-article .citation {
  color: inherit;
  cursor: inherit;
}

div.hanging-indent{
  margin-left: 1em; text-indent: -1em;
}

/* Citation hover box */

.tippy-box[data-theme~=light-border] {
  background-color: rgba(250, 250, 250, 0.95);
}

.tippy-content > p {
  margin-bottom: 0;
  padding: 2px;
}


/* Tweak 1000px media break to show more text */

@media(min-width: 1000px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }

  d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
  }
  figure .caption, .figure .caption, figure figcaption {
    font-size: 13px;
  }
}

@media(min-width: 1180px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 32px;
  }

  .grid {
    grid-column-gap: 32px;
  }
}


/* Get the citation styles for the appendix (not auto-injected on render since
   we do our own rendering of the citation appendix) */

d-appendix .citation-appendix,
.d-appendix .citation-appendix {
  font-size: 11px;
  line-height: 15px;
  border-left: 1px solid rgba(0, 0, 0, 0.1);
  padding-left: 18px;
  border: 1px solid rgba(0,0,0,0.1);
  background: rgba(0, 0, 0, 0.02);
  padding: 10px 18px;
  border-radius: 3px;
  color: rgba(150, 150, 150, 1);
  overflow: hidden;
  margin-top: -12px;
  white-space: pre-wrap;
  word-wrap: break-word;
}

/* Include appendix styles here so they can be overridden */

d-appendix {
  contain: layout style;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-top: 60px;
  margin-bottom: 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  color: rgba(0,0,0,0.5);
  padding-top: 60px;
  padding-bottom: 48px;
}

d-appendix h3 {
  grid-column: page-start / text-start;
  font-size: 15px;
  font-weight: 500;
  margin-top: 1em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.65);
}

d-appendix h3 + * {
  margin-top: 1em;
}

d-appendix ol {
  padding: 0 0 0 15px;
}

@media (min-width: 768px) {
  d-appendix ol {
    padding: 0 0 0 30px;
    margin-left: -30px;
  }
}

d-appendix li {
  margin-bottom: 1em;
}

d-appendix a {
  color: rgba(0, 0, 0, 0.6);
}

d-appendix > * {
  grid-column: text;
}

d-appendix > d-footnote-list,
d-appendix > d-citation-list,
d-appendix > distill-appendix {
  grid-column: screen;
}

/* Include footnote styles here so they can be overridden */

d-footnote-list {
  contain: layout style;
}

d-footnote-list > * {
  grid-column: text;
}

d-footnote-list a.footnote-backlink {
  color: rgba(0,0,0,0.3);
  padding-left: 0.5em;
}



/* Anchor.js */

.anchorjs-link {
  /*transition: all .25s linear; */
  text-decoration: none;
  border-bottom: none;
}
*:hover > .anchorjs-link {
  margin-left: -1.125em !important;
  text-decoration: none;
  border-bottom: none;
}

/* Social footer */

.social_footer {
  margin-top: 30px;
  margin-bottom: 0;
  color: rgba(0,0,0,0.67);
}

.disqus-comments {
  margin-right: 30px;
}

.disqus-comment-count {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  cursor: pointer;
}

#disqus_thread {
  margin-top: 30px;
}

.article-sharing a {
  border-bottom: none;
  margin-right: 8px;
}

.article-sharing a:hover {
  border-bottom: none;
}

.sidebar-section.subscribe {
  font-size: 12px;
  line-height: 1.6em;
}

.subscribe p {
  margin-bottom: 0.5em;
}


.article-footer .subscribe {
  font-size: 15px;
  margin-top: 45px;
}


.sidebar-section.custom {
  font-size: 12px;
  line-height: 1.6em;
}

.custom p {
  margin-bottom: 0.5em;
}

/* Styles for listing layout (hide title) */
.layout-listing d-title, .layout-listing .d-title {
  display: none;
}

/* Styles for posts lists (not auto-injected) */


.posts-with-sidebar {
  padding-left: 45px;
  padding-right: 45px;
}

.posts-list .description h2,
.posts-list .description p {
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
}

.posts-list .description h2 {
  font-weight: 700;
  border-bottom: none;
  padding-bottom: 0;
}

.posts-list h2.post-tag {
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
  padding-bottom: 12px;
}
.posts-list {
  margin-top: 60px;
  margin-bottom: 24px;
}

.posts-list .post-preview {
  text-decoration: none;
  overflow: hidden;
  display: block;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  padding: 24px 0;
}

.post-preview-last {
  border-bottom: none !important;
}

.posts-list .posts-list-caption {
  grid-column: screen;
  font-weight: 400;
}

.posts-list .post-preview h2 {
  margin: 0 0 6px 0;
  line-height: 1.2em;
  font-style: normal;
  font-size: 24px;
}

.posts-list .post-preview p {
  margin: 0 0 12px 0;
  line-height: 1.4em;
  font-size: 16px;
}

.posts-list .post-preview .thumbnail {
  box-sizing: border-box;
  margin-bottom: 24px;
  position: relative;
  max-width: 500px;
}
.posts-list .post-preview img {
  width: 100%;
  display: block;
}

.posts-list .metadata {
  font-size: 12px;
  line-height: 1.4em;
  margin-bottom: 18px;
}

.posts-list .metadata > * {
  display: inline-block;
}

.posts-list .metadata .publishedDate {
  margin-right: 2em;
}

.posts-list .metadata .dt-authors {
  display: block;
  margin-top: 0.3em;
  margin-right: 2em;
}

.posts-list .dt-tags {
  display: block;
  line-height: 1em;
}

.posts-list .dt-tags .dt-tag {
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0.3em 0.4em;
  margin-right: 0.2em;
  margin-bottom: 0.4em;
  font-size: 60%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

.posts-list img {
  opacity: 1;
}

.posts-list img[data-src] {
  opacity: 0;
}

.posts-more {
  clear: both;
}


.posts-sidebar {
  font-size: 16px;
}

.posts-sidebar h3 {
  font-size: 16px;
  margin-top: 0;
  margin-bottom: 0.5em;
  font-weight: 400;
  text-transform: uppercase;
}

.sidebar-section {
  margin-bottom: 30px;
}

.categories ul {
  list-style-type: none;
  margin: 0;
  padding: 0;
}

.categories li {
  color: rgba(0, 0, 0, 0.8);
  margin-bottom: 0;
}

.categories li>a {
  border-bottom: none;
}

.categories li>a:hover {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
}

.categories .active {
  font-weight: 600;
}

.categories .category-count {
  color: rgba(0, 0, 0, 0.4);
}


@media(min-width: 768px) {
  .posts-list .post-preview h2 {
    font-size: 26px;
  }
  .posts-list .post-preview .thumbnail {
    float: right;
    width: 30%;
    margin-bottom: 0;
  }
  .posts-list .post-preview .description {
    float: left;
    width: 45%;
  }
  .posts-list .post-preview .metadata {
    float: left;
    width: 20%;
    margin-top: 8px;
  }
  .posts-list .post-preview p {
    margin: 0 0 12px 0;
    line-height: 1.5em;
    font-size: 16px;
  }
  .posts-with-sidebar .posts-list {
    float: left;
    width: 75%;
  }
  .posts-with-sidebar .posts-sidebar {
    float: right;
    width: 20%;
    margin-top: 60px;
    padding-top: 24px;
    padding-bottom: 24px;
  }
}


/* Improve display for browsers without grid (IE/Edge <= 15) */

.downlevel {
  line-height: 1.6em;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  margin: 0;
}

.downlevel .d-title {
  padding-top: 6rem;
  padding-bottom: 1.5rem;
}

.downlevel .d-title h1 {
  font-size: 50px;
  font-weight: 700;
  line-height: 1.1em;
  margin: 0 0 0.5rem;
}

.downlevel .d-title p {
  font-weight: 300;
  font-size: 1.2rem;
  line-height: 1.55em;
  margin-top: 0;
}

.downlevel .d-byline {
  padding-top: 0.8em;
  padding-bottom: 0.8em;
  font-size: 0.8rem;
  line-height: 1.8em;
}

.downlevel .section-separator {
  border: none;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
}

.downlevel .d-article {
  font-size: 1.06rem;
  line-height: 1.7em;
  padding-top: 1rem;
  padding-bottom: 2rem;
}


.downlevel .d-appendix {
  padding-left: 0;
  padding-right: 0;
  max-width: none;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.5);
  padding-top: 40px;
  padding-bottom: 48px;
}

.downlevel .footnotes ol {
  padding-left: 13px;
}

.downlevel .base-grid,
.downlevel .distill-header,
.downlevel .d-title,
.downlevel .d-abstract,
.downlevel .d-article,
.downlevel .d-appendix,
.downlevel .distill-appendix,
.downlevel .d-byline,
.downlevel .d-footnote-list,
.downlevel .d-citation-list,
.downlevel .distill-footer,
.downlevel .appendix-bottom,
.downlevel .posts-container {
  padding-left: 40px;
  padding-right: 40px;
}

@media(min-width: 768px) {
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
  padding-left: 150px;
  padding-right: 150px;
  max-width: 900px;
}
}

.downlevel pre code {
  display: block;
  border-left: 2px solid rgba(0, 0, 0, .1);
  padding: 0 0 0 20px;
  font-size: 14px;
}

.downlevel code, .downlevel pre {
  color: black;
  background: none;
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

.downlevel .posts-list .post-preview {
  color: inherit;
}



</style>

<script type="application/javascript">

function is_downlevel_browser() {
  if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                 window.navigator.userAgent)) {
    return true;
  } else {
    return window.load_distill_framework === undefined;
  }
}

// show body when load is complete
function on_load_complete() {

  // add anchors
  if (window.anchors) {
    window.anchors.options.placement = 'left';
    window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
  }


  // set body to visible
  document.body.style.visibility = 'visible';

  // force redraw for leaflet widgets
  if (window.HTMLWidgets) {
    var maps = window.HTMLWidgets.findAll(".leaflet");
    $.each(maps, function(i, el) {
      var map = this.getMap();
      map.invalidateSize();
      map.eachLayer(function(layer) {
        if (layer instanceof L.TileLayer)
          layer.redraw();
      });
    });
  }

  // trigger 'shown' so htmlwidgets resize
  $('d-article').trigger('shown');
}

function init_distill() {

  init_common();

  // create front matter
  var front_matter = $('<d-front-matter></d-front-matter>');
  $('#distill-front-matter').wrap(front_matter);

  // create d-title
  $('.d-title').changeElementType('d-title');

  // create d-byline
  var byline = $('<d-byline></d-byline>');
  $('.d-byline').replaceWith(byline);

  // create d-article
  var article = $('<d-article></d-article>');
  $('.d-article').wrap(article).children().unwrap();

  // move posts container into article
  $('.posts-container').appendTo($('d-article'));

  // create d-appendix
  $('.d-appendix').changeElementType('d-appendix');

  // flag indicating that we have appendix items
  var appendix = $('.appendix-bottom').children('h3').length > 0;

  // replace footnotes with <d-footnote>
  $('.footnote-ref').each(function(i, val) {
    appendix = true;
    var href = $(this).attr('href');
    var id = href.replace('#', '');
    var fn = $('#' + id);
    var fn_p = $('#' + id + '>p');
    fn_p.find('.footnote-back').remove();
    var text = fn_p.html();
    var dtfn = $('<d-footnote></d-footnote>');
    dtfn.html(text);
    $(this).replaceWith(dtfn);
  });
  // remove footnotes
  $('.footnotes').remove();

  // move refs into #references-listing
  $('#references-listing').replaceWith($('#refs'));

  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    var id = $(this).attr('id');
    $('.d-contents a[href="#' + id + '"]').parent().remove();
    appendix = true;
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
  });

  // show d-appendix if we have appendix content
  $("d-appendix").css('display', appendix ? 'grid' : 'none');

  // localize layout chunks to just output
  $('.layout-chunk').each(function(i, val) {

    // capture layout
    var layout = $(this).attr('data-layout');

    // apply layout to markdown level block elements
    var elements = $(this).children().not('details, div.sourceCode, pre, script');
    elements.each(function(i, el) {
      var layout_div = $('<div class="' + layout + '"></div>');
      if (layout_div.hasClass('shaded')) {
        var shaded_content = $('<div class="shaded-content"></div>');
        $(this).wrap(shaded_content);
        $(this).parent().wrap(layout_div);
      } else {
        $(this).wrap(layout_div);
      }
    });


    // unwrap the layout-chunk div
    $(this).children().unwrap();
  });

  // remove code block used to force  highlighting css
  $('.distill-force-highlighting-css').parent().remove();

  // remove empty line numbers inserted by pandoc when using a
  // custom syntax highlighting theme
  $('code.sourceCode a:empty').remove();

  // load distill framework
  load_distill_framework();

  // wait for window.distillRunlevel == 4 to do post processing
  function distill_post_process() {

    if (!window.distillRunlevel || window.distillRunlevel < 4)
      return;

    // hide author/affiliations entirely if we have no authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
    if (!have_authors)
      $('d-byline').addClass('hidden');

    // article with toc class
    $('.d-contents').parent().addClass('d-article-with-toc');

    // strip links that point to #
    $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

    // add orcid ids
    $('.authors-affiliations').find('.author').each(function(i, el) {
      var orcid_id = front_matter.authors[i].orcidID;
      if (orcid_id) {
        var a = $('<a></a>');
        a.attr('href', 'https://orcid.org/' + orcid_id);
        var img = $('<img></img>');
        img.addClass('orcid-id');
        img.attr('alt', 'ORCID ID');
        img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
        a.append(img);
        $(this).append(a);
      }
    });

    // hide elements of author/affiliations grid that have no value
    function hide_byline_column(caption) {
      $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
    }

    // affiliations
    var have_affiliations = false;
    for (var i = 0; i<front_matter.authors.length; ++i) {
      var author = front_matter.authors[i];
      if (author.affiliation !== "&nbsp;") {
        have_affiliations = true;
        break;
      }
    }
    if (!have_affiliations)
      $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

    // published date
    if (!front_matter.publishedDate)
      hide_byline_column("Published");

    // document object identifier
    var doi = $('d-byline').find('h3:contains("DOI")');
    var doi_p = doi.next().empty();
    if (!front_matter.doi) {
      // if we have a citation and valid citationText then link to that
      if ($('#citation').length > 0 && front_matter.citationText) {
        doi.html('Citation');
        $('<a href="#citation"></a>')
          .text(front_matter.citationText)
          .appendTo(doi_p);
      } else {
        hide_byline_column("DOI");
      }
    } else {
      $('<a></a>')
         .attr('href', "https://doi.org/" + front_matter.doi)
         .html(front_matter.doi)
         .appendTo(doi_p);
    }

     // change plural form of authors/affiliations
    if (front_matter.authors.length === 1) {
      var grid = $('.authors-affiliations');
      grid.children('h3:contains("Authors")').text('Author');
      grid.children('h3:contains("Affiliations")').text('Affiliation');
    }

    // remove d-appendix and d-footnote-list local styles
    $('d-appendix > style:first-child').remove();
    $('d-footnote-list > style:first-child').remove();

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // hoverable references
    $('span.citation[data-cites]').each(function() {
      const citeChild = $(this).children()[0]
      // Do not process if @xyz has been used without escaping and without bibliography activated
      // https://github.com/rstudio/distill/issues/466
      if (citeChild === undefined) return true

      if (citeChild.nodeName == "D-FOOTNOTE") {
        var fn = citeChild
        $(this).html(fn.shadowRoot.querySelector("sup"))
        $(this).id = fn.id
        fn.remove()
      }
      var refs = $(this).attr('data-cites').split(" ");
      var refHtml = refs.map(function(ref) {
        // Could use CSS.escape too here, we insure backward compatibility in navigator
        return "<p>" + $('div[id="ref-' + ref + '"]').html() + "</p>";
      }).join("\n");
      window.tippy(this, {
        allowHTML: true,
        content: refHtml,
        maxWidth: 500,
        interactive: true,
        interactiveBorder: 10,
        theme: 'light-border',
        placement: 'bottom-start'
      });
    });

    // fix footnotes in tables (#411)
    // replacing broken distill.pub feature
    $('table d-footnote').each(function() {
      // we replace internal showAtNode methode which is triggered when hovering a footnote
      this.hoverBox.showAtNode = function(node) {
        // ported from https://github.com/distillpub/template/pull/105/files
        calcOffset = function(elem) {
            let x = elem.offsetLeft;
            let y = elem.offsetTop;
            // Traverse upwards until an `absolute` element is found or `elem`
            // becomes null.
            while (elem = elem.offsetParent && elem.style.position != 'absolute') {
                x += elem.offsetLeft;
                y += elem.offsetTop;
            }

            return { left: x, top: y };
        }
        // https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/offsetTop
        const bbox = node.getBoundingClientRect();
        const offset = calcOffset(node);
        this.show([offset.left + bbox.width, offset.top + bbox.height]);
      }
    })

    // clear polling timer
    clearInterval(tid);

    // show body now that everything is ready
    on_load_complete();
  }

  var tid = setInterval(distill_post_process, 50);
  distill_post_process();

}

function init_downlevel() {

  init_common();

   // insert hr after d-title
  $('.d-title').after($('<hr class="section-separator"/>'));

  // check if we have authors
  var front_matter = JSON.parse($("#distill-front-matter").html());
  var have_authors = front_matter.authors && front_matter.authors.length > 0;

  // manage byline/border
  if (!have_authors)
    $('.d-byline').remove();
  $('.d-byline').after($('<hr class="section-separator"/>'));
  $('.d-byline a').remove();

  // remove toc
  $('.d-contents').remove();

  // move appendix elements
  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
  });


  // inject headers into references and footnotes
  var refs_header = $('<h3></h3>');
  refs_header.text('References');
  $('#refs').prepend(refs_header);

  var footnotes_header = $('<h3></h3');
  footnotes_header.text('Footnotes');
  $('.footnotes').children('hr').first().replaceWith(footnotes_header);

  // move appendix-bottom entries to the bottom
  $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
  $('.appendix-bottom').remove();

  // remove appendix if it's empty
  if ($('.d-appendix').children().length === 0)
    $('.d-appendix').remove();

  // prepend separator above appendix
  $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

  // trim code
  $('pre>code').each(function(i, val) {
    $(this).html($.trim($(this).html()));
  });

  // move posts-container right before article
  $('.posts-container').insertBefore($('.d-article'));

  $('body').addClass('downlevel');

  on_load_complete();
}


function init_common() {

  // jquery plugin to change element types
  (function($) {
    $.fn.changeElementType = function(newType) {
      var attrs = {};

      $.each(this[0].attributes, function(idx, attr) {
        attrs[attr.nodeName] = attr.nodeValue;
      });

      this.replaceWith(function() {
        return $("<" + newType + "/>", attrs).append($(this).contents());
      });
    };
  })(jQuery);

  // prevent underline for linked images
  $('a > img').parent().css({'border-bottom' : 'none'});

  // mark non-body figures created by knitr chunks as 100% width
  $('.layout-chunk').each(function(i, val) {
    var figures = $(this).find('img, .html-widget');
    // ignore leaflet img layers (#106)
    figures = figures.filter(':not(img[class*="leaflet"])')
    if ($(this).attr('data-layout') !== "l-body") {
      figures.css('width', '100%');
    } else {
      figures.css('max-width', '100%');
      figures.filter("[width]").each(function(i, val) {
        var fig = $(this);
        fig.css('width', fig.attr('width') + 'px');
      });

    }
  });

  // auto-append index.html to post-preview links in file: protocol
  // and in rstudio ide preview
  $('.post-preview').each(function(i, val) {
    if (window.location.protocol === "file:")
      $(this).attr('href', $(this).attr('href') + "index.html");
  });

  // get rid of index.html references in header
  if (window.location.protocol !== "file:") {
    $('.distill-site-header a[href]').each(function(i,val) {
      $(this).attr('href', $(this).attr('href').replace(/^index[.]html/, "./"));
    });
  }

  // add class to pandoc style tables
  $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
  $('.kable-table').children('table').addClass('pandoc-table');

  // add figcaption style to table captions
  $('caption').parent('table').addClass("figcaption");

  // initialize posts list
  if (window.init_posts_list)
    window.init_posts_list();

  // implmement disqus comment link
  $('.disqus-comment-count').click(function() {
    window.headroom_prevent_pin = true;
    $('#disqus_thread').toggleClass('hidden');
    if (!$('#disqus_thread').hasClass('hidden')) {
      var offset = $(this).offset();
      $(window).resize();
      $('html, body').animate({
        scrollTop: offset.top - 35
      });
    }
  });
}

document.addEventListener('DOMContentLoaded', function() {
  if (is_downlevel_browser())
    init_downlevel();
  else
    window.addEventListener('WebComponentsReady', init_distill);
});

</script>

<!--/radix_placeholder_distill-->
  <script src="../../site_libs/header-attrs-2.21/header-attrs.js"></script>
  <script src="../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <script src="../../site_libs/popper-2.6.0/popper.min.js"></script>
  <link href="../../site_libs/tippy-6.2.7/tippy.css" rel="stylesheet" />
  <link href="../../site_libs/tippy-6.2.7/tippy-light-border.css" rel="stylesheet" />
  <script src="../../site_libs/tippy-6.2.7/tippy.umd.min.js"></script>
  <script src="../../site_libs/anchor-4.2.2/anchor.min.js"></script>
  <script src="../../site_libs/bowser-1.9.3/bowser.min.js"></script>
  <script src="../../site_libs/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="../../site_libs/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
<script defer data-domain="blogs.rstudio.com" src="https://plausible.io/js/plausible.js"></script>
<!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"LLaMA in R with Keras and TensorFlow","description":"Implementation and walk-through of LLaMA, a Large Language Model, in R, with TensorFlow and Keras.","authors":[{"author":"Tomasz Kalinowski","authorURL":"#","affiliation":"Posit","affiliationURL":"https://www.posit.co/","orcidID":""}],"publishedDate":"2023-05-25T00:00:00.000+00:00","citationText":"Kalinowski, 2023"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<header class="header header--fixed" role="banner">
<nav class="distill-site-nav distill-site-header">
<div class="nav-left">
<span class="logo">
<img src="../../images/posit.png"/>
</span>
<a href="../../index.html" class="title">AI Blog</a>
<input id="distill-search" class="nav-search hidden" type="text" placeholder="Search..."/>
</div>
<div class="nav-right">
<a href="../../index.html">Home</a>
<a href="../../gallery.html">Gallery</a>
<a href="../../about.html">About</a>
<a href="../../contributing.html">Contributing</a>
<a href="../../index.xml">
<i class="fa fa-rss" aria-hidden="true"></i>
</a>
<a href="javascript:void(0);" class="nav-toggle">&#9776;</a>
</div>
</nav>
</header>
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>LLaMA in R with Keras and TensorFlow</h1>
<!--radix_placeholder_categories-->
<div class="dt-tags">
  <a href="../../index.html#category:TensorFlow/Keras" class="dt-tag">TensorFlow/Keras</a>
  <a href="../../index.html#category:R" class="dt-tag">R</a>
  <a href="../../index.html#category:Generative_Models" class="dt-tag">Generative Models</a>
  <a href="../../index.html#category:Natural_Language_Processing" class="dt-tag">Natural Language Processing</a>
</div>
<!--/radix_placeholder_categories-->
<p><p>Implementation and walk-through of LLaMA, a Large Language Model, in R, with TensorFlow and Keras.</p></p>
</div>

<div class="d-byline">
  Tomasz Kalinowski  (Posit)<a href="https://www.posit.co/" class="uri">https://www.posit.co/</a>
  
<br/>2023-05-25
</div>

<div class="d-article">
<div class="d-contents d-contents-float">
<nav class="l-text toc figcaption" id="TOC">
<h3>Contents</h3>
<ul>
<li><a href="#setup">Setup</a></li>
<li><a href="#tokenizer">Tokenizer</a></li>
<li><a href="#transformerblock"><code>TransformerBlock</code></a></li>
<li><a href="#rmsnorm"><code>RMSNorm</code></a></li>
<li><a href="#feedforward"><code>FeedForward</code></a></li>
<li><a href="#attention"><code>Attention</code></a></li>
<li><a href="#rotary-position-embedding">Rotary Position Embedding</a></li>
<li><a href="#tying-it-all-together">Tying it all together</a></li>
<li><a href="#wrapping-up">Wrapping up</a></li>
</ul>
</nav>
</div>
<p>OpenAI’s chatGPT has awakened a collective awareness of what Large
Language Models (LLMs) are capable of. With that awakening comes a daily
march of LLM news: new products, new features, new models, new
capabilities, (and new worries). It seems we’re in the early stages of a
Cambrian explosion of LLMs and LLM powered tools; it’s not yet clear how
LLMs will impact and influence our professional and personal lives, but
it seems clear that they will, in some way.</p>
<p>Since LLMs are here to stay, it’s worthwhile to take some time to
understand how these models work from a first-principles perspective.
Starting with the mechanics can help foster durable intuitions that will
inform our usage of these models now and in the future. (Especially if
the future is one where LLMs are a staple of the data scientist’s
toolbox, as common as an <code>lm()</code> function call).</p>
<p>And what better way is there to learn than by doing. So with that
preamble, in this post we’ll walk through an implementation of an LLM,
<a href="https://arxiv.org/abs/2302.13971">LLaMA</a> <span class="citation" data-cites="touvron2023">(<a href="#ref-touvron2023" role="doc-biblioref">Touvron et al. 2023</a>)</span>
specifically, in TensorFlow and Keras, with the goal being to develop
understanding first, capability second.</p>
<p>Why LLaMA? With the sheer volume of LLM related content and news out
there, it can seem daunting to know where to get started. Almost weekly
it seems there is a new model announced. Browsing some hubs of LLM
activity (<a href="https://huggingface.co/models">HuggingFace</a>,
<a href="https://tfhub.dev/s?module-type=text-language-model">TFHub</a>,
<a href="https://www.reddit.com/r/deeplearning/">reddit</a>,
<a href="https://hn.algolia.com/?q=LLM">HackerNews</a>) muddies the waters even
more. How to pick a specific model?</p>
<p>Of the many LLM-related news items in the past months, one that stands
head-and-shoulders above the crowd is the <a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/">release of
LLaMA</a>,
a modern, foundational LLM made available to the public by Meta AI in
February 2023. On common benchmarks, LLaMA outperforms OpenAI’s GPT-3,
while being substantially smaller (though still <em>large</em>).</p>
<p>LLaMA is a great starting place because it is a simple and modern
architecture, has excellent performance on benchmarks, and is open. The
model architecture has had just a few new ideas incorporated into it since
the original Transformer architecture first described in,
“<a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>”
published from Google <span class="citation" data-cites="vaswani2017attention">(<a href="#ref-vaswani2017attention" role="doc-biblioref">Vaswani et al. 2017</a>)</span>. Four different sizes of
LLaMA have been released: 7 billion and 13 billion parameter models
trained on 1 Trillion tokens, and 33 billion and 65 billion parameter
models trained on 1.4 trillion tokens. This is an enormous amount of
training data these models have seen–the largest 65B model has been
trained on approximately the <a href="https://arxiv.org/abs/2203.15556">“Chinchilla
compute-optimum”</a> <span class="citation" data-cites="hoffmann2022training">(<a href="#ref-hoffmann2022training" role="doc-biblioref">Hoffmann et al. 2022</a>)</span>
number of tokens, while the smaller LLaMAs are substantially
beyond that optimum. In this blog post we’ll focus on the smallest, 7B
parameter LLaMA model, which you can comfortably load locally and run on
CPU with only 64Gb of RAM.</p>
<p>While not strictly necessary, to follow along locally, you’ll probably
want to acquire the pre-trained LLaMA weights <a href="https://forms.gle/jk851eBVbX1m5TAv5">one
way</a> or
<a href="https://github.com/facebookresearch/llama/pull/73">another</a>. Note, the
weights do come with their own license, which you can preview
<a href="https://github.com/facebookresearch/llama/pull/234">here</a>.</p>
<p>So, without further ado, let’s get started.</p>
<h3 id="setup">Setup</h3>
<p>First, we’ll want to install the required R and Python packages, and
configure a virtual environment:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>remotes<span class="sc">::</span><span class="fu">install_github</span>(<span class="fu">c</span>(<span class="st">&quot;rstudio/reticulate&quot;</span>,</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>                          <span class="st">&quot;rstudio/tensorflow&quot;</span>,</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>                          <span class="st">&quot;rstudio/keras&quot;</span>))</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>reticulate<span class="sc">::</span><span class="fu">virtualenv_create</span>(<span class="st">&quot;./.venv&quot;</span>, <span class="at">version =</span> <span class="st">&quot;3.10&quot;</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>tensorflow<span class="sc">::</span><span class="fu">install_tensorflow</span>(<span class="at">envname =</span> <span class="st">&quot;./.venv&quot;</span>, <span class="at">version =</span> <span class="st">&quot;release&quot;</span>)</span></code></pre></div>
<p>With that out of the way, let’s load some packages and prepare our R
session:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(purrr)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(envir)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tensorflow)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tfautograph)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="fu">use_virtualenv</span>(<span class="st">&quot;./.venv&quot;</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span>(<span class="at">tensorflow.extract.warn_tensors_passed_asis =</span> <span class="cn">FALSE</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="fu">attach_eval</span>({</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">import_from</span>(glue, glue)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">import_from</span>(jsonlite, read_json)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">import_from</span>(withr, with_dir, with_options)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">import_from</span>(keras<span class="sc">$</span>layers, Dense)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>  np <span class="ot">&lt;-</span> reticulate<span class="sc">::</span><span class="fu">import</span>(<span class="st">&quot;numpy&quot;</span>, <span class="at">convert =</span> <span class="cn">FALSE</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>  seq_len0 <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">seq.int</span>(<span class="at">from =</span> 0L, <span class="at">length.out =</span> x)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>})</span></code></pre></div>
<p>If you’ve acquired the pre-trained weights, it’ll be convenient to
convert them from the torch checkpoint format to something that’s more
framework agnostic (you only need to do this once, of course):</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># reticulate::py_install(&quot;torch&quot;, pip = TRUE)</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>torch <span class="ot">&lt;-</span> reticulate<span class="sc">::</span><span class="fu">import</span>(<span class="st">&quot;torch&quot;</span>, <span class="at">convert =</span> <span class="cn">FALSE</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">with_dir</span>(<span class="st">&quot;~/github/facebookresearch/llama/weights/LLaMA/7B&quot;</span>, {</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  pretrained_weights <span class="ot">&lt;-</span> torch<span class="sc">$</span><span class="fu">load</span>(<span class="st">&quot;consolidated.00.pth&quot;</span>,</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">map_location =</span> <span class="st">&quot;cpu&quot;</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (name <span class="cf">in</span> <span class="fu">names</span>(pretrained_weights)) {</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    filename <span class="ot">&lt;-</span> <span class="fu">sprintf</span>(<span class="st">&quot;%s.npy&quot;</span>, name)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    array <span class="ot">&lt;-</span> pretrained_weights[[nm]]<span class="sc">$</span><span class="fu">numpy</span>()</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    np<span class="sc">$</span><span class="fu">save</span>(filename, array)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">message</span>(<span class="fu">glue</span>(</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>      <span class="st">&quot;wrote: &#39;{basename(filename)}&#39; with shape: {array$shape}&quot;</span>))</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>})</span></code></pre></div>
<p>We’ll also define a helper function so we can avoid having to retype the
full path to our weights:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>weights_path <span class="ot">&lt;-</span> <span class="cf">function</span>(filename) <span class="fu">normalizePath</span>(<span class="fu">file.path</span>(</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;~/github/facebookresearch/llama/weights/LLaMA/&quot;</span>,</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">glue</span>(filename, <span class="at">.envir =</span> <span class="fu">parent.frame</span>())), <span class="at">mustWork =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>And load the model configuration parameters specific to the 7B LLaMA,
which we’ll use to build the model.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>params <span class="ot">&lt;-</span> <span class="fu">read_json</span>(<span class="fu">weights_path</span>(<span class="st">&quot;7B/params.json&quot;</span>))</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(params)</span></code></pre></div>
<pre><code>List of 6
 $ dim        : int 4096
 $ multiple_of: int 256
 $ n_heads    : int 32
 $ n_layers   : int 32
 $ norm_eps   : num 1e-06
 $ vocab_size : int -1</code></pre>
<h3 id="tokenizer">Tokenizer</h3>
<p>The first component to LLaMA is the tokenizer, which converts text to a
sequence of integers. The LLaMA model uses the
<a href="https://github.com/google/sentencepiece">SentencePiece</a> tokenizer from
Google. SentencePiece is available as a TensorFlow graph operation
through
<a href="https://www.tensorflow.org/text/api_docs/python/text/SentencepieceTokenizer"><code>tf_text.SentencepieceTokenizer</code></a>,
and also as a Keras layer in
<a href="https://keras.io/api/keras_nlp/tokenizers/sentence_piece_tokenizer/"><code>keras_nlp.tokenizers.SentencepieceTokenizer</code></a>.
By choice of a coin flip, we’ll use the lower-level <code>tf_text</code> interface.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>tf_text <span class="ot">&lt;-</span> reticulate<span class="sc">::</span><span class="fu">import</span>(<span class="st">&quot;tensorflow_text&quot;</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>tokenizer_path <span class="ot">&lt;-</span> <span class="fu">weights_path</span>(<span class="st">&quot;tokenizer.model&quot;</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="ot">&lt;-</span> tf_text<span class="sc">$</span><span class="fu">SentencepieceTokenizer</span>(</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  tf<span class="sc">$</span>io<span class="sc">$</span>gfile<span class="sc">$</span><span class="fu">GFile</span>(tokenizer_path, <span class="st">&quot;rb&quot;</span>)<span class="sc">$</span><span class="fu">read</span>(),</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">add_bos =</span> <span class="cn">TRUE</span>, <span class="at">add_eos =</span> <span class="cn">FALSE</span>,</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>Let’s test it out with a prompt:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="ot">&lt;-</span> <span class="st">&quot;The best way to attract bees&quot;</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>tokenizer<span class="sc">$</span><span class="fu">tokenize</span>(prompt)</span></code></pre></div>
<pre><code>tf.Tensor([    1   450  1900   982   304 13978   367   267], shape=(8), dtype=int32)</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="sc">|&gt;</span> tokenizer<span class="sc">$</span><span class="fu">tokenize</span>() <span class="sc">|&gt;</span> tokenizer<span class="sc">$</span><span class="fu">detokenize</span>()</span></code></pre></div>
<pre><code>tf.Tensor(b&#39;The best way to attract bees&#39;, shape=(), dtype=string)</code></pre>
<p>Let’s define a <code>show_tokens()</code> helper function and play with the
tokenizer a little.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>show_tokens <span class="ot">&lt;-</span> <span class="cf">function</span>(what) {</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(<span class="fu">is.character</span>(what))</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    token_ids <span class="ot">&lt;-</span> what <span class="sc">|&gt;</span> tokenizer<span class="sc">$</span><span class="fu">tokenize</span>() <span class="sc">|&gt;</span> <span class="fu">as.integer</span>()</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    token_ids <span class="ot">&lt;-</span> <span class="fu">as.integer</span>(what)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>  tokens <span class="ot">&lt;-</span> token_ids <span class="sc">|&gt;</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">map_chr</span>(<span class="cf">function</span>(id) {</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>      id <span class="sc">|&gt;</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>        <span class="fu">as_tensor</span>(<span class="at">shape =</span> <span class="fu">c</span>(<span class="dv">1</span>)) <span class="sc">|&gt;</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>        tokenizer<span class="sc">$</span><span class="fu">detokenize</span>() <span class="sc">|&gt;</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>        <span class="fu">as.character</span>()</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">names</span>(tokens) <span class="ot">&lt;-</span> token_ids</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>  tokens</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="fu">show_tokens</span>(prompt)</span></code></pre></div>
<pre><code>        1       450      1900       982       304     13978       367       267
       &quot;&quot;     &quot;The&quot;    &quot;best&quot;     &quot;way&quot;      &quot;to&quot; &quot;attract&quot;      &quot;be&quot;      &quot;es&quot;</code></pre>
<p>Note that “bees” is two tokens. Not every token corresponds to a word.
For example, one non-word token we can reliably expect to show up in a
tokenizer trained on a corpus of English text is “ing.” However, <em>when</em> the
“ing” token shows up will not always follow your intuitions, because
common words get their own token id, even if they can be decomposed into
multiple tokens.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">show_tokens</span>(<span class="st">&quot;ing&quot;</span>)</span></code></pre></div>
<pre><code>    1  2348
   &quot;&quot; &quot;ing&quot;</code></pre>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">show_tokens</span>(<span class="st">&quot;working&quot;</span>)</span></code></pre></div>
<pre><code>        1      1985
       &quot;&quot; &quot;working&quot;</code></pre>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">show_tokens</span>(<span class="st">&quot;flexing&quot;</span>)</span></code></pre></div>
<pre><code>     1   8525    292
    &quot;&quot; &quot;flex&quot;  &quot;ing&quot;</code></pre>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">show_tokens</span>(<span class="st">&quot;wonking&quot;</span>)</span></code></pre></div>
<pre><code>     1   2113   9292
    &quot;&quot;  &quot;won&quot; &quot;king&quot;</code></pre>
<p>Another thing to note about the tokenizer is that each token sequence
starts with token id <code>1</code>. This is a special <em>beginning-of-sequence</em>
token that we requested be added when we loaded the tokenizer with
<code>add_bos = TRUE</code>. There are two other such special tokens that we will
encounter later: an <em>end-of-sequence</em> special tokens with id <code>2</code>, and an
<em>unknown-token</em> with id <code>0</code>.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">as.character</span>(tokenizer<span class="sc">$</span><span class="fu">id_to_string</span>(0L))</span></code></pre></div>
<pre><code>[1] &quot;&lt;unk&gt;&quot;</code></pre>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">as.character</span>(tokenizer<span class="sc">$</span><span class="fu">id_to_string</span>(1L))</span></code></pre></div>
<pre><code>[1] &quot;&lt;s&gt;&quot;</code></pre>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">as.character</span>(tokenizer<span class="sc">$</span><span class="fu">id_to_string</span>(2L))</span></code></pre></div>
<pre><code>[1] &quot;&lt;/s&gt;&quot;</code></pre>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">show_tokens</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>))</span></code></pre></div>
<pre><code>    1     0     2
   &quot;&quot; &quot; ⁇ &quot;    &quot;&quot;</code></pre>
<p>Overall, there are 32,000 tokens.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">as.integer</span>(tokenizer<span class="sc">$</span><span class="fu">vocab_size</span>())</span></code></pre></div>
<pre><code>[1] 32000</code></pre>
<p>One last observation is that the more frequently encountered tokens are
assigned lower ids.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">show_tokens</span>(<span class="fu">seq</span>(<span class="dv">50</span>, <span class="at">len =</span> <span class="dv">10</span>))</span></code></pre></div>
<pre><code> 50  51  52  53  54  55  56  57  58  59
&quot;/&quot; &quot;0&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot;</code></pre>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">show_tokens</span>(<span class="fu">seq</span>(<span class="dv">100</span>, <span class="at">len =</span> <span class="dv">10</span>))</span></code></pre></div>
<pre><code>100 101 102 103 104 105 106 107 108 109
&quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot;</code></pre>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu">show_tokens</span>(<span class="fu">seq</span>(<span class="dv">1000</span>, <span class="at">len =</span> <span class="dv">10</span>))</span></code></pre></div>
<pre><code>   1000    1001    1002    1003    1004    1005    1006    1007    1008    1009
  &quot;ied&quot;    &quot;ER&quot;  &quot;stat&quot;   &quot;fig&quot;    &quot;me&quot;   &quot;von&quot; &quot;inter&quot;  &quot;roid&quot;  &quot;ater&quot; &quot;their&quot;</code></pre>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">show_tokens</span>(<span class="fu">seq</span>(<span class="dv">10000</span>, <span class="at">len =</span> <span class="dv">10</span>))</span></code></pre></div>
<pre><code>   10000    10001    10002    10003    10004    10005    10006    10007
   &quot;ång&quot;  &quot;citep&quot;    &quot;Ill&quot;   &quot;rank&quot; &quot;sender&quot;   &quot;beim&quot;    &quot;рак&quot; &quot;compat&quot;
   10008    10009
&quot;occurs&quot;  &quot;diese&quot;</code></pre>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="fu">show_tokens</span>(<span class="fu">seq</span>(<span class="dv">20000</span>, <span class="at">len =</span> <span class="dv">10</span>))</span></code></pre></div>
<pre><code>    20000     20001     20002     20003     20004     20005     20006     20007
  &quot;admit&quot; &quot;Comment&quot;     &quot;стя&quot;    &quot;Vien&quot;      &quot;ці&quot;  &quot;permut&quot;     &quot;cgi&quot;    &quot;crít&quot;
    20008     20009
&quot;Console&quot;    &quot;ctic&quot;</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="fu">show_tokens</span>(<span class="fu">seq</span>(<span class="at">to =</span> <span class="fu">as.integer</span>(tokenizer<span class="sc">$</span><span class="fu">vocab_size</span>()) <span class="sc">-</span> <span class="dv">1</span>, <span class="at">len =</span> <span class="dv">10</span>))</span></code></pre></div>
<pre><code>31990 31991 31992 31993 31994 31995 31996 31997 31998 31999
  &quot;ὀ&quot;  &quot;げ&quot;  &quot;べ&quot;  &quot;边&quot;  &quot;还&quot;  &quot;黃&quot;  &quot;왕&quot;  &quot;收&quot;  &quot;弘&quot;  &quot;给&quot;</code></pre>
<p>Moving on, the next step after tokenization is embedding. An embedding
layer is effectively a dictionary lookup that converts an integer (token
id) to a 1-d float array. For this we can use the standard keras
<code>Embedding</code> layer.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>tok_embeddings <span class="ot">&lt;-</span> keras<span class="sc">$</span>layers<span class="sc">$</span><span class="fu">Embedding</span>(</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">input_dim =</span> tokenizer<span class="sc">$</span><span class="fu">vocab_size</span>(),</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">output_dim =</span> params<span class="sc">$</span>dim,</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">embeddings_initializer =</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>    \(...) np<span class="sc">$</span><span class="fu">load</span>(<span class="fu">weights_path</span>(<span class="st">&quot;7B/tok_embeddings.weight.npy&quot;</span>))</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a><span class="fu">tok_embeddings</span>(3L) <span class="sc">|&gt;</span> <span class="fu">str</span>()</span></code></pre></div>
<pre><code>&lt;tf.Tensor: shape=(4096), dtype=float32, numpy=…&gt;</code></pre>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="sc">|&gt;</span> <span class="co"># &quot;The best way to attract bees&quot;</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>  tokenizer<span class="sc">$</span><span class="fu">tokenize</span>() <span class="sc">|&gt;</span></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tok_embeddings</span>() <span class="sc">|&gt;</span></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">str</span>()</span></code></pre></div>
<pre><code>&lt;tf.Tensor: shape=(8, 4096), dtype=float32, numpy=…&gt;</code></pre>
<h3 id="transformerblock"><code>TransformerBlock</code></h3>
<p>Once it’s tokenized and embedded, the input then passes through the bulk
of the model, a sequence of repeating <code>TransformerBlock</code> layers. The 7B
model has 32 of these <code>TransformerBlock</code> layers, while the 65B model has
80 of them.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="fu">weights_path</span>(<span class="st">&quot;7B/params.json&quot;</span>)  <span class="sc">|&gt;</span> <span class="fu">read_json</span>() <span class="sc">|&gt;</span> _<span class="sc">$</span>n_layers</span></code></pre></div>
<pre><code>[1] 32</code></pre>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="fu">weights_path</span>(<span class="st">&quot;65B/params.json&quot;</span>) <span class="sc">|&gt;</span> <span class="fu">read_json</span>() <span class="sc">|&gt;</span> _<span class="sc">$</span>n_layers</span></code></pre></div>
<pre><code>[1] 80</code></pre>
<p>Here is what the transformer block looks like:</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="fu">TransformerBlock</span>(keras<span class="sc">$</span>layers<span class="sc">$</span>Layer) <span class="sc">%py_class%</span> {</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>  initialize <span class="ot">&lt;-</span> <span class="cf">function</span>(attn_head_size, attn_n_heads,</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>                         <span class="at">norm_eps =</span> <span class="fu">k_epsilon</span>(), ...,</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>                         <span class="at">block_id =</span> <span class="cn">NULL</span>) {</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>    super<span class="sc">$</span><span class="fu">initialize</span>(...)</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>attention <span class="ot">&lt;-</span> <span class="fu">Attention</span>(attn_head_size, attn_n_heads,</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>                                <span class="at">block_id =</span> block_id)</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>feed_forward <span class="ot">&lt;-</span> <span class="fu">FeedForward</span>(</span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>      <span class="at">hidden_dim =</span> <span class="dv">4</span> <span class="sc">*</span> attn_head_size <span class="sc">*</span> attn_n_heads,</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a>      <span class="at">block_id =</span> block_id)</span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>attention_norm <span class="ot">&lt;-</span> <span class="fu">RMSNorm</span>(<span class="at">eps =</span> norm_eps,</span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">block_id =</span> block_id,</span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">feeds_into =</span> <span class="st">&quot;attention&quot;</span>)</span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>feed_forward_norm <span class="ot">&lt;-</span> <span class="fu">RMSNorm</span>(<span class="at">eps =</span> norm_eps,</span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a>                                      <span class="at">block_id =</span> block_id,</span>
<span id="cb52-19"><a href="#cb52-19" aria-hidden="true" tabindex="-1"></a>                                      <span class="at">feeds_into =</span> <span class="st">&quot;ffn&quot;</span>)</span>
<span id="cb52-20"><a href="#cb52-20" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb52-21"><a href="#cb52-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-22"><a href="#cb52-22" aria-hidden="true" tabindex="-1"></a>  call <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb52-23"><a href="#cb52-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-24"><a href="#cb52-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># norm and attention</span></span>
<span id="cb52-25"><a href="#cb52-25" aria-hidden="true" tabindex="-1"></a>    x2 <span class="ot">&lt;-</span> x <span class="sc">|&gt;</span></span>
<span id="cb52-26"><a href="#cb52-26" aria-hidden="true" tabindex="-1"></a>      self<span class="sc">$</span><span class="fu">attention_norm</span>() <span class="sc">|&gt;</span></span>
<span id="cb52-27"><a href="#cb52-27" aria-hidden="true" tabindex="-1"></a>      self<span class="sc">$</span><span class="fu">attention</span>()</span>
<span id="cb52-28"><a href="#cb52-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-29"><a href="#cb52-29" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">&lt;-</span> x <span class="sc">+</span> x2 <span class="co"># add residual</span></span>
<span id="cb52-30"><a href="#cb52-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-31"><a href="#cb52-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># norm and swiglu</span></span>
<span id="cb52-32"><a href="#cb52-32" aria-hidden="true" tabindex="-1"></a>    x2 <span class="ot">&lt;-</span> x <span class="sc">%&gt;%</span></span>
<span id="cb52-33"><a href="#cb52-33" aria-hidden="true" tabindex="-1"></a>      self<span class="sc">$</span><span class="fu">feed_forward_norm</span>() <span class="sc">%&gt;%</span></span>
<span id="cb52-34"><a href="#cb52-34" aria-hidden="true" tabindex="-1"></a>      self<span class="sc">$</span><span class="fu">feed_forward</span>()</span>
<span id="cb52-35"><a href="#cb52-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-36"><a href="#cb52-36" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">&lt;-</span> x <span class="sc">+</span> x2 <span class="co"># residual again</span></span>
<span id="cb52-37"><a href="#cb52-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-38"><a href="#cb52-38" aria-hidden="true" tabindex="-1"></a>    x</span>
<span id="cb52-39"><a href="#cb52-39" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb52-40"><a href="#cb52-40" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>While there is not a lot of code, there are a lot of ideas packed in
there. This block forms the main trunk of the model, so it’s worth
taking the time to go through it slowly.</p>
<p>We implement the <code>TransformerBlock</code> as a subclassed
<code>keras.layers.Layer</code>. This is gives us some niceties like the ability to
compose with other Keras layers, but these are mostly irrelevant to the
purpose of this blog post; we could just as easily implement this as,
for example, a vanilla R6 class. Our <code>TransformerBlock</code> class has two
methods: <code>initialize</code>, called when we first create the block, and
<code>call</code>, called when we run the forward pass of the block.</p>
<p>In <code>initialize</code>, we create 4 layers: an <code>Attention</code> layer, a
<code>FeedForward</code> layer, and 2 <code>RMSNorm</code> layers. We’ll take a close look at
each of these soon, but even before we do so, we can see how they fit
together by looking at the <code>TransformerBlock$call()</code> method.</p>
<p>The <code>call</code> method has a few simple ideas. In no particular order, the
first one to observe is the composition pattern of adding residuals.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> x <span class="sc">|&gt;</span> ...</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> x <span class="sc">+</span> x2 <span class="co"># add residual x to x2</span></span></code></pre></div>
<p>This is a common pattern that helps with model training, and especially
to help with the <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">vanishing gradient
problem</a>. It’s
a skip-connection in the other-wise linear sequence of matrix
transformations. It reinjects information (during the forward pass), and
gradients (during back propagation), back into the trunk. You can think
of these residual connections as freeing the learnable layers in-between
(the <code>...</code> in the pseudo code) from the burden of having to
“pass-through” or “preserve” information in <code>x</code>, allowing the weights to
instead focus on learning transformations that are, (in corporatese
vernacular), <em>value-adding</em>.</p>
<p>The next composition pattern to note is the repeating usage of a
normalization layer:</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> x <span class="sc">|&gt;</span> <span class="fu">norm</span>() <span class="sc">|&gt;</span> ...</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> x <span class="sc">+</span> x2</span></code></pre></div>
<p>There are many kinds of normalization layers, but to slightly
over-generalize, they can all be thought of as a stabilizer that helps
with training. Like their deep-learning cousins the regularizers, their
main function is to keep values passing through in a sensible range–in
the ball park of (-1, 1), typically. We’ll take a closer look at
<code>RMSNorm</code> soon.</p>
<p>Stripped of two tricks that are mostly there to help the model train,
residuals and normalization, the core of the <code>TransformerBlock</code> is just
this:</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>x <span class="sc">|&gt;</span> <span class="fu">attention</span>() <span class="sc">|&gt;</span> <span class="fu">feed_forward</span>()</span></code></pre></div>
<p>In a moment we’ll see that that <code>feed_foward</code> is a slightly fancier
variation of a conventional sequence of <code>Dense</code> layer. Before we get
there we can we safely skip ahead to distill the following intuition: a
<code>TransformerBlock</code> is basically an <code>Attention</code> layer followed by a few
(fancy) dense layers, with some simple composition patterns (tricks)
that help with training. <code>Attention</code> is the heart of the model: it’s the
most interesting, and also the most involved.</p>
<p>With the framing in place, let’s go through and take a closer look at
<code>RMSNorm</code>, <code>FeedForward</code>, and then with the foundation in place, we’ll
turn our attention to <code>Attention</code>.</p>
<h3 id="rmsnorm"><code>RMSNorm</code></h3>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="fu">RMSNorm</span>(keras<span class="sc">$</span>layers<span class="sc">$</span>Layer) <span class="sc">%py_class%</span> {</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>  initialize <span class="ot">&lt;-</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">function</span>(<span class="at">eps =</span> <span class="fl">1e-6</span>, ..., <span class="at">block_id =</span> <span class="cn">NULL</span>, <span class="at">feeds_into =</span> <span class="cn">NULL</span>) {</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>      super<span class="sc">$</span><span class="fu">initialize</span>(...)</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>      self<span class="sc">$</span>eps <span class="ot">&lt;-</span> eps</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>      self<span class="sc">$</span>block_id <span class="ot">&lt;-</span> block_id</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>      self<span class="sc">$</span>feeds_into <span class="ot">&lt;-</span> feeds_into</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>  build <span class="ot">&lt;-</span> <span class="cf">function</span>(input_shape) {</span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># input_shape == (batch_size, seqlen, params$dim)</span></span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># self$w will broadcast over batch_size and seqlen dims.</span></span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># w_shape == (1, 1, params$dim)</span></span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a>    w_shape <span class="ot">&lt;-</span> <span class="fu">rep</span>(1L, <span class="fu">length</span>(input_shape))</span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a>    w_shape[<span class="fu">length</span>(input_shape)] <span class="ot">&lt;-</span> <span class="fu">as.integer</span>(input_shape) <span class="sc">|&gt;</span> <span class="fu">tail</span>(1L)</span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># define a local function that will load</span></span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the pretrained-weights if we supplied `block_id` and `feeds_into`</span></span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true" tabindex="-1"></a>    <span class="fu">import_from</span>({self}, block_id, feeds_into)</span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true" tabindex="-1"></a>    initializer <span class="ot">&lt;-</span><span class="cf">if</span> (<span class="fu">is.null</span>(block_id))</span>
<span id="cb56-21"><a href="#cb56-21" aria-hidden="true" tabindex="-1"></a>      <span class="st">&quot;ones&quot;</span></span>
<span id="cb56-22"><a href="#cb56-22" aria-hidden="true" tabindex="-1"></a>      <span class="cf">else</span> <span class="cf">if</span> (block_id <span class="sc">&gt;=</span><span class="dv">0</span>) {</span>
<span id="cb56-23"><a href="#cb56-23" aria-hidden="true" tabindex="-1"></a>        \(...) <span class="fu">weights_path</span>(<span class="st">&quot;7B/layers.{block_id}.{feeds_into}_norm.weight.npy&quot;</span>) <span class="sc">|&gt;</span></span>
<span id="cb56-24"><a href="#cb56-24" aria-hidden="true" tabindex="-1"></a>               np<span class="sc">$</span><span class="fu">load</span>() <span class="sc">|&gt;</span> np<span class="sc">$</span><span class="fu">expand_dims</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">1</span>)</span>
<span id="cb56-25"><a href="#cb56-25" aria-hidden="true" tabindex="-1"></a>      } <span class="cf">else</span> <span class="cf">if</span>(block_id <span class="sc">==</span> <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb56-26"><a href="#cb56-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># load weights for the final output normalization layer, which is not</span></span>
<span id="cb56-27"><a href="#cb56-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># part of a TransformerBlock</span></span>
<span id="cb56-28"><a href="#cb56-28" aria-hidden="true" tabindex="-1"></a>        \(...) <span class="fu">weights_path</span>(<span class="st">&quot;7B/norm.weight.npy&quot;</span>) <span class="sc">|&gt;</span></span>
<span id="cb56-29"><a href="#cb56-29" aria-hidden="true" tabindex="-1"></a>               np<span class="sc">$</span><span class="fu">load</span>() <span class="sc">|&gt;</span> np<span class="sc">$</span><span class="fu">expand_dims</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">1</span>)</span>
<span id="cb56-30"><a href="#cb56-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-31"><a href="#cb56-31" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>w <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">add_weight</span>(<span class="at">shape =</span> w_shape,</span>
<span id="cb56-32"><a href="#cb56-32" aria-hidden="true" tabindex="-1"></a>                              <span class="at">initializer =</span> initializer,</span>
<span id="cb56-33"><a href="#cb56-33" aria-hidden="true" tabindex="-1"></a>                              <span class="at">trainable =</span> <span class="cn">TRUE</span>)</span>
<span id="cb56-34"><a href="#cb56-34" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb56-35"><a href="#cb56-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-36"><a href="#cb56-36" aria-hidden="true" tabindex="-1"></a>  rrms <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb56-37"><a href="#cb56-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># reciprocal root mean square along the last axis</span></span>
<span id="cb56-38"><a href="#cb56-38" aria-hidden="true" tabindex="-1"></a>    x <span class="sc">%&gt;%</span> <span class="co"># (batch_size, seqlen, n_features)</span></span>
<span id="cb56-39"><a href="#cb56-39" aria-hidden="true" tabindex="-1"></a>      tf<span class="sc">$</span>math<span class="sc">$</span><span class="fu">square</span>() <span class="sc">%&gt;%</span></span>
<span id="cb56-40"><a href="#cb56-40" aria-hidden="true" tabindex="-1"></a>      tf<span class="sc">$</span><span class="fu">reduce_mean</span>(<span class="at">axis =</span> <span class="sc">-</span>1L, <span class="at">keepdims =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span> <span class="co"># (batch_size, seqlen, 1)</span></span>
<span id="cb56-41"><a href="#cb56-41" aria-hidden="true" tabindex="-1"></a>      tf<span class="sc">$</span>math<span class="sc">$</span><span class="fu">add</span>(self<span class="sc">$</span>eps) <span class="sc">%&gt;%</span> <span class="co"># for numerical stability</span></span>
<span id="cb56-42"><a href="#cb56-42" aria-hidden="true" tabindex="-1"></a>      tf<span class="sc">$</span>math<span class="sc">$</span><span class="fu">rsqrt</span>()</span>
<span id="cb56-43"><a href="#cb56-43" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb56-44"><a href="#cb56-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-45"><a href="#cb56-45" aria-hidden="true" tabindex="-1"></a>  call <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb56-46"><a href="#cb56-46" aria-hidden="true" tabindex="-1"></a>    x <span class="sc">*</span> self<span class="sc">$</span><span class="fu">rrms</span>(x) <span class="sc">*</span> self<span class="sc">$</span>w</span>
<span id="cb56-47"><a href="#cb56-47" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb56-48"><a href="#cb56-48" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p><code>RMSnorm()</code> has a single trainable tensor <code>w</code>. In the forward pass, each
value in the input is multiplied by the reciprocal-root-mean-square of
all the values in the feature axis and by <code>w</code>. Certainly a mouthful, but
just a simple sequence of arithmetic transformations in the end,
designed for the express purpose of adjusting the range of values
passing through.</p>
<p>Let’s kick the tires on it:</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>norm <span class="ot">&lt;-</span> <span class="fu">RMSNorm</span>()</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>,</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>              <span class="dv">2</span>, <span class="dv">3</span>), <span class="at">nrow =</span> <span class="dv">2</span>)</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="fu">norm</span>(m)</span></code></pre></div>
<pre><code>tf.Tensor(
[[0.         1.4142132 ]
 [0.44721353 1.3416406 ]], shape=(2, 2), dtype=float32)</code></pre>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="fu">norm</span>(m<span class="sc">*</span><span class="dv">10</span>)</span></code></pre></div>
<pre><code>tf.Tensor(
[[0.         1.4142137 ]
 [0.44721362 1.3416408 ]], shape=(2, 2), dtype=float32)</code></pre>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="fu">norm</span>(m<span class="sc">*</span><span class="dv">100</span>)</span></code></pre></div>
<pre><code>tf.Tensor(
[[0.        1.4142137]
 [0.4472136 1.3416408]], shape=(2, 2), dtype=float32)</code></pre>
<h3 id="feedforward"><code>FeedForward</code></h3>
<p>Next up is <code>FeedForward()</code></p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="fu">FeedForward</span>(keras<span class="sc">$</span>layers<span class="sc">$</span>Layer) <span class="sc">%py_class%</span> {</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>  initialize <span class="ot">&lt;-</span> <span class="cf">function</span>(hidden_dim, <span class="at">multiple_of =</span> 256L,</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>                         ..., <span class="at">block_id =</span> <span class="cn">NULL</span>) {</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>    super<span class="sc">$</span><span class="fu">initialize</span>()</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(<span class="sc">!</span><span class="fu">is.null</span>(multiple_of)) {</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>      hidden_dim <span class="ot">&lt;-</span> hidden_dim <span class="sc">%&gt;%</span></span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>        { <span class="fu">as.integer</span>( . <span class="sc">*</span> (<span class="dv">2</span><span class="sc">/</span><span class="dv">3</span>)) } <span class="sc">%&gt;%</span></span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>        { (. <span class="sc">+</span> multiple_of <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">%/%</span> multiple_of } <span class="sc">%&gt;%</span></span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>        { . <span class="sc">*</span> multiple_of }</span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>hidden_dim <span class="ot">&lt;-</span> hidden_dim</span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>block_id <span class="ot">&lt;-</span> block_id</span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a>  build <span class="ot">&lt;-</span> <span class="cf">function</span>(input_shape) {</span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a>    output_dim <span class="ot">&lt;-</span> input_shape <span class="sc">|&gt;</span> <span class="fu">as.integer</span>() <span class="sc">|&gt;</span> <span class="fu">tail</span>(<span class="dv">1</span>)</span>
<span id="cb63-20"><a href="#cb63-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-21"><a href="#cb63-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(<span class="fu">is.null</span>(self<span class="sc">$</span>block_id))</span>
<span id="cb63-22"><a href="#cb63-22" aria-hidden="true" tabindex="-1"></a>      load_weight <span class="ot">&lt;-</span> \(...) <span class="cn">NULL</span></span>
<span id="cb63-23"><a href="#cb63-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span></span>
<span id="cb63-24"><a href="#cb63-24" aria-hidden="true" tabindex="-1"></a>      load_weight <span class="ot">&lt;-</span> \(name) \(...) np<span class="sc">$</span><span class="fu">load</span>(<span class="fu">weights_path</span>(</span>
<span id="cb63-25"><a href="#cb63-25" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;7B/layers.{self$block_id}.feed_forward.{name}.weight.npy&quot;</span>))<span class="sc">$</span><span class="st">`</span><span class="at">T</span><span class="st">`</span></span>
<span id="cb63-26"><a href="#cb63-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-27"><a href="#cb63-27" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>w1 <span class="ot">&lt;-</span> <span class="fu">Dense</span>(self<span class="sc">$</span>hidden_dim, <span class="at">use_bias =</span> <span class="cn">FALSE</span>,</span>
<span id="cb63-28"><a href="#cb63-28" aria-hidden="true" tabindex="-1"></a>                     <span class="at">kernel_initializer =</span> <span class="fu">load_weight</span>(<span class="st">&quot;w1&quot;</span>))</span>
<span id="cb63-29"><a href="#cb63-29" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>w2 <span class="ot">&lt;-</span> <span class="fu">Dense</span>(output_dim, <span class="at">use_bias =</span> <span class="cn">FALSE</span>,</span>
<span id="cb63-30"><a href="#cb63-30" aria-hidden="true" tabindex="-1"></a>                     <span class="at">kernel_initializer =</span> <span class="fu">load_weight</span>(<span class="st">&quot;w2&quot;</span>))</span>
<span id="cb63-31"><a href="#cb63-31" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>w3 <span class="ot">&lt;-</span> <span class="fu">Dense</span>(self<span class="sc">$</span>hidden_dim, <span class="at">use_bias =</span> <span class="cn">FALSE</span>,</span>
<span id="cb63-32"><a href="#cb63-32" aria-hidden="true" tabindex="-1"></a>                     <span class="at">kernel_initializer =</span> <span class="fu">load_weight</span>(<span class="st">&quot;w3&quot;</span>))</span>
<span id="cb63-33"><a href="#cb63-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-34"><a href="#cb63-34" aria-hidden="true" tabindex="-1"></a>    super<span class="sc">$</span><span class="fu">build</span>(input_shape)</span>
<span id="cb63-35"><a href="#cb63-35" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb63-36"><a href="#cb63-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-37"><a href="#cb63-37" aria-hidden="true" tabindex="-1"></a>  call <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb63-38"><a href="#cb63-38" aria-hidden="true" tabindex="-1"></a>    <span class="fu">import_from</span>({self}, w1, w2, w3)</span>
<span id="cb63-39"><a href="#cb63-39" aria-hidden="true" tabindex="-1"></a>    <span class="fu">import_from</span>(tf<span class="sc">$</span>nn, silu)</span>
<span id="cb63-40"><a href="#cb63-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-41"><a href="#cb63-41" aria-hidden="true" tabindex="-1"></a>    x <span class="sc">%&gt;%</span></span>
<span id="cb63-42"><a href="#cb63-42" aria-hidden="true" tabindex="-1"></a>      { <span class="fu">silu</span>(<span class="fu">w1</span>(.)) <span class="sc">*</span> <span class="fu">w3</span>(.) } <span class="sc">%&gt;%</span> <span class="co"># SwiGLU</span></span>
<span id="cb63-43"><a href="#cb63-43" aria-hidden="true" tabindex="-1"></a>      <span class="fu">w2</span>()</span>
<span id="cb63-44"><a href="#cb63-44" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb63-45"><a href="#cb63-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-46"><a href="#cb63-46" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p><code>FeedForward</code> consists of three <code>Dense</code> layers. <code>initialize</code> does some
simple arithmetic, munging on the input value <code>hidden_dim</code> to ensure the
size is a performant multiple of 256, and <code>build</code> is mostly boiler plate
for creating the layers and loading the weights.</p>
<p>The novelty of <code>FeedForward()</code> is in the <code>call()</code> method, where rather
than composing the <code>Dense</code> layers in a conventional sequential model
with, say, ReLU activations in between and maybe some dropout, the
layers are composed to form a “SwiGLU” unit. The publication by <span class="citation" data-cites="shazeer2020glu"><a href="#ref-shazeer2020glu" role="doc-biblioref">Shazeer</a> (<a href="#ref-shazeer2020glu" role="doc-biblioref">2020</a>)</span>
of SwiGLU and other variations on GLU is an exemplar of the types
of explorations and improvements around the Transformer architecture
since its initial publication in
<a href="https://arxiv.org/abs/1706.03762">2017</a>; a steady accretion of
enhancements that has brought us to today. The <code>Feedforward$call()</code> is
just a single SwiGLU followed by a linear projection. In its essence,
it’s a clever composition of three (learned) linear projections, an
element-wise multiplication, and a <a href="https://www.tensorflow.org/api_docs/python/tf/nn/silu"><code>silu()</code>
activation</a>
function.</p>
<p>Perhaps the most surprising observation to make here is the relative
dearth of activation functions, or even non-linearities, not just in
<code>FeedForward</code>, but overall. The <code>silu()</code> in this feedforward, the
reciprocal-root-mean-square in <code>RMSnorm()</code>, and a <code>softmax()</code> in
<code>Attention()</code> are the only non-linear transformations in the whole
sequence of <code>TransformerBlock</code>s. Everything else is a linear
transformation!</p>
<h3 id="attention"><code>Attention</code></h3>
<p>Finally, let’s turn our attention to <code>Attention()</code>.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="fu">Attention</span>(keras<span class="sc">$</span>layers<span class="sc">$</span>Layer) <span class="sc">%py_class%</span> {</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>  initialize <span class="ot">&lt;-</span> <span class="cf">function</span>(head_size, n_heads,</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>                         ..., <span class="at">block_id =</span> <span class="cn">NULL</span>) {</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>    super<span class="sc">$</span><span class="fu">initialize</span>(...)</span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>head_size <span class="ot">&lt;-</span> head_size</span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>n_heads <span class="ot">&lt;-</span> n_heads</span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">is.null</span>(block_id))</span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a>      load_weight <span class="ot">&lt;-</span> <span class="cf">function</span>(name) <span class="cn">NULL</span></span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span></span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a>      load_weight <span class="ot">&lt;-</span> \(name) \(...) np<span class="sc">$</span><span class="fu">load</span>(<span class="fu">weights_path</span>(</span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;7B/layers.{block_id}.attention.{name}.weight.npy&quot;</span>))<span class="sc">$</span><span class="st">`</span><span class="at">T</span><span class="st">`</span></span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a>    Dense <span class="ot">&lt;-</span> <span class="cf">function</span>(name) keras<span class="sc">$</span>layers<span class="sc">$</span><span class="fu">Dense</span>(</span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a>      <span class="at">units =</span> n_heads <span class="sc">*</span> head_size,</span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a>      <span class="at">use_bias =</span> <span class="cn">FALSE</span>,</span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a>      <span class="at">kernel_initializer =</span> <span class="fu">load_weight</span>(name)</span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>wq <span class="ot">&lt;-</span> <span class="fu">Dense</span>(<span class="st">&quot;wq&quot;</span>)</span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>wk <span class="ot">&lt;-</span> <span class="fu">Dense</span>(<span class="st">&quot;wk&quot;</span>)</span>
<span id="cb64-23"><a href="#cb64-23" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>wv <span class="ot">&lt;-</span> <span class="fu">Dense</span>(<span class="st">&quot;wv&quot;</span>)</span>
<span id="cb64-24"><a href="#cb64-24" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>wo <span class="ot">&lt;-</span> <span class="fu">Dense</span>(<span class="st">&quot;wo&quot;</span>)</span>
<span id="cb64-25"><a href="#cb64-25" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb64-26"><a href="#cb64-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-27"><a href="#cb64-27" aria-hidden="true" tabindex="-1"></a>  call <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb64-28"><a href="#cb64-28" aria-hidden="true" tabindex="-1"></a>    <span class="fu">c</span>(batch_size, seqlen, n_features) <span class="sc">%&lt;-%</span> tf<span class="sc">$</span><span class="fu">unstack</span>(tf<span class="sc">$</span><span class="fu">shape</span>(x))</span>
<span id="cb64-29"><a href="#cb64-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-30"><a href="#cb64-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. project (linear transform) x into</span></span>
<span id="cb64-31"><a href="#cb64-31" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    query, key, and value tensors</span></span>
<span id="cb64-32"><a href="#cb64-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. reshape q k v, splitting out the last dim (n_features)</span></span>
<span id="cb64-33"><a href="#cb64-33" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    into n_heads independent subspaces,</span></span>
<span id="cb64-34"><a href="#cb64-34" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    each with size head_size.</span></span>
<span id="cb64-35"><a href="#cb64-35" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    (n_features == head_size * n_heads)</span></span>
<span id="cb64-36"><a href="#cb64-36" aria-hidden="true" tabindex="-1"></a>    split_heads_shape <span class="ot">&lt;-</span> <span class="fu">c</span>(batch_size, seqlen,</span>
<span id="cb64-37"><a href="#cb64-37" aria-hidden="true" tabindex="-1"></a>                           self<span class="sc">$</span>n_heads, self<span class="sc">$</span>head_size)</span>
<span id="cb64-38"><a href="#cb64-38" aria-hidden="true" tabindex="-1"></a>    q <span class="ot">&lt;-</span> x <span class="sc">|&gt;</span> self<span class="sc">$</span><span class="fu">wq</span>() <span class="sc">|&gt;</span> tf<span class="sc">$</span><span class="fu">reshape</span>(split_heads_shape)</span>
<span id="cb64-39"><a href="#cb64-39" aria-hidden="true" tabindex="-1"></a>    k <span class="ot">&lt;-</span> x <span class="sc">|&gt;</span> self<span class="sc">$</span><span class="fu">wk</span>() <span class="sc">|&gt;</span> tf<span class="sc">$</span><span class="fu">reshape</span>(split_heads_shape)</span>
<span id="cb64-40"><a href="#cb64-40" aria-hidden="true" tabindex="-1"></a>    v <span class="ot">&lt;-</span> x <span class="sc">|&gt;</span> self<span class="sc">$</span><span class="fu">wv</span>() <span class="sc">|&gt;</span> tf<span class="sc">$</span><span class="fu">reshape</span>(split_heads_shape)</span>
<span id="cb64-41"><a href="#cb64-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-42"><a href="#cb64-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># embed positional information in query and key</span></span>
<span id="cb64-43"><a href="#cb64-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (bsz, seqlen, n_heads, head_size)</span></span>
<span id="cb64-44"><a href="#cb64-44" aria-hidden="true" tabindex="-1"></a>    q <span class="sc">%&lt;&gt;%</span> <span class="fu">apply_rotary_embedding</span>()</span>
<span id="cb64-45"><a href="#cb64-45" aria-hidden="true" tabindex="-1"></a>    k <span class="sc">%&lt;&gt;%</span> <span class="fu">apply_rotary_embedding</span>()</span>
<span id="cb64-46"><a href="#cb64-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-47"><a href="#cb64-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># reshape:</span></span>
<span id="cb64-48"><a href="#cb64-48" aria-hidden="true" tabindex="-1"></a>    <span class="co">#   move heads out of the last 2 axes,</span></span>
<span id="cb64-49"><a href="#cb64-49" aria-hidden="true" tabindex="-1"></a>    <span class="co">#   so later matmuls are performed across the subspaces (heads)</span></span>
<span id="cb64-50"><a href="#cb64-50" aria-hidden="true" tabindex="-1"></a>    <span class="co">#   between (seqlen, head_size) axes</span></span>
<span id="cb64-51"><a href="#cb64-51" aria-hidden="true" tabindex="-1"></a>    v <span class="ot">&lt;-</span> tf<span class="sc">$</span><span class="fu">transpose</span>(v, <span class="fu">c</span>(0L, 2L, 1L, 3L)) <span class="co"># (bsz, n_heads, seqlen, head_size)</span></span>
<span id="cb64-52"><a href="#cb64-52" aria-hidden="true" tabindex="-1"></a>    q <span class="ot">&lt;-</span> tf<span class="sc">$</span><span class="fu">transpose</span>(q, <span class="fu">c</span>(0L, 2L, 1L, 3L)) <span class="co"># (bsz, n_heads, seqlen, head_size)</span></span>
<span id="cb64-53"><a href="#cb64-53" aria-hidden="true" tabindex="-1"></a>    k <span class="ot">&lt;-</span> tf<span class="sc">$</span><span class="fu">transpose</span>(k, <span class="fu">c</span>(0L, 2L, 3L, 1L)) <span class="co"># (bsz, n_heads, head_size, seqlen)</span></span>
<span id="cb64-54"><a href="#cb64-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-55"><a href="#cb64-55" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate and normalize attention scores</span></span>
<span id="cb64-56"><a href="#cb64-56" aria-hidden="true" tabindex="-1"></a>    scores <span class="ot">&lt;-</span> q <span class="sc">%*%</span> k                       <span class="co"># (bsz, n_heads, seqlen, seqlen)</span></span>
<span id="cb64-57"><a href="#cb64-57" aria-hidden="true" tabindex="-1"></a>    scores <span class="ot">&lt;-</span> scores <span class="sc">/</span> <span class="fu">sqrt</span>(self<span class="sc">$</span>head_size) <span class="co"># scale</span></span>
<span id="cb64-58"><a href="#cb64-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-59"><a href="#cb64-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># apply causal mask, so the model can&#39;t &quot;look ahead&quot; during training</span></span>
<span id="cb64-60"><a href="#cb64-60" aria-hidden="true" tabindex="-1"></a>    mask <span class="ot">&lt;-</span> <span class="fu">make_mask</span>(seqlen, <span class="at">dtype =</span> scores<span class="sc">$</span>dtype)</span>
<span id="cb64-61"><a href="#cb64-61" aria-hidden="true" tabindex="-1"></a>    scores <span class="sc">%&lt;&gt;%</span> { . <span class="sc">+</span> mask }</span>
<span id="cb64-62"><a href="#cb64-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-63"><a href="#cb64-63" aria-hidden="true" tabindex="-1"></a>    scores <span class="ot">&lt;-</span> tf<span class="sc">$</span>nn<span class="sc">$</span><span class="fu">softmax</span>(scores, <span class="at">axis =</span> <span class="sc">-</span>1L)</span>
<span id="cb64-64"><a href="#cb64-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-65"><a href="#cb64-65" aria-hidden="true" tabindex="-1"></a>    <span class="co"># adjust values tensor with attention scores</span></span>
<span id="cb64-66"><a href="#cb64-66" aria-hidden="true" tabindex="-1"></a>                      <span class="co"># scores (bsz, n_heads, seqlen, seqlen)</span></span>
<span id="cb64-67"><a href="#cb64-67" aria-hidden="true" tabindex="-1"></a>                      <span class="co"># v      (bsz, n_heads, seqlen, head_size)</span></span>
<span id="cb64-68"><a href="#cb64-68" aria-hidden="true" tabindex="-1"></a>    output <span class="ot">&lt;-</span> scores <span class="sc">%*%</span> v   <span class="co"># (bsz, n_heads, seqlen, head_size)</span></span>
<span id="cb64-69"><a href="#cb64-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-70"><a href="#cb64-70" aria-hidden="true" tabindex="-1"></a>    <span class="co"># combine heads back into a single features dim,</span></span>
<span id="cb64-71"><a href="#cb64-71" aria-hidden="true" tabindex="-1"></a>    <span class="co"># so Attention output_shape==input_shape</span></span>
<span id="cb64-72"><a href="#cb64-72" aria-hidden="true" tabindex="-1"></a>    output <span class="ot">&lt;-</span> output <span class="sc">|&gt;</span></span>
<span id="cb64-73"><a href="#cb64-73" aria-hidden="true" tabindex="-1"></a>      tf<span class="sc">$</span><span class="fu">transpose</span>(<span class="fu">c</span>(0L, 2L, 1L, 3L)) <span class="sc">|&gt;</span> <span class="co"># (bsz, seqlen, n_heads, head_size)</span></span>
<span id="cb64-74"><a href="#cb64-74" aria-hidden="true" tabindex="-1"></a>      tf<span class="sc">$</span><span class="fu">reshape</span>(tf<span class="sc">$</span><span class="fu">shape</span>(x))            <span class="co"># (bsz, seqlen, n_heads * head_size)</span></span>
<span id="cb64-75"><a href="#cb64-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-76"><a href="#cb64-76" aria-hidden="true" tabindex="-1"></a>    <span class="co"># one more trainable linear projection for good luck</span></span>
<span id="cb64-77"><a href="#cb64-77" aria-hidden="true" tabindex="-1"></a>    output <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">wo</span>(output) <span class="co"># (bsz, seqlen, n_heads * head_size)</span></span>
<span id="cb64-78"><a href="#cb64-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-79"><a href="#cb64-79" aria-hidden="true" tabindex="-1"></a>    output</span>
<span id="cb64-80"><a href="#cb64-80" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb64-81"><a href="#cb64-81" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p><code>Attention</code> in LLaMA is similar but not identical to the Attention
described in the <a href="https://arxiv.org/abs/1706.03762">original Transformers
paper</a> (and available as a keras
builtin under <code>keras$layers$MultiHeadAttention()</code>). The core novelty is
the addition of the <code>apply_rotary_embedding()</code> function, which we’ll
describe shortly. The additional novelty is balanced by the simplicity
from the fact that the layer is performing self-attention—we don’t need
to pass in different query, key, and value tensors (or reason about what
that means), since the same input serves all three roles. Note that the
conventional <code>MultiHeadAttention()</code> layer is covered quite thoroughly in
the 2nd Edition of <a href="http://rstd.io/dlwr-2e">Deep Learning with R</a>,
including a full implementation of attention in base R.</p>
<p>To develop an understanding of the mechanics in a layer like this, it’s
helpful to temporarily <em>unsee</em> some of the minutia that can act as a fog
obscuring the essence of the operation. In this instance, if we
temporarily strip out the <code>transpose()</code>s and <code>reshape()</code>s (as clever and
vital as they are), this is what’s left:</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>call <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># split input into three learned linear projections</span></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>  q <span class="ot">&lt;-</span> x <span class="sc">|&gt;</span> self<span class="sc">$</span><span class="fu">wq</span>()</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>  k <span class="ot">&lt;-</span> x <span class="sc">|&gt;</span> self<span class="sc">$</span><span class="fu">wk</span>()</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>  v <span class="ot">&lt;-</span> x <span class="sc">|&gt;</span> self<span class="sc">$</span><span class="fu">wv</span>()</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># rotate q,k to inject position information.</span></span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># cross q,k to calculate an attention score for each token pair.</span></span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>  scores <span class="ot">&lt;-</span> <span class="fu">rotate</span>(q) <span class="sc">%*%</span> <span class="fu">rotate</span>(k)   <span class="sc">|&gt;</span>  <span class="fu">normalize_scores</span>()</span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># adjust the 3rd projection with the attention scores</span></span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> scores <span class="sc">%*%</span> v</span>
<span id="cb65-13"><a href="#cb65-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-14"><a href="#cb65-14" aria-hidden="true" tabindex="-1"></a>  self<span class="sc">$</span><span class="fu">wo</span>(output) <span class="co"># one more learned linear projection for good luck</span></span>
<span id="cb65-15"><a href="#cb65-15" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Returning to the <code>transpose()</code>s and <code>reshapes()</code>, you can observe that
their purpose is to make it so that the attention calculations are
performed across <code>n_heads</code> independent subspaces, rather than in a
single larger space. The same reasoning drives this decision as that
driving usage of depthwise-separable convolutions in image models.
Empirically, for the fixed compute budget, factoring features into
independent subspaces performs better than doing the same core
operations in single larger feature space. As with all things, there is
a balance to strike between <code>n_heads</code> (the number of subspaces) and
<code>head_dim</code> (the size of each subspace). The LLaMA authors have struck
the balance like this at the various model sizes:</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lapply</span>(<span class="fu">c</span>(<span class="st">&quot;7B&quot;</span>, <span class="st">&quot;13B&quot;</span>, <span class="st">&quot;30B&quot;</span>, <span class="st">&quot;65B&quot;</span>), \(size) {</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">&lt;-</span> <span class="fu">read_json</span>(<span class="fu">weights_path</span>(<span class="st">&quot;{size}/params.json&quot;</span>))</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">with</span>(p, <span class="fu">list</span>(<span class="at">llama_size =</span> size,</span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>               <span class="at">n_heads =</span> n_heads,</span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>               <span class="at">head_dim =</span> dim <span class="sc">%/%</span> n_heads))</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>}) <span class="sc">|&gt;</span> dplyr<span class="sc">::</span><span class="fu">bind_rows</span>()</span></code></pre></div>
<pre><code># A tibble: 4 × 3
  llama_size n_heads head_dim
  &lt;chr&gt;        &lt;int&gt;    &lt;int&gt;
1 7B              32      128
2 13B             40      128
3 30B             52      128
4 65B             64      128</code></pre>
<p>Next lets turn our attention to the causal attention mask.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>make_mask <span class="ot">&lt;-</span> <span class="cf">function</span>(seqlen, <span class="at">dtype =</span> <span class="fu">k_floatx</span>()) {</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;-</span> tf<span class="sc">$</span><span class="fu">range</span>(seqlen)</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>  mask <span class="ot">&lt;-</span> tf<span class="sc">$</span><span class="fu">where</span>(x[, tf<span class="sc">$</span>newaxis] <span class="sc">&lt;</span> x[tf<span class="sc">$</span>newaxis, ],</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>                   tf<span class="sc">$</span><span class="fu">constant</span>(<span class="sc">-</span><span class="cn">Inf</span>, <span class="at">dtype =</span> dtype),</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>                   tf<span class="sc">$</span><span class="fu">constant</span>(<span class="dv">0</span>, <span class="at">dtype =</span> dtype))</span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># broadcast over batch and heads dim</span></span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>  mask[tf<span class="sc">$</span>newaxis, tf<span class="sc">$</span>newaxis, , ] <span class="co"># (1, 1, seqlen, seqlen)</span></span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>The mask is a strictly upper triangular matrix filled with <code>-Inf</code>
values. Adding the mask to the attention scores prevents the model from
being able to “look ahead” and see the attention score for a token
pairing it hasn’t seen yet at a particular position in the sequence.
This need for a mask is best thought of as a vestige from training,
an apparatus that the model needed to learn with and now it can’t function without.
During training, gradients are calculated for predictions from all
token positions in a sequence, including predictions tokens where the correct
answer is <em>right there</em>, as the very next token in same sequence. The mask
prevents the model from being able to cheat and look ahead into the future,
something it won’t be able to do once it’s we’re running it for inference.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="fu">make_mask</span>(<span class="at">seqlen =</span> 5L)</span></code></pre></div>
<pre><code>tf.Tensor(
[[[[  0. -inf -inf -inf -inf]
   [  0.   0. -inf -inf -inf]
   [  0.   0.   0. -inf -inf]
   [  0.   0.   0.   0. -inf]
   [  0.   0.   0.   0.   0.]]]], shape=(1, 1, 5, 5), dtype=float32)</code></pre>
<h3 id="rotary-position-embedding">Rotary Position Embedding</h3>
<p>Next lets turn our attention to <code>apply_rotary_embedding()</code>. This core
innovation was published by <span class="citation" data-cites="su2022roformer"><a href="#ref-su2022roformer" role="doc-biblioref">Su et al.</a> (<a href="#ref-su2022roformer" role="doc-biblioref">2022</a>)</span> in the paper titled
<a href="https://arxiv.org/abs/2104.09864">“RoFormer: Enhanced Transformer with Rotary Position Embedding”</a>.</p>
<p>Some context:</p>
<ul>
<li><p>The bare <code>Attention()</code> mechanism doesn’t leave any possibility for a
token’s position in a sequence to affect the attention scores, since
only token-pairs are scored. Attention treats its input like a
bag-of-tokens.</p></li>
<li><p>The position of a token in a sequence is clearly important, and the
attention layer should have access to that information.</p></li>
<li><p>The absolute position of a token in a sequence is less important
than the relative position between tokens. (Especially so for long
sequences).</p></li>
</ul>
<p>Which leads us into the complex plane. If we imagine the features as
complex numbers, we can rotate them, and we can calculate angles between
them. From the Roformers paper:</p>
<blockquote>
<p>Specifically, incorporating the relative position embedding is
straightforward: simply rotate the affine-transformed word embedding
vector by amount of angle multiples of its position index and thus
interprets the intuition behind <em>Rotary Position Embedding</em></p>
</blockquote>
<p>Expanding slightly: the rotation matrix is designed so that
subsequently, after rotating our <code>q</code> and <code>k</code> token sequence embedding
the same way, the <em>angle</em> between token features is a function of the
relative distance between those tokens in the token sequence. The
relative angle between two tokens is invariant to the absolute
position of those tokens in the full sequence.</p>
<p>In short, the rotation injects positional information. The meaning or
interpretability of that positional information, or how it is meant to
be used, or even extracted from the result of <code>q %*% k</code>, is left to the
model to learn.</p>
<p>Here is the code:</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>apply_rotary_embedding <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(., seqlen, ., head_size) <span class="sc">%&lt;-%</span></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>    tf<span class="sc">$</span><span class="fu">unstack</span>(tf<span class="sc">$</span><span class="fu">shape</span>(x))</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>  rotation_matrix <span class="ot">&lt;-</span> <span class="fu">compute_rotation_matrix</span>(seqlen, head_size)</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>  x <span class="sc">%&gt;%</span></span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">view_as_complex</span>() <span class="sc">%&gt;%</span></span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>    { . <span class="sc">*</span> rotation_matrix } <span class="sc">%&gt;%</span></span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">view_as_real</span>()</span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-14"><a href="#cb71-14" aria-hidden="true" tabindex="-1"></a>compute_rotation_matrix <span class="ot">&lt;-</span></span>
<span id="cb71-15"><a href="#cb71-15" aria-hidden="true" tabindex="-1"></a>  <span class="cf">function</span>(seqlen, feature_dim, <span class="at">theta =</span> <span class="dv">10000</span>) {</span>
<span id="cb71-16"><a href="#cb71-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># `feature_dim` here is going to be attention$head_size</span></span>
<span id="cb71-17"><a href="#cb71-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># `seqlen` is going to match the token sequence length.</span></span>
<span id="cb71-18"><a href="#cb71-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-19"><a href="#cb71-19" aria-hidden="true" tabindex="-1"></a>    t <span class="ot">&lt;-</span> tf<span class="sc">$</span><span class="fu">range</span>(seqlen, <span class="at">dtype =</span> tf<span class="sc">$</span>float32)</span>
<span id="cb71-20"><a href="#cb71-20" aria-hidden="true" tabindex="-1"></a>    freqs <span class="ot">&lt;-</span> tf<span class="sc">$</span><span class="fu">range</span>(<span class="at">start =</span> <span class="dv">0</span>, <span class="at">limit =</span> <span class="dv">1</span>, <span class="at">delta =</span> <span class="dv">1</span> <span class="sc">/</span> (feature_dim <span class="sc">%/%</span> <span class="dv">2</span>),</span>
<span id="cb71-21"><a href="#cb71-21" aria-hidden="true" tabindex="-1"></a>                      <span class="at">dtype =</span> tf<span class="sc">$</span>float32)</span>
<span id="cb71-22"><a href="#cb71-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">tf_assert</span>(tf<span class="sc">$</span><span class="fu">size</span>(freqs) <span class="sc">==</span> feature_dim <span class="sc">%/%</span> <span class="dv">2</span>)</span>
<span id="cb71-23"><a href="#cb71-23" aria-hidden="true" tabindex="-1"></a>    freqs <span class="ot">&lt;-</span> <span class="fl">1.0</span> <span class="sc">/</span> (theta <span class="sc">^</span> freqs)</span>
<span id="cb71-24"><a href="#cb71-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-25"><a href="#cb71-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># outer product; (seqlen, head_size/2)</span></span>
<span id="cb71-26"><a href="#cb71-26" aria-hidden="true" tabindex="-1"></a>    freqs <span class="ot">&lt;-</span> tf<span class="sc">$</span><span class="fu">einsum</span>(<span class="st">&#39;a,b-&gt;ab&#39;</span>, t, freqs)</span>
<span id="cb71-27"><a href="#cb71-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-28"><a href="#cb71-28" aria-hidden="true" tabindex="-1"></a>    rot_mat <span class="ot">&lt;-</span> tf<span class="sc">$</span><span class="fu">complex</span>(tf<span class="sc">$</span><span class="fu">cos</span>(freqs), tf<span class="sc">$</span><span class="fu">sin</span>(freqs))</span>
<span id="cb71-29"><a href="#cb71-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-30"><a href="#cb71-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the positional embedding will be broadcast across batch and heads dim</span></span>
<span id="cb71-31"><a href="#cb71-31" aria-hidden="true" tabindex="-1"></a>    rot_mat[tf<span class="sc">$</span>newaxis, , tf<span class="sc">$</span>newaxis, ] <span class="co">#(1, seqlen, 1, headdim/2)</span></span>
<span id="cb71-32"><a href="#cb71-32" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb71-33"><a href="#cb71-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-34"><a href="#cb71-34" aria-hidden="true" tabindex="-1"></a>view_as_complex <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb71-35"><a href="#cb71-35" aria-hidden="true" tabindex="-1"></a>  tf<span class="sc">$</span><span class="fu">complex</span>(x[<span class="fu">all_dims</span>(), <span class="st">`</span><span class="at">::2</span><span class="st">`</span>],</span>
<span id="cb71-36"><a href="#cb71-36" aria-hidden="true" tabindex="-1"></a>             x[<span class="fu">all_dims</span>(), <span class="st">`</span><span class="at">2::2</span><span class="st">`</span>])</span>
<span id="cb71-37"><a href="#cb71-37" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb71-38"><a href="#cb71-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-39"><a href="#cb71-39" aria-hidden="true" tabindex="-1"></a>view_as_real <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb71-40"><a href="#cb71-40" aria-hidden="true" tabindex="-1"></a>  <span class="co"># xs = (..., f);  xs2 = (..., f*2)</span></span>
<span id="cb71-41"><a href="#cb71-41" aria-hidden="true" tabindex="-1"></a>  xs <span class="ot">&lt;-</span> tf<span class="sc">$</span><span class="fu">shape</span>(x)</span>
<span id="cb71-42"><a href="#cb71-42" aria-hidden="true" tabindex="-1"></a>  xs2 <span class="ot">&lt;-</span> tf<span class="sc">$</span><span class="fu">concat</span>(<span class="fu">list</span>(xs[<span class="dv">1</span><span class="sc">:</span>(<span class="fu">length</span>(xs)<span class="sc">-</span><span class="dv">1</span>)],</span>
<span id="cb71-43"><a href="#cb71-43" aria-hidden="true" tabindex="-1"></a>                        xs[<span class="fu">length</span>(xs), <span class="at">drop =</span> <span class="cn">FALSE</span>] <span class="sc">*</span> 2L),</span>
<span id="cb71-44"><a href="#cb71-44" aria-hidden="true" tabindex="-1"></a>                   <span class="at">axis =</span> 0L)</span>
<span id="cb71-45"><a href="#cb71-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-46"><a href="#cb71-46" aria-hidden="true" tabindex="-1"></a>  x2 <span class="ot">&lt;-</span> tf<span class="sc">$</span><span class="fu">stack</span>(<span class="fu">list</span>(<span class="fu">Re</span>(x), <span class="fu">Im</span>(x)), <span class="at">axis =</span> <span class="sc">-</span>1L)</span>
<span id="cb71-47"><a href="#cb71-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-48"><a href="#cb71-48" aria-hidden="true" tabindex="-1"></a>  <span class="co"># (..., f, 2) -&gt; (..., f*2)</span></span>
<span id="cb71-49"><a href="#cb71-49" aria-hidden="true" tabindex="-1"></a>  tf<span class="sc">$</span><span class="fu">reshape</span>(x2, xs2)</span>
<span id="cb71-50"><a href="#cb71-50" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>As you can see, to imagine the embedding features as existing in the
complex plane, we merely treat adjacent pairs of floats in the
underlying array as the real and imaginary part of a complex number. We
rotate the embeddings in the complex plane, then go back to imagining
the features as existing in the real plane. Again, the job of
interpreting the meaning of the features after rotation is left to the
model to learn.</p>
<p>We can quickly confirm that the rotary embeddings <em>only</em> rotate features
and don’t scale them:</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>near <span class="ot">&lt;-</span> <span class="cf">function</span> (x, y, <span class="at">tol =</span> <span class="fl">1e-6</span>) <span class="fu">abs</span>(x <span class="sc">-</span> y) <span class="sc">&lt;</span> tol</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a><span class="fu">all</span>(<span class="fu">near</span>(<span class="dv">1</span>, <span class="fu">Mod</span>(<span class="fu">compute_rotation_matrix</span>(2048L, 128L))))</span></code></pre></div>
<pre><code>tf.Tensor(True, shape=(), dtype=bool)</code></pre>
<p>There is one more trick to observe before moving on: because of some of
the mathematical properties of the rotation matrix, it’s possible to
avoid doing a full complex multiply operation and still arrive at the
same result. Also, since the rotation matrix never changes, it makes
sense to only compute it once and cache it, like so:</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>precomputed_rotation_matrix <span class="ot">&lt;-</span> <span class="fu">compute_rotation_matrix</span>(</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">seqlen =</span> 2048L, <span class="co"># LLaMA max seqlen</span></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">feature_dim =</span> <span class="fu">with</span>(params, dim <span class="sc">%/%</span> n_heads)  <span class="co"># head_size</span></span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>apply_rotary_embedding_faster <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a>  rotate_every_two <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a>    x1 <span class="ot">&lt;-</span> x[<span class="fu">all_dims</span>(), <span class="st">`</span><span class="at">::2</span><span class="st">`</span>]</span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a>    x2 <span class="ot">&lt;-</span> x[<span class="fu">all_dims</span>(), <span class="st">`</span><span class="at">2::2</span><span class="st">`</span>]</span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a>    x_ <span class="ot">&lt;-</span> tf<span class="sc">$</span><span class="fu">stack</span>(<span class="fu">list</span>(<span class="sc">-</span>x2, x1), <span class="at">axis =</span> <span class="sc">-</span>1L)</span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a>    tf<span class="sc">$</span><span class="fu">reshape</span>(x_, tf<span class="sc">$</span><span class="fu">shape</span>(x))</span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb74-14"><a href="#cb74-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-15"><a href="#cb74-15" aria-hidden="true" tabindex="-1"></a>  repeat_each_twice <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb74-16"><a href="#cb74-16" aria-hidden="true" tabindex="-1"></a>    tf<span class="sc">$</span><span class="st">`</span><span class="at">repeat</span><span class="st">`</span>(x, 2L, <span class="at">axis =</span> <span class="sc">-</span>1L)</span>
<span id="cb74-17"><a href="#cb74-17" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb74-18"><a href="#cb74-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-19"><a href="#cb74-19" aria-hidden="true" tabindex="-1"></a>  seqlen <span class="ot">&lt;-</span> tf<span class="sc">$</span><span class="fu">shape</span>(x)[<span class="dv">2</span>]</span>
<span id="cb74-20"><a href="#cb74-20" aria-hidden="true" tabindex="-1"></a>  rot <span class="ot">&lt;-</span> precomputed_rotation_matrix[, <span class="cn">NA</span><span class="sc">:</span>seqlen, , ]</span>
<span id="cb74-21"><a href="#cb74-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-22"><a href="#cb74-22" aria-hidden="true" tabindex="-1"></a>  cos <span class="ot">&lt;-</span> <span class="fu">Re</span>(rot) <span class="sc">|&gt;</span> <span class="fu">repeat_each_twice</span>()</span>
<span id="cb74-23"><a href="#cb74-23" aria-hidden="true" tabindex="-1"></a>  sin <span class="ot">&lt;-</span> <span class="fu">Im</span>(rot) <span class="sc">|&gt;</span> <span class="fu">repeat_each_twice</span>()</span>
<span id="cb74-24"><a href="#cb74-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-25"><a href="#cb74-25" aria-hidden="true" tabindex="-1"></a>  (x <span class="sc">*</span> cos) <span class="sc">+</span> (<span class="fu">rotate_every_two</span>(x) <span class="sc">*</span> sin)</span>
<span id="cb74-26"><a href="#cb74-26" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>rand <span class="ot">&lt;-</span> tf<span class="sc">$</span>random<span class="sc">$</span><span class="fu">uniform</span>(<span class="fu">shape</span>(<span class="dv">3</span>, <span class="dv">8</span>, params<span class="sc">$</span>n_heads, <span class="dv">128</span>))</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a><span class="fu">all</span>(<span class="fu">apply_rotary_embedding</span>(rand) <span class="sc">==</span></span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">apply_rotary_embedding_faster</span>(rand))</span></code></pre></div>
<pre><code>tf.Tensor(True, shape=(), dtype=bool)</code></pre>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>apply_rotary_embedding <span class="ot">&lt;-</span> apply_rotary_embedding_faster</span></code></pre></div>
<p>Finally, note that the rotary positional embeddings are applied within
each <code>Attention</code> layer. This is different from the original Transformer
implementation, where a positional embedding was only added once at the
head of the model. Similar to residual connections, you can think of the
presence of these repeated injections of positional information as
relieving the remaining trainable layers from the burden of allocating
some of their weights to the task of “passing through” or “preserving”
the positional information for later layers.</p>
<p>Positional embeddings are a rich subject that also comes up in other
deep learning architectures, like denoising diffusion <span class="citation" data-cites="falbelkeydiffusion">(<a href="#ref-falbelkeydiffusion" role="doc-biblioref">Falbel and Keydana 2023</a>)</span>,
so time spent understanding them better is time well
spent. For the purposes of this blog post we’ve covered the points
needed and we’ll move on to tying all pieces together. To go deeper and
develop a more mathematically informed understand of RoPE, two excellent
starting points are:</p>
<ol type="1">
<li><p><a href="https://arxiv.org/abs/2104.09864">The original paper</a> by <span class="citation" data-cites="su2022roformer"><a href="#ref-su2022roformer" role="doc-biblioref">Su et al.</a> (<a href="#ref-su2022roformer" role="doc-biblioref">2022</a>)</span></p></li>
<li><p><a href="https://blog.eleuther.ai/rotary-embeddings/">This blog post</a> by
<span class="citation" data-cites="rope-eleutherai"><a href="#ref-rope-eleutherai" role="doc-biblioref">Biderman et al.</a> (<a href="#ref-rope-eleutherai" role="doc-biblioref">2021</a>)</span></p></li>
</ol>
<h3 id="tying-it-all-together">Tying it all together</h3>
<p>With <code>Tokenizer</code>, <code>Embedding</code>, <code>TransformerBlock</code> (<code>RMSNorm</code>,
<code>Attention</code> <code>FeedForward</code> and <code>apply_rotary_embedding</code>) all covered,
it’s time to tie all the pieces together into a <code>Transformer</code> model. We
could do this using <code>%py_class%</code> like with the other layers above, but
it’s just as easy to move over to using the Keras functional API at this
point.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>layer_transformer_block <span class="ot">&lt;-</span> <span class="fu">create_layer_wrapper</span>(TransformerBlock)</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>layer_rms_norm <span class="ot">&lt;-</span> <span class="fu">create_layer_wrapper</span>(RMSNorm)</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a><span class="co"># input to the model will be output from the tokenizer</span></span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>input <span class="ot">&lt;-</span> <span class="fu">layer_input</span>(<span class="fu">shape</span>(<span class="cn">NA</span>)) <span class="co">#, dtype = &quot;int32&quot;)</span></span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> input <span class="sc">|&gt;</span></span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tok_embeddings</span>()  <span class="co"># instantiated earlier in the blog-post</span></span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(block_id <span class="cf">in</span> <span class="fu">seq_len0</span>(params<span class="sc">$</span>n_layers)) {</span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;-</span> x <span class="sc">|&gt;</span></span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_transformer_block</span>(<span class="at">attn_head_size =</span> params<span class="sc">$</span>dim <span class="sc">%/%</span> params<span class="sc">$</span>n_heads,</span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a>                            <span class="at">attn_n_heads =</span> params<span class="sc">$</span>n_heads,</span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a>                            <span class="at">norm_eps =</span> params<span class="sc">$</span>norm_eps,</span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a>                            <span class="at">block_id =</span> block_id)</span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-18"><a href="#cb78-18" aria-hidden="true" tabindex="-1"></a><span class="co"># final output projection into logits of output tokens</span></span>
<span id="cb78-19"><a href="#cb78-19" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> x <span class="sc">|&gt;</span></span>
<span id="cb78-20"><a href="#cb78-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_rms_norm</span>(<span class="at">block_id =</span> <span class="sc">-</span><span class="dv">1</span>, <span class="at">eps =</span> params<span class="sc">$</span>norm_eps) <span class="sc">|&gt;</span></span>
<span id="cb78-21"><a href="#cb78-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(</span>
<span id="cb78-22"><a href="#cb78-22" aria-hidden="true" tabindex="-1"></a>    tokenizer<span class="sc">$</span><span class="fu">vocab_size</span>(), <span class="at">use_bias =</span> <span class="cn">FALSE</span>,</span>
<span id="cb78-23"><a href="#cb78-23" aria-hidden="true" tabindex="-1"></a>    <span class="at">kernel_initializer =</span> \(...) np<span class="sc">$</span><span class="fu">load</span>(<span class="fu">weights_path</span>(<span class="st">&quot;7B/output.weight.npy&quot;</span>))<span class="sc">$</span><span class="st">`</span><span class="at">T</span><span class="st">`</span></span>
<span id="cb78-24"><a href="#cb78-24" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb78-25"><a href="#cb78-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-26"><a href="#cb78-26" aria-hidden="true" tabindex="-1"></a><span class="co"># slice out the logits for the last token</span></span>
<span id="cb78-27"><a href="#cb78-27" aria-hidden="true" tabindex="-1"></a><span class="fu">with_options</span>(<span class="fu">c</span>(<span class="at">tensorflow.extract.warn_negatives_pythonic =</span> <span class="cn">FALSE</span>), {</span>
<span id="cb78-28"><a href="#cb78-28" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> x[, <span class="sc">-</span><span class="dv">1</span>, ]</span>
<span id="cb78-29"><a href="#cb78-29" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb78-30"><a href="#cb78-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-31"><a href="#cb78-31" aria-hidden="true" tabindex="-1"></a>llama <span class="ot">&lt;-</span> <span class="fu">keras_model</span>(input, output) <span class="sc">%&gt;%</span></span>
<span id="cb78-32"><a href="#cb78-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compile</span>(<span class="at">jit_compile =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>The input to the model is tokenized text and the output is the
(unnormalized) probabilities for each token in <code>tokenizer$vocab_size()</code>
being the next token in the sequence.</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>next_token_probs <span class="ot">&lt;-</span> prompt <span class="sc">%&gt;%</span></span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>  tokenizer<span class="sc">$</span><span class="fu">tokenize</span>() <span class="sc">%&gt;%</span></span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">llama</span>()</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>next_token_probs</span></code></pre></div>
<pre><code>tf.Tensor(
[[-2.4503722e+00 -3.4463339e+00  1.3200411e+01 ...  4.8804146e-01
  -1.3277926e+00  9.9985600e-03]], shape=(1, 32000), dtype=float32)</code></pre>
<p>Sampling strategies for selecting a token from the token logits is a
rich topic, (also covered thoroughly in the <a href="http://rstd.io/dlwr-2e">Deep Learning with
R</a> book), but this blog post is long enough
already. So for now, let’s just take the <code>argmax()</code>.</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>sampler <span class="ot">&lt;-</span> \(logits) tf<span class="sc">$</span><span class="fu">argmax</span>(logits, <span class="at">axis =</span> <span class="sc">-</span>1L, <span class="at">output_type =</span> <span class="st">&quot;int32&quot;</span>)</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>(next_token <span class="ot">&lt;-</span> <span class="fu">sampler</span>(next_token_probs))</span></code></pre></div>
<pre><code>tf.Tensor([304], shape=(1), dtype=int32)</code></pre>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>tokenizer<span class="sc">$</span><span class="fu">detokenize</span>(next_token) <span class="sc">|&gt;</span> <span class="fu">as.character</span>()</span></code></pre></div>
<pre><code>[1] &quot;to&quot;</code></pre>
<p>Let’s run it for a few tokens and let LLaMa finish the sentence:</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>prompt_tokens <span class="ot">&lt;-</span> tokenizer<span class="sc">$</span><span class="fu">tokenize</span>(<span class="st">&quot;The best way to attract bees&quot;</span>)</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>) {</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a>  next_token_probs <span class="ot">&lt;-</span> prompt_tokens <span class="sc">|&gt;</span> <span class="fu">llama</span>()</span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a>  next_token <span class="ot">&lt;-</span> <span class="fu">sampler</span>(next_token_probs)</span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a>  prompt_tokens <span class="sc">%&lt;&gt;%</span> { tf<span class="sc">$</span><span class="fu">concat</span>(<span class="fu">c</span>(., next_token), <span class="at">axis =</span> <span class="sc">-</span>1L) }</span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-10"><a href="#cb85-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># end of sentence</span></span>
<span id="cb85-11"><a href="#cb85-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">as.logical</span>(next_token <span class="sc">==</span> tokenizer<span class="sc">$</span><span class="fu">string_to_id</span>(<span class="st">&quot;.&quot;</span>)))</span>
<span id="cb85-12"><a href="#cb85-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">break</span></span>
<span id="cb85-13"><a href="#cb85-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb85-14"><a href="#cb85-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-15"><a href="#cb85-15" aria-hidden="true" tabindex="-1"></a>prompt_tokens <span class="sc">|&gt;</span></span>
<span id="cb85-16"><a href="#cb85-16" aria-hidden="true" tabindex="-1"></a>  tokenizer<span class="sc">$</span><span class="fu">detokenize</span>() <span class="sc">|&gt;</span></span>
<span id="cb85-17"><a href="#cb85-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.character</span>() <span class="sc">|&gt;</span></span>
<span id="cb85-18"><a href="#cb85-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">strwrap</span>(<span class="dv">60</span>) <span class="sc">|&gt;</span> <span class="fu">writeLines</span>()</span></code></pre></div>
<pre><code>The best way to attract bees to your garden is to plant a
variety of flowers that bloom at different times.</code></pre>
<h3 id="wrapping-up">Wrapping up</h3>
<p>In this blog post we’ve walked through the LLaMA architecture
implemented in R TensorFlow, including how to load pretrained weights,
and then run the model to generate a sentence. Note, much of the code in
this blog post is tailored for didactic purposes. While the
implementation of the LLaMA architecture covered in this blog post is
appropriate for training, there are a few modifications you’ll want to
make before doing a lot of text generation. Those include things like:</p>
<ul>
<li><p>In the <code>Attention</code> layer, caching the <code>k</code> and <code>v</code> tensors. Then,
after the first forward pass with the initial prompt, only feeding
the model the one new token from the <code>sampler()</code>, rather than
feeding the model all the tokens of the full prompt on each forward
pass.</p></li>
<li><p>Only generating the causal mask <code>make_mask()</code> and <code>rotary_matrix</code>
slices once per forward pass, instead of within each <code>Attention</code>
call.</p></li>
<li><p>Updating the <code>TransformerBlock</code> to be cache-aware and to pass
through the appropriate arguments to <code>Attention()</code></p></li>
<li><p>Wrapping all the additional book-keeping logic in a custom
<code>TransformerDecoder()</code> class.</p></li>
</ul>
<p>The changes required to implement these optimizations for inference
balloon the code size and are mostly about book-keeping, so we won’t go
through them in this blog post. However, you can find a fuller
implementation of LLaMA in R Tensorflow, including a cache-aware
<code>generate()</code> method that only feeds the model one token at a time during
the main inference loop, (and compiles to XLA!),
<a href="https://gist.github.com/t-kalinowski/62e9a1bbf8d670b712082c1765be4df4">here</a>.</p>
<p>That’s all for now. Thanks for reading and happy travels to all
exploring this exciting LLM terrain!</p>
<p>Photo by <a href="https://unsplash.com/@sebastiengoldberg?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Sébastien Goldberg</a> on <a href="https://unsplash.com/photos/xgQZ1rXbYa4?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a></p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r distill-force-highlighting-css"><code class="sourceCode r"></code></pre></div>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-rope-eleutherai" class="csl-entry" role="doc-biblioentry">
Biderman, Stella, Sid Black, Charles Foster, Leo Gao, Eric Hallahan, Horace He, Ben Wang, and Phil Wang. 2021. <span>“Rotary Embeddings: A Relative Revolution.”</span> <a href="blog.eleuther.ai/rotary-embeddings/">blog.eleuther.ai/rotary-embeddings/</a>.
</div>
<div id="ref-falbelkeydiffusion" class="csl-entry" role="doc-biblioentry">
Falbel, Daniel, and Sigrid Keydana. 2023. <span>“Posit AI Blog: De-Noising Diffusion with Torch.”</span> <a href="https://blogs.rstudio.com/tensorflow/posts/2023-04-13-denoising-diffusion/">https://blogs.rstudio.com/tensorflow/posts/2023-04-13-denoising-diffusion/</a>.
</div>
<div id="ref-hoffmann2022training" class="csl-entry" role="doc-biblioentry">
Hoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. <span>“Training Compute-Optimal Large Language Models.”</span> <a href="https://arxiv.org/abs/2203.15556">https://arxiv.org/abs/2203.15556</a>.
</div>
<div id="ref-shazeer2020glu" class="csl-entry" role="doc-biblioentry">
Shazeer, Noam. 2020. <span>“GLU Variants Improve Transformer.”</span> <a href="https://arxiv.org/abs/2002.05202">https://arxiv.org/abs/2002.05202</a>.
</div>
<div id="ref-su2022roformer" class="csl-entry" role="doc-biblioentry">
Su, Jianlin, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2022. <span>“RoFormer: Enhanced Transformer with Rotary Position Embedding.”</span> <a href="https://arxiv.org/abs/2104.09864">https://arxiv.org/abs/2104.09864</a>.
</div>
<div id="ref-touvron2023" class="csl-entry" role="doc-biblioentry">
Touvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, et al. 2023. <span>“LLaMA: Open and Efficient Foundation Language Models.”</span> <a href="https://doi.org/10.48550/ARXIV.2302.13971">https://doi.org/10.48550/ARXIV.2302.13971</a>.
</div>
<div id="ref-vaswani2017attention" class="csl-entry" role="doc-biblioentry">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention Is All You Need.”</span> <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.
</div>
</div>
<!--radix_placeholder_article_footer-->
<div class="article-footer">
  <p class="social_footer">
    <span class="disqus-comments">
      <i class="fas fa-comments"></i>
      &nbsp;
      <span class="disqus-comment-count" data-disqus-identifier="posts/2023-05-25-llama-tensorflow-keras/">Comment on this article</span>
    </span>
    <span class="article-sharing">
      Share: &nbsp;
      <a href="https://twitter.com/share?text=LLaMA%20in%20R%20with%20Keras%20and%20TensorFlow&amp;url=https%3A%2F%2Fblogs.rstudio.com%2Ftensorflow%2Fposts%2F2023-05-25-llama-tensorflow-keras%2F" aria-label="share on twitter">
        <i class="fab fa-twitter" aria-hidden="true"></i>
      </a>
      <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3A%2F%2Fblogs.rstudio.com%2Ftensorflow%2Fposts%2F2023-05-25-llama-tensorflow-keras%2F&amp;title=LLaMA%20in%20R%20with%20Keras%20and%20TensorFlow" aria-label="share on linkedin">
        <i class="fab fa-linkedin" aria-hidden="true"></i>
      </a>
    </span>
  </p>
  <script id="dsq-count-scr" src="https://tensorflow-for-r-blog.disqus.com/count.js" async></script>
  <div id="disqus_thread" class="hidden"></div>
  <script type="text/javascript" cookie-consent="functionality">
var disqus_config = function () {
  this.page.url = 'https://blogs.rstudio.com/tensorflow/posts/2023-05-25-llama-tensorflow-keras/';
  this.page.identifier = 'posts/2023-05-25-llama-tensorflow-keras/';
};
(function() {
  var d = document, s = d.createElement('script');
  s.src = 'https://tensorflow-for-r-blog.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
})();
</script>
  <p>
    <div class="subscribe">
<div id="subscribe-caption" style="line-height: 1.2; margin-bottom: 2px;">Enjoy this blog? Get notified of new posts by email:</div>

<script src="https://app-ab02.marketo.com/js/forms2/js/forms2.min.js"></script>
<form class="mtktoBlogEmailForm" id="mktoForm_2224"></form>
<script>

// establish metrics based on where the form is located
var in_sidebar = $('#subscribe-caption').parents('.sidebar-section').length;
if (in_sidebar) {
  var form_width = 190;
  var base_width = form_width - 23;
  var email_width = base_width + 'px';
  var label_width = (base_width - 20) + 'px';
  var button_width = email_width;
  var button_padding = '30px';
  var button_margin = '';
  var font_size = '12px';
} else {
  var form_width = 400;
  var base_width = form_width - 23;
  var email_width = base_width + 'px';
  var label_width = (base_width - 20) + 'px';
  var button_width = email_width;
  var button_padding = '30px';
  var button_margin = '12px';
  var font_size = '15px';
}

$('#subscribe-caption')
  .css('width', base_width)
  .css('font-size', font_size);


MktoForms2.loadForm("https://app-ab02.marketo.com", "709-NXN-706", 2224, function(form) {

  // get jquery reference to form
  form = $(form.getFormElem().get(0));

  $(form).css('width', form_width + 'px');
  $(form).find('.mktoOffset').remove();
  $(form).find('.mktoGutter').remove();
  $(form).find('.mktoEmailField').css('width', email_width);
  $(form).find('.mktoLabel').children().attr('style', 'font-weight: 400');
  $(form).find('.mktoLabel')
      .css('width', label_width)
      .css('font-size', font_size);
  $(form).find('.mktoButtonRow')
      .css('width', button_width)
      .css('text-align', 'center');
  $(form).find('.mktoButtonWrap').attr('style', '');
  $(form).find('.mktoButton')
      .css('margin-top', '10px')
      .css('padding-left', button_padding)
      .css('padding-right', button_padding)
      .css('font-size', font_size)
      .css('margin-top', button_margin);
});
</script>
Posts also available at <a href="https://www.r-bloggers.com">r-bloggers</a>

<script>
  document.addEventListener("DOMContentLoaded", function(){
    var menulink = document.querySelector("a[href='#category:R']");
    if (menulink) menulink.parentNode.style.display = "None";
    
    for (var e of document.querySelectorAll(".dt-tag")) {
      if (e.innerHTML == 'R') e.style.display = "None";
    }
  });
</script>
</div>
  </p>
</div>
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>


<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
  <h3 id="references">References</h3>
  <div id="references-listing"></div>
  <h3 id="reuse">Reuse</h3>
  <p>Text and figures are licensed under Creative Commons Attribution <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>. The figures that have been reused from other sources don't fall under this license and can be recognized by a note in their caption: "Figure from ...".</p>
  <h3 id="citation">Citation</h3>
  <p>For attribution, please cite this work as</p>
  <pre class="citation-appendix short">Kalinowski (2023, May 25). Posit AI Blog: LLaMA in R with Keras and TensorFlow. Retrieved from https://blogs.rstudio.com/tensorflow/posts/2023-05-25-llama-tensorflow-keras/</pre>
  <p>BibTeX citation</p>
  <pre class="citation-appendix long">@misc{kalinowskillama,
  author = {Kalinowski, Tomasz},
  title = {Posit AI Blog: LLaMA in R with Keras and TensorFlow},
  url = {https://blogs.rstudio.com/tensorflow/posts/2023-05-25-llama-tensorflow-keras/},
  year = {2023}
}</pre>
</div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
