<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { color: #00769e; background-color: #f1f3f5; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span { color: #00769e; } /* Normal */
code span.al { color: #ad0000; } /* Alert */
code span.an { color: #5e5e5e; } /* Annotation */
code span.at { color: #657422; } /* Attribute */
code span.bn { color: #ad0000; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #00769e; } /* ControlFlow */
code span.ch { color: #20794d; } /* Char */
code span.cn { color: #8f5902; } /* Constant */
code span.co { color: #5e5e5e; } /* Comment */
code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
code span.dt { color: #ad0000; } /* DataType */
code span.dv { color: #ad0000; } /* DecVal */
code span.er { color: #ad0000; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #ad0000; } /* Float */
code span.fu { color: #4758ab; } /* Function */
code span.im { } /* Import */
code span.in { color: #5e5e5e; } /* Information */
code span.kw { color: #00769e; } /* Keyword */
code span.op { color: #5e5e5e; } /* Operator */
code span.ot { color: #00769e; } /* Other */
code span.pp { color: #ad0000; } /* Preprocessor */
code span.sc { color: #5e5e5e; } /* SpecialChar */
code span.ss { color: #20794d; } /* SpecialString */
code span.st { color: #20794d; } /* String */
code span.va { color: #111111; } /* Variable */
code span.vs { color: #20794d; } /* VerbatimString */
code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
</style>

<style>
  div.csl-bib-body { }
  div.csl-entry {
    clear: both;
    }
  .hanging div.csl-entry {
    margin-left:2em;
    text-indent:-2em;
  }
  div.csl-left-margin {
    min-width:2em;
    float:left;
  }
  div.csl-right-inline {
    margin-left:2em;
    padding-left:1em;
  }
  div.csl-indent {
    margin-left: 2em;
  }
</style>

  <!--radix_placeholder_meta_tags-->
<title>Posit AI Blog: GPT-2 from scratch with torch</title>

<meta property="description" itemprop="description" content="Implementing a language model from scratch is, arguably, the best way to develop an accurate idea of how its engine works. Here, we use torch to code GPT-2, the immediate successor to the original GPT. In the end, you&#39;ll dispose of an R-native model that can make direct use of Hugging Face&#39;s pre-trained GPT-2 model weights."/>

<link rel="canonical" href="https://blogs.rstudio.com/tensorflow/posts/2023-06-20-gpt2-torch/"/>
<link rel="license" href="https://creativecommons.org/licenses/by/4.0/"/>
<link rel="icon" type="image/png" href="../../images/favicon.png"/>

<!--  https://schema.org/Article -->
<meta property="article:published" itemprop="datePublished" content="2023-06-20"/>
<meta property="article:created" itemprop="dateCreated" content="2023-06-20"/>
<meta name="article:author" content="Sigrid Keydana"/>

<!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
<meta property="og:title" content="Posit AI Blog: GPT-2 from scratch with torch"/>
<meta property="og:type" content="article"/>
<meta property="og:description" content="Implementing a language model from scratch is, arguably, the best way to develop an accurate idea of how its engine works. Here, we use torch to code GPT-2, the immediate successor to the original GPT. In the end, you&#39;ll dispose of an R-native model that can make direct use of Hugging Face&#39;s pre-trained GPT-2 model weights."/>
<meta property="og:url" content="https://blogs.rstudio.com/tensorflow/posts/2023-06-20-gpt2-torch/"/>
<meta property="og:image" content="https://blogs.rstudio.com/tensorflow/posts/2023-06-20-gpt2-torch/images/preview.jpg"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:site_name" content="Posit AI Blog"/>

<!--  https://dev.twitter.com/cards/types/summary -->
<meta property="twitter:card" content="summary_large_image"/>
<meta property="twitter:title" content="Posit AI Blog: GPT-2 from scratch with torch"/>
<meta property="twitter:description" content="Implementing a language model from scratch is, arguably, the best way to develop an accurate idea of how its engine works. Here, we use torch to code GPT-2, the immediate successor to the original GPT. In the end, you&#39;ll dispose of an R-native model that can make direct use of Hugging Face&#39;s pre-trained GPT-2 model weights."/>
<meta property="twitter:url" content="https://blogs.rstudio.com/tensorflow/posts/2023-06-20-gpt2-torch/"/>
<meta property="twitter:image" content="https://blogs.rstudio.com/tensorflow/posts/2023-06-20-gpt2-torch/images/preview.jpg"/>

<!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
<meta name="citation_title" content="Posit AI Blog: GPT-2 from scratch with torch"/>
<meta name="citation_fulltext_html_url" content="https://blogs.rstudio.com/tensorflow/posts/2023-06-20-gpt2-torch/"/>
<meta name="citation_fulltext_world_readable" content=""/>
<meta name="citation_online_date" content="2023/06/20"/>
<meta name="citation_publication_date" content="2023/06/20"/>
<meta name="citation_author" content="Sigrid Keydana"/>
<meta name="citation_author_institution" content="Posit"/>
<!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=RoFormer: Enhanced transformer with rotary position embedding;citation_author=Jianlin Su;citation_author=Yu Lu;citation_author=Shengfeng Pan;citation_author=Bo Wen;citation_author=Yunfeng Liu"/>
  <meta name="citation_reference" content="citation_title=Attention is all you need;citation_author=Ashish Vaswani;citation_author=Noam Shazeer;citation_author=Niki Parmar;citation_author=Jakob Uszkoreit;citation_author=Llion Jones;citation_author=Aidan N. Gomez;citation_author=Lukasz Kaiser;citation_author=Illia Polosukhin"/>
  <meta name="citation_reference" content="citation_title=Language models are unsupervised multitask learners;citation_author=Alec Radford;citation_author=Jeff Wu;citation_author=Rewon Child;citation_author=David Luan;citation_author=Dario Amodei;citation_author=Ilya Sutskever"/>
  <meta name="citation_reference" content="citation_title=Improving language understanding by generative pre-training;citation_author=Alec Radford;citation_author=Karthik Narasimhan"/>
  <meta name="citation_reference" content="citation_title=Layer normalization;citation_author=Jimmy Lei Ba;citation_author=Jamie Ryan Kiros;citation_author=Geoffrey E. Hinton"/>
  <meta name="citation_reference" content="citation_title=Gaussian error linear units (GELUs);citation_author=Dan Hendrycks;citation_author=Kevin Gimpel"/>
  <meta name="citation_reference" content="citation_title=Neural machine translation by jointly learning to align and translate;citation_volume=abs/1409.0473;citation_author=Dzmitry Bahdanau;citation_author=Kyunghyun Cho;citation_author=Yoshua Bengio"/>
  <!--radix_placeholder_rmarkdown_metadata-->

<script type="text/json" id="radix-rmarkdown-metadata">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["creative_commons","title","description","author","slug","date","categories","bibliography","output","preview","citation_url","canonical_url"]}},"value":[{"type":"character","attributes":{},"value":["CC BY"]},{"type":"character","attributes":{},"value":["GPT-2 from scratch with torch"]},{"type":"character","attributes":{},"value":["Implementing a language model from scratch is, arguably, the best way to develop an accurate idea of how its engine works. Here, we use torch to code GPT-2, the immediate successor to the original GPT. In the end, you'll dispose of an R-native model that can make direct use of Hugging Face's pre-trained GPT-2 model weights."]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","affiliation_url"]}},"value":[{"type":"character","attributes":{},"value":["Sigrid Keydana"]},{"type":"character","attributes":{},"value":["Posit"]},{"type":"character","attributes":{},"value":["https://www.posit.co/"]}]}]},{"type":"character","attributes":{},"value":["keydanagpt2"]},{"type":"character","attributes":{},"value":["2023-06-20"]},{"type":"character","attributes":{},"value":["Torch","R","Natural Language Processing"]},{"type":"character","attributes":{},"value":["references.bib"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained","toc"]}},"value":[{"type":"logical","attributes":{},"value":[false]},{"type":"logical","attributes":{},"value":[true]}]}]},{"type":"character","attributes":{},"value":["images/preview.jpg"]},{"type":"character","attributes":{},"value":["https://blogs.rstudio.com/tensorflow/posts/2023-06-20-gpt2-torch/"]},{"type":"character","attributes":{},"value":["https://blogs.rstudio.com/tensorflow/posts/2023-06-20-gpt2-torch/"]}]}
</script>
<!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["images/preview.jpg","images/transformer.png","references.bib"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
<meta name="distill:offset" content="../.."/>

<script type="application/javascript">

  window.headroom_prevent_pin = false;

  window.document.addEventListener("DOMContentLoaded", function (event) {

    // initialize headroom for banner
    var header = $('header').get(0);
    var headerHeight = header.offsetHeight;
    var headroom = new Headroom(header, {
      tolerance: 5,
      onPin : function() {
        if (window.headroom_prevent_pin) {
          window.headroom_prevent_pin = false;
          headroom.unpin();
        }
      }
    });
    headroom.init();
    if(window.location.hash)
      headroom.unpin();
    $(header).addClass('headroom--transition');

    // offset scroll location for banner on hash change
    // (see: https://github.com/WickyNilliams/headroom.js/issues/38)
    window.addEventListener("hashchange", function(event) {
      window.scrollTo(0, window.pageYOffset - (headerHeight + 25));
    });

    // responsive menu
    $('.distill-site-header').each(function(i, val) {
      var topnav = $(this);
      var toggle = topnav.find('.nav-toggle');
      toggle.on('click', function() {
        topnav.toggleClass('responsive');
      });
    });

    // nav dropdowns
    $('.nav-dropbtn').click(function(e) {
      $(this).next('.nav-dropdown-content').toggleClass('nav-dropdown-active');
      $(this).parent().siblings('.nav-dropdown')
         .children('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $("body").click(function(e){
      $('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $(".nav-dropdown").click(function(e){
      e.stopPropagation();
    });
  });
</script>

<style type="text/css">

/* Theme (user-documented overrideables for nav appearance) */

.distill-site-nav {
  color: rgba(255, 255, 255, 0.8);
  background-color: #0F2E3D;
  font-size: 15px;
  font-weight: 300;
}

.distill-site-nav a {
  color: inherit;
  text-decoration: none;
}

.distill-site-nav a:hover {
  color: white;
}

@media print {
  .distill-site-nav {
    display: none;
  }
}

.distill-site-header {

}

.distill-site-footer {

}


/* Site Header */

.distill-site-header {
  width: 100%;
  box-sizing: border-box;
  z-index: 3;
}

.distill-site-header .nav-left {
  display: inline-block;
  margin-left: 8px;
}

@media screen and (max-width: 768px) {
  .distill-site-header .nav-left {
    margin-left: 0;
  }
}


.distill-site-header .nav-right {
  float: right;
  margin-right: 8px;
}

.distill-site-header a,
.distill-site-header .title {
  display: inline-block;
  text-align: center;
  padding: 14px 10px 14px 10px;
}

.distill-site-header .title {
  font-size: 18px;
  min-width: 150px;
}

.distill-site-header .logo {
  padding: 0;
}

.distill-site-header .logo img {
  display: none;
  max-height: 20px;
  width: auto;
  margin-bottom: -4px;
}

.distill-site-header .nav-image img {
  max-height: 18px;
  width: auto;
  display: inline-block;
  margin-bottom: -3px;
}



@media screen and (min-width: 1000px) {
  .distill-site-header .logo img {
    display: inline-block;
  }
  .distill-site-header .nav-left {
    margin-left: 20px;
  }
  .distill-site-header .nav-right {
    margin-right: 20px;
  }
  .distill-site-header .title {
    padding-left: 12px;
  }
}


.distill-site-header .nav-toggle {
  display: none;
}

.nav-dropdown {
  display: inline-block;
  position: relative;
}

.nav-dropdown .nav-dropbtn {
  border: none;
  outline: none;
  color: rgba(255, 255, 255, 0.8);
  padding: 16px 10px;
  background-color: transparent;
  font-family: inherit;
  font-size: inherit;
  font-weight: inherit;
  margin: 0;
  margin-top: 1px;
  z-index: 2;
}

.nav-dropdown-content {
  display: none;
  position: absolute;
  background-color: white;
  min-width: 200px;
  border: 1px solid rgba(0,0,0,0.15);
  border-radius: 4px;
  box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.1);
  z-index: 1;
  margin-top: 2px;
  white-space: nowrap;
  padding-top: 4px;
  padding-bottom: 4px;
}

.nav-dropdown-content hr {
  margin-top: 4px;
  margin-bottom: 4px;
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.nav-dropdown-active {
  display: block;
}

.nav-dropdown-content a, .nav-dropdown-content .nav-dropdown-header {
  color: black;
  padding: 6px 24px;
  text-decoration: none;
  display: block;
  text-align: left;
}

.nav-dropdown-content .nav-dropdown-header {
  display: block;
  padding: 5px 24px;
  padding-bottom: 0;
  text-transform: uppercase;
  font-size: 14px;
  color: #999999;
  white-space: nowrap;
}

.nav-dropdown:hover .nav-dropbtn {
  color: white;
}

.nav-dropdown-content a:hover {
  background-color: #ddd;
  color: black;
}

.nav-right .nav-dropdown-content {
  margin-left: -45%;
  right: 0;
}

@media screen and (max-width: 768px) {
  .distill-site-header a, .distill-site-header .nav-dropdown  {display: none;}
  .distill-site-header a.nav-toggle {
    float: right;
    display: block;
  }
  .distill-site-header .title {
    margin-left: 0;
  }
  .distill-site-header .nav-right {
    margin-right: 0;
  }
  .distill-site-header {
    overflow: hidden;
  }
  .nav-right .nav-dropdown-content {
    margin-left: 0;
  }
}


@media screen and (max-width: 768px) {
  .distill-site-header.responsive {position: relative; min-height: 500px; }
  .distill-site-header.responsive a.nav-toggle {
    position: absolute;
    right: 0;
    top: 0;
  }
  .distill-site-header.responsive a,
  .distill-site-header.responsive .nav-dropdown {
    display: block;
    text-align: left;
  }
  .distill-site-header.responsive .nav-left,
  .distill-site-header.responsive .nav-right {
    width: 100%;
  }
  .distill-site-header.responsive .nav-dropdown {float: none;}
  .distill-site-header.responsive .nav-dropdown-content {position: relative;}
  .distill-site-header.responsive .nav-dropdown .nav-dropbtn {
    display: block;
    width: 100%;
    text-align: left;
  }
}

/* Site Footer */

.distill-site-footer {
  width: 100%;
  overflow: hidden;
  box-sizing: border-box;
  z-index: 3;
  margin-top: 30px;
  padding-top: 30px;
  padding-bottom: 30px;
  text-align: center;
}

/* Headroom */

d-title {
  padding-top: 6rem;
}

@media print {
  d-title {
    padding-top: 4rem;
  }
}

.headroom {
  z-index: 1000;
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
}

.headroom--transition {
  transition: all .4s ease-in-out;
}

.headroom--unpinned {
  top: -100px;
}

.headroom--pinned {
  top: 0;
}

/* adjust viewport for navbar height */
/* helps vertically center bootstrap (non-distill) content */
.min-vh-100 {
  min-height: calc(100vh - 100px) !important;
}

</style>

<script src="../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="../../site_libs/font-awesome-6.4.2/css/all.min.css" rel="stylesheet"/>
<link href="../../site_libs/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet"/>
<script src="../../site_libs/headroom-0.9.4/headroom.min.js"></script>
<script src="../../site_libs/autocomplete-0.37.1/autocomplete.min.js"></script>
<script src="../../site_libs/fuse-6.4.1/fuse.min.js"></script>

<script type="application/javascript">

function getMeta(metaName) {
  var metas = document.getElementsByTagName('meta');
  for (let i = 0; i < metas.length; i++) {
    if (metas[i].getAttribute('name') === metaName) {
      return metas[i].getAttribute('content');
    }
  }
  return '';
}

function offsetURL(url) {
  var offset = getMeta('distill:offset');
  return offset ? offset + '/' + url : url;
}

function createFuseIndex() {

  // create fuse index
  var options = {
    keys: [
      { name: 'title', weight: 20 },
      { name: 'categories', weight: 15 },
      { name: 'description', weight: 10 },
      { name: 'contents', weight: 5 },
    ],
    ignoreLocation: true,
    threshold: 0
  };
  var fuse = new window.Fuse([], options);

  // fetch the main search.json
  return fetch(offsetURL('search.json'))
    .then(function(response) {
      if (response.status == 200) {
        return response.json().then(function(json) {
          // index main articles
          json.articles.forEach(function(article) {
            fuse.add(article);
          });
          // download collections and index their articles
          return Promise.all(json.collections.map(function(collection) {
            return fetch(offsetURL(collection)).then(function(response) {
              if (response.status === 200) {
                return response.json().then(function(articles) {
                  articles.forEach(function(article) {
                    fuse.add(article);
                  });
                })
              } else {
                return Promise.reject(
                  new Error('Unexpected status from search index request: ' +
                            response.status)
                );
              }
            });
          })).then(function() {
            return fuse;
          });
        });

      } else {
        return Promise.reject(
          new Error('Unexpected status from search index request: ' +
                      response.status)
        );
      }
    });
}

window.document.addEventListener("DOMContentLoaded", function (event) {

  // get search element (bail if we don't have one)
  var searchEl = window.document.getElementById('distill-search');
  if (!searchEl)
    return;

  createFuseIndex()
    .then(function(fuse) {

      // make search box visible
      searchEl.classList.remove('hidden');

      // initialize autocomplete
      var options = {
        autoselect: true,
        hint: false,
        minLength: 2,
      };
      window.autocomplete(searchEl, options, [{
        source: function(query, callback) {
          const searchOptions = {
            isCaseSensitive: false,
            shouldSort: true,
            minMatchCharLength: 2,
            limit: 10,
          };
          var results = fuse.search(query, searchOptions);
          callback(results
            .map(function(result) { return result.item; })
          );
        },
        templates: {
          suggestion: function(suggestion) {
            var img = suggestion.preview && Object.keys(suggestion.preview).length > 0
              ? `<img src="${offsetURL(suggestion.preview)}"</img>`
              : '';
            var html = `
              <div class="search-item">
                <h3>${suggestion.title}</h3>
                <div class="search-item-description">
                  ${suggestion.description || ''}
                </div>
                <div class="search-item-preview">
                  ${img}
                </div>
              </div>
            `;
            return html;
          }
        }
      }]).on('autocomplete:selected', function(event, suggestion) {
        window.location.href = offsetURL(suggestion.path);
      });
      // remove inline display style on autocompleter (we want to
      // manage responsive display via css)
      $('.algolia-autocomplete').css("display", "");
    })
    .catch(function(error) {
      console.log(error);
    });

});

</script>

<style type="text/css">

.nav-search {
  font-size: x-small;
}

/* Algolioa Autocomplete */

.algolia-autocomplete {
  display: inline-block;
  margin-left: 10px;
  vertical-align: sub;
  background-color: white;
  color: black;
  padding: 6px;
  padding-top: 8px;
  padding-bottom: 0;
  border-radius: 6px;
  border: 1px #0F2E3D solid;
  width: 180px;
}


@media screen and (max-width: 768px) {
  .distill-site-nav .algolia-autocomplete {
    display: none;
    visibility: hidden;
  }
  .distill-site-nav.responsive .algolia-autocomplete {
    display: inline-block;
    visibility: visible;
  }
  .distill-site-nav.responsive .algolia-autocomplete .aa-dropdown-menu {
    margin-left: 0;
    width: 400px;
    max-height: 400px;
  }
}

.algolia-autocomplete .aa-input, .algolia-autocomplete .aa-hint {
  width: 90%;
  outline: none;
  border: none;
}

.algolia-autocomplete .aa-hint {
  color: #999;
}
.algolia-autocomplete .aa-dropdown-menu {
  width: 550px;
  max-height: 70vh;
  overflow-x: visible;
  overflow-y: scroll;
  padding: 5px;
  margin-top: 3px;
  margin-left: -150px;
  background-color: #fff;
  border-radius: 5px;
  border: 1px solid #999;
  border-top: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion {
  cursor: pointer;
  padding: 5px 4px;
  border-bottom: 1px solid #eee;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion:last-of-type {
  border-bottom: none;
  margin-bottom: 2px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item {
  overflow: hidden;
  font-size: 0.8em;
  line-height: 1.4em;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item h3 {
  font-size: 1rem;
  margin-block-start: 0;
  margin-block-end: 5px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-description {
  display: inline-block;
  overflow: hidden;
  height: 2.8em;
  width: 80%;
  margin-right: 4%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview {
  display: inline-block;
  width: 15%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img {
  height: 3em;
  width: auto;
  display: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img[src] {
  display: initial;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion.aa-cursor {
  background-color: #eee;
}
.algolia-autocomplete .aa-dropdown-menu .aa-suggestion em {
  font-weight: bold;
  font-style: normal;
}

</style>


<!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

<style type="text/css">

body {
  background-color: white;
}

.pandoc-table {
  width: 100%;
}

.pandoc-table>caption {
  margin-bottom: 10px;
}

.pandoc-table th:not([align]) {
  text-align: left;
}

.pagedtable-footer {
  font-size: 15px;
}

d-byline .byline {
  grid-template-columns: 2fr 2fr;
}

d-byline .byline h3 {
  margin-block-start: 1.5em;
}

d-byline .byline .authors-affiliations h3 {
  margin-block-start: 0.5em;
}

.authors-affiliations .orcid-id {
  width: 16px;
  height:16px;
  margin-left: 4px;
  margin-right: 4px;
  vertical-align: middle;
  padding-bottom: 2px;
}

d-title .dt-tags {
  margin-top: 1em;
  grid-column: text;
}

.dt-tags .dt-tag {
  text-decoration: none;
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0em 0.4em;
  margin-right: 0.5em;
  margin-bottom: 0.4em;
  font-size: 70%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

d-article table.gt_table td,
d-article table.gt_table th {
  border-bottom: none;
  font-size: 100%;
}

.html-widget {
  margin-bottom: 2.0em;
}

.l-screen-inset {
  padding-right: 16px;
}

.l-screen .caption {
  margin-left: 10px;
}

.shaded {
  background: rgb(247, 247, 247);
  padding-top: 20px;
  padding-bottom: 20px;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .html-widget {
  margin-bottom: 0;
  border: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .shaded-content {
  background: white;
}

.text-output {
  margin-top: 0;
  line-height: 1.5em;
}

.hidden {
  display: none !important;
}

hr.section-separator {
  border: none;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  margin: 0px;
}


d-byline {
  border-top: none;
}

d-article {
  padding-top: 2.5rem;
  padding-bottom: 30px;
  border-top: none;
}

d-appendix {
  padding-top: 30px;
}

d-article>p>img {
  width: 100%;
}

d-article h2 {
  margin: 1rem 0 1.5rem 0;
}

d-article h3 {
  margin-top: 1.5rem;
}

d-article iframe {
  border: 1px solid rgba(0, 0, 0, 0.1);
  margin-bottom: 2.0em;
  width: 100%;
}

/* Tweak code blocks */

d-article div.sourceCode code,
d-article pre code {
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
}

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: auto;
}

d-article div.sourceCode {
  background-color: white;
}

d-article div.sourceCode pre {
  padding-left: 10px;
  font-size: 12px;
  border-left: 2px solid rgba(0,0,0,0.1);
}

d-article pre {
  font-size: 12px;
  color: black;
  background: none;
  margin-top: 0;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

d-article pre a {
  border-bottom: none;
}

d-article pre a:hover {
  border-bottom: none;
  text-decoration: underline;
}

d-article details {
  grid-column: text;
  margin-bottom: 0.8em;
}

@media(min-width: 768px) {

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: visible !important;
}

d-article div.sourceCode pre {
  padding-left: 18px;
  font-size: 14px;
}

/* tweak for Pandoc numbered line within distill */
d-article pre.numberSource code > span {
    left: -2em;
}

d-article pre {
  font-size: 14px;
}

}

figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

/* CSS for d-contents */

.d-contents {
  grid-column: text;
  color: rgba(0,0,0,0.8);
  font-size: 0.9em;
  padding-bottom: 1em;
  margin-bottom: 1em;
  padding-bottom: 0.5em;
  margin-bottom: 1em;
  padding-left: 0.25em;
  justify-self: start;
}

@media(min-width: 1000px) {
  .d-contents.d-contents-float {
    height: 0;
    grid-column-start: 1;
    grid-column-end: 4;
    justify-self: center;
    padding-right: 3em;
    padding-left: 2em;
  }
}

.d-contents nav h3 {
  font-size: 18px;
  margin-top: 0;
  margin-bottom: 1em;
}

.d-contents li {
  list-style-type: none
}

.d-contents nav > ul {
  padding-left: 0;
}

.d-contents ul {
  padding-left: 1em
}

.d-contents nav ul li {
  margin-top: 0.6em;
  margin-bottom: 0.2em;
}

.d-contents nav a {
  font-size: 13px;
  border-bottom: none;
  text-decoration: none
  color: rgba(0, 0, 0, 0.8);
}

.d-contents nav a:hover {
  text-decoration: underline solid rgba(0, 0, 0, 0.6)
}

.d-contents nav > ul > li > a {
  font-weight: 600;
}

.d-contents nav > ul > li > ul {
  font-weight: inherit;
}

.d-contents nav > ul > li > ul > li {
  margin-top: 0.2em;
}


.d-contents nav ul {
  margin-top: 0;
  margin-bottom: 0.25em;
}

.d-article-with-toc h2:nth-child(2) {
  margin-top: 0;
}


/* Figure */

.figure {
  position: relative;
  margin-bottom: 2.5em;
  margin-top: 1.5em;
}

.figure .caption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

.figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

.figure .caption a {
  color: rgba(0, 0, 0, 0.6);
}

.figure .caption b,
.figure .caption strong, {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

/* Citations */

d-article .citation {
  color: inherit;
  cursor: inherit;
}

div.hanging-indent{
  margin-left: 1em; text-indent: -1em;
}

/* Citation hover box */

.tippy-box[data-theme~=light-border] {
  background-color: rgba(250, 250, 250, 0.95);
}

.tippy-content > p {
  margin-bottom: 0;
  padding: 2px;
}


/* Tweak 1000px media break to show more text */

@media(min-width: 1000px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }

  d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
  }
  figure .caption, .figure .caption, figure figcaption {
    font-size: 13px;
  }
}

@media(min-width: 1180px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 32px;
  }

  .grid {
    grid-column-gap: 32px;
  }
}


/* Get the citation styles for the appendix (not auto-injected on render since
   we do our own rendering of the citation appendix) */

d-appendix .citation-appendix,
.d-appendix .citation-appendix {
  font-size: 11px;
  line-height: 15px;
  border-left: 1px solid rgba(0, 0, 0, 0.1);
  padding-left: 18px;
  border: 1px solid rgba(0,0,0,0.1);
  background: rgba(0, 0, 0, 0.02);
  padding: 10px 18px;
  border-radius: 3px;
  color: rgba(150, 150, 150, 1);
  overflow: hidden;
  margin-top: -12px;
  white-space: pre-wrap;
  word-wrap: break-word;
}

/* Include appendix styles here so they can be overridden */

d-appendix {
  contain: layout style;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-top: 60px;
  margin-bottom: 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  color: rgba(0,0,0,0.5);
  padding-top: 60px;
  padding-bottom: 48px;
}

d-appendix h3 {
  grid-column: page-start / text-start;
  font-size: 15px;
  font-weight: 500;
  margin-top: 1em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.65);
}

d-appendix h3 + * {
  margin-top: 1em;
}

d-appendix ol {
  padding: 0 0 0 15px;
}

@media (min-width: 768px) {
  d-appendix ol {
    padding: 0 0 0 30px;
    margin-left: -30px;
  }
}

d-appendix li {
  margin-bottom: 1em;
}

d-appendix a {
  color: rgba(0, 0, 0, 0.6);
}

d-appendix > * {
  grid-column: text;
}

d-appendix > d-footnote-list,
d-appendix > d-citation-list,
d-appendix > distill-appendix {
  grid-column: screen;
}

/* Include footnote styles here so they can be overridden */

d-footnote-list {
  contain: layout style;
}

d-footnote-list > * {
  grid-column: text;
}

d-footnote-list a.footnote-backlink {
  color: rgba(0,0,0,0.3);
  padding-left: 0.5em;
}



/* Anchor.js */

.anchorjs-link {
  /*transition: all .25s linear; */
  text-decoration: none;
  border-bottom: none;
}
*:hover > .anchorjs-link {
  margin-left: -1.125em !important;
  text-decoration: none;
  border-bottom: none;
}

/* Social footer */

.social_footer {
  margin-top: 30px;
  margin-bottom: 0;
  color: rgba(0,0,0,0.67);
}

.disqus-comments {
  margin-right: 30px;
}

.disqus-comment-count {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  cursor: pointer;
}

#disqus_thread {
  margin-top: 30px;
}

.article-sharing a {
  border-bottom: none;
  margin-right: 8px;
}

.article-sharing a:hover {
  border-bottom: none;
}

.sidebar-section.subscribe {
  font-size: 12px;
  line-height: 1.6em;
}

.subscribe p {
  margin-bottom: 0.5em;
}


.article-footer .subscribe {
  font-size: 15px;
  margin-top: 45px;
}


.sidebar-section.custom {
  font-size: 12px;
  line-height: 1.6em;
}

.custom p {
  margin-bottom: 0.5em;
}

/* Styles for listing layout (hide title) */
.layout-listing d-title, .layout-listing .d-title {
  display: none;
}

/* Styles for posts lists (not auto-injected) */


.posts-with-sidebar {
  padding-left: 45px;
  padding-right: 45px;
}

.posts-list .description h2,
.posts-list .description p {
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
}

.posts-list .description h2 {
  font-weight: 700;
  border-bottom: none;
  padding-bottom: 0;
}

.posts-list h2.post-tag {
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
  padding-bottom: 12px;
}
.posts-list {
  margin-top: 60px;
  margin-bottom: 24px;
}

.posts-list .post-preview {
  text-decoration: none;
  overflow: hidden;
  display: block;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  padding: 24px 0;
}

.post-preview-last {
  border-bottom: none !important;
}

.posts-list .posts-list-caption {
  grid-column: screen;
  font-weight: 400;
}

.posts-list .post-preview h2 {
  margin: 0 0 6px 0;
  line-height: 1.2em;
  font-style: normal;
  font-size: 24px;
}

.posts-list .post-preview p {
  margin: 0 0 12px 0;
  line-height: 1.4em;
  font-size: 16px;
}

.posts-list .post-preview .thumbnail {
  box-sizing: border-box;
  margin-bottom: 24px;
  position: relative;
  max-width: 500px;
}
.posts-list .post-preview img {
  width: 100%;
  display: block;
}

.posts-list .metadata {
  font-size: 12px;
  line-height: 1.4em;
  margin-bottom: 18px;
}

.posts-list .metadata > * {
  display: inline-block;
}

.posts-list .metadata .publishedDate {
  margin-right: 2em;
}

.posts-list .metadata .dt-authors {
  display: block;
  margin-top: 0.3em;
  margin-right: 2em;
}

.posts-list .dt-tags {
  display: block;
  line-height: 1em;
}

.posts-list .dt-tags .dt-tag {
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0.3em 0.4em;
  margin-right: 0.2em;
  margin-bottom: 0.4em;
  font-size: 60%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

.posts-list img {
  opacity: 1;
}

.posts-list img[data-src] {
  opacity: 0;
}

.posts-more {
  clear: both;
}


.posts-sidebar {
  font-size: 16px;
}

.posts-sidebar h3 {
  font-size: 16px;
  margin-top: 0;
  margin-bottom: 0.5em;
  font-weight: 400;
  text-transform: uppercase;
}

.sidebar-section {
  margin-bottom: 30px;
}

.categories ul {
  list-style-type: none;
  margin: 0;
  padding: 0;
}

.categories li {
  color: rgba(0, 0, 0, 0.8);
  margin-bottom: 0;
}

.categories li>a {
  border-bottom: none;
}

.categories li>a:hover {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
}

.categories .active {
  font-weight: 600;
}

.categories .category-count {
  color: rgba(0, 0, 0, 0.4);
}


@media(min-width: 768px) {
  .posts-list .post-preview h2 {
    font-size: 26px;
  }
  .posts-list .post-preview .thumbnail {
    float: right;
    width: 30%;
    margin-bottom: 0;
  }
  .posts-list .post-preview .description {
    float: left;
    width: 45%;
  }
  .posts-list .post-preview .metadata {
    float: left;
    width: 20%;
    margin-top: 8px;
  }
  .posts-list .post-preview p {
    margin: 0 0 12px 0;
    line-height: 1.5em;
    font-size: 16px;
  }
  .posts-with-sidebar .posts-list {
    float: left;
    width: 75%;
  }
  .posts-with-sidebar .posts-sidebar {
    float: right;
    width: 20%;
    margin-top: 60px;
    padding-top: 24px;
    padding-bottom: 24px;
  }
}


/* Improve display for browsers without grid (IE/Edge <= 15) */

.downlevel {
  line-height: 1.6em;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  margin: 0;
}

.downlevel .d-title {
  padding-top: 6rem;
  padding-bottom: 1.5rem;
}

.downlevel .d-title h1 {
  font-size: 50px;
  font-weight: 700;
  line-height: 1.1em;
  margin: 0 0 0.5rem;
}

.downlevel .d-title p {
  font-weight: 300;
  font-size: 1.2rem;
  line-height: 1.55em;
  margin-top: 0;
}

.downlevel .d-byline {
  padding-top: 0.8em;
  padding-bottom: 0.8em;
  font-size: 0.8rem;
  line-height: 1.8em;
}

.downlevel .section-separator {
  border: none;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
}

.downlevel .d-article {
  font-size: 1.06rem;
  line-height: 1.7em;
  padding-top: 1rem;
  padding-bottom: 2rem;
}


.downlevel .d-appendix {
  padding-left: 0;
  padding-right: 0;
  max-width: none;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.5);
  padding-top: 40px;
  padding-bottom: 48px;
}

.downlevel .footnotes ol {
  padding-left: 13px;
}

.downlevel .base-grid,
.downlevel .distill-header,
.downlevel .d-title,
.downlevel .d-abstract,
.downlevel .d-article,
.downlevel .d-appendix,
.downlevel .distill-appendix,
.downlevel .d-byline,
.downlevel .d-footnote-list,
.downlevel .d-citation-list,
.downlevel .distill-footer,
.downlevel .appendix-bottom,
.downlevel .posts-container {
  padding-left: 40px;
  padding-right: 40px;
}

@media(min-width: 768px) {
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
  padding-left: 150px;
  padding-right: 150px;
  max-width: 900px;
}
}

.downlevel pre code {
  display: block;
  border-left: 2px solid rgba(0, 0, 0, .1);
  padding: 0 0 0 20px;
  font-size: 14px;
}

.downlevel code, .downlevel pre {
  color: black;
  background: none;
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

.downlevel .posts-list .post-preview {
  color: inherit;
}



</style>

<script type="application/javascript">

function is_downlevel_browser() {
  if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                 window.navigator.userAgent)) {
    return true;
  } else {
    return window.load_distill_framework === undefined;
  }
}

// show body when load is complete
function on_load_complete() {

  // add anchors
  if (window.anchors) {
    window.anchors.options.placement = 'left';
    window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
  }


  // set body to visible
  document.body.style.visibility = 'visible';

  // force redraw for leaflet widgets
  if (window.HTMLWidgets) {
    var maps = window.HTMLWidgets.findAll(".leaflet");
    $.each(maps, function(i, el) {
      var map = this.getMap();
      map.invalidateSize();
      map.eachLayer(function(layer) {
        if (layer instanceof L.TileLayer)
          layer.redraw();
      });
    });
  }

  // trigger 'shown' so htmlwidgets resize
  $('d-article').trigger('shown');
}

function init_distill() {

  init_common();

  // create front matter
  var front_matter = $('<d-front-matter></d-front-matter>');
  $('#distill-front-matter').wrap(front_matter);

  // create d-title
  $('.d-title').changeElementType('d-title');

  // separator
  var separator = '<hr class="section-separator" style="clear: both"/>';
  // prepend separator above appendix
  $('.d-byline').before(separator);
  $('.d-article').before(separator);

  // create d-byline
  var byline = $('<d-byline></d-byline>');
  $('.d-byline').replaceWith(byline);

  // create d-article
  var article = $('<d-article></d-article>');
  $('.d-article').wrap(article).children().unwrap();

  // move posts container into article
  $('.posts-container').appendTo($('d-article'));

  // create d-appendix
  $('.d-appendix').changeElementType('d-appendix');

  // flag indicating that we have appendix items
  var appendix = $('.appendix-bottom').children('h3').length > 0;

  // replace footnotes with <d-footnote>
  $('.footnote-ref').each(function(i, val) {
    appendix = true;
    var href = $(this).attr('href');
    var id = href.replace('#', '');
    var fn = $('#' + id);
    var fn_p = $('#' + id + '>p');
    fn_p.find('.footnote-back').remove();
    var text = fn_p.html();
    var dtfn = $('<d-footnote></d-footnote>');
    dtfn.html(text);
    $(this).replaceWith(dtfn);
  });
  // remove footnotes
  $('.footnotes').remove();

  // move refs into #references-listing
  $('#references-listing').replaceWith($('#refs'));

  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    var id = $(this).attr('id');
    $('.d-contents a[href="#' + id + '"]').parent().remove();
    appendix = true;
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
  });

  // show d-appendix if we have appendix content
  $("d-appendix").css('display', appendix ? 'grid' : 'none');

  // localize layout chunks to just output
  $('.layout-chunk').each(function(i, val) {

    // capture layout
    var layout = $(this).attr('data-layout');

    // apply layout to markdown level block elements
    var elements = $(this).children().not('details, div.sourceCode, pre, script');
    elements.each(function(i, el) {
      var layout_div = $('<div class="' + layout + '"></div>');
      if (layout_div.hasClass('shaded')) {
        var shaded_content = $('<div class="shaded-content"></div>');
        $(this).wrap(shaded_content);
        $(this).parent().wrap(layout_div);
      } else {
        $(this).wrap(layout_div);
      }
    });


    // unwrap the layout-chunk div
    $(this).children().unwrap();
  });

  // remove code block used to force  highlighting css
  $('.distill-force-highlighting-css').parent().remove();

  // remove empty line numbers inserted by pandoc when using a
  // custom syntax highlighting theme, except when numbering line
  // in code chunk
  $('pre:not(.numberLines) code.sourceCode a:empty').remove();

  // load distill framework
  load_distill_framework();

  // wait for window.distillRunlevel == 4 to do post processing
  function distill_post_process() {

    if (!window.distillRunlevel || window.distillRunlevel < 4)
      return;

    // hide author/affiliations entirely if we have no authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
    if (!have_authors)
      $('d-byline').addClass('hidden');

    // article with toc class
    $('.d-contents').parent().addClass('d-article-with-toc');

    // strip links that point to #
    $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

    // add orcid ids
    $('.authors-affiliations').find('.author').each(function(i, el) {
      var orcid_id = front_matter.authors[i].orcidID;
      var author_name = front_matter.authors[i].author
      if (orcid_id) {
        var a = $('<a></a>');
        a.attr('href', 'https://orcid.org/' + orcid_id);
        var img = $('<img></img>');
        img.addClass('orcid-id');
        img.attr('alt', author_name ? 'ORCID ID for ' + author_name : 'ORCID ID');
        img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
        a.append(img);
        $(this).append(a);
      }
    });

    // hide elements of author/affiliations grid that have no value
    function hide_byline_column(caption) {
      $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
    }

    // affiliations
    var have_affiliations = false;
    for (var i = 0; i<front_matter.authors.length; ++i) {
      var author = front_matter.authors[i];
      if (author.affiliation !== "&nbsp;") {
        have_affiliations = true;
        break;
      }
    }
    if (!have_affiliations)
      $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

    // published date
    if (!front_matter.publishedDate)
      hide_byline_column("Published");

    // document object identifier
    var doi = $('d-byline').find('h3:contains("DOI")');
    var doi_p = doi.next().empty();
    if (!front_matter.doi) {
      // if we have a citation and valid citationText then link to that
      if ($('#citation').length > 0 && front_matter.citationText) {
        doi.html('Citation');
        $('<a href="#citation"></a>')
          .text(front_matter.citationText)
          .appendTo(doi_p);
      } else {
        hide_byline_column("DOI");
      }
    } else {
      $('<a></a>')
         .attr('href', "https://doi.org/" + front_matter.doi)
         .html(front_matter.doi)
         .appendTo(doi_p);
    }

     // change plural form of authors/affiliations
    if (front_matter.authors.length === 1) {
      var grid = $('.authors-affiliations');
      grid.children('h3:contains("Authors")').text('Author');
      grid.children('h3:contains("Affiliations")').text('Affiliation');
    }

    // remove d-appendix and d-footnote-list local styles
    $('d-appendix > style:first-child').remove();
    $('d-footnote-list > style:first-child').remove();

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // hoverable references
    $('span.citation[data-cites]').each(function() {
      const citeChild = $(this).children()[0]
      // Do not process if @xyz has been used without escaping and without bibliography activated
      // https://github.com/rstudio/distill/issues/466
      if (citeChild === undefined) return true

      if (citeChild.nodeName == "D-FOOTNOTE") {
        var fn = citeChild
        $(this).html(fn.shadowRoot.querySelector("sup"))
        $(this).id = fn.id
        fn.remove()
      }
      var refs = $(this).attr('data-cites').split(" ");
      var refHtml = refs.map(function(ref) {
        // Could use CSS.escape too here, we insure backward compatibility in navigator
        return "<p>" + $('div[id="ref-' + ref + '"]').html() + "</p>";
      }).join("\n");
      window.tippy(this, {
        allowHTML: true,
        content: refHtml,
        maxWidth: 500,
        interactive: true,
        interactiveBorder: 10,
        theme: 'light-border',
        placement: 'bottom-start'
      });
    });

    // fix footnotes in tables (#411)
    // replacing broken distill.pub feature
    $('table d-footnote').each(function() {
      // we replace internal showAtNode methode which is triggered when hovering a footnote
      this.hoverBox.showAtNode = function(node) {
        // ported from https://github.com/distillpub/template/pull/105/files
        calcOffset = function(elem) {
            let x = elem.offsetLeft;
            let y = elem.offsetTop;
            // Traverse upwards until an `absolute` element is found or `elem`
            // becomes null.
            while (elem = elem.offsetParent && elem.style.position != 'absolute') {
                x += elem.offsetLeft;
                y += elem.offsetTop;
            }

            return { left: x, top: y };
        }
        // https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/offsetTop
        const bbox = node.getBoundingClientRect();
        const offset = calcOffset(node);
        this.show([offset.left + bbox.width, offset.top + bbox.height]);
      }
    })

    // clear polling timer
    clearInterval(tid);

    // show body now that everything is ready
    on_load_complete();
  }

  var tid = setInterval(distill_post_process, 50);
  distill_post_process();

}

function init_downlevel() {

  init_common();

   // insert hr after d-title
  $('.d-title').after($('<hr class="section-separator"/>'));

  // check if we have authors
  var front_matter = JSON.parse($("#distill-front-matter").html());
  var have_authors = front_matter.authors && front_matter.authors.length > 0;

  // manage byline/border
  if (!have_authors)
    $('.d-byline').remove();
  $('.d-byline').after($('<hr class="section-separator"/>'));
  $('.d-byline a').remove();

  // remove toc
  $('.d-contents').remove();

  // move appendix elements
  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
  });


  // inject headers into references and footnotes
  var refs_header = $('<h3></h3>');
  refs_header.text('References');
  $('#refs').prepend(refs_header);

  var footnotes_header = $('<h3></h3');
  footnotes_header.text('Footnotes');
  $('.footnotes').children('hr').first().replaceWith(footnotes_header);

  // move appendix-bottom entries to the bottom
  $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
  $('.appendix-bottom').remove();

  // remove appendix if it's empty
  if ($('.d-appendix').children().length === 0)
    $('.d-appendix').remove();

  // prepend separator above appendix
  $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

  // trim code
  $('pre>code').each(function(i, val) {
    $(this).html($.trim($(this).html()));
  });

  // move posts-container right before article
  $('.posts-container').insertBefore($('.d-article'));

  $('body').addClass('downlevel');

  on_load_complete();
}


function init_common() {

  // jquery plugin to change element types
  (function($) {
    $.fn.changeElementType = function(newType) {
      var attrs = {};

      $.each(this[0].attributes, function(idx, attr) {
        attrs[attr.nodeName] = attr.nodeValue;
      });

      this.replaceWith(function() {
        return $("<" + newType + "/>", attrs).append($(this).contents());
      });
    };
  })(jQuery);

  // prevent underline for linked images
  $('a > img').parent().css({'border-bottom' : 'none'});

  // mark non-body figures created by knitr chunks as 100% width
  $('.layout-chunk').each(function(i, val) {
    var figures = $(this).find('img, .html-widget');
    // ignore leaflet img layers (#106)
    figures = figures.filter(':not(img[class*="leaflet"])')
    if ($(this).attr('data-layout') !== "l-body") {
      figures.css('width', '100%');
    } else {
      figures.css('max-width', '100%');
      figures.filter("[width]").each(function(i, val) {
        var fig = $(this);
        fig.css('width', fig.attr('width') + 'px');
      });

    }
  });

  // auto-append index.html to post-preview links in file: protocol
  // and in rstudio ide preview
  $('.post-preview').each(function(i, val) {
    if (window.location.protocol === "file:")
      $(this).attr('href', $(this).attr('href') + "index.html");
  });

  // get rid of index.html references in header
  if (window.location.protocol !== "file:") {
    $('.distill-site-header a[href]').each(function(i,val) {
      $(this).attr('href', $(this).attr('href').replace(/^index[.]html/, "./"));
    });
  }

  // add class to pandoc style tables
  $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
  $('.kable-table').children('table').addClass('pandoc-table');

  // add figcaption style to table captions
  $('caption').parent('table').addClass("figcaption");

  // initialize posts list
  if (window.init_posts_list)
    window.init_posts_list();

  // implmement disqus comment link
  $('.disqus-comment-count').click(function() {
    window.headroom_prevent_pin = true;
    $('#disqus_thread').toggleClass('hidden');
    if (!$('#disqus_thread').hasClass('hidden')) {
      var offset = $(this).offset();
      $(window).resize();
      $('html, body').animate({
        scrollTop: offset.top - 35
      });
    }
  });
}

document.addEventListener('DOMContentLoaded', function() {
  if (is_downlevel_browser())
    init_downlevel();
  else
    window.addEventListener('WebComponentsReady', init_distill);
});

</script>

<!--/radix_placeholder_distill-->
  <script src="../../site_libs/header-attrs-2.26/header-attrs.js"></script>
  <script src="../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <script src="../../site_libs/popper-2.6.0/popper.min.js"></script>
  <link href="../../site_libs/tippy-6.2.7/tippy.css" rel="stylesheet" />
  <link href="../../site_libs/tippy-6.2.7/tippy-light-border.css" rel="stylesheet" />
  <script src="../../site_libs/tippy-6.2.7/tippy.umd.min.js"></script>
  <script src="../../site_libs/anchor-4.2.2/anchor.min.js"></script>
  <script src="../../site_libs/bowser-1.9.3/bowser.min.js"></script>
  <script src="../../site_libs/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="../../site_libs/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
<script defer data-domain="blogs.rstudio.com" src="https://plausible.io/js/plausible.js"></script>
<!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"GPT-2 from scratch with torch","description":"Implementing a language model from scratch is, arguably, the best way to develop an accurate idea of how its engine works. Here, we use torch to code GPT-2, the immediate successor to the original GPT. In the end, you'll dispose of an R-native model that can make direct use of Hugging Face's pre-trained GPT-2 model weights.","authors":[{"author":"Sigrid Keydana","authorURL":"#","affiliation":"Posit","affiliationURL":"https://www.posit.co/","orcidID":""}],"publishedDate":"2023-06-20T00:00:00.000+00:00","citationText":"Keydana, 2023"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<header class="header header--fixed" role="banner">
<nav class="distill-site-nav distill-site-header">
<div class="nav-left">
<span class="logo">
<img src="../../images/posit.png"/>
</span>
<a href="../../index.html" class="title">AI Blog</a>
<input id="distill-search" class="nav-search hidden" type="text" placeholder="Search..."/>
</div>
<div class="nav-right">
<a href="../../index.html">Home</a>
<a href="../../gallery.html">Gallery</a>
<a href="../../about.html">About</a>
<a href="../../contributing.html">Contributing</a>
<a href="../../index.xml">
<i class="fa fa-rss" aria-hidden="true"></i>
</a>
<a href="javascript:void(0);" class="nav-toggle">&#9776;</a>
</div>
</nav>
</header>
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>GPT-2 from scratch with torch</h1>

<!--radix_placeholder_categories-->
<div class="dt-tags">
  <a href="../../index.html#category:Torch" class="dt-tag">Torch</a>
  <a href="../../index.html#category:R" class="dt-tag">R</a>
  <a href="../../index.html#category:Natural_Language_Processing" class="dt-tag">Natural Language Processing</a>
</div>
<!--/radix_placeholder_categories-->
<p><p>Implementing a language model from scratch is, arguably, the best way to develop an accurate idea of how its engine works. Here, we use torch to code GPT-2, the immediate successor to the original GPT. In the end, youll dispose of an R-native model that can make direct use of Hugging Faces pre-trained GPT-2 model weights.</p></p>
</div>

<div class="d-byline">
  Sigrid Keydana  (Posit)<a href="https://www.posit.co/" class="uri">https://www.posit.co/</a>
  
<br/>2023-06-20
</div>

<div class="d-article">
<div class="d-contents d-contents-float">
<nav class="l-text toc figcaption" id="TOC">
<h3>Contents</h3>
<ul>
<li><a href="#sources-resources">Sources, resources</a></li>
<li><a href="#a-minimal-gpt-2">A minimal GPT-2</a></li>
<li><a href="#end-to-end-usage-using-pre-trained-weights">End-to-end-usage, using pre-trained weights</a></li>
</ul>
</nav>
</div>
<p>Whatever your take on Large Language Models (LLMs)  are they beneficial? dangerous? a short-lived fashion, like crypto?  they are <em>here</em>, <em>now</em>. And that means, it is a good thing to know (at a level one needs to decide for oneself) how they work. On this same day, I am publishing <a href="https://blogs.rstudio.com/ai/posts/2023-06-20-llm-intro">What are Large Language Models? What are they not?</a>, intended for a more general audience. In this post, Id like to address deep learning practitioners, walking through a <code>torch</code> implementation of GPT-2 <span class="citation" data-cites="Radford2019LanguageMA">(<a href="#ref-Radford2019LanguageMA" role="doc-biblioref">Radford et al. 2019</a>)</span>, the second in OpenAIs succession of ever-larger models trained on ever-more-vast text corpora. Youll see that a complete model implementation fits in fewer than 250 lines of R code.</p>
<h2 id="sources-resources">Sources, resources</h2>
<p>The code Im going to present is found in the <a href="https://github.com/mlverse/minhub"><code>minhub</code></a> repository. This repository deserves a mention of its own. As emphasized in the README,</p>
<blockquote>
<p><em>minhub</em> is a collection of minimal implementations of deep learning models, inspired by <a href="https://github.com/karpathy/minGPT/blob/master/mingpt/model.py">minGPT</a>. All models are designed to be self-contained, single-file, and devoid of external dependencies, making them easy to copy and integrate into your own projects.</p>
</blockquote>
<p>Evidently, this makes them excellent learning material; but that is not all. Models also come with the option to load pre-trained weights from Hugging Faces <a href="https://Hugging%20Face.co/models">model hub</a>. And if that werent enormously convenient already, you dont have to worry about how to get tokenization right: Just download the matching tokenizer from Hugging Face, as well. Ill show how this works in the <a href="#end-to-end-usage-using-pre-trained-weights">final section</a> of this post. As noted in the <code>minhub</code> README, these facilities are provided by packages <a href="https://github.com/mlverse/hfhub"><code>hfhub</code></a> and <a href="https://github.com/mlverse/tok"><code>tok</code></a>.</p>
<p>As realized in <code>minhub</code>, <a href="https://github.com/mlverse/minhub/blob/main/R/gpt2.R">gpt2.R</a> is, mostly, a port of Karpathys <a href="https://github.com/karpathy/minGPT/blob/master/mingpt/model.py">MinGPT</a>. Hugging Faces (more sophisticated) <a href="https://github.com/Hugging%20Face/transformers/blob/v4.29.1/src/transformers/models/gpt2/modeling_gpt2.py">implementation</a> has also been consulted. For a Python code walk-through, see <a href="https://amaarora.github.io/posts/2020-02-18-annotatedGPT2.html" class="uri">https://amaarora.github.io/posts/2020-02-18-annotatedGPT2.html</a>. This text also consolidates links to blog posts and learning materials on language modeling with deep learning that have become classics in the short time since they were written.</p>
<h2 id="a-minimal-gpt-2">A minimal GPT-2</h2>
<h4 id="overall-architecture">Overall architecture</h4>
<p>The original Transformer <span class="citation" data-cites="vaswani2017attention">(<a href="#ref-vaswani2017attention" role="doc-biblioref">Vaswani et al. 2017</a>)</span> was built up of both an encoder and a decoder stack, a prototypical use case being machine translation. Subsequent developments, dependent on envisaged primary usage, tended to forego one of the stacks. The first GPT, which differs from GPT-2 only in relative subtleties, kept only the decoder stack. With self-attention wired into every decoder block, as well as an initial embedding step, this is not a problem  external input is not technically different from successive internal representations.</p>
<p>Here is a screenshot from the initial GPT paper <span class="citation" data-cites="Radford2018ImprovingLU">(<a href="#ref-Radford2018ImprovingLU" role="doc-biblioref">Radford and Narasimhan 2018</a>)</span>, visualizing the overall architecture. It is still valid for GPT-2. Token as well as position embedding are followed by a twelve-fold repetition of (identical in structure, though not sharing weights) transformer blocks, with a task-dependent linear layer constituting model output.</p>
<div class="layout-chunk" data-layout="l-body">
<p><img src="images/transformer.png" alt="Overall architecture of GPT-2. The central part is a twelve-fold repetition of a transformer block, chaining, consecutively, multi-head self-attention, layer normalization, a feed-forward sub-network, and a second instance of layer normalization. Inside this block, arrows indicate residual connections omitting the attention and feed-forward layers. Below this central component, an input-transformation block indicates both token and position embedding. On its top, output blocks list a few alternative, task-dependent modules." width="144" /></p>
</div>
<p>In <a href="https://github.com/mlverse/minhub/blob/main/R/gpt2.R">gpt2.R</a>, this global structure and what it does is defined in <code>nn_gpt2_model()</code>. (The code is more modularized  so dont be confused if code and screenshot dont perfectly match.)</p>
<p>First, in <code>initialize()</code>, we have the definition of modules:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>self</span><span class='op'>$</span><span class='va'>transformer</span> <span class='op'>&lt;-</span> <span class='fu'>nn_module_dict</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span></span>
<span>  wte <span class='op'>=</span> <span class='fu'>nn_embedding</span><span class='op'>(</span><span class='va'>vocab_size</span>, <span class='va'>n_embd</span><span class='op'>)</span>,</span>
<span>  wpe <span class='op'>=</span> <span class='fu'>nn_embedding</span><span class='op'>(</span><span class='va'>max_pos</span>, <span class='va'>n_embd</span><span class='op'>)</span>,</span>
<span>  drop <span class='op'>=</span> <span class='fu'>nn_dropout</span><span class='op'>(</span><span class='va'>pdrop</span><span class='op'>)</span>,</span>
<span>  h <span class='op'>=</span> <span class='fu'>nn_sequential</span><span class='op'>(</span><span class='op'>!</span><span class='op'>!</span><span class='op'>!</span><span class='fu'>map</span><span class='op'>(</span></span>
<span>    <span class='fl'>1</span><span class='op'>:</span><span class='va'>n_layer</span>,</span>
<span>    \<span class='op'>(</span><span class='va'>x</span><span class='op'>)</span> <span class='fu'>nn_gpt2_transformer_block</span><span class='op'>(</span><span class='va'>n_embd</span>, <span class='va'>n_head</span>, <span class='va'>n_layer</span>, <span class='va'>max_pos</span>, <span class='va'>pdrop</span><span class='op'>)</span></span>
<span>  <span class='op'>)</span><span class='op'>)</span>,</span>
<span>  ln_f <span class='op'>=</span> <span class='fu'>nn_layer_norm</span><span class='op'>(</span><span class='va'>n_embd</span>, eps <span class='op'>=</span> <span class='fl'>1e-5</span><span class='op'>)</span></span>
<span><span class='op'>)</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>self</span><span class='op'>$</span><span class='va'>lm_head</span> <span class='op'>&lt;-</span> <span class='fu'>nn_linear</span><span class='op'>(</span><span class='va'>n_embd</span>, <span class='va'>vocab_size</span>, bias <span class='op'>=</span> <span class='cn'>FALSE</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>The two top-level components in this model are the <code>transformer</code> and <code>lm_head</code>, the output layer. This code-level distinction has an important semantic dimension, with two aspects standing out. First, and quite directly, <code>transformer</code>s definition communicates, in a succinct way, what it is that constitutes a Transformer. What comes thereafter  <code>lm_head</code>, in our case  may vary. Second, and importantly, the distinction reflects the essential underlying idea, or essential operationalization, of natural language processing in deep learning. Learning consists of two steps, the first  and indispensable one  being to learn about <em>language</em> (this is what LLMs do), and the second, much less resource-consuming, one consisting of adaptation to a concrete task (such as question answering, or text summarization).</p>
<p>To see in what order (and how often) things happen, we look inside <code>forward()</code>:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>tok_emb</span> <span class='op'>&lt;-</span> <span class='va'>self</span><span class='op'>$</span><span class='va'>transformer</span><span class='op'>$</span><span class='fu'>wte</span><span class='op'>(</span><span class='va'>x</span><span class='op'>)</span> </span>
<span><span class='va'>pos</span> <span class='op'>&lt;-</span> <span class='fu'>torch_arange</span><span class='op'>(</span><span class='fl'>1</span>, <span class='va'>x</span><span class='op'>$</span><span class='fu'>size</span><span class='op'>(</span><span class='fl'>2</span><span class='op'>)</span><span class='op'>)</span><span class='op'>$</span><span class='fu'>to</span><span class='op'>(</span>dtype <span class='op'>=</span> <span class='st'>"long"</span><span class='op'>)</span><span class='op'>$</span><span class='fu'>unsqueeze</span><span class='op'>(</span><span class='fl'>1</span><span class='op'>)</span> </span>
<span><span class='va'>pos_emb</span> <span class='op'>&lt;-</span> <span class='va'>self</span><span class='op'>$</span><span class='va'>transformer</span><span class='op'>$</span><span class='fu'>wpe</span><span class='op'>(</span><span class='va'>pos</span><span class='op'>)</span></span>
<span><span class='va'>x</span> <span class='op'>&lt;-</span> <span class='va'>self</span><span class='op'>$</span><span class='va'>transformer</span><span class='op'>$</span><span class='fu'>drop</span><span class='op'>(</span><span class='va'>tok_emb</span> <span class='op'>+</span> <span class='va'>pos_emb</span><span class='op'>)</span></span>
<span><span class='va'>x</span> <span class='op'>&lt;-</span> <span class='va'>self</span><span class='op'>$</span><span class='va'>transformer</span><span class='op'>$</span><span class='fu'>h</span><span class='op'>(</span><span class='va'>x</span><span class='op'>)</span></span>
<span><span class='va'>x</span> <span class='op'>&lt;-</span> <span class='va'>self</span><span class='op'>$</span><span class='va'>transformer</span><span class='op'>$</span><span class='fu'>ln_f</span><span class='op'>(</span><span class='va'>x</span><span class='op'>)</span></span>
<span><span class='va'>x</span> <span class='op'>&lt;-</span> <span class='va'>self</span><span class='op'>$</span><span class='fu'>lm_head</span><span class='op'>(</span><span class='va'>x</span><span class='op'>)</span></span>
<span><span class='va'>x</span></span></code></pre>
</div>
</div>
<p>All modules in <code>transformer</code> are called, and thus executed, once; this includes <code>h</code>  but <code>h</code> itself is a sequential module made up of transformer <em>blocks</em>.</p>
<p>Since these blocks are the core of the model, well look at them next.</p>
<h4 id="transformer-block">Transformer block</h4>
<p>Heres how, in <code>nn_gpt2_transformer_block()</code>, each of the twelve blocks is defined.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>self</span><span class='op'>$</span><span class='va'>ln_1</span> <span class='op'>&lt;-</span> <span class='fu'>nn_layer_norm</span><span class='op'>(</span><span class='va'>n_embd</span>, eps <span class='op'>=</span> <span class='fl'>1e-5</span><span class='op'>)</span></span>
<span><span class='va'>self</span><span class='op'>$</span><span class='va'>attn</span> <span class='op'>&lt;-</span> <span class='fu'>nn_gpt2_attention</span><span class='op'>(</span><span class='va'>n_embd</span>, <span class='va'>n_head</span>, <span class='va'>n_layer</span>, <span class='va'>max_pos</span>, <span class='va'>pdrop</span><span class='op'>)</span></span>
<span><span class='va'>self</span><span class='op'>$</span><span class='va'>ln_2</span> <span class='op'>&lt;-</span> <span class='fu'>nn_layer_norm</span><span class='op'>(</span><span class='va'>n_embd</span>, eps <span class='op'>=</span> <span class='fl'>1e-5</span><span class='op'>)</span></span>
<span><span class='va'>self</span><span class='op'>$</span><span class='va'>mlp</span> <span class='op'>&lt;-</span> <span class='fu'>nn_gpt2_mlp</span><span class='op'>(</span><span class='va'>n_embd</span>, <span class='va'>pdrop</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>On this level of resolution, we see that self-attention is computed afresh at every stage, and that the other constitutive ingredient is a feed-forward neural network. In addition, there are two modules computing <em>layer normalization</em>, the type of normalization employed in transformer blocks. Different normalization algorithms tend to distinguish themselves from one another in what they average over; layer normalization <span class="citation" data-cites="ba2016layer">(<a href="#ref-ba2016layer" role="doc-biblioref">Ba, Kiros, and Hinton 2016</a>)</span>  surprisingly, maybe, to some readers  does so per batch <em>item</em>. That is, there is one mean, and one standard deviation, for each unit in a module. All other dimensions (in an image, that would be spatial dimensions as well as channels) constitute the input to that item-wise statistics computation.</p>
<p>Continuing to zoom in, we will look at both the attention- and the feed-forward network shortly. Before, though, we need to see how these layers are called. Here is all that happens in <code>forward()</code>:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>x</span> <span class='op'>&lt;-</span> <span class='va'>x</span> <span class='op'>+</span> <span class='va'>self</span><span class='op'>$</span><span class='fu'>attn</span><span class='op'>(</span><span class='va'>self</span><span class='op'>$</span><span class='fu'>ln_1</span><span class='op'>(</span><span class='va'>x</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='va'>x</span> <span class='op'>+</span> <span class='va'>self</span><span class='op'>$</span><span class='fu'>mlp</span><span class='op'>(</span><span class='va'>self</span><span class='op'>$</span><span class='fu'>ln_2</span><span class='op'>(</span><span class='va'>x</span><span class='op'>)</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>These two lines deserve to be read attentively. As opposed to just calling each consecutive layer on the previous ones output, this inserts skip (also termed <em>residual</em>) connections that, each, circumvent one of the parent modules principal stages. The effect is that each sub-module does not replace, but just update what is passed in with its own view on things.</p>
<h4 id="transformer-block-up-close-self-attention">Transformer block up close: Self-attention</h4>
<p>Of all modules in GPT-2, this is by far the most intimidating-looking. But the basic algorithm employed here is the same as what the classic dot product attention paper <span class="citation" data-cites="BahdanauCB14">(<a href="#ref-BahdanauCB14" role="doc-biblioref">Bahdanau, Cho, and Bengio 2014</a>)</span> proposed in 2014: Attention is conceptualized as similarity, and similarity is measured via the dot product. One thing that can be confusing is the self in self-attention. This term first appeared in the Transformer paper <span class="citation" data-cites="vaswani2017attention">(<a href="#ref-vaswani2017attention" role="doc-biblioref">Vaswani et al. 2017</a>)</span>, which had an encoder as well as a decoder stack. There, attention referred to how the decoder blocks decided where to focus in the message received from the encoding stage, while self-attention was the term coined for this technique being applied inside the stacks themselves (i.e., between a stacks internal blocks). With GPT-2, only the (now redundantly-named) self-attention remains.</p>
<p>Resuming from the above, there are two reasons why this might look complicated. For one, the triplication of tokens introduced, in Transformer, through the query - key - value frame<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. And secondly, the additional batching introduced by having not just one, but several, parallel, independent attention-calculating processes per layer (multi-head attention). Walking through the code, Ill point to both as they make their appearance.</p>
<p>We again start with module initialization. This is how <code>nn_gpt2_attention()</code> lists its components:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='co'># key, query, value projections for all heads, but in a batch</span></span>
<span><span class='va'>self</span><span class='op'>$</span><span class='va'>c_attn</span> <span class='op'>&lt;-</span> <span class='fu'>nn_linear</span><span class='op'>(</span><span class='va'>n_embd</span>, <span class='fl'>3</span> <span class='op'>*</span> <span class='va'>n_embd</span><span class='op'>)</span></span>
<span><span class='co'># output projection</span></span>
<span><span class='va'>self</span><span class='op'>$</span><span class='va'>c_proj</span> <span class='op'>&lt;-</span> <span class='fu'>nn_linear</span><span class='op'>(</span><span class='va'>n_embd</span>, <span class='va'>n_embd</span><span class='op'>)</span></span>
<span></span>
<span><span class='co'># regularization</span></span>
<span><span class='va'>self</span><span class='op'>$</span><span class='va'>attn_dropout</span> <span class='op'>&lt;-</span> <span class='fu'>nn_dropout</span><span class='op'>(</span><span class='va'>pdrop</span><span class='op'>)</span></span>
<span><span class='va'>self</span><span class='op'>$</span><span class='va'>resid_dropout</span> <span class='op'>&lt;-</span> <span class='fu'>nn_dropout</span><span class='op'>(</span><span class='va'>pdrop</span><span class='op'>)</span></span>
<span></span>
<span><span class='co'># causal mask to ensure that attention is only applied to the left in the input sequence</span></span>
<span><span class='va'>self</span><span class='op'>$</span><span class='va'>bias</span> <span class='op'>&lt;-</span> <span class='fu'>torch_ones</span><span class='op'>(</span><span class='va'>max_pos</span>, <span class='va'>max_pos</span><span class='op'>)</span><span class='op'>$</span></span>
<span>  <span class='fu'>bool</span><span class='op'>(</span><span class='op'>)</span><span class='op'>$</span></span>
<span>  <span class='fu'>tril</span><span class='op'>(</span><span class='op'>)</span><span class='op'>$</span></span>
<span>  <span class='fu'>view</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>1</span>, <span class='fl'>1</span>, <span class='va'>max_pos</span>, <span class='va'>max_pos</span><span class='op'>)</span><span class='op'>)</span> <span class='op'>|&gt;</span></span>
<span>  <span class='fu'>nn_buffer</span><span class='op'>(</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>Besides two dropout layers, we see:</p>
<ul>
<li>A linear module that effectuates the above-mentioned triplication. Note how this is different from just having three identical versions of a token: Assuming all representations were initially mostly equivalent (through random initialization, for example), they will not remain so once weve begun to train the model.</li>
<li>A module, called <code>c_proj</code>, that applies a final affine transformation. We will need to look at usage to see what this module is for.</li>
<li>A <em>buffer</em>  a tensor that is part of a modules state, but exempt from training  that makes sure that attention is not applied to previous-block output that lies in the future. Basically, this is achieved by masking out future tokens, making use of a lower-triangular matrix.</li>
</ul>
<p>As to <code>forward()</code>, I am splitting it up into easy-to-digest pieces.</p>
<p>As we enter the method, the argument, <code>x</code>, is shaped just as expected, for a language model: batch dimension times sequence length times embedding dimension.</p>
<pre><code>x$shape
[1]   1  24 768</code></pre>
<p>Next, two batching operations happen: (1) triplication into queries, keys, and values; and (2) making space such that attention can be computed for the desired number of attention heads all at once. Ill explain how after listing the complete piece.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='co'># batch size, sequence length, embedding dimensionality (n_embd)</span></span>
<span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='va'>b</span>, <span class='va'>t</span>, <span class='va'>c</span><span class='op'>)</span> <span class='op'>%&lt;-%</span> <span class='va'>x</span><span class='op'>$</span><span class='va'>shape</span></span>
<span></span>
<span><span class='co'># calculate query, key, values for all heads in batch and move head forward to be the batch dim</span></span>
<span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='va'>q</span>, <span class='va'>k</span>, <span class='va'>v</span><span class='op'>)</span> <span class='op'>%&lt;-%</span> <span class='op'>(</span><span class='op'>(</span><span class='va'>self</span><span class='op'>$</span><span class='fu'>c_attn</span><span class='op'>(</span><span class='va'>x</span><span class='op'>)</span><span class='op'>$</span></span>
<span>  <span class='fu'>split</span><span class='op'>(</span><span class='va'>self</span><span class='op'>$</span><span class='va'>n_embd</span>, dim <span class='op'>=</span> <span class='op'>-</span><span class='fl'>1</span><span class='op'>)</span><span class='op'>)</span> <span class='op'>|&gt;</span></span>
<span>  <span class='fu'>map</span><span class='op'>(</span>\<span class='op'>(</span><span class='va'>x</span><span class='op'>)</span> <span class='va'>x</span><span class='op'>$</span><span class='fu'>view</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='va'>b</span>, <span class='va'>t</span>, <span class='va'>self</span><span class='op'>$</span><span class='va'>n_head</span>, <span class='va'>c</span> <span class='op'>/</span> <span class='va'>self</span><span class='op'>$</span><span class='va'>n_head</span><span class='op'>)</span><span class='op'>)</span><span class='op'>)</span> <span class='op'>|&gt;</span></span>
<span>  <span class='fu'>map</span><span class='op'>(</span>\<span class='op'>(</span><span class='va'>x</span><span class='op'>)</span> <span class='va'>x</span><span class='op'>$</span><span class='fu'>transpose</span><span class='op'>(</span><span class='fl'>2</span>, <span class='fl'>3</span><span class='op'>)</span><span class='op'>)</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>First, the call to <code>self$c_attn()</code> yields query, key, and value vectors for each embedded input token. <code>split()</code> separates the resulting matrix into a list. Then <code>map()</code> takes care of the second batching operation. All of the three matrices are re-shaped, adding a fourth dimension. This fourth dimension takes care of the attention heads. Note how, as opposed to the multiplying process that triplicated the embeddings, this divides up what we have among the heads, leaving each of them to work with a subset inversely proportional to the number of heads used. Finally, <code>map(\(x) x$transpose(2, 3)</code> mutually exchanges head and sequence-position dimensions.</p>
<p>Next comes the computation of attention itself.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='co'># causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh, T, T)</span></span>
<span><span class='va'>att</span> <span class='op'>&lt;-</span> <span class='va'>q</span><span class='op'>$</span><span class='fu'>matmul</span><span class='op'>(</span><span class='va'>k</span><span class='op'>$</span><span class='fu'>transpose</span><span class='op'>(</span><span class='op'>-</span><span class='fl'>2</span>, <span class='op'>-</span><span class='fl'>1</span><span class='op'>)</span><span class='op'>)</span> <span class='op'>*</span> <span class='op'>(</span><span class='fl'>1</span> <span class='op'>/</span> <span class='fu'><a href='https://rdrr.io/r/base/MathFun.html'>sqrt</a></span><span class='op'>(</span><span class='va'>k</span><span class='op'>$</span><span class='fu'>size</span><span class='op'>(</span><span class='op'>-</span><span class='fl'>1</span><span class='op'>)</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='va'>att</span> <span class='op'>&lt;-</span> <span class='va'>att</span><span class='op'>$</span><span class='fu'>masked_fill</span><span class='op'>(</span><span class='va'>self</span><span class='op'>$</span><span class='va'>bias</span><span class='op'>[</span>, , <span class='fl'>1</span><span class='op'>:</span><span class='va'>t</span>, <span class='fl'>1</span><span class='op'>:</span><span class='va'>t</span><span class='op'>]</span> <span class='op'>==</span> <span class='fl'>0</span>, <span class='op'>-</span><span class='cn'>Inf</span><span class='op'>)</span></span>
<span><span class='va'>att</span> <span class='op'>&lt;-</span> <span class='va'>att</span><span class='op'>$</span><span class='fu'>softmax</span><span class='op'>(</span>dim <span class='op'>=</span> <span class='op'>-</span><span class='fl'>1</span><span class='op'>)</span></span>
<span><span class='va'>att</span> <span class='op'>&lt;-</span> <span class='va'>self</span><span class='op'>$</span><span class='fu'>attn_dropout</span><span class='op'>(</span><span class='va'>att</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>First, similarity between queries and keys is computed, matrix multiplication effectively being a batched dot product. (If youre wondering about the final division term in line one, this scaling operation is one of the few aspects where GPT-2 differs from its predecessor. Check out the paper if youre interested in the related considerations.) Next, the aforementioned mask is applied, resultant scores are normalized, and dropout regularization is used to encourage sparsity.</p>
<p>Finally, the computed <em>attention</em><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> needs to be passed on to the ensuing layer. This is where the value vectors come in  those members of this trinity that we havent yet seen in action.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>y</span> <span class='op'>&lt;-</span> <span class='va'>att</span><span class='op'>$</span><span class='fu'>matmul</span><span class='op'>(</span><span class='va'>v</span><span class='op'>)</span> <span class='co'># (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)</span></span>
<span><span class='va'>y</span> <span class='op'>&lt;-</span> <span class='va'>y</span><span class='op'>$</span><span class='fu'>transpose</span><span class='op'>(</span><span class='fl'>2</span>, <span class='fl'>3</span><span class='op'>)</span><span class='op'>$</span><span class='fu'>contiguous</span><span class='op'>(</span><span class='op'>)</span><span class='op'>$</span><span class='fu'>view</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='va'>b</span>, <span class='va'>t</span>, <span class='va'>c</span><span class='op'>)</span><span class='op'>)</span> <span class='co'># re-assemble all head outputs side by side</span></span>
<span></span>
<span><span class='co'># output projection</span></span>
<span><span class='va'>y</span> <span class='op'>&lt;-</span> <span class='va'>self</span><span class='op'>$</span><span class='fu'>resid_dropout</span><span class='op'>(</span><span class='va'>self</span><span class='op'>$</span><span class='fu'>c_proj</span><span class='op'>(</span><span class='va'>y</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='va'>y</span></span></code></pre>
</div>
</div>
<p>Concretely, what the matrix multiplication does here is weight the value vectors by the <em>attention</em>, and add them up. This happens for all attention heads at the same time, and really represents the outcome of the algorithm as a whole.</p>
<p>Remaining steps then restore the original input size. This involves aligning the results for all heads one after the other, and then, applying the linear layer <code>c_proj</code> to make sure these results are not treated equally and/or independently, but combined in a useful way. Thus, the projection operation hinted at here really is a made up of a mechanical step (<code>view()</code>) and an intelligent one (transformation by <code>c_proj()</code>).</p>
<h4 id="transformer-block-up-close-feed-forward-network-mlp">Transformer block up close: Feed-forward network (MLP)</h4>
<p>Compared to the first, the attention module, there really is not much to say about the second core component of the transformer block (<code>nn_gpt2_mlp()</code>). It really is just an MLP  no tricks involved. Two things deserve pointing out, though.</p>
<p>First, you may have heard about the MLP in a transformer block working position-wise, and wondered what is meant by this. Consider what happens in such a block:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>x</span> <span class='op'>&lt;-</span> <span class='va'>x</span> <span class='op'>+</span> <span class='va'>self</span><span class='op'>$</span><span class='fu'>attn</span><span class='op'>(</span><span class='va'>self</span><span class='op'>$</span><span class='fu'>ln_1</span><span class='op'>(</span><span class='va'>x</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='va'>x</span> <span class='op'>+</span> <span class='va'>self</span><span class='op'>$</span><span class='fu'>mlp</span><span class='op'>(</span><span class='va'>self</span><span class='op'>$</span><span class='fu'>ln_2</span><span class='op'>(</span><span class='va'>x</span><span class='op'>)</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>The MLP receives its input (almost) directly from the attention module. But that, as we saw, was returning tensors of size [<code>batch size</code>, <code>sequence length</code>, embedding dimension]. Inside the MLP  cf.its <code>forward()</code>  the number of dimensions never changes:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>x</span> <span class='op'>|&gt;</span></span>
<span>  <span class='va'>self</span><span class='op'>$</span><span class='fu'>c_fc</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>|&gt;</span>       <span class='co'># nn_linear(n_embd, 4 * n_embd)</span></span>
<span>  <span class='va'>self</span><span class='op'>$</span><span class='fu'>act</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>|&gt;</span>        <span class='co'># nn_gelu(approximate = "tanh")</span></span>
<span>  <span class='va'>self</span><span class='op'>$</span><span class='fu'>c_proj</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>|&gt;</span>     <span class='co'># nn_linear(4 * n_embd, n_embd)</span></span>
<span>  <span class='va'>self</span><span class='op'>$</span><span class='fu'>dropout</span><span class='op'>(</span><span class='op'>)</span>       <span class='co'># nn_dropout(pdrop)</span></span></code></pre>
</div>
</div>
<p>Thus, these transformations are applied to all elements in the sequence, <em>independently</em>.</p>
<p>Second, since this is the only place where it appears, a note on the activation function employed. GeLU stands for Gaussian Error Linear Units, proposed in <span class="citation" data-cites="hendrycks2020gaussian">(<a href="#ref-hendrycks2020gaussian" role="doc-biblioref">Hendrycks and Gimpel 2020</a>)</span>. The idea here is to combine ReLU-like activation effects with regularization/stochasticity. In theory, each intermediate computation would be weighted by its position in the (Gaussian) cumulative distribution function  effectively, by how much bigger (smaller) it is than the others. In practice, as you see from the modules instantiation, an approximation is used.</p>
<p>And thats it for GPT-2s main actor, the repeated transformer block. Remain two things: what happens before, and what happens thereafter.</p>
<h4 id="from-words-to-codes-token-and-position-embeddings">From words to codes: Token and position embeddings</h4>
<p>Admittedly, if you tokenize the input dataset as required (using the matching tokenizer from Hugging Face  see below), you do not really end up with <em>words</em>. But still, the well-established fact holds: Some change of representation has to happen if the model is to successfully extract linguistic knowledge. Like many Transformer-based models, the GPT family encodes tokens in two ways. For one, as word embeddings. Looking back to <code>nn_gpt2_model()</code>, the top-level module we started this walk-through with, we see:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>wte</span> <span class='op'>=</span> <span class='fu'>nn_embedding</span><span class='op'>(</span><span class='va'>vocab_size</span>, <span class='va'>n_embd</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>This is useful already, but the representation space that results does not include information about semantic relations that may vary with <em>position in the sequence</em>  syntactic rules, for example, or phrase pragmatics. The second type of encoding remedies this. Referred to as position embedding, it appears in <code>nn_gpt2_model()</code> like so:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>wpe</span> <span class='op'>=</span> <span class='fu'>nn_embedding</span><span class='op'>(</span><span class='va'>max_pos</span>, <span class='va'>n_embd</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>Another embedding layer? Yes, though this one embeds not tokens, but a pre-specified number of valid positions (ranging from 1 to 1024, in GPTs case). In other words, the network is supposed to <em>learn</em> what position in a sequence entails. This is an area where different models may vary vastly. The original Transformer employed a form of sinusoidal encoding; a more recent refinement is found in, e.g., GPT-NeoX <span class="citation" data-cites="rope-paper">(<a href="#ref-rope-paper" role="doc-biblioref">Su et al. 2021</a>)</span>.</p>
<p>Once both encodings are available, they are straightforwardly added (see <code>nn_gpt2_model()$forward()</code>):</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>tok_emb</span> <span class='op'>&lt;-</span> <span class='va'>self</span><span class='op'>$</span><span class='va'>transformer</span><span class='op'>$</span><span class='fu'>wte</span><span class='op'>(</span><span class='va'>x</span><span class='op'>)</span> </span>
<span><span class='va'>pos</span> <span class='op'>&lt;-</span> <span class='fu'>torch_arange</span><span class='op'>(</span><span class='fl'>1</span>, <span class='va'>x</span><span class='op'>$</span><span class='fu'>size</span><span class='op'>(</span><span class='fl'>2</span><span class='op'>)</span><span class='op'>)</span><span class='op'>$</span><span class='fu'>to</span><span class='op'>(</span>dtype <span class='op'>=</span> <span class='st'>"long"</span><span class='op'>)</span><span class='op'>$</span><span class='fu'>unsqueeze</span><span class='op'>(</span><span class='fl'>1</span><span class='op'>)</span> </span>
<span><span class='va'>pos_emb</span> <span class='op'>&lt;-</span> <span class='va'>self</span><span class='op'>$</span><span class='va'>transformer</span><span class='op'>$</span><span class='fu'>wpe</span><span class='op'>(</span><span class='va'>pos</span><span class='op'>)</span></span>
<span><span class='va'>x</span> <span class='op'>&lt;-</span> <span class='va'>self</span><span class='op'>$</span><span class='va'>transformer</span><span class='op'>$</span><span class='fu'>drop</span><span class='op'>(</span><span class='va'>tok_emb</span> <span class='op'>+</span> <span class='va'>pos_emb</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>The resultant tensor is then passed to the chain of transformer blocks.</p>
<h4 id="output">Output</h4>
<p>Once the transformer blocks have been applied, the last mapping is taken care of by <code>lm_head</code>:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>x</span> <span class='op'>&lt;-</span> <span class='va'>self</span><span class='op'>$</span><span class='fu'>lm_head</span><span class='op'>(</span><span class='va'>x</span><span class='op'>)</span> <span class='co'># nn_linear(n_embd, vocab_size, bias = FALSE)</span></span></code></pre>
</div>
</div>
<p>This is a linear transformation that maps internal representations back to discrete vocabulary indices, assigning a score to every index. That being the models final action, it is left to the sample generation process is to decide what to make of these scores. Or, put differently, that process is free to choose among different established techniques. Well see one  pretty standard  way in the next section.</p>
<p>This concludes model walk-through. I have left out a few details (such as weight initialization); consult <a href="https://github.com/mlverse/minhub/blob/main/R/gpt2.R">gpt.R</a> if youre interested.</p>
<h2 id="end-to-end-usage-using-pre-trained-weights">End-to-end-usage, using pre-trained weights</h2>
<p>Its unlikely that many users will want to train GPT-2 from scratch. Lets see, thus, how we can quickly set this up for sample generation.</p>
<h4 id="create-model-load-weights-get-tokenizer">Create model, load weights, get tokenizer</h4>
<p>The Hugging Face <a href="https://Hugging%20Face.co/models">model hub</a> lets you access (and download) all required files (<a href="https://Hugging%20Face.co/gpt2/blob/main/model.safetensors">weights</a> and <a href="https://Hugging%20Face.co/gpt2/blob/main/tokenizer.json">tokenizer</a>) directly from the <a href="https://Hugging%20Face.co/gpt2/tree/main">GPT-2 page</a>. All files are versioned; we use the most recent version.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span> <span class='va'>identifier</span> <span class='op'>&lt;-</span> <span class='st'>"gpt2"</span></span>
<span> <span class='va'>revision</span> <span class='op'>&lt;-</span> <span class='st'>"e7da7f2"</span></span>
<span> <span class='co'># instantiate model and load Hugging Face weights</span></span>
<span> <span class='va'>model</span> <span class='op'>&lt;-</span> <span class='fu'>gpt2_from_pretrained</span><span class='op'>(</span><span class='va'>identifier</span>, <span class='va'>revision</span><span class='op'>)</span></span>
<span> <span class='co'># load matching tokenizer</span></span>
<span> <span class='va'>tok</span> <span class='op'>&lt;-</span> <span class='fu'>tok</span><span class='fu'>::</span><span class='va'>tokenizer</span><span class='op'>$</span><span class='fu'>from_pretrained</span><span class='op'>(</span><span class='va'>identifier</span><span class='op'>)</span></span>
<span> <span class='va'>model</span><span class='op'>$</span><span class='fu'>eval</span><span class='op'>(</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<h4 id="tokenize">tokenize</h4>
<p>Decoder-only transformer-type models dont need a prompt. But usually, applications will want to pass input to the generation process. Thanks to <code>tok</code>, tokenizing that input couldnt be more convenient:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>idx</span> <span class='op'>&lt;-</span> <span class='fu'>torch_tensor</span><span class='op'>(</span></span>
<span>  <span class='va'>tok</span><span class='op'>$</span><span class='fu'>encode</span><span class='op'>(</span></span>
<span>    <span class='fu'><a href='https://rdrr.io/r/base/paste.html'>paste</a></span><span class='op'>(</span></span>
<span>      <span class='st'>"No duty is imposed on the rich, rights of the poor is a hollow phrase...)"</span>,</span>
<span>      <span class='st'>"Enough languishing in custody. Equality"</span></span>
<span>    <span class='op'>)</span></span>
<span>  <span class='op'>)</span><span class='op'>$</span></span>
<span>    <span class='va'>ids</span></span>
<span><span class='op'>)</span><span class='op'>$</span></span>
<span>  <span class='fu'>view</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>1</span>, <span class='op'>-</span><span class='fl'>1</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='va'>idx</span></span></code></pre>
</div>
</div>
<pre><code>torch_tensor
Columns 1 to 11  2949   7077    318  10893    319    262   5527     11   2489    286    262

Columns 12 to 22  3595    318    257  20596   9546   2644  31779   2786   3929    287  10804

Columns 23 to 24    13  31428
[ CPULongType{1,24} ]</code></pre>
<h4 id="generate-samples">Generate samples</h4>
<p>Sample generation is an iterative process, the models last prediction getting appended to the  growing  prompt.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>prompt_length</span> <span class='op'>&lt;-</span> <span class='va'>idx</span><span class='op'>$</span><span class='fu'>size</span><span class='op'>(</span><span class='op'>-</span><span class='fl'>1</span><span class='op'>)</span></span>
<span></span>
<span><span class='kw'>for</span> <span class='op'>(</span><span class='va'>i</span> <span class='kw'>in</span> <span class='fl'>1</span><span class='op'>:</span><span class='fl'>30</span><span class='op'>)</span> <span class='op'>{</span> <span class='co'># decide on maximal length of output sequence</span></span>
<span>  <span class='co'># obtain next prediction (raw score)</span></span>
<span>  <span class='fu'>with_no_grad</span><span class='op'>(</span><span class='op'>{</span></span>
<span>    <span class='va'>logits</span> <span class='op'>&lt;-</span> <span class='fu'>model</span><span class='op'>(</span><span class='va'>idx</span> <span class='op'>+</span> <span class='fl'>1L</span><span class='op'>)</span></span>
<span>  <span class='op'>}</span><span class='op'>)</span></span>
<span>  <span class='va'>last_logits</span> <span class='op'>&lt;-</span> <span class='va'>logits</span><span class='op'>[</span>, <span class='op'>-</span><span class='fl'>1</span>, <span class='op'>]</span></span>
<span>  <span class='co'># pick highest scores (how many is up to you)</span></span>
<span>  <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='va'>prob</span>, <span class='va'>ind</span><span class='op'>)</span> <span class='op'>%&lt;-%</span> <span class='va'>last_logits</span><span class='op'>$</span><span class='fu'>topk</span><span class='op'>(</span><span class='fl'>50</span><span class='op'>)</span></span>
<span>  <span class='va'>last_logits</span> <span class='op'>&lt;-</span> <span class='fu'>torch_full_like</span><span class='op'>(</span><span class='va'>last_logits</span>, <span class='op'>-</span><span class='cn'>Inf</span><span class='op'>)</span><span class='op'>$</span><span class='fu'>scatter_</span><span class='op'>(</span><span class='op'>-</span><span class='fl'>1</span>, <span class='va'>ind</span>, <span class='va'>prob</span><span class='op'>)</span></span>
<span>  <span class='co'># convert to probabilities</span></span>
<span>  <span class='va'>probs</span> <span class='op'>&lt;-</span> <span class='fu'>nnf_softmax</span><span class='op'>(</span><span class='va'>last_logits</span>, dim <span class='op'>=</span> <span class='op'>-</span><span class='fl'>1</span><span class='op'>)</span></span>
<span>  <span class='co'># probabilistic sampling</span></span>
<span>  <span class='va'>id_next</span> <span class='op'>&lt;-</span> <span class='fu'>torch_multinomial</span><span class='op'>(</span><span class='va'>probs</span>, num_samples <span class='op'>=</span> <span class='fl'>1</span><span class='op'>)</span> <span class='op'>-</span> <span class='fl'>1L</span></span>
<span>  <span class='co'># stop if end of sequence predicted</span></span>
<span>  <span class='kw'>if</span> <span class='op'>(</span><span class='va'>id_next</span><span class='op'>$</span><span class='fu'>item</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>==</span> <span class='fl'>0</span><span class='op'>)</span> <span class='op'>{</span></span>
<span>    <span class='kw'>break</span></span>
<span>  <span class='op'>}</span></span>
<span>  <span class='co'># append prediction to prompt</span></span>
<span>  <span class='va'>idx</span> <span class='op'>&lt;-</span> <span class='fu'>torch_cat</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span><span class='va'>idx</span>, <span class='va'>id_next</span><span class='op'>)</span>, dim <span class='op'>=</span> <span class='fl'>2</span><span class='op'>)</span></span>
<span><span class='op'>}</span></span></code></pre>
</div>
</div>
<p>To see the output, just use <code>tok$decode()</code>:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>tok</span><span class='op'>$</span><span class='fu'>decode</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/integer.html'>as.integer</a></span><span class='op'>(</span><span class='va'>idx</span><span class='op'>)</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<pre><code>[1] &quot;No duty is imposed on the rich, rights of the poor is a hollow phrase...
     Enough languishing in custody. Equality is over&quot;</code></pre>
<p>To experiment with text generation, just copy the self-contained file, and try different sampling-related parameters. (And prompts, of course!)</p>
<p>As always, thanks for reading!</p>
<p>Photo by <a 
href="https://unsplash.com/@marjan_blan?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Marjan
Blan</a> on <a 
href="https://unsplash.com/photos/UDdkJlfn7cU?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r distill-force-highlighting-css"><code class="sourceCode r"></code></pre></div>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-ba2016layer" class="csl-entry" role="doc-biblioentry">
Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. <span>Layer Normalization.</span> <a href="https://arxiv.org/abs/1607.06450">https://arxiv.org/abs/1607.06450</a>.
</div>
<div id="ref-BahdanauCB14" class="csl-entry" role="doc-biblioentry">
Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. <span>Neural Machine Translation by Jointly Learning to Align and Translate.</span> <em>CoRR</em> abs/1409.0473. <a href="http://arxiv.org/abs/1409.0473">http://arxiv.org/abs/1409.0473</a>.
</div>
<div id="ref-hendrycks2020gaussian" class="csl-entry" role="doc-biblioentry">
Hendrycks, Dan, and Kevin Gimpel. 2020. <span>Gaussian Error Linear Units (GELUs).</span> <a href="https://arxiv.org/abs/1606.08415">https://arxiv.org/abs/1606.08415</a>.
</div>
<div id="ref-Radford2018ImprovingLU" class="csl-entry" role="doc-biblioentry">
Radford, Alec, and Karthik Narasimhan. 2018. <span>Improving Language Understanding by Generative Pre-Training.</span> In.
</div>
<div id="ref-Radford2019LanguageMA" class="csl-entry" role="doc-biblioentry">
Radford, Alec, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. <span>Language Models Are Unsupervised Multitask Learners.</span> In.
</div>
<div id="ref-rope-paper" class="csl-entry" role="doc-biblioentry">
Su, Jianlin, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. 2021. <span>RoFormer: Enhanced Transformer with Rotary Position Embedding.</span> <em>arXiv Preprint arXiv:2104.09864</em>.
</div>
<div id="ref-vaswani2017attention" class="csl-entry" role="doc-biblioentry">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. <span>Attention Is All You Need.</span> <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.
</div>
</div>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>If this terminology is unfamiliar, youll find a nice (and very popular) introduction <a href="http://jalammar.github.io/illustrated-transformer/">here</a>.<a href="#fnref1" class="footnote-back" role="doc-backlink"></a></p></li>
<li id="fn2" role="doc-endnote"><p>I am italicizing the word so as to hint at a special way of using the term. While the expression in itself does sound rather strange, <em>attention</em> is often employed to signify the state reached after normalizing the  usually seen as raw  <em>scores</em>.<a href="#fnref2" class="footnote-back" role="doc-backlink"></a></p></li>
</ol>
</section>
<!--radix_placeholder_article_footer-->
<div class="article-footer">
  <p class="social_footer">
    <span class="disqus-comments">
      <i class="fas fa-comments"></i>
      &nbsp;
      <span class="disqus-comment-count" data-disqus-identifier="posts/2023-06-20-gpt2-torch/">Comment on this article</span>
    </span>
    <span class="article-sharing">
      Share: &nbsp;
      <a href="https://twitter.com/share?text=GPT-2%20from%20scratch%20with%20torch&amp;url=https%3A%2F%2Fblogs.rstudio.com%2Ftensorflow%2Fposts%2F2023-06-20-gpt2-torch%2F" aria-label="share on twitter">
        <i class="fab fa-twitter" aria-hidden="true"></i>
      </a>
      <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3A%2F%2Fblogs.rstudio.com%2Ftensorflow%2Fposts%2F2023-06-20-gpt2-torch%2F&amp;title=GPT-2%20from%20scratch%20with%20torch" aria-label="share on linkedin">
        <i class="fab fa-linkedin" aria-hidden="true"></i>
      </a>
    </span>
  </p>
  <script id="dsq-count-scr" src="https://tensorflow-for-r-blog.disqus.com/count.js" async></script>
  <div id="disqus_thread" class="hidden"></div>
  <script type="text/javascript" cookie-consent="functionality">
var disqus_config = function () {
  this.page.url = 'https://blogs.rstudio.com/tensorflow/posts/2023-06-20-gpt2-torch/';
  this.page.identifier = 'posts/2023-06-20-gpt2-torch/';
};
(function() {
  var d = document, s = d.createElement('script');
  s.src = 'https://tensorflow-for-r-blog.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
})();
</script>
  <p>
    <div class="subscribe">
<div id="subscribe-caption" style="line-height: 1.2; margin-bottom: 2px;">Enjoy this blog? Get notified of new posts by email:</div>

<script src="https://app-ab02.marketo.com/js/forms2/js/forms2.min.js"></script>
<form class="mtktoBlogEmailForm" id="mktoForm_2224"></form>
<script>

// establish metrics based on where the form is located
var in_sidebar = $('#subscribe-caption').parents('.sidebar-section').length;
if (in_sidebar) {
  var form_width = 190;
  var base_width = form_width - 23;
  var email_width = base_width + 'px';
  var label_width = (base_width - 20) + 'px';
  var button_width = email_width;
  var button_padding = '30px';
  var button_margin = '';
  var font_size = '12px';
} else {
  var form_width = 400;
  var base_width = form_width - 23;
  var email_width = base_width + 'px';
  var label_width = (base_width - 20) + 'px';
  var button_width = email_width;
  var button_padding = '30px';
  var button_margin = '12px';
  var font_size = '15px';
}

$('#subscribe-caption')
  .css('width', base_width)
  .css('font-size', font_size);


MktoForms2.loadForm("https://app-ab02.marketo.com", "709-NXN-706", 2224, function(form) {

  // get jquery reference to form
  form = $(form.getFormElem().get(0));

  $(form).css('width', form_width + 'px');
  $(form).find('.mktoOffset').remove();
  $(form).find('.mktoGutter').remove();
  $(form).find('.mktoEmailField').css('width', email_width);
  $(form).find('.mktoLabel').children().attr('style', 'font-weight: 400');
  $(form).find('.mktoLabel')
      .css('width', label_width)
      .css('font-size', font_size);
  $(form).find('.mktoButtonRow')
      .css('width', button_width)
      .css('text-align', 'center');
  $(form).find('.mktoButtonWrap').attr('style', '');
  $(form).find('.mktoButton')
      .css('margin-top', '10px')
      .css('padding-left', button_padding)
      .css('padding-right', button_padding)
      .css('font-size', font_size)
      .css('margin-top', button_margin);
});
</script>
Posts also available at <a href="https://www.r-bloggers.com">r-bloggers</a>

<script>
  document.addEventListener("DOMContentLoaded", function(){
    var menulink = document.querySelector("a[href='#category:R']");
    if (menulink) menulink.parentNode.style.display = "None";
    
    for (var e of document.querySelectorAll(".dt-tag")) {
      if (e.innerHTML == 'R') e.style.display = "None";
    }
  });
</script>
</div>
  </p>
</div>
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>


<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
  <h3 id="references">References</h3>
  <div id="references-listing"></div>
  <h3 id="reuse">Reuse</h3>
  <p>Text and figures are licensed under Creative Commons Attribution <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>. The figures that have been reused from other sources don't fall under this license and can be recognized by a note in their caption: "Figure from ...".</p>
  <h3 id="citation">Citation</h3>
  <p>For attribution, please cite this work as</p>
  <pre class="citation-appendix short">Keydana (2023, June 20). Posit AI Blog: GPT-2 from scratch with torch. Retrieved from https://blogs.rstudio.com/tensorflow/posts/2023-06-20-gpt2-torch/</pre>
  <p>BibTeX citation</p>
  <pre class="citation-appendix long">@misc{keydanagpt2,
  author = {Keydana, Sigrid},
  title = {Posit AI Blog: GPT-2 from scratch with torch},
  url = {https://blogs.rstudio.com/tensorflow/posts/2023-06-20-gpt2-torch/},
  year = {2023}
}</pre>
</div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
