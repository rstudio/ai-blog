@article{365153.365168,
author = {Weizenbaum, Joseph},
title = {ELIZA - a Computer Program for the Study of Natural Language Communication between Man and Machine},
year = {1966},
issue_date = {Jan. 1966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/365153.365168},
doi = {10.1145/365153.365168},
journal = {Commun. ACM},
month = {jan},
pages = {36â€“45},
numpages = {10}
}

@inproceedings{bender-koller-2020-climbing,
    title = "Climbing towards {NLU}: {On} Meaning, Form, and Understanding in the Age of Data",
    author = "Bender, Emily M.  and
      Koller, Alexander",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.463",
    doi = "10.18653/v1/2020.acl-main.463",
    pages = "5185--5198",
    abstract = "The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as {``}understanding{''} language or capturing {``}meaning{''}. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of {``}Taking Stock of Where We{'}ve Been and Where We{'}re Going{''}, we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.",
}

@inproceedings{Sprck2004LanguageM,
  title={Language modelling's generative model : is it rational?},
  author={Karen Spaerck},
  year={2004}
}

@inproceedings{3445922,
author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445922},
doi = {10.1145/3442188.3445922},
abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {610â€“623},
numpages = {14},
location = {Virtual Event, Canada},
series = {FAccT '21}
}


@article{abs-1907-07355,
  author       = {Timothy Niven and
                  Hung{-}Yu Kao},
  title        = {Probing Neural Network Comprehension of Natural Language Arguments},
  journal      = {CoRR},
  volume       = {abs/1907.07355},
  year         = {2019},
  url          = {http://arxiv.org/abs/1907.07355},
  eprinttype    = {arXiv},
  eprint       = {1907.07355},
  timestamp    = {Tue, 23 Jul 2019 10:54:22 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1907-07355.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{lstm,
author = {Hochreiter, Sepp and Schmidhuber, JÃ¼rgen},
year = {1997},
month = {12},
pages = {1735-80},
title = {Long Short-term Memory},
volume = {9},
journal = {Neural computation},
doi = {10.1162/neco.1997.9.8.1735}
}

@inproceedings{habernal-etal-2018-argument,
    title = "The Argument Reasoning Comprehension Task: Identification and Reconstruction of Implicit Warrants",
    author = "Habernal, Ivan  and
      Wachsmuth, Henning  and
      Gurevych, Iryna  and
      Stein, Benno",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1175",
    doi = "10.18653/v1/N18-1175",
    pages = "1930--1940",
    abstract = "Reasoning is a crucial part of natural language argumentation. To comprehend an argument, one must analyze its warrant, which explains why its claim follows from its premises. As arguments are highly contextualized, warrants are usually presupposed and left implicit. Thus, the comprehension does not only require language understanding and logic skills, but also depends on common sense. In this paper we develop a methodology for reconstructing warrants systematically. We operationalize it in a scalable crowdsourcing process, resulting in a freely licensed dataset with warrants for 2k authentic arguments from news comments. On this basis, we present a new challenging task, the argument reasoning comprehension task. Given an argument with a claim and a premise, the goal is to choose the correct implicit warrant from two options. Both warrants are plausible and lexically close, but lead to contradicting claims. A solution to this task will define a substantial step towards automatic warrant reconstruction. However, experiments with several neural attention and language models reveal that current approaches do not suffice.",
}


@misc{vaswani2017attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Radford2019LanguageMA,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019}
}

@inproceedings{Radford2018ImprovingLU,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018}
}

@inproceedings{Caliskan_2022,
	doi = {10.1145/3514094.3534162},
	url = {https://doi.org/10.1145%2F3514094.3534162},
	year = 2022,
	month = {jul},
	publisher = {{ACM}},
	author = {Aylin Caliskan and Pimparkar Parth Ajay and Tessa Charlesworth and Robert Wolfe and Mahzarin R. Banaji},
	title = {Gender Bias in Word Embeddings},
	booktitle = {Proceedings of the 2022 {AAAI}/{ACM} Conference on {AI},   Ethics, and Society}
}
