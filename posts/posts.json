[
  {
    "path": "posts/2024-10-30-mall/",
    "title": "Introducing mall for R...and Python",
    "description": "We are proud to introduce the {mall} package. With {mall}, you can use a  local LLM to run NLP operations across a data frame. (sentiment,  summarization, translation, etc). {mall} has been simultaneously released to CRAN and PyPi (as an extension to Polars).",
    "author": [
      {
        "name": "Edgar Ruiz",
        "url": {}
      }
    ],
    "date": "2024-10-30",
    "categories": [
      "Python",
      "R",
      "LLM",
      "Polars",
      "Natural Language Processing",
      "Tabular Data"
    ],
    "contents": "\n\nContents\nThe beginning\nReaching viability\nThe project\nThe approach\nWhat’s next\n\nThe beginning\nA few months ago, while working on the Databricks with R workshop, I came\nacross some of their custom SQL functions. These particular functions are\nprefixed with “ai_”, and they run NLP with a simple SQL call:\n> SELECT ai_analyze_sentiment('I am happy');\n  positive\n\n> SELECT ai_analyze_sentiment('I am sad');\n  negative\nThis was a revelation to me. It showcased a new way to use\nLLMs in our daily work as analysts. To-date, I had primarily employed LLMs\nfor code completion and development tasks. However, this new approach\nfocuses on using LLMs directly against our data instead.\nMy first reaction was to try and access the custom functions via R. With\ndbplyr we can access SQL functions\nin R, and it was great to see them work:\norders |>\n  mutate(\n    sentiment = ai_analyze_sentiment(o_comment)\n  )\n#> # Source:   SQL [6 x 2]\n#>   o_comment                   sentiment\n#>   <chr>                        <chr>    \n#> 1 \", pending theodolites …    neutral  \n#> 2 \"uriously special foxes …   neutral  \n#> 3 \"sleep. courts after the …  neutral  \n#> 4 \"ess foxes may sleep …      neutral  \n#> 5 \"ts wake blithely unusual … mixed    \n#> 6 \"hins sleep. fluffily …     neutral\nOne downside of this integration is that even though accessible through R, we\nrequire a live connection to Databricks in order to utilize an LLM in this\nmanner, thereby limiting the number of people who can benefit from it.\nAccording to their documentation, Databricks is leveraging the Llama 3.1 70B\nmodel. While this is a highly effective Large Language Model, its enormous size\nposes a significant challenge for most users’ machines, making it impractical\nto run on standard hardware.\nReaching viability\nLLM development has been accelerating at a rapid pace. Initially, only online\nLarge Language Models (LLMs) were viable for daily use. This sparked concerns among\ncompanies hesitant to share their data externally. Moreover, the cost of using\nLLMs online can be substantial, per-token charges can add up quickly.\nThe ideal solution would be to integrate an LLM into our own systems, requiring\nthree essential components:\nA model that can fit comfortably in memory\nA model that achieves sufficient accuracy for NLP tasks\nAn intuitive interface between the model and the user’s laptop\nIn the past year, having all three of these elements was nearly impossible.\nModels capable of fitting in-memory were either inaccurate or excessively slow.\nHowever, recent advancements, such as Llama from Meta\nand cross-platform interaction engines like Ollama, have\nmade it feasible to deploy these models, offering a promising solution for\ncompanies looking to integrate LLMs into their workflows.\nThe project\nThis project started as an exploration, driven by my interest in leveraging a\n“general-purpose” LLM to produce results comparable to those from Databricks AI\nfunctions. The primary challenge was determining how much setup and preparation\nwould be required for such a model to deliver reliable and consistent results.\nWithout access to a design document or open-source code, I relied solely on the\nLLM’s output as a testing ground. This presented several obstacles, including\nthe numerous options available for fine-tuning the model. Even within prompt\nengineering, the possibilities are vast. To ensure the model was not too\nspecialized or focused on a specific subject or outcome, I needed to strike a\ndelicate balance between accuracy and generality.\nFortunately, after conducting extensive testing, I discovered that a simple\n“one-shot” prompt yielded the best results. By “best,” I mean that the answers\nwere both accurate for a given row and consistent across multiple rows.\nConsistency was crucial, as it meant providing answers that were one of the\nspecified options (positive, negative, or neutral), without any additional\nexplanations.\nThe following is an example of a prompt that worked reliably against\nLlama 3.2:\n>>> You are a helpful sentiment engine. Return only one of the \n... following answers: positive, negative, neutral. No capitalization. \n... No explanations. The answer is based on the following text: \n... I am happy\npositive\nAs a side note, my attempts to submit multiple rows at once proved unsuccessful.\nIn fact, I spent a significant amount of time exploring different approaches,\nsuch as submitting 10 or 2 rows simultaneously, formatting them in JSON or\nCSV formats. The results were often inconsistent, and it didn’t seem to accelerate\nthe process enough to be worth the effort.\nOnce I became comfortable with the approach, the next step was wrapping the\nfunctionality within an R package.\nThe approach\nOne of my goals was to make the mall package as “ergonomic” as possible. In\nother words, I wanted to ensure that using the package in R and Python\nintegrates seamlessly with how data analysts use their preferred language on a\ndaily basis.\nFor R, this was relatively straightforward. I simply needed to verify that the\nfunctions worked well with pipes (%>% and |>) and could be easily\nincorporated into packages like those in the tidyverse:\nreviews |> \n  llm_sentiment(review) |> \n  filter(.sentiment == \"positive\") |> \n  select(review) \n#>                                                               review\n#> 1 This has been the best TV I've ever used. Great screen, and sound.\nHowever, for Python, being a non-native language for me, meant that I had to adapt my\nthinking about data manipulation. Specifically, I learned that in Python,\nobjects (like pandas DataFrames) “contain” transformation functions by design.\nThis insight led me to investigate if the Pandas API allows for extensions,\nand fortunately, it did! After exploring the possibilities, I decided to start\nwith Polar, which allowed me to extend its API by creating a new namespace.\nThis simple addition enabled users to easily access the necessary functions:\n>>> import polars as pl\n>>> import mall\n>>> df = pl.DataFrame(dict(x = [\"I am happy\", \"I am sad\"]))\n>>> df.llm.sentiment(\"x\")\nshape: (2, 2)\n┌────────────┬───────────┐\n│ x          ┆ sentiment │\n│ ---        ┆ ---       │\n│ str        ┆ str       │\n╞════════════╪═══════════╡\n│ I am happy ┆ positive  │\n│ I am sad   ┆ negative  │\n└────────────┴───────────┘\nBy keeping all the new functions within the llm namespace, it becomes very easy\nfor users to find and utilize the ones they need:\n\nWhat’s next\nI think it will be easier to know what is to come for mall once the community\nuses it and provides feedback. I anticipate that adding more LLM back ends will\nbe the main request. The other possible enhancement will be when new updated\nmodels are available, then the prompts may need to be updated for that given\nmodel. I experienced this going from LLama 3.1 to Llama 3.2. There was a need\nto tweak one of the prompts. The package is structured in a way the future\ntweaks like that will be additions to the package, and not replacements to the\nprompts, so as to retains backwards compatibility.\nThis is the first time I write an article about the history and structure of a\nproject. This particular effort was so unique because of the R + Python, and the\nLLM aspects of it, that I figured it is worth sharing.\nIf you wish to learn more about mall, feel free to visit its official site:\nhttps://mlverse.github.io/mall/\n\n\n\n",
    "preview": "posts/2024-10-30-mall/images/article.png",
    "last_modified": "2024-11-21T15:48:17+00:00",
    "input_file": {},
    "preview_width": 1176,
    "preview_height": 767
  },
  {
    "path": "posts/2024-05-21-keras3/",
    "title": "Introducing Keras 3 for R",
    "description": "We are thrilled to introduce {keras3}, the next version of the Keras R package. {keras3} is a ground-up rebuild of {keras}, maintaining the beloved features of the original while refining and simplifying the API based on valuable insights gathered over the past few years.",
    "author": [
      {
        "name": "Tomasz Kalinowski",
        "url": {}
      }
    ],
    "date": "2024-05-21",
    "categories": [
      "TensorFlow/Keras",
      "R"
    ],
    "contents": "\n\nContents\nInstallation\nWhat’s new:\nDocumentation\nMulti-backend support\nThe ‘Ops’ family\nIngest tabular data with layer_feature_space()\nNew Subclassing API\nSaving and Export\nNew random family\nOther additions:\nMigrating from {keras} to {keras3}\n\nSummary\n\nWe are thrilled to introduce keras3, the next version of the Keras R\npackage. keras3 is a ground-up rebuild of {keras}, maintaining the\nbeloved features of the original while refining and simplifying the API\nbased on valuable insights gathered over the past few years.\nKeras provides a complete toolkit for building deep learning models in\nR—it’s never been easier to build, train, evaluate, and deploy deep\nlearning models.\nInstallation\nTo install Keras 3:\ninstall.packages(\"keras3\")\nlibrary(keras3)\ninstall_keras()\nWhat’s new:\nDocumentation\nGreat documentation is essential, and we’ve worked hard to make sure\nthat keras3 has excellent documentation, both now, and in the future.\nKeras 3 comes with a full refresh of the website:\nhttps://keras.posit.co. There, you will find guides, tutorials,\nreference pages with rendered examples, and a new examples gallery. All\nthe reference pages and guides are also available via R’s built-in help\nsystem.\nIn a fast moving ecosystem like deep learning, creating great\ndocumentation and wrappers once is not enough. There also need to be\nworkflows that ensure the documentation is up-to-date with upstream\ndependencies. To accomplish this, {keras3} includes two new maintainer\nfeatures that ensure the R documentation and function wrappers will stay\nup-to-date:\nWe now take snapshots of the upstream documentation and API surface.\nWith each release, all R documentation is rebased on upstream\nupdates. This workflow ensures that all R documentation (guides,\nexamples, vignettes, and reference pages) and R function signatures\nstay up-to-date with upstream. This snapshot-and-rebase\nfunctionality is implemented in a new standalone R package,\n{doctether}, which may\nbe useful for R package maintainers needing to keep documentation in\nparity with dependencies.\nAll examples and vignettes can now be evaluated and rendered during\na package build. This ensures that no stale or broken example code\nmakes it into a release. It also means all user facing example code\nnow additionally serves as an extended suite of snapshot unit and\nintegration tests.\nEvaluating code in vignettes and examples is still not permitted\naccording to CRAN restrictions. We work around the CRAN restriction\nby adding additional package build steps that pre-render\nexamples\nand\nvignettes.\nCombined, these two features will make it substantially easier for Keras\nin R to maintain feature parity and up-to-date documentation with the\nPython API to Keras.\nMulti-backend support\nSoon after its launch in 2015, Keras featured support for most popular\ndeep learning frameworks: TensorFlow, Theano, MXNet, and CNTK. Over\ntime, the landscape shifted; Theano, MXNet, and CNTK were retired, and\nTensorFlow surged in popularity. In 2021, three years ago, TensorFlow\nbecame the premier and only supported Keras backend. Now, the landscape\nhas shifted again.\nKeras 3 brings the return of multi-backend support. Choose a backend by\ncalling:\nuse_backend(\"jax\") # or \"tensorflow\", \"torch\", \"numpy\"\nThe default backend continues to be TensorFlow, which is the best choice\nfor most users today; for small-to-medium sized models this is still the\nfastest backend. However, each backend has different strengths, and\nbeing able to switch easily will let you adapt to changes as your\nproject, or the frameworks themselves, evolve.\nToday, switching to the Jax backend can, for some model types, bring\nsubstantial speed improvements. Jax is also the only backend that has\nsupport for a new model parallelism distributed training API. Switching\nto Torch can be helpful during development, often producing simpler\ntrackbacks while debugging.\nKeras 3 also lets you incorporate any pre-existing Torch, Jax, or Flax\nmodule as a standard Keras layer by using the appropriate wrapper,\nletting you build atop existing projects with Keras. For example, train\na Torch model using the Keras high-level training API (compile() +\nfit()), or include a Flax module as a component of a larger Keras\nmodel. The new multi-backend support lets you use Keras à la carte.\nThe ‘Ops’ family\n{keras3} introduces a new “Operations” family of function. The Ops\nfamily, currently with over 200\nfunctions,\nprovides a comprehensive suite of operations typically needed when\noperating on nd-arrays for deep learning. The Operation family\nsupersedes and greatly expands on the former family of backend functions\nprefixed with k_ in the {keras} package.\nThe Ops functions let you write backend-agnostic code. They provide a\nuniform API, regardless of if you’re working with TensorFlow Tensors,\nJax Arrays, Torch Tensors, Keras Symbolic Tensors, NumPy arrays, or R\narrays.\nThe Ops functions:\nall start with prefix op_ (e.g., op_stack())\nall are pure functions (they produce no side-effects)\nall use consistent 1-based indexing, and coerce doubles to integers\nas needed\nall are safe to use with any backend (tensorflow, jax, torch, numpy)\nall are safe to use in both eager and graph/jit/tracing modes\nThe Ops API includes:\nThe entirety of the NumPy API (numpy.*)\nThe TensorFlow NN API (tf.nn.*)\nCommon linear algebra functions (A subset of scipy.linalg.*)\nA subfamily of image transformers\nA comprehensive set of loss functions\nAnd more!\nIngest tabular data with layer_feature_space()\nkeras3 provides a new set of functions for building models that ingest\ntabular data: layer_feature_space() and a family of feature\ntransformer functions (prefix, feature_) for building keras models\nthat can work with tabular data, either as inputs to a keras model, or\nas preprocessing steps in a data loading pipeline (e.g., a\ntfdatasets::dataset_map()).\nSee the reference\npage and an\nexample usage in a full end-to-end\nexample\nto learn more.\nNew Subclassing API\nThe subclassing API has been refined and extended to more Keras\ntypes.\nDefine subclasses simply by calling: Layer(), Loss(), Metric(),\nCallback(), Constraint(), Model(), and LearningRateSchedule().\nDefining {R6} proxy classes is no longer necessary.\nAdditionally the documentation page for each of the subclassing\nfunctions now contains a comprehensive listing of all the available\nattributes and methods for that type. Check out\n?Layer to see what’s\npossible.\nSaving and Export\nKeras 3 brings a new model serialization and export API. It is now much\nsimpler to save and restore models, and also, to export them for\nserving.\nsave_model()/load_model():\nA new high-level file format (extension: .keras) for saving and\nrestoring a full model.\nThe file format is backend-agnostic. This means that you can convert\ntrained models between backends, simply by saving with one backend,\nand then loading with another. For example, train a model using Jax,\nand then convert to Tensorflow for export.\nexport_savedmodel():\nExport just the forward pass of a model as a compiled artifact for\ninference with TF\nServing or (soon)\nPosit Connect. This\nis the easiest way to deploy a Keras model for efficient and\nconcurrent inference serving, all without any R or Python runtime\ndependency.\nLower level entry points:\nsave_model_weights() / load_model_weights():\nsave just the weights as .h5 files.\nsave_model_config() / load_model_config():\nsave just the model architecture as a json file.\n\nregister_keras_serializable():\nRegister custom objects to enable them to be serialized and\ndeserialized.\nserialize_keras_object() / deserialize_keras_object():\nConvert any Keras object to an R list of simple types that is safe\nto convert to JSON or rds.\nSee the new Serialization and Saving\nvignette\nfor more details and examples.\nNew random family\nA new family of random tensor\ngenerators.\nLike the Ops family, these work with all backends. Additionally, all the\nRNG-using methods have support for stateless usage when you pass in a\nseed generator. This enables tracing and compilation by frameworks that\nhave special support for stateless, pure, functions, like Jax. See\n?random_seed_generator()\nfor example usage.\nOther additions:\nNew shape()\nfunction, one-stop utility for working with tensor shapes in all\ncontexts.\nNew and improved print(model) and plot(model) method. See some\nexamples of output in the Functional API\nguide\nAll new fit() progress bar and live metrics viewer output,\nincluding new dark-mode support in the RStudio IDE.\nNew config\nfamily,\na curated set of functions for getting and setting Keras global\nconfigurations.\nAll of the other function families have expanded with new members:\nLayers\n(prefix, layer_)\nActivation\nfunctions\n(prefix, activation_)\nOptimizers\n(prefix, optimizer_)\nMetrics\n(prefix metric_)\nLosses\n(prefix loss_)\nImage\npreprocesing\n(prefixes image_ and op_image_)\nApplications\n(prefix, application_)\n\nMigrating from {keras} to {keras3}\n{keras3} supersedes the {keras} package.\nIf you’re writing new code today, you can start using {keras3} right\naway.\nIf you have legacy code that uses {keras}, you are encouraged to\nupdate the code for {keras3}. For many high-level API functions, such\nas layer_dense(), fit(), and keras_model(), minimal to no changes\nare required. However there is a long tail of small changes that you\nmight need to make when updating code that made use of the lower-level\nKeras API. Some of those are documented here:\nhttps://keras.io/guides/migrating_to_keras_3/.\nIf you’re running into issues or have questions about updating, don’t\nhesitate to ask on https://github.com/rstudio/keras/issues or\nhttps://github.com/rstudio/keras/discussions.\nThe {keras} and {keras3} packages will coexist while the community\ntransitions. During the transition, {keras} will continue to receive\npatch updates for compatibility with Keras v2, which continues to be\npublished to PyPi under the package name tf-keras. After tf-keras is\nno longer maintained, the {keras} package will be archived.\nSummary\nIn summary, {keras3} is a robust update to the Keras R package,\nincorporating new features while preserving the ease of use and\nfunctionality of the original. The new multi-backend support,\ncomprehensive suite of Ops functions, refined model serialization API,\nand updated documentation workflows enable users to easily take\nadvantage of the latest developments in the deep learning community.\nWhether you are a seasoned Keras user or just starting your deep\nlearning journey, Keras 3 provides the tools and flexibility to build,\ntrain, and deploy models with ease and confidence. As we transition from\nKeras 2 to Keras 3, we are committed to supporting the community and\nensuring a smooth migration. We invite you to explore the new features,\ncheck out the updated documentation, and join the conversation on our\nGitHub discussions page. Welcome to the next chapter of deep learning in\nR with Keras 3!\n\n\n\n",
    "preview": "posts/2024-05-21-keras3/images/preview.png",
    "last_modified": "2024-11-21T15:53:50+00:00",
    "input_file": {},
    "preview_width": 774,
    "preview_height": 269
  },
  {
    "path": "posts/2024-04-22-sparklyr-updates/",
    "title": "News from the sparkly-verse",
    "description": "Highlights to the most recent updates to `sparklyr` and friends",
    "author": [
      {
        "name": "Edgar Ruiz",
        "url": {}
      }
    ],
    "date": "2024-04-22",
    "categories": [
      "Packages/Releases",
      "Spark",
      "R"
    ],
    "contents": "\n\nContents\nHighlights\npysparklyr 0.1.4\nsparkxgb\nsparklyr 1.8.5\n\nHighlights\nsparklyr and friends have been getting some important updates in the past few\nmonths, here are some highlights:\nspark_apply() now works on Databricks Connect v2\nsparkxgb is coming back to life\nSupport for Spark 2.3 and below has ended\npysparklyr 0.1.4\nspark_apply() now works on Databricks Connect v2. The latest pysparklyr\nrelease uses the rpy2 Python library as the backbone of the integration.\nDatabricks Connect v2, is based on Spark Connect. At this time, it supports\nPython user-defined functions (UDFs), but not R user-defined functions.\nUsing rpy2 circumvents this limitation. As shown in the diagram, sparklyr\nsends the the R code to the locally installed rpy2, which in turn sends it\nto Spark. Then the rpy2 installed in the remote Databricks cluster will run\nthe R code.\n\n\n\nFigure 1: R code via rpy2\n\n\n\nA big advantage of this approach, is that rpy2 supports Arrow. In fact it\nis the recommended Python library to use when integrating Spark, Arrow and\nR.\nThis means that the data exchange between the three environments will be much\nfaster!\nAs in its original implementation, schema inferring works, and as with the\noriginal implementation, it has a performance cost. But unlike the original,\nthis implementation will return a ‘columns’ specification that you can use\nfor the next time you run the call.\nspark_apply(\n  tbl_mtcars,\n  nrow,\n  group_by = \"am\"\n)\n\n#> To increase performance, use the following schema:\n#> columns = \"am double, x long\"\n\n#> # Source:   table<`sparklyr_tmp_table_b84460ea_b1d3_471b_9cef_b13f339819b6`> [2 x 2]\n#> # Database: spark_connection\n#>      am     x\n#>   <dbl> <dbl>\n#> 1     0    19\n#> 2     1    13\nA full article about this new capability is available here:\nRun R inside Databricks Connect\nsparkxgb\nThe sparkxgb is an extension of sparklyr. It enables integration with\nXGBoost. The current CRAN release\ndoes not support the latest versions of XGBoost. This limitation has recently\nprompted a full refresh of sparkxgb. Here is a summary of the improvements,\nwhich are currently in the development version of the package:\nThe xgboost_classifier() and xgboost_regressor() functions no longer\npass values of two arguments. These were deprecated by XGBoost and\ncause an error if used. In the R function, the arguments will remain for\nbackwards compatibility, but will generate an informative error if not left NULL:\nsketch_eps - As of XGBoost version 1.6.0\nsketch_eps was replaced by max_bins\ntimeout_request_workers - Removed in XGBoost version 1.7.0\nbecause it was no longer needed when XGBoost added barrier support\n\nUpdates the JVM version used during the Spark session. It now uses xgboost4j-spark\nversion 2.0.3,\ninstead of 0.8.1. This gives us access to XGboost’s most recent Spark code.\nUpdates code that used deprecated functions from upstream R dependencies. It\nalso stops using an un-maintained package as a dependency (forge). This\neliminated all of the warnings that were happening when fitting a model.\nMajor improvements to package testing. Unit tests were updated and expanded,\nthe way sparkxgb automatically starts and stops the Spark session for testing\nwas modernized, and the continuous integration tests were restored. This will\nensure the package’s health going forward.\nremotes::install_github(\"rstudio/sparkxgb\")\n\nlibrary(sparkxgb)\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- copy_to(sc, iris)\n\nxgb_model <- xgboost_classifier(\n  iris_tbl,\n  Species ~ .,\n  num_class = 3,\n  num_round = 50,\n  max_depth = 4\n)\n\nxgb_model %>% \n  ml_predict(iris_tbl) %>% \n  select(Species, predicted_label, starts_with(\"probability_\")) %>% \n  dplyr::glimpse()\n#> Rows: ??\n#> Columns: 5\n#> Database: spark_connection\n#> $ Species                <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa…\n#> $ predicted_label        <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa…\n#> $ probability_setosa     <dbl> 0.9971547, 0.9948581, 0.9968392, 0.9968392, 0.9…\n#> $ probability_versicolor <dbl> 0.002097376, 0.003301427, 0.002284616, 0.002284…\n#> $ probability_virginica  <dbl> 0.0007479066, 0.0018403779, 0.0008762418, 0.000…\nsparklyr 1.8.5\nThe new version of sparklyr does not have user facing improvements. But\ninternally, it has crossed an important milestone. Support for Spark version 2.3\nand below has effectively ended. The Scala\ncode needed to do so is no longer part of the package. As per Spark’s versioning\npolicy, found here,\nSpark 2.3 was ‘end-of-life’ in 2018.\nThis is part of a larger, and ongoing effort to make the immense code-base of\nsparklyr a little easier to maintain, and hence reduce the risk of failures.\nAs part of the same effort, the number of upstream packages that sparklyr\ndepends on have been reduced. This has been happening across multiple CRAN\nreleases, and in this latest release tibble, and rappdirs are no longer\nimported by sparklyr.\n\n\n\n",
    "preview": "posts/2024-04-22-sparklyr-updates/images/sparklyr.png",
    "last_modified": "2024-11-21T15:48:07+00:00",
    "input_file": {},
    "preview_width": 995,
    "preview_height": 664
  },
  {
    "path": "posts/2024-04-04-chat-with-llms-using-chattr/",
    "title": "Chat with AI in RStudio",
    "description": "Interact with Github Copilot and OpenAI's GPT (ChatGPT) models directly in RStudio. The `chattr` Shiny add-in makes it easy for you to interact with  these and other Large Language Models (LLMs).",
    "author": [
      {
        "name": "Edgar Ruiz",
        "url": {}
      }
    ],
    "date": "2024-04-04",
    "categories": [
      "Generenative Models",
      "Packages/Releases",
      "R"
    ],
    "contents": "\n\nContents\nGetting started\nPersonalized setup\nAdvanced customization\n\nBeyond the app\nRStudio Add-ins\nWorks with local LLMs\nExtending chattr\nFeedback welcome\n\nchattr is a package that enables interaction with Large Language Models (LLMs),\nsuch as GitHub Copilot Chat, and OpenAI’s GPT 3.5 and 4. The main vehicle is a\nShiny app that runs inside the RStudio IDE. Here is an example of what it looks\nlike running inside the Viewer pane:\n\n\n\nFigure 1: chattr’s Shiny app\n\n\n\nEven though this article highlights chattr’s integration with the RStudio IDE,\nit is worth mentioning that it works outside RStudio, for example the terminal.\nGetting started\nTo get started, install the package from CRAN, and then call the Shiny app\nusing the chattr_app() function:\n# Install from CRAN\ninstall.packages(\"chattr\")\n\n# Run the app\nchattr::chattr_app()\n\n#> ── chattr - Available models \n#> Select the number of the model you would like to use:\n#>\n#> 1: GitHub - Copilot Chat -  (copilot) \n#>\n#> 2: OpenAI - Chat Completions - gpt-3.5-turbo (gpt35) \n#>\n#> 3: OpenAI - Chat Completions - gpt-4 (gpt4) \n#>\n#> 4: LlamaGPT - ~/ggml-gpt4all-j-v1.3-groovy.bin (llamagpt) \n#>\n#>\n#> Selection:\n>\nAfter you select the model you wish to interact with, the app will open. The\nfollowing screenshot provides an overview of the different buttons and\nkeyboard shortcuts you can use with the app:\n\n\n\nFigure 2: chattr’s UI\n\n\n\nYou can start writing your requests in the main text box at the top left of the\napp. Then submit your question by either clicking on the ‘Submit’ button, or\nby pressing Shift+Enter.\nchattr parses the output of the LLM, and displays the code inside chunks. It\nalso places three buttons at the top of each chunk. One to copy the code to the\nclipboard, the other to copy it directly to your active script in RStudio, and\none to copy the code to a new script. To close the app, press the ‘Escape’ key.\nPressing the ‘Settings’ button will open the defaults that the chat session\nis using. These can be changed as you see fit. The ‘Prompt’ text box is\nthe additional text being sent to the LLM as part of your question.\n\n\n\nFigure 3: chattr’s UI - Settings page\n\n\n\nPersonalized setup\nchattr will try and identify which models you have setup,\nand will include only those in the selection menu. For Copilot and OpenAI,\nchattr confirms that there is an available authentication token in order to\ndisplay them in the menu. For example, if you have only have\nOpenAI setup, then the prompt will look something like this:\nchattr::chattr_app()\n#> ── chattr - Available models \n#> Select the number of the model you would like to use:\n#>\n#> 2: OpenAI - Chat Completions - gpt-3.5-turbo (gpt35) \n#>\n#> 3: OpenAI - Chat Completions - gpt-4 (gpt4) \n#>\n#> Selection:\n>\nIf you wish to avoid the menu, use the chattr_use() function. Here is an example\nof setting GPT 4 as the default:\nlibrary(chattr)\nchattr_use(\"gpt4\")\nchattr_app()\nYou can also select a model by setting the CHATTR_USE environment\nvariable.\nAdvanced customization\nIt is possible to customize many aspects of your interaction with the LLM. To do\nthis, use the chattr_defaults() function. This function displays and sets the\nadditional prompt sent to the LLM, the model to be used, determines if the\nhistory of the chat is to be sent to the LLM, and model specific arguments.\nFor example, you may wish to change the maximum number of tokens used per response,\nfor OpenAI you can use this:\n# Default for max_tokens is 1,000\nlibrary(chattr)\nchattr_use(\"gpt4\")\nchattr_defaults(model_arguments = list(\"max_tokens\" = 100))\n#> \n#> ── chattr ──────────────────────────────────────────────────────────────────────\n#> \n#> ── Defaults for: Default ──\n#> \n#> ── Prompt:\n#> • {{readLines(system.file('prompt/base.txt', package = 'chattr'))}}\n#> \n#> ── Model\n#> • Provider: OpenAI - Chat Completions\n#> • Path/URL: https://api.openai.com/v1/chat/completions\n#> • Model: gpt-4\n#> • Label: GPT 4 (OpenAI)\n#> \n#> ── Model Arguments:\n#> • max_tokens: 100\n#> • temperature: 0.01\n#> • stream: TRUE\n#> \n#> ── Context:\n#> Max Data Files: 0\n#> Max Data Frames: 0\n#> ✔ Chat History\n#> ✖ Document contents\nIf you wish to persist your changes to the defaults, use the chattr_defaults_save()\nfunction. This will create a yaml file, named ‘chattr.yml’ by default. If found,\nchattr will use this file to load all of the defaults, including the selected\nmodel.\nA more extensive description of this feature is available in the chattr website\nunder\nModify prompt enhancements\nBeyond the app\nIn addition to the Shiny app, chattr offers a couple of other ways to interact\nwith the LLM:\nUse the chattr() function\nHighlight a question in your script, and use it as your prompt\n> chattr(\"how do I remove the legend from a ggplot?\")\n#> You can remove the legend from a ggplot by adding \n#> `theme(legend.position = \"none\")` to your ggplot code. \nA more detailed article is available in chattr website\nhere.\nRStudio Add-ins\nchattr comes with two RStudio add-ins:\nSend prompt - It will submit the highlighted question from your script\nto the LLM\nOpen Chat - It will open the chattr app as a Shiny gadget\n\n\n\nFigure 4: chattr add-ins\n\n\n\nYou can bind these add-in calls to keyboard shortcuts, making it easy to open the app without having to write\nthe command every time. To learn how to do that, see the Keyboard Shortcut section in the\nchattr official website.\nWorks with local LLMs\nOpen-source, trained models, that are able to run in your laptop are widely\navailable today. Instead of integrating with each model individually, chattr\nworks with LlamaGPTJ-chat. This is a lightweight application that communicates\nwith a variety of local models. At this time, LlamaGPTJ-chat integrates with the\nfollowing families of models:\nGPT-J (ggml and gpt4all models)\nLLaMA (ggml Vicuna models from Meta)\nMosaic Pretrained Transformers (MPT)\nLlamaGPTJ-chat works right off the terminal. chattr integrates with the\napplication by starting an ‘hidden’ terminal session. There it initializes the\nselected model, and makes it available to start chatting with it.\nTo get started, you need to install LlamaGPTJ-chat, and download a compatible\nmodel. More detailed instructions are found\nhere.\nchattr looks for the location of the LlamaGPTJ-chat, and the installed model\nin a specific folder location in your machine. If your installation paths do\nnot match the locations expected by chattr, then the LlamaGPT will not show\nup in the menu. But that is OK, you can still access it with chattr_use():\nlibrary(chattr)\nchattr_use(\n  \"llamagpt\",   \n  path = \"[path to compiled program]\",\n  model = \"[path to model]\"\n  )\n#> \n#> ── chattr\n#> • Provider: LlamaGPT\n#> • Path/URL: [path to compiled program]\n#> • Model: [path to model]\n#> • Label: GPT4ALL 1.3 (LlamaGPT)\nExtending chattr\nchattr aims to make it easy for new LLM APIs to be added. chattr\nhas two components, the user-interface (Shiny app and\nchattr() function), and the included back-ends (GPT, Copilot, LLamaGPT).\nNew back-ends do not need to be added directly in chattr.\nIf you are a package\ndeveloper and would like to take advantage of the chattr UI, all you need to do is define ch_submit() method in your package.\nThe two output requirements for ch_submit() are:\nAs the final return value, send the full response from the model you are\nintegrating into chattr.\nIf streaming (stream is TRUE), output the current output as it is occurring.\nGenerally through a cat() function call.\nHere is a simple toy example that shows how to create a custom method for\nchattr:\nlibrary(chattr)\nch_submit.ch_my_llm <- function(defaults,\n                                prompt = NULL,\n                                stream = NULL,\n                                prompt_build = TRUE,\n                                preview = FALSE,\n                                ...) {\n  # Use `prompt_build` to prepend the prompt\n  if(prompt_build) prompt <- paste0(\"Use the tidyverse\\n\", prompt)\n  # If `preview` is true, return the resulting prompt back\n  if(preview) return(prompt)\n  llm_response <- paste0(\"You said this: \\n\", prompt)\n  if(stream) {\n    cat(\">> Streaming:\\n\")\n    for(i in seq_len(nchar(llm_response))) {\n      # If `stream` is true, make sure to `cat()` the current output\n      cat(substr(llm_response, i, i))\n      Sys.sleep(0.1)\n    }\n  }\n  # Make sure to return the entire output from the LLM at the end\n  llm_response\n}\n\nchattr_defaults(\"console\", provider = \"my llm\")\n#>\nchattr(\"hello\")\n#> >> Streaming:\n#> You said this: \n#> Use the tidyverse\n#> hello\nchattr(\"I can use it right from RStudio\", prompt_build = FALSE)\n#> >> Streaming:\n#> You said this: \n#> I can use it right from RStudio\nFor more detail, please visit the function’s reference page, link\nhere.\nFeedback welcome\nAfter trying it out, feel free to submit your thoughts or issues in the\nchattr’s GitHub repository.\n\n\n\n",
    "preview": "posts/2024-04-04-chat-with-llms-using-chattr/images/chattr.png",
    "last_modified": "2024-11-21T15:48:08+00:00",
    "input_file": {},
    "preview_width": 932,
    "preview_height": 357
  },
  {
    "path": "posts/2023-07-12-hugging-face-integrations/",
    "title": "Hugging Face Integrations",
    "description": "Hugging Face rapidly became a very popular platform to build, share and collaborate on  deep learning applications. We have worked on integrating the torch for R ecosystem with Hugging Face tools, allowing users to load and execute language models from their platform.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": {}
      }
    ],
    "date": "2023-07-12",
    "categories": [
      "Torch",
      "Releases",
      "R"
    ],
    "contents": "\n\nContents\nhfhub\ntok\nSpaces\nLooking forward\n\nWe are happy to announce the first releases of hfhub and tok are now on CRAN.\nhfhub is an R interface to Hugging Face Hub, allowing users to download and cache files\nfrom Hugging Face Hub while tok implements R bindings for the Hugging Face tokenizers\nlibrary.\nHugging Face rapidly became the platform to build, share and collaborate on\ndeep learning applications and we hope these integrations will help R users to\nget started using Hugging Face tools as well as building novel applications.\nWe also have previously announced the safetensors\npackage allowing to read and write files in the safetensors format.\nhfhub\nhfhub is an R interface to the Hugging Face Hub. hfhub currently implements a single\nfunctionality: downloading files from Hub repositories. Model Hub repositories are\nmainly used to store pre-trained model weights together with any other metadata\nnecessary to load the model, such as the hyperparameters configurations and the\ntokenizer vocabulary.\nDownloaded files are ached using the same layout as the Python library, thus cached\nfiles can be shared between the R and Python implementation, for easier and quicker\nswitching between languages.\nWe already use hfhub in the minhub package and\nin the ‘GPT-2 from scratch with torch’ blog post to\ndownload pre-trained weights from Hugging Face Hub.\nYou can use hub_download() to download any file from a Hugging Face Hub repository\nby specifying the repository id and the path to file that you want to download.\nIf the file is already in the cache, then the function returns the file path imediately,\notherwise the file is downloaded, cached and then the access path is returned.\npath <- hfhub::hub_download(\"gpt2\", \"model.safetensors\")\npath\n#> /Users/dfalbel/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/model.safetensors\ntok\nTokenizers are responsible for converting raw text into the sequence of integers that\nis often used as the input for NLP models, making them an critical component of the\nNLP pipelines. If you want a higher level overview of NLP pipelines, you might want to read\nour previous blog post ‘What are Large Language Models? What are they not?’.\nWhen using a pre-trained model (both for inference or for fine tuning) it’s very\nimportant that you use the exact same tokenization process that has been used during\ntraining, and the Hugging Face team has done an amazing job making sure that its algorithms\nmatch the tokenization strategies used most LLM’s.\ntok provides R bindings to the 🤗 tokenizers library. The tokenizers library is itself\nimplemented in Rust for performance and our bindings use the extendr project\nto help interfacing with R. Using tok we can tokenize text the exact same way most\nNLP models do, making it easier to load pre-trained models in R as well as sharing\nour models with the broader NLP community.\ntok can be installed from CRAN, and currently it’s usage is restricted to loading\ntokenizers vocabularies from files. For example, you can load the tokenizer for the GPT2\nmodel with:\ntokenizer <- tok::tokenizer$from_pretrained(\"gpt2\")\nids <- tokenizer$encode(\"Hello world! You can use tokenizers from R\")$ids\nids\n#> [1] 15496   995     0   921   460   779 11241 11341   422   371\ntokenizer$decode(ids)\n#> [1] \"Hello world! You can use tokenizers from R\"\nSpaces\nRemember that you can already host\nShiny (for R and Python) on Hugging Face Spaces. As an example, we have built a Shiny\napp that uses:\ntorch to implement GPT-NeoX (the neural network architecture of StableLM - the model used for chatting)\nhfhub to download and cache pre-trained weights from the StableLM repository\ntok to tokenize and pre-process text as input for the torch model. tok also uses hfhub to download the tokenizer’s vocabulary.\nThe app is hosted at in this Space.\nIt currently runs on CPU, but you can easily switch the the Docker image if you want\nto run it on a GPU for faster inference.\nThe app source code is also open-source and can be found in the Spaces file tab.\nLooking forward\nIt’s the very early days of hfhub and tok and there’s still a lot of work to do\nand functionality to implement. We hope to get community help to prioritize work,\nthus, if there’s a feature that you are missing, please open an issue in the\nGitHub repositories.\n\n\n\n",
    "preview": "posts/2023-07-12-hugging-face-integrations/images/install.png",
    "last_modified": "2024-11-21T15:50:22+00:00",
    "input_file": {},
    "preview_width": 1350,
    "preview_height": 728
  },
  {
    "path": "posts/2023-06-22-understanding-lora/",
    "title": "Understanding LoRA with a minimal example",
    "description": "LoRA (Low Rank Adaptation) is a new technique for fine-tuning deep learning models that works by reducing the number of trainable parameters and enables efficient task switching. In this blog post we will talk about the key ideas behind LoRA in a very minimal torch example.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": {}
      }
    ],
    "date": "2023-06-22",
    "categories": [
      "Torch",
      "Concepts",
      "R"
    ],
    "contents": "\n\nContents\nMethod\nImplementing in torch\nConcluding\n\nLoRA (Low-Rank Adaptation) is a new technique for fine tuning large scale pre-trained\nmodels. Such models are usually trained on general domain data, so as to have\nthe maximum amount of data. In order to obtain better results in tasks like chatting\nor question answering, these models can be further ‘fine-tuned’ or adapted on domain\nspecific data.\nIt’s possible to fine-tune a model just by initializing the model with the pre-trained\nweights and further training on the domain specific data. With the increasing size of\npre-trained models, a full forward and backward cycle requires a large amount of computing\nresources. Fine tuning by simply continuing training also requires a full copy of all\nparameters for each task/domain that the model is adapted to.\nLoRA: Low-Rank Adaptation of Large Language Models\nproposes a solution for both problems by using a low rank matrix decomposition.\nIt can reduce the number of trainable weights by 10,000 times and GPU memory requirements\nby 3 times.\nMethod\nThe problem of fine-tuning a neural network can be expressed by finding a \\(\\Delta \\Theta\\)\nthat minimizes \\(L(X, y; \\Theta_0 + \\Delta\\Theta)\\) where \\(L\\) is a loss function, \\(X\\) and \\(y\\)\nare the data and \\(\\Theta_0\\) the weights from a pre-trained model.\nWe learn the parameters \\(\\Delta \\Theta\\) with dimension \\(|\\Delta \\Theta|\\)\nequals to \\(|\\Theta_0|\\). When \\(|\\Theta_0|\\) is very large, such as in large scale\npre-trained models, finding \\(\\Delta \\Theta\\) becomes computationally challenging.\nAlso, for each task you need to learn a new \\(\\Delta \\Theta\\) parameter set, making\nit even more challenging to deploy fine-tuned models if you have more than a\nfew specific tasks.\nLoRA proposes using an approximation \\(\\Delta \\Phi \\approx \\Delta \\Theta\\) with \\(|\\Delta \\Phi| << |\\Delta \\Theta|\\).\nThe observation is that neural nets have many dense layers performing matrix multiplication,\nand while they typically have full-rank during pre-training, when adapting to a specific task\nthe weight updates will have a low “intrinsic dimension”.\nA simple matrix decomposition is applied for each weight matrix update \\(\\Delta \\theta \\in \\Delta \\Theta\\).\nConsidering \\(\\Delta \\theta_i \\in \\mathbb{R}^{d \\times k}\\) the update for the \\(i\\)th weight\nin the network, LoRA approximates it with:\n\\[\\Delta \\theta_i  \\approx \\Delta \\phi_i = BA\\]\nwhere \\(B \\in \\mathbb{R}^{d \\times r}\\), \\(A \\in \\mathbb{R}^{r \\times d}\\) and the rank \\(r << min(d, k)\\).\nThus instead of learning \\(d \\times k\\) parameters we now need to learn \\((d + k) \\times r\\) which is easily\na lot smaller given the multiplicative aspect. In practice, \\(\\Delta \\theta_i\\) is scaled\nby \\(\\frac{\\alpha}{r}\\) before being added to \\(\\theta_i\\), which can be interpreted as a\n‘learning rate’ for the LoRA update.\nLoRA does not increase inference latency, as once fine tuning is done, you can simply\nupdate the weights in \\(\\Theta\\) by adding their respective \\(\\Delta \\theta \\approx \\Delta \\phi\\).\nIt also makes it simpler to deploy multiple task specific models on top of one large model,\nas \\(|\\Delta \\Phi|\\) is much smaller than \\(|\\Delta \\Theta|\\).\nImplementing in torch\nNow that we have an idea of how LoRA works, let’s implement it using torch for a\nminimal problem. Our plan is the following:\nSimulate training data using a simple \\(y = X \\theta\\) model. \\(\\theta \\in \\mathbb{R}^{1001, 1000}\\).\nTrain a full rank linear model to estimate \\(\\theta\\) - this will be our ‘pre-trained’ model.\nSimulate a different distribution by applying a transformation in \\(\\theta\\).\nTrain a low rank model using the pre=trained weights.\nLet’s start by simulating the training data:\n\n\nlibrary(torch)\n\nn <- 10000\nd_in <- 1001\nd_out <- 1000\n\nthetas <- torch_randn(d_in, d_out)\n\nX <- torch_randn(n, d_in)\ny <- torch_matmul(X, thetas)\n\n\nWe now define our base model:\n\n\nmodel <- nn_linear(d_in, d_out, bias = FALSE)\n\n\nWe also define a function for training a model, which we are also reusing later.\nThe function does the standard traning loop in torch using the Adam optimizer.\nThe model weights are updated in-place.\n\n\ntrain <- function(model, X, y, batch_size = 128, epochs = 100) {\n  opt <- optim_adam(model$parameters)\n\n  for (epoch in 1:epochs) {\n    for(i in seq_len(n/batch_size)) {\n      idx <- sample.int(n, size = batch_size)\n      loss <- nnf_mse_loss(model(X[idx,]), y[idx])\n      \n      with_no_grad({\n        opt$zero_grad()\n        loss$backward()\n        opt$step()  \n      })\n    }\n    \n    if (epoch %% 10 == 0) {\n      with_no_grad({\n        loss <- nnf_mse_loss(model(X), y)\n      })\n      cat(\"[\", epoch, \"] Loss:\", loss$item(), \"\\n\")\n    }\n  }\n}\n\n\nThe model is then trained:\n\n\ntrain(model, X, y)\n#> [ 10 ] Loss: 577.075 \n#> [ 20 ] Loss: 312.2 \n#> [ 30 ] Loss: 155.055 \n#> [ 40 ] Loss: 68.49202 \n#> [ 50 ] Loss: 25.68243 \n#> [ 60 ] Loss: 7.620944 \n#> [ 70 ] Loss: 1.607114 \n#> [ 80 ] Loss: 0.2077137 \n#> [ 90 ] Loss: 0.01392935 \n#> [ 100 ] Loss: 0.0004785107\n\n\nOK, so now we have our pre-trained base model. Let’s suppose that we have data from\na slighly different distribution that we simulate using:\n\n\nthetas2 <- thetas + 1\n\nX2 <- torch_randn(n, d_in)\ny2 <- torch_matmul(X2, thetas2)\n\n\nIf we apply out base model to this distribution, we don’t get a good performance:\n\n\nnnf_mse_loss(model(X2), y2)\n#> torch_tensor\n#> 992.673\n#> [ CPUFloatType{} ][ grad_fn = <MseLossBackward0> ]\n\n\nWe now fine-tune our initial model. The distribution of the new data is just slighly\ndifferent from the initial one. It’s just a rotation of the data points, by adding 1\nto all thetas. This means that the weight updates are not expected to be complex, and\nwe shouldn’t need a full-rank update in order to get good results.\nLet’s define a new torch module that implements the LoRA logic:\n\n\nlora_nn_linear <- nn_module(\n  initialize = function(linear, r = 16, alpha = 1) {\n    self$linear <- linear\n    \n    # parameters from the original linear module are 'freezed', so they are not\n    # tracked by autograd. They are considered just constants.\n    purrr::walk(self$linear$parameters, \\(x) x$requires_grad_(FALSE))\n    \n    # the low rank parameters that will be trained\n    self$A <- nn_parameter(torch_randn(linear$in_features, r))\n    self$B <- nn_parameter(torch_zeros(r, linear$out_feature))\n    \n    # the scaling constant\n    self$scaling <- alpha / r\n  },\n  forward = function(x) {\n    # the modified forward, that just adds the result from the base model\n    # and ABx.\n    self$linear(x) + torch_matmul(x, torch_matmul(self$A, self$B)*self$scaling)\n  }\n)\n\n\nWe now initialize the LoRA model. We will use \\(r = 1\\), meaning that A and B will be just\nvectors. The base model has 1001x1000 trainable parameters. The LoRA model that we are\nare going to fine tune has just (1001 + 1000) which makes it 1/500 of the base model\nparameters.\n\n\nlora <- lora_nn_linear(model, r = 1)\n\n\nNow let’s train the lora model on the new distribution:\n\n\ntrain(lora, X2, Y2)\n#> [ 10 ] Loss: 798.6073 \n#> [ 20 ] Loss: 485.8804 \n#> [ 30 ] Loss: 257.3518 \n#> [ 40 ] Loss: 118.4895 \n#> [ 50 ] Loss: 46.34769 \n#> [ 60 ] Loss: 14.46207 \n#> [ 70 ] Loss: 3.185689 \n#> [ 80 ] Loss: 0.4264134 \n#> [ 90 ] Loss: 0.02732975 \n#> [ 100 ] Loss: 0.001300132 \n\n\nIf we look at \\(\\Delta \\theta\\) we will see a matrix full of 1s, the exact transformation\nthat we applied to the weights:\n\n\ndelta_theta <- torch_matmul(lora$A, lora$B)*lora$scaling\ndelta_theta[1:5, 1:5]\n#> torch_tensor\n#>  1.0002  1.0001  1.0001  1.0001  1.0001\n#>  1.0011  1.0010  1.0011  1.0011  1.0011\n#>  0.9999  0.9999  0.9999  0.9999  0.9999\n#>  1.0015  1.0014  1.0014  1.0014  1.0014\n#>  1.0008  1.0008  1.0008  1.0008  1.0008\n#> [ CPUFloatType{5,5} ][ grad_fn = <SliceBackward0> ]\n\n\nTo avoid the additional inference latency of the separate computation of the deltas,\nwe could modify the original model by adding the estimated deltas to its parameters.\nWe use the add_ method to modify the weight in-place.\n\n\nwith_no_grad({\n  model$weight$add_(delta_theta$t())  \n})\n\n\nNow, applying the base model to data from the new distribution yields good performance,\nso we can say the model is adapted for the new task.\n\n\nnnf_mse_loss(model(X2), y2)\n#> torch_tensor\n#> 0.00130013\n#> [ CPUFloatType{} ]\n\n\nConcluding\nNow that we learned how LoRA works for this simple example we can think how it could\nwork on large pre-trained models.\nTurns out that Transformers models are mostly clever organization of these matrix\nmultiplications, and applying LoRA only to these layers is enough for reducing the\nfine tuning cost by a large amount while still getting good performance. You can see\nthe experiments in the LoRA paper.\nOf course, the idea of LoRA is simple enough that it can be applied not only to\nlinear layers. You can apply it to convolutions, embedding layers and actually any other layer.\nImage by Hu et al on the LoRA paper\n\n\n\n",
    "preview": "posts/2023-06-22-understanding-lora/images/lora.png",
    "last_modified": "2024-11-21T15:51:02+00:00",
    "input_file": {},
    "preview_width": 592,
    "preview_height": 564
  },
  {
    "path": "posts/2023-06-20-gpt2-torch/",
    "title": "GPT-2 from scratch with torch",
    "description": "Implementing a language model from scratch is, arguably, the best way to develop an accurate idea of how its engine works. Here, we use torch to code GPT-2, the immediate successor to the original GPT. In the end, you'll dispose of an R-native model that can make direct use of Hugging Face's pre-trained GPT-2 model weights.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2023-06-20",
    "categories": [
      "Torch",
      "R",
      "Natural Language Processing"
    ],
    "contents": "\n\nContents\nSources, resources\nA minimal GPT-2\nEnd-to-end-usage, using pre-trained weights\n\nWhatever your take on Large Language Models (LLMs) – are they beneficial? dangerous? a short-lived fashion, like crypto? – they are here, now. And that means, it is a good thing to know (at a level one needs to decide for oneself) how they work. On this same day, I am publishing What are Large Language Models? What are they not?, intended for a more general audience. In this post, I’d like to address deep learning practitioners, walking through a torch implementation of GPT-2 (Radford et al. 2019), the second in OpenAI’s succession of ever-larger models trained on ever-more-vast text corpora. You’ll see that a complete model implementation fits in fewer than 250 lines of R code.\nSources, resources\nThe code I’m going to present is found in the minhub repository. This repository deserves a mention of its own. As emphasized in the README,\n\nminhub is a collection of minimal implementations of deep learning models, inspired by minGPT. All models are designed to be self-contained, single-file, and devoid of external dependencies, making them easy to copy and integrate into your own projects.\n\nEvidently, this makes them excellent learning material; but that is not all. Models also come with the option to load pre-trained weights from Hugging Face’s model hub. And if that weren’t enormously convenient already, you don’t have to worry about how to get tokenization right: Just download the matching tokenizer from Hugging Face, as well. I’ll show how this works in the final section of this post. As noted in the minhub README, these facilities are provided by packages hfhub and tok.\nAs realized in minhub, gpt2.R is, mostly, a port of Karpathy’s MinGPT. Hugging Face’s (more sophisticated) implementation has also been consulted. For a Python code walk-through, see https://amaarora.github.io/posts/2020-02-18-annotatedGPT2.html. This text also consolidates links to blog posts and learning materials on language modeling with deep learning that have become “classics” in the short time since they were written.\nA minimal GPT-2\nOverall architecture\nThe original Transformer (Vaswani et al. 2017) was built up of both an encoder and a decoder stack, a prototypical use case being machine translation. Subsequent developments, dependent on envisaged primary usage, tended to forego one of the stacks. The first GPT, which differs from GPT-2 only in relative subtleties, kept only the decoder stack. With “self-attention” wired into every decoder block, as well as an initial embedding step, this is not a problem – external input is not technically different from successive internal representations.\nHere is a screenshot from the initial GPT paper (Radford and Narasimhan 2018), visualizing the overall architecture. It is still valid for GPT-2. Token as well as position embedding are followed by a twelve-fold repetition of (identical in structure, though not sharing weights) transformer blocks, with a task-dependent linear layer constituting model output.\n\n\n\nIn gpt2.R, this global structure and what it does is defined in nn_gpt2_model(). (The code is more modularized – so don’t be confused if code and screenshot don’t perfectly match.)\nFirst, in initialize(), we have the definition of modules:\n\n\nself$transformer <- nn_module_dict(list(\n  wte = nn_embedding(vocab_size, n_embd),\n  wpe = nn_embedding(max_pos, n_embd),\n  drop = nn_dropout(pdrop),\n  h = nn_sequential(!!!map(\n    1:n_layer,\n    \\(x) nn_gpt2_transformer_block(n_embd, n_head, n_layer, max_pos, pdrop)\n  )),\n  ln_f = nn_layer_norm(n_embd, eps = 1e-5)\n))\n\nself$lm_head <- nn_linear(n_embd, vocab_size, bias = FALSE)\n\n\nThe two top-level components in this model are the transformer and lm_head, the output layer. This code-level distinction has an important semantic dimension, with two aspects standing out. First, and quite directly, transformer’s definition communicates, in a succinct way, what it is that constitutes a Transformer. What comes thereafter – lm_head, in our case – may vary. Second, and importantly, the distinction reflects the essential underlying idea, or essential operationalization, of natural language processing in deep learning. Learning consists of two steps, the first – and indispensable one – being to learn about language (this is what LLMs do), and the second, much less resource-consuming, one consisting of adaptation to a concrete task (such as question answering, or text summarization).\nTo see in what order (and how often) things happen, we look inside forward():\n\n\ntok_emb <- self$transformer$wte(x) \npos <- torch_arange(1, x$size(2))$to(dtype = \"long\")$unsqueeze(1) \npos_emb <- self$transformer$wpe(pos)\nx <- self$transformer$drop(tok_emb + pos_emb)\nx <- self$transformer$h(x)\nx <- self$transformer$ln_f(x)\nx <- self$lm_head(x)\nx\n\n\nAll modules in transformer are called, and thus executed, once; this includes h – but h itself is a sequential module made up of transformer blocks.\nSince these blocks are the core of the model, we’ll look at them next.\nTransformer block\nHere’s how, in nn_gpt2_transformer_block(), each of the twelve blocks is defined.\n\n\nself$ln_1 <- nn_layer_norm(n_embd, eps = 1e-5)\nself$attn <- nn_gpt2_attention(n_embd, n_head, n_layer, max_pos, pdrop)\nself$ln_2 <- nn_layer_norm(n_embd, eps = 1e-5)\nself$mlp <- nn_gpt2_mlp(n_embd, pdrop)\n\n\nOn this level of resolution, we see that self-attention is computed afresh at every stage, and that the other constitutive ingredient is a feed-forward neural network. In addition, there are two modules computing layer normalization, the type of normalization employed in transformer blocks. Different normalization algorithms tend to distinguish themselves from one another in what they average over; layer normalization (Ba, Kiros, and Hinton 2016) – surprisingly, maybe, to some readers – does so per batch item. That is, there is one mean, and one standard deviation, for each unit in a module. All other dimensions (in an image, that would be spatial dimensions as well as channels) constitute the input to that item-wise statistics computation.\nContinuing to zoom in, we will look at both the attention- and the feed-forward network shortly. Before, though, we need to see how these layers are called. Here is all that happens in forward():\n\n\nx <- x + self$attn(self$ln_1(x))\nx + self$mlp(self$ln_2(x))\n\n\nThese two lines deserve to be read attentively. As opposed to just calling each consecutive layer on the previous one’s output, this inserts skip (also termed residual) connections that, each, circumvent one of the parent module’s principal stages. The effect is that each sub-module does not replace, but just update what is passed in with its own view on things.\nTransformer block up close: Self-attention\nOf all modules in GPT-2, this is by far the most intimidating-looking. But the basic algorithm employed here is the same as what the classic “dot product attention paper” (Bahdanau, Cho, and Bengio 2014) proposed in 2014: Attention is conceptualized as similarity, and similarity is measured via the dot product. One thing that can be confusing is the “self” in self-attention. This term first appeared in the Transformer paper (Vaswani et al. 2017), which had an encoder as well as a decoder stack. There, “attention” referred to how the decoder blocks decided where to focus in the message received from the encoding stage, while “self-attention” was the term coined for this technique being applied inside the stacks themselves (i.e., between a stack’s internal blocks). With GPT-2, only the (now redundantly-named) self-attention remains.\nResuming from the above, there are two reasons why this might look complicated. For one, the “triplication” of tokens introduced, in Transformer, through the “query - key - value” frame1. And secondly, the additional batching introduced by having not just one, but several, parallel, independent attention-calculating processes per layer (“multi-head attention”). Walking through the code, I’ll point to both as they make their appearance.\nWe again start with module initialization. This is how nn_gpt2_attention() lists its components:\n\n\n# key, query, value projections for all heads, but in a batch\nself$c_attn <- nn_linear(n_embd, 3 * n_embd)\n# output projection\nself$c_proj <- nn_linear(n_embd, n_embd)\n\n# regularization\nself$attn_dropout <- nn_dropout(pdrop)\nself$resid_dropout <- nn_dropout(pdrop)\n\n# causal mask to ensure that attention is only applied to the left in the input sequence\nself$bias <- torch_ones(max_pos, max_pos)$\n  bool()$\n  tril()$\n  view(c(1, 1, max_pos, max_pos)) |>\n  nn_buffer()\n\n\nBesides two dropout layers, we see:\nA linear module that effectuates the above-mentioned triplication. Note how this is different from just having three identical versions of a token: Assuming all representations were initially mostly equivalent (through random initialization, for example), they will not remain so once we’ve begun to train the model.\nA module, called c_proj, that applies a final affine transformation. We will need to look at usage to see what this module is for.\nA buffer – a tensor that is part of a module’s state, but exempt from training – that makes sure that attention is not applied to previous-block output that “lies in the future.” Basically, this is achieved by masking out future tokens, making use of a lower-triangular matrix.\nAs to forward(), I am splitting it up into easy-to-digest pieces.\nAs we enter the method, the argument, x, is shaped just as expected, for a language model: batch dimension times sequence length times embedding dimension.\nx$shape\n[1]   1  24 768\nNext, two batching operations happen: (1) triplication into queries, keys, and values; and (2) making space such that attention can be computed for the desired number of attention heads all at once. I’ll explain how after listing the complete piece.\n\n\n# batch size, sequence length, embedding dimensionality (n_embd)\nc(b, t, c) %<-% x$shape\n\n# calculate query, key, values for all heads in batch and move head forward to be the batch dim\nc(q, k, v) %<-% ((self$c_attn(x)$\n  split(self$n_embd, dim = -1)) |>\n  map(\\(x) x$view(c(b, t, self$n_head, c / self$n_head))) |>\n  map(\\(x) x$transpose(2, 3)))\n\n\nFirst, the call to self$c_attn() yields query, key, and value vectors for each embedded input token. split() separates the resulting matrix into a list. Then map() takes care of the second batching operation. All of the three matrices are re-shaped, adding a fourth dimension. This fourth dimension takes care of the attention heads. Note how, as opposed to the multiplying process that triplicated the embeddings, this divides up what we have among the heads, leaving each of them to work with a subset inversely proportional to the number of heads used. Finally, map(\\(x) x$transpose(2, 3) mutually exchanges head and sequence-position dimensions.\nNext comes the computation of attention itself.\n\n\n# causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\natt <- q$matmul(k$transpose(-2, -1)) * (1 / sqrt(k$size(-1)))\natt <- att$masked_fill(self$bias[, , 1:t, 1:t] == 0, -Inf)\natt <- att$softmax(dim = -1)\natt <- self$attn_dropout(att)\n\n\nFirst, similarity between queries and keys is computed, matrix multiplication effectively being a batched dot product. (If you’re wondering about the final division term in line one, this scaling operation is one of the few aspects where GPT-2 differs from its predecessor. Check out the paper if you’re interested in the related considerations.) Next, the aforementioned mask is applied, resultant scores are normalized, and dropout regularization is used to encourage sparsity.\nFinally, the computed attention2 needs to be passed on to the ensuing layer. This is where the value vectors come in – those members of this trinity that we haven’t yet seen in action.\n\n\ny <- att$matmul(v) # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\ny <- y$transpose(2, 3)$contiguous()$view(c(b, t, c)) # re-assemble all head outputs side by side\n\n# output projection\ny <- self$resid_dropout(self$c_proj(y))\ny\n\n\nConcretely, what the matrix multiplication does here is weight the value vectors by the attention, and add them up. This happens for all attention heads at the same time, and really represents the outcome of the algorithm as a whole.\nRemaining steps then restore the original input size. This involves aligning the results for all heads one after the other, and then, applying the linear layer c_proj to make sure these results are not treated equally and/or independently, but combined in a useful way. Thus, the projection operation hinted at here really is a made up of a mechanical step (view()) and an “intelligent” one (transformation by c_proj()).\nTransformer block up close: Feed-forward network (MLP)\nCompared to the first, the attention module, there really is not much to say about the second core component of the transformer block (nn_gpt2_mlp()). It really is “just” an MLP – no “tricks” involved. Two things deserve pointing out, though.\nFirst, you may have heard about the MLP in a transformer block working “position-wise,” and wondered what is meant by this. Consider what happens in such a block:\n\n\nx <- x + self$attn(self$ln_1(x))\nx + self$mlp(self$ln_2(x))\n\n\nThe MLP receives its input (almost) directly from the attention module. But that, as we saw, was returning tensors of size [batch size, sequence length, embedding dimension]. Inside the MLP – cf. its forward() – the number of dimensions never changes:\n\n\nx |>\n  self$c_fc() |>       # nn_linear(n_embd, 4 * n_embd)\n  self$act() |>        # nn_gelu(approximate = \"tanh\")\n  self$c_proj() |>     # nn_linear(4 * n_embd, n_embd)\n  self$dropout()       # nn_dropout(pdrop)\n\n\nThus, these transformations are applied to all elements in the sequence, independently.\nSecond, since this is the only place where it appears, a note on the activation function employed. GeLU stands for “Gaussian Error Linear Units,” proposed in (Hendrycks and Gimpel 2020). The idea here is to combine ReLU-like activation effects with regularization/stochasticity. In theory, each intermediate computation would be weighted by its position in the (Gaussian) cumulative distribution function – effectively, by how much bigger (smaller) it is than the others. In practice, as you see from the module’s instantiation, an approximation is used.\nAnd that’s it for GPT-2’s main actor, the repeated transformer block. Remain two things: what happens before, and what happens thereafter.\nFrom words to codes: Token and position embeddings\nAdmittedly, if you tokenize the input dataset as required (using the matching tokenizer from Hugging Face – see below), you do not really end up with words. But still, the well-established fact holds: Some change of representation has to happen if the model is to successfully extract linguistic knowledge. Like many Transformer-based models, the GPT family encodes tokens in two ways. For one, as word embeddings. Looking back to nn_gpt2_model(), the top-level module we started this walk-through with, we see:\n\n\nwte = nn_embedding(vocab_size, n_embd)\n\n\nThis is useful already, but the representation space that results does not include information about semantic relations that may vary with position in the sequence – syntactic rules, for example, or phrase pragmatics. The second type of encoding remedies this. Referred to as “position embedding,” it appears in nn_gpt2_model() like so:\n\n\nwpe = nn_embedding(max_pos, n_embd)\n\n\nAnother embedding layer? Yes, though this one embeds not tokens, but a pre-specified number of valid positions (ranging from 1 to 1024, in GPT’s case). In other words, the network is supposed to learn what position in a sequence entails. This is an area where different models may vary vastly. The original Transformer employed a form of sinusoidal encoding; a more recent refinement is found in, e.g., GPT-NeoX (Su et al. 2021).\nOnce both encodings are available, they are straightforwardly added (see nn_gpt2_model()$forward()):\n\n\ntok_emb <- self$transformer$wte(x) \npos <- torch_arange(1, x$size(2))$to(dtype = \"long\")$unsqueeze(1) \npos_emb <- self$transformer$wpe(pos)\nx <- self$transformer$drop(tok_emb + pos_emb)\n\n\nThe resultant tensor is then passed to the chain of transformer blocks.\nOutput\nOnce the transformer blocks have been applied, the last mapping is taken care of by lm_head:\n\n\nx <- self$lm_head(x) # nn_linear(n_embd, vocab_size, bias = FALSE)\n\n\nThis is a linear transformation that maps internal representations back to discrete vocabulary indices, assigning a score to every index. That being the model’s final action, it is left to the sample generation process is to decide what to make of these scores. Or, put differently, that process is free to choose among different established techniques. We’ll see one – pretty standard – way in the next section.\nThis concludes model walk-through. I have left out a few details (such as weight initialization); consult gpt.R if you’re interested.\nEnd-to-end-usage, using pre-trained weights\nIt’s unlikely that many users will want to train GPT-2 from scratch. Let’s see, thus, how we can quickly set this up for sample generation.\nCreate model, load weights, get tokenizer\nThe Hugging Face model hub lets you access (and download) all required files (weights and tokenizer) directly from the GPT-2 page. All files are versioned; we use the most recent version.\n\n\n identifier <- \"gpt2\"\n revision <- \"e7da7f2\"\n # instantiate model and load Hugging Face weights\n model <- gpt2_from_pretrained(identifier, revision)\n # load matching tokenizer\n tok <- tok::tokenizer$from_pretrained(identifier)\n model$eval()\n\n\ntokenize\nDecoder-only transformer-type models don’t need a prompt. But usually, applications will want to pass input to the generation process. Thanks to tok, tokenizing that input couldn’t be more convenient:\n\n\nidx <- torch_tensor(\n  tok$encode(\n    paste(\n      \"No duty is imposed on the rich, rights of the poor is a hollow phrase...)\",\n      \"Enough languishing in custody. Equality\"\n    )\n  )$\n    ids\n)$\n  view(c(1, -1))\nidx\n\n\ntorch_tensor\nColumns 1 to 11  2949   7077    318  10893    319    262   5527     11   2489    286    262\n\nColumns 12 to 22  3595    318    257  20596   9546   2644  31779   2786   3929    287  10804\n\nColumns 23 to 24    13  31428\n[ CPULongType{1,24} ]\nGenerate samples\nSample generation is an iterative process, the model’s last prediction getting appended to the – growing – prompt.\n\n\nprompt_length <- idx$size(-1)\n\nfor (i in 1:30) { # decide on maximal length of output sequence\n  # obtain next prediction (raw score)\n  with_no_grad({\n    logits <- model(idx + 1L)\n  })\n  last_logits <- logits[, -1, ]\n  # pick highest scores (how many is up to you)\n  c(prob, ind) %<-% last_logits$topk(50)\n  last_logits <- torch_full_like(last_logits, -Inf)$scatter_(-1, ind, prob)\n  # convert to probabilities\n  probs <- nnf_softmax(last_logits, dim = -1)\n  # probabilistic sampling\n  id_next <- torch_multinomial(probs, num_samples = 1) - 1L\n  # stop if end of sequence predicted\n  if (id_next$item() == 0) {\n    break\n  }\n  # append prediction to prompt\n  idx <- torch_cat(list(idx, id_next), dim = 2)\n}\n\n\nTo see the output, just use tok$decode():\n\n\ntok$decode(as.integer(idx))\n\n\n[1] \"No duty is imposed on the rich, rights of the poor is a hollow phrase...\n     Enough languishing in custody. Equality is over\"\nTo experiment with text generation, just copy the self-contained file, and try different sampling-related parameters. (And prompts, of course!)\nAs always, thanks for reading!\nPhoto by Marjan\nBlan on Unsplash\n\n\n\nBa, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. “Layer Normalization.” https://arxiv.org/abs/1607.06450.\n\n\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural Machine Translation by Jointly Learning to Align and Translate.” CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473.\n\n\nHendrycks, Dan, and Kevin Gimpel. 2020. “Gaussian Error Linear Units (GELUs).” https://arxiv.org/abs/1606.08415.\n\n\nRadford, Alec, and Karthik Narasimhan. 2018. “Improving Language Understanding by Generative Pre-Training.” In.\n\n\nRadford, Alec, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. “Language Models Are Unsupervised Multitask Learners.” In.\n\n\nSu, Jianlin, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. 2021. “RoFormer: Enhanced Transformer with Rotary Position Embedding.” arXiv Preprint arXiv:2104.09864.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” https://arxiv.org/abs/1706.03762.\n\n\nIf this terminology is unfamiliar, you’ll find a nice (and very popular) introduction here.↩︎\nI am italicizing the word so as to hint at a special way of using the term. While the expression in itself does sound rather strange, attention is often employed to signify the state reached after normalizing the – usually seen as “raw” – scores.↩︎\n",
    "preview": "posts/2023-06-20-gpt2-torch/images/preview.jpg",
    "last_modified": "2024-11-21T15:53:01+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-06-20-llm-intro/",
    "title": "What are Large Language Models? What are they not?",
    "description": "This is a high-level, introductory article about Large Language Models (LLMs), the core technology that enables the much-en-vogue chatbots as well as other Natural Language Processing (NLP) applications. It is directed at a general audience, possibly with some technical and/or scientific background, but no knowledge is assumed of either deep learning or NLP. Having looked at major model ingredients, training workflow, and mechanics of output generation, we also talk about what these models are not.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2023-06-20",
    "categories": [
      "Meta",
      "Concepts",
      "Natural Language Processing"
    ],
    "contents": "\n\nContents\nLarge Language Models: What they are\nOverall architecture\nGPT-type models up close\nModel training\nMechanics of text generation\n\nLarge Language Models: What they are not\nA few facts\nIt’s us: Language learning, shared goals, and a shared world\nIt’s us, really: Anthropomorphism unleashed\n\n\n\n“At this writing, the only serious ELIZA scripts which exist are some which cause ELIZA to respond roughly as would certain psychotherapists (Rogerians). ELIZA performs best when its human correspondent is initially instructed to”talk” to it, via the typewriter of course, just as one would to a psychiatrist. This mode of conversation was chosen because the psychiatric interview is one of the few examples of categorized dyadic natural language communication in which one of the participating pair is free to assume the pose of knowing almost nothing of the real world. If, for example, one were to tell a psychiatrist “I went for a long boat ride” and he responded “Tell me about boats,” one would not assume that he knew nothing about boats, but that he had some purpose in so directing the subsequent conversation. It is important to note that this assumption is one made by the speaker. Whether it is realistic or not is an altogether separate question. In any case, it has a crucial psychological utility in that it serves the speaker to maintain his sense of being heard and understood. The speaker furher defends his impression (which even in real life may be illusory) by attributing to his conversational partner all sorts of background knowledge, insights and reasoning ability. But again, these are the speaker’s contribution to the conversation.”\nJoseph Weizenbaum, creator of ELIZA (Weizenbaum 1966).\n\nGPT, the ancestor all numbered GPTs, was released in June, 2018 – five years ago, as I write this. Five years: that’s a long time. It certainly is as measured on the time scale of deep learning, the thing that is, usually, behind when people talk of “AI.” One year later, GPT was followed by GPT-2; another year later, by GPT-3. At this point, public attention was still modest – as expected, really, for these kinds of technologies that require lots of specialist knowledge. (For GPT-2, what may have increased attention beyond the normal, a bit, was OpenAI ’s refusal to publish the complete training code and full model weights, supposedly due to the threat posed by the model’s capabilities – alternatively, as argued by others, as a marketing strategy, or yet alternatively, as a way to preserve one’s own competitive advantage just a tiny little bit longer.\nAs of 2023, with GPT-3.5 and GPT-4 having followed, everything looks different. (Almost) everyone seems to know GPT, at least when that acronym appears prefixed by a certain syllable. Depending on who you talk to, people don’t seem to stop talking about that fantastic [insert thing here] ChatGPT generated for them, about its enormous usefulness with respect to [insert goal here]… or about the flagrant mistakes it made, and the danger that legal regulation and political enforcement will never be able to catch up.\nWhat made the difference? Obviously, it’s ChatGPT, or put differently, the fact that now, there is a means for people to make active use of such a tool, employing it for whatever their personal needs or interests are1. In fact, I’d argue it’s more than that: ChatGPT is not some impersonal tool – it talks to you, picking up your clarifications, changes of topic, mood… It is someone rather than something, or at least that’s how it seems. I’ll come back to that point in It’s us, really: Anthropomorphism unleashed. Before, let’s take a look at the underlying technology.\nLarge Language Models: What they are\nHow is it even possible to build a machine that talks to you? One way is to have that machine listen a lot. And listen is what these machines do; they do it a lot. But listening alone would never be enough to attain results as impressive as those we see. Instead, LLMs practice some form of “maximally active listening”: Continuously, they try to predict the speaker’s next utterance. By “continuously,” I mean word-by-word: At each training step, the model is asked to produce the subsequent word in a text.\nMaybe in my last sentence, you noted the term “train.” As per common sense, “training” implies some form of supervision. It also implies some form of method. Since learning material is scraped from the internet, the true continuation is always known. The precondition for supervision is thus always fulfilled: A supervisor can just compare model prediction with what really follows in the text. Remains the question of method. That’s where we need to talk about deep learning, and we’ll do that in Model training.\nOverall architecture\nToday’s LLMs are, in some way or the other, based on an architecture known as the Transformer. This architecture was originally introduced in a paper catchily titled “Attention is all you need” (Vaswani et al. 2017). Of course, this was not the first attempt at automating natural-language generation – not even in deep learning, the sub-type of machine learning whose defining characteristic are many-layered (“deep”) artificial neural networks. But there, in deep learning, it constituted some kind of paradigm change. Before, models designed to solve sequence-prediction tasks (time-series forecasting, text generation…) tended to be based on some form of recurrent architecture, introduced in the 1990’s (eternities ago, on the time scale of deep-learning) by (Hochreiter and Schmidhuber 1997). Basically, the concept of recurrence, with its associated threading of a latent state, was replaced by “attention.” That’s what the paper’s title was meant to communicate: The authors did not introduce “attention”2; instead, they fundamentally expanded its usage so as to render recurrence superfluous.\nHow did that ancestral Transformer look? – One prototypical task in natural language processing is machine translation. In translation, be it done by a machine or by a human, there is an input (in one language) and an output (in another). That input, call it a code. Whoever wants to establish its counterpart in the target language first needs to decode it. Indeed, one of two top-level building blocks of the archetypal Transformer was a decoder, or rather, a stack of decoders applied in succession. At its end, out popped a phrase in the target language3. What, then, was the other high-level block? It was an encoder, something that takes text (or tokens, rather, i.e., something that has undergone tokenization) and converts it into a form the decoder can make sense of. (Obviously, there is no analogue to this in human translation.)\nFrom this two-stack architecture, subsequent developments tended to keep just one. The GPT family, together with many others, just kept the decoder stack. Now, doesn’t the decoder need some kind of input – if not to translate to a different language, then to reply to, as in the chatbot scenario? Turns out that no, it doesn’t – and that’s why you can also have the bot initiate the conversation. Unbeknownst to you, there will, in fact, be an input to the model – some kind of token signifying “end of input.” In that case, the model will draw on its training experience to generate a word likely to start out a phrase. That one word will then become the new input to continue from, and so forth. Summing up so far, then, GPT-like LLMs are Transformer Decoders.\nThe question is, how does such a stack of decoders succeed in fulfilling the task?\nGPT-type models up close\nIn opening the black box, we focus on its two interfaces – input and output – as well as on the internals, its core.\nInput\nFor simplicity, let me speak of words, not tokens. Now imagine a machine that is to work with – more even: “understand”4 – words. For a computer to process non-numeric data, a conversion to numbers necessarily has to happen. The straightforward way to effectuate this is to decide on a fixed lexicon, and assign each word a number. And this works: The way deep neural networks are trained, they don’t need semantic relationships to exist between entities in the training data to memorize formal structure. Does this mean they will appear perfect while training, but fail in real-world prediction? – If the training data are representative of how we converse, all will be fine. In a world of perfect surveillance, machines could exist that have internalized our every spoken word. Before that happens, though, the training data will be imperfect.\nA much more promising approach than to simply index words, then, is to represent them in a richer, higher-dimensional space, an embedding space. This idea, popular not just in deep learning but in natural language processing overall, really goes far beyond anything domain-specific – linguistic entities, say5. You may be able to fruitfully employ it in virtually any domain – provided you can devise a method to sensibly map the given data into that space. In deep learning, these embeddings are obtained in a clever way: as a by-product of sorts of the overall training workflow. Technically, this is achieved by means of a dedicated neural-network layer6 tasked with evolving these mappings. Note how, smart though this strategy may be, it implies that the overall setting – everything from training data via model architecture to optimization algorithms employed – necessarily affects the resulting embeddings. And since these may be extracted and made use of in down-stream tasks, this matters7.\nAs to the GPT family, such an embedding layer constitutes part of its input interface – one “half,” so to say. Technically, the second makes use of the same type of layer, but with a different purpose. To contrast the two, let me spell out clearly what, in the part we’ve talked about already, is getting mapped to what. The mapping is between a word index – a sequence 1, 2, …, <vocabulary size> – on the one hand and a set of continuous-valued vectors of some length – 100, say – on the other. (One of them could like this: \\(\\begin{bmatrix} 1.002 & 0.71 & 0.0004 &...\\\\ \\end{bmatrix}\\)) Thus, we obtain an embedding for every word. But language is more than an unordered assembly of words. Rearranging words, if syntactically allowed, may result in drastically changed semantics. In the pre-transformer paradigma, threading a sequentially-updated hidden state took care of this. Put differently, in that type of model, information about input order never got lost throughout the layers. Transformer-type architectures, however, need to find a different way. Here, a variety of rivaling methods exists. Some assume an underlying periodicity in semanto-syntactic structure. Others – and the GPT family, as yet and insofar we know, has been part of them8 – approach the challenge in exactly the same way as for the lexical units: They make learning these so-called position embeddings a by-product of model training. Implementation-wise, the only difference is that now the input to the mapping looks like this: 1, 2, …, <maximum position> where “maximum position” reflects choice of maximal sequence length supported.\nSumming up, verbal input is thus encoded – embedded, enriched – twofold as it enters the machine. The two types of embedding are combined and passed on to the model core, the already-mentioned decoder stack.\nCore Processing\nThe decoder stack is made up of some number of identical blocks (12, in the case of GPT-2). (By “identical” I mean that the architecture is the same; the weights – the place where a neural-network layer stores what it “knows” – are not. More on these “weights” soon.)\nInside each block, some sub-layers are pretty much “business as usual.” One is not: the attention module, the “magic” ingredient that enabled Transformer-based architectures to forego keeping a latent state. To explain how this works, let’s take translation as an example.\nIn the classical encoder-decoder setup, the one most intuitive for machine translation, imagine the very first decoder in the stack of decoders. It receives as input a length-seven cypher, the encoded version of an original length-seven phrase. Since, due to how the encoder blocks are built, input order is conserved, we have a faithful representation of source-language word order. In the target language, however, word order can be very different. A decoder module, in producing the translation, had rather not do this by translating each word as it appears. Instead, it would be desirable for it to know which among the already-seen tokens is most relevant right now, to generate the very next output token. Put differently, it had better know where to direct its attention.\nThus, figure out how to distribute focus is what attention modules do. How do they do it? They compute, for each available input-language token, how good a match, a fit, it is for their own current input. Remember that every token, at every processing stage, is encoded as a vector of continuous values. How good a match any of, say, three source-language vectors is is then computed by projecting one’s current input vector onto each of the three. The closer the vectors, the longer the projected vector.9 Based on the projection onto each source-input token, that token is weighted, and the attention module passes on the aggregated assessments to the ensuing neural-network module.\nTo explain what attention modules are for, I’ve made use of the machine-translation scenario, a scenario that should lend a certain intuitiveness to the operation. But for GPT-family models, we need to abstract this a bit. First, there is no encoder stack, so “attention” is computed among decoder-resident tokens only. And second – remember I said a stack was built up of identical modules? – this happens in every decoder block. That is, when intermediate results are bubbled up the stack, at each stage the input is weighted as appropriate at that stage. While this is harder to intuit than what happened in the translation scenario, I’d argue that in the abstract, it makes a lot of sense. For an analogy, consider some form of hierarchical categorization of entities. As higher-level categories are built from lower-level ones, at each stage the process needs to look at its input afresh, and decide on a sensible way of subsuming similar-in-some-way categories.\nOutput\nStack of decoders traversed, the multi-dimensional codes that pop out need to be converted into something that can be compared with the actual phrase continuation we see in the training corpus. Technically, this involves a projection operation as well a strategy for picking the output word – that word in target-language vocabulary that has the highest probability. How do you decide on a strategy? I’ll say more about that in the section Mechanics of text generation, where I assume a chatbot user’s perspective.\nModel training\nBefore we get there, just a quick word about model training. LLMs are deep neural networks, and as such, they are trained like any network is. First, assuming you have access to the so-called “ground truth,” you can always compare model prediction with the true target. You then quantify the difference – by which algorithm will affect training results. Then, you communicate that difference – the loss – to the network. It, in turn, goes through its modules, from back/top to start/bottom, and updates its stored “knowledge” – matrices of continuous numbers called weights. Since information is passed from layer to layer, in a direction reverse to that followed in computing predictions, this technique is known as back-propagation.\nAnd all that is not triggered once, but iteratively, for a certain number of so-called “epochs,” and modulated by a set of so-called “hyper-parameters.” In practice, a lot of experimentation goes into deciding on the best-working configuration of these settings.\nMechanics of text generation\nWe already know that during model training, predictions are generated word-by-word; at every step, the model’s knowledge about what has been said so far is augmented by one token: the word that really was following at that point. If, making use of a trained model, a bot is asked to reply to a question, its response must by necessity be generated in the same way. However, the actual “correct word” is not known. The only way, then, is to feed back to the model its own most recent prediction. (By necessity, this lends to text generation a very special character, where every decision the bot makes co-determines its future behavior.)\nWhy, though, talk about decisions? Doesn’t the bot just act on behalf of the core model, the LLM – thus passing on the final output? Not quite. At each prediction step, the model yields a vector, with values as many as there are entries in the vocabulary. As per model design and training rationale, these vectors are “scores” – ratings, sort of, how good a fit a word would be in this situation. Like in life, higher is better. But that doesn’t mean you’d just pick the word with the highest value. In any case, these scores are converted to probabilities, and a suitable probability distribution is used to non-deterministically pick a likely (or likely-ish) word. The probability distribution commonly used is the multinomial distribution, appropriate for discrete choice among more than two alternatives. But what about the conversion to probabilities? Here, there is room for experimentation.\nTechnically, the algorithm employed is known as the softmax function. It is a simplified version of the Boltzmann distribution, famous in statistical mechanics, used to obtain the probability of a system’s state given that state’s energy and the temperature of the system. But for temperature10, both formulae are, in fact, identical. In physical systems, temperature modulates probabilities in the following way: The hotter the system, the closer the states’ probabilities are to each other; the colder it gets, the more distinct those probabilities. In the extreme, at very low temperatures there will be a few clear “winners” and a silent majority of “losers.”\nIn deep learning, a like effect is easy to achieve (by means of a scaling factor). That’s why you may have heard people talk about some weird thing called “temperature” that resulted in [insert adjective here] answers. If the application you use lets you vary that factor, you’ll see that a low temperature will result in deterministic-looking, repetitive, “boring” continuations, while a high one may make the machine appear as though it were on drugs.\nThat concludes our high-level overview of LLMs. Having seen the machine dissected in this way may already have left you with some sort of opinion of what these models are – not. This topic more than deserves a dedicated exposition – and papers are being written pointing to important aspects all the time – but in this text, I’d like to at least offer some input for thought.\nLarge Language Models: What they are not\nIn part one,describing LLMs technically, I’ve sometimes felt tempted to use terms like “understanding” or “knowledge” when applied to the machine. I may have ended up using them; in that case, I’ve tried to remember to always surround them with quotes. The latter, the adding quotes, stands in contrast to many texts, even ones published in an academic context (Bender and Koller 2020). The question is, though: Why did I even feel compelled to use these terms, given I do not think they apply, in their usual meaning? I can think of a simple – shockingly simple, maybe – answer: It’s because us, humans, we think, talk, share our thoughts in these terms. When I say understand, I surmise you will know what I mean.\nNow, why do I think that these machines do not understand human language, in the sense we usually imply when using that word?\nA few facts\nI’ll start out briefly mentioning empirical results, conclusive thought experiments, and theoretical considerations. All aspects touched upon (and many more) are more than worthy of in-depth discussion, but such discussion is clearly out of scope for this synoptic-in-character text.\nFirst, while it is hard to put a number on the quality of a chatbot’s answers, performance on standardized benchmarks is the “bread and butter” of machine learning – its reporting being an essential part of the prototypical deep-learning publication. (You could even call it the “cookie,” the driving incentive, since models usually are explicitly trained and fine-tuned for good results on these benchmarks.) And such benchmarks exist for most of the down-stream tasks the LLMs are used for: machine translation, generating summaries, text classification, and even rather ambitious-sounding setups associated with – quote/unquote – reasoning.\nHow do you assess such a capability? Here is an example from a benchmark named “Argument Reasoning Comprehension Task” (Habernal et al. 2018).\nClaim: Google is not a harmful monopoly\nReason: People can choose not to use Google\nWarrant: Other search engines don’t redirect to Google\nAlternative: All other search engines redirect to Google\nHere claim and reason together make up the argument. But what, exactly, is it that links them? At first look, this can even be confusing to a human. The missing link is what is called warrant here – add it in, and it all starts to make sense. The task, then, is to decide which of warrant or alternative supports the conclusion, and which one does not.\nIf you think about it, this is a surprisingly challenging task. Specifically, it seems to inescapingly require world knowledge. So if language models, as has been claimed, perform nearly as well as humans, it seems they must have such knowledge – no quotes added. However, in response to such claims, research has been performed to uncover the hidden mechanism that enables such seemingly-superior results. For that benchmark, it has been found (Niven and Kao 2019) that there were spurious statistical cues in the way the dataset was constructed – those removed, LLM performance was no better than random.\nWorld knowledge, in fact, is one of the main things an LLM lacks. Bender et al. (Bender and Koller 2020) convincingly demonstrate its essentiality by means of two thought experiments. One of them, situated on a lone island, imagines an octopus11 inserting itself into some cable-mediated human communication, learning the chit-chat, and finally – having gotten bored – impersonating one of the humans. This works fine, until one day, its communication partner finds themselves in an emergency, and needs to build some rescue tool out of things given in the environment. They urgently ask for advice – and the octopus has no idea what to respond. It has no ideas what these words actually refer to.\nThe other argument comes directly from machine learning, and strikingly simple though it may be, it makes its point very well. Imagine an LLM trained as usual, including on lots of text involving plants. It has also been trained on a dataset of unlabeled photos, the actual task being unsubstantial – say it had to fill out masked areas. Now, we pull out a picture and ask: How many of that blackberry’s blossoms have already opened? The model has no chance to answer the question.\nNow, please look back at the Joseph Weizenbaum quote I opened this article with. It is still true that language-generating machine have no knowledge of the world we live in.\nBefore moving on, I’d like to just quickly hint at a totally different type of consideration, brought up in a (2003!) paper by Spärck Jones (Spaerck 2004). Though written long before LLMs, and long before deep learning started its winning conquest, on an abstract level it is still very applicable to today’s situation. Today, LLMs are employed to “learn language,” i.e., for language acquisition. That skill is then built upon by specialized models, of task-dependent architecture. Popular real-world12 down-stream tasks are translation, document retrieval, or text summarization. When the paper was written, there was no such two-stage pipeline. The author was questioning the fit between how language modeling was conceptualized – namely, as a form of recovery – and the character of these down-stream tasks. Was recovery – inferring a missing, for whatever reasons – piece of text a good model, of, say, condensing a long, detailed piece of text into a short, concise, factual one? If not, could the reason it still seemed to work just fine be of a very different nature – a technical, operational, coincidental one?\n\n[…] the crucial characterisation of the relationship between the input and the output is in fact offloaded in the LM approach onto the choice of training data. We can use LM for summarising because we know that some set of training data consists of full texts paired with their summaries.13\n\nIt seems to me that today’s two-stage process notwithstanding, this is still an aspect worth giving some thought.\nIt’s us: Language learning, shared goals, and a shared world\nWe’ve already talked about world knowledge. What else are LLMs missing out on?\nIn our world, you’ll hardly find anything that does not involve other people. This goes a lot deeper than the easily observable facts: our constantly communicating, reading and typing messages, documenting our lives on social networks… We don’t experience, explore, explain a world of our own. Instead, all these activities are inter-subjectively constructed. Feelings are14. Cognition is; meaning is. And it goes deeper yet. Implicit assumptions guide us to constantly look for meaning, be it in overheard fragments, mysterious symbols, or life events.\nHow does this relate to LLMs? For one, they’re islands of their own. When you ask them for advice – to develop a research hypothesis and a matching operationalization, say, or whether a detainee should be released on parole – they have no stakes in the outcome, no motivation (be it intrinsic or extrinsic), no goals. If an innocent person is harmed, they don’t feel the remorse; if an experiment is successful but lacks explanatory power, they don’t sense the shallowness; if the world blows up, it won’t have been their world.\nSecondly, it’s us who are not islands. In Bender et al.’s octopus scenario, the human on one side of the cable plays an active role not just when they speak. In making sense of what the octopus says, they contribute an essential ingredient: namely, what they think the octopus wants, thinks, feels, expects… Anticipating, they reflect on what the octopus anticipates.\nAs Bender et al. put it:\n\nIt is not that O’s utterances make sense, but rather, that A can make sense of them.\n\nThat article (Bender and Koller 2020) also brings impressive evidence from human language acquisition: Our predisposition towards language learning notwithstanding, infants don’t learn from the availability of input alone. A situation of joint attention is needed for them to learn. Psychologizing, one could hypothesize they need to get the impression that these sounds, these words, and the fact they’re linked together, actually matters.\nLet me conclude, then, with my final “psychologization.”\nIt’s us, really: Anthropomorphism unleashed\nYes, it is amazing what these machines do. (And that makes them incredibly dangerous power instruments.) But this in no way affects the human-machine differences that have been existing throughout history, and continue to exist today. That we are inclined to think they understand, know, mean – that maybe even they’re conscious: that’s on us. We can experience deep emotions watching a movie; hope that if we just try enough, we can sense what a distant-in-evolutionary-genealogy creature is feeling; see a cloud encouragingly smiling at us; read a sign in an arrangement of pebbles.\nOur inclination to anthropomorphize is a gift; but it can sometimes be harmful. And nothing of this is special to the twenty-first century.\nLike I began with him, let me conclude with Weizenbaum.\n\nSome subjects have been very hard to convince that ELIZA (with its present script) is not human.\n\nPhoto by Marjan\nBlan on Unsplash\n\n\n\nBender, Emily M., and Alexander Koller. 2020. “Climbing Towards NLU: On Meaning, Form, and Understanding in the Age of Data.” In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 5185–98. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.acl-main.463.\n\n\nCaliskan, Aylin, Pimparkar Parth Ajay, Tessa Charlesworth, Robert Wolfe, and Mahzarin R. Banaji. 2022. “Gender Bias in Word Embeddings.” In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society. ACM. https://doi.org/10.1145/3514094.3534162.\n\n\nHabernal, Ivan, Henning Wachsmuth, Iryna Gurevych, and Benno Stein. 2018. “The Argument Reasoning Comprehension Task: Identification and Reconstruction of Implicit Warrants.” In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), 1930–40. New Orleans, Louisiana: Association for Computational Linguistics. https://doi.org/10.18653/v1/N18-1175.\n\n\nHochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-Term Memory.” Neural Computation 9 (December): 1735–80. https://doi.org/10.1162/neco.1997.9.8.1735.\n\n\nNiven, Timothy, and Hung-Yu Kao. 2019. “Probing Neural Network Comprehension of Natural Language Arguments.” CoRR abs/1907.07355. http://arxiv.org/abs/1907.07355.\n\n\nSpaerck, Karen. 2004. “Language Modelling’s Generative Model : Is It Rational?” In.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” https://arxiv.org/abs/1706.03762.\n\n\nWeizenbaum, Joseph. 1966. “ELIZA - a Computer Program for the Study of Natural Language Communication Between Man and Machine.” Commun. ACM 9 (1): 36–45. https://doi.org/10.1145/365153.365168.\n\n\nEvidently, this is not about singling out ChatGPT as opposed to other chatbots; rather, I’m adopting it as the prototypical such application, since it is the one omnipresent in the media these days.↩︎\nI’m using quotes to refer to how attention is operationalized in deep learning, as opposed to how it is conceptualized in cognitive science or psychology.↩︎\nIf you’re wondering how that is possible – shouldn’t there be a separate, top-level module for generation? – no, there need not be. That’s because training implies prediction.↩︎\nWhy the quotes? See Large Language Models: What they are not.↩︎\nAs a fascinating example from dynamical systems theory, take delay coordinate embeddings.↩︎\nSuitably named embedding layer.↩︎\nSee, for example, (Caliskan et al. 2022).↩︎\nFor GPT-4, even high-level model information has not been released.↩︎\nMathematically, this is achieved by a pretty standard and pervasively-used, in machine learning, operation, the dot product.↩︎\n… and the Boltzmann constant – but that being a constant, we don’t consider it here.↩︎\nThat choice of species is probably not a coincidence: see https://en.wikipedia.org/wiki/Cephalopod_intelligence.↩︎\nAs opposed to the aforementioned problems subsumed under “reasoning,” those having been constructed for research purposes.↩︎\nFrom (Spaerck 2004).↩︎\nSee https://lisafeldmanbarrett.com/books/how-emotions-are-made/.↩︎\n",
    "preview": "posts/2023-06-20-llm-intro/images/preview.jpg",
    "last_modified": "2024-11-21T15:52:50+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-06-15-safetensors/",
    "title": "safetensors 0.1.0",
    "description": "Announcing safetensors, a new R package allowing for reading and writing files in the safetensors format.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": {}
      }
    ],
    "date": "2023-06-15",
    "categories": [
      "Packages/Releases",
      "R"
    ],
    "contents": "\n\nContents\nMotivation\nFormat\nBasic usage\nFuture directions\n\nsafetensors is a new, simple, fast, and safe file format for storing tensors. The design of the file format and its original implementation are being led\nby Hugging Face, and it’s getting largely adopted in their popular ‘transformers’ framework. The safetensors R package is a pure-R implementation, allowing to both read and write safetensor files.\nThe initial version (0.1.0) of safetensors is now on CRAN.\nMotivation\nThe main motivation for safetensors in the Python community is security. As noted\nin the official documentation:\n\nThe main rationale for this crate is to remove the need to use pickle on PyTorch which is used by default.\n\nPickle is considered an unsafe format, as the action of loading a Pickle file can\ntrigger the execution of arbitrary code. This has never been a concern for torch\nfor R users, since the Pickle parser that is included in LibTorch only supports a subset\nof the Pickle format, which doesn’t include executing code.\nHowever, the file format has additional advantages over other commonly used formats, including:\nSupport for lazy loading: You can choose to read a subset of the tensors stored in the file.\nZero copy: Reading the file does not require more memory than the file itself.\n(Technically the current R implementation does makes a single copy, but that can\nbe optimized out if we really need it at some point).\nSimple: Implementing the file format is simple, and doesn’t require complex dependencies.\nThis means that it’s a good format for exchanging tensors between ML frameworks and\nbetween different programming languages. For instance, you can write a safetensors file\nin R and load it in Python, and vice-versa.\nThere are additional advantages compared to other file formats common in this space, and\nyou can see a comparison table here.\nFormat\nThe safetensors format is described in the figure below. It’s basically a header file\ncontaining some metadata, followed by raw tensor buffers.\nDiagram describing the safetensors file format.Basic usage\nsafetensors can be installed from CRAN using:\ninstall.packages(\"safetensors\")\nWe can then write any named list of torch tensors:\nlibrary(torch)\nlibrary(safetensors)\n\ntensors <- list(\n  x = torch_randn(10, 10),\n  y = torch_ones(10, 10)\n)\n\nstr(tensors)\n#> List of 2\n#>  $ x:Float [1:10, 1:10]\n#>  $ y:Float [1:10, 1:10]\n\ntmp <- tempfile()\nsafe_save_file(tensors, tmp)\nIt’s possible to pass additional metadata to the saved file by providing a metadata\nparameter containing a named list.\nReading safetensors files is handled by safe_load_file, and it returns the named\nlist of tensors along with the metadata attribute containing the parsed file header.\ntensors <- safe_load_file(tmp)\nstr(tensors)\n#> List of 2\n#>  $ x:Float [1:10, 1:10]\n#>  $ y:Float [1:10, 1:10]\n#>  - attr(*, \"metadata\")=List of 2\n#>   ..$ x:List of 3\n#>   .. ..$ shape       : int [1:2] 10 10\n#>   .. ..$ dtype       : chr \"F32\"\n#>   .. ..$ data_offsets: int [1:2] 0 400\n#>   ..$ y:List of 3\n#>   .. ..$ shape       : int [1:2] 10 10\n#>   .. ..$ dtype       : chr \"F32\"\n#>   .. ..$ data_offsets: int [1:2] 400 800\n#>  - attr(*, \"max_offset\")= int 929\nCurrently, safetensors only supports writing torch tensors, but we plan to add\nsupport for writing plain R arrays and tensorflow tensors in the future.\nFuture directions\nThe next version of torch will use safetensors as its serialization format,\nmeaning that when calling torch_save() on a model, list of tensors, or other\ntypes of objects supported by torch_save, you will get a valid safetensors file.\nThis is an improvement over the previous implementation because:\nIt’s much faster. More than 10x for medium sized models. Could be even more for large files.\nThis also improves the performance of parallel dataloaders by ~30%.\nIt enhances cross-language and cross-framework compatibility. You can train your model\nin R and use it in Python (and vice-versa), or train your model in tensorflow and run it\nwith torch.\nIf you want to try it out, you can install the development version of torch with:\nremotes::install_github(\"mlverse/torch\")\nPhoto by Nick Fewings on Unsplash\n\n\n\n",
    "preview": "posts/2023-06-15-safetensors/images/nick-fewings-4pZu15OeTXA-unsplash.jpg",
    "last_modified": "2024-11-21T15:52:45+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-06-07-torch-0-11/",
    "title": "torch 0.11.0",
    "description": "torch v0.11.0 is now on CRAN. This release features much-enhanced support for executing JIT operations. We also amended loading of model parameters, and added a few quality-of-life improvements, like support for temporarily modifying the default torch device, support for specifying data types as strings, and many more.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": {}
      }
    ],
    "date": "2023-06-07",
    "categories": [
      "Torch",
      "Packages/Releases",
      "R"
    ],
    "contents": "\n\nContents\nImproved loading of state dicts\nUsing JIT operations\nOther small improvements\nFinal remarks\n\ntorch v0.11.0 is now on CRAN! This blog post highlights some of the changes included\nin this release. But you can always find the full changelog\non the torch website.\nImproved loading of state dicts\nFor a long time it has been possible to use torch from R to load state dicts (i.e. \nmodel weights) trained with PyTorch using the load_state_dict() function.\nHowever, it was common to get the error:\nError in cpp_load_state_dict(path) :  isGenericDict() INTERNAL ASSERT FAILED at\nThis happened because when saving the state_dict from Python, it wasn’t really\na dictionary, but an ordered dictionary. Weights in PyTorch are serialized as Pickle files – a Python-specific format similar to our RDS. To load them in C++, without a Python runtime,\nLibTorch implements a pickle reader that’s able to read only a subset of the\nfile format, and this subset didn’t include ordered dicts.\nThis release adds support for reading the ordered dictionaries, so you won’t see\nthis error any longer.\nBesides that, reading theses files requires half of the peak memory usage, and in\nconsequence also is much faster. Here are the timings for reading a 3B parameter\nmodel (StableLM-3B) with v0.10.0:\n\n\nsystem.time({\n  x <- torch::load_state_dict(\"~/Downloads/pytorch_model-00001-of-00002.bin\")\n  y <- torch::load_state_dict(\"~/Downloads/pytorch_model-00002-of-00002.bin\")\n})\n\n\n   user  system elapsed \n662.300  26.859 713.484 \nand with v0.11.0\n   user  system elapsed \n  0.022   3.016   4.016 \nMeaning that we went from minutes to just a few seconds.\nUsing JIT operations\nOne of the most common ways of extending LibTorch/PyTorch is by implementing JIT\noperations. This allows developers to write custom, optimized code in C++ and\nuse it directly in PyTorch, with full support for JIT tracing and scripting.\nSee our ‘Torch outside the box’\nblog post if you want to learn more about it.\nUsing JIT operators in R used to require package developers to implement C++/Rcpp\nfor each operator if they wanted to be able to call them from R directly.\nThis release added support for calling JIT operators without requiring authors to\nimplement the wrappers.\nThe only visible change is that we now have a new symbol in the torch namespace, called\njit_ops. Let’s load torchvisionlib, a torch extension that registers many different\nJIT operations. Just loading the package with library(torchvisionlib) will make\nits operators available for torch to use - this is because the mechanism that registers\nthe operators acts when the package DLL (or shared library) is loaded.\nFor instance, let’s use the read_file operator that efficiently reads a file\ninto a raw (bytes) torch tensor.\n\n\nlibrary(torchvisionlib)\ntorch::jit_ops$image$read_file(\"img.png\")\n\n\ntorch_tensor\n 137\n  80\n  78\n  71\n ...\n   0\n   0\n 103\n... [the output was truncated (use n=-1 to disable)]\n[ CPUByteType{325862} ]\nWe’ve made it so autocomplete works nicely, such that you can interactively explore the available\noperators using jit_ops$ and pressing  to trigger RStudio’s autocomplete.\nOther small improvements\nThis release also adds many small improvements that make torch more intuitive:\nYou can now specify the tensor dtype using a string, eg: torch_randn(3, dtype = \"float64\"). (Previously you had to specify the dtype using a torch function, such as torch_float64()).\n\n\ntorch_randn(3, dtype = \"float64\")\n\n\ntorch_tensor\n-1.0919\n 1.3140\n 1.3559\n[ CPUDoubleType{3} ]\nYou can now use with_device() and local_device() to temporarily modify the device\non which tensors are created. Before, you had to use device in each tensor\ncreation function call. This allows for initializing a module on a specific device:\n\n\nwith_device(device=\"mps\", {\n  linear <- nn_linear(10, 1)\n})\nlinear$weight$device\n\n\ntorch_device(type='mps', index=0)\nIt’s now possible to temporarily modify the torch seed, which makes creating\nreproducible programs easier.\n\n\nwith_torch_manual_seed(seed = 1, {\n  torch_randn(1)\n})\n\n\ntorch_tensor\n 0.6614\n[ CPUFloatType{1} ]\nFinal remarks\nThank you to all contributors to the torch ecosystem. This work would not be possible without\nall the helpful issues opened, PRs you created, and your hard work.\nIf you are new to torch and want to learn more, we highly recommend the recently announced book ‘Deep Learning and Scientific Computing with R torch’.\nIf you want to start contributing to torch, feel free to reach out on GitHub and see our contributing guide.\nThe full changelog for this release can be found here.\nPhoto by Ian Schneider on Unsplash\n\n\n\n",
    "preview": "posts/2023-06-07-torch-0-11/images/ian-schneider-PAykYb-8Er8-unsplash.jpg",
    "last_modified": "2024-11-21T15:50:47+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-05-25-llama-tensorflow-keras/",
    "title": "LLaMA in R with Keras and TensorFlow",
    "description": "Implementation and walk-through of LLaMA, a Large Language Model, in R, with TensorFlow and Keras.",
    "author": [
      {
        "name": "Tomasz Kalinowski",
        "url": {}
      }
    ],
    "date": "2023-05-25",
    "categories": [
      "TensorFlow/Keras",
      "R",
      "Generative Models",
      "Natural Language Processing"
    ],
    "contents": "\n\nContents\nSetup\nTokenizer\nTransformerBlock\nRMSNorm\nFeedForward\nAttention\nRotary Position Embedding\nTying it all together\nWrapping up\n\nOpenAI’s chatGPT has awakened a collective awareness of what Large\nLanguage Models (LLMs) are capable of. With that awakening comes a daily\nmarch of LLM news: new products, new features, new models, new\ncapabilities, (and new worries). It seems we’re in the early stages of a\nCambrian explosion of LLMs and LLM powered tools; it’s not yet clear how\nLLMs will impact and influence our professional and personal lives, but\nit seems clear that they will, in some way.\nSince LLMs are here to stay, it’s worthwhile to take some time to\nunderstand how these models work from a first-principles perspective.\nStarting with the mechanics can help foster durable intuitions that will\ninform our usage of these models now and in the future. (Especially if\nthe future is one where LLMs are a staple of the data scientist’s\ntoolbox, as common as an lm() function call).\nAnd what better way is there to learn than by doing. So with that\npreamble, in this post we’ll walk through an implementation of an LLM,\nLLaMA (Touvron et al. 2023)\nspecifically, in TensorFlow and Keras, with the goal being to develop\nunderstanding first, capability second.\nWhy LLaMA? With the sheer volume of LLM related content and news out\nthere, it can seem daunting to know where to get started. Almost weekly\nit seems there is a new model announced. Browsing some hubs of LLM\nactivity (HuggingFace,\nTFHub,\nreddit,\nHackerNews) muddies the waters even\nmore. How to pick a specific model?\nOf the many LLM-related news items in the past months, one that stands\nhead-and-shoulders above the crowd is the release of\nLLaMA,\na modern, foundational LLM made available to the public by Meta AI in\nFebruary 2023. On common benchmarks, LLaMA outperforms OpenAI’s GPT-3,\nwhile being substantially smaller (though still large).\nLLaMA is a great starting place because it is a simple and modern\narchitecture, has excellent performance on benchmarks, and is open. The\nmodel architecture has had just a few new ideas incorporated into it since\nthe original Transformer architecture first described in,\n“Attention Is All You Need”\npublished from Google (Vaswani et al. 2017). Four different sizes of\nLLaMA have been released: 7 billion and 13 billion parameter models\ntrained on 1 Trillion tokens, and 33 billion and 65 billion parameter\nmodels trained on 1.4 trillion tokens. This is an enormous amount of\ntraining data these models have seen–the largest 65B model has been\ntrained on approximately the “Chinchilla\ncompute-optimum” (Hoffmann et al. 2022)\nnumber of tokens, while the smaller LLaMAs are substantially\nbeyond that optimum. In this blog post we’ll focus on the smallest, 7B\nparameter LLaMA model, which you can comfortably load locally and run on\nCPU with only 64Gb of RAM.\nWhile not strictly necessary, to follow along locally, you’ll probably\nwant to acquire the pre-trained LLaMA weights one\nway or\nanother. Note, the\nweights do come with their own license, which you can preview\nhere.\nSo, without further ado, let’s get started.\nSetup\nFirst, we’ll want to install the required R and Python packages, and\nconfigure a virtual environment:\nremotes::install_github(c(\"rstudio/reticulate\",\n                          \"rstudio/tensorflow\",\n                          \"rstudio/keras\"))\n# reticulate::install_python(\"3.10:latest\")                          \nreticulate::virtualenv_create(\"./.venv\", version = \"3.10:latest\")\ntensorflow::install_tensorflow(envname = \"./.venv\", version = \"release\",\n                               extra_packages = \"tensorflow-text\")\nWith that out of the way, let’s load some packages and prepare our R\nsession:\nlibrary(purrr)\nlibrary(envir)\n\nlibrary(tensorflow)\nlibrary(tfautograph)\nlibrary(keras)\n\nuse_virtualenv(\"./.venv\")\noptions(tensorflow.extract.warn_tensors_passed_asis = FALSE)\n\nattach_eval({\n  import_from(glue, glue)\n  import_from(jsonlite, read_json)\n  import_from(withr, with_dir, with_options)\n  import_from(keras$layers, Dense)\n  np <- reticulate::import(\"numpy\", convert = FALSE)\n\n  seq_len0 <- function(x) seq.int(from = 0L, length.out = x)\n})\nIf you’ve acquired the pre-trained weights, it’ll be convenient to\nconvert them from the torch checkpoint format to something that’s more\nframework agnostic (you only need to do this once, of course):\n# reticulate::py_install(\"torch\", pip = TRUE)\ntorch <- reticulate::import(\"torch\", convert = FALSE)\nwith_dir(\"~/github/facebookresearch/llama/weights/LLaMA/7B\", {\n  pretrained_weights <- torch$load(\"consolidated.00.pth\",\n                                   map_location = \"cpu\")\n  for (name in names(pretrained_weights)) {\n    filename <- sprintf(\"%s.npy\", name)\n    array <- pretrained_weights[[name]]$numpy()\n    np$save(filename, array)\n    message(glue(\n      \"wrote: '{basename(filename)}' with shape: {array$shape}\"))\n  }\n})\nWe’ll also define a helper function so we can avoid having to retype the\nfull path to our weights:\nweights_path <- function(filename) normalizePath(file.path(\n  \"~/github/facebookresearch/llama/weights/LLaMA/\",\n  glue(filename, .envir = parent.frame())), mustWork = TRUE)\nAnd load the model configuration parameters specific to the 7B LLaMA,\nwhich we’ll use to build the model.\nparams <- read_json(weights_path(\"7B/params.json\"))\nstr(params)\nList of 6\n $ dim        : int 4096\n $ multiple_of: int 256\n $ n_heads    : int 32\n $ n_layers   : int 32\n $ norm_eps   : num 1e-06\n $ vocab_size : int -1\nTokenizer\nThe first component to LLaMA is the tokenizer, which converts text to a\nsequence of integers. The LLaMA model uses the\nSentencePiece tokenizer from\nGoogle. SentencePiece is available as a TensorFlow graph operation\nthrough\ntf_text.SentencepieceTokenizer,\nand also as a Keras layer in\nkeras_nlp.tokenizers.SentencepieceTokenizer.\nBy choice of a coin flip, we’ll use the lower-level tf_text interface.\ntf_text <- reticulate::import(\"tensorflow_text\")\ntokenizer_path <- weights_path(\"tokenizer.model\")\ntokenizer <- tf_text$SentencepieceTokenizer(\n  tf$io$gfile$GFile(tokenizer_path, \"rb\")$read(),\n  add_bos = TRUE, add_eos = FALSE,\n)\nLet’s test it out with a prompt:\nprompt <- \"The best way to attract bees\"\ntokenizer$tokenize(prompt)\ntf.Tensor([    1   450  1900   982   304 13978   367   267], shape=(8), dtype=int32)\nprompt |> tokenizer$tokenize() |> tokenizer$detokenize()\ntf.Tensor(b'The best way to attract bees', shape=(), dtype=string)\nLet’s define a show_tokens() helper function and play with the\ntokenizer a little.\nshow_tokens <- function(what) {\n  if(is.character(what))\n    token_ids <- what |> tokenizer$tokenize() |> as.integer()\n  else\n    token_ids <- as.integer(what)\n  tokens <- token_ids |>\n    map_chr(function(id) {\n      id |>\n        as_tensor(shape = c(1)) |>\n        tokenizer$detokenize() |>\n        as.character()\n    })\n\n  names(tokens) <- token_ids\n  tokens\n}\n\nshow_tokens(prompt)\n        1       450      1900       982       304     13978       367       267\n       \"\"     \"The\"    \"best\"     \"way\"      \"to\" \"attract\"      \"be\"      \"es\"\nNote that “bees” is two tokens. Not every token corresponds to a word.\nFor example, one non-word token we can reliably expect to show up in a\ntokenizer trained on a corpus of English text is “ing.” However, when the\n“ing” token shows up will not always follow your intuitions, because\ncommon words get their own token id, even if they can be decomposed into\nmultiple tokens.\nshow_tokens(\"ing\")\n    1  2348\n   \"\" \"ing\"\nshow_tokens(\"working\")\n        1      1985\n       \"\" \"working\"\nshow_tokens(\"flexing\")\n     1   8525    292\n    \"\" \"flex\"  \"ing\"\nshow_tokens(\"wonking\")\n     1   2113   9292\n    \"\"  \"won\" \"king\"\nAnother thing to note about the tokenizer is that each token sequence\nstarts with token id 1. This is a special beginning-of-sequence\ntoken that we requested be added when we loaded the tokenizer with\nadd_bos = TRUE. There are two other such special tokens that we will\nencounter later: an end-of-sequence special tokens with id 2, and an\nunknown-token with id 0.\nas.character(tokenizer$id_to_string(0L))\n[1] \"<unk>\"\nas.character(tokenizer$id_to_string(1L))\n[1] \"<s>\"\nas.character(tokenizer$id_to_string(2L))\n[1] \"<\/s>\"\nshow_tokens(c(1, 0, 2))\n    1     0     2\n   \"\" \" ⁇ \"    \"\"\nOverall, there are 32,000 tokens.\nas.integer(tokenizer$vocab_size())\n[1] 32000\nOne last observation is that the more frequently encountered tokens are\nassigned lower ids.\nshow_tokens(seq(50, len = 10))\n 50  51  52  53  54  55  56  57  58  59\n\"/\" \"0\" \"1\" \"2\" \"3\" \"4\" \"5\" \"6\" \"7\" \"8\"\nshow_tokens(seq(100, len = 10))\n100 101 102 103 104 105 106 107 108 109\n\"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\"\nshow_tokens(seq(1000, len = 10))\n   1000    1001    1002    1003    1004    1005    1006    1007    1008    1009\n  \"ied\"    \"ER\"  \"stat\"   \"fig\"    \"me\"   \"von\" \"inter\"  \"roid\"  \"ater\" \"their\"\nshow_tokens(seq(10000, len = 10))\n   10000    10001    10002    10003    10004    10005    10006    10007\n   \"ång\"  \"citep\"    \"Ill\"   \"rank\" \"sender\"   \"beim\"    \"рак\" \"compat\"\n   10008    10009\n\"occurs\"  \"diese\"\nshow_tokens(seq(20000, len = 10))\n    20000     20001     20002     20003     20004     20005     20006     20007\n  \"admit\" \"Comment\"     \"стя\"    \"Vien\"      \"ці\"  \"permut\"     \"cgi\"    \"crít\"\n    20008     20009\n\"Console\"    \"ctic\"\nshow_tokens(seq(to = as.integer(tokenizer$vocab_size()) - 1, len = 10))\n31990 31991 31992 31993 31994 31995 31996 31997 31998 31999\n  \"ὀ\"  \"げ\"  \"べ\"  \"边\"  \"还\"  \"黃\"  \"왕\"  \"收\"  \"弘\"  \"给\"\nMoving on, the next step after tokenization is embedding. An embedding\nlayer is effectively a dictionary lookup that converts an integer (token\nid) to a 1-d float array. For this we can use the standard keras\nEmbedding layer.\ntok_embeddings <- keras$layers$Embedding(\n  input_dim = tokenizer$vocab_size(),\n  output_dim = params$dim,\n  embeddings_initializer =\n    \\(...) np$load(weights_path(\"7B/tok_embeddings.weight.npy\"))\n)\n\ntok_embeddings(3L) |> str()\n<tf.Tensor: shape=(4096), dtype=float32, numpy=…>\nprompt |> # \"The best way to attract bees\"\n  tokenizer$tokenize() |>\n  tok_embeddings() |>\n  str()\n<tf.Tensor: shape=(8, 4096), dtype=float32, numpy=…>\nTransformerBlock\nOnce it’s tokenized and embedded, the input then passes through the bulk\nof the model, a sequence of repeating TransformerBlock layers. The 7B\nmodel has 32 of these TransformerBlock layers, while the 65B model has\n80 of them.\nweights_path(\"7B/params.json\")  |> read_json() |> _$n_layers\n[1] 32\nweights_path(\"65B/params.json\") |> read_json() |> _$n_layers\n[1] 80\nHere is what the transformer block looks like:\nTransformerBlock(keras$layers$Layer) %py_class% {\n  initialize <- function(attn_head_size, attn_n_heads,\n                         norm_eps = k_epsilon(), ...,\n                         block_id = NULL) {\n    super$initialize(...)\n\n    self$attention <- Attention(attn_head_size, attn_n_heads,\n                                block_id = block_id)\n\n    self$feed_forward <- FeedForward(\n      hidden_dim = 4 * attn_head_size * attn_n_heads,\n      block_id = block_id)\n\n    self$attention_norm <- RMSNorm(eps = norm_eps,\n                                   block_id = block_id,\n                                   feeds_into = \"attention\")\n    self$feed_forward_norm <- RMSNorm(eps = norm_eps,\n                                      block_id = block_id,\n                                      feeds_into = \"ffn\")\n  }\n\n  call <- function(x) {\n\n    # norm and attention\n    x2 <- x |>\n      self$attention_norm() |>\n      self$attention()\n\n    x <- x + x2 # add residual\n\n    # norm and swiglu\n    x2 <- x %>%\n      self$feed_forward_norm() %>%\n      self$feed_forward()\n\n    x <- x + x2 # residual again\n\n    x\n  }\n}\nWhile there is not a lot of code, there are a lot of ideas packed in\nthere. This block forms the main trunk of the model, so it’s worth\ntaking the time to go through it slowly.\nWe implement the TransformerBlock as a subclassed\nkeras.layers.Layer. This is gives us some niceties like the ability to\ncompose with other Keras layers, but these are mostly irrelevant to the\npurpose of this blog post; we could just as easily implement this as,\nfor example, a vanilla R6 class. Our TransformerBlock class has two\nmethods: initialize, called when we first create the block, and\ncall, called when we run the forward pass of the block.\nIn initialize, we create 4 layers: an Attention layer, a\nFeedForward layer, and 2 RMSNorm layers. We’ll take a close look at\neach of these soon, but even before we do so, we can see how they fit\ntogether by looking at the TransformerBlock$call() method.\nThe call method has a few simple ideas. In no particular order, the\nfirst one to observe is the composition pattern of adding residuals.\nx2 <- x |> ...\nx <- x + x2 # add residual x to x2\nThis is a common pattern that helps with model training, and especially\nto help with the vanishing gradient\nproblem. It’s\na skip-connection in the other-wise linear sequence of matrix\ntransformations. It reinjects information (during the forward pass), and\ngradients (during back propagation), back into the trunk. You can think\nof these residual connections as freeing the learnable layers in-between\n(the ... in the pseudo code) from the burden of having to\n“pass-through” or “preserve” information in x, allowing the weights to\ninstead focus on learning transformations that are, (in corporatese\nvernacular), value-adding.\nThe next composition pattern to note is the repeating usage of a\nnormalization layer:\nx2 <- x |> norm() |> ...\nx <- x + x2\nThere are many kinds of normalization layers, but to slightly\nover-generalize, they can all be thought of as a stabilizer that helps\nwith training. Like their deep-learning cousins the regularizers, their\nmain function is to keep values passing through in a sensible range–in\nthe ball park of (-1, 1), typically. We’ll take a closer look at\nRMSNorm soon.\nStripped of two tricks that are mostly there to help the model train,\nresiduals and normalization, the core of the TransformerBlock is just\nthis:\nx |> attention() |> feed_forward()\nIn a moment we’ll see that that feed_foward is a slightly fancier\nvariation of a conventional sequence of Dense layer. Before we get\nthere we can we safely skip ahead to distill the following intuition: a\nTransformerBlock is basically an Attention layer followed by a few\n(fancy) dense layers, with some simple composition patterns (tricks)\nthat help with training. Attention is the heart of the model: it’s the\nmost interesting, and also the most involved.\nWith the framing in place, let’s go through and take a closer look at\nRMSNorm, FeedForward, and then with the foundation in place, we’ll\nturn our attention to Attention.\nRMSNorm\nRMSNorm(keras$layers$Layer) %py_class% {\n  initialize <-\n    function(eps = 1e-6, ..., block_id = NULL, feeds_into = NULL) {\n      super$initialize(...)\n      self$eps <- eps\n      self$block_id <- block_id\n      self$feeds_into <- feeds_into\n    }\n\n  build <- function(input_shape) {\n    # input_shape == (batch_size, seqlen, params$dim)\n    # self$w will broadcast over batch_size and seqlen dims.\n    # w_shape == (1, 1, params$dim)\n    w_shape <- rep(1L, length(input_shape))\n    w_shape[length(input_shape)] <- as.integer(input_shape) |> tail(1L)\n\n    # define a local function that will load\n    # the pretrained-weights if we supplied `block_id` and `feeds_into`\n    import_from({self}, block_id, feeds_into)\n    initializer <-if (is.null(block_id))\n      \"ones\"\n      else if (block_id >=0) {\n        \\(...) weights_path(\"7B/layers.{block_id}.{feeds_into}_norm.weight.npy\") |>\n               np$load() |> np$expand_dims(0:1)\n      } else if(block_id == -1)\n        # load weights for the final output normalization layer, which is not\n        # part of a TransformerBlock\n        \\(...) weights_path(\"7B/norm.weight.npy\") |>\n               np$load() |> np$expand_dims(0:1)\n\n    self$w <- self$add_weight(shape = w_shape,\n                              initializer = initializer,\n                              trainable = TRUE)\n  }\n\n  rrms <- function(x) {\n    # reciprocal root mean square along the last axis\n    x %>% # (batch_size, seqlen, n_features)\n      tf$math$square() %>%\n      tf$reduce_mean(axis = -1L, keepdims = TRUE) %>% # (batch_size, seqlen, 1)\n      tf$math$add(self$eps) %>% # for numerical stability\n      tf$math$rsqrt()\n  }\n\n  call <- function(x) {\n    x * self$rrms(x) * self$w\n  }\n}\nRMSnorm() has a single trainable tensor w. In the forward pass, each\nvalue in the input is multiplied by the reciprocal-root-mean-square of\nall the values in the feature axis and by w. Certainly a mouthful, but\njust a simple sequence of arithmetic transformations in the end,\ndesigned for the express purpose of adjusting the range of values\npassing through.\nLet’s kick the tires on it:\nnorm <- RMSNorm()\nm <- matrix(c(0, 1,\n              2, 3), nrow = 2)\nnorm(m)\ntf.Tensor(\n[[0.         1.4142132 ]\n [0.44721353 1.3416406 ]], shape=(2, 2), dtype=float32)\nnorm(m*10)\ntf.Tensor(\n[[0.         1.4142137 ]\n [0.44721362 1.3416408 ]], shape=(2, 2), dtype=float32)\nnorm(m*100)\ntf.Tensor(\n[[0.        1.4142137]\n [0.4472136 1.3416408]], shape=(2, 2), dtype=float32)\nFeedForward\nNext up is FeedForward()\nFeedForward(keras$layers$Layer) %py_class% {\n\n  initialize <- function(hidden_dim, multiple_of = 256L,\n                         ..., block_id = NULL) {\n    super$initialize()\n\n    if(!is.null(multiple_of)) {\n      hidden_dim <- hidden_dim %>%\n        { as.integer( . * (2/3)) } %>%\n        { (. + multiple_of - 1) %/% multiple_of } %>%\n        { . * multiple_of }\n    }\n\n    self$hidden_dim <- hidden_dim\n    self$block_id <- block_id\n  }\n\n  build <- function(input_shape) {\n    output_dim <- input_shape |> as.integer() |> tail(1)\n\n    if(is.null(self$block_id))\n      load_weight <- \\(...) NULL\n    else\n      load_weight <- \\(name) \\(...) np$load(weights_path(\n        \"7B/layers.{self$block_id}.feed_forward.{name}.weight.npy\"))$`T`\n\n    self$w1 <- Dense(self$hidden_dim, use_bias = FALSE,\n                     kernel_initializer = load_weight(\"w1\"))\n    self$w2 <- Dense(output_dim, use_bias = FALSE,\n                     kernel_initializer = load_weight(\"w2\"))\n    self$w3 <- Dense(self$hidden_dim, use_bias = FALSE,\n                     kernel_initializer = load_weight(\"w3\"))\n\n    super$build(input_shape)\n  }\n\n  call <- function(x) {\n    import_from({self}, w1, w2, w3)\n    import_from(tf$nn, silu)\n\n    x %>%\n      { silu(w1(.)) * w3(.) } %>% # SwiGLU\n      w2()\n  }\n\n}\nFeedForward consists of three Dense layers. initialize does some\nsimple arithmetic, munging on the input value hidden_dim to ensure the\nsize is a performant multiple of 256, and build is mostly boiler plate\nfor creating the layers and loading the weights.\nThe novelty of FeedForward() is in the call() method, where rather\nthan composing the Dense layers in a conventional sequential model\nwith, say, ReLU activations in between and maybe some dropout, the\nlayers are composed to form a “SwiGLU” unit. The publication by Shazeer (2020)\nof SwiGLU and other variations on GLU is an exemplar of the types\nof explorations and improvements around the Transformer architecture\nsince its initial publication in\n2017; a steady accretion of\nenhancements that has brought us to today. The Feedforward$call() is\njust a single SwiGLU followed by a linear projection. In its essence,\nit’s a clever composition of three (learned) linear projections, an\nelement-wise multiplication, and a silu()\nactivation\nfunction.\nPerhaps the most surprising observation to make here is the relative\ndearth of activation functions, or even non-linearities, not just in\nFeedForward, but overall. The silu() in this feedforward, the\nreciprocal-root-mean-square in RMSnorm(), and a softmax() in\nAttention() are the only non-linear transformations in the whole\nsequence of TransformerBlocks. Everything else is a linear\ntransformation!\nAttention\nFinally, let’s turn our attention to Attention().\nAttention(keras$layers$Layer) %py_class% {\n  initialize <- function(head_size, n_heads,\n                         ..., block_id = NULL) {\n    super$initialize(...)\n\n    self$head_size <- head_size\n    self$n_heads <- n_heads\n\n    if (is.null(block_id))\n      load_weight <- function(name) NULL\n    else\n      load_weight <- \\(name) \\(...) np$load(weights_path(\n        \"7B/layers.{block_id}.attention.{name}.weight.npy\"))$`T`\n\n    Dense <- function(name) keras$layers$Dense(\n      units = n_heads * head_size,\n      use_bias = FALSE,\n      kernel_initializer = load_weight(name)\n    )\n\n    self$wq <- Dense(\"wq\")\n    self$wk <- Dense(\"wk\")\n    self$wv <- Dense(\"wv\")\n    self$wo <- Dense(\"wo\")\n  }\n\n  call <- function(x) {\n    c(batch_size, seqlen, n_features) %<-% tf$unstack(tf$shape(x))\n\n    # 1. project (linear transform) x into\n    #    query, key, and value tensors\n    # 2. reshape q k v, splitting out the last dim (n_features)\n    #    into n_heads independent subspaces,\n    #    each with size head_size.\n    #    (n_features == head_size * n_heads)\n    split_heads_shape <- c(batch_size, seqlen,\n                           self$n_heads, self$head_size)\n    q <- x |> self$wq() |> tf$reshape(split_heads_shape)\n    k <- x |> self$wk() |> tf$reshape(split_heads_shape)\n    v <- x |> self$wv() |> tf$reshape(split_heads_shape)\n\n    # embed positional information in query and key\n    # (bsz, seqlen, n_heads, head_size)\n    q %<>% apply_rotary_embedding()\n    k %<>% apply_rotary_embedding()\n\n    # reshape:\n    #   move heads out of the last 2 axes,\n    #   so later matmuls are performed across the subspaces (heads)\n    #   between (seqlen, head_size) axes\n    v <- tf$transpose(v, c(0L, 2L, 1L, 3L)) # (bsz, n_heads, seqlen, head_size)\n    q <- tf$transpose(q, c(0L, 2L, 1L, 3L)) # (bsz, n_heads, seqlen, head_size)\n    k <- tf$transpose(k, c(0L, 2L, 3L, 1L)) # (bsz, n_heads, head_size, seqlen)\n\n    # calculate and normalize attention scores\n    scores <- q %*% k                       # (bsz, n_heads, seqlen, seqlen)\n    scores <- scores / sqrt(self$head_size) # scale\n\n    # apply causal mask, so the model can't \"look ahead\" during training\n    mask <- make_mask(seqlen, dtype = scores$dtype)\n    scores %<>% { . + mask }\n\n    scores <- tf$nn$softmax(scores, axis = -1L)\n\n    # adjust values tensor with attention scores\n                      # scores (bsz, n_heads, seqlen, seqlen)\n                      # v      (bsz, n_heads, seqlen, head_size)\n    output <- scores %*% v   # (bsz, n_heads, seqlen, head_size)\n\n    # combine heads back into a single features dim,\n    # so Attention output_shape==input_shape\n    output <- output |>\n      tf$transpose(c(0L, 2L, 1L, 3L)) |> # (bsz, seqlen, n_heads, head_size)\n      tf$reshape(tf$shape(x))            # (bsz, seqlen, n_heads * head_size)\n\n    # one more trainable linear projection for good luck\n    output <- self$wo(output) # (bsz, seqlen, n_heads * head_size)\n\n    output\n  }\n}\nAttention in LLaMA is similar but not identical to the Attention\ndescribed in the original Transformers\npaper (and available as a keras\nbuiltin under keras$layers$MultiHeadAttention()). The core novelty is\nthe addition of the apply_rotary_embedding() function, which we’ll\ndescribe shortly. The additional novelty is balanced by the simplicity\nfrom the fact that the layer is performing self-attention—we don’t need\nto pass in different query, key, and value tensors (or reason about what\nthat means), since the same input serves all three roles. Note that the\nconventional MultiHeadAttention() layer is covered quite thoroughly in\nthe 2nd Edition of Deep Learning with R,\nincluding a full implementation of attention in base R.\nTo develop an understanding of the mechanics in a layer like this, it’s\nhelpful to temporarily unsee some of the minutia that can act as a fog\nobscuring the essence of the operation. In this instance, if we\ntemporarily strip out the transpose()s and reshape()s (as clever and\nvital as they are), this is what’s left:\ncall <- function(x) {\n  # split input into three learned linear projections\n  q <- x |> self$wq()\n  k <- x |> self$wk()\n  v <- x |> self$wv()\n\n  # rotate q,k to inject position information.\n  # cross q,k to calculate an attention score for each token pair.\n  scores <- rotate(q) %*% rotate(k)   |>  normalize_scores()\n\n  # adjust the 3rd projection with the attention scores\n  output <- scores %*% v\n\n  self$wo(output) # one more learned linear projection for good luck\n}\nReturning to the transpose()s and reshapes(), you can observe that\ntheir purpose is to make it so that the attention calculations are\nperformed across n_heads independent subspaces, rather than in a\nsingle larger space. The same reasoning drives this decision as that\ndriving usage of depthwise-separable convolutions in image models.\nEmpirically, for the fixed compute budget, factoring features into\nindependent subspaces performs better than doing the same core\noperations in single larger feature space. As with all things, there is\na balance to strike between n_heads (the number of subspaces) and\nhead_dim (the size of each subspace). The LLaMA authors have struck\nthe balance like this at the various model sizes:\nlapply(c(\"7B\", \"13B\", \"30B\", \"65B\"), \\(size) {\n  p <- read_json(weights_path(\"{size}/params.json\"))\n  with(p, list(llama_size = size,\n               n_heads = n_heads,\n               head_dim = dim %/% n_heads))\n}) |> dplyr::bind_rows()\n# A tibble: 4 × 3\n  llama_size n_heads head_dim\n  <chr>        <int>    <int>\n1 7B              32      128\n2 13B             40      128\n3 30B             52      128\n4 65B             64      128\nNext lets turn our attention to the causal attention mask.\nmake_mask <- function(seqlen, dtype = k_floatx()) {\n  x <- tf$range(seqlen)\n  mask <- tf$where(x[, tf$newaxis] < x[tf$newaxis, ],\n                   tf$constant(-Inf, dtype = dtype),\n                   tf$constant(0, dtype = dtype))\n\n  # broadcast over batch and heads dim\n  mask[tf$newaxis, tf$newaxis, , ] # (1, 1, seqlen, seqlen)\n}\nThe mask is a strictly upper triangular matrix filled with -Inf\nvalues. Adding the mask to the attention scores prevents the model from\nbeing able to “look ahead” and see the attention score for a token\npairing it hasn’t seen yet at a particular position in the sequence.\nThis need for a mask is best thought of as a vestige from training,\nan apparatus that the model needed to learn with and now it can’t function without.\nDuring training, gradients are calculated for predictions from all\ntoken positions in a sequence, including predictions tokens where the correct\nanswer is right there, as the very next token in same sequence. The mask\nprevents the model from being able to cheat and look ahead into the future,\nsomething it won’t be able to do once it’s we’re running it for inference.\nmake_mask(seqlen = 5L)\ntf.Tensor(\n[[[[  0. -inf -inf -inf -inf]\n   [  0.   0. -inf -inf -inf]\n   [  0.   0.   0. -inf -inf]\n   [  0.   0.   0.   0. -inf]\n   [  0.   0.   0.   0.   0.]]]], shape=(1, 1, 5, 5), dtype=float32)\nRotary Position Embedding\nNext lets turn our attention to apply_rotary_embedding(). This core\ninnovation was published by Su et al. (2022) in the paper titled\n“RoFormer: Enhanced Transformer with Rotary Position Embedding”.\nSome context:\nThe bare Attention() mechanism doesn’t leave any possibility for a\ntoken’s position in a sequence to affect the attention scores, since\nonly token-pairs are scored. Attention treats its input like a\nbag-of-tokens.\nThe position of a token in a sequence is clearly important, and the\nattention layer should have access to that information.\nThe absolute position of a token in a sequence is less important\nthan the relative position between tokens. (Especially so for long\nsequences).\nWhich leads us into the complex plane. If we imagine the features as\ncomplex numbers, we can rotate them, and we can calculate angles between\nthem. From the Roformers paper:\n\nSpecifically, incorporating the relative position embedding is\nstraightforward: simply rotate the affine-transformed word embedding\nvector by amount of angle multiples of its position index and thus\ninterprets the intuition behind Rotary Position Embedding\n\nExpanding slightly: the rotation matrix is designed so that\nsubsequently, after rotating our q and k token sequence embedding\nthe same way, the angle between token features is a function of the\nrelative distance between those tokens in the token sequence. The\nrelative angle between two tokens is invariant to the absolute\nposition of those tokens in the full sequence.\nIn short, the rotation injects positional information. The meaning or\ninterpretability of that positional information, or how it is meant to\nbe used, or even extracted from the result of q %*% k, is left to the\nmodel to learn.\nHere is the code:\napply_rotary_embedding <- function(x) {\n  c(., seqlen, ., head_size) %<-%\n    tf$unstack(tf$shape(x))\n\n  rotation_matrix <- compute_rotation_matrix(seqlen, head_size)\n\n  x %>%\n    view_as_complex() %>%\n    { . * rotation_matrix } %>%\n    view_as_real()\n\n}\n\ncompute_rotation_matrix <-\n  function(seqlen, feature_dim, theta = 10000) {\n    # `feature_dim` here is going to be attention$head_size\n    # `seqlen` is going to match the token sequence length.\n\n    t <- tf$range(seqlen, dtype = tf$float32)\n    freqs <- tf$range(start = 0, limit = 1, delta = 1 / (feature_dim %/% 2),\n                      dtype = tf$float32)\n    tf_assert(tf$size(freqs) == feature_dim %/% 2)\n    freqs <- 1.0 / (theta ^ freqs)\n\n    # outer product; (seqlen, head_size/2)\n    freqs <- tf$einsum('a,b->ab', t, freqs)\n\n    rot_mat <- tf$complex(tf$cos(freqs), tf$sin(freqs))\n\n    # the positional embedding will be broadcast across batch and heads dim\n    rot_mat[tf$newaxis, , tf$newaxis, ] #(1, seqlen, 1, headdim/2)\n  }\n\nview_as_complex <- function(x) {\n  tf$complex(x[all_dims(), `::2`],\n             x[all_dims(), `2::2`])\n}\n\nview_as_real <- function(x) {\n  # xs = (..., f);  xs2 = (..., f*2)\n  xs <- tf$shape(x)\n  xs2 <- tf$concat(list(xs[1:(length(xs)-1)],\n                        xs[length(xs), drop = FALSE] * 2L),\n                   axis = 0L)\n\n  x2 <- tf$stack(list(Re(x), Im(x)), axis = -1L)\n\n  # (..., f, 2) -> (..., f*2)\n  tf$reshape(x2, xs2)\n}\nAs you can see, to imagine the embedding features as existing in the\ncomplex plane, we merely treat adjacent pairs of floats in the\nunderlying array as the real and imaginary part of a complex number. We\nrotate the embeddings in the complex plane, then go back to imagining\nthe features as existing in the real plane. Again, the job of\ninterpreting the meaning of the features after rotation is left to the\nmodel to learn.\nWe can quickly confirm that the rotary embeddings only rotate features\nand don’t scale them:\nnear <- function (x, y, tol = 1e-6) abs(x - y) < tol\nall(near(1, Mod(compute_rotation_matrix(2048L, 128L))))\ntf.Tensor(True, shape=(), dtype=bool)\nThere is one more trick to observe before moving on: because of some of\nthe mathematical properties of the rotation matrix, it’s possible to\navoid doing a full complex multiply operation and still arrive at the\nsame result. Also, since the rotation matrix never changes, it makes\nsense to only compute it once and cache it, like so:\nprecomputed_rotation_matrix <- compute_rotation_matrix(\n  seqlen = 2048L, # LLaMA max seqlen\n  feature_dim = with(params, dim %/% n_heads)  # head_size\n)\n\napply_rotary_embedding_faster <- function(x) {\n\n  rotate_every_two <- function(x) {\n    x1 <- x[all_dims(), `::2`]\n    x2 <- x[all_dims(), `2::2`]\n    x_ <- tf$stack(list(-x2, x1), axis = -1L)\n    tf$reshape(x_, tf$shape(x))\n  }\n\n  repeat_each_twice <- function(x) {\n    tf$`repeat`(x, 2L, axis = -1L)\n  }\n\n  seqlen <- tf$shape(x)[2]\n  rot <- precomputed_rotation_matrix[, NA:seqlen, , ]\n\n  cos <- Re(rot) |> repeat_each_twice()\n  sin <- Im(rot) |> repeat_each_twice()\n\n  (x * cos) + (rotate_every_two(x) * sin)\n}\nrand <- tf$random$uniform(shape(3, 8, params$n_heads, 128))\nall(apply_rotary_embedding(rand) ==\n    apply_rotary_embedding_faster(rand))\ntf.Tensor(True, shape=(), dtype=bool)\napply_rotary_embedding <- apply_rotary_embedding_faster\nFinally, note that the rotary positional embeddings are applied within\neach Attention layer. This is different from the original Transformer\nimplementation, where a positional embedding was only added once at the\nhead of the model. Similar to residual connections, you can think of the\npresence of these repeated injections of positional information as\nrelieving the remaining trainable layers from the burden of allocating\nsome of their weights to the task of “passing through” or “preserving”\nthe positional information for later layers.\nPositional embeddings are a rich subject that also comes up in other\ndeep learning architectures, like denoising diffusion (Falbel and Keydana 2023),\nso time spent understanding them better is time well\nspent. For the purposes of this blog post we’ve covered the points\nneeded and we’ll move on to tying all pieces together. To go deeper and\ndevelop a more mathematically informed understand of RoPE, two excellent\nstarting points are:\nThe original paper by Su et al. (2022)\nThis blog post by\nBiderman et al. (2021)\nTying it all together\nWith Tokenizer, Embedding, TransformerBlock (RMSNorm,\nAttention FeedForward and apply_rotary_embedding) all covered,\nit’s time to tie all the pieces together into a Transformer model. We\ncould do this using %py_class% like with the other layers above, but\nit’s just as easy to move over to using the Keras functional API at this\npoint.\nlayer_transformer_block <- create_layer_wrapper(TransformerBlock)\nlayer_rms_norm <- create_layer_wrapper(RMSNorm)\n\n# input to the model will be output from the tokenizer\ninput <- layer_input(shape(NA)) #, dtype = \"int32\")\n\nx <- input |>\n  tok_embeddings()  # instantiated earlier in the blog-post\n\nfor(block_id in seq_len0(params$n_layers)) {\n  x <- x |>\n    layer_transformer_block(attn_head_size = params$dim %/% params$n_heads,\n                            attn_n_heads = params$n_heads,\n                            norm_eps = params$norm_eps,\n                            block_id = block_id)\n}\n\n# final output projection into logits of output tokens\nx <- x |>\n  layer_rms_norm(block_id = -1, eps = params$norm_eps) |>\n  layer_dense(\n    tokenizer$vocab_size(), use_bias = FALSE,\n    kernel_initializer = \\(...) np$load(weights_path(\"7B/output.weight.npy\"))$`T`\n  )\n\n# slice out the logits for the last token\nwith_options(c(tensorflow.extract.warn_negatives_pythonic = FALSE), {\n  output <- x[, -1, ]\n})\n\nllama <- keras_model(input, output) %>%\n  compile(jit_compile = TRUE)\nThe input to the model is tokenized text and the output is the\n(unnormalized) probabilities for each token in tokenizer$vocab_size()\nbeing the next token in the sequence.\nnext_token_probs <- prompt %>%\n  tokenizer$tokenize() %>%\n  llama()\n\nnext_token_probs\ntf.Tensor(\n[[-2.4503722e+00 -3.4463339e+00  1.3200411e+01 ...  4.8804146e-01\n  -1.3277926e+00  9.9985600e-03]], shape=(1, 32000), dtype=float32)\nSampling strategies for selecting a token from the token logits is a\nrich topic, (also covered thoroughly in the Deep Learning with\nR book), but this blog post is long enough\nalready. So for now, let’s just take the argmax().\nsampler <- \\(logits) tf$argmax(logits, axis = -1L, output_type = \"int32\")\n\n(next_token <- sampler(next_token_probs))\ntf.Tensor([304], shape=(1), dtype=int32)\ntokenizer$detokenize(next_token) |> as.character()\n[1] \"to\"\nLet’s run it for a few tokens and let LLaMa finish the sentence:\nprompt_tokens <- tokenizer$tokenize(\"The best way to attract bees\")\n\nfor (i in 1:20) {\n\n  next_token_probs <- prompt_tokens |> llama()\n  next_token <- sampler(next_token_probs)\n\n  prompt_tokens %<>% { tf$concat(c(., next_token), axis = -1L) }\n\n  # end of sentence\n  if (as.logical(next_token == tokenizer$string_to_id(\".\")))\n    break\n}\n\nprompt_tokens |>\n  tokenizer$detokenize() |>\n  as.character() |>\n  strwrap(60) |> writeLines()\nThe best way to attract bees to your garden is to plant a\nvariety of flowers that bloom at different times.\nWrapping up\nIn this blog post we’ve walked through the LLaMA architecture\nimplemented in R TensorFlow, including how to load pretrained weights,\nand then run the model to generate a sentence. Note, much of the code in\nthis blog post is tailored for didactic purposes. While the\nimplementation of the LLaMA architecture covered in this blog post is\nappropriate for training, there are a few modifications you’ll want to\nmake before doing a lot of text generation. Those include things like:\nIn the Attention layer, caching the k and v tensors. Then,\nafter the first forward pass with the initial prompt, only feeding\nthe model the one new token from the sampler(), rather than\nfeeding the model all the tokens of the full prompt on each forward\npass.\nOnly generating the causal mask make_mask() and rotary_matrix\nslices once per forward pass, instead of within each Attention\ncall.\nUpdating the TransformerBlock to be cache-aware and to pass\nthrough the appropriate arguments to Attention()\nWrapping all the additional book-keeping logic in a custom\nTransformerDecoder() class.\nThe changes required to implement these optimizations for inference\nballoon the code size and are mostly about book-keeping, so we won’t go\nthrough them in this blog post. However, you can find a fuller\nimplementation of LLaMA in R Tensorflow, including a cache-aware\ngenerate() method that only feeds the model one token at a time during\nthe main inference loop, (and compiles to XLA!),\nhere.\nThat’s all for now. Thanks for reading and happy travels to all\nexploring this exciting LLM terrain!\nPhoto by Sébastien Goldberg on Unsplash\n\n\n\nBiderman, Stella, Sid Black, Charles Foster, Leo Gao, Eric Hallahan, Horace He, Ben Wang, and Phil Wang. 2021. “Rotary Embeddings: A Relative Revolution.” blog.eleuther.ai/rotary-embeddings/.\n\n\nFalbel, Daniel, and Sigrid Keydana. 2023. “Posit AI Blog: De-Noising Diffusion with Torch.” https://blogs.rstudio.com/tensorflow/posts/2023-04-13-denoising-diffusion/.\n\n\nHoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. “Training Compute-Optimal Large Language Models.” https://arxiv.org/abs/2203.15556.\n\n\nShazeer, Noam. 2020. “GLU Variants Improve Transformer.” https://arxiv.org/abs/2002.05202.\n\n\nSu, Jianlin, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2022. “RoFormer: Enhanced Transformer with Rotary Position Embedding.” https://arxiv.org/abs/2104.09864.\n\n\nTouvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, et al. 2023. “LLaMA: Open and Efficient Foundation Language Models.” https://doi.org/10.48550/ARXIV.2302.13971.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” https://arxiv.org/abs/1706.03762.\n\n\n\n\n",
    "preview": "posts/2023-05-25-llama-tensorflow-keras/images/preview.jpg",
    "last_modified": "2024-11-21T15:50:35+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-05-09-group-equivariant-cnn-3/",
    "title": "Group-equivariant neural networks with escnn",
    "description": "Escnn, built on PyTorch, is a library that, in the spirit of Geometric Deep Learning, provides a high-level interface to designing and training group-equivariant neural networks. This post introduces important mathematical concepts, the library's key actors, and essential library use.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2023-05-09",
    "categories": [
      "Torch",
      "R",
      "Concepts",
      "Image Recognition & Image Processing"
    ],
    "contents": "\n\nContents\nPurpose and scope of this post\nUsing escnn\nSpaces, groups, and representations: escnn$gspaces\nRepresentations, for real: escnn$nn$FieldType\nGroup-equivariant convolution\nA group-equivariant neural network\nWhere to from here\n\nToday, we resume our exploration of group equivariance. This is the third post in the series. The first was a high-level introduction: what this is all about; how equivariance is operationalized; and why it is of relevance to many deep-learning applications. The second sought to concretize the key ideas by developing a group-equivariant CNN from scratch. That being instructive, but too tedious for practical use, today we look at a carefully designed, highly-performant library that hides the technicalities and enables a convenient workflow.\nFirst though, let me again set the context. In physics, an all-important concept is that of symmetry1, a symmetry being present whenever some quantity is being conserved. But we don’t even need to look to science. Examples arise in daily life, and – otherwise why write about it - in the tasks we apply deep learning to.\nIn daily life: Think about speech – me stating “it is cold,” for example. Formally, or denotation-wise, the sentence will have the same meaning now as in five hours. (Connotations, on the other hand, can and will probably be different!). This is a form of translation symmetry, translation in time.\nIn deep learning: Take image classification. For the usual convolutional neural network, a cat in the center of the image is just that, a cat; a cat on the bottom is, too. But one sleeping, comfortably curled like a half-moon “open to the right,” will not be “the same” as one in a mirrored position. Of course, we can train the network to treat both as equivalent by providing training images of cats in both positions, but that is not a scaleable approach. Instead, we’d like to make the network aware of these symmetries, so they are automatically preserved throughout the network architecture.\nPurpose and scope of this post\nHere, I introduce escnn, a PyTorch extension that implements forms of group equivariance for CNNs operating on the plane or in (3d) space. The library is used in various, amply illustrated research papers; it is appropriately documented; and it comes with introductory notebooks both relating the math and exercising the code. Why, then, not just refer to the first notebook, and immediately start using it for some experiment?\nIn fact, this post should – as quite a few texts I’ve written – be regarded as an introduction to an introduction. To me, this topic seems anything but easy, for various reasons. Of course, there’s the math. But as so often in machine learning, you don’t need to go to great depths to be able to apply an algorithm correctly. So if not the math itself, what generates the difficulty? For me, it’s two things.\nFirst, to map my understanding of the mathematical concepts to the terminology used in the library, and from there, to correct use and application. Expressed schematically: We have a concept A, which figures (among other concepts) in technical term (or object class) B. What does my understanding of A tell me about how object class B is to be used correctly? More importantly: How do I use it to best attain my goal C? This first difficulty I’ll address in a very pragmatic way. I’ll neither dwell on mathematical details, nor try to establish the links between A, B, and C in detail. Instead, I’ll present the characters2 in this story by asking what they’re good for.\nSecond – and this will be of relevance to just a subset of readers – the topic of group equivariance, particularly as applied to image processing, is one where visualizations can be of tremendous help. The quaternity3 of conceptual explanation, math, code, and visualization can, together, produce an understanding of emergent-seeming quality… if, and only if, all of these explanation modes “work” for you. (Or if, in an area, a mode that does not wouldn’t contribute that much anyway.) Here, it so happens that from what I saw, several papers have excellent visualizations4, and the same holds for some lecture slides and accompanying notebooks5. But for those among us with limited spatial-imagination capabilities – e.g., people with Aphantasia – these illustrations, intended to help, can be very hard to make sense of themselves. If you’re not one of these, I totally recommend checking out the resources linked in the above footnotes. This text, though, will try to make the best possible use of verbal explanation to introduce the concepts involved, the library, and how to use it.\nThat said, let’s start with the software.\nUsing escnn\nEscnn depends on PyTorch. Yes, PyTorch, not torch; unfortunately, the library hasn’t been ported to R yet.6 For now, thus, we’ll employ reticulate7 to access the Python objects directly.\nThe way I’m doing this is install escnn in a virtual environment, with PyTorch version 1.13.1. As of this writing, Python 3.11 is not yet supported by one of escnn’s dependencies; the virtual environment thus builds on Python 3.10. As to the library itself, I am using the development version from GitHub, running pip install git+https://github.com/QUVA-Lab/escnn.\nOnce you’re ready, issue\n\n\nlibrary(reticulate)\n# Verify correct environment is used.\n# Different ways exist to ensure this; I've found most convenient to configure this on\n# a per-project basis in RStudio's project file (<myproj>.Rproj)\npy_config()\n\n# bind to required libraries and get handles to their namespaces\ntorch <- import(\"torch\")\nescnn <- import(\"escnn\")\n\n\nEscnn loaded, let me introduce its main objects and their roles in the play.\nSpaces, groups, and representations: escnn$gspaces\nWe start by peeking into gspaces, one of the two sub-modules we are going to make direct use of.\n\n\ngspaces <- escnn$gspaces\npy_list_attributes(gspaces) |> (\\(vec) grep(\"On\", vec, value = TRUE))() |> sort()\n\n\n[1] \"conicalOnR3\" \"cylindricalOnR3\" \"dihedralOnR3\" \"flip2dOnR2\" \"flipRot2dOnR2\" \"flipRot3dOnR3\"\n[7] \"fullCylindricalOnR3\" \"fullIcoOnR3\" \"fullOctaOnR3\" \"icoOnR3\" \"invOnR3\" \"mirOnR3 \"octaOnR3\"\n[14] \"rot2dOnR2\" \"rot2dOnR3\" \"rot3dOnR3\" \"trivialOnR2\" \"trivialOnR3\"    \nThe methods I’ve listed instantiate a gspace. If you look closely, you see that they’re all composed of two strings, joined by “On.” In all instances, the second part is either R2 or R3. These two are the available base spaces – \\(\\mathbb{R}^2\\) and \\(\\mathbb{R}^3\\) – an input signal can live in. Signals can, thus, be images, made up of pixels, or three-dimensional volumes, composed of voxels. The first part refers to the group you’d like to use. Choosing a group means choosing the symmetries to be respected. For example, rot2dOnR2() implies equivariance as to rotations, flip2dOnR2() guarantees the same for mirroring actions, and flipRot2dOnR2() subsumes both.\nLet’s define such a gspace. Here we ask for rotation equivariance on the Euclidean plane, making use of the same cyclic group – \\(C_4\\) – we developed in our from-scratch implementation:\n\n\nr2_act <- gspaces$rot2dOnR2(N = 4L)\nr2_act$fibergroup\n\n\nIn this post, I’ll stay with that setup, but we could as well pick another rotation angle – N = 8, say, resulting in eight equivariant positions separated by forty-five degrees. Alternatively, we might want any rotated position to be accounted for. The group to request then would be SO(2), called the special orthogonal group, of continuous, distance- and orientation-preserving transformations on the Euclidean plane:\n\n\n(gspaces$rot2dOnR2(N = -1L))$fibergroup\n\n\nSO(2)\nGoing back to \\(C_4\\), let’s investigate its representations:\n\n\nr2_act$representations\n\n\n$irrep_0\nC4|[irrep_0]:1\n\n$irrep_1\nC4|[irrep_1]:2\n\n$irrep_2\nC4|[irrep_2]:1\n\n$regular\nC4|[regular]:4\nA representation, in our current context and very roughly speaking, is a way to encode a group action as a matrix, meeting certain conditions. In escnn, representations are central, and we’ll see how in the next section.\nFirst, let’s inspect the above output. Four representations are available, three of which share an important property: they’re all irreducible. On \\(C_4\\), any non-irreducible representation can be decomposed into into irreducible ones. These irreducible representations are what escnn works with internally. Of those three, the most interesting one is the second. To see its action, we need to choose a group element. How about counterclockwise rotation by ninety degrees:\n\n\nelem_1 <- r2_act$fibergroup$element(1L)\nelem_1\n\n\n1[2pi/4]\nAssociated to this group element is the following matrix:\n\n\nr2_act$representations[[2]](elem_1)\n\n\n             [,1]          [,2]\n[1,] 6.123234e-17 -1.000000e+00\n[2,] 1.000000e+00  6.123234e-17\nThis is the so-called standard representation,\n\\[\n\\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix}\n\\]\n, evaluated at \\(\\theta = \\pi/2\\). (It is called the standard representation because it directly comes from how the group is defined (namely, a rotation by \\(\\theta\\) in the plane).\nThe other interesting representation to point out is the fourth: the only one that’s not irreducible.\n\n\nr2_act$representations[[4]](elem_1)\n\n\n[1,]  5.551115e-17 -5.551115e-17 -8.326673e-17  1.000000e+00\n[2,]  1.000000e+00  5.551115e-17 -5.551115e-17 -8.326673e-17\n[3,]  5.551115e-17  1.000000e+00  5.551115e-17 -5.551115e-17\n[4,] -5.551115e-17  5.551115e-17  1.000000e+00  5.551115e-17\nThis is the so-called regular representation. The regular representation acts via permutation of group elements, or, to be more precise, of the basis vectors that make up the matrix. Obviously, this is only possible for finite groups like \\(C_n\\), since otherwise there’d be an infinite amount of basis vectors to permute.\nTo better see the action encoded in the above matrix, we clean up a bit:\n\n\nround(r2_act$representations[[4]](elem_1))\n\n\n    [,1] [,2] [,3] [,4]\n[1,]    0    0    0    1\n[2,]    1    0    0    0\n[3,]    0    1    0    0\n[4,]    0    0    1    0\nThis is a step-one shift to the right of the identity matrix. The identity matrix, mapped to element 0, is the non-action; this matrix instead maps the zeroth action to the first, the first to the second, the second to the third, and the third to the first.\nWe’ll see the regular representation used in a neural network soon. Internally – but that need not concern the user – escnn works with its decomposition into irreducible matrices. Here, that’s just the bunch of irreducible representations we saw above, numbered from one to three.\nHaving looked at how groups and representations figure in escnn, it is time we approach the task of building a network.\nRepresentations, for real: escnn$nn$FieldType\nSo far, we’ve characterized the input space (\\(\\mathbb{R}^2\\)), and specified the group action. But once we enter the network, we’re not in the plane anymore, but in a space that has been extended by the group action. Rephrasing, the group action produces feature vector fields that assign a feature vector to each spatial position in the image.\nNow we have those feature vectors, we need to specify how they transform under the group action. This is encoded in an escnn$nn$FieldType . Informally, we could say that a field type is the data type of a feature space. In defining it, we indicate two things: the base space, a gspace, and the representation type(s) to be used.\nIn an equivariant neural network, field types play a role similar to that of channels in a convnet. Each layer has an input and an output field type. Assuming we’re working with grey-scale images, we can specify the input type for the first layer like this:\n\n\nnn <- escnn$nn\nfeat_type_in <- nn$FieldType(r2_act, list(r2_act$trivial_repr))\n\n\nThe trivial representation is used to indicate that, while the image as a whole will be rotated, the pixel values themselves should be left alone. If this were an RGB image, instead of r2_act$trivial_repr we’d pass a list of three such objects.\nSo we’ve characterized the input. At any later stage, though, the situation will have changed. We will have performed convolution once for every group element. Moving on to the next layer, these feature fields will have to transform equivariantly, as well. This can be achieved by requesting the regular representation for an output field type:\n\n\nfeat_type_out <- nn$FieldType(r2_act, list(r2_act$regular_repr))\n\n\nThen, a convolutional layer may be defined like so:\n\n\nconv <- nn$R2Conv(feat_type_in, feat_type_out, kernel_size = 3L)\n\n\nGroup-equivariant convolution\nWhat does such a convolution do to its input? Just like, in a usual convnet, capacity can be increased by having more channels, an equivariant convolution can pass on several feature vector fields, possibly of different type (assuming that makes sense). In the code snippet below, we request a list of three, all behaving according to the regular representation.\n\n\nfeat_type_in <- nn$FieldType(r2_act, list(r2_act$trivial_repr))\nfeat_type_out <- nn$FieldType(\n  r2_act,\n  list(r2_act$regular_repr, r2_act$regular_repr, r2_act$regular_repr)\n)\n\nconv <- nn$R2Conv(feat_type_in, feat_type_out, kernel_size = 3L)\n\n\nWe then perform convolution on a batch of images, made aware of their “data type” by wrapping them in feat_type_in:\n\n\nx <- torch$rand(2L, 1L, 32L, 32L)\nx <- feat_type_in(x)\ny <- conv(x)\ny$shape |> unlist()\n\n\n[1]  2  12 30 30\nThe output has twelve “channels,” this being the product of group cardinality – four distinguished positions – and number of feature vector fields (three).\nIf we choose the simplest possible, approximately, test case, we can verify that such a convolution is equivariant by direct inspection. Here’s my setup:\n\n\nfeat_type_in <- nn$FieldType(r2_act, list(r2_act$trivial_repr))\nfeat_type_out <- nn$FieldType(r2_act, list(r2_act$regular_repr))\nconv <- nn$R2Conv(feat_type_in, feat_type_out, kernel_size = 3L)\n\ntorch$nn$init$constant_(conv$weights, 1.)\nx <- torch$vander(torch$arange(0,4))$view(tuple(1L, 1L, 4L, 4L)) |> feat_type_in()\nx\n\n\ng_tensor([[[[ 0.,  0.,  0.,  1.],\n            [ 1.,  1.,  1.,  1.],\n            [ 8.,  4.,  2.,  1.],\n            [27.,  9.,  3.,  1.]]]], [C4_on_R2[(None, 4)]: {irrep_0 (x1)}(1)])\nInspection could be performed using any group element. I’ll pick rotation by \\(\\pi/2\\):\n\n\nall <- iterate(r2_act$testing_elements)\ng1 <- all[[2]]\ng1\n\n\nJust for fun, let’s see how we can – literally – come whole circle by letting this element act on the input tensor four times:\n\n\nall <- iterate(r2_act$testing_elements)\ng1 <- all[[2]]\n\nx1 <- x$transform(g1)\nx1$tensor\nx2 <- x1$transform(g1)\nx2$tensor\nx3 <- x2$transform(g1)\nx3$tensor\nx4 <- x3$transform(g1)\nx4$tensor\n\n\ntensor([[[[ 1.,  1.,  1.,  1.],\n          [ 0.,  1.,  2.,  3.],\n          [ 0.,  1.,  4.,  9.],\n          [ 0.,  1.,  8., 27.]]]])\n          \ntensor([[[[ 1.,  3.,  9., 27.],\n          [ 1.,  2.,  4.,  8.],\n          [ 1.,  1.,  1.,  1.],\n          [ 1.,  0.,  0.,  0.]]]])\n          \ntensor([[[[27.,  8.,  1.,  0.],\n          [ 9.,  4.,  1.,  0.],\n          [ 3.,  2.,  1.,  0.],\n          [ 1.,  1.,  1.,  1.]]]])\n          \ntensor([[[[ 0.,  0.,  0.,  1.],\n          [ 1.,  1.,  1.,  1.],\n          [ 8.,  4.,  2.,  1.],\n          [27.,  9.,  3.,  1.]]]])\nYou see that at the end, we are back at the original “image.”\nNow, for equivariance. We could first apply a rotation, then convolve.\nRotate:\n\n\nx_rot <- x$transform(g1)\nx_rot$tensor\n\n\nThis is the first in the above list of four tensors.\nConvolve:\n\n\ny <- conv(x_rot)\ny$tensor\n\n\ntensor([[[[ 1.1955,  1.7110],\n          [-0.5166,  1.0665]],\n\n         [[-0.0905,  2.6568],\n          [-0.3743,  2.8144]],\n\n         [[ 5.0640, 11.7395],\n          [ 8.6488, 31.7169]],\n\n         [[ 2.3499,  1.7937],\n          [ 4.5065,  5.9689]]]], grad_fn=<ConvolutionBackward0>)\nAlternatively, we can do the convolution first, then rotate its output.\nConvolve:\n\n\ny_conv <- conv(x)\ny_conv$tensor\n\n\ntensor([[[[-0.3743, -0.0905],\n          [ 2.8144,  2.6568]],\n\n         [[ 8.6488,  5.0640],\n          [31.7169, 11.7395]],\n\n         [[ 4.5065,  2.3499],\n          [ 5.9689,  1.7937]],\n\n         [[-0.5166,  1.1955],\n          [ 1.0665,  1.7110]]]], grad_fn=<ConvolutionBackward0>)\nRotate:\n\n\ny <- y_conv$transform(g1)\ny$tensor\n\n\ntensor([[[[ 1.1955,  1.7110],\n          [-0.5166,  1.0665]],\n\n         [[-0.0905,  2.6568],\n          [-0.3743,  2.8144]],\n\n         [[ 5.0640, 11.7395],\n          [ 8.6488, 31.7169]],\n\n         [[ 2.3499,  1.7937],\n          [ 4.5065,  5.9689]]]])\nIndeed, final results are the same.\nAt this point, we know how to employ group-equivariant convolutions. The final step is to compose the network.\nA group-equivariant neural network\nBasically, we have two questions to answer. The first concerns the non-linearities; the second is how to get from extended space to the data type of the target.\nFirst, about the non-linearities. This is a potentially intricate topic, but as long as we stay with point-wise operations (such as that performed by ReLU) equivariance is given intrinsically.\nIn consequence, we can already assemble a model:\n\n\nfeat_type_in <- nn$FieldType(r2_act, list(r2_act$trivial_repr))\nfeat_type_hid <- nn$FieldType(\n  r2_act,\n  list(r2_act$regular_repr, r2_act$regular_repr, r2_act$regular_repr, r2_act$regular_repr)\n  )\nfeat_type_out <- nn$FieldType(r2_act, list(r2_act$regular_repr))\n\nmodel <- nn$SequentialModule(\n  nn$R2Conv(feat_type_in, feat_type_hid, kernel_size = 3L),\n  nn$InnerBatchNorm(feat_type_hid),\n  nn$ReLU(feat_type_hid),\n  nn$R2Conv(feat_type_hid, feat_type_hid, kernel_size = 3L),\n  nn$InnerBatchNorm(feat_type_hid),\n  nn$ReLU(feat_type_hid),\n  nn$R2Conv(feat_type_hid, feat_type_out, kernel_size = 3L)\n)$eval()\n\nmodel\n\n\nSequentialModule(\n  (0): R2Conv([C4_on_R2[(None, 4)]:\n       {irrep_0 (x1)}(1)], [C4_on_R2[(None, 4)]: {regular (x4)}(16)], kernel_size=3, stride=1)\n  (1): InnerBatchNorm([C4_on_R2[(None, 4)]:\n       {regular (x4)}(16)], eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (2): ReLU(inplace=False, type=[C4_on_R2[(None, 4)]: {regular (x4)}(16)])\n  (3): R2Conv([C4_on_R2[(None, 4)]:\n       {regular (x4)}(16)], [C4_on_R2[(None, 4)]: {regular (x4)}(16)], kernel_size=3, stride=1)\n  (4): InnerBatchNorm([C4_on_R2[(None, 4)]:\n       {regular (x4)}(16)], eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (5): ReLU(inplace=False, type=[C4_on_R2[(None, 4)]: {regular (x4)}(16)])\n  (6): R2Conv([C4_on_R2[(None, 4)]:\n       {regular (x4)}(16)], [C4_on_R2[(None, 4)]: {regular (x1)}(4)], kernel_size=3, stride=1)\n)\nCalling this model on some input image, we get:\n\n\nx <- torch$randn(1L, 1L, 17L, 17L)\nx <- feat_type_in(x)\nmodel(x)$shape |> unlist()\n\n\n[1]  1  4 11 11\nWhat we do now depends on the task. Since we didn’t preserve the original resolution anyway – as would have been required for, say, segmentation – we probably want one feature vector per image. That we can achieve by spatial pooling:\n\n\navgpool <- nn$PointwiseAvgPool(feat_type_out, 11L)\ny <- avgpool(model(x))\ny$shape |> unlist()\n\n\n[1] 1 4 1 1\nWe still have four “channels,” corresponding to four group elements. This feature vector is (approximately) translation-invariant, but rotation-equivariant, in the sense expressed by the choice of group. Often, the final output will be expected to be group-invariant as well as translation-invariant (as in image classification). If that’s the case, we pool over group elements, as well:\n\n\ninvariant_map <- nn$GroupPooling(feat_type_out)\ny <- invariant_map(avgpool(model(x)))\ny$tensor\n\n\ntensor([[[[-0.0293]]]], grad_fn=<CopySlices>)\nWe end up with an architecture that, from the outside, will look like a standard convnet, while on the inside, all convolutions have been performed in a rotation-equivariant way. Training and evaluation then are no different from the usual procedure.\nWhere to from here\nThis “introduction to an introduction” has been the attempt to draw a high-level map of the terrain, so you can decide if this is useful to you. If it’s not just useful, but interesting theory-wise as well, you’ll find lots of excellent materials linked from the README. The way I see it, though, this post already should enable you to actually experiment with different setups.\nOne such experiment, that would be of high interest to me, might investigate how well different types and degrees of equivariance actually work for a given task and dataset. Overall, a reasonable assumption is that, the higher “up” we go in the feature hierarchy, the less equivariance we require. For edges and corners, taken by themselves, full rotation equivariance seems desirable, as does equivariance to reflection; for higher-level features, we might want to successively restrict allowed operations, maybe ending up with equivariance to mirroring merely. Experiments could be designed to compare different ways, and levels, of restriction.8\nThanks for reading!\nPhoto by Volodymyr Tokar on Unsplash\n\n\n\nWeiler, Maurice, Patrick Forré, Erik Verlinde, and Max Welling. 2021. “Coordinate Independent Convolutional Networks - Isometry and Gauge Equivariant Convolutions on Riemannian Manifolds.” CoRR abs/2106.06020. https://arxiv.org/abs/2106.06020.\n\n\nThis is nicely explained in, for example, Jakob Schwichtenberg’s Physics from symmetry.↩︎\nIf you have some background on representations: No, not those characters …↩︎\nYes, that word exists, although I must admit I didn’t know before typing it into a search engine.↩︎\nOne paper particularly stood out to me: (Weiler et al. 2021).↩︎\nDirectly pertinent to today’s topic, thinking of the materials produced for University of Amsterdam’s course on group-equivariant deep learning.↩︎\nIf, after reading this post, you feel that maybe you would be interested in porting it – I can definitely say I think that’s a great idea!↩︎\nIf you’re new to reticulate, please consult its excellent documentation on topics like calling Python from R, determining the Python version used, installing Python packages, as well as an outstanding introduction to Python for R users.↩︎\nSee the documentation for escnn$nn$RestrictionModule for how to do this.↩︎\n",
    "preview": "posts/2023-05-09-group-equivariant-cnn-3/images/preview.jpg",
    "last_modified": "2024-11-21T15:52:43+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-17-luz-0-4/",
    "title": "luz 0.4.0",
    "description": "luz v0.4.0 is now on CRAN. This release adds support for training models on ARM Mac GPUs, reduces the overhead of using luz, and makes it easier to checkpoint and resume failed runs.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": {}
      }
    ],
    "date": "2023-04-17",
    "categories": [
      "Torch",
      "Packages/Releases",
      "R"
    ],
    "contents": "\n\nContents\nSupport for Apple Silicon\nCheckpointing\nBug fixes\nFinal remarks\n\nA new version of luz is now available on CRAN. luz is a high-level interface for torch. It aims to reduce the boilerplate code necessary to train torch models while being as flexible as possible,\nso you can adapt it to run all kinds of deep learning models.\nIf you want to get started with luz we recommend reading the\nprevious release blog post as well as the ‘Training with luz’ chapter of the ‘Deep Learning and Scientific Computing with R torch’ book.\nThis release adds numerous smaller features, and you can check the full changelog here. In this blog post we highlight the features we are most excited for.\nSupport for Apple Silicon\nSince torch v0.9.0, it’s possible to run computations on the GPU of Apple Silicon equipped Macs. luz wouldn’t automatically make use of the GPUs though, and instead used to run the models on CPU.\nStarting from this release, luz will automatically use the ‘mps’ device when running models on Apple Silicon computers, and thus let you benefit from the speedups of running models on the GPU.\nTo get an idea, running a simple CNN model on MNIST from this example for one epoch on an Apple M1 Pro chip would take 24 seconds when using the GPU:\n  user  system elapsed \n19.793   1.463  24.231 \nWhile it would take 60 seconds on the CPU:\n  user  system elapsed \n83.783  40.196  60.253 \nThat is a nice speedup!\nNote that this feature is still somewhat experimental, and not every torch operation is supported to run on MPS. It’s likely that you see a warning message explaining that it might need to use the CPU fallback for some operator:\n[W MPSFallback.mm:11] Warning: The operator 'at:****' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (function operator())\nCheckpointing\nThe checkpointing functionality has been refactored in luz, and\nit’s now easier to restart training runs if they crash for some\nunexpected reason. All that’s needed is to add a resume callback\nwhen training the model:\n\n\n# ... model definition omitted\n# ...\n# ...\nresume <- luz_callback_resume_from_checkpoint(path = \"checkpoints/\")\n\nresults <- model %>% fit(\n  list(x, y),\n  callbacks = list(resume),\n  verbose = FALSE\n)\n\n\nIt’s also easier now to save model state at\nevery epoch, or if the model has obtained better validation results.\nLearn more with the ‘Checkpointing’ article.\nBug fixes\nThis release also includes a few small bug fixes, like respecting usage of the CPU (even when there’s a faster device available), or making the metrics environments more consistent.\nThere’s one bug fix though that we would like to especially highlight in this blog post. We found that the algorithm that we were using to accumulate the loss during training had exponential complexity; thus if you had many steps per epoch during your model training,\nluz would be very slow.\nFor instance, considering a dummy model running for 500 steps, luz would take 61 seconds for one epoch:\nEpoch 1/1\nTrain metrics: Loss: 1.389                                                                \n   user  system elapsed \n 35.533   8.686  61.201 \nThe same model with the bug fixed now takes 5 seconds:\nEpoch 1/1\nTrain metrics: Loss: 1.2499                                                                                             \n   user  system elapsed \n  4.801   0.469   5.209\nThis bugfix results in a 10x speedup for this model. However, the speedup may vary depending on the model type. Models that are faster per batch and have more iterations per epoch will benefit more from this bugfix.\nFinal remarks\nThank you very much for reading this blog post. As always, we welcome every contribution to the torch ecosystem. Feel free to open issues to suggest new features, improve documentation, or extend the code base.\nLast week, we announced the torch v0.10.0 release – here’s a link to the release blog post, in case you missed it.\nPhoto by Peter John Maridable on Unsplash\n\n\n\n",
    "preview": "posts/2023-04-17-luz-0-4/images/luz.jpg",
    "last_modified": "2024-11-21T15:52:06+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-14-torch-0-10/",
    "title": "torch 0.10.0",
    "description": "torch v0.10.0 is now on CRAN. This version upgraded the underlying LibTorch to 1.13.1, and  added support for Automatic Mixed Precision. As an experimental feature, we now also support pre-built binaries, so you can install torch without having to deal with the CUDA installation.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": {}
      }
    ],
    "date": "2023-04-14",
    "categories": [
      "Torch",
      "Packages/Releases",
      "R"
    ],
    "contents": "\n\nContents\nAutomatic Mixed Precision\nPre-built binaries\nSpeedups\nBuild system refactoring\nFinal remarks\n\nWe are happy to announce that torch v0.10.0 is now on CRAN. In this blog post we\nhighlight some of the changes that have been introduced in this version. You can\ncheck the full changelog here.\nAutomatic Mixed Precision\nAutomatic Mixed Precision (AMP) is a technique that enables faster training of deep learning models, while maintaining model accuracy by using a combination of single-precision (FP32) and half-precision (FP16) floating-point formats.\nIn order to use automatic mixed precision with torch, you will need to use the with_autocast\ncontext switcher to allow torch to use different implementations of operations that can run\nwith half-precision. In general it’s also recommended to scale the loss function in order to\npreserve small gradients, as they get closer to zero in half-precision.\nHere’s a minimal example, ommiting the data generation process. You can find more information in the amp article.\n...\nloss_fn <- nn_mse_loss()$cuda()\nnet <- make_model(in_size, out_size, num_layers)\nopt <- optim_sgd(net$parameters, lr=0.1)\nscaler <- cuda_amp_grad_scaler()\n\nfor (epoch in seq_len(epochs)) {\n  for (i in seq_along(data)) {\n    with_autocast(device_type = \"cuda\", {\n      output <- net(data[[i]])\n      loss <- loss_fn(output, targets[[i]])  \n    })\n    \n    scaler$scale(loss)$backward()\n    scaler$step(opt)\n    scaler$update()\n    opt$zero_grad()\n  }\n}\nIn this example, using mixed precision led to a speedup of around 40%. This speedup is\neven bigger if you are just running inference, i.e., don’t need to scale the loss.\nPre-built binaries\nWith pre-built binaries, installing torch gets a lot easier and faster, especially if\nyou are on Linux and use the CUDA-enabled builds. The pre-built binaries include\nLibLantern and LibTorch, both external dependencies necessary to run torch. Additionally,\nif you install the CUDA-enabled builds, the CUDA and\ncuDNN libraries are already included..\nTo install the pre-built binaries, you can use:\noptions(timeout = 600) # increasing timeout is recommended since we will be downloading a 2GB file.\nkind <- \"cu117\" # \"cpu\", \"cu117\" are the only currently supported.\nversion <- \"0.10.0\"\noptions(repos = c(\n  torch = sprintf(\"https://storage.googleapis.com/torch-lantern-builds/packages/%s/%s/\", kind, version),\n  CRAN = \"https://cloud.r-project.org\" # or any other from which you want to install the other R dependencies.\n))\ninstall.packages(\"torch\")\nAs a nice example, you can get up and running with a GPU on Google Colaboratory in\nless than 3 minutes!\nColaboratory running torchSpeedups\nThanks to an issue opened by @egillax, we could find and fix a bug that caused\ntorch functions returning a list of tensors to be very slow. The function in case\nwas torch_split().\nThis issue has been fixed in v0.10.0, and relying on this behavior should be much\nfaster now. Here’s a minimal benchmark comparing both v0.9.1 with v0.10.0:\nbench::mark(\n  torch::torch_split(1:100000, split_size = 10)\n)\nWith v0.9.1 we get:\n# A tibble: 1 × 13\n  expression      min  median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  <bch:expr> <bch:tm> <bch:t>     <dbl> <bch:byt>    <dbl> <int> <dbl>   <bch:tm>\n1 x             322ms   350ms      2.85     397MB     24.3     2    17      701ms\n# ℹ 4 more variables: result <list>, memory <list>, time <list>, gc <list>\nwhile with v0.10.0:\n# A tibble: 1 × 13\n  expression      min  median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  <bch:expr> <bch:tm> <bch:t>     <dbl> <bch:byt>    <dbl> <int> <dbl>   <bch:tm>\n1 x              12ms  12.8ms      65.7     120MB     8.96    22     3      335ms\n# ℹ 4 more variables: result <list>, memory <list>, time <list>, gc <list>\nBuild system refactoring\nThe torch R package depends on LibLantern, a C interface to LibTorch. Lantern is part of\nthe torch repository, but until v0.9.1 one would need to build LibLantern in a separate\nstep before building the R package itself.\nThis approach had several downsides, including:\nInstalling the package from GitHub was not reliable/reproducible, as you would depend\non a transient pre-built binary.\nCommon devtools workflows like devtools::load_all() wouldn’t work, if the user didn’t build\nLantern before, which made it harder to contribute to torch.\nFrom now on, building LibLantern is part of the R package-building workflow, and can be enabled\nby setting the BUILD_LANTERN=1 environment variable. It’s not enabled by default, because\nbuilding Lantern requires cmake and other tools (specially if building the with GPU support),\nand using the pre-built binaries is preferable in those cases. With this environment variable set,\nusers can run devtools::load_all() to locally build and test torch.\nThis flag can also be used when installing torch dev versions from GitHub. If it’s set to 1,\nLantern will be built from source instead of installing the pre-built binaries, which should lead\nto better reproducibility with development versions.\nAlso, as part of these changes, we have improved the torch automatic installation process. It now has\nimproved error messages to help debugging issues related to the installation. It’s also easier to customize\nusing environment variables, see help(install_torch) for more information.\nFinal remarks\nThank you to all contributors to the torch ecosystem. This work would not be possible without\nall the helpful issues opened, PRs you created and your hard work.\nIf you are new to torch and want to learn more, we highly recommend the recently announced book ‘Deep Learning and Scientific Computing with R torch’.\nIf you want to start contributing to torch, feel free to reach out on GitHub and see our contributing guide.\nThe full changelog for this release can be found here.\n\n\n\n",
    "preview": "posts/2023-04-14-torch-0-10/images/torch.png",
    "last_modified": "2024-11-21T15:49:43+00:00",
    "input_file": {},
    "preview_width": 1408,
    "preview_height": 854
  },
  {
    "path": "posts/2023-04-13-denoising-diffusion/",
    "title": "De-noising Diffusion with torch",
    "description": "Currently, in generative deep learning, no other approach seems to outperform the family of diffusion models. Would you like to try for yourself? If so, our torch implementation of de-noising diffusion  provides an easy-to-use, easy-to-configure interface.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": {}
      },
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2023-04-13",
    "categories": [
      "Torch",
      "R",
      "Image Recognition & Image Processing",
      "Generative Models"
    ],
    "contents": "\n\nContents\nDiffusion models in context: Generative deep learning\nGenerative models: If you wanted to draw a mind map…\nZooming in: Diffusion models\nOur implementation – overview\n\nA Preamble, sort of\nAs we’re writing this – it’s April, 2023 – it is hard to overstate\nthe attention going to, the hopes associated with, and the fears\nsurrounding deep-learning-powered image and text generation. Impacts on\nsociety, politics, and human well-being deserve more than a short,\ndutiful paragraph. We thus defer appropriate treatment of this topic to\ndedicated publications, and would just like to say one thing: The more\nyou know, the better; the less you’ll be impressed by over-simplifying,\ncontext-neglecting statements made by public figures; the easier it will\nbe for you to take your own stance on the subject. That said, we begin.\n\nIn this post, we introduce an R torch implementation of De-noising\nDiffusion Implicit Models (J. Song, Meng, and Ermon (2020)). The code is on\nGitHub, and comes with\nan extensive README detailing everything from mathematical underpinnings\nvia implementation choices and code organization to model training and\nsample generation. Here, we give a high-level overview, situating the\nalgorithm in the broader context of generative deep learning. Please\nfeel free to consult the README for any details you’re particularly\ninterested in!\nDiffusion models in context: Generative deep learning\nIn generative deep learning, models are trained to generate new\nexemplars that could likely come from some familiar distribution: the\ndistribution of landscape images, say, or Polish verse. While diffusion\nis all the hype now, the last decade had much attention go to other\napproaches, or families of approaches. Let’s quickly enumerate some of\nthe most talked-about, and give a quick characterization.\nFirst, diffusion models themselves. Diffusion, the general term,\ndesignates entities (molecules, for example) spreading from areas of\nhigher concentration to lower-concentration ones, thereby increasing\nentropy. In other words, information is\nlost. In diffusion models, this information loss is intentional: In a\n“forward” process, a sample is taken and successively transformed into\n(Gaussian, usually) noise. A “reverse” process then is supposed to take\nan instance of noise, and sequentially de-noise it until it looks like\nit came from the original distribution. For sure, though, we can’t\nreverse the arrow of time? No, and that’s where deep learning comes in:\nDuring the forward process, the network learns what needs to be done for\n“reversal.”\nA totally different idea underlies what happens in GANs, Generative\nAdversarial Networks. In a GAN we have two agents at play, each trying\nto outsmart the other. One tries to generate samples that look as\nrealistic as could be; the other sets its energy into spotting the\nfakes. Ideally, they both get better over time, resulting in the desired\noutput (as well as a “regulator” who is not bad, but always a step\nbehind).\nThen, there’s VAEs: Variational Autoencoders. In a VAE, like in a\nGAN, there are two networks (an encoder and a decoder, this time).\nHowever, instead of having each strive to minimize their own cost\nfunction, training is subject to a single – though composite – loss.\nOne component makes sure that reconstructed samples closely resemble the\ninput; the other, that the latent code confirms to pre-imposed\nconstraints.\nLastly, let us mention flows (although these tend to be used for a\ndifferent purpose, see next section). A flow is a sequence of\ndifferentiable, invertible mappings from data to some “nice”\ndistribution, nice meaning “something we can easily sample, or obtain a\nlikelihood from.” With flows, like with diffusion, learning happens\nduring the forward stage. Invertibility, as well as differentiability,\nthen assure that we can go back to the input distribution we started\nwith.\nBefore we dive into diffusion, we sketch – very informally – some\naspects to consider when mentally mapping the space of generative\nmodels.\nGenerative models: If you wanted to draw a mind map…\nAbove, I’ve given rather technical characterizations of the different\napproaches: What is the overall setup, what do we optimize for…\nStaying on the technical side, we could look at established\ncategorizations such as likelihood-based vs. not-likelihood-based\nmodels. Likelihood-based models directly parameterize the data\ndistribution; the parameters are then fitted by maximizing the\nlikelihood of the data under the model. From the above-listed\narchitectures, this is the case with VAEs and flows; it is not with\nGANs.\nBut we can also take a different perspective – that of purpose.\nFirstly, are we interested in representation learning? That is, would we\nlike to condense the space of samples into a sparser one, one that\nexposes underlying features and gives hints at useful categorization? If\nso, VAEs are the classical candidates to look at.\nAlternatively, are we mainly interested in generation, and would like to\nsynthesize samples corresponding to different levels of coarse-graining?\nThen diffusion algorithms are a good choice. It has been shown that\n\n[…] representations learnt using different noise levels tend to\ncorrespond to different scales of features: the higher the noise\nlevel, the larger-scale the features that are captured.1\n\nAs a final example, what if we aren’t interested in synthesis, but would\nlike to assess if a given piece of data could likely be part of some\ndistribution? If so, flows might be an option.2\nZooming in: Diffusion models\nJust like about every deep-learning architecture, diffusion models\nconstitute a heterogeneous family. Here, let us just name a few of the\nmost en-vogue members.3\nWhen, above, we said that the idea of diffusion models was to\nsequentially transform an input into noise, then sequentially de-noise\nit again, we left open how that transformation is operationalized. This,\nin fact, is one area where rivaling approaches tend to differ.\nY. Song et al. (2020), for example, make use of a a stochastic differential\nequation (SDE) that maintains the desired distribution during the\ninformation-destroying forward phase. In stark contrast, other\napproaches, inspired by Ho, Jain, and Abbeel (2020), rely on Markov chains to realize state\ntransitions. The variant introduced here – J. Song, Meng, and Ermon (2020) – keeps the same\nspirit, but improves on efficiency.\nOur implementation – overview\nThe README provides a\nvery thorough introduction, covering (almost) everything from\ntheoretical background via implementation details to training procedure\nand tuning. Here, we just outline a few basic facts.\nAs already hinted at above, all the work happens during the forward\nstage. The network takes two inputs, the images as well as information\nabout the signal-to-noise ratio to be applied at every step in the\ncorruption process. That information may be encoded in various ways,\nand is then embedded, in some form, into a higher-dimensional space more\nconducive to learning. Here is how that could look, for two different types of scheduling/embedding:\n\n\n\nArchitecture-wise, inputs as well as intended outputs being images, the\nmain workhorse is a U-Net. It forms part of a top-level model that, for\neach input image, creates corrupted versions, corresponding to the noise\nrates requested, and runs the U-Net on them. From what is returned, it\ntries to deduce the noise level that was governing each instance.\nTraining then consists in getting those estimates to improve.\nModel trained, the reverse process – image generation – is\nstraightforward: It consists in recursive de-noising according to the\n(known) noise rate schedule. All in all, the complete process then might look like this:\n\n\n\nWrapping up, this post, by itself, is really just an invitation. To\nfind out more, check out the GitHub\nrepository. Should you\nneed additional motivation to do so, here are some flower images.\n\n\n\nThanks for reading!\n\n\n\nDieleman, Sander. 2022. “Diffusion Models Are Autoencoders.” https://benanne.github.io/2022/01/31/diffusion.html.\n\n\nHo, Jonathan, Ajay Jain, and Pieter Abbeel. 2020. “Denoising Diffusion Probabilistic Models.” https://doi.org/10.48550/ARXIV.2006.11239.\n\n\nSong, Jiaming, Chenlin Meng, and Stefano Ermon. 2020. “Denoising Diffusion Implicit Models.” https://doi.org/10.48550/ARXIV.2010.02502.\n\n\nSong, Yang, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2020. “Score-Based Generative Modeling Through Stochastic Differential Equations.” CoRR abs/2011.13456. https://arxiv.org/abs/2011.13456.\n\n\nFrom Dieleman (2022), an excellent high-level introduction\nto diffusion models.↩︎\nFor more information on this lesser-used family of algorithms, see\nhttps://blog.evjang.com/2018/01/nf1.html.↩︎\nEn vogue as of this writing (the usual caveat that comes with\nusing qualifiers like this).↩︎\n",
    "preview": "posts/2023-04-13-denoising-diffusion/images/flowers.png",
    "last_modified": "2024-11-21T15:52:44+00:00",
    "input_file": {},
    "preview_width": 672,
    "preview_height": 480
  },
  {
    "path": "posts/2023-04-05-deep-learning-scientific-computing-R-torch/",
    "title": "Deep Learning and Scientific Computing with R torch: the book",
    "description": "Please allow us to introduce Deep Learning and Scientific Computing with R torch. Released in e-book format today, and available freely online, this book starts out by introducing torch basics. From there, it moves on to various deep-learning use cases. Finally, it shows how to use torch for more general topics, such as matrix computations and the Fourier Transform.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2023-04-05",
    "categories": [
      "Torch",
      "R",
      "Meta",
      "Concepts",
      "Packages/Releases"
    ],
    "contents": "\n\nContents\nWhat’s in the book?\nWho’s it for?\nWhat do I get?\n\nFirst things first: Where can you get it? As of today, you can download the e-book or order a print copy from the publisher, CRC Press; the free online edition is here. There is, to my knowledge, no problem to perusing the online version – besides one: It doesn’t have the squirrel that’s on the book cover.\n\n\n\nSo if you’re a lover of amazing creatures…\nWhat’s in the book?\nDeep Learning and Scientific Computing with R torch has three parts.\nThe first covers the indispensible basics: tensors, and how to manipulate them; automatic differentiation, the sine qua non of deep learning; optimization, the strategy that drives most of what we call artificial intelligence; and neural-network modules, torch's way of encapsulating algorithmic flow. The focus is on understanding the concepts, on how things “work” – that’s why we do things like code a neural network from scratch, something you’ll probably never do in later use.\nFoundations laid, part two – considerably more sizeable – dives into deep-learning applications. It is here that the ecosystem surrounding core torch enters the spotlight. First, we see how luz automates and considerably simplifies many programming tasks related to network training, performance evaluation, and prediction. Making use of the wrappers and instrumentation facilities it provides, we next learn about two aspects of deep learning no real-world application can afford to neglect: How to make models generalize to unseen data, and how to accelerate training. Techniques we introduce keep re-appearing throughout the use cases we then look at: image classification and segmentation, regression on tabular data, time-series forecasting, and classifying speech utterances. It’s in working with images and sound that essential ecosystem libraries, namely, torchvision and torchaudio, make their appearance, to be used for domain-dependent functionality.\nIn part three, we move beyond deep learning, and explore how torch can figure in general mathematical or scientific applications. Prominent topics are regression using matrix decompositions, the Discrete Fourier Transform, and the Wavelet Transform. The primary goal here is to understand the underlying ideas, and why they are so important. That’s why, here just like in part one, we code algorithms from scratch, before introducing the speed-optimized torch equivalents.\nNow that you know about the book’s content, you may be asking:\nWho’s it for?\nIn short, Deep Learning and Scientific Computing with R torch – being the only comprehensive text, as of this writing, on this topic – addresses a wide audience. The hope is that there’s something in it for everyone (well, most everyone).\nIf you’ve never used torch, nor any other deep-learning framework, starting right from the beginning is the thing to do. No prior knowledge of deep learning is expected. The assumption is that you know some basic R, and are familiar with machine-learning terms such as supervised vs. unsupervised learning, training-validation-test set, et cetera. Having worked through part one, you’ll find that parts two and three – independently – continue right from where you left off.\nIf, on the other hand, you do have basic experience with torch and/or other automatic-differentiation frameworks, and are mostly interested in applied deep learning, you may be inclined to skim part one, and go to part two, checking out the applications that interest you most (or just browse, looking for inspiration). The domain-dependent examples were chosen to be rather generic and straightforward, so as to have the code generalize to a whole range of similar applications.\nFinally, if it was the “scientific computing” in the title that caught your attention, I certainly hope that part three has something for you! (As the book’s author, I may say that writing this part was an extremely satisfying, incredibly engaging experience.1) Part three really is where it makes sense to talk of “browsing” – its topics hardly depend on each other, just look around for what appeals to you.\nTo wrap up, then:\nWhat do I get?\nContent-wise, I think I can consider this question answered. If there were other books on torch with R, I’d probably stress two things: First, the already-referred-to focus on concepts and understanding. Second, the usefulness of the code examples. By using off-the-shelf datasets, and performing the usual types of tasks, we write code fit to serve as a start in your own applications – providing templates ready to copy-paste and adapt to a purpose.\nThanks for reading, and I hope you enjoy the book!\n\nthough challenging, as well (though … or because?)↩︎\n",
    "preview": "posts/2023-04-05-deep-learning-scientific-computing-R-torch/images/book.jpg",
    "last_modified": "2024-11-21T15:49:35+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-03-27-group-equivariant-cnn-2/",
    "title": "Implementing rotation equivariance: Group-equivariant CNN from scratch",
    "description": "We code up a simple group-equivariant convolutional neural network (GCNN) that is equivariant to rotation. The world may be upside down, but the network will know.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2023-03-27",
    "categories": [
      "Torch",
      "R",
      "Spatial Data",
      "Image Recognition & Image Processing"
    ],
    "contents": "\n\nContents\nStep 1: The symmetry group \\(C_4\\)\nStep 2: The lifting convolution\nStep 3: Group convolutions\nStep 4: Group-equivariant CNN\nRotated digits!\nA challenge\n\nConvolutional neural networks (CNNs) are great – they’re able to detect features in an image no matter where. Well, not exactly. They’re not indifferent to just any kind of movement. Shifting up or down, or left or right, is fine; rotating around an axis is not. That’s because of how convolution works: traverse by row, then traverse by column (or the other way round). If we want “more” (e.g., successful detection of an upside-down object), we need to extend convolution to an operation that is rotation-equivariant. An operation that is equivariant to some type of action will not only register the moved feature per se, but also, keep track of which concrete action made it appear where it is.\nThis is the second post in a series that introduces group-equivariant CNNs (GCNNs). The first was a high-level introduction to why we’d want them, and how they work. There, we introduced the key player, the symmetry group, which specifies what kinds of transformations are to be treated equivariantly. If you haven’t, please take a look at that post first, since here I’ll make use of terminology and concepts it introduced.\nToday, we code a simple GCNN from scratch. Code and presentation tightly follow a notebook provided as part of University of Amsterdam’s 2022 Deep Learning Course. They can’t be thanked enough for making available such excellent learning materials.\nIn what follows, my intent is to explain the general thinking, and how the resulting architecture is built up from smaller modules, each of which is assigned a clear purpose. For that reason, I won’t reproduce all the code here; instead, I’ll make use of the package gcnn. Its methods are heavily annotated; so to see some details, don’t hesitate to look at the code.\nAs of today, gcnn implements one symmetry group: \\(C_4\\), the one that serves as a running example throughout post one. It is straightforwardly extensible, though, making use of class hierarchies throughout.\nStep 1: The symmetry group \\(C_4\\)\nIn coding a GCNN, the first thing we need to provide is an implementation of the symmetry group we’d like to use. Here, it is \\(C_4\\), the four-element group that rotates by 90 degrees.\nWe can ask gcnn to create one for us, and inspect its elements.\n\n\n# remotes::install_github(\"skeydan/gcnn\")\nlibrary(gcnn)\nlibrary(torch)\n\nC_4 <- CyclicGroup(order = 4)\nelems <- C_4$elements()\nelems\n\n\ntorch_tensor\n 0.0000\n 1.5708\n 3.1416\n 4.7124\n[ CPUFloatType{4} ]\nElements are represented by their respective rotation angles: \\(0\\), \\(\\frac{\\pi}{2}\\), \\(\\pi\\), and \\(\\frac{3 \\pi}{2}\\).\nGroups are aware of the identity, and know how to construct an element’s inverse:\n\n\nC_4$identity\n\ng1 <- elems[2]\nC_4$inverse(g1)\n\n\ntorch_tensor\n 0\n[ CPUFloatType{1} ]\n\ntorch_tensor\n4.71239\n[ CPUFloatType{} ]\nHere, what we care about most is the group elements’ action. Implementation-wise, we need to distinguish between them acting on each other, and their action on the vector space \\(\\mathbb{R}^2\\), where our input images live. The former part is the easy one: It may simply be implemented by adding angles. In fact, this is what gcnn does when we ask it to let g1 act on g2:\n\n\ng2 <- elems[3]\n\n# in C_4$left_action_on_H(), H stands for the symmetry group\nC_4$left_action_on_H(torch_tensor(g1)$unsqueeze(1), torch_tensor(g2)$unsqueeze(1))\n\n\ntorch_tensor\n 4.7124\n[ CPUFloatType{1,1} ]\nWhat’s with the unsqueeze()s? Since \\(C_4\\)’s ultimate raison d’être is to be part of a neural network, left_action_on_H() works with batches of elements, not scalar tensors.\nThings are a bit less straightforward where the group action on \\(\\mathbb{R}^2\\) is concerned. Here, we need the concept of a group representation. This is an involved topic, which we won’t go into here. In our current context, it works about like this: We have an input signal, a tensor we’d like to operate on in some way. (That “some way” will be convolution, as we’ll see soon.) To render that operation group-equivariant, we first have the representation apply the inverse group action to the input. That accomplished, we go on with the operation as though nothing had happened.\nTo give a concrete example, let’s say the operation is a measurement. Imagine a runner, standing at the foot of some mountain trail, ready to run up the climb. We’d like to record their height. One option we have is to take the measurement, then let them run up. Our measurement will be as valid up the mountain as it was down here. Alternatively, we might be polite and not make them wait. Once they’re up there, we ask them to come down, and when they’re back, we measure their height. The result is the same: Body height is equivariant (more than that: invariant, even) to the action of running up or down. (Of course, height is a pretty dull measure. But something more interesting, such as heart rate, would not have worked so well in this example.)\nReturning to the implementation, it turns out that group actions are encoded as matrices. There is one matrix for each group element. For \\(C_4\\), the so-called standard representation is a rotation matrix:\n\\[\n\\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix}\n\\]\nIn gcnn, the function applying that matrix is left_action_on_R2(). Like its sibling, it is designed to work with batches (of group elements as well as \\(\\mathbb{R}^2\\) vectors). Technically, what it does is rotate the grid the image is defined on, and then, re-sample the image. To make this more concrete, that method’s code looks about as follows.\nHere is a goat.\n\n\nimg_path <- system.file(\"imgs\", \"z.jpg\", package = \"gcnn\")\nimg <- torchvision::base_loader(img_path) |> torchvision::transform_to_tensor()\nimg$permute(c(2, 3, 1)) |> as.array() |> as.raster() |> plot()\n\n\n\n\n\nFirst, we call C_4$left_action_on_R2() to rotate the grid.\n\n\n# Grid shape is [2, 1024, 1024], for a 2d, 1024 x 1024 image.\nimg_grid_R2 <- torch::torch_stack(torch::torch_meshgrid(\n    list(\n      torch::torch_linspace(-1, 1, dim(img)[2]),\n      torch::torch_linspace(-1, 1, dim(img)[3])\n    )\n))\n\n# Transform the image grid with the matrix representation of some group element.\ntransformed_grid <- C_4$left_action_on_R2(C_4$inverse(g1)$unsqueeze(1), img_grid_R2)\n\n\nSecond, we re-sample the image on the transformed grid. The goat now looks up to the sky.\n\n\ntransformed_img <- torch::nnf_grid_sample(\n  img$unsqueeze(1), transformed_grid,\n  align_corners = TRUE, mode = \"bilinear\", padding_mode = \"zeros\"\n)\n\ntransformed_img[1,..]$permute(c(2, 3, 1)) |> as.array() |> as.raster() |> plot()\n\n\n\n\n\nStep 2: The lifting convolution\nWe want to make use of existing, efficient torch functionality as much as possible. Concretely, we want to use nn_conv2d(). What we need, though, is a convolution kernel that’s equivariant not just to translation, but also to the action of \\(C_4\\). This can be achieved by having one kernel for each possible rotation.\nImplementing that idea is exactly what LiftingConvolution does. The principle is the same as before: First, the grid is rotated, and then, the kernel (weight matrix) is re-sampled to the transformed grid.\nWhy, though, call this a lifting convolution? The usual convolution kernel operates on \\(\\mathbb{R}^2\\); while our extended version operates on combinations of \\(\\mathbb{R}^2\\) and \\(C_4\\). In math speak, it has been lifted to the semi-direct product \\(\\mathbb{R}^2\\rtimes C_4\\).\n\n\nlifting_conv <- LiftingConvolution(\n    group = CyclicGroup(order = 4),\n    kernel_size = 5,\n    in_channels = 3,\n    out_channels = 8\n  )\n\nx <- torch::torch_randn(c(2, 3, 32, 32))\ny <- lifting_conv(x)\ny$shape\n\n\n[1]  2  8  4 28 28\nSince, internally, LiftingConvolution uses an additional dimension to realize the product of translations and rotations, the output is not four-, but five-dimensional.\nStep 3: Group convolutions\nNow that we’re in “group-extended space”, we can chain a number of layers where both input and output are group convolution layers. For example:\n\n\ngroup_conv <- GroupConvolution(\n  group = CyclicGroup(order = 4),\n    kernel_size = 5,\n    in_channels = 8,\n    out_channels = 16\n)\n\nz <- group_conv(y)\nz$shape\n\n\n[1]  2 16  4 24 24\nAll that remains to be done is package this up. That’s what gcnn::GroupEquivariantCNN() does.\nStep 4: Group-equivariant CNN\nWe can call GroupEquivariantCNN() like so.\n\n\ncnn <- GroupEquivariantCNN(\n    group = CyclicGroup(order = 4),\n    kernel_size = 5,\n    in_channels = 1,\n    out_channels = 1,\n    num_hidden = 2, # number of group convolutions\n    hidden_channels = 16 # number of channels per group conv layer\n)\n\nimg <- torch::torch_randn(c(4, 1, 32, 32))\ncnn(img)$shape\n\n\n[1] 4 1\nAt casual glance, this GroupEquivariantCNN looks like any old CNN … weren’t it for the group argument.\nNow, when we inspect its output, we see that the additional dimension is gone. That’s because after a sequence of group-to-group convolution layers, the module projects down to a representation that, for each batch item, retains channels only. It thus averages not just over locations – as we normally do – but over the group dimension as well. A final linear layer will then provide the requested classifier output (of dimension out_channels).\nAnd there we have the complete architecture. It is time for a real-world(ish) test.\nRotated digits!\nThe idea is to train two convnets, a “normal” CNN and a group-equivariant one, on the usual MNIST training set. Then, both are evaluated on an augmented test set where each image is randomly rotated by a continuous rotation between 0 and 360 degrees. We don’t expect GroupEquivariantCNN to be “perfect” – not if we equip with \\(C_4\\) as a symmetry group. Strictly, with \\(C_4\\), equivariance extends over four positions only. But we do hope it will perform significantly better than the shift-equivariant-only standard architecture.\nFirst, we prepare the data; in particular, the augmented test set.\n\n\ndir <- \"/tmp/mnist\"\n\ntrain_ds <- torchvision::mnist_dataset(\n  dir,\n  download = TRUE,\n  transform = torchvision::transform_to_tensor\n)\n\ntest_ds <- torchvision::mnist_dataset(\n  dir,\n  train = FALSE,\n  transform = function(x) {\n    x |>\n      torchvision::transform_to_tensor() |>\n      torchvision::transform_random_rotation(\n        degrees = c(0, 360),\n        resample = 2,\n        fill = 0\n      )\n  }\n)\n\ntrain_dl <- dataloader(train_ds, batch_size = 128, shuffle = TRUE)\ntest_dl <- dataloader(test_ds, batch_size = 128)\n\n\nHow does it look?\n\n\ntest_images <- coro::collect(\n  test_dl, 1\n)[[1]]$x[1:32, 1, , ] |> as.array()\n\npar(mfrow = c(4, 8), mar = rep(0, 4), mai = rep(0, 4))\ntest_images |>\n  purrr::array_tree(1) |>\n  purrr::map(as.raster) |>\n  purrr::iwalk(~ {\n    plot(.x)\n  })\n\n\n\n\n\nWe first define and train a conventional CNN. It is as similar to GroupEquivariantCNN(), architecture-wise, as possible, and is given twice the number of hidden channels, so as to have comparable capacity overall.\n\n\n default_cnn <- nn_module(\n   \"default_cnn\",\n   initialize = function(kernel_size, in_channels, out_channels, num_hidden, hidden_channels) {\n     self$conv1 <- torch::nn_conv2d(in_channels, hidden_channels, kernel_size)\n     self$convs <- torch::nn_module_list()\n     for (i in 1:num_hidden) {\n       self$convs$append(torch::nn_conv2d(hidden_channels, hidden_channels, kernel_size))\n     }\n     self$avg_pool <- torch::nn_adaptive_avg_pool2d(1)\n     self$final_linear <- torch::nn_linear(hidden_channels, out_channels)\n   },\n   forward = function(x) {\n     x <- x |>\n       self$conv1() |>\n       (\\(.) torch::nnf_layer_norm(., .$shape[2:4]))() |>\n       torch::nnf_relu()\n     for (i in 1:(length(self$convs))) {\n       x <- x |>\n         self$convs[[i]]() |>\n         (\\(.) torch::nnf_layer_norm(., .$shape[2:4]))() |>\n         torch::nnf_relu()\n     }\n     x <- x |>\n       self$avg_pool() |>\n       torch::torch_squeeze() |>\n       self$final_linear()\n     x\n   }\n )\n\nfitted <- default_cnn |>\n    luz::setup(\n      loss = torch::nn_cross_entropy_loss(),\n      optimizer = torch::optim_adam,\n      metrics = list(\n        luz::luz_metric_accuracy()\n      )\n    ) |>\n    luz::set_hparams(\n      kernel_size = 5,\n      in_channels = 1,\n      out_channels = 10,\n      num_hidden = 4,\n      hidden_channels = 32\n    ) %>%\n    luz::set_opt_hparams(lr = 1e-2, weight_decay = 1e-4) |>\n    luz::fit(train_dl, epochs = 10, valid_data = test_dl) \n\n\nTrain metrics: Loss: 0.0498 - Acc: 0.9843\nValid metrics: Loss: 3.2445 - Acc: 0.4479\nUnsurprisingly, accuracy on the test set is not that great.\nNext, we train the group-equivariant version.\n\n\nfitted <- GroupEquivariantCNN |>\n  luz::setup(\n    loss = torch::nn_cross_entropy_loss(),\n    optimizer = torch::optim_adam,\n    metrics = list(\n      luz::luz_metric_accuracy()\n    )\n  ) |>\n  luz::set_hparams(\n    group = CyclicGroup(order = 4),\n    kernel_size = 5,\n    in_channels = 1,\n    out_channels = 10,\n    num_hidden = 4,\n    hidden_channels = 16\n  ) |>\n  luz::set_opt_hparams(lr = 1e-2, weight_decay = 1e-4) |>\n  luz::fit(train_dl, epochs = 10, valid_data = test_dl)\n\n\nTrain metrics: Loss: 0.1102 - Acc: 0.9667\nValid metrics: Loss: 0.4969 - Acc: 0.8549\nFor the group-equivariant CNN, accuracies on test and training sets are a lot closer. That is a nice result! Let’s wrap up today’s exploit resuming a thought from the first, more high-level post.\nA challenge\nGoing back to the augmented test set, or rather, the samples of digits displayed, we notice a problem. In row two, column four, there is a digit that “under normal circumstances”, should be a 9, but, most probably, is an upside-down 6. (To a human, what suggests this is the squiggle-like thing that seems to be found more often with sixes than with nines.) However, you could ask: does this have to be a problem? Maybe the network just needs to learn the subtleties, the kinds of things a human would spot?\nThe way I view it, it all depends on the context: What really should be accomplished, and how an application is going to be used. With digits on a letter, I’d see no reason why a single digit should appear upside-down; accordingly, full rotation equivariance would be counter-productive. In a nutshell, we arrive at the same canonical imperative advocates of fair, just machine learning keep reminding us of:\n\nAlways think of the way an application is going to be used!\n\nIn our case, though, there is another aspect to this, a technical one. gcnn::GroupEquivariantCNN() is a simple wrapper, in that its layers all make use of the same symmetry group. In principle, there is no need to do this. With more coding effort, different groups can be used depending on a layer’s position in the feature-detection hierarchy.\nHere, let me finally tell you why I chose the goat picture. The goat is seen through a red-and-white fence, a pattern – slightly rotated, due to the viewing angle – made up of squares (or edges, if you like). Now, for such a fence, types of rotation equivariance such as that encoded by \\(C_4\\) make a lot of sense. The goat itself, though, we’d rather not have look up to the sky, the way I illustrated \\(C_4\\) action before. Thus, what we’d do in a real-world image-classification task is use rather flexible layers at the bottom, and increasingly restrained layers at the top of the hierarchy.\nThanks for reading!\nPhoto by Marjan Blan | @marjanblan on Unsplash\n\n\n\n",
    "preview": "posts/2023-03-27-group-equivariant-cnn-2/images/preview.jpg",
    "last_modified": "2024-11-21T15:52:33+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-03-09-group-equivariant-cnn-1/",
    "title": "Upside down, a cat's still a cat: Evolving image recognition with Geometric Deep Learning",
    "description": "In this first in a series of posts on group-equivariant convolutional neural networks (GCNNs), meet the main actors — groups — and concepts (equivariance). With GCNNs, we finally revisit the topic of Geometric Deep Learning, a principled, math-driven approach to neural networks that has consistently been rising in scope and impact.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2023-03-09",
    "categories": [
      "Torch",
      "R",
      "Spatial Data",
      "Concepts"
    ],
    "contents": "\n\nContents\nFrom alchemy to science: Geometric Deep Learning in two minutes\nThe “group” in group-equivariance\nGroups from symmetries\nViewing groups through the action lens\n\nOutlook: Group-equivariant CNN\n\nThis is the first in a series of posts on group-equivariant convolutional neural networks (GCNNs). Today, we keep it short, high-level, and conceptual; examples and implementations will follow. In looking at GCNNs, we are resuming a topic we first wrote about in 2021: Geometric Deep Learning, a principled, math-driven approach to network design that, since then, has only risen in scope and impact.\nFrom alchemy to science: Geometric Deep Learning in two minutes\nIn a nutshell, Geometric Deep Learning is all about deriving network structure from two things: the domain, and the task. The posts will go into a lot of detail, but let me give a quick preview here:\nBy domain, I’m referring to the underlying physical space, and the way it is represented in the input data. For example, images are usually coded as a two-dimensional grid, with values indicating pixel intensities.1\nThe task is what we’re training the network to do: classification, say, or segmentation. Tasks may be different at different stages in the architecture. At each stage, the task in question will have its word to say about how layer design should look.\nFor instance, take MNIST. The dataset consists of images of ten digits, 0 to 10, all gray-scale. The task – unsurprisingly – is to assign each image the digit represented.\nFirst, consider the domain. A \\(7\\) is a \\(7\\) wherever it appears on the grid. We thus need an operation that is translation-equivariant: It flexibly adapts to shifts (translations) in its input. More concretely, in our context, equivariant operations are able to detect some object’s properties even if that object has been moved, vertically and/or horizontally, to another location. Convolution, ubiquitous not just in deep learning, is just such a shift-equivariant operation.\nLet me call special attention to the fact that, in equivariance, the essential thing is that “flexible adaptation.” Translation-equivariant operations do care about an object’s new position; they record a feature not abstractly, but at the object’s new position. To see why this is important, consider the network as a whole. When we compose convolutions, we build a hierarchy of feature detectors. That hierarchy should be functional no matter where in the image. In addition, it has to be consistent: Location information needs to be preserved between layers.\nTerminology-wise, thus, it is important to distinguish equivariance from invariance. An invariant operation, in our context, would still be able to spot a feature wherever it occurs; however, it would happily forget where that feature happened to be. Clearly, then, to build up a hierarchy of features, translation-invariance is not enough.\nWhat we’ve done right now is derive a requirement from the domain, the input grid. What about the task? If, finally, all we’re supposed to do is name the digit, now suddenly location does not matter anymore. In other words, once the hierarchy exists, invariance is enough. In neural networks, pooling is an operation that forgets about (spatial) detail. It only cares about the mean, say, or the maximum value itself. This is what makes it suited to “summing up” information about a region, or a complete image, if at the end we only care about returning a class label.\nIn a nutshell, we were able to formulate a design wishlist based on (1) what we’re given and (2) what we’re tasked with.\nAfter this high-level sketch of Geometric Deep Learning, we zoom in on this series of posts’ designated topic: group-equivariant convolutional neural networks.\nThe why of “equivariant” should not, by now, pose too much of a riddle. What about that “group” prefix, though?\nThe “group” in group-equivariance\nAs you may have guessed from the introduction, talking of “principled” and “math-driven”, this really is about groups in the “math sense.” Depending on your background, the last time you heard about groups was in school, and with not even a hint at why they matter. I’m certainly not qualified to summarize the whole richness of what they’re good for, but I hope that by the end of this post, their importance in deep learning will make intuitive sense.\nGroups from symmetries\nHere is a square.\n\n\n\nNow close your eyes.\nNow look again. Did something happen to the square?\n\n\n\nYou can’t tell. Maybe it was rotated; maybe it was not. On the other hand, what if the vertices were numbered?\n\n\n\nNow you’d know.\nWithout the numbering, could I have rotated the square in any way I wanted? Evidently not. This would not go through unnoticed:\n\n\n\nThere are exactly four ways I could have rotated the square without raising suspicion. Those ways can be referred to in different ways; one simple way is by degree of rotation: 90, 180, or 270 degrees. Why not more? Any further addition of 90 degrees would result in a configuration we’ve already seen.\n\n\n\nThe above picture shows three squares, but I’ve listed three possible rotations. What about the situation on the left, the one I’ve taken as an initial state? It could be reached by rotating 360 degrees (or twice that, or thrice, or …) But the way this is handled, in math, is by treating it as some sort of “null rotation”, analogously to how \\(0\\) acts in addition, \\(1\\) in multiplication, or the identity matrix in linear algebra.\nAltogether, we thus have four actions that could be performed on the square (an un-numbered square!) that would leave it as-is, or invariant. These are called the symmetries of the square. A symmetry, in math/physics, is a quantity that remains the same no matter what happens as time evolves. And this is where groups come in. Groups – concretely, their elements – effectuate actions like rotation.\nBefore I spell out how, let me give another example. Take this sphere.\n\n\n\nHow many symmetries does a sphere have? Infinitely many. This implies that whatever group is chosen to act on the square, it won’t be much good to represent the symmetries of the sphere.\nViewing groups through the action lens\nFollowing these examples, let me generalize. Here is typical definition. 2\n\nA group \\(G\\) is a finite or infinite set of elements together with a binary operation (called the group operation) that together satisfy the four fundamental properties of closure, associativity, the identity property, and the inverse property. The operation with respect to which a group is defined is often called the “group operation,” and a set is said to be a group “under” this operation. Elements \\(A\\), \\(B\\), \\(C\\), … with binary operation between \\(A\\) and \\(B\\) denoted \\(AB\\) form a group if\nClosure: If \\(A\\) and \\(B\\) are two elements in \\(G\\), then the product \\(AB\\) is also in \\(G\\).\nAssociativity: The defined multiplication is associative, i.e., for all \\(A\\),\\(B\\),\\(C\\) in \\(G\\), \\((AB)C=A(BC)\\).\nIdentity: There is an identity element \\(I\\) (a.k.a. \\(1\\), \\(E\\), or \\(e\\)) such that \\(IA=AI=A\\) for every element \\(A\\) in \\(G\\).\nInverse: There must be an inverse (a.k.a. reciprocal) of each element. Therefore, for each element \\(A\\) of \\(G\\), the set contains an element \\(B=A^{-1}\\) such that \\(AA^{-1}=A^{-1}A=I\\).\n\nIn action-speak, group elements specify allowable actions; or more precisely, ones that are distinguishable from each other. Two actions can be composed; that’s the “binary operation”. The requirements now make intuitive sense:\nA combination of two actions – two rotations, say – is still an action of the same type (a rotation).\nIf we have three such actions, it doesn’t matter how we group them. (Their order of application has to remain the same, though.)\nOne possible action is always the “null action”. (Just like in life.) As to “doing nothing”, it doesn’t make a difference if that happens before or after a “something”; that “something” is always the final result.\nEvery action needs to have an “undo button”. In the squares example, if I rotate by 180 degrees, and then, by 180 degrees again, I am back in the original state. It is if I had done nothing.\nResuming a more “birds-eye view”, what we’ve seen right now is the definition of a group by how its elements act on each other. But if groups are to matter “in the real world”, they need to act on something outside (neural network components, for example). How this works is the topic of the following posts, but I’ll briefly outline the intuition here.\nOutlook: Group-equivariant CNN\nAbove, we noted that, in image classification, a translation-invariant operation (like convolution) is needed: A \\(1\\) is a \\(1\\) whether moved horizontally, vertically, both ways, or not at all. What about rotations, though? Standing on its head, a digit is still what it is. Conventional convolution does not support this type of action.\nWe can add to our architectural wishlist by specifying a symmetry group. What group? If we wanted to detect squares aligned to the axes, a suitable group would be \\(C_4\\), the cyclic group of order four. (Above, we saw that we needed four elements, and that we could cycle through the group.) If, on the other hand, we don’t care about alignment, we’d want any position to count. In principle, we should end up in the same situation as we did with the sphere. However, images live on discrete grids; there won’t be an unlimited number of rotations in practice.\nWith more realistic applications, we need to think more carefully. Take digits. When is a number “the same”? For one, it depends on the context. Were it about a hand-written address on an envelope, would we accept a \\(7\\) as such had it been rotated by 90 degrees? Maybe. (Although we might wonder what would make someone change ball-pen position for just a single digit.) What about a \\(7\\) standing on its head? On top of similar psychological considerations, we should be seriously unsure about the intended message, and, at least, down-weight the data point were it part of our training set.\nImportantly, it also depends on the digit itself. A \\(6\\), upside-down, is a \\(9\\).\nZooming in on neural networks, there is room for yet more complexity. We know that CNNs build up a hierarchy of features, starting from simple ones, like edges and corners. Even if, for later layers, we may not want rotation equivariance, we would still like to have it in the initial set of layers. (The output layer – we’ve hinted at that already – is to be considered separately in any case, since its requirements result from the specifics of what we’re tasked with.)\nThat’s it for today. Hopefully, I’ve managed to illuminate a bit of why we would want to have group-equivariant neural networks. The question remains: How do we get them? This is what the subsequent posts in the series will be about.\nTill then, and thanks for reading!\nPhoto by Ihor OINUA on Unsplash\n\nThat’s when the image has a single channel. Otherwise, there are a number of grids, each mapped to a separate channel.↩︎\nFrom Wolfram Alpha (beginning of article).↩︎\n",
    "preview": "posts/2023-03-09-group-equivariant-cnn-1/images/preview.jpg",
    "last_modified": "2024-11-21T15:54:12+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-01-19-torchwavelets/",
    "title": "AO, NAO, ENSO: A wavelet analysis example",
    "description": "El Niño-Southern Oscillation (ENSO), North Atlantic Oscillation (NAO), and Arctic Oscillation (AO) are atmospheric phenomena of global impact that strongly affect people's lives. ENSO, first and foremost, brings with it floods, droughts, and ensuing poverty, in developing countries in the Southern Hemisphere. Here, we use the new torchwavelets package to comparatively inspect patterns in the three series.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2023-01-19",
    "categories": [
      "Torch",
      "R",
      "Packages/Releases",
      "Time Series"
    ],
    "contents": "\n\nContents\nThree oscillations\nAnalysis: ENSO\nAnalysis: NAO\nAnalysis: AO\nConclusion\n\nRecently, we showed how to use torch for wavelet analysis. A member of the family of spectral analysis methods, wavelet analysis bears some similarity to the Fourier Transform, and specifically, to its popular two-dimensional application, the spectrogram.\nAs explained in that book excerpt, though, there are significant differences. For the purposes of the current post, it suffices to know that frequency-domain patterns are discovered by having a little “wave” (that, really, can be of any shape) “slide” over the data, computing degree of match (or mismatch) in the neighborhood of every sample.\nWith this post, then, my goal is two-fold.\nFirst, to introduce torchwavelets, a tiny, yet useful package that automates all of the essential steps involved. Compared to the Fourier Transform and its applications, the topic of wavelets is rather “chaotic” – meaning, it enjoys much less shared terminology, and much less shared practice. Consequently, it makes sense for implementations to follow established, community-embraced approaches, whenever such are available and well documented. With torchwavelets, we provide an implementation of Torrence and Compo’s 1998 “Practical Guide to Wavelet Analysis” (Torrence and Compo (1998)), an oft-cited paper that proved influential across a wide range of application domains. Code-wise, our package is mostly a port of Tom Runia’s PyTorch implementation, itself based on a prior implementation by Aaron O’Leary.\nSecond, to show an attractive use case of wavelet analysis in an area of great scientific interest and tremendous social importance (meteorology/climatology). Being by no means an expert myself, I’d hope this could be inspiring to people working in these fields, as well as to scientists and analysts in other areas where temporal data arise.\nConcretely, what we’ll do is take three different atmospheric phenomena – El Niño–Southern Oscillation (ENSO), North Atlantic Oscillation (NAO), and Arctic Oscillation (AO) – and investigate them using wavelet analysis. In each case, we also look at the overall frequency spectrum, given by the Discrete Fourier Transform (DFT), as well as a classic time-series decomposition into trend, seasonal components, and remainder.\nThree oscillations\nBy far the best-known – the most infamous, I should say – among the three is El Niño–Southern Oscillation (ENSO), a.k.a. El Niño/La Niña. The term refers to a changing pattern of sea surface temperatures and sea-level pressures occurring in the equatorial Pacific. Both El Niño and La Niña can and do have catastrophic impact on people’s lives, most notably, for people in developing countries west and east of the Pacific.\nEl Niño occurs when surface water temperatures in the eastern Pacific are higher than normal, and the strong winds that normally blow from east to west are unusually weak. From April to October, this leads to hot, extremely wet weather conditions along the coasts of northern Peru and Ecuador, continually resulting in major floods. La Niña, on the other hand, causes a drop in sea surface temperatures over Southeast Asia as well as heavy rains over Malaysia, the Philippines, and Indonesia. While these are the areas most gravely impacted, changes in ENSO reverberate across the globe.\nLess well known than ENSO, but highly influential as well, is the North Atlantic Oscillation (NAO). It strongly affects winter weather in Europe, Greenland, and North America. Its two states relate to the size of the pressure difference between the Icelandic High and the Azores Low. When the pressure difference is high, the jet stream – those strong westerly winds that blow between North America and Northern Europe – is yet stronger than normal, leading to warm, wet European winters and calmer-than-normal conditions in Eastern North America. With a lower-than-normal pressure difference, however, the American East tends to incur more heavy storms and cold-air outbreaks, while winters in Northern Europe are colder and more dry.\nFinally, the Arctic Oscillation (AO) is a ring-like pattern of sea-level pressure anomalies centered at the North Pole. (Its Southern-hemisphere equivalent is the Antarctic Oscillation.) AO’s influence extends beyond the Arctic Circle, however; it is indicative of whether and how much Arctic air flows down into the middle latitudes. AO and NAO are strongly related, and might designate the same physical phenomenon at a fundamental level.\nNow, let’s make these characterizations more concrete by looking at actual data.\nAnalysis: ENSO\nWe begin with the best-known of these phenomena: ENSO. Data are available from 1854 onwards; however, for comparability with AO, we discard all records prior to January, 1950. For analysis, we pick NINO34_MEAN, the monthly average sea surface temperature in the Niño 3.4 region (i.e., the area between 5° South, 5° North, 190° East, and 240° East). Finally, we convert to a tsibble, the format expected by feasts::STL().\n\n\nlibrary(tidyverse)\nlibrary(tsibble)\n\ndownload.file(\n  \"https://bmcnoldy.rsmas.miami.edu/tropics/oni/ONI_NINO34_1854-2022.txt\",\n  destfile = \"ONI_NINO34_1854-2022.txt\"\n)\n\nenso <- read_table(\"ONI_NINO34_1854-2022.txt\", skip = 9) %>%\n  mutate(x = yearmonth(as.Date(paste0(YEAR, \"-\", `MON/MMM`, \"-01\")))) %>%\n  select(x, enso = NINO34_MEAN) %>%\n  filter(x >= yearmonth(\"1950-01\"), x <= yearmonth(\"2022-09\")) %>%\n  as_tsibble(index = x)\n\nenso\n\n\n# A tsibble: 873 x 2 [1M]\n          x  enso\n      <mth> <dbl>\n 1 1950 Jan  24.6\n 2 1950 Feb  25.1\n 3 1950 Mar  25.9\n 4 1950 Apr  26.3\n 5 1950 May  26.2\n 6 1950 Jun  26.5\n 7 1950 Jul  26.3\n 8 1950 Aug  25.9\n 9 1950 Sep  25.7\n10 1950 Oct  25.7\n# … with 863 more rows\nAs already announced, we want to look at seasonal decomposition, as well. In terms of seasonal periodicity, what do we expect? Unless told otherwise, feasts::STL() will happily pick a window size for us. However, there’ll likely be several important frequencies in the data. (Not wanting to ruin the suspense, but for AO and NAO, this will definitely be the case!). Besides, we want to compute the Fourier Transform anyway, so why not do that first?\nHere is the power spectrum:\n\n\nlibrary(torch)\nfft <- torch_fft_fft(as.numeric(scale(enso$enso)))\n\n\nIn the below plot, the x axis corresponds to frequencies, expressed as “number of times per year.” We only display frequencies up to and including the Nyquist frequency, i.e., half the sampling rate, which in our case is 12 (per year).\n\n\nnum_samples <- nrow(enso)\nnyquist_cutoff <- ceiling(num_samples / 2) # highest discernible frequency\nbins_below_nyquist <- 0:nyquist_cutoff\n\nsampling_rate <- 12 # per year\nfrequencies_per_bin <- sampling_rate / num_samples\nfrequencies <- frequencies_per_bin * bins_below_nyquist\n\ndf <- data.frame(f = frequencies, y = as.numeric(fft[1:(nyquist_cutoff + 1)]$abs()))\ndf %>% ggplot(aes(f, y)) +\n  geom_line() +\n  xlab(\"frequency (per year)\") +\n  ylab(\"magnitude\") +\n  ggtitle(\"Spectrum of Niño 3.4 data\")\n\n\nFrequency spectrum of monthly average sea surface temperature in the Niño 3.4 region, 1950 to present.There is one dominant frequency, corresponding to about once a year. From this component alone, we’d expect one El Niño event – or equivalently, one La Niña – per year. But let’s locate important frequencies more precisely. With not many other periodicities standing out, we may as well restrict ourselves to three:\n\n\nstrongest <- torch_topk(fft[1:(nyquist_cutoff/2)]$abs(), 3)\nstrongest\n\n\n[[1]]\ntorch_tensor\n233.9855\n172.2784\n142.3784\n[ CPUFloatType{3} ]\n\n[[2]]\ntorch_tensor\n74\n21\n7\n[ CPULongType{3} ]\nWhat we have here are the magnitudes of the dominant components, as well as their respective bins in the spectrum. Let’s see which actual frequencies these correspond to:\n\n\nimportant_freqs <- frequencies[as.numeric(strongest[[2]])]\nimportant_freqs\n\n\n[1] 1.00343643 0.27491409 0.08247423 \nThat’s once per year, once per quarter, and once every twelve years, approximately. Or, expressed as periodicity, in terms of months (i.e., how many months are there in a period):\n\n\nnum_observations_in_season <- 12/important_freqs  \nnum_observations_in_season\n\n\n[1] 11.95890  43.65000 145.50000  \nWe now pass these to feasts::STL(), to obtain a five-fold decomposition into trend, seasonal components, and remainder.\n\n\nlibrary(feasts)\nenso %>%\n  model(STL(enso ~ season(period = 12) + season(period = 44) +\n              season(period = 145))) %>%\n  components() %>%\n  autoplot()\n\n\nDecomposition of ENSO data into trend, seasonal components, and remainder by feasts::STL().According to Loess decomposition, there still is significant noise in the data – the remainder remaining high despite our hinting at important seasonalities. In fact, there is no big surprise in that: Looking back at the DFT output, not only are there many, close to one another, low- and lowish-frequency components, but in addition, high-frequency components just won’t cease to contribute. And really, as of today, ENSO forecasting – tremendously important in terms of human impact – is focused on predicting oscillation state just a year in advance. This will be interesting to keep in mind for when we proceed to the other series – as you’ll see, it’ll only get worse.\nBy now, we’re well informed about how dominant temporal rhythms determine, or fail to determine, what actually happens in atmosphere and ocean. But we don’t know anything about whether, and how, those rhythms may have varied in strength over the time span considered. This is where wavelet analysis comes in.\nIn torchwavelets, the central operation is a call to wavelet_transform(), to instantiate an object that takes care of all required operations. One argument is required: signal_length, the number of data points in the series. And one of the defaults we need to override: dt, the time between samples, expressed in the unit we’re working with. In our case, that’s year, and, having monthly samples, we need to pass a value of 1/12. With all other defaults untouched, analysis will be done using the Morlet wavelet (available alternatives are Mexican Hat and Paul), and the transform will be computed in the Fourier domain (the fastest way, unless you have a GPU).\n\n\nlibrary(torchwavelets)\nenso_idx <- enso$enso %>% as.numeric() %>% torch_tensor()\ndt <- 1/12\nwtf <- wavelet_transform(length(enso_idx), dt = dt)\n\n\nA call to power() will then compute the wavelet transform:\n\n\npower_spectrum <- wtf$power(enso_idx)\npower_spectrum$shape\n\n\n[1]  71 873\nThe result is two-dimensional. The second dimension holds measurement times, i.e., the months between January, 1950 and September, 2022. The first dimension warrants some more explanation.\nNamely, we have here the set of scales the transform has been computed for. If you’re familiar with the Fourier Transform and its analogue, the spectrogram, you’ll probably think in terms of time versus frequency. With wavelets, there is an additional parameter, the scale, that determines the spread of the analysis pattern.\n\nThe aforementioned book excerpt discusses this in detail.\nSome wavelets have both a scale and a frequency, in which case these can interact in complex ways. Others are defined such that no separate frequency appears. In the latter case, you immediately end up with the time vs. scale layout we see in wavelet diagrams (scaleograms). In the former, most software hides the complexity by merging scale and frequency into one, leaving just scale as a user-visible parameter. In torchwavelets, too, the wavelet frequency (if existent) has been “streamlined away.” Consequently, we’ll end up plotting time versus scale, as well. I’ll say more when we actually see such a scaleogram.\nFor visualization, we transpose the data and put it into a ggplot-friendly format:\n\n\ntimes <- lubridate::year(enso$x) + lubridate::month(enso$x) / 12\nscales <- as.numeric(wtf$scales)\n\ndf <- as_tibble(as.matrix(power_spectrum$t()), .name_repair = \"universal\") %>%\n  mutate(time = times) %>%\n  pivot_longer(!time, names_to = \"scale\", values_to = \"power\") %>%\n  mutate(scale = scales[scale %>%\n    str_remove(\"[\\\\.]{3}\") %>%\n    as.numeric()])\ndf %>% glimpse()\n\n\nRows: 61,983\nColumns: 3\n$ time  <dbl> 1950.083, 1950.083, 1950.083, 1950.083, 195…\n$ scale <dbl> 0.1613356, 0.1759377, 0.1918614, 0.2092263,…\n$ power <dbl> 0.03617507, 0.05985500, 0.07948010, 0.09819…\nThere is one additional piece of information to be incorporated, still: the so-called “cone of influence” (COI). Visually, this is a shading that tells us which part of the plot reflects incomplete, and thus, unreliable and to-be-disregarded, data. Namely, the bigger the scale, the more spread-out the analysis wavelet, and the more incomplete the overlap at the borders of the series when the wavelet slides over the data. You’ll see what I mean in a second.\nThe COI gets its own data frame:\n\n\ncoi <- wtf$coi(times[1], times[length(enso_idx)])\ncoi_df <- data.frame(x = as.numeric(coi[[1]]), y = as.numeric(coi[[2]]))\n\n\nAnd now we’re ready to create the scaleogram:\n\n\nlabeled_scales <- c(0.25, 0.5, 1, 2, 4, 8, 16, 32, 64)\nlabeled_frequencies <- round(as.numeric(wtf$fourier_period(labeled_scales)), 1)\n\nggplot(df) +\n  scale_y_continuous(\n    trans = scales::compose_trans(scales::log2_trans(), scales::reverse_trans()),\n    breaks = c(0.25, 0.5, 1, 2, 4, 8, 16, 32, 64),\n    limits = c(max(scales), min(scales)),\n    expand = c(0, 0),\n    sec.axis = dup_axis(\n      labels = scales::label_number(labeled_frequencies),\n      name = \"Fourier period (years)\"\n    )\n  ) +\n  ylab(\"scale (years)\") +\n  scale_x_continuous(breaks = seq(1950, 2020, by = 5), expand = c(0, 0)) +\n  xlab(\"year\") +\n  geom_contour_filled(aes(time, scale, z = power), show.legend = FALSE) +\n  scale_fill_viridis_d(option = \"turbo\") +\n  geom_ribbon(data = coi_df, aes(x = x, ymin = y, ymax = max(scales)),\n              fill = \"black\", alpha = 0.6) +\n  theme(legend.position = \"none\")\n\n\nScaleogram of ENSO data.What we see here is how, in ENSO, different rhythms have prevailed over time. Instead of “rhythms,” I could have said “scales,” or “frequencies,” or “periods” – all those translate into one another. Since, to us humans, wavelet scales don’t mean that much, the period (in years) is displayed on an additional y axis on the right.\nSo, we see that in the eighties, an (approximately) four-year period had exceptional influence. Thereafter, yet longer periodicities gained in dominance. And, in accordance with what we expect from prior analysis, there is a basso continuo of annual similarity.\nAlso, note how, at first sight, there seems to have been a decade where a six-year period stood out: right at the beginning of where (for us) measurement starts, in the fifties. However, the dark shading – the COI – tells us that, in this region, the data is not to be trusted.\nSumming up, the two-dimensional analysis nicely complements the more compressed characterization we got from the DFT. Before we move on to the next series, however, let me just quickly address one question, in case you were wondering (if not, just read on, since I won’t be going into details anyway): How is this different from a spectrogram?\nIn a nutshell, the spectrogram splits the data into several “windows,” and computes the DFT independently on all of them. To compute the scaleogram, on the other hand, the analysis wavelet slides continuously over the data, resulting in a spectrum-equivalent for the neighborhood of each sample in the series. With the spectrogram, a fixed window size means that not all frequencies are resolved equally well: The higher frequencies appear more frequently in the interval than the lower ones, and thus, will allow for better resolution. Wavelet analysis, in contrast, is done on a set of scales deliberately arranged so as to capture a broad range of frequencies theoretically visible in a series of given length.\n\nTake a look at the aforementioned book excerpt to see spectrogram and scaleogram compared in a nice application.\nAnalysis: NAO\nThe data file for NAO is in fixed-table format. After conversion to a tsibble, we have:\n\n\ndownload.file(\n \"https://crudata.uea.ac.uk/cru/data//nao/nao.dat\",\n destfile = \"nao.dat\"\n)\n\n# needed for AO, as well\nuse_months <- seq.Date(\n  from = as.Date(\"1950-01-01\"),\n  to = as.Date(\"2022-09-01\"),\n  by = \"months\"\n)\n\nnao <-\n  read_table(\n    \"nao.dat\",\n    col_names = FALSE,\n    na = \"-99.99\",\n    skip = 3\n  ) %>%\n  select(-X1, -X14) %>%\n  as.matrix() %>%\n  t() %>%\n  as.vector() %>%\n  .[1:length(use_months)] %>%\n  tibble(\n    x = use_months,\n    nao = .\n  ) %>%\n  mutate(x = yearmonth(x)) %>%\n  fill(nao) %>%\n  as_tsibble(index = x)\n\nnao\n\n\n# A tsibble: 873 x 2 [1M]\n          x   nao\n      <mth> <dbl>\n 1 1950 Jan -0.16\n 2 1950 Feb  0.25\n 3 1950 Mar -1.44\n 4 1950 Apr  1.46\n 5 1950 May  1.34\n 6 1950 Jun -3.94\n 7 1950 Jul -2.75\n 8 1950 Aug -0.08\n 9 1950 Sep  0.19\n10 1950 Oct  0.19\n# … with 863 more rows\nLike before, we start with the spectrum:\n\n\nfft <- torch_fft_fft(as.numeric(scale(nao$nao)))\n\nnum_samples <- nrow(nao)\nnyquist_cutoff <- ceiling(num_samples / 2)\nbins_below_nyquist <- 0:nyquist_cutoff\n\nsampling_rate <- 12 \nfrequencies_per_bin <- sampling_rate / num_samples\nfrequencies <- frequencies_per_bin * bins_below_nyquist\n\ndf <- data.frame(f = frequencies, y = as.numeric(fft[1:(nyquist_cutoff + 1)]$abs()))\ndf %>% ggplot(aes(f, y)) +\n  geom_line() +\n  xlab(\"frequency (per year)\") +\n  ylab(\"magnitude\") +\n  ggtitle(\"Spectrum of NAO data\")\n\n\nSpectrum of NAO data, 1950 to present.Have you been wondering for a tiny moment whether this was time-domain data – not spectral? It does look a lot more noisy than the ENSO spectrum for sure. And really, with NAO, predictability is much worse - forecast lead time usually amounts to just one or two weeks.\nProceeding as before, we pick dominant seasonalities (at least this still is possible!) to pass to feasts::STL().\n\n\nstrongest <- torch_topk(fft[1:(nyquist_cutoff/2)]$abs(), 6)\nstrongest\n\n\n[[1]]\ntorch_tensor\n102.7191\n80.5129\n76.1179\n75.9949\n72.9086\n60.8281\n[ CPUFloatType{6} ]\n\n[[2]]\ntorch_tensor\n147\n99\n146\n59\n33\n78\n[ CPULongType{6} ]\n\n\nimportant_freqs <- frequencies[as.numeric(strongest[[2]])]\nimportant_freqs\n\n\n[1] 2.0068729 1.3470790 1.9931271 0.7972509 0.4398625 1.0584192\n\n\nnum_observations_in_season <- 12/important_freqs  \nnum_observations_in_season\n\n\n[1]  5.979452  8.908163  6.020690 15.051724 27.281250 11.337662\nImportant seasonal periods are of length six, nine, eleven, fifteen, and twenty-seven months, approximately - pretty close together indeed! No wonder that, in STL decomposition, the remainder is even more significant than with ENSO:\n\n\nnao %>%\n  model(STL(nao ~ season(period = 6) + season(period = 9) +\n              season(period = 15) + season(period = 27) +\n              season(period = 12))) %>%\n  components() %>%\n  autoplot()\n\n\nDecomposition of NAO data into trend, seasonal components, and remainder by feasts::STL().Now, what will we see in terms of temporal evolution? Much of the code that follows is the same as for ENSO, repeated here for the reader’s convenience:\n\n\nnao_idx <- nao$nao %>% as.numeric() %>% torch_tensor()\ndt <- 1/12 # same interval as for ENSO\nwtf <- wavelet_transform(length(nao_idx), dt = dt)\npower_spectrum <- wtf$power(nao_idx)\n\ntimes <- lubridate::year(nao$x) + lubridate::month(nao$x)/12 # also same\nscales <- as.numeric(wtf$scales) # will be same because both series have same length\n\ndf <- as_tibble(as.matrix(power_spectrum$t()), .name_repair = \"universal\") %>%\n  mutate(time = times) %>%\n  pivot_longer(!time, names_to = \"scale\", values_to = \"power\") %>%\n  mutate(scale = scales[scale %>%\n    str_remove(\"[\\\\.]{3}\") %>%\n    as.numeric()])\n\ncoi <- wtf$coi(times[1], times[length(nao_idx)])\ncoi_df <- data.frame(x = as.numeric(coi[[1]]), y = as.numeric(coi[[2]]))\n\nlabeled_scales <- c(0.25, 0.5, 1, 2, 4, 8, 16, 32, 64) # same since scales are same \nlabeled_frequencies <- round(as.numeric(wtf$fourier_period(labeled_scales)), 1)\n\nggplot(df) +\n  scale_y_continuous(\n    trans = scales::compose_trans(scales::log2_trans(), scales::reverse_trans()),\n    breaks = c(0.25, 0.5, 1, 2, 4, 8, 16, 32, 64),\n    limits = c(max(scales), min(scales)),\n    expand = c(0, 0),\n    sec.axis = dup_axis(\n      labels = scales::label_number(labeled_frequencies),\n      name = \"Fourier period (years)\"\n    )\n  ) +\n  ylab(\"scale (years)\") +\n  scale_x_continuous(breaks = seq(1950, 2020, by = 5), expand = c(0, 0)) +\n  xlab(\"year\") +\n  geom_contour_filled(aes(time, scale, z = power), show.legend = FALSE) +\n  scale_fill_viridis_d(option = \"turbo\") +\n  geom_ribbon(data = coi_df, aes(x = x, ymin = y, ymax = max(scales)),\n              fill = \"black\", alpha = 0.6) +\n  theme(legend.position = \"none\")\n\n\nScaleogram of NAO data.That, literally, is a much more colorful picture than with ENSO! High frequencies are present, and continually dominant, over the whole time period.\nInterestingly, though, we see similarities to ENSO, as well: In both, there is an important pattern, of periodicity four or slightly more years, that exerces influence during the eighties, nineties, and early two-thousands – only with ENSO, it shows peak impact during the nineties, while with NAO, its dominance is most visible in the first decade of this century. Also, both phenomena exhibit a strongly visible peak, of period two years, around 1970. So, is there a close(-ish) connection between both oscillations? This question, of course, is for the domain experts to answer. At least I found a recent study (Scaife et al. (2014)) that not only suggests there is, but uses one (ENSO, the more predictable one) to inform forecasts of the other:\n\nPrevious studies have shown that the El Niño–Southern Oscillation can drive interannual variations in the NAO [Brönnimann et al., 2007] and hence Atlantic and European winter climate via the stratosphere [Bell et al., 2009]. […] this teleconnection to the tropical Paciﬁc is active in our experiments, with forecasts initialized in El Niño/La Niña conditions in November tending to be followed by negative/positive NAO conditions in winter.\n\nWill we see a similar relationship for AO, our third series under investigation? We might expect so, since AO and NAO are closely related (or even, two sides of the same coin).\nAnalysis: AO\nFirst, the data:\n\n\ndownload.file(\n \"https://www.cpc.ncep.noaa.gov/products/precip/CWlink/daily_ao_index/monthly.ao.index.b50.current.ascii.table\",\n destfile = \"ao.dat\"\n)\n\nao <-\n  read_table(\n    \"ao.dat\",\n    col_names = FALSE,\n    skip = 1\n  ) %>%\n  select(-X1) %>%\n  as.matrix() %>% \n  t() %>%\n  as.vector() %>%\n  .[1:length(use_months)] %>%\n  tibble(x = use_months,\n         ao = .) %>%\n  mutate(x = yearmonth(x)) %>%\n  fill(ao) %>%\n  as_tsibble(index = x) \n\nao\n\n\n# A tsibble: 873 x 2 [1M]\n          x     ao\n      <mth>  <dbl>\n 1 1950 Jan -0.06 \n 2 1950 Feb  0.627\n 3 1950 Mar -0.008\n 4 1950 Apr  0.555\n 5 1950 May  0.072\n 6 1950 Jun  0.539\n 7 1950 Jul -0.802\n 8 1950 Aug -0.851\n 9 1950 Sep  0.358\n10 1950 Oct -0.379\n# … with 863 more rows\nAnd the spectrum:\n\n\nfft <- torch_fft_fft(as.numeric(scale(ao$ao)))\n\nnum_samples <- nrow(ao)\nnyquist_cutoff <- ceiling(num_samples / 2)\nbins_below_nyquist <- 0:nyquist_cutoff\n\nsampling_rate <- 12 # per year\nfrequencies_per_bin <- sampling_rate / num_samples\nfrequencies <- frequencies_per_bin * bins_below_nyquist\n\ndf <- data.frame(f = frequencies, y = as.numeric(fft[1:(nyquist_cutoff + 1)]$abs()))\ndf %>% ggplot(aes(f, y)) +\n  geom_line() +\n  xlab(\"frequency (per year)\") +\n  ylab(\"magnitude\") +\n  ggtitle(\"Spectrum of AO data\")\n\n\nSpectrum of AO data, 1950 to present.Well, this spectrum looks even more random than NAO’s, in that not even a single frequency stands out. For completeness, here is the STL decomposition:\n\n\nstrongest <- torch_topk(fft[1:(nyquist_cutoff/2)]$abs(), 5)\n\nimportant_freqs <- frequencies[as.numeric(strongest[[2]])]\nimportant_freqs\n# [1] 0.01374570 0.35738832 1.77319588 1.27835052 0.06872852\n\nnum_observations_in_season <- 12/important_freqs  \nnum_observations_in_season\n# [1] 873.000000  33.576923   6.767442   9.387097 174.600000 \n\nao %>%\n  model(STL(ao ~ season(period = 33) + season(period = 7) +\n              season(period = 9) + season(period = 174))) %>%\n  components() %>%\n  autoplot()\n\n\nDecomposition of AO data into trend, seasonal components, and remainder by feasts::STL().Finally, what can the scaleogram tell us about dominant patterns?\n\n\nao_idx <- ao$ao %>% as.numeric() %>% torch_tensor()\ndt <- 1/12 # same interval as for ENSO and NAO\nwtf <- wavelet_transform(length(ao_idx), dt = dt)\npower_spectrum <- wtf$power(ao_idx)\n\ntimes <- lubridate::year(ao$x) + lubridate::month(ao$x)/12 # also same\nscales <- as.numeric(wtf$scales) # will be same because all series have same length\n\ndf <- as_tibble(as.matrix(power_spectrum$t()), .name_repair = \"universal\") %>%\n  mutate(time = times) %>%\n  pivot_longer(!time, names_to = \"scale\", values_to = \"power\") %>%\n  mutate(scale = scales[scale %>%\n    str_remove(\"[\\\\.]{3}\") %>%\n    as.numeric()])\n\ncoi <- wtf$coi(times[1], times[length(ao_idx)])\ncoi_df <- data.frame(x = as.numeric(coi[[1]]), y = as.numeric(coi[[2]]))\n\nlabeled_scales <- c(0.25, 0.5, 1, 2, 4, 8, 16, 32, 64) # same since scales are same \nlabeled_frequencies <- round(as.numeric(wtf$fourier_period(labeled_scales)), 1)\n\nggplot(df) +\n  scale_y_continuous(\n    trans = scales::compose_trans(scales::log2_trans(), scales::reverse_trans()),\n    breaks = c(0.25, 0.5, 1, 2, 4, 8, 16, 32, 64),\n    limits = c(max(scales), min(scales)),\n    expand = c(0, 0),\n    sec.axis = dup_axis(\n      labels = scales::label_number(labeled_frequencies),\n      name = \"Fourier period (years)\"\n    )\n  ) +\n  ylab(\"scale (years)\") +\n  scale_x_continuous(breaks = seq(1950, 2020, by = 5), expand = c(0, 0)) +\n  xlab(\"year\") +\n  geom_contour_filled(aes(time, scale, z = power), show.legend = FALSE) +\n  scale_fill_viridis_d(option = \"turbo\") +\n  geom_ribbon(data = coi_df, aes(x = x, ymin = y, ymax = max(scales)),\n              fill = \"black\", alpha = 0.6) +\n  theme(legend.position = \"none\")\n\n\nScaleogram of AO data.Having seen the overall spectrum, the lack of strongly dominant patterns in the scaleogram does not come as a big surprise. It is tempting – for me, at least – to see a reflection of ENSO around 1970, all the more since by transitivity, AO and ENSO should be related in some way. But here, qualified judgment really is reserved to the experts.\nConclusion\nLike I said in the beginning, this post would be about inspiration, not technical detail or reportable results. And I hope that inspirational it has been, at least a little bit. If you’re experimenting with wavelets yourself, or plan to – or if you work in the atmospheric sciences, and would like to provide some insight on the above data/phenomena – we’d love to hear from you!\nAs always, thanks for reading!\nPhoto by ActionVance on Unsplash\n\n\n\nScaife, A. A., Alberto Arribas Herranz, E. Blockley, A. Brookshaw, R. T. Clark, N. Dunstone, R. Eade, et al. 2014. “Skillful Long-Range Prediction of European and North American Winters.” Geophysical Research Letters 41 (7): 2514–19. https://www.microsoft.com/en-us/research/publication/skillful-long-range-prediction-of-european-and-north-american-winters/.\n\n\nTorrence, C., and G. P. Compo. 1998. “A Practical Guide to Wavelet Analysis.” Bulletin of the American Meteorological Society 79 (1): 61–78.\n\n\n\n\n",
    "preview": "posts/2023-01-19-torchwavelets/images/preview.jpg",
    "last_modified": "2024-11-21T15:53:46+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-27-wavelets/",
    "title": "Wavelet Transform - with torch",
    "description": "torch does not have built-in functionality to do wavelet analysis. But we can efficiently implement what we need, making use of the Fast Fourier Transform (FFT). This post is a very first introduction to wavelets, suitable for readers that have not encountered it before. At the same time, it provides useful starter code, showing an (extensible) way to perform wavelet analysis in torch. It is an excerpt from the corresponding chapter in the forthcoming book, Deep Learning and Scientific Computing with R torch, to be published by CRC Press.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2022-10-27",
    "categories": [
      "Torch",
      "R",
      "Concepts"
    ],
    "contents": "\n\nContents\nWavelets and the Wavelet Transform\nIntroducing the Morlet wavelet\nThe roles of \\(K\\) and \\(\\omega_a\\)\nWavelet Transform: A straightforward implementation\nResolution in time versus in frequency\nPerforming the Wavelet Transform in the Fourier domain\nCreating the wavelet diagram\nA real-world example: Chaffinch’s song\n\nNote: Like several prior ones, this post is an excerpt from the forthcoming book, Deep Learning and Scientific Computing with R torch. And like many excerpts, it is a product of hard trade-offs. For additional depth and more examples, I have to ask you to please consult the book.\nWavelets and the Wavelet Transform\nWhat are wavelets? Like the Fourier basis, they’re functions; but they don’t extend infinitely. Instead, they are localized in time: Away from the center, they quickly decay to zero. In addition to a location parameter, they also have a scale: At different scales, they appear squished or stretched. Squished, they will do better at detecting high frequencies; the converse applies when they’re stretched out in time.\nThe basic operation involved in the Wavelet Transform is convolution – have the (flipped) wavelet slide over the data, computing a sequence of dot products. This way, the wavelet is basically looking for similarity.\nAs to the wavelet functions themselves, there are many of them. In a practical application, we’d want to experiment and pick the one that works best for the given data. Compared to the DFT and spectrograms, more experimentation tends to be involved in wavelet analysis.\nThe topic of wavelets is very different from that of Fourier transforms in other respects, as well. Notably, there is a lot less standardization in terminology, use of symbols, and actual practices. In this introduction, I’m leaning heavily on one specific exposition, the one in Arnt Vistnes’ very nice book on waves (Vistnes 2018). In other words, both terminology and examples reflect the choices made in that book.\nIntroducing the Morlet wavelet\nThe Morlet, also known as Gabor1, wavelet is defined like so:\n\\[\n\\Psi_{\\omega_{a},K,t_{k}}(t_n) = (e^{-i \\omega_{a} (t_n - t_k)} - e^{-K^2}) \\ e^{- \\omega_a^2 (t_n - t_k )^2 /(2K )^2} \n\\]\nThis formulation pertains to discretized data, the kinds of data we work with in practice. Thus, \\(t_k\\) and \\(t_n\\) designate points in time, or equivalently, individual time-series samples.\nThis equation looks daunting at first, but we can “tame” it a bit by analyzing its structure, and pointing to the main actors. For concreteness, though, we first look at an example wavelet.\nWe start by implementing the above equation:\n\n\nlibrary(torch)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(tidyr)\nlibrary(zeallot)\n  \nmorlet <- function(omega, K, t_k, t) {\n  (torch_exp(-1i * omega * (t - t_k)) -\n    torch_exp(-torch_square(K))) *\n    torch_exp(-torch_square(omega) * torch_square(t - t_k) /\n      torch_square(2 * K))\n}\n\n\nComparing code and mathematical formulation, we notice a difference. The function itself takes one argument, \\(t_n\\); its realization, four (omega, K, t_k, and t). This is because the torch code is vectorized: On the one hand, omega, K, and t_k, which, in the formula, correspond to \\(\\omega_{a}\\), \\(K\\), and \\(t_k\\) , are scalars. (In the equation, they’re assumed to be fixed.) t, on the other hand, is a vector; it will hold the measurement times of the series to be analyzed.\nWe pick example values for omega, K, and t_k, as well as a range of times to evaluate the wavelet on, and plot its values:\n\n\nomega <- 6 * pi\nK <- 6\nt_k <- 5\n \nsample_time <- torch_arange(3, 7, 0.0001)\n\ncreate_wavelet_plot <- function(omega, K, t_k, sample_time) {\n  morlet <- morlet(omega, K, t_k, sample_time)\n  df <- data.frame(\n    x = as.numeric(sample_time),\n    real = as.numeric(morlet$real),\n    imag = as.numeric(morlet$imag)\n  ) %>%\n    pivot_longer(-x, names_to = \"part\", values_to = \"value\")\n  ggplot(df, aes(x = x, y = value, color = part)) +\n    geom_line() +\n    scale_colour_grey(start = 0.8, end = 0.4) +\n    xlab(\"time\") +\n    ylab(\"wavelet value\") +\n    ggtitle(\"Morlet wavelet\",\n      subtitle = paste0(\"ω_a = \", omega / pi, \"π , K = \", K)\n    ) +\n    theme_minimal()\n}\n\ncreate_wavelet_plot(omega, K, t_k, sample_time)\n\n\nA Morlet wavelet.What we see here is a complex sine curve – note the real and imaginary parts, separated by a phase shift of \\(\\pi/2\\) – that decays on both sides of the center. Looking back at the equation, we can identify the factors responsible for both features. The first term in the equation, \\(e^{-i \\omega_{a} (t_n - t_k)}\\), generates the oscillation; the third, \\(e^{- \\omega_a^2 (t_n - t_k )^2 /(2K )^2}\\), causes the exponential decay away from the center. (In case you’re wondering about the second term, \\(e^{-K^2}\\): For given \\(K\\), it is just a constant.)\nThe third term actually is a Gaussian, with location parameter \\(t_k\\) and scale \\(K\\). We’ll talk about \\(K\\) in great detail soon, but what’s with \\(t_k\\)? \\(t_k\\) is the center of the wavelet; for the Morlet wavelet, this is also the location of maximum amplitude. As distance from the center increases, values quickly approach zero. This is what is meant by wavelets being localized: They are “active” only on a short range of time.\nThe roles of \\(K\\) and \\(\\omega_a\\)\nNow, we already said that \\(K\\) is the scale of the Gaussian; it thus determines how far the curve spreads out in time. But there is also \\(\\omega_a\\). Looking back at the Gaussian term, it, too, will impact the spread.\nFirst though, what is \\(\\omega_a\\)? The subscript \\(a\\) stands for “analysis”; thus, \\(\\omega_a\\) denotes a single frequency being probed.\nNow, let’s first inspect visually the respective impacts of \\(\\omega_a\\) and \\(K\\).\n\n\np1 <- create_wavelet_plot(6 * pi, 4, 5, sample_time)\np2 <- create_wavelet_plot(6 * pi, 6, 5, sample_time)\np3 <- create_wavelet_plot(6 * pi, 8, 5, sample_time)\np4 <- create_wavelet_plot(4 * pi, 6, 5, sample_time)\np5 <- create_wavelet_plot(6 * pi, 6, 5, sample_time)\np6 <- create_wavelet_plot(8 * pi, 6, 5, sample_time)\n\n(p1 | p4) /\n  (p2 | p5) /\n  (p3 | p6)\n\n\nMorlet wavelet: Effects of varying scale and analysis frequency.In the left column, we keep \\(\\omega_a\\) constant, and vary \\(K\\). On the right, \\(\\omega_a\\) changes, and \\(K\\) stays the same.\nFirstly, we observe that the higher \\(K\\), the more the curve gets spread out. In a wavelet analysis, this means that more points in time will contribute to the transform’s output, resulting in high precision as to frequency content, but loss of resolution in time. (We’ll return to this – central – trade-off soon.)\nAs to \\(\\omega_a\\), its impact is twofold. On the one hand, in the Gaussian term, it counteracts – exactly, even – the scale parameter, \\(K\\). On the other, it determines the frequency, or equivalently, the period, of the wave. To see this, take a look at the right column. Corresponding to the different frequencies, we have, in the interval between 4 and 6, four, six, or eight peaks, respectively.\nThis double role of \\(\\omega_a\\) is the reason why, all-in-all, it does make a difference whether we shrink \\(K\\), keeping \\(\\omega_a\\) constant, or increase \\(\\omega_a\\), holding \\(K\\) fixed.\nThis state of things sounds complicated, but is less problematic than it might seem. In practice, understanding the role of \\(K\\) is important, since we need to pick sensible \\(K\\) values to try. As to the \\(\\omega_a\\), on the other hand, there will be a multitude of them, corresponding to the range of frequencies we analyze.\nSo we can understand the impact of \\(K\\) in more detail, we need to take a first look at the Wavelet Transform.\nWavelet Transform: A straightforward implementation\nWhile overall, the topic of wavelets is more multifaceted, and thus, may seem more enigmatic than Fourier analysis, the transform itself is easier to grasp. It is a sequence of local convolutions between wavelet and signal. Here is the formula for specific scale parameter \\(K\\), analysis frequency \\(\\omega_a\\), and wavelet location \\(t_k\\):\n\\[\nW_{K, \\omega_a, t_k} = \\sum_n  x_n \\Psi_{\\omega_{a},K,t_{k}}^*(t_n)\n\\]\nThis is just a dot product, computed between signal and complex-conjugated wavelet. (Here complex conjugation flips the wavelet in time, making this convolution, not correlation – a fact that matters a lot, as you’ll see soon.)\nCorrespondingly, straightforward implementation results in a sequence of dot products, each corresponding to a different alignment of wavelet and signal. Below, in wavelet_transform(), arguments omega and K are scalars, while x, the signal, is a vector. The result is the wavelet-transformed signal, for some specific K and omega of interest.\n\n\nwavelet_transform <- function(x, omega, K) {\n  n_samples <- dim(x)[1]\n  W <- torch_complex(\n    torch_zeros(n_samples), torch_zeros(n_samples)\n  )\n  for (i in 1:n_samples) {\n    # move center of wavelet\n    t_k <- x[i, 1]\n    m <- morlet(omega, K, t_k, x[, 1])\n    # compute local dot product\n    # note wavelet is conjugated\n    dot <- torch_matmul(\n      m$conj()$unsqueeze(1),\n      x[, 2]$to(dtype = torch_cfloat())\n    )\n    W[i] <- dot\n  }\n  W\n}\n\n\nTo test this, we generate a simple sine wave that has a frequency of 100 Hertz in its first part, and double that in the second.\n\n\ngencos <- function(amp, freq, phase, fs, duration) {\n  x <- torch_arange(0, duration, 1 / fs)[1:-2]$unsqueeze(2)\n  y <- amp * torch_cos(2 * pi * freq * x + phase)\n  torch_cat(list(x, y), dim = 2)\n}\n\n# sampling frequency\nfs <- 8000\n\nf1 <- 100\nf2 <- 200\nphase <- 0\nduration <- 0.25\n\ns1 <- gencos(1, f1, phase, fs, duration)\ns2 <- gencos(1, f2, phase, fs, duration)\n\ns3 <- torch_cat(list(s1, s2), dim = 1)\ns3[(dim(s1)[1] + 1):(dim(s1)[1] * 2), 1] <-\n  s3[(dim(s1)[1] + 1):(dim(s1)[1] * 2), 1] + duration\n\ndf <- data.frame(\n  x = as.numeric(s3[, 1]),\n  y = as.numeric(s3[, 2])\n)\nggplot(df, aes(x = x, y = y)) +\n  geom_line() +\n  xlab(\"time\") +\n  ylab(\"amplitude\") +\n  theme_minimal()\n\n\nAn example signal, consisting of a low-frequency and a high-frequency half.Now, we run the Wavelet Transform on this signal, for an analysis frequency of 100 Hertz, and with a K parameter of 2, found through quick experimentation:\n\n\nK <- 2\nomega <- 2 * pi * f1\n\nres <- wavelet_transform(x = s3, omega, K)\ndf <- data.frame(\n  x = as.numeric(s3[, 1]),\n  y = as.numeric(res$abs())\n)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_line() +\n  xlab(\"time\") +\n  ylab(\"Wavelet Transform\") +\n  theme_minimal()\n\n\nWavelet Transform of the above two-part signal. Analysis frequency is 100 Hertz.The transform correctly picks out the part of the signal that matches the analysis frequency. If you feel like, you might want to double-check what happens for an analysis frequency of 200 Hertz.\nNow, in reality we will want to run this analysis not for a single frequency, but a range of frequencies we’re interested in. And we will want to try different scales K. Now, if you executed the code above, you might be worried that this could take a lot of time.\nWell, it by necessity takes longer to compute than its Fourier analogue, the spectrogram. For one, that’s because with spectrograms, the analysis is “just” two-dimensional, the axes being time and frequency. With wavelets there are, in addition, different scales to be explored. And secondly, spectrograms operate on whole windows (with configurable overlap); a wavelet, on the other hand, slides over the signal in unit steps.\nStill, the situation is not as grave as it sounds. The Wavelet Transform being a convolution, we can implement it in the Fourier domain instead. We’ll do that very soon, but first, as promised, let’s revisit the topic of varying K.\nResolution in time versus in frequency\nWe already saw that the higher K, the more spread-out the wavelet. We can use our first, maximally straightforward, example, to investigate one immediate consequence. What, for example, happens for K set to twenty?\n\n\nK <- 20\n\nres <- wavelet_transform(x = s3, omega, K)\ndf <- data.frame(\n  x = as.numeric(s3[, 1]),\n  y = as.numeric(res$abs())\n)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_line() +\n  xlab(\"time\") +\n  ylab(\"Wavelet Transform\") +\n  theme_minimal()\n\n\nWavelet Transform of the above two-part signal, with K set to twenty instead of two.The Wavelet Transform still picks out the correct region of the signal – but now, instead of a rectangle-like result, we get a significantly smoothed version that does not sharply separate the two regions.\nNotably, the first 0.05 seconds, too, show considerable smoothing. The larger a wavelet, the more element-wise products will be lost at the end and the beginning. This is because transforms are computed aligning the wavelet at all signal positions, from the very first to the last. Concretely, when we compute the dot product at location t_k = 1, just a single sample of the signal is considered.\nApart from possibly introducing unreliability at the boundaries, how does wavelet scale affect the analysis? Well, since we’re correlating (convolving, technically; but in this case, the effect, in the end, is the same) the wavelet with the signal, point-wise similarity is what matters. Concretely, assume the signal is a pure sine wave, the wavelet we’re using is a windowed sinusoid like the Morlet, and that we’ve found an optimal K that nicely captures the signal’s frequency. Then any other K, be it larger or smaller, will result in less point-wise overlap.\nPerforming the Wavelet Transform in the Fourier domain\nSoon, we will run the Wavelet Transform on a longer signal. Thus, it is time to speed up computation. We already said that here, we benefit from time-domain convolution being equivalent to multiplication in the Fourier domain. The overall process then is this: First, compute the DFT of both signal and wavelet; second, multiply the results; third, inverse-transform back to the time domain.\nThe DFT of the signal is quickly computed:\n\n\nF <- torch_fft_fft(s3[ , 2])\n\n\nWith the Morlet wavelet, we don’t even have to run the FFT: Its Fourier-domain representation can be stated in closed form. We’ll just make use of that formulation from the outset. Here it is:\n\n\nmorlet_fourier <- function(K, omega_a, omega) {\n  2 * (torch_exp(-torch_square(\n    K * (omega - omega_a) / omega_a\n  )) -\n    torch_exp(-torch_square(K)) *\n      torch_exp(-torch_square(K * omega / omega_a)))\n}\n\n\nComparing this statement of the wavelet to the time-domain one, we see that – as expected – instead of parameters t and t_k it now takes omega and omega_a. The latter, omega_a, is the analysis frequency, the one we’re probing for, a scalar; the former, omega, the range of frequencies that appear in the DFT of the signal.\nIn instantiating the wavelet, there is one thing we need to pay special attention to. In FFT-think, the frequencies are bins; their number is determined by the length of the signal (a length that, for its part, directly depends on sampling frequency). Our wavelet, on the other hand, works with frequencies in Hertz (nicely, from a user’s perspective; since this unit is meaningful to us). What this means is that to morlet_fourier, as omega_a we need to pass not the value in Hertz, but the corresponding FFT bin. Conversion is done relating the number of bins, dim(x)[1], to the sampling frequency of the signal, fs:\n\n\n# again look for 100Hz parts\nomega <- 2 * pi * f1\n\n# need the bin corresponding to some frequency in Hz\nomega_bin <- f1/fs * dim(s3)[1]\n\n\nWe instantiate the wavelet, perform the Fourier-domain multiplication, and inverse-transform the result:\n\n\nK <- 3\n\nm <- morlet_fourier(K, omega_bin, 1:dim(s3)[1])\nprod <- F * m\ntransformed <- torch_fft_ifft(prod)\n\n\nPutting together wavelet instantiation and the steps involved in the analysis, we have the following. (Note how to wavelet_transform_fourier, we now, conveniently, pass in the frequency value in Hertz.)\n\n\nwavelet_transform_fourier <- function(x, omega_a, K, fs) {\n  N <- dim(x)[1]\n  omega_bin <- omega_a / fs * N\n  m <- morlet_fourier(K, omega_bin, 1:N)\n  x_fft <- torch_fft_fft(x)\n  prod <- x_fft * m\n  w <- torch_fft_ifft(prod)\n  w\n}\n\n\nWe’ve already made significant progress. We’re ready for the final step: automating analysis over a range of frequencies of interest. This will result in a three-dimensional representation, the wavelet diagram.\nCreating the wavelet diagram\nIn the Fourier Transform, the number of coefficients we obtain depends on signal length, and effectively reduces to half the sampling frequency. With its wavelet analogue, since anyway we’re doing a loop over frequencies, we might as well decide which frequencies to analyze.\nFirstly, the range of frequencies of interest can be determined running the DFT. The next question, then, is about granularity. Here, I’ll be following the recommendation given in Vistnes’ book, which is based on the relation between current frequency value and wavelet scale, K.\nIteration over frequencies is then implemented as a loop:\n\n\nwavelet_grid <- function(x, K, f_start, f_end, fs) {\n  # downsample analysis frequency range\n  # as per Vistnes, eq. 14.17\n  num_freqs <- 1 + log(f_end / f_start)/ log(1 + 1/(8 * K))\n  freqs <- seq(f_start, f_end, length.out = floor(num_freqs))\n  \n  transformed <- torch_zeros(\n    num_freqs, dim(x)[1],\n    dtype = torch_cfloat()\n    )\n  for(i in 1:num_freqs) {\n    w <- wavelet_transform_fourier(x, freqs[i], K, fs)\n    transformed[i, ] <- w\n  }\n  list(transformed, freqs)\n}\n\n\nCalling wavelet_grid() will give us the analysis frequencies used, together with the respective outputs from the Wavelet Transform.\nNext, we create a utility function that visualizes the result. By default, plot_wavelet_diagram() displays the magnitude of the wavelet-transformed series; it can, however, plot the squared magnitudes, too, as well as their square root, a method much recommended by Vistnes whose effectiveness we will soon have opportunity to witness.\nThe function deserves a few further comments.\nFirstly, same as we did with the analysis frequencies, we down-sample the signal itself, avoiding to suggest a resolution that is not actually present. The formula, again, is taken from Vistnes’ book.\nThen, we use interpolation to obtain a new time-frequency grid. This step may even be necessary if we keep the original grid, since when distances between grid points are very small, R’s image() may refuse to accept axes as evenly spaced.\nFinally, note how frequencies are arranged on a log scale. This leads to much more useful visualizations.\n\n\nplot_wavelet_diagram <- function(x,\n                                 freqs,\n                                 grid,\n                                 K,\n                                 fs,\n                                 f_end,\n                                 type = \"magnitude\") {\n  grid <- switch(type,\n    magnitude = grid$abs(),\n    magnitude_squared = torch_square(grid$abs()),\n    magnitude_sqrt = torch_sqrt(grid$abs())\n  )\n\n  # downsample time series\n  # as per Vistnes, eq. 14.9\n  new_x_take_every <- max(K / 24 * fs / f_end, 1)\n  new_x_length <- floor(dim(grid)[2] / new_x_take_every)\n  new_x <- torch_arange(\n    x[1],\n    x[dim(x)[1]],\n    step = x[dim(x)[1]] / new_x_length\n  )\n  \n  # interpolate grid\n  new_grid <- nnf_interpolate(\n    grid$view(c(1, 1, dim(grid)[1], dim(grid)[2])),\n    c(dim(grid)[1], new_x_length)\n  )$squeeze()\n  out <- as.matrix(new_grid)\n\n  # plot log frequencies\n  freqs <- log10(freqs)\n  \n  image(\n    x = as.numeric(new_x),\n    y = freqs,\n    z = t(out),\n    ylab = \"log frequency [Hz]\",\n    xlab = \"time [s]\",\n    col = hcl.colors(12, palette = \"Light grays\")\n  )\n  main <- paste0(\"Wavelet Transform, K = \", K)\n  sub <- switch(type,\n    magnitude = \"Magnitude\",\n    magnitude_squared = \"Magnitude squared\",\n    magnitude_sqrt = \"Magnitude (square root)\"\n  )\n\n  mtext(side = 3, line = 2, at = 0, adj = 0, cex = 1.3, main)\n  mtext(side = 3, line = 1, at = 0, adj = 0, cex = 1, sub)\n}\n\n\nLet’s use this on a real-world example.\nA real-world example: Chaffinch’s song\nFor the case study, I’ve chosen what, to me, was the most impressive wavelet analysis shown in Vistnes’ book. It’s a sample of a chaffinch’s singing, and it’s available on Vistnes’ website.\n\n\nurl <- \"http://www.physics.uio.no/pow/wavbirds/chaffinch.wav\"\n\ndownload.file(\n file.path(url),\n destfile = \"/tmp/chaffinch.wav\"\n)\n\n\nWe use torchaudio to load the file, and convert from stereo to mono using tuneR’s appropriately named mono(). (For the kind of analysis we’re doing, there is no point in keeping two channels around.)\n\n\nlibrary(torchaudio)\nlibrary(tuneR)\n\nwav <- tuneR_loader(\"/tmp/chaffinch.wav\")\nwav <- mono(wav, \"both\")\nwav\n\n\nWave Object\n    Number of Samples:      1864548\n    Duration (seconds):     42.28\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Mono\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \nFor analysis, we don’t need the complete sequence. Helpfully, Vistnes also published a recommendation as to which range of samples to analyze.\n\n\nwaveform_and_sample_rate <- transform_to_tensor(wav)\nx <- waveform_and_sample_rate[[1]]$squeeze()\nfs <- waveform_and_sample_rate[[2]]\n\n# http://www.physics.uio.no/pow/wavbirds/chaffinchInfo.txt\nstart <- 34000\nN <- 1024 * 128\nend <- start + N - 1\nx <- x[start:end]\n\ndim(x)\n\n\n[1] 131072\nHow does this look in the time domain? (Don’t miss out on the occasion to actually listen to it, on your laptop.)\n\n\ndf <- data.frame(x = 1:dim(x)[1], y = as.numeric(x))\nggplot(df, aes(x = x, y = y)) +\n  geom_line() +\n  xlab(\"sample\") +\n  ylab(\"amplitude\") +\n  theme_minimal()\n\n\nChaffinch’s song.Now, we need to determine a reasonable range of analysis frequencies. To that end, we run the FFT:\n\n\nF <- torch_fft_fft(x)\n\n\nOn the x-axis, we plot frequencies, not sample numbers, and for better visibility, we zoom in a bit.\n\n\nbins <- 1:dim(F)[1]\nfreqs <- bins / N * fs\n\n# the bin, not the frequency\ncutoff <- N/4\n\ndf <- data.frame(\n  x = freqs[1:cutoff],\n  y = as.numeric(F$abs())[1:cutoff]\n)\nggplot(df, aes(x = x, y = y)) +\n  geom_col() +\n  xlab(\"frequency (Hz)\") +\n  ylab(\"magnitude\") +\n  theme_minimal()\n\n\nChaffinch’s song, Fourier spectrum (excerpt).Based on this distribution, we can safely restrict the range of analysis frequencies to between, approximately, 1800 and 8500 Hertz. (This is also the range recommended by Vistnes.)\nFirst, though, let’s anchor expectations by creating a spectrogram for this signal. Suitable values for FFT size and window size were found experimentally. And though, in spectrograms, you don’t see this done often, I found that displaying square roots of coefficient magnitudes yielded the most informative output.\n\n\nfft_size <- 1024\nwindow_size <- 1024\npower <- 0.5\n\nspectrogram <- transform_spectrogram(\n  n_fft = fft_size,\n  win_length = window_size,\n  normalized = TRUE,\n  power = power\n)\n\nspec <- spectrogram(x)\ndim(spec)\n\n\n[1] 513 257\nLike we do with wavelet diagrams, we plot frequencies on a log scale.\n\n\nbins <- 1:dim(spec)[1]\nfreqs <- bins * fs / fft_size\nlog_freqs <- log10(freqs)\n\nframes <- 1:(dim(spec)[2])\nseconds <- (frames / dim(spec)[2])  * (dim(x)[1] / fs)\n\nimage(x = seconds,\n      y = log_freqs,\n      z = t(as.matrix(spec)),\n      ylab = 'log frequency [Hz]',\n      xlab = 'time [s]',\n      col = hcl.colors(12, palette = \"Light grays\")\n)\nmain <- paste0(\"Spectrogram, window size = \", window_size)\nsub <- \"Magnitude (square root)\"\nmtext(side = 3, line = 2, at = 0, adj = 0, cex = 1.3, main)\nmtext(side = 3, line = 1, at = 0, adj = 0, cex = 1, sub)\n\n\nChaffinch’s song, spectrogram.The spectrogram already shows a distinctive pattern. Let’s see what can be done with wavelet analysis. Having experimented with a few different K, I agree with Vistnes that K = 48 makes for an excellent choice:\n\n\nf_start <- 1800\nf_end <- 8500\n\nK <- 48\nc(grid, freqs) %<-% wavelet_grid(x, K, f_start, f_end, fs)\nplot_wavelet_diagram(\n  torch_tensor(1:dim(grid)[2]),\n  freqs, grid, K, fs, f_end,\n  type = \"magnitude_sqrt\"\n)\n\n\nChaffinch’s song, wavelet diagram.The gain in resolution, on both the time and the frequency axis, is utterly impressive.\nThanks for reading!\nPhoto by Vlad Panov on Unsplash\n\n\n\nVistnes, Arnt Inge. 2018. Physics of Oscillations and Waves. With Use of Matlab and Python. Springer.\n\n\nAfter Dennis Gabor, who came up with the idea of using Gaussian-windowed complex exponentials for time-frequency decomposition, and Jean Morlet, who elaborated on and formalized the resulting transform.↩︎\n",
    "preview": "posts/2022-10-27-wavelets/images/squirrel.jpg",
    "last_modified": "2024-11-21T15:54:15+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-25-torch-0-9/",
    "title": "torch 0.9.0",
    "description": "torch v0.9.0 is now on CRAN. This version adds support for ARM systems running macOS, and brings significant performance improvements.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": {}
      }
    ],
    "date": "2022-10-25",
    "categories": [
      "Torch",
      "Packages/Releases",
      "R"
    ],
    "contents": "\nWe are happy to announce that torch v0.9.0 is now on CRAN. This version adds support for ARM systems running macOS, and brings significant performance improvements. This release also includes many smaller bug fixes and features. The full changelog can be found here.\nPerformance improvements\ntorch for R uses LibTorch as its backend. This is the same library that powers PyTorch – meaning that we should see very similar performance when\ncomparing programs.\nHowever, torch has a very different design, compared to other machine learning libraries wrapping C++ code bases (e.g’, xgboost). There, the overhead is insignificant because there’s only a few R function calls before we start training the model; the whole training then happens without ever leaving C++. In torch, C++ functions are wrapped at the operation level. And since a model consists of multiple calls to operators, this can render the R function call overhead more substantial.\nWe have established a set of benchmarks, each trying to identify performance bottlenecks in specific torch features. In some of the benchmarks we were able to make the new version up to 250x faster than the last CRAN version. In Figure 1 we can see the relative performance of torch v0.9.0 and torch v0.8.1 in each of the benchmarks running on the CUDA device:\n\n\n\nFigure 1: Relative performance of v0.8.1 vs v0.9.0 on the CUDA device. Relative performance is measured by (new_time/old_time)^-1.\n\n\n\nThe main source of performance improvements on the GPU is due to better memory\nmanagement, by avoiding unnecessary calls to the R garbage collector. See more details in\nthe ‘Memory management’ article in the torch documentation.\nOn the CPU device we have less expressive results, even though some of the benchmarks\nare 25x faster with v0.9.0. On CPU, the main bottleneck for performance that has been\nsolved is the use of a new thread for each backward call. We now use a thread pool, making the backward and optim benchmarks almost 25x faster for some batch sizes.\n\n\n\nFigure 2: Relative performance of v0.8.1 vs v0.9.0 on the CPU device. Relative performance is measured by (new_time/old_time)^-1.\n\n\n\nThe benchmark code is fully available for reproducibility. Although this release brings\nsignificant improvements in torch for R performance, we will continue working on this topic, and hope to further improve results in the next releases.\nSupport for Apple Silicon\ntorch v0.9.0 can now run natively on devices equipped with Apple Silicon. When\ninstalling torch from a ARM R build, torch will automatically download the pre-built\nLibTorch binaries that target this platform.\nAdditionally you can now run torch operations on your Mac GPU. This feature is\nimplemented in LibTorch through the Metal Performance Shaders API, meaning that it\nsupports both Mac devices equipped with AMD GPU’s and those with Apple Silicon chips. So far, it\nhas only been tested on Apple Silicon devices. Don’t hesitate to open an issue if you\nhave problems testing this feature.\nIn order to use the macOS GPU, you need to place tensors on the MPS device. Then,\noperations on those tensors will happen on the GPU. For example:\n\n\nx <- torch_randn(100, 100, device=\"mps\")\ntorch_mm(x, x)\n\n\nIf you are using nn_modules you also need to move the module to the MPS device,\nusing the $to(device=\"mps\") method.\nNote that this feature is in beta as\nof this blog post, and you might find operations that are not yet implemented on the\nGPU. In this case, you might need to set the environment variable PYTORCH_ENABLE_MPS_FALLBACK=1, so torch automatically uses the CPU as a fallback for\nthat operation.\nOther\nMany other small changes have been added in this release, including:\nUpdate to LibTorch v1.12.1\nAdded torch_serialize() to allow creating a raw vector from torch objects.\ntorch_movedim() and $movedim() are now both 1-based indexed.\nRead the full changelog available here.\n\n\n\n",
    "preview": "posts/2022-10-25-torch-0-9/images/banner.jpeg",
    "last_modified": "2024-11-21T15:51:27+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-20-dft/",
    "title": "Discrete Fourier Transform - with torch",
    "description": "About the Fourier Transform, it has been said that it is one of the greatest wonders of the universe. At the same time, it can be realized in a mere half-dozen lines of code. Even if in the end, you're just going to call torch's built-in functions directly, it helps to understand, and be able to reproduce in code, the ideas that underlie the magic. This post is an excerpt from the forthcoming book, Deep Learning and Scientific Computing with R torch, to be published by CRC Press.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2022-10-20",
    "categories": [
      "Torch",
      "R",
      "Concepts"
    ],
    "contents": "\n\nContents\nUnderstanding the output of torch_fft_fft()\nStarting point: A cosine of frequency 1\nReconstructing the magic\nVarying frequency\nVarying amplitude\nAdding phase\nSuperposition of sinusoids\n\nCoding the DFT\n\nNote: This post is an excerpt from the forthcoming book, Deep Learning and Scientific Computing with R torch. The chapter in question is on the Discrete Fourier Transform (DFT), and is located in part three. Part three is dedicated to scientific computation beyond deep learning. There are two chapters on the Fourier Transform. The first strives to, in as “verbal” and lucid a way as was possible to me, cast a light on what’s behind the magic; it also shows how, surprisingly, you can code the DFT in merely half a dozen lines. The second focuses on fast implementation (the Fast Fourier Transform, or FFT), again with both conceptual/explanatory as well as practical, code-it-yourself parts. Together, these cover far more material than could sensibly fit into a blog post; therefore, please consider what follows more as a “teaser” than a fully fledged article.\nIn the sciences, the Fourier Transform is just about everywhere. Stated very generally, it converts data from one representation to another, without any loss of information (if done correctly, that is.) If you use torch, it is just a function call away: torch_fft_fft() goes one way, torch_fft_ifft() the other. For the user, that’s convenient – you “just” need to know how to interpret the results. Here, I want to help with that. We start with an example function call, playing around with its output, and then, try to get a grip on what is going on behind the scenes.\nUnderstanding the output of torch_fft_fft()\nAs we care about actual understanding, we start from the simplest possible example signal, a pure cosine that performs one revolution over the complete sampling period.\nStarting point: A cosine of frequency 1\nThe way we set things up, there will be sixty-four samples; the sampling period thus equals N = 64. The content of frequency(), the below helper function used to construct the signal, reflects how we represent the cosine. Namely:\n\\[\nf(x) = cos(\\frac{2 \\pi}{N} \\ k \\ x)\n\\]\nHere \\(x\\) values progress over time (or space), and \\(k\\) is the frequency index. A cosine is periodic with period \\(2 \\pi\\); so if we want it to first return to its starting state after sixty-four samples, and \\(x\\) runs between zero and sixty-three, we’ll want \\(k\\) to be equal to \\(1\\). Like that, we’ll reach the initial state again at position \\(x = \\frac{2 \\pi}{64} * 1 * 64\\).\n\n\nlibrary(torch)\nlibrary(ggplot2)\nlibrary(patchwork)\n\nN <- 64\nsample_positions <- 0:(N - 1)\n\nfrequency <- function(k, N) {\n  (2 * pi / N) * k\n}\n\nx <- torch_cos(frequency(1, N) * sample_positions)\n\n\nLet’s quickly confirm this did what it was supposed to:\n\n\ndf <- data.frame(x = sample_positions, y = as.numeric(x))\n\nggplot(df, aes(x = x, y = y)) +\n  geom_line() +\n  xlab(\"time\") +\n  ylab(\"amplitude\") +\n  theme_minimal()\n\n\nPure cosine that accomplishes one revolution over the complete sample period (64 samples).Now that we have the input signal, torch_fft_fft() computes for us the Fourier coefficients, that is, the importance of the various frequencies present in the signal. The number of frequencies considered will equal the number of sampling points: So \\(X\\) will be of length sixty-four as well.\n(In our example, you’ll notice that the second half of coefficients will equal the first in magnitude.1 This is the case for every real-valued signal. In such cases, you could call torch_fft_rfft() instead, which yields “nicer” (in the sense of shorter) vectors to work with. Here though, I want to explain the general case, since that’s what you’ll find done in most expositions on the topic.)\n\n\nFt <- torch_fft_fft(x)\n\n\nEven with the signal being real, the Fourier coefficients are complex numbers. There are four ways to inspect them. The first is to extract the real part:\n\n\nreal_part <- Ft$real\nas.numeric(real_part) %>% round(5)\n\n\n[1]  0 32 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[29] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n[57] 0 0 0 0 0 0 0 32\nOnly a single coefficient is non-zero, the one at position 1. (We start counting from zero, and may discard the second half, as explained above.)\nNow looking at the imaginary part, we find it is zero throughout:\n\n\nimag_part <- Ft$imag\nas.numeric(imag_part) %>% round(5)\n\n\n[1]  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[29] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[57] 0 0 0 0 0 0 0 0\nAt this point we know that there is just a single frequency present in the signal, namely, that at \\(k = 1\\). This matches (and it better had to) the way we constructed the signal: namely, as accomplishing a single revolution over the complete sampling period.\nSince, in theory, every coefficient could have non-zero real and imaginary parts, often what you’d report is the magnitude (the square root of the sum of squared real and imaginary parts):\n\n\nmagnitude <- torch_abs(Ft)\nas.numeric(magnitude) %>% round(5)\n\n\n[1]  0 32 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[29] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n[57] 0 0 0 0 0 0 0 32\nUnsurprisingly, these values exactly reflect the respective real parts.\nFinally, there’s the phase, indicating a possible shift of the signal (a pure cosine is unshifted). In torch, we have torch_angle() complementing torch_abs(), but we need to take into account roundoff error here. We know that in each but a single case, the real and imaginary parts are both exactly zero; but due to finite precision in how numbers are presented in a computer, the actual values will often not be zero. Instead, they’ll be very small. If we take one of these “fake non-zeroes” and divide it by another, as happens in the angle calculation, big values can result. To prevent this from happening, our custom implementation rounds both inputs before triggering the division.\n\n\nphase <- function(Ft, threshold = 1e5) {\n  torch_atan2(\n    torch_abs(torch_round(Ft$imag * threshold)),\n    torch_abs(torch_round(Ft$real * threshold))\n  )\n}\n\nas.numeric(phase(Ft)) %>% round(5)\n\n\n[1]  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[29] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[57] 0 0 0 0 0 0 0 0\nAs expected, there is no phase shift in the signal.\nLet’s visualize what we found.\n\n\ncreate_plot <- function(x, y, quantity) {\n  df <- data.frame(\n    x_ = x,\n    y_ = as.numeric(y) %>% round(5)\n  )\n  ggplot(df, aes(x = x_, y = y_)) +\n    geom_col() +\n    xlab(\"frequency\") +\n    ylab(quantity) +\n    theme_minimal()\n}\n\np_real <- create_plot(\n  sample_positions,\n  real_part,\n  \"real part\"\n)\np_imag <- create_plot(\n  sample_positions,\n  imag_part,\n  \"imaginary part\"\n)\np_magnitude <- create_plot(\n  sample_positions,\n  magnitude,\n  \"magnitude\"\n)\np_phase <- create_plot(\n  sample_positions,\n  phase(Ft),\n  \"phase\"\n)\n\np_real + p_imag + p_magnitude + p_phase\n\n\nReal parts, imaginary parts, magnitudes and phases of the Fourier coefficients, obtained on a pure cosine that performs a single revolution over the sampling period. Imaginary parts as well as phases are all zero.It’s fair to say that we have no reason to doubt what torch_fft_fft() has done. But with a pure sinusoid like this, we can understand exactly what’s going on by computing the DFT ourselves, by hand. Doing this now will significantly help us later, when we’re writing the code.\nReconstructing the magic\nOne caveat about this section. With a topic as rich as the Fourier Transform, and an audience who I imagine to vary widely on a dimension of math and sciences education, my chances to meet your expectations, dear reader, must be very close to zero. Still, I want to take the risk. If you’re an expert on these things, you’ll anyway be just scanning the text, looking out for pieces of torch code. If you’re moderately familiar with the DFT, you may still like being reminded of its inner workings. And – most importantly – if you’re rather new, or even completely new, to this topic, you’ll hopefully take away (at least) one thing: that what seems like one of the greatest wonders of the universe (assuming there is a reality somehow corresponding to what goes on in our minds) may well be a wonder, but neither “magic” nor a thing reserved to the initiated.\nIn a nutshell, the Fourier Transform is a basis transformation. In the case of the DFT – the Discrete Fourier Transform, where time and frequency representations both are finite vectors, not functions – the new basis looks like this:\n\\[\n\\begin{aligned}\n&\\mathbf{w}^{0n}_N = e^{i\\frac{2 \\pi}{N}* 0 * n} = 1\\\\\n&\\mathbf{w}^{1n}_N = e^{i\\frac{2 \\pi}{N}* 1 * n} = e^{i\\frac{2 \\pi}{N} n}\\\\\n&\\mathbf{w}^{2n}_N = e^{i\\frac{2 \\pi}{N}* 2 * n} = e^{i\\frac{2 \\pi}{N}2n}\\\\& ... \\\\\n&\\mathbf{w}^{(N-1)n}_N = e^{i\\frac{2 \\pi}{N}* (N-1) * n} = e^{i\\frac{2 \\pi}{N}(N-1)n}\\\\\n\\end{aligned}\n\\]\nHere \\(N\\), as before, is the number of samples (64, in our case); thus, there are \\(N\\) basis vectors. With \\(k\\) running through the basis vectors, they can be written:\n\\[\n\\mathbf{w}^{kn}_N = e^{i\\frac{2 \\pi}{N}k n}\n\\] {#eq-dft-1}\nLike \\(k\\), \\(n\\) runs from \\(0\\) to \\(N-1\\). To understand what these basis vectors are doing, it is helpful to temporarily switch to a shorter sampling period, \\(N = 4\\), say. If we do so, we have four basis vectors: \\(\\mathbf{w}^{0n}_N\\), \\(\\mathbf{w}^{1n}_N\\), \\(\\mathbf{w}^{2n}_N\\), and \\(\\mathbf{w}^{3n}_N\\). The first one looks like this:\n\\[\n\\mathbf{w}^{0n}_N\n=\n\\begin{bmatrix}\n   e^{i\\frac{2 \\pi}{4}* 0 * 0}\\\\\n   e^{i\\frac{2 \\pi}{4}* 0 * 1}\\\\\n   e^{i\\frac{2 \\pi}{4}* 0 * 2}\\\\\n   e^{i\\frac{2 \\pi}{4}* 0 * 3}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n   1\\\\\n   1\\\\\n   1\\\\\n   1\\\\\n\\end{bmatrix}\n\\]\nThe second, like so:\n\\[\n\\mathbf{w}^{1n}_N\n=\n\\begin{bmatrix}\n   e^{i\\frac{2 \\pi}{4}* 1 * 0}\\\\\n   e^{i\\frac{2 \\pi}{4}* 1 * 1}\\\\\n   e^{i\\frac{2 \\pi}{4}* 1 * 2}\\\\\n   e^{i\\frac{2 \\pi}{4}* 1 * 3}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n   1\\\\\n   e^{i\\frac{\\pi}{2}}\\\\\n   e^{i \\pi}\\\\\n   e^{i\\frac{3 \\pi}{4}}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n   1\\\\\n   i\\\\\n   -1\\\\\n   -i\\\\\n\\end{bmatrix}\n\\]\nThis is the third:\n\\[\n\\mathbf{w}^{2n}_N\n=\n\\begin{bmatrix}\n   e^{i\\frac{2 \\pi}{4}* 2 * 0}\\\\\n   e^{i\\frac{2 \\pi}{4}* 2 * 1}\\\\\n   e^{i\\frac{2 \\pi}{4}* 2 * 2}\\\\\n   e^{i\\frac{2 \\pi}{4}* 2 * 3}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n   1\\\\\n   e^{i\\pi}\\\\\n   e^{i 2 \\pi}\\\\\n   e^{i\\frac{3 \\pi}{2}}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n   1\\\\\n   -1\\\\\n   1\\\\\n   -1\\\\\n\\end{bmatrix}\n\\]\nAnd finally, the fourth:\n\\[\n\\mathbf{w}^{3n}_N\n=\n\\begin{bmatrix}\n   e^{i\\frac{2 \\pi}{4}* 3 * 0}\\\\\n   e^{i\\frac{2 \\pi}{4}* 3 * 1}\\\\\n   e^{i\\frac{2 \\pi}{4}* 3 * 2}\\\\\n   e^{i\\frac{2 \\pi}{4}* 3 * 3}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n   1\\\\\n   e^{i\\frac{3 \\pi}{2}}\\\\\n   e^{i 3 \\pi}\\\\\n   e^{i\\frac{9 \\pi}{2}}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n   1\\\\\n   -i\\\\\n   -1\\\\\n   i\\\\\n\\end{bmatrix}\n\\]\nWe can characterize these four basis vectors in terms of their “speed”: how fast they move around the unit circle. To do this, we simply look at the rightmost column vectors, where the final calculation results appear. The values in that column correspond to positions pointed to by the revolving basis vector at different points in time. This means that looking at a single “update of position”, we can see how fast the vector is moving in a single time step.\nLooking first at \\(\\mathbf{w}^{0n}_N\\), we see that it does not move at all. \\(\\mathbf{w}^{1n}_N\\) goes from \\(1\\) to \\(i\\) to \\(-1\\) to \\(-i\\); one more step, and it would be back where it started. That’s one revolution in four steps, or a step size of \\(\\frac{\\pi}{2}\\). Then \\(\\mathbf{w}^{2n}_N\\) goes at double that pace, moving a distance of \\(\\pi\\) along the circle. That way, it ends up completing two revolutions overall. Finally, \\(\\mathbf{w}^{3n}_N\\) achieves three complete loops, for a step size of \\(\\frac{3 \\pi}{2}\\).\nThe thing that makes these basis vectors so useful is that they are mutually orthogonal. That is, their dot product is zero:\n\\[\n\\langle \\mathbf{w}^{kn}_N, \\mathbf{w}^{ln}_N \\rangle \\ = \\ \\sum_{n=0}^{N-1} ({e^{i\\frac{2 \\pi}{N}k n}})^* e^{i\\frac{2 \\pi}{N}l n} = \\ \\sum_{n=0}^{N-1} ({e^{-i\\frac{2 \\pi}{N}k n}})e^{i\\frac{2 \\pi}{N}l n} = 0\n\\] {#eq-dft-2}\nLet’s take, for example, \\(\\mathbf{w}^{2n}_N\\) and \\(\\mathbf{w}^{3n}_N\\). Indeed, their dot product evaluates to zero.\n\\[\n\\begin{bmatrix}\n   1 & -1 & 1 & -1\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n   1\\\\\n   -i\\\\\n   -1\\\\\n   i\\\\\n\\end{bmatrix}\n=\n1 + i + (-1) + (-i)  = 0\n\\]\nNow, we’re about to see how the orthogonality of the Fourier basis substantially simplifies the calculation of the DFT. Did you notice the similarity between these basis vectors and the way we wrote the example signal? Here it is again:\n\\[\nf(x) = cos(\\frac{2 \\pi}{N} k x)\n\\]\nIf we manage to represent this function in terms of the basis vectors \\(\\mathbf{w}^{kn}_N = e^{i\\frac{2 \\pi}{N}k n}\\), the inner product between the function and each basis vector will be either zero (the “default”) or a multiple of one (in case the function has a component matching the basis vector in question). Luckily, sines and cosines can easily be converted into complex exponentials. In our example, this is how that goes:2\n\\[\n\\begin{aligned}\n\\mathbf{x}_n &= cos(\\frac{2 \\pi}{64} n) \\\\\n&= \\frac{1}{2} (e^{i\\frac{2 \\pi}{64} n} + e^{-i\\frac{2 \\pi}{64} n}) \\\\\n&= \\frac{1}{2} (e^{i\\frac{2 \\pi}{64} n} + e^{i\\frac{2 \\pi}{64} 63n}) \\\\\n&= \\frac{1}{2} (\\mathbf{w}^{1n}_N + \\mathbf{w}^{63n}_N)\n\\end{aligned}\n\\]\nHere the first step directly results from Euler’s formula3, and the second reflects the fact that the Fourier coefficients are periodic, with frequency -1 being the same as 63, -2 equaling 62, and so on.\nNow, the \\(k\\)th Fourier coefficient is obtained by projecting the signal onto basis vector \\(k\\).\nDue to the orthogonality of the basis vectors, only two coefficients will not be zero: those for \\(\\mathbf{w}^{1n}_N\\) and \\(\\mathbf{w}^{63n}_N\\). They are obtained by computing the inner product between the function and the basis vector in question, that is, by summing over \\(n\\). For each \\(n\\) ranging between \\(0\\) and \\(N-1\\), we have a contribution of \\(\\frac{1}{2}\\), leaving us with a final sum of \\(32\\) for both coefficients. For example, for \\(\\mathbf{w}^{1n}_N\\):\n\\[\n\\begin{aligned}\nX_1 &= \\langle \\mathbf{w}^{1n}_N, \\mathbf{x}_n \\rangle \\\\\n&= \\langle \\mathbf{w}^{1n}_N, \\frac{1}{2} (\\mathbf{w}^{1n}_N + \\mathbf{w}^{63n}_N) \\rangle \\\\\n&= \\frac{1}{2} * 64 \\\\\n&= 32\n\\end{aligned}\n\\]\nAnd analogously for \\(X_{63}\\).\nNow, looking back at what torch_fft_fft() gave us, we see we were able to arrive at the same result. And we’ve learned something along the way.\nAs long as we stay with signals composed of one or more basis vectors, we can compute the DFT in this way. At the end of the chapter, we’ll develop code that will work for all signals, but first, let’s see if we can dive even deeper into the workings of the DFT. Three things we’ll want to explore:\nWhat would happen if frequencies changed – say, a melody were sung at a higher pitch?\nWhat about amplitude changes – say, the music were played twice as loud?\nWhat about phase – e.g., there were an offset before the piece started?\nIn all cases, we’ll call torch_fft_fft() only once we’ve determined the result ourselves.\nAnd finally, we’ll see how complex sinusoids, made up of different components, can still be analyzed in this way, provided they can be expressed in terms of the frequencies that make up the basis.\nVarying frequency\nAssume we quadrupled the frequency, giving us a signal that looked like this:\n\\[\n\\mathbf{x}_n = cos(\\frac{2 \\pi}{N}*4*n)\n\\]\nFollowing the same logic as above, we can express it like so:\n\\[\n\\mathbf{x}_n = \\frac{1}{2} (\\mathbf{w}^{4n}_N + \\mathbf{w}^{60n}_N)\n\\]\nWe already see that non-zero coefficients will be obtained only for frequency indices \\(4\\) and \\(60\\). Picking the former, we obtain\n\\[\n\\begin{aligned}\nX_4 &= \\langle \\mathbf{w}^{4n}_N, \\mathbf{x}_n \\rangle \\\\\n&= \\langle \\mathbf{w}^{4n}_N, \\frac{1}{2} (\\mathbf{w}^{4n}_N + \\mathbf{w}^{60n}_N) \\rangle \\\\\n&= 32\n\\end{aligned}\n\\]\nFor the latter, we’d arrive at the same result.\nNow, let’s make sure our analysis is correct. The following code snippet contains nothing new; it generates the signal, calculates the DFT, and plots them both.\n\n\nx <- torch_cos(frequency(4, N) * sample_positions)\n\nplot_ft <- function(x) {\n\n  df <- data.frame(x = sample_positions, y = as.numeric(x))\n  p_signal <- ggplot(df, aes(x = x, y = y)) +\n    geom_line() +\n    xlab(\"time\") +\n    ylab(\"amplitude\") +\n    theme_minimal()\n\n  # in the code, I'm using Ft instead of X because not\n  # all operating systems treat variables as case-sensitive\n  Ft <- torch_fft_fft(x)\n\n  p_real <- create_plot(\n    sample_positions,\n    Ft$real,\n    \"real part\"\n  )\n  p_imag <- create_plot(\n    sample_positions,\n    Ft$imag,\n    \"imaginary part\"\n  )\n  p_magnitude <- create_plot(\n    sample_positions,\n    torch_abs(Ft),\n    \"magnitude\"\n  )\n  p_phase <- create_plot(\n    sample_positions,\n    phase(Ft),\n    \"phase\"\n  )\n\n  (p_signal | plot_spacer()) /\n    (p_real | p_imag) /\n    (p_magnitude | p_phase)\n}\n\nplot_ft(x)\n\n\nA pure cosine that performs four revolutions over the sampling period, and its DFT. Imaginary parts and phases are still are zero.This does indeed confirm our calculations.\nA special case arises when signal frequency rises to the highest one “allowed”, in the sense of being detectable without aliasing. That will be the case at one half of the number of sampling points. Then, the signal will look like so:\n\\[\n\\mathbf{x}_n = \\frac{1}{2} (\\mathbf{w}^{32n}_N + \\mathbf{w}^{32n}_N)\n\\]\nConsequently, we end up with a single coefficient, corresponding to a frequency of 32 revolutions per sample period, of double the magnitude (64, thus). Here are the signal and its DFT:\n\n\nx <- torch_cos(frequency(32, N) * sample_positions)\nplot_ft(x)\n\n\nA pure cosine that performs thirty-two revolutions over the sampling period, and its DFT. This is the highest frequency where, given sixty-four sample points, no aliasing will occur. Imaginary parts and phases still zero.Varying amplitude\nNow, let’s think about what happens when we vary amplitude. For example, say the signal gets twice as loud. Now, there will be a multiplier of 2 that can be taken outside the inner product. In consequence, the only thing that changes is the magnitude of the coefficients.\nLet’s verify this. The modification is based on the example we had before the very last one, with four revolutions over the sampling period:\n\n\nx <- 2 * torch_cos(frequency(4, N) * sample_positions)\nplot_ft(x)\n\n\nPure cosine with four revolutions over the sampling period, and doubled amplitude. Imaginary parts and phases still zero.So far, we have not once seen a coefficient with non-zero imaginary part. To change this, we add in phase.\nAdding phase\nChanging the phase of a signal means shifting it in time. Our example signal is a cosine, a function whose value is 1 at \\(t=0\\). (That also was the – arbitrarily chosen – starting point of the signal.)\nNow assume we shift the signal forward by \\(\\frac{\\pi}{2}\\). Then the peak we were seeing at zero moves over to \\(\\frac{\\pi}{2}\\); and if we still start “recording” at zero, we must find a value of zero there. An equation describing this is the following. For convenience, we assume a sampling period of \\(2 \\pi\\) and \\(k=1\\), so that the example is a simple cosine:\n\\[\nf(x) = cos(x - \\phi)\n\\]\nThe minus sign may look unintuitive at first. But it does make sense: We now want to obtain a value of 1 at \\(x=\\frac{\\pi}{2}\\), so \\(x - \\phi\\) should evaluate to zero. (Or to any multiple of \\(\\pi\\).) Summing up, a delay in time will appear as a negative phase shift.\nNow, we’re going to calculate the DFT for a shifted version of our example signal. But if you like, take a peek at the phase-shifted version of the time-domain picture now already. You’ll see that a cosine, delayed by \\(\\frac{\\pi}{2}\\), is nothing else than a sine starting at 0.\nTo compute the DFT, we follow our familiar-by-now strategy. The signal now looks like this:\n\\[\n\\mathbf{x}_n = cos(\\frac{2 \\pi}{N}*4*x - \\frac{\\pi}{2})\n\\]\nFirst, we express it in terms of basis vectors:\n\\[\n\\begin{aligned}\n\\mathbf{x}_n &= cos(\\frac{2 \\pi}{64} 4 n - \\frac{\\pi}{2}) \\\\\n&= \\frac{1}{2} (e^{i\\frac{2 \\pi}{64} 4n - \\frac{pi}{2}} + e^{i\\frac{2 \\pi}{64} 60n - \\frac{pi}{2}}) \\\\\n&= \\frac{1}{2} (e^{i\\frac{2 \\pi}{64} 4n}  e^{-i \\frac{\\pi}{2}} + e^{i\\frac{2 \\pi}{64} 60n}  e^{i\\frac{pi}{2}}) \\\\\n&= \\frac{1}{2} (e^{-i \\frac{\\pi}{2}} \\mathbf{w}^{4n}_N + e^{i \\frac{\\pi}{2}} \\mathbf{w}^{60n}_N)\n\\end{aligned}\n\\]\nAgain, we have non-zero coefficients only for frequencies \\(4\\) and \\(60\\). But they are complex now, and both coefficients are no longer identical. Instead, one is the complex conjugate of the other. First, \\(X_4\\):\n\\[\n\\begin{aligned}\nX_4 &= \\langle \\mathbf{w}^{4n}_N, \\mathbf{x}_n \\rangle \\\\\n&=\\langle \\mathbf{w}^{4n}_N, \\frac{1}{2} (e^{-i \\frac{\\pi}{2}} \\mathbf{w}^{4n}_N + e^{i \\frac{\\pi}{2}} \\mathbf{w}^{60n}_N) \\rangle\\\\\n&= 32 *e^{-i \\frac{\\pi}{2}} \\\\\n&= -32i\n\\end{aligned}\n\\]\nAnd here, \\(X_{60}\\):\n\\[\n\\begin{aligned}\nX_{60} &= \\langle \\mathbf{w}^{60n}_N, \\mathbf{x}_N \\rangle \\\\\n&= 32 *e^{i \\frac{\\pi}{2}} \\\\\n&= 32i\n\\end{aligned}\n\\]\nAs usual, we check our calculation using torch_fft_fft().\n\n\nx <- torch_cos(frequency(4, N) * sample_positions - pi / 2)\n\nplot_ft(x)\n\n\nDelaying a pure cosine wave by \\(\\pi/2\\) yields a pure sine wave. Now the real parts of all coefficients are zero; instead, non-zero imaginary values are appearing. The phase shift at those positions is \\(\\pi/2\\).For a pure sine wave, the non-zero Fourier coefficients are imaginary. The phase shift in the coefficients, reported as \\(\\frac{\\pi}{2}\\), reflects the time delay we applied to the signal.\nFinally – before we write some code – let’s put it all together, and look at a wave that has more than a single sinusoidal component.\nSuperposition of sinusoids\nThe signal we construct may still be expressed in terms of the basis vectors, but it is no longer a pure sinusoid. Instead, it is a linear combination of such:\n\\[\n\\begin{aligned}\n\\mathbf{x}_n &= 3 sin(\\frac{2 \\pi}{64} 4n) + 6 cos(\\frac{2 \\pi}{64} 2n) +2cos(\\frac{2 \\pi}{64} 8n)\n\\end{aligned}\n\\]\nI won’t go through the calculation in detail, but it is no different from the previous ones. You compute the DFT for each of the three components, and assemble the results. Without any calculation, however, there’s quite a few things we can say:\nSince the signal consists of two pure cosines and one pure sine, there will be four coefficients with non-zero real parts, and two with non-zero imaginary parts. The latter will be complex conjugates of each other.\nFrom the way the signal is written, it is easy to locate the respective frequencies, as well: The all-real coefficients will correspond to frequency indices 2, 8, 56, and 62; the all-imaginary ones to indices 4 and 60.\nFinally, amplitudes will result from multiplying with \\(\\frac{64}{2}\\) the scaling factors obtained for the individual sinusoids.\nLet’s check:\n\n\nx <- 3 * torch_sin(frequency(4, N) * sample_positions) +\n  6 * torch_cos(frequency(2, N) * sample_positions) +\n  2 * torch_cos(frequency(8, N) * sample_positions)\n\nplot_ft(x)\n\n\nSuperposition of pure sinusoids, and its DFT.Now, how do we calculate the DFT for less convenient signals?\nCoding the DFT\nFortunately, we already know what has to be done. We want to project the signal onto each of the basis vectors. In other words, we’ll be computing a bunch of inner products. Logic-wise, nothing changes: The only difference is that in general, it will not be possible to represent the signal in terms of just a few basis vectors, like we did before. Thus, all projections will actually have to be calculated. But isn’t automation of tedious tasks one thing we have computers for?\nLet’s start by stating input, output, and central logic of the algorithm to be implemented. As throughout this chapter, we stay in a single dimension. The input, thus, is a one-dimensional tensor, encoding a signal. The output is a one-dimensional vector of Fourier coefficients, of the same length as the input, each holding information about a frequency. The central idea is: To obtain a coefficient, project the signal onto the corresponding basis vector.\nTo implement that idea, we need to create the basis vectors, and for each one, compute its inner product with the signal. This can be done in a loop. Surprisingly little code is required to accomplish the goal:\n\n\ndft <- function(x) {\n  n_samples <- length(x)\n\n  n <- torch_arange(0, n_samples - 1)$unsqueeze(1)\n\n  Ft <- torch_complex(\n    torch_zeros(n_samples), torch_zeros(n_samples)\n  )\n\n  for (k in 0:(n_samples - 1)) {\n    w_k <- torch_exp(-1i * 2 * pi / n_samples * k * n)\n    dot <- torch_matmul(w_k, x$to(dtype = torch_cfloat()))\n    Ft[k + 1] <- dot\n  }\n  Ft\n}\n\n\nTo test the implementation, we can take the last signal we analysed, and compare with the output of torch_fft_fft().\n\n\nFt <- dft(x)\ntorch_round(Ft$real) %>% as.numeric()\ntorch_round(Ft$imag) %>% as.numeric()\n\n\n[1]  0 0 192 0 0 0 0 0 64 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[29] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n[57] 64 0 0 0 0 0 192 0\n\n[1]  0 0 0 0 -96 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[29] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n[57] 0 0 0 0 96 0 0 0\nReassuringly – if you look back – the results are the same.\nAbove, did I say “little code”? In fact, a loop is not even needed. Instead of working with the basis vectors one-by-one, we can stack them in a matrix. Then each row will hold the conjugate of a basis vector, and there will be \\(N\\) of them. The columns correspond to positions \\(0\\) to \\(N-1\\); there will be \\(N\\) of them as well. For example, this is how the matrix would look for \\(N=4\\):\n\\[\n\\mathbf{W}_4\n=\n\\begin{bmatrix}\n   e^{-i\\frac{2 \\pi}{4}* 0 * 0} &   e^{-i\\frac{2 \\pi}{4}* 0 * 1}  & e^{-i\\frac{2 \\pi}{4}* 0 * 2} &  e^{-i\\frac{2 \\pi}{4}* 0 * 3}\\\\\ne^{-i\\frac{2 \\pi}{4}* 1 * 0} &   e^{-i\\frac{2 \\pi}{4}* 1 * 1}  & e^{-i\\frac{2 \\pi}{4}* 1 * 2} &  e^{-i\\frac{2 \\pi}{4}* 1 * 3}\\\\\ne^{-i\\frac{2 \\pi}{4}* 2 * 0} &   e^{-i\\frac{2 \\pi}{4}* 2 * 1}  & e^{-i\\frac{2 \\pi}{4}* 2 * 2} &  e^{-i\\frac{2 \\pi}{4}* 2 * 3}\\\\\ne^{-i\\frac{2 \\pi}{4}* 3 * 0} &   e^{-i\\frac{2 \\pi}{4}* 3 * 1}  & e^{-i\\frac{2 \\pi}{4}* 3 * 2} &  e^{-i\\frac{2 \\pi}{4}* 3 * 3}\\\\\n\\end{bmatrix}\n\\] {#eq-dft-3}\nOr, evaluating the expressions:\n\\[\n\\mathbf{W}_4\n=\n\\begin{bmatrix}\n   1 &   1  & 1 &  1\\\\\n1 &   -i  & -1 &  i\\\\\n1 &   -1  & 1 &  -1\\\\\n1 &   i  & -1 &  -i\\\\\n\\end{bmatrix}\n\\]\nWith that modification, the code looks a lot more elegant:\n\n\ndft_vec <- function(x) {\n  n_samples <- length(x)\n\n  n <- torch_arange(0, n_samples - 1)$unsqueeze(1)\n  k <- torch_arange(0, n_samples - 1)$unsqueeze(2)\n\n  mat_k_m <- torch_exp(-1i * 2 * pi / n_samples * k * n)\n\n  torch_matmul(mat_k_m, x$to(dtype = torch_cfloat()))\n}\n\n\nAs you can easily verify, the result is the same.\nThanks for reading!\nPhoto by Trac Vu on Unsplash\n\nExpanding on this a bit: For real-valued signals, the magnitudes as well as the real parts of corresponding coefficients are equal, while the phases and the imaginary parts are conjugated. In other words, the coefficients are complex conjugates of each other. We’ll see this in later examples.↩︎\nI’ll be writing \\(\\mathbf{x}_n\\) instead of \\(f(x)\\) from now on to indicate that we’re working with discrete samples, not the continuous function itself.↩︎\nEuler’s formula relates complex exponentials to sines and cosines, stating that \\(e^{i \\theta} = cos \\theta + i sin \\theta\\).↩︎\n",
    "preview": "posts/2022-10-20-dft/images/squirrel.jpg",
    "last_modified": "2024-11-21T15:48:34+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-13-torch-linalg/",
    "title": "Five ways to do least squares (with torch)",
    "description": "Get to know torch's linalg module, all while learning about different ways to do least-squares regression from scratch. This post is a condensed version of the corresponding chapter in the forthcoming book, Deep Learning and Scientific Computing with R torch, to be published by CRC Press.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2022-10-13",
    "categories": [
      "Torch",
      "R",
      "Concepts",
      "Tabular Data"
    ],
    "contents": "\n\nContents\nThe plan\nRegression for weather prediction\nSetting expectations with lm()\nUsing torch, the quick way: linalg_lstsq()\nLeast squares (I): The normal equations\nLeast squares (II): Cholesky decomposition\nLeast squares (III): LU factorization\nLeast squares (IV): QR factorization\nLeast squares (V): Singular Value Decomposition (SVD)\n\nNote: This post is a condensed version of a chapter from part three of the forthcoming book, Deep Learning and Scientific Computing with R torch. Part three is dedicated to scientific computation beyond deep learning. Throughout the book, I focus on the underlying concepts, striving to explain them in as “verbal” a way as I can. This does not mean skipping the equations; it means taking care to explain why they are the way they are.\nHow do you compute linear least-squares regression? In R, using lm(); in torch, there is linalg_lstsq().\nWhere R, sometimes, hides complexity from the user, high-performance computation frameworks like torch tend to ask for a bit more effort up front, be it careful reading of documentation, or playing around some, or both. For example, here is the central piece of documentation for linalg_lstsq(), elaborating on the driver parameter to the function:\n`driver` chooses the LAPACK/MAGMA function that will be used.\nFor CPU inputs the valid values are 'gels', 'gelsy', 'gelsd, 'gelss'.\nFor CUDA input, the only valid driver is 'gels', which assumes that A is full-rank.\nTo choose the best driver on CPU consider:\n  -   If A is well-conditioned (its condition number is not too large), or you do not mind some precision loss:\n     -   For a general matrix: 'gelsy' (QR with pivoting) (default)\n     -   If A is full-rank: 'gels' (QR)\n  -   If A is not well-conditioned:\n     -   'gelsd' (tridiagonal reduction and SVD)\n     -   But if you run into memory issues: 'gelss' (full SVD).\nWhether you’ll need to know this will depend on the problem you’re solving. But if you do, it certainly will help to have an idea of what is alluded to there, if only in a high-level way.\nIn our example problem below, we’re going to be lucky. All drivers will return the same result – but only once we’ll have applied a “trick”, of sorts. The book analyzes why that works; I won’t do that here, to keep the post reasonably short. What we’ll do instead is dig deeper into the various methods used by linalg_lstsq(), as well as a few others of common use.\nThe plan\nThe way we’ll organize this exploration is by solving a least-squares problem from scratch, making use of various matrix factorizations. Concretely, we’ll approach the task:\nBy means of the so-called normal equations, the most direct way, in the sense that it immediately results from a mathematical statement of the problem.\nAgain, starting from the normal equations, but making use of Cholesky factorization in solving them.\nYet again, taking the normal equations for a point of departure, but proceeding by means of LU decomposition.\nNext, employing another type of factorization – QR – that, together with the final one, accounts for the vast majority of decompositions applied “in the real world”. With QR decomposition, the solution algorithm does not start from the normal equations.\nAnd, finally, making use of Singular Value Decomposition (SVD). Here, too, the normal equations are not needed.\nRegression for weather prediction\nThe dataset we’ll use is available from the UCI Machine Learning Repository.\n\n\nlibrary(torch)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(zeallot)\n\nuci <- \"https://archive.ics.uci.edu\"\nds_path <- \"ml/machine-learning-databases/00514\"\nds_file <- \"Bias_correction_ucl.csv\"\n\ndownload.file(\n file.path(uci, ds_path, ds_file),\n destfile = \"resources/matrix-weather.csv\"\n)\n\nweather_df <- read_csv(\"resources/matrix-weather.csv\") %>%\n  na.omit()\nweather_df %>% glimpse()\n\n\nRows: 7,588\nColumns: 25\n$ station           <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,…\n$ Date              <date> 2013-06-30, 2013-06-30,…\n$ Present_Tmax      <dbl> 28.7, 31.9, 31.6, 32.0, 31.4, 31.9,…\n$ Present_Tmin      <dbl> 21.4, 21.6, 23.3, 23.4, 21.9, 23.5,…\n$ LDAPS_RHmin       <dbl> 58.25569, 52.26340, 48.69048,…\n$ LDAPS_RHmax       <dbl> 91.11636, 90.60472, 83.97359,…\n$ LDAPS_Tmax_lapse  <dbl> 28.07410, 29.85069, 30.09129,…\n$ LDAPS_Tmin_lapse  <dbl> 23.00694, 24.03501, 24.56563,…\n$ LDAPS_WS          <dbl> 6.818887, 5.691890, 6.138224,…\n$ LDAPS_LH          <dbl> 69.45181, 51.93745, 20.57305,…\n$ LDAPS_CC1         <dbl> 0.2339475, 0.2255082, 0.2093437,…\n$ LDAPS_CC2         <dbl> 0.2038957, 0.2517714, 0.2574694,…\n$ LDAPS_CC3         <dbl> 0.1616969, 0.1594441, 0.2040915,…\n$ LDAPS_CC4         <dbl> 0.1309282, 0.1277273, 0.1421253,…\n$ LDAPS_PPT1        <dbl> 0.0000000, 0.0000000, 0.0000000,…\n$ LDAPS_PPT2        <dbl> 0.000000, 0.000000, 0.000000,…\n$ LDAPS_PPT3        <dbl> 0.0000000, 0.0000000, 0.0000000,…\n$ LDAPS_PPT4        <dbl> 0.0000000, 0.0000000, 0.0000000,…\n$ lat               <dbl> 37.6046, 37.6046, 37.5776, 37.6450,…\n$ lon               <dbl> 126.991, 127.032, 127.058, 127.022,…\n$ DEM               <dbl> 212.3350, 44.7624, 33.3068, 45.7160,…\n$ Slope             <dbl> 2.7850, 0.5141, 0.2661, 2.5348,…\n$ `Solar radiation` <dbl> 5992.896, 5869.312, 5863.556,…\n$ Next_Tmax         <dbl> 29.1, 30.5, 31.1, 31.7, 31.2, 31.5,…\n$ Next_Tmin         <dbl> 21.2, 22.5, 23.9, 24.3, 22.5, 24.0,…\nThe way we’re framing the task, nearly everything in the dataset serves as a predictor. As a target, we’ll use Next_Tmax, the maximal temperature reached on the subsequent day. This means we need to remove Next_Tmin from the set of predictors, as it would make for too powerful of a clue. We’ll do the same for station, the weather station id, and Date. This leaves us with twenty-one predictors, including measurements of actual temperature (Present_Tmax, Present_Tmin), model forecasts of various variables (LDAPS_*), and auxiliary information (lat, lon, and `Solar radiation`, among others).\n\n\nweather_df <- weather_df %>%\n  select(-c(station, Next_Tmin, Date)) %>%\n  # standardize predictors\n  mutate(across(.fns = scale))\n\n\nNote how, above, I’ve added a line to standardize the predictors. This is the “trick” I was alluding to above. To see what happens without standardization, please check out the book. (The bottom line is: You would have to call linalg_lstsq() with non-default arguments.)\nFor torch, we split up the data into two tensors: a matrix A, containing all predictors, and a vector b that holds the target.\n\n\nweather <- torch_tensor(weather_df %>% as.matrix())\nA <- weather[ , 1:-2]\nb <- weather[ , -1]\n\ndim(A)\n\n\n[1] 7588   21\nNow, first let’s determine the expected output.\nSetting expectations with lm()\nIf there’s a least squares implementation we “believe in”, it surely must be lm().\n\n\nfit <- lm(Next_Tmax ~ . , data = weather_df)\nfit %>% summary()\n\n\nCall:\nlm(formula = Next_Tmax ~ ., data = weather_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max\n-1.94439 -0.27097  0.01407  0.28931  2.04015\n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        2.605e-15  5.390e-03   0.000 1.000000    \nPresent_Tmax       1.456e-01  9.049e-03  16.089  < 2e-16 ***\nPresent_Tmin       4.029e-03  9.587e-03   0.420 0.674312    \nLDAPS_RHmin        1.166e-01  1.364e-02   8.547  < 2e-16 ***\nLDAPS_RHmax       -8.872e-03  8.045e-03  -1.103 0.270154    \nLDAPS_Tmax_lapse   5.908e-01  1.480e-02  39.905  < 2e-16 ***\nLDAPS_Tmin_lapse   8.376e-02  1.463e-02   5.726 1.07e-08 ***\nLDAPS_WS          -1.018e-01  6.046e-03 -16.836  < 2e-16 ***\nLDAPS_LH           8.010e-02  6.651e-03  12.043  < 2e-16 ***\nLDAPS_CC1         -9.478e-02  1.009e-02  -9.397  < 2e-16 ***\nLDAPS_CC2         -5.988e-02  1.230e-02  -4.868 1.15e-06 ***\nLDAPS_CC3         -6.079e-02  1.237e-02  -4.913 9.15e-07 ***\nLDAPS_CC4         -9.948e-02  9.329e-03 -10.663  < 2e-16 ***\nLDAPS_PPT1        -3.970e-03  6.412e-03  -0.619 0.535766    \nLDAPS_PPT2         7.534e-02  6.513e-03  11.568  < 2e-16 ***\nLDAPS_PPT3        -1.131e-02  6.058e-03  -1.866 0.062056 .  \nLDAPS_PPT4        -1.361e-03  6.073e-03  -0.224 0.822706    \nlat               -2.181e-02  5.875e-03  -3.713 0.000207 ***\nlon               -4.688e-02  5.825e-03  -8.048 9.74e-16 ***\nDEM               -9.480e-02  9.153e-03 -10.357  < 2e-16 ***\nSlope              9.402e-02  9.100e-03  10.331  < 2e-16 ***\n`Solar radiation`  1.145e-02  5.986e-03   1.913 0.055746 .  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.4695 on 7566 degrees of freedom\nMultiple R-squared:  0.7802,    Adjusted R-squared:  0.7796\nF-statistic:  1279 on 21 and 7566 DF,  p-value: < 2.2e-16\nWith an explained variance of 78%, the forecast is working pretty well. This is the baseline we want to check all other methods against. To that purpose, we’ll store respective predictions and prediction errors (the latter being operationalized as root mean squared error, RMSE). For now, we just have entries for lm():\n\n\nrmse <- function(y_true, y_pred) {\n  (y_true - y_pred)^2 %>%\n    sum() %>%\n    sqrt()\n}\n\nall_preds <- data.frame(\n  b = weather_df$Next_Tmax,\n  lm = fit$fitted.values\n)\nall_errs <- data.frame(lm = rmse(all_preds$b, all_preds$lm))\nall_errs\n\n\n       lm\n1 40.8369\nUsing torch, the quick way: linalg_lstsq()\nNow, for a moment let’s assume this was not about exploring different approaches, but getting a quick result. In torch, we have linalg_lstsq(), a function dedicated specifically to solving least-squares problems. (This is the function whose documentation I was citing, above.) Just like we did with lm(), we’d probably just go ahead and call it, making use of the default settings:\n\n\nx_lstsq <- linalg_lstsq(A, b)$solution\n\nall_preds$lstsq <- as.matrix(A$matmul(x_lstsq))\nall_errs$lstsq <- rmse(all_preds$b, all_preds$lstsq)\n\ntail(all_preds)\n\n\n              b         lm      lstsq\n7583 -1.1380931 -1.3544620 -1.3544616\n7584 -0.8488721 -0.9040997 -0.9040993\n7585 -0.7203294 -0.9675286 -0.9675281\n7586 -0.6239224 -0.9044044 -0.9044040\n7587 -0.5275154 -0.8738639 -0.8738635\n7588 -0.7846007 -0.8725795 -0.8725792\nPredictions resemble those of lm() very closely – so closely, in fact, that we may guess those tiny differences are just due to numerical errors surfacing from deep down the respective call stacks. RMSE, thus, should be equal as well:\n\n\nall_errs\n\n\n       lm    lstsq\n1 40.8369 40.8369\nIt is; and this is a satisfying outcome. However, it only really came about due to that “trick”: normalization. (Again, I have to ask you to consult the book for details.)\nNow, let’s explore what we can do without using linalg_lstsq().\nLeast squares (I): The normal equations\nWe start by stating the goal. Given a matrix, \\(\\mathbf{A}\\), that holds features in its columns and observations in its rows, and a vector of observed outcomes, \\(\\mathbf{b}\\), we want to find regression coefficients, one for each feature, that allow us to approximate \\(\\mathbf{b}\\) as well as possible. Call the vector of regression coefficients \\(\\mathbf{x}\\). To obtain it, we need to solve a simultaneous system of equations, that in matrix notation appears as\n\\[\n\\mathbf{Ax} = \\mathbf{b}\n\\]\nIf \\(\\mathbf{A}\\) were a square, invertible matrix, the solution could directly be computed as \\(\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}\\). This will hardly ever be possible, though; we’ll (hopefully) always have more observations than predictors. Another approach is needed. It directly starts from the problem statement.\nWhen we use the columns of \\(\\mathbf{A}\\) for \\(\\mathbf{Ax}\\) to approximate \\(\\mathbf{b}\\), that approximation necessarily is in the column space of \\(\\mathbf{A}\\). \\(\\mathbf{b}\\), on the other hand, normally won’t be. We want those two to be as close as possible. In other words, we want to minimize the distance between them. Choosing the 2-norm for the distance, this yields the objective\n\\[\nminimize \\ ||\\mathbf{Ax}-\\mathbf{b}||^2\n\\]\nThis distance is the (squared) length of the vector of prediction errors. That vector necessarily is orthogonal to \\(\\mathbf{A}\\) itself. That is, when we multiply it with \\(\\mathbf{A}\\), we get the zero vector:\n\\[\n\\mathbf{A}^T(\\mathbf{Ax} - \\mathbf{b}) = \\mathbf{0}\n\\]\nA rearrangement of this equation yields the so-called normal equations:\n\\[\n\\mathbf{A}^T \\mathbf{A} \\mathbf{x} = \\mathbf{A}^T \\mathbf{b}\n\\]\nThese may be solved for \\(\\mathbf{x}\\), computing the inverse of \\(\\mathbf{A}^T\\mathbf{A}\\):\n\\[\n\\mathbf{x} = (\\mathbf{A}^T \\mathbf{A})^{-1} \\mathbf{A}^T \\mathbf{b}\n\\]\n\\(\\mathbf{A}^T\\mathbf{A}\\) is a square matrix. It still might not be invertible, in which case the so-called pseudoinverse would be computed instead. In our case, this will not be needed; we already know \\(\\mathbf{A}\\) has full rank, and so does \\(\\mathbf{A}^T\\mathbf{A}\\).\nThus, from the normal equations we have derived a recipe for computing \\(\\mathbf{b}\\). Let’s put it to use, and compare with what we got from lm() and linalg_lstsq().\n\n\nAtA <- A$t()$matmul(A)\nAtb <- A$t()$matmul(b)\ninv <- linalg_inv(AtA)\nx <- inv$matmul(Atb)\n\nall_preds$neq <- as.matrix(A$matmul(x))\nall_errs$neq <- rmse(all_preds$b, all_preds$neq)\n\nall_errs\n\n\n       lm   lstsq     neq\n1 40.8369 40.8369 40.8369\nHaving confirmed that the direct way works, we may allow ourselves some sophistication. Four different matrix factorizations will make their appearance: Cholesky, LU, QR, and Singular Value Decomposition. The goal, in every case, is to avoid the expensive computation of the (pseudo-) inverse. That’s what all methods have in common. However, they do not differ “just” in the way the matrix is factorized, but also, in which matrix is. This has to do with the constraints the various methods impose. Roughly speaking, the order they’re listed in above reflects a falling slope of preconditions, or put differently, a rising slope of generality. Due to the constraints involved, the first two (Cholesky, as well as LU decomposition) will be performed on \\(\\mathbf{A}^T\\mathbf{A}\\), while the latter two (QR and SVD) operate on \\(\\mathbf{A}\\) directly. With them, there never is a need to compute \\(\\mathbf{A}^T\\mathbf{A}\\).\nLeast squares (II): Cholesky decomposition\nIn Cholesky decomposition, a matrix is factored into two triangular matrices of the same size, with one being the transpose of the other. This commonly is written either\n\\[\n\\mathbf{A} = \\mathbf{L} \\mathbf{L}^T\n\\] or\n\\[\n\\mathbf{A} = \\mathbf{R}^T\\mathbf{R}\n\\]\nHere symbols \\(\\mathbf{L}\\) and \\(\\mathbf{R}\\) denote lower-triangular and upper-triangular matrices, respectively.\nFor Cholesky decomposition to be possible, a matrix has to be both symmetric and positive definite. These are pretty strong conditions, ones that will not often be fulfilled in practice. In our case, \\(\\mathbf{A}\\) is not symmetric. This immediately implies we have to operate on \\(\\mathbf{A}^T\\mathbf{A}\\) instead. And since \\(\\mathbf{A}\\) already is positive definite, we know that \\(\\mathbf{A}^T\\mathbf{A}\\) is, as well.\nIn torch, we obtain the Cholesky decomposition of a matrix using linalg_cholesky(). By default, this call will return \\(\\mathbf{L}\\), a lower-triangular matrix.\n\n\n# AtA = L L_t\nAtA <- A$t()$matmul(A)\nL <- linalg_cholesky(AtA)\n\n\nLet’s check that we can reconstruct \\(\\mathbf{A}\\) from \\(\\mathbf{L}\\):\n\n\nLLt <- L$matmul(L$t())\ndiff <- LLt - AtA\nlinalg_norm(diff, ord = \"fro\")\n\n\ntorch_tensor\n0.00258896\n[ CPUFloatType{} ]\nHere, I’ve computed the Frobenius norm of the difference between the original matrix and its reconstruction. The Frobenius norm individually sums up all matrix entries, and returns the square root. In theory, we’d like to see zero here; but in the presence of numerical errors, the result is sufficient to indicate that the factorization worked fine.\nNow that we have \\(\\mathbf{L}\\mathbf{L}^T\\) instead of \\(\\mathbf{A}^T\\mathbf{A}\\), how does that help us? It’s here that the magic happens, and you’ll find the same type of magic at work in the remaining three methods. The idea is that due to some decomposition, a more performant way arises of solving the system of equations that constitute a given task.\nWith \\(\\mathbf{L}\\mathbf{L}^T\\), the point is that \\(\\mathbf{L}\\) is triangular, and when that’s the case the linear system can be solved by simple substitution. That is best visible with a tiny example:\n\\[\n\\begin{bmatrix}\n  1 & 0 & 0\\\\\n  2 & 3 & 0\\\\\n  3 & 4 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n  x1\\\\\n  x2\\\\\n  x3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n  1\\\\\n  11\\\\\n  15\n\\end{bmatrix}\n\\]\nStarting in the top row, we immediately see that \\(x1\\) equals \\(1\\); and once we know that it is straightforward to calculate, from row two, that \\(x2\\) must be \\(3\\). The last row then tells us that \\(x3\\) must be \\(0\\).\nIn code, torch_triangular_solve() is used to efficiently compute the solution to a linear system of equations where the matrix of predictors is lower- or upper-triangular. An additional requirement is for the matrix to be symmetric – but that condition we already had to satisfy in order to be able to use Cholesky factorization.\nBy default, torch_triangular_solve() expects the matrix to be upper- (not lower-) triangular; but there is a function parameter, upper, that lets us correct that expectation. The return value is a list, and its first item contains the desired solution. To illustrate, here is torch_triangular_solve(), applied to the toy example we manually solved above:\n\n\nsome_L <- torch_tensor(\n  matrix(c(1, 0, 0, 2, 3, 0, 3, 4, 1), nrow = 3, byrow = TRUE)\n)\nsome_b <- torch_tensor(matrix(c(1, 11, 15), ncol = 1))\n\nx <- torch_triangular_solve(\n  some_b,\n  some_L,\n  upper = FALSE\n)[[1]]\nx\n\n\ntorch_tensor\n 1\n 3\n 0\n[ CPUFloatType{3,1} ]\nReturning to our running example, the normal equations now look like this:\n\\[\n\\mathbf{L}\\mathbf{L}^T \\mathbf{x} = \\mathbf{A}^T \\mathbf{b}\n\\]\nWe introduce a new variable, \\(\\mathbf{y}\\), to stand for \\(\\mathbf{L}^T \\mathbf{x}\\),\n\\[\n\\mathbf{L}\\mathbf{y} = \\mathbf{A}^T \\mathbf{b}\n\\]\nand compute the solution to this system:\n\n\nAtb <- A$t()$matmul(b)\n\ny <- torch_triangular_solve(\n  Atb$unsqueeze(2),\n  L,\n  upper = FALSE\n)[[1]]\n\n\nNow that we have \\(y\\), we look back at how it was defined:\n\\[\n\\mathbf{y} = \\mathbf{L}^T \\mathbf{x}\n\\]\nTo determine \\(\\mathbf{x}\\), we can thus again use torch_triangular_solve():\n\n\nx <- torch_triangular_solve(y, L$t())[[1]]\n\n\nAnd there we are.\nAs usual, we compute the prediction error:\n\n\nall_preds$chol <- as.matrix(A$matmul(x))\nall_errs$chol <- rmse(all_preds$b, all_preds$chol)\n\nall_errs\n\n\n       lm   lstsq     neq    chol\n1 40.8369 40.8369 40.8369 40.8369\nNow that you’ve seen the rationale behind Cholesky factorization – and, as already suggested, the idea carries over to all other decompositions – you might like to save yourself some work making use of a dedicated convenience function, torch_cholesky_solve(). This will render obsolete the two calls to torch_triangular_solve().\nThe following lines yield the same output as the code above – but, of course, they do hide the underlying magic.\n\n\nL <- linalg_cholesky(AtA)\n\nx <- torch_cholesky_solve(Atb$unsqueeze(2), L)\n\nall_preds$chol2 <- as.matrix(A$matmul(x))\nall_errs$chol2 <- rmse(all_preds$b, all_preds$chol2)\nall_errs\n\n\n       lm   lstsq     neq    chol   chol2\n1 40.8369 40.8369 40.8369 40.8369 40.8369\nLet’s move on to the next method – equivalently, to the next factorization.\nLeast squares (III): LU factorization\nLU factorization is named after the two factors it introduces: a lower-triangular matrix, \\(\\mathbf{L}\\), as well as an upper-triangular one, \\(\\mathbf{U}\\). In theory, there are no restrictions on LU decomposition: Provided we allow for row exchanges, effectively turning \\(\\mathbf{A} = \\mathbf{L}\\mathbf{U}\\) into \\(\\mathbf{A} = \\mathbf{P}\\mathbf{L}\\mathbf{U}\\) (where \\(\\mathbf{P}\\) is a permutation matrix), we can factorize any matrix.\nIn practice, though, if we want to make use of torch_triangular_solve() , the input matrix has to be symmetric. Therefore, here too we have to work with \\(\\mathbf{A}^T\\mathbf{A}\\), not \\(\\mathbf{A}\\) directly. (And that’s why I’m showing LU decomposition right after Cholesky – they’re similar in what they make us do, though not at all similar in spirit.)\nWorking with \\(\\mathbf{A}^T\\mathbf{A}\\) means we’re again starting from the normal equations. We factorize \\(\\mathbf{A}^T\\mathbf{A}\\), then solve two triangular systems to arrive at the final solution. Here are the steps, including the not-always-needed permutation matrix \\(\\mathbf{P}\\):\n\\[\n\\begin{aligned}\n\\mathbf{A}^T \\mathbf{A} \\mathbf{x} &= \\mathbf{A}^T \\mathbf{b} \\\\\n\\mathbf{P} \\mathbf{L}\\mathbf{U} \\mathbf{x} &= \\mathbf{A}^T \\mathbf{b} \\\\\n\\mathbf{L} \\mathbf{y} &= \\mathbf{P}^T \\mathbf{A}^T \\mathbf{b} \\\\\n\\mathbf{y} &= \\mathbf{U} \\mathbf{x}\n\\end{aligned}\n\\]\nWe see that when \\(\\mathbf{P}\\) is needed, there is an additional computation: Following the same strategy as we did with Cholesky, we want to move \\(\\mathbf{P}\\) from the left to the right. Luckily, what may look expensive – computing the inverse – is not: For a permutation matrix, its transpose reverses the operation.\nCode-wise, we’re already familiar with most of what we need to do. The only missing piece is torch_lu(). torch_lu() returns a list of two tensors, the first a compressed representation of the three matrices \\(\\mathbf{P}\\), \\(\\mathbf{L}\\), and \\(\\mathbf{U}\\). We can uncompress it using torch_lu_unpack() :\n\n\nlu <- torch_lu(AtA)\n\nc(P, L, U) %<-% torch_lu_unpack(lu[[1]], lu[[2]])\n\n\nWe move \\(\\mathbf{P}\\) to the other side:\n\n\nAtb <- P$t()$matmul(Atb)\n\n\nAll that remains to be done is solve two triangular systems, and we are done:\n\n\ny <- torch_triangular_solve(\n  Atb$unsqueeze(2),\n  L,\n  upper = FALSE\n)[[1]]\nx <- torch_triangular_solve(y, U)[[1]]\n\nall_preds$lu <- as.matrix(A$matmul(x))\nall_errs$lu <- rmse(all_preds$b, all_preds$lu)\nall_errs[1, -5]\n\n\n       lm   lstsq     neq    chol      lu\n1 40.8369 40.8369 40.8369 40.8369 40.8369\nAs with Cholesky decomposition, we can save ourselves the trouble of calling torch_triangular_solve() twice. torch_lu_solve() takes the decomposition, and directly returns the final solution:\n\n\nlu <- torch_lu(AtA)\nx <- torch_lu_solve(Atb$unsqueeze(2), lu[[1]], lu[[2]])\n\nall_preds$lu2 <- as.matrix(A$matmul(x))\nall_errs$lu2 <- rmse(all_preds$b, all_preds$lu2)\nall_errs[1, -5]\n\n\n       lm   lstsq     neq    chol      lu      lu\n1 40.8369 40.8369 40.8369 40.8369 40.8369 40.8369\nNow, we look at the two methods that don’t require computation of \\(\\mathbf{A}^T\\mathbf{A}\\).\nLeast squares (IV): QR factorization\nAny matrix can be decomposed into an orthogonal matrix, \\(\\mathbf{Q}\\), and an upper-triangular matrix, \\(\\mathbf{R}\\). QR factorization is probably the most popular approach to solving least-squares problems; it is, in fact, the method used by R’s lm(). In what ways, then, does it simplify the task?\nAs to \\(\\mathbf{R}\\), we already know how it is useful: By virtue of being triangular, it defines a system of equations that can be solved step-by-step, by means of mere substitution. \\(\\mathbf{Q}\\) is even better. An orthogonal matrix is one whose columns are orthogonal – meaning, mutual dot products are all zero – and have unit norm; and the nice thing about such a matrix is that its inverse equals its transpose. In general, the inverse is hard to compute; the transpose, however, is easy. Seeing how computation of an inverse – solving \\(\\mathbf{x}=\\mathbf{A}^{-1}\\mathbf{b}\\) – is just the central task in least squares, it’s immediately clear how significant this is.\nCompared to our usual scheme, this leads to a slightly shortened recipe. There is no “dummy” variable \\(\\mathbf{y}\\) anymore. Instead, we directly move \\(\\mathbf{Q}\\) to the other side, computing the transpose (which is the inverse). All that remains, then, is back-substitution. Also, since every matrix has a QR decomposition, we now directly start from \\(\\mathbf{A}\\) instead of \\(\\mathbf{A}^T\\mathbf{A}\\):\n\\[\n\\begin{aligned}\n\\mathbf{A}\\mathbf{x} &= \\mathbf{b}\\\\\n\\mathbf{Q}\\mathbf{R}\\mathbf{x} &= \\mathbf{b}\\\\\n\\mathbf{R}\\mathbf{x} &= \\mathbf{Q}^T\\mathbf{b}\\\\\n\\end{aligned}\n\\]\nIn torch, linalg_qr() gives us the matrices \\(\\mathbf{Q}\\) and \\(\\mathbf{R}\\).\n\n\nc(Q, R) %<-% linalg_qr(A)\n\n\nOn the right side, we used to have a “convenience variable” holding \\(\\mathbf{A}^T\\mathbf{b}\\) ; here, we skip that step, and instead, do something “immediately useful”: move \\(\\mathbf{Q}\\) to the other side.\n\n\nQtb <- Q$t()$matmul(b)\n\n\nThe only remaining step now is to solve the remaining triangular system.\n\n\nx <- torch_triangular_solve(Qtb$unsqueeze(2), R)[[1]]\n\nall_preds$qr <- as.matrix(A$matmul(x))\nall_errs$qr <- rmse(all_preds$b, all_preds$qr)\nall_errs[1, -c(5,7)]\n\n\n       lm   lstsq     neq    chol      lu      qr\n1 40.8369 40.8369 40.8369 40.8369 40.8369 40.8369\nBy now, you’ll be expecting for me to end this section saying “there is also a dedicated solver in torch/torch_linalg, namely …”). Well, not literally, no; but effectively, yes. If you call linalg_lstsq() passing driver = \"gels\", QR factorization will be used.\nLeast squares (V): Singular Value Decomposition (SVD)\nIn true climactic order, the last factorization method we discuss is the most versatile, most diversely applicable, most semantically meaningful one: Singular Value Decomposition (SVD). The third aspect, fascinating though it is, does not relate to our current task, so I won’t go into it here. Here, it is universal applicability that matters: Every matrix can be composed into components SVD-style.\nSingular Value Decomposition factors an input \\(\\mathbf{A}\\) into two orthogonal matrices, called \\(\\mathbf{U}\\) and \\(\\mathbf{V}^T\\), and a diagonal one, named \\(\\mathbf{\\Sigma}\\), such that \\(\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\\). Here \\(\\mathbf{U}\\) and \\(\\mathbf{V}^T\\) are the left and right singular vectors, and \\(\\mathbf{\\Sigma}\\) holds the singular values.\n\\[\n\\begin{aligned}\n\\mathbf{A}\\mathbf{x} &= \\mathbf{b}\\\\\n\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T\\mathbf{x} &= \\mathbf{b}\\\\\n\\mathbf{\\Sigma}\\mathbf{V}^T\\mathbf{x} &= \\mathbf{U}^T\\mathbf{b}\\\\\n\\mathbf{V}^T\\mathbf{x} &= \\mathbf{y}\\\\\n\\end{aligned}\n\\]\nWe start by obtaining the factorization, using linalg_svd(). The argument full_matrices = FALSE tells torch that we want a \\(\\mathbf{U}\\) of dimensionality same as \\(\\mathbf{A}\\), not expanded to 7588 x 7588.\n\n\nc(U, S, Vt) %<-% linalg_svd(A, full_matrices = FALSE)\n\ndim(U)\ndim(S)\ndim(Vt)\n\n\n[1] 7588   21\n[1] 21\n[1] 21 21\nWe move \\(\\mathbf{U}\\) to the other side – a cheap operation, thanks to \\(\\mathbf{U}\\) being orthogonal.\n\n\nUtb <- U$t()$matmul(b)\n\n\nWith both \\(\\mathbf{U}^T\\mathbf{b}\\) and \\(\\mathbf{\\Sigma}\\) being same-length vectors, we can use element-wise multiplication to do the same for \\(\\mathbf{\\Sigma}\\). We introduce a temporary variable, y, to hold the result.\n\n\ny <- Utb / S\n\n\nNow left with the final system to solve, \\(\\mathbf{\\mathbf{V}^T\\mathbf{x} = \\mathbf{y}}\\), we again profit from orthogonality – this time, of the matrix \\(\\mathbf{V}^T\\).\n\n\nx <- Vt$t()$matmul(y)\n\n\nWrapping up, let’s calculate predictions and prediction error:\n\n\nall_preds$svd <- as.matrix(A$matmul(x))\nall_errs$svd <- rmse(all_preds$b, all_preds$svd)\n\nall_errs[1, -c(5, 7)]\n\n\n       lm   lstsq     neq    chol      lu     qr      svd\n1 40.8369 40.8369 40.8369 40.8369 40.8369 40.8369 40.8369\nThat concludes our tour of important least-squares algorithms. Next time, I’ll present excerpts from the chapter on the Discrete Fourier Transform (DFT), again reflecting the focus on understanding what it’s all about. Thanks for reading!\nPhoto by Pearse O’Halloran on Unsplash\n\n\n\n",
    "preview": "posts/2022-10-13-torch-linalg/images/squirrel.jpg",
    "last_modified": "2024-11-21T15:53:53+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-06-audio-classification-torch/",
    "title": "Audio classification with torch",
    "description": "Learn how to classify speech utterances with torch, making use of domain knowledge and deep learning. This post is a condensed version of the corresponding chapter in the forthcoming book, Deep Learning and Scientific Computing with R torch, to be published by CRC Press.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2022-10-06",
    "categories": [
      "Torch",
      "R",
      "Audio Processing"
    ],
    "contents": "\n\nContents\nVariations on a theme\nInspecting the data\nTwo equivalent representations\nCombining representations: The spectrogram\nTraining a neural network on spectrograms\nWrapup\n\nVariations on a theme\nSimple audio classification with Keras, Audio classification with Keras: Looking closer at the non-deep learning parts, Simple audio classification with torch: No, this is not the first post on this blog that introduces speech classification using deep learning. With two of those posts (the “applied” ones) it shares the general setup, the type of deep-learning architecture employed, and the dataset used. With the third, it has in common the interest in the ideas and concepts involved. Each of these posts has a different focus – should you read this one?\nWell, of course I can’t say “no” – all the more so because, here, you have an abbreviated and condensed version of the chapter on this topic in the forthcoming book from CRC Press, Deep Learning and Scientific Computing with R torch. By way of comparison with the previous post that used torch, written by the creator and maintainer of torchaudio, Athos Damiani, significant developments have taken place in the torch ecosystem, the end result being that the code got a lot easier (especially in the model training part). That said, let’s end the preamble already, and plunge into the topic!\nInspecting the data\nWe use the speech commands dataset (Warden (2018)) that comes with torchaudio. The dataset holds recordings of thirty different one- or two-syllable words, uttered by different speakers. There are about 65,000 audio files overall. Our task will be to predict, from the audio solely, which of thirty possible words was pronounced.\n\n\nlibrary(torch)\nlibrary(torchaudio)\nlibrary(luz)\n\nds <- speechcommand_dataset(\n  root = \"~/.torch-datasets\", \n  url = \"speech_commands_v0.01\",\n  download = TRUE\n)\n\n\nWe start by inspecting the data.\n\n\nds$classes\n\n\n[1]  \"bed\"    \"bird\"   \"cat\"    \"dog\"    \"down\"   \"eight\"\n[7]  \"five\"   \"four\"   \"go\"     \"happy\"  \"house\"  \"left\"\n[32] \" marvin\" \"nine\"   \"no\"     \"off\"    \"on\"     \"one\"\n[19] \"right\"  \"seven\" \"sheila\" \"six\"    \"stop\"   \"three\"\n[25]  \"tree\"   \"two\"    \"up\"     \"wow\"    \"yes\"    \"zero\" \nPicking a sample at random, we see that the information we’ll need is contained in four properties: waveform, sample_rate, label_index, and label.\nThe first, waveform, will be our predictor.\n\n\nsample <- ds[2000]\ndim(sample$waveform)\n\n\n[1]     1 16000\nIndividual tensor values are centered at zero, and range between -1 and 1. There are 16,000 of them, reflecting the fact that the recording lasted for one second, and was registered at (or has been converted to, by the dataset creators) a rate of 16,000 samples per second. The latter information is stored in sample$sample_rate:\n\n\nsample$sample_rate\n\n\n[1] 16000\nAll recordings have been sampled at the same rate. Their length almost always equals one second; the – very – few sounds that are minimally longer we can safely truncate.\nFinally, the target is stored, in integer form, in sample$label_index, the corresponding word being available from sample$label:\n\n\nsample$label\nsample$label_index\n\n\n[1] \"bird\"\ntorch_tensor\n2\n[ CPULongType{} ]\nHow does this audio signal “look?”\n\n\nlibrary(ggplot2)\n\ndf <- data.frame(\n  x = 1:length(sample$waveform[1]),\n  y = as.numeric(sample$waveform[1])\n  )\n\nggplot(df, aes(x = x, y = y)) +\n  geom_line(size = 0.3) +\n  ggtitle(\n    paste0(\n      \"The spoken word \\\"\", sample$label, \"\\\": Sound wave\"\n    )\n  ) +\n  xlab(\"time\") +\n  ylab(\"amplitude\") +\n  theme_minimal()\n\n\nThe spoken word “bird,” in time-domain representation.What we see is a sequence of amplitudes, reflecting the sound wave produced by someone saying “bird.” Put differently, we have here a time series of “loudness values.” Even for experts, guessing which word resulted in those amplitudes is an impossible task. This is where domain knowledge comes in. The expert may not be able to make much of the signal in this representation; but they may know a way to more meaningfully represent it.\nTwo equivalent representations\nImagine that instead of as a sequence of amplitudes over time, the above wave were represented in a way that had no information about time at all. Next, imagine we took that representation and tried to recover the original signal. For that to be possible, the new representation would somehow have to contain “just as much” information as the wave we started from. That “just as much” is obtained from the Fourier Transform, and it consists of the magnitudes and phase shifts of the different frequencies that make up the signal.\nHow, then, does the Fourier-transformed version of the “bird” sound wave look? We obtain it by calling torch_fft_fft() (where fft stands for Fast Fourier Transform):\n\n\ndft <- torch_fft_fft(sample$waveform)\ndim(dft)\n\n\n[1]     1 16000\nThe length of this tensor is the same; however, its values are not in chronological order. Instead, they represent the Fourier coefficients, corresponding to the frequencies contained in the signal. The higher their magnitude, the more they contribute to the signal:\n\n\nmag <- torch_abs(dft[1, ])\n\ndf <- data.frame(\n  x = 1:(length(sample$waveform[1]) / 2),\n  y = as.numeric(mag[1:8000])\n)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_line(size = 0.3) +\n  ggtitle(\n    paste0(\n      \"The spoken word \\\"\",\n      sample$label,\n      \"\\\": Discrete Fourier Transform\"\n    )\n  ) +\n  xlab(\"frequency\") +\n  ylab(\"magnitude\") +\n  theme_minimal()\n\n\nThe spoken word “bird,” in frequency-domain representation.From this alternate representation, we could go back to the original sound wave by taking the frequencies present in the signal, weighting them according to their coefficients, and adding them up. But in sound classification, timing information must surely matter; we don’t really want to throw it away.\nCombining representations: The spectrogram\nIn fact, what really would help us is a synthesis of both representations; some sort of “have your cake and eat it, too.” What if we could divide the signal into small chunks, and run the Fourier Transform on each of them? As you may have guessed from this lead-up, this indeed is something we can do; and the representation it creates is called the spectrogram.\nWith a spectrogram, we still keep some time-domain information – some, since there is an unavoidable loss in granularity. On the other hand, for each of the time segments, we learn about their spectral composition. There’s an important point to be made, though. The resolutions we get in time versus in frequency, respectively, are inversely related. If we split up the signals into many chunks (called “windows”), the frequency representation per window will not be very fine-grained. Conversely, if we want to get better resolution in the frequency domain, we have to choose longer windows, thus losing information about how spectral composition varies over time. What sounds like a big problem – and in many cases, will be – won’t be one for us, though, as you’ll see very soon.\nFirst, though, let’s create and inspect such a spectrogram for our example signal. In the following code snippet, the size of the – overlapping – windows is chosen so as to allow for reasonable granularity in both the time and the frequency domain. We’re left with sixty-three windows, and, for each window, obtain two hundred fifty-seven coefficients:\n\n\nfft_size <- 512\nwindow_size <- 512\npower <- 0.5\n\nspectrogram <- transform_spectrogram(\n  n_fft = fft_size,\n  win_length = window_size,\n  normalized = TRUE,\n  power = power\n)\n\nspec <- spectrogram(sample$waveform)$squeeze()\ndim(spec)\n\n\n[1]   257 63\nWe can display the spectrogram visually:\n\n\nbins <- 1:dim(spec)[1]\nfreqs <- bins / (fft_size / 2 + 1) * sample$sample_rate \nlog_freqs <- log10(freqs)\n\nframes <- 1:(dim(spec)[2])\nseconds <- (frames / dim(spec)[2]) *\n  (dim(sample$waveform$squeeze())[1] / sample$sample_rate)\n\nimage(x = as.numeric(seconds),\n      y = log_freqs,\n      z = t(as.matrix(spec)),\n      ylab = 'log frequency [Hz]',\n      xlab = 'time [s]',\n      col = hcl.colors(12, palette = \"viridis\")\n)\nmain <- paste0(\"Spectrogram, window size = \", window_size)\nsub <- \"Magnitude (square root)\"\nmtext(side = 3, line = 2, at = 0, adj = 0, cex = 1.3, main)\nmtext(side = 3, line = 1, at = 0, adj = 0, cex = 1, sub)\n\n\nThe spoken word “bird”: Spectrogram.We know that we’ve lost some resolution in both time and frequency. By displaying the square root of the coefficients’ magnitudes, though – and thus, enhancing sensitivity – we were still able to obtain a reasonable result. (With the viridis color scheme, long-wave shades indicate higher-valued coefficients; short-wave ones, the opposite.)\nFinally, let’s get back to the crucial question. If this representation, by necessity, is a compromise – why, then, would we want to employ it? This is where we take the deep-learning perspective. The spectrogram is a two-dimensional representation: an image. With images, we have access to a rich reservoir of techniques and architectures: Among all areas deep learning has been successful in, image recognition still stands out. Soon, you’ll see that for this task, fancy architectures are not even needed; a straightforward convnet will do a very good job.\nTraining a neural network on spectrograms\nWe start by creating a torch::dataset() that, starting from the original speechcommand_dataset(), computes a spectrogram for every sample.\n\n\nspectrogram_dataset <- dataset(\n  inherit = speechcommand_dataset,\n  initialize = function(...,\n                        pad_to = 16000,\n                        sampling_rate = 16000,\n                        n_fft = 512,\n                        window_size_seconds = 0.03,\n                        window_stride_seconds = 0.01,\n                        power = 2) {\n    self$pad_to <- pad_to\n    self$window_size_samples <- sampling_rate *\n      window_size_seconds\n    self$window_stride_samples <- sampling_rate *\n      window_stride_seconds\n    self$power <- power\n    self$spectrogram <- transform_spectrogram(\n        n_fft = n_fft,\n        win_length = self$window_size_samples,\n        hop_length = self$window_stride_samples,\n        normalized = TRUE,\n        power = self$power\n      )\n    super$initialize(...)\n  },\n  .getitem = function(i) {\n    item <- super$.getitem(i)\n\n    x <- item$waveform\n    # make sure all samples have the same length (57)\n    # shorter ones will be padded,\n    # longer ones will be truncated\n    x <- nnf_pad(x, pad = c(0, self$pad_to - dim(x)[2]))\n    x <- x %>% self$spectrogram()\n\n    if (is.null(self$power)) {\n      # in this case, there is an additional dimension, in position 4,\n      # that we want to appear in front\n      # (as a second channel)\n      x <- x$squeeze()$permute(c(3, 1, 2))\n    }\n\n    y <- item$label_index\n    list(x = x, y = y)\n  }\n)\n\n\nIn the parameter list to spectrogram_dataset(), note power, with a default value of 2. This is the value that, unless told otherwise, torch’s transform_spectrogram() will assume that power should have. Under these circumstances, the values that make up the spectrogram are the squared magnitudes of the Fourier coefficients. Using power, you can change the default, and specify, for example, that’d you’d like absolute values (power = 1), any other positive value (such as 0.5, the one we used above to display a concrete example) – or both the real and imaginary parts of the coefficients (power = NULL).\nDisplay-wise, of course, the full complex representation is inconvenient; the spectrogram plot would need an additional dimension. But we may well wonder whether a neural network could profit from the additional information contained in the “whole” complex number. After all, when reducing to magnitudes we lose the phase shifts for the individual coefficients, which might contain usable information. In fact, my tests showed that it did; use of the complex values resulted in enhanced classification accuracy.1\nLet’s see what we get from spectrogram_dataset():\n\n\nds <- spectrogram_dataset(\n  root = \"~/.torch-datasets\",\n  url = \"speech_commands_v0.01\",\n  download = TRUE,\n  power = NULL\n)\n\ndim(ds[1]$x)\n\n\n[1]   2 257 101\nWe have 257 coefficients for 101 windows; and each coefficient is represented by both its real and imaginary parts.\nNext, we split up the data, and instantiate the dataset() and dataloader() objects.\n\n\ntrain_ids <- sample(\n  1:length(ds),\n  size = 0.6 * length(ds)\n)\nvalid_ids <- sample(\n  setdiff(\n    1:length(ds),\n    train_ids\n  ),\n  size = 0.2 * length(ds)\n)\ntest_ids <- setdiff(\n  1:length(ds),\n  union(train_ids, valid_ids)\n)\n\nbatch_size <- 128\n\ntrain_ds <- dataset_subset(ds, indices = train_ids)\ntrain_dl <- dataloader(\n  train_ds,\n  batch_size = batch_size, shuffle = TRUE\n)\n\nvalid_ds <- dataset_subset(ds, indices = valid_ids)\nvalid_dl <- dataloader(\n  valid_ds,\n  batch_size = batch_size\n)\n\ntest_ds <- dataset_subset(ds, indices = test_ids)\ntest_dl <- dataloader(test_ds, batch_size = 64)\n\nb <- train_dl %>%\n  dataloader_make_iter() %>%\n  dataloader_next()\n\ndim(b$x)\n\n\n[1] 128   2 257 101\nThe model is a straightforward convnet, with dropout and batch normalization. The real and imaginary parts of the Fourier coefficients are passed to the model’s initial nn_conv2d() as two separate channels.\n\n\nmodel <- nn_module(\n  initialize = function() {\n    self$features <- nn_sequential(\n      nn_conv2d(2, 32, kernel_size = 3),\n      nn_batch_norm2d(32),\n      nn_relu(),\n      nn_max_pool2d(kernel_size = 2),\n      nn_dropout2d(p = 0.2),\n      nn_conv2d(32, 64, kernel_size = 3),\n      nn_batch_norm2d(64),\n      nn_relu(),\n      nn_max_pool2d(kernel_size = 2),\n      nn_dropout2d(p = 0.2),\n      nn_conv2d(64, 128, kernel_size = 3),\n      nn_batch_norm2d(128),\n      nn_relu(),\n      nn_max_pool2d(kernel_size = 2),\n      nn_dropout2d(p = 0.2),\n      nn_conv2d(128, 256, kernel_size = 3),\n      nn_batch_norm2d(256),\n      nn_relu(),\n      nn_max_pool2d(kernel_size = 2),\n      nn_dropout2d(p = 0.2),\n      nn_conv2d(256, 512, kernel_size = 3),\n      nn_batch_norm2d(512),\n      nn_relu(),\n      nn_adaptive_avg_pool2d(c(1, 1)),\n      nn_dropout2d(p = 0.2)\n    )\n\n    self$classifier <- nn_sequential(\n      nn_linear(512, 512),\n      nn_batch_norm1d(512),\n      nn_relu(),\n      nn_dropout(p = 0.5),\n      nn_linear(512, 30)\n    )\n  },\n  forward = function(x) {\n    x <- self$features(x)$squeeze()\n    x <- self$classifier(x)\n    x\n  }\n)\n\n\nWe next determine a suitable learning rate:\n\n\nmodel <- model %>%\n  setup(\n    loss = nn_cross_entropy_loss(),\n    optimizer = optim_adam,\n    metrics = list(luz_metric_accuracy())\n  )\n\nrates_and_losses <- model %>%\n  lr_finder(train_dl)\nrates_and_losses %>% plot()\n\n\nLearning rate finder, run on the complex-spectrogram model.Based on the plot, I decided to use 0.01 as a maximal learning rate. Training went on for forty epochs.\n\n\nfitted <- model %>%\n  fit(train_dl,\n    epochs = 50, valid_data = valid_dl,\n    callbacks = list(\n      luz_callback_early_stopping(patience = 3),\n      luz_callback_lr_scheduler(\n        lr_one_cycle,\n        max_lr = 1e-2,\n        epochs = 50,\n        steps_per_epoch = length(train_dl),\n        call_on = \"on_batch_end\"\n      ),\n      luz_callback_model_checkpoint(path = \"models_complex/\"),\n      luz_callback_csv_logger(\"logs_complex.csv\")\n    ),\n    verbose = TRUE\n  )\n\nplot(fitted)\n\n\nFitting the complex-spectrogram model.Let’s check actual accuracies.\n\"epoch\",\"set\",\"loss\",\"acc\"\n1,\"train\",3.09768574611813,0.12396992171405\n1,\"valid\",2.52993751740923,0.284378862793572\n2,\"train\",2.26747255972008,0.333642356819118\n2,\"valid\",1.66693911248562,0.540791100123609\n3,\"train\",1.62294889937818,0.518464153275649\n3,\"valid\",1.11740599192825,0.704882571075402\n...\n...\n38,\"train\",0.18717994078312,0.943809229501442\n38,\"valid\",0.23587799138006,0.936418417799753\n39,\"train\",0.19338578602993,0.942882159044087\n39,\"valid\",0.230597475945365,0.939431396786156\n40,\"train\",0.190593419024368,0.942727647301195\n40,\"valid\",0.243536252455384,0.936186650185414\nWith thirty classes to distinguish between, a final validation-set accuracy of ~0.94 looks like a very decent result!\nWe can confirm this on the test set:\n\n\nevaluate(fitted, test_dl)\n\n\nloss: 0.2373\nacc: 0.9324\nAn interesting question is which words get confused most often. (Of course, even more interesting is how error probabilities are related to features of the spectrograms – but this, we have to leave to the true domain experts. A nice way of displaying the confusion matrix is to create an alluvial plot. We see the predictions, on the left, “flow into” the target slots. (Target-prediction pairs less frequent than a thousandth of test set cardinality are hidden.)\nAlluvial plot for the complex-spectrogram setup.Wrapup\nThat’s it for today! In the upcoming weeks, expect more posts drawing on content from the soon-to-appear CRC book, Deep Learning and Scientific Computing with R torch. Thanks for reading!\nPhoto by alex lauzon on Unsplash\n\n\n\nWarden, Pete. 2018. “Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.” CoRR abs/1804.03209. http://arxiv.org/abs/1804.03209.\n\n\nIn the book, I compare three different setups (the third being use of Mel spectrograms). Using complex coefficients for network input was the winner.↩︎\n",
    "preview": "posts/2022-10-06-audio-classification-torch/images/squirrel.jpg",
    "last_modified": "2024-11-21T15:48:15+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-29-r-text/",
    "title": "Introducing the text package",
    "description": "The text package attempts to provide user-friendly access and pipelines to HuggingFace's transformer language models in R.",
    "author": [
      {
        "name": "Oscar Kjell",
        "url": "https://oscarkjell.se/"
      },
      {
        "name": "Salvatore Giorgi",
        "url": "http://www.salvatoregiorgi.com/"
      },
      {
        "name": "H Andrew Schwartz",
        "url": "https://www3.cs.stonybrook.edu/~has/"
      }
    ],
    "date": "2022-10-04",
    "categories": [
      "Natural Language Processing",
      "R"
    ],
    "contents": "\nAI-based language analysis has recently gone through a “paradigm shift” (Bommasani et al., 2021, p. 1), thanks in part to a new technique referred to as transformer language model (Vaswani et al., 2017, Liu et al., 2019). Companies, including Google, Meta, and OpenAI have released such models, including BERT, RoBERTa, and GPT, that have achieved unprecedented large improvements across most language tasks such as web search and sentiment analysis. While these language models are accessible in Python, and for typical AI tasks through HuggingFace, the R package text makes HuggingFace and state-of-the-art transformer language models accessible as social scientific pipelines in R.\nIntroduction\nWe developed the text package (Kjell, Giorgi & Schwartz, 2022) with two objectives in mind:\nTo serve as a modular solution for downloading and using transformer language models. This, for example, includes transforming text to word embeddings as well as accessing common language model tasks such as text classification, sentiment analysis, text generation, question answering, translation and so on.\nTo provide an end-to-end solution that is designed for human-level analyses including pipelines for state-of-the-art AI techniques tailored for predicting characteristics of the person that produced the language or eliciting insights about linguistic correlates of psychological attributes.\nThis blog post shows how to install the text package, transform text to state-of-the-art contextual word embeddings, use language analysis tasks as well as visualize words in word embedding space.\nInstallation and setting up a python environment\nThe text package is setting up a python environment to get access to the HuggingFace language models. The first time after installing the text package you need to run two functions: textrpp_install() and textrpp_initialize().\n\n\n# Install text from CRAN\ninstall.packages(\"text\")\nlibrary(text)\n\n# Install text required python packages in a conda environment (with defaults)\ntextrpp_install()\n\n# Initialize the installed conda environment\n# save_profile = TRUE saves the settings so that you do not have to run textrpp_initialize() again after restarting R\ntextrpp_initialize(save_profile = TRUE)\n\n\nSee the extended installation guide for more information.\nTransform text to word embeddings\nThe textEmbed() function is used to transform text to word embeddings (numeric representations of text). The model argument enables you to set which language model to use from HuggingFace; if you have not used the model before, it will automatically download the model and necessary files.\n\n\n# Transform the text data to BERT word embeddings\n# Note: To run faster, try something smaller: model = 'distilroberta-base'.\nword_embeddings <- textEmbed(texts = \"Hello, how are you doing?\",\n                            model = 'bert-base-uncased')\nword_embeddings\ncomment(word_embeddings)\n\n\nThe word embeddings can now be used for downstream tasks such as training models to predict related numeric variables (e.g., see the textTrain() and textPredict() functions).\n(To get token and individual layers output see the textEmbedRawLayers() function.)\nLanguage Analysis Tasks\nThere are many transformer language models at HuggingFace that can be used for various language model tasks such as text classification, sentiment analysis, text generation, question answering, translation and so on. The text package comprises user-friendly functions to access these.\n\n\nclassifications <- textClassify(\"Hello, how are you doing?\")\nclassifications\ncomment(classifications)\n\n\n\n\ngenerated_text <- textGeneration(\"The meaning of life is\")\ngenerated_text\n\n\nFor more examples of available language model tasks, for example, see textSum(), textQA(), textTranslate(), and textZeroShot() under Language Analysis Tasks.\nVisualizing Words in Embedding Space\nVisualizing words in the text package is achieved in two steps: First with a function to pre-process the data, and second to plot the words including adjusting visual characteristics such as color and font size.\nTo demonstrate these two functions we use example data included in the text package: Language_based_assessment_data_3_100. We show how to create a two-dimensional figure with words that individuals have used to describe their harmony in life, plotted according to two different well-being questionnaires: the harmony in life scale and the satisfaction with life scale. So, the x-axis shows words that are related to low versus high harmony in life scale scores, and the y-axis shows words related to low versus high satisfaction with life scale scores.\n\n\nword_embeddings_bert <- textEmbed(Language_based_assessment_data_3_100,\n                                  aggregation_from_tokens_to_word_types = \"mean\",\n                                  keep_token_embeddings = FALSE)\n\n# Pre-process the data for plotting\ndf_for_plotting <- textProjection(Language_based_assessment_data_3_100$harmonywords, \n                                  word_embeddings_bert$text$harmonywords,\n                                  word_embeddings_bert$word_types,\n                                  Language_based_assessment_data_3_100$hilstotal, \n                                  Language_based_assessment_data_3_100$swlstotal\n)\n\n# Plot the data\nplot_projection <- textProjectionPlot(\n  word_data = df_for_plotting,\n  y_axes = TRUE,\n  p_alpha = 0.05,\n  title_top = \"Supervised Bicentroid Projection of Harmony in life words\",\n  x_axes_label = \"Low vs. High HILS score\",\n  y_axes_label = \"Low vs. High SWLS score\",\n  p_adjust_method = \"bonferroni\",\n  points_without_words_size = 0.4,\n  points_without_words_alpha = 0.4\n)\nplot_projection$final_plot\n\n\nSupervised Bicentroid Projection of Harmony in life wordsConclusion\nThis post demonstrates how to carry out state-of-the-art text analysis in R using the text package. The package intends to make it easy to access and use transformers language models from HuggingFace to analyze natural language. We look forward to your feedback and contributions toward making such models available for social scientific and other applications more typical of R users.\nReferences\nBommasani et al. (2021). On the opportunities and risks of foundation models.\nKjell et al. (2022). The text package: An R-package for Analyzing and Visualizing Human Language Using Natural Language Processing and Deep Learning.\nLiu et al (2019). Roberta: A robustly optimized bert pretraining approach.\nVaswaniet al (2017). Attention is all you need. Advances in Neural Information Processing Systems, 5998–6008\n\n\n\n",
    "preview": "posts/2022-09-29-r-text/img/ai_blog_word_plot.png",
    "last_modified": "2024-11-21T15:51:38+00:00",
    "input_file": {},
    "preview_width": 880,
    "preview_height": 410
  },
  {
    "path": "posts/2022-08-24-luz-0-3/",
    "title": "luz 0.3.0",
    "description": "luz version 0.3.0 is now on CRAN. luz is a high-level interface for torch.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": {}
      }
    ],
    "date": "2022-08-24",
    "categories": [
      "Torch",
      "Packages/Releases",
      "R"
    ],
    "contents": "\nWe are happy to announce that luz version 0.3.0 is now on CRAN. This\nrelease brings a few improvements to the learning rate finder\nfirst contributed by Chris\nMcMaster. As we didn’t have a\n0.2.0 release post, we will also highlight a few improvements that\ndate back to that version.\nWhat’s luz?\nSince it is relatively new\npackage, we are\nstarting this blog post with a quick recap of how luz works. If you\nalready know what luz is, feel free to move on to the next section.\nluz is a high-level API for torch that aims to encapsulate the training\nloop into a set of reusable pieces of code. It reduces the boilerplate\nrequired to train a model with torch, avoids the error-prone\nzero_grad() - backward() - step() sequence of calls, and also\nsimplifies the process of moving data and models between CPUs and GPUs.\nWith luz you can take your torch nn_module(), for example the\ntwo-layer perceptron defined below:\n\n\nmodnn <- nn_module(\n  initialize = function(input_size) {\n    self$hidden <- nn_linear(input_size, 50)\n    self$activation <- nn_relu()\n    self$dropout <- nn_dropout(0.4)\n    self$output <- nn_linear(50, 1)\n  },\n  forward = function(x) {\n    x %>% \n      self$hidden() %>% \n      self$activation() %>% \n      self$dropout() %>% \n      self$output()\n  }\n)\n\n\nand fit it to a specified dataset like so:\n\n\nfitted <- modnn %>% \n  setup(\n    loss = nn_mse_loss(),\n    optimizer = optim_rmsprop,\n    metrics = list(luz_metric_mae())\n  ) %>% \n  set_hparams(input_size = 50) %>% \n  fit(\n    data = list(x_train, y_train),\n    valid_data = list(x_valid, y_valid),\n    epochs = 20\n  )\n\n\nluz will automatically train your model on the GPU if it’s available,\ndisplay a nice progress bar during training, and handle logging of metrics,\nall while making sure evaluation on validation data is performed in the correct way\n(e.g., disabling dropout).\nluz can be extended in many different layers of abstraction, so you can\nimprove your knowledge gradually, as you need more advanced features in your\nproject. For example, you can implement custom\nmetrics,\ncallbacks,\nor even customize the internal training\nloop.\nTo learn about luz, read the getting\nstarted\nsection on the website, and browse the examples\ngallery.\nWhat’s new in luz?\nLearning rate finder\nIn deep learning, finding a good learning rate is essential to be able\nto fit your model. If it’s too low, you will need too many iterations\nfor your loss to converge, and that might be impractical if your model\ntakes too long to run. If it’s too high, the loss can explode and you\nmight never be able to arrive at a minimum.\nThe lr_finder() function implements the algorithm detailed in Cyclical Learning Rates for\nTraining Neural Networks\n(Smith 2015) popularized in the FastAI framework (Howard and Gugger 2020). It\ntakes an nn_module() and some data to produce a data frame with the\nlosses and the learning rate at each step.\n\n\nmodel <- net %>% setup(\n  loss = torch::nn_cross_entropy_loss(),\n  optimizer = torch::optim_adam\n)\n\nrecords <- lr_finder(\n  object = model, \n  data = train_ds, \n  verbose = FALSE,\n  dataloader_options = list(batch_size = 32),\n  start_lr = 1e-6, # the smallest value that will be tried\n  end_lr = 1 # the largest value to be experimented with\n)\n\nstr(records)\n#> Classes 'lr_records' and 'data.frame':   100 obs. of  2 variables:\n#>  $ lr  : num  1.15e-06 1.32e-06 1.51e-06 1.74e-06 2.00e-06 ...\n#>  $ loss: num  2.31 2.3 2.29 2.3 2.31 ...\n\n\nYou can use the built-in plot method to display the exact results, along\nwith an exponentially smoothed value of the loss.\n\n\nplot(records) +\n  ggplot2::coord_cartesian(ylim = c(NA, 5))\n\n\nPlot displaying the results of the lr_finder()If you want to learn how to interpret the results of this plot and learn\nmore about the methodology read the learning rate finder\narticle on the\nluz website.\nData handling\nIn the first release of luz, the only kind of object that was allowed to\nbe used as input data to fit was a torch dataloader(). As of version\n0.2.0, luz also support’s R matrices/arrays (or nested lists of them) as\ninput data, as well as torch dataset()s.\nSupporting low level abstractions like dataloader() as input data is\nimportant, as with them the user has full control over how input\ndata is loaded. For example, you can create parallel dataloaders,\nchange how shuffling is done, and more. However, having to manually\ndefine the dataloader seems unnecessarily tedious when you don’t need to\ncustomize any of this.\nAnother small improvement from version 0.2.0, inspired by Keras, is that\nyou can pass a value between 0 and 1 to fit’s valid_data parameter, and luz will\ntake a random sample of that proportion from the training set, to be used for\nvalidation data.\nRead more about this in the documentation of the\nfit()\nfunction.\nNew callbacks\nIn recent releases, new built-in callbacks were added to luz:\nluz_callback_gradient_clip(): Helps avoiding loss divergence by\nclipping large gradients.\nluz_callback_keep_best_model(): Each epoch, if there’s improvement\nin the monitored metric, we serialize the model weights to a temporary\nfile. When training is done, we reload weights from the best model.\nluz_callback_mixup(): Implementation of ‘mixup: Beyond Empirical\nRisk Minimization’\n(Zhang et al. 2017). Mixup is a nice data augmentation technique that\nhelps improving model consistency and overall performance.\nFinal remarks\nYou can see the full changelog available\nhere.\nIn this post we would also like to thank:\n@jonthegeek for valuable\nimprovements in the luz getting-started guides.\n@mattwarkentin for many good\nideas, improvements and bug fixes.\n@cmcmaster1 for the initial\nimplementation of the learning rate finder and other bug fixes.\n@skeydan for the implementation of the Mixup callback and improvements in the learning rate finder.\nThank you!\nPhoto by Dil on Unsplash\n\n\n\nHoward, Jeremy, and Sylvain Gugger. 2020. “Fastai: A Layered API for Deep Learning.” Information 11 (2): 108. https://doi.org/10.3390/info11020108.\n\n\nSmith, Leslie N. 2015. “Cyclical Learning Rates for Training Neural Networks.” https://doi.org/10.48550/ARXIV.1506.01186.\n\n\nZhang, Hongyi, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. 2017. “Mixup: Beyond Empirical Risk Minimization.” https://doi.org/10.48550/ARXIV.1710.09412.\n\n\n\n\n",
    "preview": "posts/2022-08-24-luz-0-3/images/bulbs.jpeg",
    "last_modified": "2024-11-21T15:54:11+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-06-09-tf-2-9/",
    "title": "TensorFlow and Keras 2.9",
    "description": "New TensorFlow and Keras releases bring improvements big and small.",
    "author": [
      {
        "name": "Tomasz Kalinowski",
        "url": {}
      }
    ],
    "date": "2022-06-09",
    "categories": [
      "TensorFlow/Keras",
      "Packages/Releases",
      "R"
    ],
    "contents": "\n\n\n\nThe release of Deep Learning with R, 2nd\nEdition coincides with new releases of\nTensorFlow and Keras. These releases bring many refinements that allow\nfor more idiomatic and concise R code.\nFirst, the set of Tensor methods for base R generics has greatly\nexpanded. The set of R generics that work with TensorFlow Tensors is now\nquite extensive:\n\n\nmethods(class = \"tensorflow.tensor\")\n\n\n\n [1] -           !           !=          [           [<-        \n [6] *           /           &           %/%         %%         \n[11] ^           +           <           <=          ==         \n[16] >           >=          |           abs         acos       \n[21] all         any         aperm       Arg         asin       \n[26] atan        cbind       ceiling     Conj        cos        \n[31] cospi       digamma     dim         exp         expm1      \n[36] floor       Im          is.finite   is.infinite is.nan     \n[41] length      lgamma      log         log10       log1p      \n[46] log2        max         mean        min         Mod        \n[51] print       prod        range       rbind       Re         \n[56] rep         round       sign        sin         sinpi      \n[61] sort        sqrt        str         sum         t          \n[66] tan         tanpi      \n\nThis means that often you can write the same code for TensorFlow Tensors\nas you would for R arrays. For example, consider this small function\nfrom Chapter 11 of the book:\n\n\nreweight_distribution <-\n  function(original_distribution, temperature = 0.5) {\n    original_distribution %>%\n      { exp(log(.) / temperature) } %>%\n      { . / sum(.) }\n  }\n\n\nNote that functions like reweight_distribution() work with both 1D R\nvectors and 1D TensorFlow Tensors, since exp(), log(), /, and\nsum() are all R generics with methods for TensorFlow Tensors.\nIn the same vein, this Keras release brings with it a refinement to the\nway custom class extensions to Keras are defined. Partially inspired by\nthe new R7 syntax, there is a\nnew family of functions: new_layer_class(), new_model_class(),\nnew_metric_class(), and so on. This new interface substantially\nsimplifies the amount of boilerplate code required to define custom\nKeras extensions—a pleasant R interface that serves as a facade over\nthe mechanics of sub-classing Python classes. This new interface is the\nyang to the yin of %py_class%–a way to mime the Python class\ndefinition syntax in R. Of course, the “raw” API of converting an\nR6Class() to Python via r_to_py() is still available for users that\nrequire full control.\nThis release also brings with it a cornucopia of small improvements\nthroughout the Keras R interface: updated print() and plot() methods\nfor models, enhancements to freeze_weights() and load_model_tf(),\nnew exported utilities like zip_lists() and %<>%. And let’s not\nforget to mention a new family of R functions for modifying the learning\nrate during training, with a suite of built-in schedules like\nlearning_rate_schedule_cosine_decay(), complemented by an interface\nfor creating custom schedules with new_learning_rate_schedule_class().\nYou can find the full release notes for the R packages here:\ntensorflow\nkeras\nThe release notes for the R packages tell only half the story however.\nThe R interfaces to Keras and TensorFlow work by embedding a full Python\nprocess in R (via the\nreticulate package). One of\nthe major benefits of this design is that R users have full access to\neverything in both R and Python. In other words, the R interface\nalways has feature parity with the Python interface—anything you can\ndo with TensorFlow in Python, you can do in R just as easily. This means\nthe release notes for the Python releases of TensorFlow are just as\nrelevant for R users:\nTensorFlow 2.9.0 release notes (Python\nedition)\nThanks for reading!\nPhoto by Raphael\nWild\non\nUnsplash\n\n\n\n",
    "preview": "posts/2022-06-09-tf-2-9/images/chameleon.jpg",
    "last_modified": "2024-11-21T15:51:53+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-31-deep-learning-with-R-2e/",
    "title": "Deep Learning with R, 2nd Edition",
    "description": "Announcing the release of \"Deep Learning with R, 2nd Edition,\" a book that shows you how to get started with deep learning in R.",
    "author": [
      {
        "name": "Tomasz Kalinowski",
        "url": {}
      }
    ],
    "date": "2022-05-31",
    "categories": [
      "TensorFlow/Keras",
      "R"
    ],
    "contents": "\nToday we’re pleased to announce the launch of Deep Learning with R,\n2nd Edition. Compared to the first edition,\nthe book is over a third longer, with more than 75% new content. It’s\nnot so much an updated edition as a whole new book.\nThis book shows you how to get started with deep learning in R, even if\nyou have no background in mathematics or data science. The book covers:\nDeep learning from first principles\nImage classification and image segmentation\nTime series forecasting\nText classification and machine translation\nText generation, neural style transfer, and image generation\nOnly modest R knowledge is assumed; everything else is explained from\nthe ground up with examples that plainly demonstrate the mechanics.\nLearn about gradients and backpropogation—by using tf$GradientTape()\nto rediscover Earth’s gravity acceleration constant (9.8 \\(m/s^2\\)). Learn\nwhat a keras Layer is—by implementing one from scratch using only\nbase R. Learn the difference between batch normalization and layer\nnormalization, what layer_lstm() does, what happens when you call\nfit(), and so on—all through implementations in plain R code.\nEvery section in the book has received major updates. The chapters on\ncomputer vision gain a full walk-through of how to approach an image\nsegmentation task. Sections on image classification have been updated to\nuse {tfdatasets} and Keras preprocessing layers, demonstrating not just\nhow to compose an efficient and fast data pipeline, but also how to\nadapt it when your dataset calls for it.\nThe chapters on text models have been completely reworked. Learn how to\npreprocess raw text for deep learning, first by implementing a text\nvectorization layer using only base R, before using\nkeras::layer_text_vectorization() in nine different ways. Learn about\nembedding layers by implementing a custom\nlayer_positional_embedding(). Learn about the transformer architecture\nby implementing a custom layer_transformer_encoder() and\nlayer_transformer_decoder(). And along the way put it all together by\ntraining text models—first, a movie-review sentiment classifier, then,\nan English-to-Spanish translator, and finally, a movie-review text\ngenerator.\nGenerative models have their own dedicated chapter, covering not only\ntext generation, but also variational auto encoders (VAE), generative\nadversarial networks (GAN), and style transfer.\nAlong each step of the way, you’ll find sprinkled intuitions distilled\nfrom experience and empirical observation about what works, what\ndoesn’t, and why. Answers to questions like: when should you use\nbag-of-words instead of a sequence architecture? When is it better to\nuse a pretrained model instead of training a model from scratch? When\nshould you use GRU instead of LSTM? When is it better to use separable\nconvolution instead of regular convolution? When training is unstable,\nwhat troubleshooting steps should you take? What can you do to make\ntraining faster?\nThe book shuns magic and hand-waving, and instead pulls back the curtain\non every necessary fundamental concept needed to apply deep learning.\nAfter working through the material in the book, you will not only know\nhow to apply deep learning to common tasks, but also have the context to\ngo and apply deep learning to new domains and new problems.\nDeep Learning with R, Second Edition\n\n\n\n",
    "preview": "posts/2022-05-31-deep-learning-with-R-2e/images/cover.png",
    "last_modified": "2024-11-21T15:52:02+00:00",
    "input_file": {},
    "preview_width": 600,
    "preview_height": 337
  },
  {
    "path": "posts/2022-05-18-torchopt/",
    "title": "Community spotlight: Fun with torchopt",
    "description": "Today, we want to call attention to a highly useful package in the torch ecosystem: torchopt. It extends torch by providing a set of popular optimization algorithms not available in the base library. As this post will show, it is also fun to use!",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2022-05-18",
    "categories": [
      "Torch",
      "R",
      "Packages/Releases"
    ],
    "contents": "\n\nContents\ntorchopt\nThe way it works\nWhy do they always say learning rate matters?\nA second-order optimizer for neural networks: ADAHESSIAN\nBest of the classics: Revisiting L-BFGS\nAppendix\n\nFrom the beginning, it has been exciting to watch the growing number of packages developing in the torch ecosystem. What’s amazing is the variety of things people do with torch: extend its functionality; integrate and put to domain-specific use its low-level automatic differentiation infrastructure; port neural network architectures … and last but not least, answer scientific questions.\nThis blog post will introduce, in short and rather subjective form, one of these packages: torchopt. Before we start, one thing we should probably say a lot more often: If you’d like to publish a post on this blog, on the package you’re developing or the way you employ R-language deep learning frameworks, let us know – you’re more than welcome!1\ntorchopt\ntorchopt is a package developed by Gilberto Camara and colleagues at National Institute for Space Research, Brazil.\nBy the look of it, the package’s reason of being is rather self-evident. torch itself does not – nor should it – implement all the newly-published, potentially-useful-for-your-purposes optimization algorithms out there.2 The algorithms assembled here, then, are probably exactly those the authors were most eager to experiment with in their own work. As of this writing, they comprise, amongst others, various members of the popular ADA* and *ADAM* families.3 And we may safely assume the list will grow over time.\nI’m going to introduce the package by highlighting something that technically, is “merely” a utility function, but to the user, can be extremely helpful: the ability to, for an arbitrary optimizer and an arbitrary test function, plot the steps taken in optimization.\nWhile it’s true that I have no intent of comparing (let alone analyzing) different strategies, there is one that, to me, stands out in the list: ADAHESSIAN (Yao et al. 2020), a second-order algorithm designed to scale to large neural networks. I’m especially curious to see how it behaves as compared to L-BFGS, the second-order “classic” available from base torch we’ve had a dedicated blog post about last year.\nThe way it works\nThe utility function in question is named test_optim(). The only required argument concerns the optimizer to try (optim). But you’ll likely want to tweak three others as well:\ntest_fn: To use a test function different from the default (beale). You can choose among the many provided in torchopt, or you can pass in your own. In the latter case, you also need to provide information about search domain and starting points. (We’ll see that in an instant.)\nsteps: To set the number of optimization steps.\nopt_hparams: To modify optimizer hyperparameters; most notably, the learning rate.\nHere, I’m going to use the flower() function that already prominently figured in the aforementioned post on L-BFGS. It approaches its minimum as it gets closer and closer to (0,0) (but is undefined at the origin itself).\nHere it is:\n\n\nflower <- function(x, y) {\n  a <- 1\n  b <- 1\n  c <- 4\n  a * torch_sqrt(torch_square(x) + torch_square(y)) + b * torch_sin(c * torch_atan2(y, x))\n}\n\n\nTo see how it looks, just scroll down a bit. The plot may be tweaked in a myriad of ways, but I’ll stick with the default layout, with colors of shorter wavelength mapped to lower function values.\nLet’s start our explorations.\nWhy do they always say learning rate matters?\nTrue, it’s a rhetorical question. But still, sometimes visualizations make for the most memorable evidence.\nHere, we use a popular first-order optimizer, AdamW (Loshchilov and Hutter 2017). We call it with its default learning rate, 0.01, and let the search run for two-hundred steps. As in that previous post, we start from far away – the point (20,20), way outside the rectangular region of interest.\n\n\nlibrary(torchopt)\nlibrary(torch)\n\ntest_optim(\n    # call with default learning rate (0.01)\n    optim = optim_adamw,\n    # pass in self-defined test function, plus a closure indicating starting points and search domain\n    test_fn = list(flower, function() (c(x0 = 20, y0 = 20, xmax = 3, xmin = -3, ymax = 3, ymin = -3))),\n    steps = 200\n)\n\n\nMinimizing the flower function with AdamW. Setup no. 1: default learning rate, 200 steps.Whoops, what happened? Is there an error in the plotting code? – Not at all; it’s just that after the maximum number of steps allowed, we haven’t yet entered the region of interest.\nNext, we scale up the learning rate by a factor of ten.\n\n\ntest_optim(\n    optim = optim_adamw,\n    # scale default rate by a factor of 10\n    opt_hparams = list(lr = 0.1),\n    test_fn = list(flower, function() (c(x0 = 20, y0 = 20, xmax = 3, xmin = -3, ymax = 3, ymin = -3))),\n    steps = 200\n)\n\n\nMinimizing the flower function with AdamW. Setup no. 2: lr = 0.1, 200 steps.What a change! With ten-fold learning rate, the result is optimal. Does this mean the default setting is bad? Of course not; the algorithm has been tuned to work well with neural networks, not some function that has been purposefully designed to present a specific challenge.\nNaturally, we also have to see what happens for yet higher a learning rate.\n\n\ntest_optim(\n    optim = optim_adamw,\n    # scale default rate by a factor of 70\n    opt_hparams = list(lr = 0.7),\n    test_fn = list(flower, function() (c(x0 = 20, y0 = 20, xmax = 3, xmin = -3, ymax = 3, ymin = -3))),\n    steps = 200\n)\n\n\nMinimizing the flower function with AdamW. Setup no. 3: lr = 0.7, 200 steps.We see the behavior we’ve always been warned about: Optimization hops around wildly, before seemingly heading off forever. (Seemingly, because in this case, this is not what happens. Instead, the search will jump far away, and back again, continuously.)\nNow, this might make one curious. What actually happens if we choose the “good” learning rate, but don’t stop optimizing at two-hundred steps? Here, we try three-hundred instead:\n\n\ntest_optim(\n    optim = optim_adamw,\n    # scale default rate by a factor of 10\n    opt_hparams = list(lr = 0.1),\n    test_fn = list(flower, function() (c(x0 = 20, y0 = 20, xmax = 3, xmin = -3, ymax = 3, ymin = -3))),\n    # this time, continue search until we reach step 300\n    steps = 300\n)\n\n\nMinimizing the flower function with AdamW. Setup no. 4: lr = 0.1, 300 steps.Interestingly, we see the same kind of to-and-fro happening here as with a higher learning rate – it’s just delayed in time.\nAnother playful question that comes to mind is: Can we track how the optimization process “explores” the four petals? With some quick experimentation, I arrived at this:\nMinimizing the flower function with AdamW, lr = 0.1: Successive “exploration” of petals. Steps (clockwise): 300, 700, 900, 1300.Who says you need chaos4 to produce a beautiful plot?\nA second-order optimizer for neural networks: ADAHESSIAN\nOn to the one algorithm I’d like to check out specifically. Subsequent to a little bit of learning-rate experimentation, I was able to arrive at an excellent result after just thirty-five steps.\n\n\ntest_optim(\n    optim = optim_adahessian,\n    opt_hparams = list(lr = 0.3),\n    test_fn = list(flower, function() (c(x0 = 20, y0 = 20, xmax = 3, xmin = -3, ymax = 3, ymin = -3))),\n    steps = 35\n)\n\n\nMinimizing the flower function with ADAHESSIAN. Setup no. 1: lr = 0.3, 35 steps.Given our recent experiences with AdamW though – meaning, its “just not settling in” very close to the minimum – we may want to run an equivalent test with ADAHESSIAN, as well. What happens if we go on optimizing quite a bit longer – for two-hundred steps, say?\n\n\ntest_optim(\n    optim = optim_adahessian,\n    opt_hparams = list(lr = 0.3),\n    test_fn = list(flower, function() (c(x0 = 20, y0 = 20, xmax = 3, xmin = -3, ymax = 3, ymin = -3))),\n    steps = 200\n)\n\n\nMinimizing the flower function with ADAHESSIAN. Setup no. 2: lr = 0.3, 200 steps.Like AdamW, ADAHESSIAN goes on to “explore” the petals, but it does not stray as far away from the minimum.\nIs this surprising? I wouldn’t say it is. The argument is the same as with AdamW, above: Its algorithm has been tuned to perform well on large neural networks, not to solve a classic, hand-crafted minimization task.\nNow we’ve heard that argument twice already, it’s time to verify the explicit assumption: that a classic second-order algorithm handles this better. In other words, it’s time to revisit L-BFGS.\nBest of the classics: Revisiting L-BFGS\nTo use test_optim() with L-BFGS, we need to take a little detour. If you’ve read the post on L-BFGS, you may remember that with this optimizer, it is necessary to wrap both the call to the test function and the evaluation of the gradient in a closure. (The reason being that both have to be callable several times per iteration.)\nNow, seeing how L-BFGS is a very special case, and few people are likely to use test_optim() with it in the future, it wouldn’t seem worthwhile to make that function handle different cases. For this on-off test, I simply copied and modified the code as required. The result, test_optim_lbfgs(), is found in the appendix.\nIn deciding what number of steps to try, we take into account that L-BFGS has a different concept of iterations than other optimizers; meaning, it may refine its search several times per step. Indeed, from the previous post I happen to know that three iterations are sufficient:\n\n\ntest_optim_lbfgs(\n    optim = optim_lbfgs,\n    opt_hparams = list(line_search_fn = \"strong_wolfe\"),\n    test_fn = list(flower, function() (c(x0 = 20, y0 = 20, xmax = 3, xmin = -3, ymax = 3, ymin = -3))),\n    steps = 3\n)\n\n\nMinimizing the flower function with L-BFGS. Setup no. 1: 3 steps.At this point, of course, I need to stick with my rule of testing what happens with “too many steps.” (Even though this time, I have strong reasons to believe that nothing will happen.)\n\n\ntest_optim_lbfgs(\n    optim = optim_lbfgs,\n    opt_hparams = list(line_search_fn = \"strong_wolfe\"),\n    test_fn = list(flower, function() (c(x0 = 20, y0 = 20, xmax = 3, xmin = -3, ymax = 3, ymin = -3))),\n    steps = 10\n)\n\n\nMinimizing the flower function with L-BFGS. Setup no. 2: 10 steps.Hypothesis confirmed.\nAnd here ends my playful and subjective introduction to torchopt. I certainly hope you liked it; but in any case, I think you should have gotten the impression that here is a useful, extensible and likely-to-grow package, to be watched out for in the future. As always, thanks for reading!\nAppendix\n\n\ntest_optim_lbfgs <- function(optim, ...,\n                       opt_hparams = NULL,\n                       test_fn = \"beale\",\n                       steps = 200,\n                       pt_start_color = \"#5050FF7F\",\n                       pt_end_color = \"#FF5050FF\",\n                       ln_color = \"#FF0000FF\",\n                       ln_weight = 2,\n                       bg_xy_breaks = 100,\n                       bg_z_breaks = 32,\n                       bg_palette = \"viridis\",\n                       ct_levels = 10,\n                       ct_labels = FALSE,\n                       ct_color = \"#FFFFFF7F\",\n                       plot_each_step = FALSE) {\n\n\n    if (is.character(test_fn)) {\n        # get starting points\n        domain_fn <- get(paste0(\"domain_\",test_fn),\n                         envir = asNamespace(\"torchopt\"),\n                         inherits = FALSE)\n        # get gradient function\n        test_fn <- get(test_fn,\n                       envir = asNamespace(\"torchopt\"),\n                       inherits = FALSE)\n    } else if (is.list(test_fn)) {\n        domain_fn <- test_fn[[2]]\n        test_fn <- test_fn[[1]]\n    }\n\n    # starting point\n    dom <- domain_fn()\n    x0 <- dom[[\"x0\"]]\n    y0 <- dom[[\"y0\"]]\n    # create tensor\n    x <- torch::torch_tensor(x0, requires_grad = TRUE)\n    y <- torch::torch_tensor(y0, requires_grad = TRUE)\n\n    # instantiate optimizer\n    optim <- do.call(optim, c(list(params = list(x, y)), opt_hparams))\n\n    # with L-BFGS, it is necessary to wrap both function call and gradient evaluation in a closure,\n    # for them to be callable several times per iteration.\n    calc_loss <- function() {\n      optim$zero_grad()\n      z <- test_fn(x, y)\n      z$backward()\n      z\n    }\n\n    # run optimizer\n    x_steps <- numeric(steps)\n    y_steps <- numeric(steps)\n    for (i in seq_len(steps)) {\n        x_steps[i] <- as.numeric(x)\n        y_steps[i] <- as.numeric(y)\n        optim$step(calc_loss)\n    }\n\n    # prepare plot\n    # get xy limits\n\n    xmax <- dom[[\"xmax\"]]\n    xmin <- dom[[\"xmin\"]]\n    ymax <- dom[[\"ymax\"]]\n    ymin <- dom[[\"ymin\"]]\n\n    # prepare data for gradient plot\n    x <- seq(xmin, xmax, length.out = bg_xy_breaks)\n    y <- seq(xmin, xmax, length.out = bg_xy_breaks)\n    z <- outer(X = x, Y = y, FUN = function(x, y) as.numeric(test_fn(x, y)))\n\n    plot_from_step <- steps\n    if (plot_each_step) {\n        plot_from_step <- 1\n    }\n\n    for (step in seq(plot_from_step, steps, 1)) {\n\n        # plot background\n        image(\n            x = x,\n            y = y,\n            z = z,\n            col = hcl.colors(\n                n = bg_z_breaks,\n                palette = bg_palette\n            ),\n            ...\n        )\n\n        # plot contour\n        if (ct_levels > 0) {\n            contour(\n                x = x,\n                y = y,\n                z = z,\n                nlevels = ct_levels,\n                drawlabels = ct_labels,\n                col = ct_color,\n                add = TRUE\n            )\n        }\n\n        # plot starting point\n        points(\n            x_steps[1],\n            y_steps[1],\n            pch = 21,\n            bg = pt_start_color\n        )\n\n        # plot path line\n        lines(\n            x_steps[seq_len(step)],\n            y_steps[seq_len(step)],\n            lwd = ln_weight,\n            col = ln_color\n        )\n\n        # plot end point\n        points(\n            x_steps[step],\n            y_steps[step],\n            pch = 21,\n            bg = pt_end_color\n        )\n    }\n}\n\n\n\n\n\nLoshchilov, Ilya, and Frank Hutter. 2017. “Fixing Weight Decay Regularization in Adam.” CoRR abs/1711.05101. http://arxiv.org/abs/1711.05101.\n\n\nYao, Zhewei, Amir Gholami, Sheng Shen, Kurt Keutzer, and Michael W. Mahoney. 2020. “ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning.” CoRR abs/2006.00719. https://arxiv.org/abs/2006.00719.\n\n\nFor example, by creating an issue on GitHub, in either the blog’s repo or that of the respective software (e.g., keras, torch, …)↩︎\nFor why it shouldn’t, and an overview of (some of) the things torch users can do, see also our recent post on extending torch.↩︎\nSee the Readme for an up-to-date listing.↩︎\nAlluding to strange attractors here. If you’re interested, you might want to check out the mini-series on chaos and deep learning: Deep attractors: Where deep learning meets chaos, Time series prediction with FNN-LSTM, and FNN-VAE for noisy time series forecasting.↩︎\n",
    "preview": "posts/2022-05-18-torchopt/images/petals.png",
    "last_modified": "2024-11-21T15:48:37+00:00",
    "input_file": {},
    "preview_width": 2304,
    "preview_height": 2304
  },
  {
    "path": "posts/2022-04-27-torch-outside-the-box/",
    "title": "torch outside the box",
    "description": "Sometimes, a software's best feature is the one you've added yourself. This post shows by example why you may want to extend torch, and how to proceed. It also explains a bit of what is going on in the background.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2022-04-27",
    "categories": [
      "Torch",
      "R"
    ],
    "contents": "\n\nContents\nEnablers: torchexport and Torchscript\ntorchexport: Manages the “type stack” and takes care of errors\nTorchScript: Allows for code generation “on the fly”\n\nScenario one: Load a TorchVision pre-trained model\nScenario two: Implement a custom module\nScenario three: Interface to PyTorch extensions built in/on C++ code\nWhat’s next\n\nFor better or worse, we live in an ever-changing world. Focusing on the better, one salient example is the abundance, as well as rapid evolution of software that helps us achieve our goals. With that blessing comes a challenge, though. We need to be able to actually use those new features, install that new library, integrate that novel technique into our package.\nWith torch, there’s so much we can accomplish as-is, only a tiny fraction of which has been hinted at on this blog. But if there’s one thing to be sure about, it’s that there never, ever will be a lack of demand for more things to do. Here are three scenarios that come to mind.\nload a pre-trained model that has been defined in Python (without having to manually port all the code)\nmodify a neural network module, so as to incorporate some novel algorithmic refinement (without incurring the performance cost of having the custom code execute in R)\nmake use of one of the many extension libraries available in the PyTorch ecosystem (with as little coding effort as possible)\nThis post will illustrate each of these use cases in order. From a practical point of view, this constitutes a gradual move from a user’s to a developer’s perspective. But behind the scenes, it’s really the same building blocks powering them all.\nEnablers: torchexport and Torchscript\nThe R package torchexport and (PyTorch-side) TorchScript operate on very different scales, and play very different roles. Nevertheless, both of them are important in this context, and I’d even say that the “smaller-scale” actor (torchexport) is the truly essential component, from an R user’s point of view. In part, that’s because it figures in all of the three scenarios, while TorchScript is involved only in the first.\ntorchexport: Manages the “type stack” and takes care of errors\nIn R torch, the depth of the “type stack” is dizzying. User-facing code is written in R; the low-level functionality is packaged in libtorch, a C++ shared library relied upon by torch as well as PyTorch. The mediator, as is so often the case, is Rcpp. However, that is not where the story ends. Due to OS-specific compiler incompatibilities, there has to be an additional, intermediate, bidirectionally-acting layer that strips all C++ types on one side of the bridge (Rcpp or libtorch, resp.), leaving just raw memory pointers, and adds them back on the other.1 In the end, what results is a pretty involved call stack. As you could imagine, there is an accompanying need for carefully-placed, level-adequate error handling, making sure the user is presented with usable information at the end.\nNow, what holds for torch applies to every R-side extension that adds custom code, or calls external C++ libraries. This is where torchexport comes in. As an extension author, all you need to do is write a tiny fraction of the code required overall – the rest will be generated by torchexport. We’ll come back to this in scenarios two and three.\nTorchScript: Allows for code generation “on the fly”\nWe’ve already encountered TorchScript in a prior post, albeit from a different angle, and highlighting a different set of terms. In that post, we showed how you can train a model in R and trace it, resulting in an intermediate, optimized representation that may then be saved and loaded in a different (possibly R-less) environment. There, the conceptual focus was on the agent enabling this workflow: the PyTorch Just-in-time Compiler (JIT) which generates the representation in question. We quickly mentioned that on the Python-side, there is another way to invoke the JIT: not on an instantiated, “living” model, but on scripted model-defining code. It is that second way, accordingly named scripting, that is relevant in the current context.\nEven though scripting is not available from R (unless the scripted code is written in Python2), we still benefit from its existence. When Python-side extension libraries use TorchScript (instead of normal C++ code), we don’t need to add bindings to the respective functions on the R (C++) side. Instead, everything is taken care of by PyTorch.\nThis – although completely transparent to the user – is what enables scenario one. In (Python) TorchVision, the pre-trained models provided will often make use of (model-dependent) special operators. Thanks to their having been scripted, we don’t need to add a binding for each operator, let alone re-implement them on the R side.\nHaving outlined some of the underlying functionality, we now present the scenarios themselves.\nScenario one: Load a TorchVision pre-trained model\nPerhaps you’ve already used one of the pre-trained models made available by TorchVision: A subset of these have been manually ported to torchvision, the R package. But there are more of them – a lot more. Many use specialized operators – ones seldom needed outside of some algorithm’s context. There would appear to be little use in creating R wrappers for those operators. And of course, the continual appearance of new models would require continual porting efforts, on our side.\nLuckily, there is an elegant and effective solution. All the necessary infrastructure is set up by the lean, dedicated-purpose package torchvisionlib. (It can afford to be lean due to the Python side’s liberal use of TorchScript, as explained in the previous section. But to the user – whose perspective I’m taking in this scenario – these details do not need to matter.)\nOnce you’ve installed and loaded torchvisionlib, you have the choice among an impressive number of image recognition-related models. The process, then, is two-fold:\nYou instantiate the model in Python, script it, and save it.\nYou load and use the model in R.\nHere is the first step. Note how, before scripting, we put the model into eval mode, thereby making sure all layers exhibit inference-time behavior.\n\nimport torch\nimport torchvision\n\nmodel = torchvision.models.segmentation.fcn_resnet50(pretrained = True)\nmodel.eval()\n\nscripted_model = torch.jit.script(model)\ntorch.jit.save(scripted_model, \"fcn_resnet50.pt\")\n\nThe second step is even shorter: Loading the model into R requires a single line.\n\n\nlibrary(torchvisionlib)\n\nmodel <- torch::jit_load(\"fcn_resnet50.pt\")\n\n\nAt this point, you can use the model to obtain predictions, or even integrate it as a building block into a larger architecture.\nScenario two: Implement a custom module\nWouldn’t it be wonderful if every new, well-received algorithm, every promising novel variant of a layer type, or – better still – the algorithm you have in mind to reveal to the world in your next paper was already implemented in torch?\nWell, maybe; but maybe not. The far more sustainable solution is to make it reasonably easy to extend torch in small, dedicated packages that each serve a clear-cut purpose, and are fast to install. A detailed and practical walkthrough of the process is provided by the package lltm. This package has a recursive touch to it. At the same time, it is an instance of a C++ torch extension, and serves as a tutorial showing how to create such an extension.\nThe README itself explains how the code should be structured, and why. If you’re interested in how torch itself has been designed, this is an elucidating read, regardless of whether or not you plan on writing an extension. In addition to that kind of behind-the-scenes information, the README has step-by-step instructions on how to proceed in practice. In line with the package’s purpose, the source code, too, is richly documented.\nAs already hinted at in the “Enablers” section, the reason I dare write “make it reasonably easy” (referring to creating a torch extension) is torchexport, the package that auto-generates conversion-related and error-handling C++ code on several layers in the “type stack”. Typically, you’ll find the amount of auto-generated code significantly exceeds that of the code you wrote yourself.\nScenario three: Interface to PyTorch extensions built in/on C++ code\nIt is anything but unlikely that, some day, you’ll come across a PyTorch extension that you wish were available in R. In case that extension were written in Python (exclusively), you’d translate it to R “by hand”, making use of whatever applicable functionality torch provides. Sometimes, though, that extension will contain a mixture of Python and C++ code. Then, you’ll need to bind to the low-level, C++ functionality in a manner analogous to how torch binds to libtorch – and now, all the typing requirements described above will apply to your extension in just the same way.\nAgain, it’s torchexport that comes to the rescue. And here, too, the lltm README still applies; it’s just that in lieu of writing your custom code, you’ll add bindings to externally-provided C++ functions. That done, you’ll have torchexport create all required infrastructure code.\nA template of sorts can be found in the torchsparse package (currently under development). The functions in csrc/src/torchsparse.cpp all call into PyTorch Sparse, with function declarations found in that project’s csrc/sparse.h.\nOnce you’re integrating with external C++ code in this way, an additional question may pose itself. Take an example from torchsparse. In the header file, you’ll notice return types such as std::tuple<torch::Tensor, torch::Tensor>, <torch::Tensor, torch::Tensor, <torch::optional<torch::Tensor>>, torch::Tensor>> … and more. In R torch (the C++ layer) we have torch::Tensor, and we have torch::optional<torch::Tensor>, as well. But we don’t have a custom type for every possible std::tuple you could construct. Just as having base torch provide all kinds of specialized, domain-specific functionality is not sustainable, it makes little sense for it to try to foresee all kinds of types that will ever be in demand.3\nAccordingly, types should be defined in the packages that need them. How exactly to do this is explained in the torchexport Custom Types vignette. When such a custom type is being used, torchexport needs to be told how the generated types, on various levels, should be named. This is why in such cases, instead of a terse //[[torch::export]], you’ll see lines like / [[torch::export(register_types=c(\"tensor_pair\", \"TensorPair\", \"void*\", \"torchsparse::tensor_pair\"))]]. The vignette explains this in detail.\nWhat’s next\n“What’s next” is a common way to end a post, replacing, say, “Conclusion” or “Wrapping up”. But here, it’s to be taken quite literally. We hope to do our best to make using, interfacing to, and extending torch as effortless as possible. Therefore, please let us know about any difficulties you’re facing, or problems you incur. Just create an issue in torchexport, lltm, torch, or whatever repository seems applicable.\nAs always, thanks for reading!\nPhoto by Antonino Visalli on Unsplash\n\nFor an architecture overview of torch and torch extensions, cf. the README of lltm, a “tutorial package” created to demonstrate the process of extending torch. We’ll refer to that package explicitly in scenario two.↩︎\nCf. the TorchScript vignette if you are interested in doing this.↩︎\nMaybe you’re wondering why just working with <std::tuple> will not do. It would, in principle, if there weren’t the requirement of an intermediate stage devoid of all type information. But having to have such a layer means having to be able to convert between typed objects and pointers, in both directions, and for that, all types need to be explicitly addressable.↩︎\n",
    "preview": "posts/2022-04-27-torch-outside-the-box/images/preview.jpg",
    "last_modified": "2024-11-21T15:49:15+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-09-keras-preprocessing-layers/",
    "title": "Pre-processing layers in keras: What they are and how to use them",
    "description": "For keras, the last two releases have brought important new functionality, in terms of both low-level infrastructure and workflow enhancements. This post focuses on an outstanding example of the latter category: a new family of layers designed to help with pre-processing, data-augmentation, and feature-engineering tasks.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      },
      {
        "name": "Tomasz Kalinowski",
        "url": {}
      }
    ],
    "date": "2021-12-09",
    "categories": [
      "TensorFlow/Keras",
      "R"
    ],
    "contents": "\n\nContents\nPre-processing layers in a nutshell\nHow pre-processing layers make life easier\nPre-processing layers that keep state\nUsing pre-processing layers for performance\nExporting a model, complete with pre-processing\nExample 1: Image data augmentation\nExample 2: Text vectorization\nWrapup\n\nData pre-processing: What you do to the data before feeding it to the model.\n— A simple definition that, in practice, leaves open many questions. Where, exactly, should pre-processing stop, and the model begin? Are steps like normalization, or various numerical transforms, part of the model, or the pre-processing? What about data augmentation? In sum, the line between what is pre-processing and what is modeling has always, at the edges, felt somewhat fluid.\nIn this situation, the advent of keras pre-processing layers changes a long-familiar picture.\nIn concrete terms, with keras, two alternatives tended to prevail: one, to do things upfront, in R; and two, to construct a tfdatasets pipeline. The former applied whenever we needed the complete data to extract some summary information. For example, when normalizing to a mean of zero and a standard deviation of one. But often, this meant that we had to transform back-and-forth between normalized and un-normalized versions at several points in the workflow. The tfdatasets approach, on the other hand, was elegant; however, it could require one to write a lot of low-level tensorflow code.\nPre-processing layers, available as of keras version 2.6.1, remove the need for upfront R operations, and integrate nicely with tfdatasets. But that is not all there is to them. In this post, we want to highlight four essential aspects:\nPre-processing layers significantly reduce coding effort. You could code those operations yourself; but not having to do so saves time, favors modular code, and helps to avoid errors.\nPre-processing layers – a subset of them, to be precise – can produce summary information before training proper, and make use of a saved state when called upon later.\nPre-processing layers can speed up training.\nPre-processing layers are, or can be made, part of the model, thus removing the need to implement independent pre-processing procedures in the deployment environment.\nFollowing a short introduction, we’ll expand on each of those points. We conclude with two end-to-end examples (involving images and text, respectively) that nicely illustrate those four aspects.\nPre-processing layers in a nutshell\nLike other keras layers, the ones we’re talking about here all start with layer_, and may be instantiated independently of model and data pipeline. Here, we create a layer that will randomly rotate images while training, by up to 45 degrees in both directions:\n\n\nlibrary(keras)\naug_layer <- layer_random_rotation(factor = 0.125)\n\n\nOnce we have such a layer, we can immediately test it on some dummy image.\n\n\nlibrary(tensorflow)\nimg <- k_eye(5) %>% k_reshape(c(5, 5, 1))\nimg[ , , 1]\n\n\ntf.Tensor(\n[[1. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0.]\n [0. 0. 1. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 1.]], shape=(5, 5), dtype=float32)\n“Testing the layer” now literally means calling it like a function:\n\n\naug_layer(img)[ , , 1]\n\n\ntf.Tensor(\n[[0.         0.         0.         0.         0.        ]\n [0.44459596 0.32453176 0.05410459 0.         0.        ]\n [0.15844001 0.4371609  1.         0.4371609  0.15844001]\n [0.         0.         0.05410453 0.3245318  0.44459593]\n [0.         0.         0.         0.         0.        ]], shape=(5, 5), dtype=float32)\nOnce instantiated, a layer can be used in two ways. Firstly, as part of the input pipeline.\nIn pseudocode:\n\n\n# pseudocode\nlibrary(tfdatasets)\n \ntrain_ds <- ... # define dataset\npreprocessing_layer <- ... # instantiate layer\n\ntrain_ds <- train_ds %>%\n  dataset_map(function(x, y) list(preprocessing_layer(x), y))\n\n\nSecondly, the way that seems most natural, for a layer: as a layer inside the model. Schematically:\n\n\n# pseudocode\ninput <- layer_input(shape = input_shape)\n\noutput <- input %>%\n  preprocessing_layer() %>%\n  rest_of_the_model()\n\nmodel <- keras_model(input, output)\n\n\nIn fact, the latter seems so obvious that you might be wondering: Why even allow for a tfdatasets-integrated alternative? We’ll expand on that shortly, when talking about performance.\nStateful layers – who are special enough to deserve their own section – can be used in both ways as well, but they require an additional step. More on that below.\nHow pre-processing layers make life easier\nDedicated layers exist for a multitude of data-transformation tasks. We can subsume them under two broad categories, feature engineering and data augmentation.\nFeature engineering\nThe need for feature engineering may arise with all types of data. With images, we don’t normally use that term for the “pedestrian” operations that are required for a model to process them: resizing, cropping, and such. Still, there are assumptions hidden in each of these operations , so we feel justified in our categorization. Be that as it may, layers in this group include layer_resizing(), layer_rescaling(), and layer_center_crop().\nWith text, the one functionality we couldn’t do without is vectorization. layer_text_vectorization() takes care of this for us. We’ll encounter this layer in the next section, as well as in the second full-code example.\nNow, on to what is normally seen as the domain of feature engineering: numerical and categorical (we might say: “spreadsheet”) data.\nFirst, numerical data often need to be normalized for neural networks to perform well – to achieve this, use layer_normalization(). Or maybe there is a reason we’d like to put continuous values into discrete categories. That’d be a task for layer_discretization().\nSecond, categorical data come in various formats (strings, integers …), and there’s always something that needs to be done in order to process them in a meaningful way. Often, you’ll want to embed them into a higher-dimensional space, using layer_embedding(). Now, embedding layers expect their inputs to be integers; to be precise: consecutive integers. Here, the layers to look for are layer_integer_lookup() and layer_string_lookup(): They will convert random integers (strings, respectively) to consecutive integer values. In a different scenario, there might be too many categories to allow for useful information extraction. In such cases, use layer_hashing() to bin the data. And finally, there’s layer_category_encoding() to produce the classical one-hot or multi-hot representations.\nData augmentation\nIn the second category, we find layers that execute \\[configurable\\] random operations on images. To name just a few of them: layer_random_crop(), layer_random_translation(), layer_random_rotation() … These are convenient not just in that they implement the required low-level functionality; when integrated into a model, they’re also workflow-aware: Any random operations will be executed during training only.\nNow we have an idea what these layers do for us, let’s focus on the specific case of state-preserving layers.\nPre-processing layers that keep state\nA layer that randomly perturbs images doesn’t need to know anything about the data. It just needs to follow a rule: With probability \\(p\\), do \\(x\\). A layer that’s supposed to vectorize text, on the other hand, needs to have a lookup table, matching character strings to integers. The same goes for a layer that maps contingent integers to an ordered set. And in both cases, the lookup table needs to be built upfront.\nWith stateful layers, this information-buildup is triggered by calling adapt() on a freshly-created layer instance. For example, here we instantiate and “condition” a layer that maps strings to consecutive integers:\n\n\ncolors <- c(\"cyan\", \"turquoise\", \"celeste\");\n\nlayer <- layer_string_lookup()\nlayer %>% adapt(colors)\n\n\nWe can check what’s in the lookup table:\n\n\nlayer$get_vocabulary()\n\n\n[1] \"[UNK]\"     \"turquoise\" \"cyan\"      \"celeste\"  \nThen, calling the layer will encode the arguments:\n\n\nlayer(c(\"azure\", \"cyan\"))\n\n\ntf.Tensor([0 2], shape=(2,), dtype=int64)\nlayer_string_lookup() works on individual character strings, and consequently, is the transformation adequate for string-valued categorical features. To encode whole sentences (or paragraphs, or any chunks of text) you’d use layer_text_vectorization() instead. We’ll see how that works in our second end-to-end example.\nUsing pre-processing layers for performance\nAbove, we said that pre-processing layers could be used in two ways: as part of the model, or as part of the data input pipeline. If these are layers, why even allow for the second way?\nThe main reason is performance. GPUs are great at regular matrix operations, such as those involved in image manipulation and transformations of uniformly-shaped numerical data. Therefore, if you have a GPU to train on, it is preferable to have image processing layers, or layers such as layer_normalization(), be part of the model (which is run completely on GPU).\nOn the other hand, operations involving text, such as layer_text_vectorization(), are best executed on the CPU. The same holds if no GPU is available for training. In these cases, you would move the layers to the input pipeline, and strive to benefit from parallel – on-CPU – processing. For example:\n\n\n# pseudocode\n\npreprocessing_layer <- ... # instantiate layer\n\ndataset <- dataset %>%\n  dataset_map(~list(text_vectorizer(.x), .y),\n              num_parallel_calls = tf$data$AUTOTUNE) %>%\n  dataset_prefetch()\nmodel %>% fit(dataset)\n\n\nAccordingly, in the end-to-end examples below, you’ll see image data augmentation happening as part of the model, and text vectorization, as part of the input pipeline.\nExporting a model, complete with pre-processing\nSay that for training your model, you found that the tfdatasets way was the best. Now, you deploy it to a server that does not have R installed. It would seem like that either, you have to implement pre-processing in some other, available, technology. Alternatively, you’d have to rely on users sending already-pre-processed data.\nFortunately, there is something else you can do. Create a new model specifically for inference, like so:\n\n\n# pseudocode\n\ninput <- layer_input(shape = input_shape)\n\noutput <- input %>%\n  preprocessing_layer(input) %>%\n  training_model()\n\ninference_model <- keras_model(input, output)\n\n\nThis technique makes use of the functional API to create a new model that prepends the pre-processing layer to the pre-processing-less, original model.\nHaving focused on a few things especially “good to know”, we now conclude with the promised examples.\nExample 1: Image data augmentation\nOur first example demonstrates image data augmentation. Three types of transformations are grouped together, making them stand out clearly in the overall model definition. This group of layers will be active during training only.\n\n\nlibrary(keras)\nlibrary(tfdatasets)\n\n# Load CIFAR-10 data that come with keras\nc(c(x_train, y_train), ...) %<-% dataset_cifar10()\ninput_shape <- dim(x_train)[-1] # drop batch dim\nclasses <- 10\n\n# Create a tf_dataset pipeline \ntrain_dataset <- tensor_slices_dataset(list(x_train, y_train)) %>%\n  dataset_batch(16) \n\n# Use a (non-trained) ResNet architecture\nresnet <- application_resnet50(weights = NULL,\n                               input_shape = input_shape,\n                               classes = classes)\n\n# Create a data augmentation stage with horizontal flipping, rotations, zooms\ndata_augmentation <-\n  keras_model_sequential() %>%\n  layer_random_flip(\"horizontal\") %>%\n  layer_random_rotation(0.1) %>%\n  layer_random_zoom(0.1)\n\ninput <- layer_input(shape = input_shape)\n\n# Define and run the model\noutput <- input %>%\n  layer_rescaling(1 / 255) %>%   # rescale inputs\n  data_augmentation() %>%\n  resnet()\n\nmodel <- keras_model(input, output) %>%\n  compile(optimizer = \"rmsprop\", loss = \"sparse_categorical_crossentropy\") %>%\n  fit(train_dataset, steps_per_epoch = 5)\n\n\nExample 2: Text vectorization\nIn natural language processing, we often use embedding layers to present the “workhorse” (recurrent, convolutional, self-attentional, what have you) layers with the continuous, optimally-dimensioned input they need. Embedding layers expect tokens to be encoded as integers, and transform text to integers is what layer_text_vectorization() does.\nOur second example demonstrates the workflow: You have the layer learn the vocabulary upfront, then call it as part of the pre-processing pipeline. Once training has finished, we create an “all-inclusive” model for deployment.\n\n\nlibrary(tensorflow)\nlibrary(tfdatasets)\nlibrary(keras)\n\n# Example data\ntext <- as_tensor(c(\n  \"From each according to his ability, to each according to his needs!\",\n  \"Act that you use humanity, whether in your own person or in the person of any other, always at the same time as an end, never merely as a means.\",\n  \"Reason is, and ought only to be the slave of the passions, and can never pretend to any other office than to serve and obey them.\"\n))\n\n# Create and adapt layer\ntext_vectorizer <- layer_text_vectorization(output_mode=\"int\")\ntext_vectorizer %>% adapt(text)\n\n# Check\nas.array(text_vectorizer(\"To each according to his needs\"))\n\n# Create a simple classification model\ninput <- layer_input(shape(NULL), dtype=\"int64\")\n\noutput <- input %>%\n  layer_embedding(input_dim = text_vectorizer$vocabulary_size(),\n                  output_dim = 16) %>%\n  layer_gru(8) %>%\n  layer_dense(1, activation = \"sigmoid\")\n\nmodel <- keras_model(input, output)\n\n# Create a labeled dataset (which includes unknown tokens)\ntrain_dataset <- tensor_slices_dataset(list(\n    c(\"From each according to his ability\", \"There is nothing higher than reason.\"),\n    c(1L, 0L)\n))\n\n# Preprocess the string inputs\ntrain_dataset <- train_dataset %>%\n  dataset_batch(2) %>%\n  dataset_map(~list(text_vectorizer(.x), .y),\n              num_parallel_calls = tf$data$AUTOTUNE)\n\n# Train the model\nmodel %>%\n  compile(optimizer = \"adam\", loss = \"binary_crossentropy\") %>%\n  fit(train_dataset)\n\n# export inference model that accepts strings as input\ninput <- layer_input(shape = 1, dtype=\"string\")\noutput <- input %>%\n  text_vectorizer() %>%\n  model()\n\nend_to_end_model <- keras_model(input, output)\n\n# Test inference model\ntest_data <- as_tensor(c(\n  \"To each according to his needs!\",\n  \"Reason is, and ought only to be the slave of the passions.\"\n))\ntest_output <- end_to_end_model(test_data)\nas.array(test_output)\n\n\nWrapup\nWith this post, our goal was to call attention to keras’ new pre-processing layers, and show how – and why – they are useful. Many more use cases can be found in the vignette.\nThanks for reading!\nPhoto by Henning Borgersen on Unsplash\n\n\n\n",
    "preview": "posts/2021-12-09-keras-preprocessing-layers/images/preview.jpg",
    "last_modified": "2024-11-21T15:53:03+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-18-keras-updates/",
    "title": "Revisiting Keras for R",
    "description": "It's been a while since this blog featured content about Keras for R, so you might've thought that the project was dormant. It's not! In fact, Keras for R is better than ever, with two recent releases adding powerful capabilities that considerably lighten previously tedious tasks. This post provides a high-level overview. Future posts will go into more detail on some of the most helpful new features, as well as dive into the powerful low-level enhancements that make the former possible.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      },
      {
        "name": "Tomasz Kalinowski",
        "url": {}
      }
    ],
    "date": "2021-11-18",
    "categories": [
      "Packages/Releases",
      "TensorFlow/Keras",
      "R"
    ],
    "contents": "\n\nContents\nState of the ecosystem\nHave your cake and eat it, too: A philosophy of (R) Keras\nNew in Keras 2.6/7: Three highlights\nPre-processing layers\nSubclassing Python\nRNN cell API\n\n\nBefore we even talk about new features, let us answer the obvious question. Yes, there will be a second edition of Deep Learning for R! Reflecting what has been going on in the meantime, the new edition covers an extended set of proven architectures; at the same time, you’ll find that intermediate-to-advanced designs already present in the first edition have become rather more intuitive to implement, thanks to the new low-level enhancements alluded to in the summary.\nBut don’t get us wrong – the scope of the book is completely unchanged. It is still the perfect choice for people new to machine learning and deep learning. Starting from the basic ideas, it systematically progresses to intermediate and advanced topics, leaving you with both a conceptual understanding and a bag of useful application templates.\nNow, what has been going on with Keras?\nState of the ecosystem\nLet us start with a characterization of the ecosystem, and a few words on its history.\nIn this post, when we say Keras, we mean R – as opposed to Python – Keras1. Now, this immediately translates to the R package keras. But keras alone wouldn’t get you far. While keras provides the high-level functionality – neural network layers, optimizers, workflow management, and more – the basic data structure operated upon, tensors, lives in tensorflow. Thirdly, as soon as you’ll need to perform less-then-trivial pre-processing, or can no longer keep the whole training set in memory because of its size, you’ll want to look into tfdatasets.\nSo it is these three packages – tensorflow, tfdatasets, and keras – that should be understood by “Keras” in the current context2. (The R-Keras ecosystem, on the other hand, is quite a bit bigger. But other packages, such as tfruns or cloudml, are more decoupled from the core.)\nMatching their tight integration, the aforementioned packages tend to follow a common release cycle, itself dependent on the underlying Python library, TensorFlow. For each of tensorflow, tfdatasets, and keras , the current CRAN version is 2.7.0, reflecting the corresponding Python version. The synchrony of versioning between the two Kerases, R and Python, seems to indicate that their fates had developed in similar ways. Nothing could be less true, and knowing this can be helpful.\nIn R, between present-from-the-outset packages tensorflow and keras, responsibilities have always been distributed the way they are now: tensorflow providing indispensable basics, but often, remaining completely transparent to the user; keras being the thing you use in your code. In fact, it is possible to train a Keras model without ever consciously using tensorflow.\nOn the Python side, things have been undergoing significant changes, ones where, in some sense, the latter development has been inverting the first. In the beginning, TensorFlow and Keras were separate libraries, with TensorFlow providing a backend – one among several – for Keras to make use of. At some point, Keras code got incorporated into the TensorFlow codebase. Finally (as of today), following an extended period of slight confusion, Keras got moved out again, and has started to – again – considerably grow in features.\nIt is just that quick growth that has created, on the R side, the need for extensive low-level refactoring and enhancements. (Of course, the user-facing new functionality itself also had to be implemented!)\nBefore we get to the promised highlights, a word on how we think about Keras.\nHave your cake and eat it, too: A philosophy of (R) Keras\nIf you’ve used Keras in the past, you know what it’s always been intended to be: a high-level library, making it easy (as far as such a thing can be easy) to train neural networks in R. Actually, it’s not just about ease. Keras enables users to write natural-feeling, idiomatic-looking code. This, to a high degree, is achieved by its allowing for object composition though the pipe operator; it is also a consequence of its abundant wrappers, convenience functions, and functional (stateless) semantics.3\nHowever, due to the way TensorFlow and Keras have developed on the Python side – referring to the big architectural and semantic changes between versions 1.x and 2.x, first comprehensively characterized on this blog here – it has become more challenging to provide all of the functionality available on the Python side to the R user. In addition, maintaining compatibility with several versions of Python TensorFlow – something R Keras has always done – by necessity gets more and more challenging, the more wrappers and convenience functions you add.\nSo this is where we complement the above “make it R-like and natural, where possible” with “make it easy to port from Python, where necessary”. With the new low-level functionality, you won’t have to wait for R wrappers to make use of Python-defined objects. Instead, Python objects may be sub-classed directly from R; and any additional functionality you’d like to add to the subclass is defined in a Python-like syntax. What this means, concretely, is that translating Python code to R has become a lot easier. We’ll catch a glimpse of this in the second of our three highlights.\nNew in Keras 2.6/7: Three highlights\nAmong the many new capabilities added in Keras 2.6 and 2.7, we quickly introduce three of the most important.\nPre-processing layers significantly help to streamline the training workflow, integrating data manipulation and data augmentation.\nThe ability to subclass Python objects (already alluded to several times) is the new low-level magic available to the keras user and which powers many user-facing enhancements underneath.\nRecurrent neural network (RNN) layers gain a new cell-level API.\nOf these, the first two definitely deserve some deeper treatment; more detailed posts will follow.\nPre-processing layers\nBefore the advent of these dedicated layers, pre-processing used to be done as part of the tfdatasets pipeline. You would chain operations as required; maybe, integrating random transformations to be applied while training. Depending on what you wanted to achieve, significant programming effort may have ensued.\nThis is one area where the new capabilities can help. Pre-processing layers exist for several types of data, allowing for the usual “data wrangling”, as well as data augmentation and feature engineering (as in, hashing categorical data, or vectorizing text).\nThe mention of text vectorization leads to a second advantage. Unlike, say, a random distortion, vectorization is not something that may be forgotten about once done. We don’t want to lose the original information, namely, the words. The same happens, for numerical data, with normalization. We need to keep the summary statistics. This means there are two types of pre-processing layers: stateless and stateful ones. The former are part of the training process; the latter are called in advance.\nStateless layers, on the other hand, can appear in two places in the training workflow: as part of the tfdatasets pipeline, or as part of the model.\nThis is, schematically, how the former would look.\n\n\nlibrary(tfdatasets)\ndataset <- ... # define dataset\ndataset <- dataset %>%\n  dataset_map(function(x, y) list(preprocessing_layer(x), y))\n\n\nWhile here, the pre-processing layer is the first in a larger model:\n\n\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  preprocessing_layer() %>%\n  rest_of_the_model()\nmodel <- keras_model(input, output)\n\n\nWe’ll talk about which way is preferable when, as well as showcase a few specialized layers in a future post. Until then, please feel free to consult the – detailed and example-rich vignette.\nSubclassing Python\nImagine you wanted to port a Python model that made use of the following constraint:\n\nclass NonNegative(tf.keras.constraints.Constraint):\n    def __call__(self, w):\n        return w * tf.cast(tf.math.greater_equal(w, 0.), w.dtype)\n\nHow can we have such a thing in R? Previously, there used to exist4 various methods to create Python-based objects, both R6-based and functional-style. The former, in all but the most straightforward cases, could be effort-rich and error-prone; the latter, elegant-in-style but hard to adapt to more advanced requirements.\nThe new way, %py_class%, now allows for translating the above code like this:\n\n\nNonNegative(keras$constraints$Constraint) %py_class% {\n  \"__call__\" <- function(x) {\n    w * k_cast(w >= 0, k_floatx())\n  }\n}\n\n\nUsing %py_class%, we directly subclass the Python object tf.keras.constraints.Constraint, and override its __call__ method.\nWhy is this so powerful? The first advantage is visible from the example: Translating Python code becomes an almost mechanical task. But there’s more: The above method is independent from what kind of object you’re subclassing. Want to implement a new layer? A callback? A loss? An optimizer? The procedure is always the same. No need to find a pre-defined R6 object in the keras codebase; one %py_class% delivers them all.\nThere is a lot more to say on this topic, though; in fact, if you don’t want to use %py_class% directly, there are wrappers available for the most frequent use cases. More on this in a dedicated post. Until then, consult the vignette for numerous examples, syntactic sugar, and low-level details.\nRNN cell API\nOur third point is at least half as much shout-out to excellent documentation as alert to a new feature. The piece of documentation in question is a new vignette on RNNs. The vignette gives a useful overview of how RNNs function in Keras, addressing the usual questions that tend to come up once you haven’t been using them in a while: What exactly are states vs. outputs, and when does a layer return what? How do I initialize the state in an application-dependent way? What’s the difference between stateful and stateless RNNs?\nIn addition, the vignette covers more advanced questions: How do I pass nested data to an RNN? How do I write custom cells?\nIn fact, this latter question brings us to the new feature we wanted to call out: the new cell-level API. Conceptually, with RNNs, there’s always two things involved: the logic of what happens at a single timestep; and the threading of state across timesteps. So-called “simple RNNs” are concerned with the latter (recursion) aspect only; they tend to exhibit the classic vanishing-gradients problem. Gated architectures, such as the LSTM and the GRU, have specially been designed to avoid those problems; both can be easily integrated into a model using the respective layer_x() constructors. What if you’d like, not a GRU, but something like a GRU (using some fancy new activation method, say)?\nWith Keras 2.7, you can now create a single-timestep RNN cell (using the above-described %py_class% API), and obtain a recursive version – a complete layer – using layer_rnn():\n\n\nrnn <- layer_rnn(cell = cell)\n\n\nIf you’re interested, check out the vignette for an extended example.\nWith that, we end our news from Keras, for today. Thanks for reading, and stay tuned for more!\nPhoto by Hans-Jurgen Mager on Unsplash\n\nOriginally, both Keras and Tensorflow are Python libraries, bound to by R at runtime. In practice, this means that those Python libraries have to be present on the R user’s machine. This is in contrast to R torch , which has no Python dependencies. (Instead, it directly binds to a C++ library.)↩︎\nDefinitely, tfautograph belongs in this group as well, providing even-lower-level support for already-low-level tensorflow. But normally, the user will not have to call its functions directly.↩︎\nProvided stateless semantics is possible, in some context. This is anything but a matter of course, with an object-oriented language like Python in the background.↩︎\nWe’re not implying they don’t exist anymore; they do. But the preferred method is now %py_class%.↩︎\n",
    "preview": "posts/2021-11-18-keras-updates/images/preview.jpg",
    "last_modified": "2024-11-21T15:49:05+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-29-segmentation-torch-android/",
    "title": "Train in R, run on Android: Image segmentation with torch",
    "description": "We train a model for image segmentation in R, using torch together with luz, its high-level interface. We then JIT-trace the model on example input, so as to obtain an optimized representation that can run with no R installed. Finally, we show the model being run on Android.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2021-10-29",
    "categories": [
      "Torch",
      "Image Recognition & Image Processing",
      "R"
    ],
    "contents": "\n\nContents\nTrain in R\nPre-processing and data augmentation\nModel definition\nModel training and (visual) evaluation\n\nJIT-trace and run on Android\n\nIn a sense, image segmentation is not that different from image classification. It’s just that instead of categorizing an image as a whole, segmentation results in a label for every single pixel. And as in image classification, the categories of interest depend on the task: Foreground versus background, say; different types of tissue; different types of vegetation; et cetera.\nThe present post is not the first on this blog to treat that topic; and like all prior1 ones, it makes use of a U-Net architecture2 to achieve its goal. Central characteristics (of this post, not U-Net) are:\nIt demonstrates how to perform data augmentation for an image segmentation task.\nIt uses luz, torch’s high-level interface, to train the model.\nIt JIT-traces the trained model and saves it for deployment on mobile devices. (JIT being the acronym commonly used for the torch just-in-time compiler.)\nIt includes proof-of-concept code (though not a discussion) of the saved model being run on Android.\nAnd if you think that this in itself is not exciting enough – our task here is to find cats and dogs. What could be more helpful than a mobile application making sure you can distinguish your cat from the fluffy sofa she’s reposing on?\nA cat from the Oxford Pet Dataset (Parkhi et al. (2012)).Train in R\nWe start by preparing the data.\nPre-processing and data augmentation\nAs provided by torchdatasets, the Oxford Pet Dataset comes with three variants of target data to choose from: the overall class (cat or dog), the individual breed (there are thirty-seven of them), and a pixel-level segmentation with three categories: foreground, boundary, and background. The latter is the default; and it’s exactly the type of target we need.\nA call to oxford_pet_dataset(root = dir) will trigger the initial download:\n\n\n# need torch > 0.6.1\n# may have to run remotes::install_github(\"mlverse/torch\", ref = remotes::github_pull(\"713\")) depending on when you read this\nlibrary(torch) \nlibrary(torchvision)\nlibrary(torchdatasets)\nlibrary(luz)\n\ndir <- \"~/.torch-datasets/oxford_pet_dataset\"\n\nds <- oxford_pet_dataset(root = dir)\n\n\nImages (and corresponding masks) come in different sizes. For training, however, we’ll need all of them to be the same size. This can be accomplished by passing in transform = and target_transform = arguments. But what about data augmentation (basically always a useful measure to take)? Imagine we make use of random flipping. An input image will be flipped – or not – according to some probability. But if the image is flipped, the mask better had be, as well! Input and target transformations are not independent, in this case.\nA solution is to create a wrapper around oxford_pet_dataset() that lets us “hook into” the .getitem() method, like so:\n\n\npet_dataset <- torch::dataset(\n  \n  inherit = oxford_pet_dataset,\n  \n  initialize = function(..., size, normalize = TRUE, augmentation = NULL) {\n    \n    self$augmentation <- augmentation\n    \n    input_transform <- function(x) {\n      x <- x %>%\n        transform_to_tensor() %>%\n        transform_resize(size) \n      # we'll make use of pre-trained MobileNet v2 as a feature extractor\n      # => normalize in order to match the distribution of images it was trained with\n      if (isTRUE(normalize)) x <- x %>%\n        transform_normalize(mean = c(0.485, 0.456, 0.406),\n                            std = c(0.229, 0.224, 0.225))\n      x\n    }\n    \n    target_transform <- function(x) {\n      x <- torch_tensor(x, dtype = torch_long())\n      x <- x[newaxis,..]\n      # interpolation = 0 makes sure we still end up with integer classes\n      x <- transform_resize(x, size, interpolation = 0)\n    }\n    \n    super$initialize(\n      ...,\n      transform = input_transform,\n      target_transform = target_transform\n    )\n    \n  },\n  .getitem = function(i) {\n    \n    item <- super$.getitem(i)\n    if (!is.null(self$augmentation)) \n      self$augmentation(item)\n    else\n      list(x = item$x, y = item$y[1,..])\n  }\n)\n\n\nAll we have to do now is create a custom function that lets us decide on what augmentation to apply to each input-target pair, and then, manually call the respective transformation functions.\nHere, we flip, on average, every second image, and if we do, we flip the mask as well. The second transformation – orchestrating random changes in brightness, saturation, and contrast – is applied to the input image only.\n\n\naugmentation <- function(item) {\n  \n  vflip <- runif(1) > 0.5\n  \n  x <- item$x\n  y <- item$y\n  \n  if (isTRUE(vflip)) {\n    x <- transform_vflip(x)\n    y <- transform_vflip(y)\n  }\n  \n  x <- transform_color_jitter(x, brightness = 0.5, saturation = 0.3, contrast = 0.3)\n  \n  list(x = x, y = y[1,..])\n  \n}\n\n\nWe now make use of the wrapper, pet_dataset(), to instantiate the training and validation sets, and create the respective data loaders.\n\n\ntrain_ds <- pet_dataset(root = dir,\n                        split = \"train\",\n                        size = c(224, 224),\n                        augmentation = augmentation)\nvalid_ds <- pet_dataset(root = dir,\n                        split = \"valid\",\n                        size = c(224, 224))\n\ntrain_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE)\nvalid_dl <- dataloader(valid_ds, batch_size = 32)\n\n\nModel definition\nThe model implements a classic U-Net architecture, with an encoding stage (the “down” pass), a decoding stage (the “up” pass), and importantly, a “bridge” that passes features preserved from the encoding stage on to corresponding layers in the decoding stage.\nEncoder\nFirst, we have the encoder. It uses a pre-trained model (MobileNet v2) as its feature extractor.\nThe encoder splits up MobileNet v2’s feature extraction blocks into several stages, and applies one stage after the other. Respective results are saved in a list.\n\n\nencoder <- nn_module(\n  \n  initialize = function() {\n    model <- model_mobilenet_v2(pretrained = TRUE)\n    self$stages <- nn_module_list(list(\n      nn_identity(),\n      model$features[1:2],\n      model$features[3:4],\n      model$features[5:7],\n      model$features[8:14],\n      model$features[15:18]\n    ))\n\n    for (par in self$parameters) {\n      par$requires_grad_(FALSE)\n    }\n\n  },\n  forward = function(x) {\n    features <- list()\n    for (i in 1:length(self$stages)) {\n      x <- self$stages[[i]](x)\n      features[[length(features) + 1]] <- x\n    }\n    features\n  }\n)\n\n\nDecoder\nThe decoder is made up of configurable blocks. A block receives two input tensors: one that is the result of applying the previous decoder block, and one that holds the feature map produced in the matching encoder stage. In the forward pass, first the former is upsampled, and passed through a nonlinearity. The intermediate result is then prepended to the second argument, the channeled-through feature map. On the resultant tensor, a convolution is applied, followed by another nonlinearity.\n\n\ndecoder_block <- nn_module(\n  \n  initialize = function(in_channels, skip_channels, out_channels) {\n    self$upsample <- nn_conv_transpose2d(\n      in_channels = in_channels,\n      out_channels = out_channels,\n      kernel_size = 2,\n      stride = 2\n    )\n    self$activation <- nn_relu()\n    self$conv <- nn_conv2d(\n      in_channels = out_channels + skip_channels,\n      out_channels = out_channels,\n      kernel_size = 3,\n      padding = \"same\"\n    )\n  },\n  forward = function(x, skip) {\n    x <- x %>%\n      self$upsample() %>%\n      self$activation()\n\n    input <- torch_cat(list(x, skip), dim = 2)\n\n    input %>%\n      self$conv() %>%\n      self$activation()\n  }\n)\n\n\nThe decoder itself “just” instantiates and runs through the blocks:\n\n\ndecoder <- nn_module(\n  \n  initialize = function(\n    decoder_channels = c(256, 128, 64, 32, 16),\n    encoder_channels = c(16, 24, 32, 96, 320)\n  ) {\n\n    encoder_channels <- rev(encoder_channels)\n    skip_channels <- c(encoder_channels[-1], 3)\n    in_channels <- c(encoder_channels[1], decoder_channels)\n\n    depth <- length(encoder_channels)\n\n    self$blocks <- nn_module_list()\n    for (i in seq_len(depth)) {\n      self$blocks$append(decoder_block(\n        in_channels = in_channels[i],\n        skip_channels = skip_channels[i],\n        out_channels = decoder_channels[i]\n      ))\n    }\n\n  },\n  forward = function(features) {\n    features <- rev(features)\n    x <- features[[1]]\n    for (i in seq_along(self$blocks)) {\n      x <- self$blocks[[i]](x, features[[i+1]])\n    }\n    x\n  }\n)\n\n\nTop-level module\nFinally, the top-level module generates the class score. In our task, there are three pixel classes. The score-producing submodule can then just be a final convolution, producing three channels:\n\n\nmodel <- nn_module(\n  \n  initialize = function() {\n    self$encoder <- encoder()\n    self$decoder <- decoder()\n    self$output <- nn_sequential(\n      nn_conv2d(in_channels = 16,\n                out_channels = 3,\n                kernel_size = 3,\n                padding = \"same\")\n    )\n  },\n  forward = function(x) {\n    x %>%\n      self$encoder() %>%\n      self$decoder() %>%\n      self$output()\n  }\n)\n\n\nModel training and (visual) evaluation\nWith luz, model training is a matter of two verbs, setup() and fit(). The learning rate has been determined, for this specific case, using luz::lr_finder(); you will likely have to change it when experimenting with different forms of data augmentation (and different data sets).\n\n\nmodel <- model %>%\n  setup(optimizer = optim_adam, loss = nn_cross_entropy_loss())\n\nfitted <- model %>%\n  set_opt_hparams(lr = 1e-3) %>%\n  fit(train_dl, epochs = 10, valid_data = valid_dl)\n\n\nHere is an excerpt of how training performance developed in my case:\n# Epoch 1/10\n# Train metrics: Loss: 0.504                                                           \n# Valid metrics: Loss: 0.3154\n\n# Epoch 2/10\n# Train metrics: Loss: 0.2845                                                           \n# Valid metrics: Loss: 0.2549\n\n...\n...\n\n# Epoch 9/10\n# Train metrics: Loss: 0.1368                                                           \n# Valid metrics: Loss: 0.2332\n\n# Epoch 10/10\n# Train metrics: Loss: 0.1299                                                           \n# Valid metrics: Loss: 0.2511\nNumbers are just numbers – how good is the trained model really at segmenting pet images? To find out, we generate segmentation masks for the first eight observations in the validation set, and plot them overlaid on the images. A convenient way to plot an image and superimpose a mask is provided by the raster package.\n\n\nlibrary(raster)\n\n\nPixel intensities have to be between zero and one, which is why in the dataset wrapper, we have made it so normalization can be switched off. To plot the actual images, we just instantiate a clone of valid_ds that leaves the pixel values unchanged. (The predictions, on the other hand, will still have to be obtained from the original validation set.)\n\n\nvalid_ds_4plot <- pet_dataset(\n  root = dir,\n  split = \"valid\",\n  size = c(224, 224),\n  normalize = FALSE\n)\n\n\nFinally, the predictions are generated in a loop, and overlaid over the images one-by-one:\n\n\nindices <- 1:8\n\npreds <- predict(fitted, dataloader(dataset_subset(valid_ds, indices)))\n\npng(\"pet_segmentation.png\", width = 1200, height = 600, bg = \"black\")\n\npar(mfcol = c(2, 4), mar = rep(2, 4))\n\nfor (i in indices) {\n  \n  mask <- as.array(torch_argmax(preds[i,..], 1)$to(device = \"cpu\"))\n  mask <- raster::ratify(raster::raster(mask))\n  \n  img <- as.array(valid_ds_4plot[i][[1]]$permute(c(2,3,1)))\n  cond <- img > 0.99999\n  img[cond] <- 0.99999\n  img <- raster::brick(img)\n  \n  # plot image\n  raster::plotRGB(img, scale = 1, asp = 1, margins = TRUE)\n  # overlay mask\n  plot(mask, alpha = 0.4, legend = FALSE, axes = FALSE, add = TRUE)\n  \n}\n\n\nLearned segmentation masks, overlaid on images from the validation set.Now onto running this model “in the wild” (well, sort of).\nJIT-trace and run on Android\nTracing the trained model will convert it to a form that can be loaded in R-less environments – for example, from Python, C++, or Java.\nPlease see our introduction to the torch JIT compiler.\nWe access the torch model underlying the fitted luz object, and trace it – where tracing means calling it once, on a sample observation:\n\n\nm <- fitted$model\nx <- coro::collect(train_dl, 1)\n\ntraced <- jit_trace(m, x[[1]]$x)\n\n\nThe traced model could now be saved for use with Python or C++, like so:\n\n\ntraced %>% jit_save(\"traced_model.pt\")\n\n\nHowever, since we already know we’d like to deploy it on Android, we instead make use of the specialized function jit_save_for_mobile() that, additionally, generates bytecode:\n\n\n# need torch > 0.6.1\njit_save_for_mobile(traced_model, \"model_bytecode.pt\")\n\n\nAnd that’s it for the R side!\nFor running on Android, I made heavy use of PyTorch Mobile’s Android example apps, especially the image segmentation one.\nThe actual proof-of-concept code for this post (which was used to generate the below picture) may be found here: https://github.com/skeydan/ImageSegmentation. (Be warned though – it’s my first Android application!).\nOf course, we still have to try to find the cat. Here is the model, run on a device emulator in Android Studio, on three images (from the Oxford Pet Dataset) selected for, firstly, a wide range in difficulty, and secondly, well … for cuteness:\nWhere’s my cat?Thanks for reading!\n\n\n\nParkhi, Omkar M., Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. 2012. “Cats and Dogs.” In IEEE Conference on Computer Vision and Pattern Recognition.\n\n\nRonneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” CoRR abs/1505.04597. http://arxiv.org/abs/1505.04597.\n\n\nCf. Brain image segmentation with torch (using, non-surprisingly, torch), and Image segmentation with U-Net (using TensorFlow/Keras).↩︎\nRonneberger, Fischer, and Brox (2015)↩︎\n",
    "preview": "posts/2021-10-29-segmentation-torch-android/images/segmentation_android.png",
    "last_modified": "2024-11-21T15:50:12+00:00",
    "input_file": {},
    "preview_width": 2070,
    "preview_height": 1752
  },
  {
    "path": "posts/2021-08-26-geometric-deep-learning/",
    "title": "Beyond alchemy: A first look at geometric deep learning",
    "description": "Geometric deep learning is a \"program\" that aspires to situate deep learning architectures and techniques in a framework of mathematical priors. The priors, such as various types of invariance, first arise in some physical domain. A neural network that well matches the domain will preserve as many invariances as possible. In this post, we present a very conceptual, high-level overview, and highlight a few applications.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2021-08-26",
    "categories": [
      "Concepts",
      "Meta",
      "R"
    ],
    "contents": "\n\nContents\nGeometric deep learning: An attempt at unification\nGeometric priors\nFrom domain priors to algorithmic ones\n\nDomains, priors, architectures\nImages and CNNs\nGraphs and GNNs\nSequences and RNNs\nWhat’s next?\n\n\nTo the practitioner, it may often seem that with deep learning, there is a lot of magic involved. Magic in how hyper-parameter choices affect performance, for example. More fundamentally yet, magic in the impacts of architectural decisions. Magic, sometimes, in that it even works (or not). Sure, papers abound that strive to mathematically prove why, for specific solutions, in specific contexts, this or that technique will yield better results. But theory and practice are strangely dissociated: If a technique does turn out to be helpful in practice, doubts may still arise to whether that is, in fact, due to the purported mechanism. Moreover, level of generality often is low.\nIn this situation, one may feel grateful for approaches that aim to elucidate, complement, or replace some of the magic. By “complement or replace,” I’m alluding to attempts to incorporate domain-specific knowledge into the training process. Interesting examples exist in several sciences, and I certainly hope to be able to showcase a few of these, on this blog at a later time. As for the “elucidate,” this characterization is meant to lead on to the topic of this post: the program of geometric deep learning.\nGeometric deep learning: An attempt at unification\nGeometric deep learning (henceforth: GDL) is what a group of researchers, including Michael Bronstein, Joan Bruna, Taco Cohen, and Petar Velicković, call their attempt to build a framework that places deep learning (DL) on a solid mathematical basis.\nPrima facie, this is a scientific endeavor: They take existing architectures and practices and show where these fit into the “DL blueprint.” DL research being all but confined to the ivory tower, though, it’s fair to assume that this is not all: From those mathematical foundations, it should be possible to derive new architectures, new techniques to fit a given task. Who, then, should be interested in this? Researchers, for sure; to them, the framework may well prove highly inspirational. Secondly, everyone interested in the mathematical constructions themselves — this probably goes without saying. Finally, the rest of us, as well: Even understood at a purely conceptual level, the framework offers an exciting, inspiring view on DL architectures that – I think – is worth getting to know about as an end in itself. The goal of this post is to provide a high-level introduction .\nBefore we get started though, let me mention the primary source for this text: Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges (Bronstein et al. (2021)).\nGeometric priors\nA prior, in the context of machine learning, is a constraint imposed on the learning task. A generic prior could come about in different ways; a geometric prior, as defined by the GDL group, arises, originally, from the underlying domain of the task. Take image classification, for example. The domain is a two-dimensional grid. Or graphs: The domain consists of collections of nodes and edges.\nIn the GDL framework, two all-important geometric priors are symmetry and scale separation.\nSymmetry\nA symmetry, in physics and mathematics, is a transformation that leaves some property of an object unchanged. The appropriate meaning of “unchanged” depends on what sort of property we’re talking about. Say the property is some “essence,” or identity — what object something is. If I move a few steps to the left, I’m still myself: The essence of being “myself” is shift-invariant. (Or: translation-invariant.) But say the property is location. If I move to the left, my location moves to the left. Location is shift-equivariant. (Translation-equivariant.)\nSo here we have two forms of symmetry: invariance and equivariance. One means that when we transform an object, the thing we’re interested in stays the same. The other means that we have to transform that thing as well.\nThe next question then is: What are possible transformations? Translation we already mentioned; on images, rotation or flipping are others. Transformations are composable; I can rotate the digit 3 by thirty degrees, then move it to the left by five units; I could also do things the other way around. (In this case, though not necessarily in general, the results are the same.) Transformations can be undone: If first I rotate, in some direction, by five degrees, I can then rotate in the opposite one, also by five degrees, and end up in the original position. We’ll see why this matters when we cross the bridge from the domain (grids, sets, etc.) to the learning algorithm.\nScale separation\nAfter symmetry, another important geometric prior is scale separation. Scale separation means that even if something is very “big” (extends a long way in, say, one or two dimensions), we can still start from small patches and “work our way up.” For example, take a cuckoo clock. To discern the hands, you don’t need to pay attention to the pendulum. And vice versa. And once you’ve taken inventory of hands and pendulum, you don’t have to care about their texture or exact position anymore.\nIn a nutshell, given scale separation, the top-level structure can be determined through successive steps of coarse-graining. We’ll see this prior nicely reflected in some neural-network algorithms.\nFrom domain priors to algorithmic ones\nSo far, all we’ve really talked about is the domain, using the word in the colloquial sense of “on what structure,” or “in terms of what structure,” something is given. In mathematical language, though, domain is used in a more narrow way, namely, for the “input space” of a function. And a function, or rather, two of them, is what we need to get from priors on the (physical) domain to priors on neural networks.\nThe first function maps from the physical domain to signal space. If, for images, the domain was the two-dimensional grid, the signal space now consists of images the way they are represented in a computer, and will be worked with by a learning algorithm. For example, in the case of RGB images, that representation is three-dimensional, with a color dimension on top of the inherited spatial structure. What matters is that by this function, the priors are preserved. If something is translation-invariant before “real-to-virtual” conversion, it will still be translation-invariant thereafter.\nNext, we have another function: the algorithm, or neural network, acting on signal space. Ideally, this function, again, would preserve the priors. Below, we’ll see how basic neural-network architectures typically preserve some important symmetries, but not necessarily all of them. We’ll also see how, at this point, the actual task makes a difference. Depending on what we’re trying to achieve, we may want to maintain some symmetry, but not care about another. The task here is analogous to the property in physical space. Just like in physical space, a movement to the left does not alter identity, a classifier, presented with that same shift, won’t care at all. But a segmentation algorithm will – mirroring the real-world shift in position.\nNow that we’ve made our way to algorithm space, the above requirement, formulated on physical space – that transformations be composable – makes sense in another light: Composing functions is exactly what neural networks do; we want these compositions to work just as deterministically as those of real-world transformations.\nIn sum, the geometric priors and the way they impose constraints, or desiderates, rather, on the learning algorithm lead to what the GDL group call their deep learning “blueprint.” Namely, a network should be composed of the following types of modules:\nLinear group-equivariant layers. (Here group is the group of transformations whose symmetries we’re interested to preserve.)\nNonlinearities. (This really does not follow from geometric arguments, but from the observation, often stated in introductions to DL, that without nonlinearities, there is no hierarchical composition of features, since all operations can be implemented in a single matrix multiplication.)\nLocal pooling layers. (These achieve the effect of coarse-graining, as enabled by the scale separation prior.)\nA group-invariant layer (global pooling). (Not every task will require such a layer to be present.)\nHaving talked so much about the concepts, which are highly fascinating, this list may seem a bit underwhelming. That’s what we’ve been doing anyway, right? Maybe; but once you look at a few domains and associated network architectures, the picture gets colorful again. So colorful, in fact, that we can only present a very sparse selection of highlights.\nDomains, priors, architectures\nGiven cues like “local” and “pooling,” what better architecture is there to start with than CNNs, the (still) paradigmatic deep learning architecture? Probably, it’s also the one a prototypic practitioner would be most familiar with.\nImages and CNNs\nVanilla CNNs are easily mapped to the four types of layers that make up the blueprint. Skipping over the nonlinearities, which, in this context, are of least interest, we next have two kinds of pooling.\nFirst, a local one, corresponding to max- or average-pooling layers with small strides (2 or 3, say). This reflects the idea of successive coarse-graining, where, once we’ve made use of some fine-grained information, all we need to proceed is a summary.\nSecond, a global one, used to effectively remove the spatial dimensions. In practice, this would usually be global average pooling. Here, there’s an interesting detail worth mentioning. A common practice, in image classification, is to replace global pooling by a combination of flattening and one or more feedforward layers. Since with feedforward layers, position in the input matters, this will do away with translation invariance.\nHaving covered three of the four layer types, we come to the most interesting one. In CNNs, the local, group-equivariant layers are the convolutional ones. What kinds of symmetries does convolution preserve? Think about how a kernel slides over an image, computing a dot product at every location. Say that, through training, it has developed an inclination toward singling out penguin bills. It will detect, and mark, one everywhere in an image — be it shifted left, right, top or bottom in the image. What about rotational motion, though? Since kernels move vertically and horizontally, but not in a circle, a rotated bill will be missed. Convolution is shift-equivariant, not rotation-invariant.\nThere is something that can be done about this, though, while fully staying within the framework of GDL. Convolution, in a more generic sense, does not have to imply constraining filter movement to horizontal and vertical translation. When reflecting a general group convolution, that motion is determined by whatever transformations constitute the group action. If, for example, that action included translation by sixty degrees, we could rotate the filter to all valid positions, then take these filters and have them slide over the image. In effect, we’d just wind up with more channels in the subsequent layer – the intended base number of filters times the number of attainable positions.\nThis, it must be said, it just one way to do it. A more elegant one is to apply the filter in the Fourier domain, where convolution maps to multiplication. The Fourier domain, however, is as fascinating as it is out of scope for this post.\nThe same goes for extensions of convolution from the Euclidean grid to manifolds, where distances are no longer measured by a straight line as we know it. Often on manifolds, we’re interested in invariances beyond translation or rotation: Namely, algorithms may have to support various types of deformation. (Imagine, for example, a moving rabbit, with its muscles stretching and contracting as it hobbles.) If you’re interested in these kinds of problems, the GDL book goes into those in great detail.\nFor group convolution on grids – in fact, we may want to say “on things that can be arranged in a grid” – the authors give two illustrative examples. (One thing I like about these examples is something that extends to the whole book: Many applications are from the world of natural sciences, encouraging some optimism as to the role of deep learning (“AI”) in society.)\nOne example is from medical volumetric imaging (MRI or CT, say), where signals are represented on a three-dimensional grid. Here the task calls not just for translation in all directions, but also, rotations, of some sensible degree, about all three spatial axes. The other is from DNA sequencing, and it brings into play a new kind of invariance we haven’t mentioned yet: reverse-complement symmetry. This is because once we’ve decoded one strand of the double helix, we already know the other one.\nFinally, before we wrap up the topic of CNNs, let’s mention how through creativity, one can achieve – or put cautiously, try to achieve – certain invariances by means other than network architecture. A great example, originally associated mostly with images, is data augmentation. Through data augmentation, we may hope to make training invariant to things like slight changes in color, illumination, perspective, and the like.\nGraphs and GNNs\nAnother type of domain, underlying many scientific and non-scientific applications, are graphs. Here, we are going to be a lot more brief. One reason is that so far, we have not had many posts on deep learning on graphs, so to the readers of this blog, the topic may seem fairly abstract. The other reason is complementary: That state of affairs is exactly something we’d like to see changing. Once we write more about graph DL, occasions to talk about respective concepts will be plenty.\nIn a nutshell, though, the dominant type of invariance in graph DL is permutation equivariance. Permutation, because when you stack a node and its features in a matrix, it doesn’t matter whether node one is in row three or row fifteen. Equivariance, because once you do permute the nodes, you also have to permute the adjacency matrix, the matrix that captures which node is linked to what other nodes. This is very different from what holds for images: We can’t just randomly permute the pixels.\nSequences and RNNs\nWith RNNs, we are going be very brief as well, although for a different reason. My impression is that so far, this area of research – meaning, GDL as it relates to sequences – has not received too much attention yet, and (maybe) for that reason, seems of lesser impact on real-world applications.\nIn a nutshell, the authors refer two types of symmetry: First, translation-invariance, as long as a sequence is left-padded for a sufficient number of steps. (This is due to the hidden units having to be initialized somehow.) This holds for RNNs in general.\nSecond, time warping: If a network can be trained that correctly works on a sequence measured on some time scale, there is another network, of the same architecture but likely with different weights, that will work equivalently on re-scaled time. This invariance only applies to gated RNNs, such as the LSTM.\nWhat’s next?\nAt this point, we conclude this conceptual introduction. If you want to learn more, and are not too scared by the math, definitely check out the book. (I’d also say it lends itself well to incremental understanding, as in, iteratively going back to some details once one has acquired more background.)\nSomething else to wish for certainly is practice. There is an intimate connection between GDL and deep learning on graphs; which is one reason we’re hoping to be able to feature the latter more frequently in the future. The other is the wealth of interesting applications that take graphs as their input. Until then, thanks for reading!\nPhoto by NASA on Unsplash\n\n\n\nBronstein, Michael M., Joan Bruna, Taco Cohen, and Petar Velickovic. 2021. “Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges.” CoRR abs/2104.13478. https://arxiv.org/abs/2104.13478.\n\n\n\n\n",
    "preview": "posts/2021-08-26-geometric-deep-learning/images/preview.jpg",
    "last_modified": "2024-11-21T15:53:06+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-10-jit-trace-module/",
    "title": "torch: Just-in-time compilation (JIT) for R-less model deployment",
    "description": "Using the torch just-in-time (JIT) compiler, it is possible to query a model trained in R from a different language, provided that language can make use of the low-level libtorch library. This post shows how. In addition, we try to untangle a bit of the terminological jumble surrounding the topic.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2021-08-10",
    "categories": [
      "Torch",
      "R"
    ],
    "contents": "\n\nContents\nTerminological introduction\nWhat’s coming (in this text)\nHow to make use of torch JIT compilation\nA first glance at optimizations\ntorch without R\nConclusion\n\nNote: To follow along with this post, you will need torch version 0.5, which as of this writing is not yet on CRAN. In the meantime, please install the development version from GitHub.\nEvery domain has its concepts, and these are what one needs to understand, at some point, on one’s journey from copy-and-make-it-work to purposeful, deliberate utilization. In addition, unfortunately, every domain has its jargon, whereby terms are used in a way that is technically correct, but fails to evoke a clear image to the yet-uninitiated. (Py-)Torch’s JIT is an example.\nTerminological introduction\n“The JIT”, much talked about in PyTorch-world and an eminent feature of R torch, as well, is two things at the same time – depending on how you look at it: an optimizing compiler; and a free pass to execution in many environments where neither R nor Python are present.\nCompiled, interpreted, just-in-time compiled\n“JIT” is a common acronym for “just in time” [to wit: compilation]. Compilation means generating machine-executable code; it is something that has to happen to every program for it to be runnable. The question is when.\nC code, for example, is compiled “by hand”, at some arbitrary time prior to execution. Many other languages, however (among them Java, R, and Python) are – in their default implementations, at least – interpreted: They come with executables (java, R, and python, resp.) that create machine code at run time, based on either the original program as written or an intermediate format called bytecode. Interpretation can proceed line-by-line, such as when you enter some code in R’s REPL (read-eval-print loop), or in chunks (if there’s a whole script or application to be executed). In the latter case, since the interpreter knows what is likely to be run next, it can implement optimizations that would be impossible otherwise. This process is commonly known as just-in-time compilation. Thus, in general parlance, JIT compilation is compilation, but at a point in time where the program is already running.\nThe torch just-in-time compiler\nCompared to that notion of JIT, at once generic (in technical regard) and specific (in time), what (Py-)Torch people have in mind when they talk of “the JIT” is both more narrowly-defined (in terms of operations) and more inclusive (in time): What is understood is the complete process from providing code input that can be converted into an intermediate representation (IR), via generation of that IR, via successive optimization of the same by the JIT compiler, via conversion (again, by the compiler) to bytecode, to – finally – execution, again taken care of by that same compiler, that now is acting as a virtual machine.\nIf that sounded complicated, don’t be scared. To actually make use of this feature from R, not much needs to be learned in terms of syntax; a single function, augmented by a few specialized helpers, is stemming all the heavy load. What matters, though, is understanding a bit about how JIT compilation works, so you know what to expect, and are not surprised by unintended outcomes.\nWhat’s coming (in this text)\nThis post has three further parts.\nIn the first, we explain how to make use of JIT capabilities in R torch. Beyond the syntax, we focus on the semantics (what essentially happens when you “JIT trace” a piece of code), and how that affects the outcome.\nIn the second, we “peek under the hood” a little bit; feel free to just cursorily skim if this does not interest you too much.\nIn the third, we show an example of using JIT compilation to enable deployment in an environment that does not have R installed.\nHow to make use of torch JIT compilation\nIn Python-world, or more specifically, in Python incarnations of deep learning frameworks, there is a magic verb “trace” that refers to a way of obtaining a graph representation from executing code eagerly. Namely, you run a piece of code – a function, say, containing PyTorch operations – on example inputs. These example inputs are arbitrary value-wise, but (naturally) need to conform to the shapes expected by the function. Tracing will then record operations as executed, meaning: those operations that were in fact executed, and only those. Any code paths not entered are consigned to oblivion.\nIn R, too, tracing is how we obtain a first intermediate representation.1 This is done using the aptly named function jit_trace(). For example:\n\n\nlibrary(torch)\n\nf <- function(x) {\n  torch_sum(x)\n}\n\n# call with example input tensor\nf_t <- jit_trace(f, torch_tensor(c(2, 2)))\n\nf_t\n\n\n<script_function>\nWe can now call the traced function just like the original one:\n\n\nf_t(torch_randn(c(3, 3)))\n\n\ntorch_tensor\n3.19587\n[ CPUFloatType{} ]\nWhat happens if there is control flow, such as an if statement?\n\n\nf <- function(x) {\n  if (as.numeric(torch_sum(x)) > 0) torch_tensor(1) else torch_tensor(2)\n}\n\nf_t <- jit_trace(f, torch_tensor(c(2, 2)))\n\n\nHere tracing must have entered the if branch. Now call the traced function with a tensor that does not sum to a value greater than zero:\n\n\nf_t(torch_tensor(-1))\n\n\ntorch_tensor\n 1\n[ CPUFloatType{1} ]\nThis is how tracing works. The paths not taken are lost forever. The lesson here is to not ever have control flow inside a function that is to be traced.2\nBefore we move on, let’s quickly mention two of the most-used, besides jit_trace(), functions in the torch JIT ecosystem: jit_save() and jit_load(). Here they are:\n\n\njit_save(f_t, \"/tmp/f_t\")\n\nf_t_new <- jit_load(\"/tmp/f_t\")\n\n\nA first glance at optimizations\nOptimizations performed by the torch JIT compiler happen in stages. On the first pass, we see things like dead code elimination and pre-computation of constants. Take this function:\n\n\nf <- function(x) {\n  \n  a <- 7\n  b <- 11\n  c <- 2\n  d <- a + b + c\n  e <- a + b + c + 25\n  \n  \n  x + d \n  \n}\n\n\nHere computation of e is useless – it is never used. Consequently, in the intermediate representation, e does not even appear. Also, as the values of a, b, and c are known already at compile time, the only constant present in the IR is d, their sum.\nNicely, we can verify that for ourselves. To peek at the IR – the initial IR, to be precise – we first trace f, and then access the traced function’s graph property:\n\n\nf_t <- jit_trace(f, torch_tensor(0))\n\nf_t$graph\n\n\ngraph(%0 : Float(1, strides=[1], requires_grad=0, device=cpu)):\n  %1 : float = prim::Constant[value=20.]()\n  %2 : int = prim::Constant[value=1]()\n  %3 : Float(1, strides=[1], requires_grad=0, device=cpu) = aten::add(%0, %1, %2)\n  return (%3)\nAnd really, the only computation recorded is the one that adds 20 to the passed-in tensor.\nSo far, we’ve been talking about the JIT compiler’s initial pass. But the process does not stop there. On subsequent passes, optimization expands into the realm of tensor operations.\nTake the following function:\n\n\nf <- function(x) {\n  \n  m1 <- torch_eye(5, device = \"cuda\")\n  x <- x$mul(m1)\n\n  m2 <- torch_arange(start = 1, end = 25, device = \"cuda\")$view(c(5,5))\n  x <- x$add(m2)\n  \n  x <- torch_relu(x)\n  \n  x$matmul(m2)\n  \n}\n\n\nHarmless though this function may look, it incurs quite a bit of scheduling overhead. A separate GPU kernel (a C function, to be parallelized over many CUDA threads) is required for each of torch_mul() , torch_add(), torch_relu() , and torch_matmul().\nUnder certain conditions, several operations can be chained (or fused, to use the technical term) into a single one. Here, three of those four methods (namely, all but torch_matmul()) operate point-wise; that is, they modify each element of a tensor in isolation. In consequence, not only do they lend themselves optimally to parallelization individually, – the same would be true of a function that were to compose (“fuse”) them: To compute a composite function “multiply then add then ReLU”\n\\[\nrelu() \\ \\circ \\ (+) \\ \\circ \\ (*)\n\\]\non a tensor element, nothing needs to be known about other elements in the tensor. The aggregate operation could then be run on the GPU in a single kernel.\nTo make this happen, you normally would have to write custom CUDA code. Thanks to the JIT compiler, in many cases you don’t have to: It will create such a kernel on the fly.\nTo see fusion in action, we use graph_for() (a method) instead of graph (a property):\n\n\nv <- jit_trace(f, torch_eye(5, device = \"cuda\"))\n\nv$graph_for(torch_eye(5, device = \"cuda\"))\n\n\ngraph(%x.1 : Tensor):\n  %1 : Float(5, 5, strides=[5, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()\n  %24 : Float(5, 5, strides=[5, 1], requires_grad=0, device=cuda:0), %25 : bool = prim::TypeCheck[types=[Float(5, 5, strides=[5, 1], requires_grad=0, device=cuda:0)]](%x.1)\n  %26 : Tensor = prim::If(%25)\n    block0():\n      %x.14 : Float(5, 5, strides=[5, 1], requires_grad=0, device=cuda:0) = prim::TensorExprGroup_0(%24)\n      -> (%x.14)\n    block1():\n      %34 : Function = prim::Constant[name=\"fallback_function\", fallback=1]()\n      %35 : (Tensor) = prim::CallFunction(%34, %x.1)\n      %36 : Tensor = prim::TupleUnpack(%35)\n      -> (%36)\n  %14 : Tensor = aten::matmul(%26, %1) # <stdin>:7:0\n  return (%14)\nwith prim::TensorExprGroup_0 = graph(%x.1 : Float(5, 5, strides=[5, 1], requires_grad=0, device=cuda:0)):\n  %4 : int = prim::Constant[value=1]()\n  %3 : Float(5, 5, strides=[5, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()\n  %7 : Float(5, 5, strides=[5, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()\n  %x.10 : Float(5, 5, strides=[5, 1], requires_grad=0, device=cuda:0) = aten::mul(%x.1, %7) # <stdin>:4:0\n  %x.6 : Float(5, 5, strides=[5, 1], requires_grad=0, device=cuda:0) = aten::add(%x.10, %3, %4) # <stdin>:5:0\n  %x.2 : Float(5, 5, strides=[5, 1], requires_grad=0, device=cuda:0) = aten::relu(%x.6) # <stdin>:6:0\n  return (%x.2)\nFrom this output, we learn that three of the four operations have been grouped together to form a TensorExprGroup . This TensorExprGroup will be compiled into a single CUDA kernel. The matrix multiplication, however – not being a pointwise operation – has to be executed by itself.\nAt this point, we stop our exploration of JIT optimizations, and move on to the last topic: model deployment in R-less environments. If you’d like to know more, Thomas Viehmann’s blog has posts that go into incredible detail on (Py-)Torch JIT compilation.\ntorch without R\nOur plan is the following: We define and train a model, in R. Then, we trace and save it. The saved file is then jit_load()ed in another environment, an environment that does not have R installed. Any language that has an implementation of Torch will do, provided that implementation includes the JIT functionality. The most straightforward way to show how this works is using Python. For deployment with C++, please see the detailed instructions on the PyTorch website.\nDefine model\nOur example model is a straightforward multi-layer perceptron. Note, though, that it has two dropout layers. Dropout layers behave differently during training and evaluation; and as we’ve learned, decisions made during tracing are set in stone. This is something we’ll need to take care of once we’re done training the model.\n\n\nlibrary(torch)\nnet <- nn_module( \n  \n  initialize = function() {\n    \n    self$l1 <- nn_linear(3, 8)\n    self$l2 <- nn_linear(8, 16)\n    self$l3 <- nn_linear(16, 1)\n    self$d1 <- nn_dropout(0.2)\n    self$d2 <- nn_dropout(0.2)\n    \n  },\n  \n  forward = function(x) {\n    x %>%\n      self$l1() %>%\n      nnf_relu() %>%\n      self$d1() %>%\n      self$l2() %>%\n      nnf_relu() %>%\n      self$d2() %>%\n      self$l3()\n  }\n)\n\ntrain_model <- net()\n\n\nTrain model on toy dataset\nFor demonstration purposes, we create a toy dataset with three predictors and a scalar target.\n\n\ntoy_dataset <- dataset(\n  \n  name = \"toy_dataset\",\n  \n  initialize = function(input_dim, n) {\n    \n    df <- na.omit(df) \n    self$x <- torch_randn(n, input_dim)\n    self$y <- self$x[, 1, drop = FALSE] * 0.2 -\n      self$x[, 2, drop = FALSE] * 1.3 -\n      self$x[, 3, drop = FALSE] * 0.5 +\n      torch_randn(n, 1)\n    \n  },\n  \n  .getitem = function(i) {\n    list(x = self$x[i, ], y = self$y[i])\n  },\n  \n  .length = function() {\n    self$x$size(1)\n  }\n)\n\ninput_dim <- 3\nn <- 1000\n\ntrain_ds <- toy_dataset(input_dim, n)\n\ntrain_dl <- dataloader(train_ds, shuffle = TRUE)\n\n\nWe train long enough to make sure we can distinguish an untrained model’s output from that of a trained one.\n\n\noptimizer <- optim_adam(train_model$parameters, lr = 0.001)\nnum_epochs <- 10\n\ntrain_batch <- function(b) {\n  \n  optimizer$zero_grad()\n  output <- train_model(b$x)\n  target <- b$y\n  \n  loss <- nnf_mse_loss(output, target)\n  loss$backward()\n  optimizer$step()\n  \n  loss$item()\n}\n\nfor (epoch in 1:num_epochs) {\n  \n  train_loss <- c()\n  \n  coro::loop(for (b in train_dl) {\n    loss <- train_batch(b)\n    train_loss <- c(train_loss, loss)\n  })\n  \n  cat(sprintf(\"\\nEpoch: %d, loss: %3.4f\\n\", epoch, mean(train_loss)))\n  \n}\n\n\nEpoch: 1, loss: 2.6753\n\nEpoch: 2, loss: 1.5629\n\nEpoch: 3, loss: 1.4295\n\nEpoch: 4, loss: 1.4170\n\nEpoch: 5, loss: 1.4007\n\nEpoch: 6, loss: 1.2775\n\nEpoch: 7, loss: 1.2971\n\nEpoch: 8, loss: 1.2499\n\nEpoch: 9, loss: 1.2824\n\nEpoch: 10, loss: 1.2596\nTrace in eval mode\nNow, for deployment, we want a model that does not drop out any tensor elements. This means that before tracing, we need to put the model into eval() mode.\n\n\ntrain_model$eval()\n\ntrain_model <- jit_trace(train_model, torch_tensor(c(1.2, 3, 0.1))) \n\njit_save(train_model, \"/tmp/model.zip\")\n\n\nThe saved model could now be copied to a different system.\nQuery model from Python\nTo make use of this model from Python, we jit.load() it, then call it like we would in R. Let’s see: For an input tensor of (1, 1, 1), we expect a prediction somewhere around -1.6:\n\nimport torch\n\ndeploy_model = torch.jit.load(\"/tmp/model.zip\")\ndeploy_model(torch.tensor((1, 1, 1), dtype = torch.float)) \n\ntensor([-1.3630], device='cuda:0', grad_fn=<AddBackward0>)\nThis is close enough to reassure us that the deployed model has kept the trained model’s weights.\nConclusion\nIn this post, we’ve focused on resolving a bit of the terminological jumble surrounding the torch JIT compiler, and showed how to train a model in R, trace it, and query the freshly loaded model from Python. Deliberately, we haven’t gone into complex and/or corner cases, – in R, this feature is still under active development. Should you run into problems with your own JIT-using code, please don’t hesitate to create a GitHub issue!\nAnd as always – thanks for reading!\nPhoto by Jonny Kennaugh on Unsplash\n\nIn PyTorch, there is an additional way, scripting. As of this writing, scripting is not implemented in R (not natively, at least ), which is why we don’t discuss it much in this post.↩︎\nWorkarounds involving the use of TorchScript (Python) code are possible; please refer to the TorchScript vignette.↩︎\n",
    "preview": "posts/2021-08-10-jit-trace-module/images/preview.jpg",
    "last_modified": "2024-11-21T15:53:49+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-15-AI-fairness/",
    "title": "Starting to think about AI Fairness",
    "description": "The topic of AI fairness metrics is as important to society as it is confusing. Confusing it is due to a number of reasons: terminological proliferation, abundance of formulae, and last not least the impression that everyone else seems to know what they're talking about. This text hopes to counteract some of that confusion by starting from a common-sense approach of contrasting two basic positions: On the one hand, the assumption that dataset features may be taken as reflecting the underlying concepts ML practitioners are interested in; on the other, that there inevitably is a gap between concept and measurement, a gap that may be bigger or smaller depending on what is being measured. In contrasting these fundamental views, we bring together concepts from ML, legal science, and political philosophy.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2021-07-15",
    "categories": [
      "R",
      "Concepts",
      "Meta",
      "AI & Society"
    ],
    "contents": "\n\nContents\nThe forest for the trees\nCommon sense, not math\nSituating fairness criteria\nFairness, viewed the technical way\nFairness, viewed as a social construct\n\nA quick glance at neighboring fields: law and political philosophy\n(A) Conclusion\n\nIf you use deep learning for unsupervised part-of-speech tagging of\nSanskrit,1 or knowledge discovery in physics,2 you probably\ndon’t need to worry about model fairness. If you’re a data scientist\nworking at a place where decisions are made about people, however, or\nan academic researching models that will be used to such ends, chances\nare that you’ve already been thinking about this topic. — Or feeling that\nyou should. And thinking about this is hard.\nIt is hard for several reasons. In this text, I will go into just one.\nThe forest for the trees\nNowadays, it is hard to find a modeling framework that does not\ninclude functionality to assess fairness. (Or is at least planning to.)\nAnd the terminology sounds so familiar, as well: “calibration,”\n“predictive parity,” “equal true [false] positive rate”… It almost\nseems as though we could just take the metrics we make use of anyway\n(recall or precision, say), test for equality across groups, and that’s\nit. Let’s assume, for a second, it really was that simple. Then the\nquestion still is: Which metrics, exactly, do we choose?\nIn reality things are not simple. And it gets worse. For very good\nreasons, there is a close connection in the ML fairness literature to\nconcepts that are primarily treated in other disciplines, such as the\nlegal sciences: discrimination and disparate impact (both not being\nfar from yet another statistical concept, statistical parity).\nStatistical parity means that if we have a classifier, say to decide\nwhom to hire, it should result in as many applicants from the\ndisadvantaged group (e.g., Black people) being hired as from the\nadvantaged one(s). But that is quite a different requirement from, say,\nequal true/false positive rates!\nSo despite all that abundance of software, guides, and decision trees,\neven: This is not a simple, technical decision. It is, in fact, a\ntechnical decision only to a small degree.\nCommon sense, not math\nLet me start this section with a disclaimer: Most of the sources\nreferenced in this text appear, or are implied on the “Guidance”\npage of IBM’s framework\nAI Fairness 360. If you read that page, and everything that’s said and\nnot said there appears clear from the outset, then you may not need this\nmore verbose exposition. If not, I invite you to read on.\nPapers on fairness in machine learning, as is common in fields like\ncomputer science, abound with formulae. Even the papers referenced here,\nthough selected not for their theorems and proofs but for the ideas they\nharbor, are no exception. But to start thinking about fairness as it\nmight apply to an ML process at hand, common language – and common\nsense – will do just fine. If, after analyzing your use case, you judge\nthat the more technical results are relevant to the process in\nquestion, you will find that their verbal characterizations will often\nsuffice. It is only when you doubt their correctness that you will need\nto work through the proofs.\nAt this point, you may be wondering what it is I am contrasting those\n“more technical results” with. This is the topic of the next section,\nwhere I’ll try to give a birds-eye characterization of fairness criteria\nand what they imply.\nSituating fairness criteria\nThink back to the example of a hiring algorithm. What does it mean for\nthis algorithm to be fair? We approach this question under two –\nincompatible, mostly – assumptions:\nThe algorithm is fair if it behaves the same way independent of\nwhich demographic group it is applied to. Here demographic group\ncould be defined by ethnicity, gender, abledness, or in fact any\ncategorization suggested by the context.\nThe algorithm is fair if it does not discriminate against any\ndemographic group.\nI’ll call these the technical and societal views, respectively.\nFairness, viewed the technical way\nWhat does it mean for an algorithm to “behave the same way” regardless\nof which group it is applied to?\nIn a classification setting, we can view the relationship between\nprediction (\\(\\hat{Y}\\)) and target (\\(Y\\)) as a doubly directed path. In\none direction: Given true target \\(Y\\), how accurate is prediction\n\\(\\hat{Y}\\)? In the other: Given \\(\\hat{Y}\\), how well does it predict the\ntrue class \\(Y\\)?\nBased on the direction they operate in, metrics popular in machine\nlearning overall can be split into two categories. In the first,\nstarting from the true target, we have recall, together with “the\nrates”: true positive, true negative, false positive, false negative.\nIn the second, we have precision, together with positive (negative,\nresp.) predictive value.\nIf now we demand that these metrics be the same across groups, we arrive\nat corresponding fairness criteria: equal false positive rate, equal\npositive predictive value, etc. In the inter-group setting, the two\ntypes of metrics may be arranged under headings “equality of\nopportunity” and “predictive parity.” You’ll encounter these as actual\nheaders in the summary table at the end of this text.\nSaid table organizes concepts from different areas into a three-category\nformat. The overall narrative builds up towards that “map” in a\nbottom-up way – meaning, most entries will not make sense at this\npoint.\nWhile overall, the terminology around metrics can be confusing (to me it\nis), these headings have some mnemonic value. Equality of opportunity\nsuggests that people similar in real life (\\(Y\\)) get classified similarly\n(\\(\\hat{Y}\\)). Predictive parity suggests that people classified\nsimilarly (\\(\\hat{Y}\\)) are, in fact, similar (\\(Y\\)).\nThe two criteria can concisely be characterized using the language of\nstatistical independence. Following Barocas, Hardt, and Narayanan (2019), these are:\nSeparation: Given true target \\(Y\\), prediction \\(\\hat{Y}\\) is\nindependent of group membership (\\(\\hat{Y} \\perp A | Y\\)).\nSufficiency: Given prediction \\(\\hat{Y}\\), target \\(Y\\) is independent\nof group membership (\\(Y \\perp A | \\hat{Y}\\)).\nGiven those two fairness criteria – and two sets of corresponding\nmetrics – the natural question arises: Can we satisfy both? Above, I\nwas mentioning precision and recall on purpose: to maybe “prime” you to\nthink in the direction of “precision-recall trade-off.” And really,\nthese two categories reflect different preferences; usually, it is\nimpossible to optimize for both. The most famous, probably, result is\ndue to Chouldechova (2016) : It says that predictive parity (testing\nfor sufficiency) is incompatible with error rate balance (separation)\nwhen prevalence differs across groups. This is a theorem (yes, we’re in\nthe realm of theorems and proofs here) that may not be surprising, in\nlight of Bayes’ theorem, but is of great practical importance\nnonetheless: Unequal prevalence usually is the norm, not the exception.\nThis necessarily means we have to make a choice. And this is where the\ntheorems and proofs do matter. For example, Yeom and Tschantz (2018) show that\nin this framework – the strictly technical approach to fairness –\nseparation should be preferred over sufficiency, because the latter\nallows for arbitrary disparity amplification. Thus, in this framework,\nwe may have to work through the theorems.\nWhat is the alternative?\nFairness, viewed as a social construct\nStarting with what I just wrote: No one will likely challenge fairness\nbeing a social construct. But what does that entail?\nLet me start with a biographical reminiscence. In undergraduate\npsychology (a long time ago), probably the most hammered-in distinction\nrelevant to experiment planning was that between a hypothesis and its\noperationalization. The hypothesis is what you want to substantiate,\nconceptually; the operationalization is what you measure. There\nnecessarily can’t be a one-to-one correspondence; we’re just striving to\nimplement the best operationalization possible.\nIn the world of datasets and algorithms, all we have are measurements.\nAnd often, these are treated as though they were the concepts. This\nwill get more concrete with an example, and we’ll stay with the hiring\nsoftware scenario.\nAssume the dataset used for training, assembled from scoring previous\nemployees, contains a set of predictors (among which, high-school\ngrades) and a target variable, say an indicator whether an employee did\n“survive” probation. There is a concept-measurement mismatch on both\nsides.\nFor one, say the grades are intended to reflect ability to learn, and\nmotivation to learn. But depending on the circumstances, there\nare influence factors of much higher impact: socioeconomic status,\nconstantly having to struggle with prejudice, overt discrimination, and\nmore.\nAnd then, the target variable. If the thing it’s supposed to measure\nis “was hired for seemed like a good fit, and was retained since was a\ngood fit,” then all is good. But normally, HR departments are aiming for\nmore than just a strategy of “keep doing what we’ve always been doing.”\nUnfortunately, that concept-measurement mismatch is even more fatal,\nand even less talked about, when it’s about the target and not the\npredictors. (Not accidentally, we also call the target the “ground\ntruth.”) An infamous example is recidivism prediction, where what we\nreally want to measure – whether someone did, in fact, commit a crime\n– is replaced, for measurability reasons, by whether they were\nconvicted. These are not the same: Conviction depends on more\nthen what someone has done – for instance, if they’ve been under\nintense scrutiny from the outset.\nFortunately, though, the mismatch is clearly pronounced in the AI\nfairness literature. Friedler, Scheidegger, and Venkatasubramanian (2016) distinguish between the construct\nand observed spaces; depending on whether a near-perfect mapping is\nassumed between these, they talk about two “worldviews”: “We’re all\nequal” (WAE) vs. “What you see is what you get” (WYSIWIG). If we’re all\nequal, membership in a societally disadvantaged group should not – in\nfact, may not – affect classification. In the hiring scenario, any\nalgorithm employed thus has to result in the same proportion of\napplicants being hired, regardless of which demographic group they\nbelong to. If “What you see is what you get,” we don’t question that the\n“ground truth” is the truth.\nThis talk of worldviews may seem unnecessary philosophical, but the\nauthors go on and clarify: All that matters, in the end, is whether the\ndata is seen as reflecting reality in a naïve, take-at-face-value way.\nFor example, we might be ready to concede that there could be small,\nalbeit uninteresting effect-size-wise, statistical differences between\nmen and women as to spatial vs. linguistic abilities, respectively. We\nknow for sure, though, that there are much greater effects of\nsocialization, starting in the core family and reinforced,\nprogressively, as adolescents go through the education system. We\ntherefore apply WAE, trying to (partly) compensate for historical\ninjustice. This way, we’re effectively applying affirmative action,\ndefined as\n\nA set of procedures designed to eliminate unlawful discrimination\namong applicants, remedy the results of such prior discrimination, and\nprevent such discrimination in the future.\n\nIn the already-mentioned summary table, you’ll find the WYSIWIG\nprinciple mapped to both equal opportunity and predictive parity\nmetrics. WAE maps to the third category, one we haven’t dwelled upon\nyet: demographic parity, also known as statistical parity. In line\nwith what was said before, the requirement here is for each group to be\npresent in the positive-outcome class in proportion to its\nrepresentation in the input sample. For example, if thirty percent of\napplicants are Black, then at least thirty percent of people selected\nshould be Black, as well. A term commonly used for cases where this does\nnot happen is disparate impact: The algorithm affects different\ngroups in different ways.\nSimilar in spirit to demographic parity, but possibly leading to\ndifferent outcomes in practice, is conditional demographic parity.3\nHere we additionally take into account other predictors in the dataset;\nto be precise: all other predictors. The desiderate now is that for\nany choice of attributes, outcome proportions should be equal, given the\nprotected attribute and the other attributes in question. I’ll come\nback to why this may sound better in theory than work in practice in the\nnext section.\nSumming up, we’ve seen commonly used fairness metrics organized into\nthree groups, two of which share a common assumption: that the data used\nfor training can be taken at face value. The other starts from the\noutside, contemplating what historical events, and what political and\nsocietal factors have made the given data look as they do.\nBefore we conclude, I’d like to try a quick glance at other disciplines,\nbeyond machine learning and computer science, domains where fairness\nfigures among the central topics. This section is necessarily limited in\nevery respect; it should be seen as a flashlight, an invitation to read\nand reflect rather than an orderly exposition. The short section will\nend with a word of caution: Since drawing analogies can feel highly\nenlightening (and is intellectually satisfying, for sure), it is easy to\nabstract away practical realities. But I’m getting ahead of myself.\nA quick glance at neighboring fields: law and political philosophy\nIn jurisprudence, fairness and discrimination constitute an important\nsubject. A recent paper that caught my attention is Wachter, Mittelstadt, and Russell (2020a) . From a\nmachine learning perspective, the interesting point is the\nclassification of metrics into bias-preserving and bias-transforming.\nThe terms speak for themselves: Metrics in the first group reflect\nbiases in the dataset used for training; ones in the second do not. In\nthat way, the distinction parallels Friedler, Scheidegger, and Venkatasubramanian (2016) ’s confrontation of\ntwo “worldviews.” But the exact words used also hint at how guidance by\nmetrics feeds back into society: Seen as strategies, one preserves\nexisting biases; the other, to consequences unknown a priori, changes\nthe world.\nTo the ML practitioner, this framing is of great help in evaluating what\ncriteria to apply in a project. Helpful, too, is the systematic mapping\nprovided of metrics to the two groups; it is here that, as alluded to\nabove, we encounter conditional demographic parity among the\nbias-transforming ones. I agree that in spirit, this metric can be seen\nas bias-transforming; if we take two sets of people who, per all\navailable criteria, are equally qualified for a job, and then find the\nwhites favored over the Blacks, fairness is clearly violated. But the\nproblem here is “available”: per all available criteria. What if we\nhave reason to assume that, in a dataset, all predictors are biased?\nThen it will be very hard to prove that discrimination has occurred.\nA similar problem, I think, surfaces when we look at the field of\npolitical philosophy, and consult theories on distributive\njustice for\nguidance. Heidari et al. (2018) have written a paper comparing the three\ncriteria – demographic parity, equality of opportunity, and predictive\nparity – to egalitarianism, equality of opportunity (EOP) in the\nRawlsian sense, and EOP seen through the glass of luck egalitarianism,\nrespectively. While the analogy is fascinating, it too assumes that we\nmay take what is in the data at face value. In their likening predictive\nparity to luck egalitarianism, they have to go to especially great\nlengths, in assuming that the predicted class reflects effort\nexerted. In the below table, I therefore take the liberty to disagree,\nand map a libertarian view of distributive justice to both equality of\nopportunity and predictive parity metrics.\nIn summary, we end up with two highly controversial categories of\nfairness criteria, one bias-preserving, “what you see is what you\nget”-assuming, and libertarian, the other bias-transforming, “we’re all\nequal”-thinking, and egalitarian. Here, then, is that often-announced\ntable.\n\nDemographic\nparity\nEquality of\nopportunity\nPredictive\nparity\nA.K.A. /\nsubsumes /\nrelated\nconcepts\nstatistical\nparity, group\nfairness,\ndisparate\nimpact,\nconditional\ndemographic\nparity4\nequalized\nodds, equal\nfalse positive\n/ negative\nrates\nequal positive\n/ negative\npredictive\nvalues,\ncalibration by\ngroup\nStatistical\nindependence\ncriterion5\nindependence\n\\(\\hat{Y}  \\perp A\\)\nseparation\n\\(\\hat{Y}  \\perp A | Y\\)\nsufficiency\n\\(Y \\perp  A | \\hat{Y}\\)\nIndividual /\ngroup\ngroup\ngroup (most)\nor individual\n(fairness\nthrough\nawareness)\ngroup\nDistributive\nJustice\negalitarian\nlibertarian\n(contra\nHeidari et\nal., see\nabove)\nlibertarian\n(contra\nHeidari et\nal., see\nabove)\nEffect on\nbias6\ntransforming\npreserving\npreserving\nPolicy /\n“worldview”7\nWe’re all\nequal (WAE)\nWhat you see\nis what you\nget (WYSIWIG)\nWhat you see\nis what you\nget (WYSIWIG)\n(A) Conclusion\nIn line with its original goal – to provide some help in starting to\nthink about AI fairness metrics – this article does not end with\nrecommendations. It does, however, end with an observation. As the last\nsection has shown, amidst all theorems and theories, all proofs and\nmemes, it makes sense to not lose sight of the concrete: the data trained\non, and the ML process as a whole. Fairness is not something to be\nevaluated post hoc; the feasibility of fairness is to be reflected on\nright from the beginning.\nIn that regard, assessing impact on fairness is not that different from\nthat essential, but often toilsome and non-beloved, stage of modeling\nthat precedes the modeling itself: exploratory data analysis.\nThanks for reading!\nPhoto by Anders Jildén on Unsplash\n\n\n\nBarocas, Solon, Moritz Hardt, and Arvind Narayanan. 2019. Fairness and Machine Learning. fairmlbook.org.\n\n\nChouldechova, Alexandra. 2016. “Fair prediction with disparate impact: A study of bias in recidivism prediction instruments.” arXiv e-Prints, October, arXiv:1610.07524. https://arxiv.org/abs/1610.07524.\n\n\nCranmer, Miles D., Alvaro Sanchez-Gonzalez, Peter W. Battaglia, Rui Xu, Kyle Cranmer, David N. Spergel, and Shirley Ho. 2020. “Discovering Symbolic Models from Deep Learning with Inductive Biases.” CoRR abs/2006.11287. https://arxiv.org/abs/2006.11287.\n\n\nFriedler, Sorelle A., Carlos Scheidegger, and Suresh Venkatasubramanian. 2016. “On the (Im)possibility of Fairness.” CoRR abs/1609.07236. http://arxiv.org/abs/1609.07236.\n\n\nHeidari, Hoda, Michele Loi, Krishna P. Gummadi, and Andreas Krause. 2018. “A Moral Framework for Understanding of Fair ML Through Economic Models of Equality of Opportunity.” CoRR abs/1809.03400. http://arxiv.org/abs/1809.03400.\n\n\nSrivastava, Prakhar, Kushal Chauhan, Deepanshu Aggarwal, Anupam Shukla, Joydip Dhar, and Vrashabh Prasad Jain. 2018. “Deep Learning Based Unsupervised POS Tagging for Sanskrit.” In Proceedings of the 2018 International Conference on Algorithms, Computing and Artificial Intelligence. ACAI 2018. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3302425.3302487.\n\n\nWachter, Sandra, Brent D. Mittelstadt, and Chris Russell. 2020a. “Bias Preservation in Machine Learning: The Legality of Fairness Metrics Under EU Non-Discrimination Law.” West Virginia Law Review, Forthcoming abs/2005.05906. https://ssrn.com/abstract=3792772.\n\n\n———. 2020b. “Why Fairness Cannot Be Automated: Bridging the Gap Between EU Non-Discrimination Law and AI.” CoRR abs/2005.05906. https://arxiv.org/abs/2005.05906.\n\n\nYeom, Samuel, and Michael Carl Tschantz. 2018. “Discriminative but Not Discriminatory: A Comparison of Fairness Definitions Under Different Worldviews.” CoRR abs/1808.08619. http://arxiv.org/abs/1808.08619.\n\n\nSrivastava et al. (2018)↩︎\nCranmer et al. (2020)↩︎\nWachter, Mittelstadt, and Russell (2020b)↩︎\nWachter, Mittelstadt, and Russell (2020b)↩︎\nBarocas, Hardt, and Narayanan (2019)↩︎\nWachter, Mittelstadt, and Russell (2020a)↩︎\nFriedler, Scheidegger, and Venkatasubramanian (2016)↩︎\n",
    "preview": "posts/2021-07-15-AI-fairness/images/preview.jpg",
    "last_modified": "2024-11-21T15:53:22+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-07-sparklyr-sedona/",
    "title": "sparklyr.sedona: A sparklyr extension for analyzing geospatial data",
    "description": "We are excited to announce the availability of sparklyr.sedona, a sparklyr extension making geospatial functionalities of the Apache Sedona library easily accessible from R.",
    "author": [
      {
        "name": "Yitao Li",
        "url": "https://github.com/yitao-li"
      }
    ],
    "date": "2021-07-07",
    "categories": [
      "R",
      "Packages/Releases",
      "Distributed Computing",
      "Spatial Data"
    ],
    "contents": "\n\nContents\nMotivation for sparklyr.sedona\nThe lay of the land\nAcknowledgements\n\nsparklyr.sedona is now available\nas the sparklyr-based R interface for Apache Sedona.\nTo install sparklyr.sedona from GitHub using\nthe remotes package\n1, run\n\n\nremotes::install_github(repo = \"apache/incubator-sedona\", subdir = \"R/sparklyr.sedona\")\n\n\nIn this blog post, we will provide a quick introduction to sparklyr.sedona, outlining the motivation behind\nthis sparklyr extension, and presenting some example sparklyr.sedona use cases involving Spark spatial RDDs,\nSpark dataframes, and visualizations.\nMotivation for sparklyr.sedona\nA suggestion from the\nmlverse survey results earlier\nthis year mentioned the need for up-to-date R interfaces for Spark-based GIS frameworks.\nWhile looking into this suggestion, we learned about\nApache Sedona, a geospatial data system powered by Spark\nthat is modern, efficient, and easy to use. We also realized that while our friends from the\nSpark open-source community had developed a\nsparklyr extension for GeoSpark, the\npredecessor of Apache Sedona, there was no similar extension making more recent Sedona\nfunctionalities easily accessible from R yet.\nWe therefore decided to work on sparklyr.sedona, which aims to bridge the gap between\nSedona and R.\nThe lay of the land2\nWe hope you are ready for a quick tour through some of the RDD-based and\nSpark-dataframe-based functionalities in sparklyr.sedona, and also, some bedazzling\nvisualizations derived from geospatial data in Spark.\nIn Apache Sedona,\nSpatial Resilient Distributed Datasets(SRDDs)\nare basic building blocks of distributed spatial data encapsulating\n“vanilla” RDDs of\ngeometrical objects and indexes. SRDDs support low-level operations such as Coordinate Reference System (CRS)\ntransformations, spatial partitioning, and spatial indexing. For example, with sparklyr.sedona, SRDD-based operations we can perform include the following:\nImporting some external data source into a SRDD:\n\n\nlibrary(sparklyr)\nlibrary(sparklyr.sedona)\n\nsedona_git_repo <- normalizePath(\"~/incubator-sedona\")\ndata_dir <- file.path(sedona_git_repo, \"core\", \"src\", \"test\", \"resources\")\n\nsc <- spark_connect(master = \"local\")\n\npt_rdd <- sedona_read_dsv_to_typed_rdd(\n  sc,\n  location = file.path(data_dir, \"arealm.csv\"),\n  type = \"point\"\n)\n\n\nApplying spatial partitioning to all data points:\n\n\nsedona_apply_spatial_partitioner(pt_rdd, partitioner = \"kdbtree\")\n\n\nBuilding spatial index on each partition:\n\n\nsedona_build_index(pt_rdd, type = \"quadtree\")\n\n\nJoining one spatial data set with another using “contain” or “overlap” as the join predicate:\n\n\npolygon_rdd <- sedona_read_dsv_to_typed_rdd(\n  sc,\n  location = file.path(data_dir, \"primaryroads-polygon.csv\"),\n  type = \"polygon\"\n)\n\npts_per_region_rdd <- sedona_spatial_join_count_by_key(\n  pt_rdd,\n  polygon_rdd,\n  join_type = \"contain\",\n  partitioner = \"kdbtree\"\n)\n\n\nIt is worth mentioning that sedona_spatial_join() will perform spatial partitioning\nand indexing on the inputs using the partitioner and index_type only if the inputs\nare not partitioned or indexed as specified already.\nFrom the examples above, one can see that SRDDs are great for spatial operations requiring\nfine-grained control, e.g., for ensuring a spatial join query is executed as efficiently\nas possible with the right types of spatial partitioning and indexing.\nFinally, we can try visualizing the join result above, using a choropleth map:\n\n\nsedona_render_choropleth_map(\n  pts_per_region_rdd,\n  resolution_x = 1000,\n  resolution_y = 600,\n  output_location = tempfile(\"choropleth-map-\"),\n  boundary = c(-126.790180, -64.630926, 24.863836, 50.000),\n  base_color = c(63, 127, 255)\n)\n\n\nwhich gives us the following:\nExample choropleth map outputWait, but something seems amiss. To make the visualization above look nicer, we can\noverlay it with the contour of each polygonal region:\n\n\ncontours <- sedona_render_scatter_plot(\n  polygon_rdd,\n  resolution_x = 1000,\n  resolution_y = 600,\n  output_location = tempfile(\"scatter-plot-\"),\n  boundary = c(-126.790180, -64.630926, 24.863836, 50.000),\n  base_color = c(255, 0, 0),\n  browse = FALSE\n)\n\nsedona_render_choropleth_map(\n  pts_per_region_rdd,\n  resolution_x = 1000,\n  resolution_y = 600,\n  output_location = tempfile(\"choropleth-map-\"),\n  boundary = c(-126.790180, -64.630926, 24.863836, 50.000),\n  base_color = c(63, 127, 255),\n  overlay = contours\n)\n\n\nwhich gives us the following:\nChoropleth map with overlayWith some low-level spatial operations taken care of using the SRDD API and\nthe right spatial partitioning and indexing data structures, we can then\nimport the results from SRDDs to Spark dataframes. When working with spatial\nobjects within Spark dataframes, we can write high-level, declarative queries\non these objects using dplyr verbs in conjunction with Sedona\nspatial UDFs, e.g.\n3\n, the\nfollowing query tells us whether each of the 8 nearest polygons to the\nquery point contains that point, and also, the convex hull of each polygon.\n\n\ntbl <- DBI::dbGetQuery(\n  sc, \"SELECT ST_GeomFromText(\\\"POINT(-66.3 18)\\\") AS `pt`\"\n)\npt <- tbl$pt[[1]]\nknn_rdd <- sedona_knn_query(\n  polygon_rdd, x = pt, k = 8, index_type = \"rtree\"\n)\n\nknn_sdf <- knn_rdd %>%\n  sdf_register() %>%\n  dplyr::mutate(\n    contains_pt = ST_contains(geometry, ST_Point(-66.3, 18)),\n    convex_hull = ST_ConvexHull(geometry)\n  )\n\nknn_sdf %>% print()\n\n\n# Source: spark<?> [?? x 3]\n  geometry                         contains_pt convex_hull\n  <list>                           <lgl>       <list>\n1 <POLYGON ((-66.335674 17.986328… TRUE        <POLYGON ((-66.335674 17.986328,…\n2 <POLYGON ((-66.335432 17.986626… TRUE        <POLYGON ((-66.335432 17.986626,…\n3 <POLYGON ((-66.335432 17.986626… TRUE        <POLYGON ((-66.335432 17.986626,…\n4 <POLYGON ((-66.335674 17.986328… TRUE        <POLYGON ((-66.335674 17.986328,…\n5 <POLYGON ((-66.242489 17.988637… FALSE       <POLYGON ((-66.242489 17.988637,…\n6 <POLYGON ((-66.242489 17.988637… FALSE       <POLYGON ((-66.242489 17.988637,…\n7 <POLYGON ((-66.24221 17.988799,… FALSE       <POLYGON ((-66.24221 17.988799, …\n8 <POLYGON ((-66.24221 17.988799,… FALSE       <POLYGON ((-66.24221 17.988799, …\nAcknowledgements\nThe author of this blog post would like to thank Jia Yu,\nthe creator of Apache Sedona, and Lorenz Walthert for\ntheir suggestion to contribute sparklyr.sedona to the upstream\nincubator-sedona repository. Jia has provided\nextensive code-review feedback to ensure sparklyr.sedona complies with coding standards\nand best practices of the Apache Sedona project, and has also been very helpful in the\ninstrumentation of CI workflows verifying sparklyr.sedona works as expected with snapshot\nversions of Sedona libraries from development branches.\nThe author is also grateful for his colleague Sigrid Keydana\nfor valuable editorial suggestions on this blog post.\nThat’s all. Thank you for reading!\nPhoto by NASA on Unsplash\n\nsparklyr.sedona was not released to CRAN yet at the time of writing.↩︎\nYes, pun intended↩︎\nThis demo requires sparklyr 1.7 or above to generate the required Spark SQL type casts for ST_Point() automatically.↩︎\n",
    "preview": "posts/2021-07-07-sparklyr-sedona/images/nasa-Q1p7bh3SHj8-unsplash.jpg",
    "last_modified": "2024-11-21T15:53:40+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-06-sparklyr-1.7.0-released/",
    "title": "sparklyr 1.7: New data sources and spark_apply() capabilities, better interfaces for sparklyr extensions, and more!",
    "description": "Sparklyr 1.7 delivers much-anticipated improvements, including R interfaces for image and binary data sources, several new spark_apply() capabilities, and better integration with sparklyr extensions.",
    "author": [
      {
        "name": "Yitao Li",
        "url": "https://github.com/yitao-li"
      }
    ],
    "date": "2021-07-06",
    "categories": [
      "R",
      "Packages/Releases",
      "Distributed Computing"
    ],
    "contents": "\nSparklyr 1.7 is now available on CRAN!\nTo install sparklyr 1.7 from CRAN, run\n\n\ninstall.packages(\"sparklyr\")\n\n\nIn this blog post, we wish to present the following highlights from the sparklyr 1.7 release:\nImage and binary data sources\nNew spark_apply() capabilities\nBetter integration with sparklyr extensions\nOther exciting news\nImage and binary data sources\nAs a unified analytics engine for large-scale data processing, Apache Spark\nis well-known for its ability to tackle challenges associated with the volume, velocity, and last but\nnot least, the variety of big data. Therefore it is hardly surprising to see that – in response to recent\nadvances in deep learning frameworks – Apache Spark has introduced built-in support for\nimage data sources\nand binary data sources (in releases 2.4 and 3.0, respectively).\nThe corresponding R interfaces for both data sources, namely,\nspark_read_image() and\nspark_read_binary(), were shipped\nrecently as part of sparklyr 1.7.\nThe usefulness of data source functionalities such as spark_read_image() is perhaps best illustrated\nby a quick demo below, where spark_read_image(), through the standard Apache Spark\nImageSchema,\nhelps connecting raw image inputs to a sophisticated feature extractor and a classifier, forming a powerful\nSpark application for image classifications.\nThe demo\n\nPhoto by Daniel Tuttle on\nUnsplash\nIn this demo, we shall construct a scalable Spark ML pipeline capable of classifying images of cats and dogs\naccurately and efficiently, using spark_read_image() and a pre-trained convolutional neural network\ncode-named Inception (Szegedy et al. (2015)).\nThe first step to building such a demo with maximum portability and repeatability is to create a\nsparklyr extension that accomplishes the following:\nSpecifying the required MVN dependencies of this demo (namely, the\nSpark Deep Learning library\n(Databricks, Inc. (2019)), which contains an Inception-V3-based image feature extractor accessible through\nthe Spark ML Transformer interface)\nBundling with itself two randomly selected1 and disjoint subsets of the\ndogs-vs-cats dataset (Elson et al. (2007)) as train and test data, which are stored in the extdata/{train,test} sub\ndirectories of the package)\nA reference implementation of such a sparklyr extension can be found in\nhere.\nThe second step, of course, is to make use of the above-mentioned sparklyr extension to perform some feature\nengineering. We will see very high-level features being extracted intelligently from each cat/dog image based\non what the pre-built Inception-V3 convolutional neural network has already learned from classifying a much\nbroader collection of images:\n\n\nlibrary(sparklyr)\nlibrary(sparklyr.deeperer)\n\n# NOTE: the correct spark_home path to use depends on the configuration of the\n# Spark cluster you are working with.\nspark_home <- \"/usr/lib/spark\"\nsc <- spark_connect(master = \"yarn\", spark_home = spark_home)\n\ndata_dir <- copy_images_to_hdfs()\n\n# extract features from train- and test-data\nimage_data <- list()\nfor (x in c(\"train\", \"test\")) {\n  # import\n  image_data[[x]] <- c(\"dogs\", \"cats\") %>%\n    lapply(\n      function(label) {\n        numeric_label <- ifelse(identical(label, \"dogs\"), 1L, 0L)\n        spark_read_image(\n          sc, dir = file.path(data_dir, x, label, fsep = \"/\")\n        ) %>%\n          dplyr::mutate(label = numeric_label)\n      }\n    ) %>%\n      do.call(sdf_bind_rows, .)\n\n  dl_featurizer <- invoke_new(\n    sc,\n    \"com.databricks.sparkdl.DeepImageFeaturizer\",\n    random_string(\"dl_featurizer\") # uid\n  ) %>%\n    invoke(\"setModelName\", \"InceptionV3\") %>%\n    invoke(\"setInputCol\", \"image\") %>%\n    invoke(\"setOutputCol\", \"features\")\n  image_data[[x]] <-\n    dl_featurizer %>%\n    invoke(\"transform\", spark_dataframe(image_data[[x]])) %>%\n    sdf_register()\n}\n\n\nThird step: equipped with features that summarize the content of each image well, we can\nbuild a Spark ML pipeline that recognizes cats and dogs using only logistic regression2\n\n\nlabel_col <- \"label\"\nprediction_col <- \"prediction\"\npipeline <- ml_pipeline(sc) %>%\n  ml_logistic_regression(\n    features_col = \"features\",\n    label_col = label_col,\n    prediction_col = prediction_col\n  )\nmodel <- pipeline %>% ml_fit(image_data$train)\n\n\nFinally, we can evaluate the accuracy of this model on the test images:\n\n\npredictions <- model %>%\n  ml_transform(image_data$test) %>%\n  dplyr::compute()\n\ncat(\"Predictions vs. labels:\\n\")\npredictions %>%\n  dplyr::select(!!label_col, !!prediction_col) %>%\n  print(n = sdf_nrow(predictions))\n\ncat(\"\\nAccuracy of predictions:\\n\")\npredictions %>%\n  ml_multiclass_classification_evaluator(\n    label_col = label_col,\n    prediction_col = prediction_col,\n    metric_name = \"accuracy\"\n  ) %>%\n    print()\n\n\n## Predictions vs. labels:\n## # Source: spark<?> [?? x 2]\n##    label prediction\n##    <int>      <dbl>\n##  1     1          1\n##  2     1          1\n##  3     1          1\n##  4     1          1\n##  5     1          1\n##  6     1          1\n##  7     1          1\n##  8     1          1\n##  9     1          1\n## 10     1          1\n## 11     0          0\n## 12     0          0\n## 13     0          0\n## 14     0          0\n## 15     0          0\n## 16     0          0\n## 17     0          0\n## 18     0          0\n## 19     0          0\n## 20     0          0\n##\n## Accuracy of predictions:\n## [1] 1\nNew spark_apply() capabilities\nOptimizations & custom serializers\nMany sparklyr users who have tried to run\nspark_apply() or\ndoSpark to\nparallelize R computations among Spark workers have probably encountered some\nchallenges arising from the serialization of R closures.\nIn some scenarios, the\nserialized size of the R closure can become too large, often due to the size\nof the enclosing R environment required by the closure. In other\nscenarios, the serialization itself may take too much time, partially offsetting\nthe performance gain from parallelization. Recently, multiple optimizations went\ninto sparklyr to address those challenges. One of the optimizations was to\nmake good use of the\nbroadcast variable\nconstruct in Apache Spark to reduce the overhead of distributing shared and\nimmutable task states across all Spark workers. In sparklyr 1.7, there is\nalso support for custom spark_apply() serializers, which offers more fine-grained\ncontrol over the trade-off between speed and compression level of serialization\nalgorithms. For example, one can specify\n\n\noptions(sparklyr.spark_apply.serializer = \"qs\")\n\n\n,\nwhich will apply the default options of qs::qserialize() to achieve a high\ncompression level, or\n\n\noptions(sparklyr.spark_apply.serializer = function(x) qs::qserialize(x, preset = \"fast\"))\noptions(sparklyr.spark_apply.deserializer = function(x) qs::qdeserialize(x))\n\n\n,\nwhich will aim for faster serialization speed with less compression.\nInferring dependencies automatically\nIn sparklyr 1.7, spark_apply() also provides the experimental\nauto_deps = TRUE option. With auto_deps enabled, spark_apply() will\nexamine the R closure being applied, infer the list of required R packages,\nand only copy the required R packages and their transitive dependencies\nto Spark workers. In many scenarios, the auto_deps = TRUE option will be a\nsignificantly better alternative compared to the default packages = TRUE\nbehavior, which is to ship everything within .libPaths() to Spark worker\nnodes, or the advanced packages = <package config> option, which requires\nusers to supply the list of required R packages or manually create a\nspark_apply() bundle.\nBetter integration with sparklyr extensions\nSubstantial effort went into sparklyr 1.7 to make lives easier for sparklyr\nextension authors. Experience suggests two areas where any sparklyr extension\ncan go through a frictional and non-straightforward path integrating with\nsparklyr are the following:\nThe dbplyr SQL translation environment\nInvocation of Java/Scala functions from R\nWe will elaborate on recent progress in both areas in the sub-sections below.\nCustomizing the dbplyr SQL translation environment\nsparklyr extensions can now customize sparklyr’s dbplyr SQL translations\nthrough the\nspark_dependency()\nspecification returned from spark_dependencies() callbacks.\nThis type of flexibility becomes useful, for instance, in scenarios where a\nsparklyr extension needs to insert type casts for inputs to custom Spark\nUDFs. We can find a concrete example of this in\nsparklyr.sedona,\na sparklyr extension to facilitate geo-spatial analyses using\nApache Sedona. Geo-spatial UDFs supported by Apache\nSedona such as ST_Point() and ST_PolygonFromEnvelope() require all inputs to be\nDECIMAL(24, 20) quantities rather than DOUBLEs. Without any customization to\nsparklyr’s dbplyr SQL variant, the only way for a dplyr\nquery involving ST_Point() to actually work in sparklyr would be to explicitly\nimplement any type cast needed by the query using dplyr::sql(), e.g.,\n\n\nmy_geospatial_sdf <- my_geospatial_sdf %>%\n  dplyr::mutate(\n    x = dplyr::sql(\"CAST(`x` AS DECIMAL(24, 20))\"),\n    y = dplyr::sql(\"CAST(`y` AS DECIMAL(24, 20))\")\n  ) %>%\n  dplyr::mutate(pt = ST_Point(x, y))\n\n\n.\nThis would, to some extent, be antithetical to dplyr’s goal of freeing R users from\nlaboriously spelling out SQL queries. Whereas by customizing sparklyr’s dplyr SQL\ntranslations (as implemented in\nhere\nand\nhere\n), sparklyr.sedona allows users to simply write\n\n\nmy_geospatial_sdf <- my_geospatial_sdf %>% dplyr::mutate(pt = ST_Point(x, y))\n\n\ninstead, and the required Spark SQL type casts are generated automatically.\nImproved interface for invoking Java/Scala functions\nIn sparklyr 1.7, the R interface for Java/Scala invocations saw a number of\nimprovements.\nWith previous versions of sparklyr, many sparklyr extension authors would\nrun into trouble when attempting to invoke Java/Scala functions accepting an\nArray[T] as one of their parameters, where T is any type bound more specific\nthan java.lang.Object / AnyRef. This was because any array of objects passed\nthrough sparklyr’s Java/Scala invocation interface will be interpreted as simply\nan array of java.lang.Objects in absence of additional type information.\nFor this reason, a helper function\njarray() was implemented as\npart of sparklyr 1.7 as a way to overcome the aforementioned problem.\nFor example, executing\n\n\nsc <- spark_connect(...)\n\narr <- jarray(\n  sc,\n  seq(5) %>% lapply(function(x) invoke_new(sc, \"MyClass\", x)),\n  element_type = \"MyClass\"\n)\n\n\nwill assign to arr a reference to an Array[MyClass] of length 5, rather\nthan an Array[AnyRef]. Subsequently, arr becomes suitable to be passed as a\nparameter to functions accepting only Array[MyClass]s as inputs. Previously,\nsome possible workarounds of this sparklyr limitation included changing\nfunction signatures to accept Array[AnyRef]s instead of Array[MyClass]s, or\nimplementing a “wrapped” version of each function accepting Array[AnyRef]\ninputs and converting them to Array[MyClass] before the actual invocation.\nNone of such workarounds was an ideal solution to the problem.\nAnother similar hurdle that was addressed in sparklyr 1.7 as well involves\nfunction parameters that must be single-precision floating point numbers or\narrays of single-precision floating point numbers.\nFor those scenarios,\njfloat() and\njfloat_array()\nare the helper functions that allow numeric quantities in R to be passed to\nsparklyr’s Java/Scala invocation interface as parameters with desired types.\nIn addition, while previous verisons of sparklyr failed to serialize\nparameters with NaN values correctly, sparklyr 1.7 preserves NaNs as\nexpected in its Java/Scala invocation interface.\nOther exciting news\nThere are numerous other new features, enhancements, and bug fixes made to\nsparklyr 1.7, all listed in the\nNEWS.md\nfile of the sparklyr repo and documented in sparklyr’s\nHTML reference pages.\nIn the interest of brevity, we will not describe all of them in great detail\nwithin this blog post.\nAcknowledgement\nIn chronological order, we would like to thank the following individuals who\nhave authored or co-authored pull requests that were part of the sparklyr 1.7\nrelease:\n@yitao-li\n@mzorko\n@jozefhajnala\n@lresende\nWe’re also extremely grateful to everyone who has submitted\nfeature requests or bug reports, many of which have been tremendously helpful in\nshaping sparklyr into what it is today.\nFurthermore, the author of this blog post is indebted to\n@skeydan for her awesome editorial suggestions.\nWithout her insights about good writing and story-telling, expositions like this\none would have been less readable.\nIf you wish to learn more about sparklyr, we recommend visiting\nsparklyr.ai, spark.rstudio.com,\nand also reading some previous sparklyr release posts such as\nsparklyr 1.6\nand\nsparklyr 1.5.\nThat is all. Thanks for reading!\n\n\n\nDatabricks, Inc. 2019. Deep Learning Pipelines for Apache Spark (version 1.5.0). https://spark-packages.org/package/databricks/spark-deep-learning.\n\n\nElson, Jeremy, John (JD) Douceur, Jon Howell, and Jared Saul. 2007. “Asirra: A CAPTCHA That Exploits Interest-Aligned Manual Image Categorization.” In Proceedings of 14th ACM Conference on Computer and Communications Security (CCS), Proceedings of 14th ACM Conference on Computer and Communications Security (CCS). Association for Computing Machinery, Inc. https://www.microsoft.com/en-us/research/publication/asirra-a-captcha-that-exploits-interest-aligned-manual-image-categorization/.\n\n\nSzegedy, Christian, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015. “Going Deeper with Convolutions.” In Computer Vision and Pattern Recognition (CVPR). http://arxiv.org/abs/1409.4842.\n\n\nFun exercise for our readers: why not experiment with different subsets of cats-vs-dogs images for training\nand testing, or even better, replace train and test images with your own images of cats and dogs, and see what\nhappens?↩︎\nAnother way to see why it works: in fact the pre-built Inception-based feature\nextractor simply applies all transformations Inception would have applied to its input,\nexcept for the last logistic-regression-esque affine transformation plus non-linearity\nproducing the final categorical output, and Inception is a highly successful\nconvolutional neural network trained to recognize 1000 categories of animals and objects,\nincluding multiple types of cats and dogs.↩︎\n",
    "preview": "posts/2021-07-06-sparklyr-1.7.0-released/images/sparklyr-1.7.png",
    "last_modified": "2024-11-21T15:51:13+00:00",
    "input_file": {},
    "preview_width": 327,
    "preview_height": 142
  },
  {
    "path": "posts/2021-06-17-luz/",
    "title": "Que haja luz: More light for torch!",
    "description": "Today, we're introducing luz, a high-level interface to torch that lets you train neural networks in a concise, declarative style. In some sense, it is to torch what Keras is to TensorFlow: It provides both a streamlined workflow and powerful ways for customization.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2021-06-17",
    "categories": [
      "Torch",
      "R",
      "Packages/Releases"
    ],
    "contents": "\n\nContents\nTrain and validate, then test: A basic deep-learning workflow with luz\nHow to do (almost) anything (almost) anytime\n\n… Before we start, my apologies to our Spanish-speaking readers … I had to make a choice between “haja”1 and “haya”2, and in the end it was all up to a coin flip …\nAs I write this, we’re more than happy with the rapid adoption we’ve seen of torch – not just for immediate use, but also, in packages that build on it, making use of its core functionality.\nIn an applied scenario, though – a scenario that involves training and validating in lockstep, computing metrics and acting on them, and dynamically changing hyper-parameters during the process – it may sometimes seem like there’s a non-negligible amount of boilerplate code involved. For one, there is the main loop over epochs, and inside, the loops over training and validation batches. Furthermore, steps like updating the model’s mode (training or validation, resp.), zeroing out and computing gradients, and propagating back model updates have to be performed in the correct order. Last not least, care has to be taken that at any moment, tensors are located on the expected device.\nWouldn’t it be dreamy if, as the popular-in-the-early-2000s “Head First …”3 series used to say, there was a way to eliminate those manual steps, while keeping the flexibility? With luz, there is.\nIn this post, our focus is on two things: First of all, the streamlined workflow itself; and second, generic mechanisms that allow for customization. For more detailed examples of the latter, plus concrete coding instructions, we will link to the (already-extensive) documentation.\nTrain and validate, then test: A basic deep-learning workflow with luz\nTo demonstrate the essential workflow, we make use of a dataset that’s readily available and won’t distract us too much, pre-processing-wise: namely, the Dogs vs. Cats collection that comes with torchdatasets. torchvision will be needed for image transformations; apart from those two packages all we need are torch and luz.\n\n\n# all these are available on CRAN\nlibrary(torch)\nlibrary(torchvision)\nlibrary(torchdatasets)\nlibrary(luz)\n\n\nData\nThe dataset is downloaded from Kaggle; you’ll need to edit the path below to reflect the location of your own Kaggle token.\n\n\ndir <- \"~/Downloads/dogs-vs-cats\" \n\nds <- torchdatasets::dogs_vs_cats_dataset(\n  dir,\n  token = \"~/.kaggle/kaggle.json\",\n  transform = . %>%\n    torchvision::transform_to_tensor() %>%\n    torchvision::transform_resize(size = c(224, 224)) %>% \n    torchvision::transform_normalize(rep(0.5, 3), rep(0.5, 3)),\n  target_transform = function(x) as.double(x) - 1\n)\n\n\nConveniently, we can use dataset_subset() to partition the data into training, validation, and test sets.\n\n\ntrain_ids <- sample(1:length(ds), size = 0.6 * length(ds))\nvalid_ids <- sample(setdiff(1:length(ds), train_ids), size = 0.2 * length(ds))\ntest_ids <- setdiff(1:length(ds), union(train_ids, valid_ids))\n\ntrain_ds <- dataset_subset(ds, indices = train_ids)\nvalid_ds <- dataset_subset(ds, indices = valid_ids)\ntest_ds <- dataset_subset(ds, indices = test_ids)\n\n\nNext, we instantiate the respective dataloaders.\n\n\ntrain_dl <- dataloader(train_ds, batch_size = 64, shuffle = TRUE, num_workers = 4)\nvalid_dl <- dataloader(valid_ds, batch_size = 64, num_workers = 4)\ntest_dl <- dataloader(test_ds, batch_size = 64, num_workers = 4)\n\n\nThat’s it for the data – no change in workflow so far. Neither is there a difference in how we define the model.\nModel\nTo speed up training, we build on pre-trained AlexNet ( Krizhevsky (2014)).\n\n\nnet <- torch::nn_module(\n  \n  initialize = function(output_size) {\n    self$model <- model_alexnet(pretrained = TRUE)\n\n    for (par in self$parameters) {\n      par$requires_grad_(FALSE)\n    }\n\n    self$model$classifier <- nn_sequential(\n      nn_dropout(0.5),\n      nn_linear(9216, 512),\n      nn_relu(),\n      nn_linear(512, 256),\n      nn_relu(),\n      nn_linear(256, output_size)\n    )\n  },\n  forward = function(x) {\n    self$model(x)[,1]\n  }\n  \n)\n\n\nIf you look closely, you see that all we’ve done so far is define the model. Unlike in a torch-only workflow, we are not going to instantiate it, and neither are we going to move it to an eventual GPU.\nExpanding on the latter, we can say more: All of device handling is managed by luz. It probes for existence of a CUDA-capable GPU, and if it finds one, makes sure both model weights and data tensors are moved there transparently whenever needed. The same goes for the opposite direction: Predictions computed on the test set, for example, are silently transferred to the CPU, ready for the user to further manipulate them in R. But as to predictions, we’re not quite there yet: On to model training, where the difference made by luz jumps right to the eye.\nTraining\nBelow, you see four calls to luz, two of which are required in every setting, and two are case-dependent. The always-needed ones are setup() and fit() :\nIn setup(), you tell luz what the loss should be, and which optimizer to use. Optionally, beyond the loss itself (the primary metric, in a sense, in that it informs weight updating) you can have luz compute additional ones. Here, for example, we ask for classification accuracy. (For a human watching a progress bar, a two-class accuracy of 0.91 is way more indicative than cross-entropy loss of 1.26.)\nIn fit(), you pass references to the training and validation dataloaders. Although a default exists for the number of epochs to train for, you’ll normally want to pass a custom value for this parameter, too.\nThe case-dependent calls here, then, are those to set_hparams() and set_opt_hparams(). Here,\nset_hparams() appears because, in the model definition, we had initialize() take a parameter, output_size. Any arguments expected by initialize() need to be passed via this method.\nset_opt_hparams() is there because we want to use a non-default learning rate with optim_adam(). Were we content with the default, no such call would be in order.\n\n\nfitted <- net %>%\n  setup(\n    loss = nn_bce_with_logits_loss(),\n    optimizer = optim_adam,\n    metrics = list(\n      luz_metric_binary_accuracy_with_logits()\n    )\n  ) %>%\n  set_hparams(output_size = 1) %>%\n  set_opt_hparams(lr = 0.01) %>%\n  fit(train_dl, epochs = 3, valid_data = valid_dl)\n\n\nHere’s how the output looked for me:\n\nEpoch 1/3\nTrain metrics: Loss: 0.8692 - Acc: 0.9093\nValid metrics: Loss: 0.1816 - Acc: 0.9336\nEpoch 2/3\nTrain metrics: Loss: 0.1366 - Acc: 0.9468\nValid metrics: Loss: 0.1306 - Acc: 0.9458\nEpoch 3/3\nTrain metrics: Loss: 0.1225 - Acc: 0.9507\nValid metrics: Loss: 0.1339 - Acc: 0.947\n\nTraining finished, we can ask luz to save the trained model:\n\n\nluz_save(fitted, \"dogs-and-cats.pt\")\n\n\nTest set predictions\nAnd finally, predict() will obtain predictions on the data pointed to by a passed-in dataloader – here, the test set. It expects a fitted model as its first argument.\n\n\npreds <- predict(fitted, test_dl)\n\nprobs <- torch_sigmoid(preds)\nprint(probs, n = 5)\n\n\ntorch_tensor\n 1.2959e-01\n 1.3032e-03\n 6.1966e-05\n 5.9575e-01\n 4.5577e-03\n... [the output was truncated (use n=-1 to disable)]\n[ CPUFloatType{5000} ]\nAnd that’s it for a complete workflow. In case you have prior experience with Keras, this should feel pretty familiar. The same can be said for the most versatile-yet-standardized customization technique implemented in luz.\nHow to do (almost) anything (almost) anytime\nLike Keras, luz has the concept of callbacks that can “hook into” the training process and execute arbitrary R code. Specifically, code can be scheduled to run at any of the following points in time:\nwhen the overall training process starts or ends (on_fit_begin() / on_fit_end());\nwhen an epoch of training plus validation starts or ends (on_epoch_begin() / on_epoch_end());\nwhen during an epoch, the training (validation, resp.) half starts or ends (on_train_begin() / on_train_end(); on_valid_begin() / on_valid_end());\nwhen during training (validation, resp.) a new batch is either about to, or has been processed (on_train_batch_begin() / on_train_batch_end(); on_valid_batch_begin() / on_valid_batch_end());\nand even at specific landmarks inside the “innermost” training / validation logic, such as “after loss computation,” “after backward,” or “after step.”\nWhile you can implement any logic you wish using this technique, luz already comes equipped with a very useful set of callbacks.\n\nSee the main vignette, Getting started with luz, for instructions on how to create callback objects.\nFor example:\nluz_callback_model_checkpoint() periodically saves model weights.\nluz_callback_lr_scheduler() allows to activate one of torch’s learning rate schedulers. Different schedulers exist, each following their own logic in how they dynamically adjust the learning rate.\nluz_callback_early_stopping() terminates training once model performance stops improving.\nCallbacks are passed to fit() in a list. Here we adapt our above example, making sure that (1) model weights are saved after each epoch and (2), training terminates if validation loss does not improve for two epochs in a row.\n\n\nfitted <- net %>%\n  setup(\n    loss = nn_bce_with_logits_loss(),\n    optimizer = optim_adam,\n    metrics = list(\n      luz_metric_binary_accuracy_with_logits()\n    )\n  ) %>%\n  set_hparams(output_size = 1) %>%\n  set_opt_hparams(lr = 0.01) %>%\n  fit(train_dl,\n      epochs = 10,\n      valid_data = valid_dl,\n      callbacks = list(luz_callback_model_checkpoint(path = \"./models\"),\n                       luz_callback_early_stopping(patience = 2)))\n\n\nWhat about other types of flexibility requirements – such as in the scenario of multiple, interacting models, equipped, each, with their own loss functions and optimizers4? In such cases, the code will get a bit longer than what we’ve been seeing here, but luz can still help considerably with streamlining the workflow.\n\nSeveral alternatives, trading off control and convenience, are discussed in the Custom loops and Accelerator vignettes.\nTo conclude, using luz, you lose nothing of the flexibility that comes with torch, while gaining a lot in code simplicity, modularity, and maintainability. We’d be happy to hear you’ll give it a try!\nThanks for reading!\nPhoto by JD Rincs on Unsplash\n\n\n\nKrizhevsky, Alex. 2014. “One Weird Trick for Parallelizing Convolutional Neural Networks.” CoRR abs/1404.5997. http://arxiv.org/abs/1404.5997.\n\n\nPortuguese↩︎\nSpanish↩︎\nA well-known (at the time) exemplar having been, e.g., “Head First Design Patterns,” by Freeman et al..↩︎\nThink GANs (Generative Adversarial Networks), a popular architecture rooted in game-theoretic concepts.↩︎\n",
    "preview": "posts/2021-06-17-luz/images/preview.jpg",
    "last_modified": "2024-11-21T15:52:05+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-22-torch-for-optimization/",
    "title": "torch for optimization",
    "description": "Torch is not just for deep learning. Its L-BFGS optimizer, complete with Strong-Wolfe line search, is a powerful tool in unconstrained as well as constrained optimization.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2021-04-27",
    "categories": [
      "Torch",
      "R"
    ],
    "contents": "\n\nContents\nFunction minimization, DYI approach\nFunction minimization with torch optimizers\nAdam\nL-BFGS\n\n(Yet) more fun with L-BFGS\nL-BFGS with line search\nQuadratic penalty for constrained optimization\n\nConclusion\nAppendix\nRosenbrock function plotting code\nFlower function plotting code\n\n\nSo far, all torch use cases we’ve discussed here have been in deep learning. However, its automatic differentiation feature is useful in other areas. One prominent example is numerical optimization: We can use torch to find the minimum of a function.\nIn fact, function minimization is exactly what happens in training a neural network. But there, the function in question normally is far too complex to even imagine finding its minima analytically. Numerical optimization aims at building up the tools to handle just this complexity. To that end, however, it starts from functions that are far less deeply composed. Instead, they are hand-crafted to pose specific challenges.\nThis post is a first introduction to numerical optimization with torch. Central takeaways are the existence and usefulness of its L-BFGS optimizer, as well as the impact of running L-BFGS with line search. As a fun add-on, we show an example of constrained optimization, where a constraint is enforced via a quadratic penalty function.\nTo warm up, we take a detour, minimizing a function “ourselves” using nothing but tensors. This will turn out to be relevant later, though, as the overall process will still be the same. All changes will be related to integration of optimizers and their capabilities.\nFunction minimization, DYI approach\nTo see how we can minimize a function “by hand”, let’s try the iconic Rosenbrock function. This is a function with two variables:\n\\[\nf(x_1, x_2) = (a - x_1)^2 + b * (x_2 - x_1^2)^2\n\\]\n, with \\(a\\) and \\(b\\) configurable parameters often set to 1 and 5, respectively.\nIn R:\n\n\nlibrary(torch)\n\na <- 1\nb <- 5\n\nrosenbrock <- function(x) {\n  x1 <- x[1]\n  x2 <- x[2]\n  (a - x1)^2 + b * (x2 - x1^2)^2\n}\n\n\nIts minimum is located at (1,1), inside a narrow valley surrounded by breakneck-steep cliffs:1\n\n\n\nFigure 1: Rosenbrock function.\n\n\n\nOur goal and strategy are as follows.\nWe want to find the values \\(x_1\\) and \\(x_2\\) for which the function attains its minimum. We have to start somewhere; and from wherever that gets us on the graph we follow the negative of the gradient “downwards”, descending into regions of consecutively smaller function value.\nConcretely, in every iteration, we take the current \\((x1,x2)\\) point, compute the function value as well as the gradient, and subtract some fraction of the latter to arrive at a new \\((x1,x2)\\) candidate. This process goes on until we either reach the minimum – the gradient is zero – or improvement is below a chosen threshold.\nHere is the corresponding code. For no special reasons, we start at (-1,1) . The learning rate (the fraction of the gradient to subtract) needs some experimentation. (Try 0.1 and 0.001 to see its impact.)\n\n\nnum_iterations <- 1000\n\n# fraction of the gradient to subtract \nlr <- 0.01\n\n# function input (x1,x2)\n# this is the tensor w.r.t. which we'll have torch compute the gradient\nx_star <- torch_tensor(c(-1, 1), requires_grad = TRUE)\n\nfor (i in 1:num_iterations) {\n\n  if (i %% 100 == 0) cat(\"Iteration: \", i, \"\\n\")\n\n  # call function\n  value <- rosenbrock(x_star)\n  if (i %% 100 == 0) cat(\"Value is: \", as.numeric(value), \"\\n\")\n\n  # compute gradient of value w.r.t. params\n  value$backward()\n  if (i %% 100 == 0) cat(\"Gradient is: \", as.matrix(x_star$grad), \"\\n\\n\")\n\n  # manual update\n  with_no_grad({\n    x_star$sub_(lr * x_star$grad)\n    x_star$grad$zero_()\n  })\n}\n\n\nIteration:  100 \nValue is:  0.3502924 \nGradient is:  -0.667685 -0.5771312 \n\nIteration:  200 \nValue is:  0.07398106 \nGradient is:  -0.1603189 -0.2532476 \n\n...\n...\n\nIteration:  900 \nValue is:  0.0001532408 \nGradient is:  -0.004811743 -0.009894371 \n\nIteration:  1000 \nValue is:  6.962555e-05 \nGradient is:  -0.003222887 -0.006653666 \nWhile this works, it really serves to illustrate the principle. With torch providing a bunch of proven optimization algorithms, there is no need for us to manually compute the candidate \\(\\mathbf{x}\\) values.\nFunction minimization with torch optimizers\nInstead, we let a torch optimizer update the candidate \\(\\mathbf{x}\\) for us. Habitually, our first try is Adam.\nAdam\nWith Adam, optimization proceeds a lot faster. Truth be told, though, choosing a good learning rate still takes non-negligeable experimentation. (Try the default learning rate, 0.001, for comparison.)\n\n\nnum_iterations <- 100\n\nx_star <- torch_tensor(c(-1, 1), requires_grad = TRUE)\n\nlr <- 1\noptimizer <- optim_adam(x_star, lr)\n\nfor (i in 1:num_iterations) {\n  \n  if (i %% 10 == 0) cat(\"Iteration: \", i, \"\\n\")\n  \n  optimizer$zero_grad()\n  value <- rosenbrock(x_star)\n  if (i %% 10 == 0) cat(\"Value is: \", as.numeric(value), \"\\n\")\n  \n  value$backward()\n  optimizer$step()\n  \n  if (i %% 10 == 0) cat(\"Gradient is: \", as.matrix(x_star$grad), \"\\n\\n\")\n  \n}\n\n\nIteration:  10 \nValue is:  0.8559565 \nGradient is:  -1.732036 -0.5898831 \n\nIteration:  20 \nValue is:  0.1282992 \nGradient is:  -3.22681 1.577383 \n\n...\n...\n\nIteration:  90 \nValue is:  4.003079e-05 \nGradient is:  -0.05383469 0.02346456 \n\nIteration:  100 \nValue is:  6.937736e-05 \nGradient is:  -0.003240437 -0.006630421 \nIt took us about a hundred iterations to arrive at a decent value. This is a lot faster than the manual approach above, but still quite a lot. Luckily, further improvements are possible.\nL-BFGS\nAmong the many torch optimizers commonly used in deep learning (Adam, AdamW, RMSprop …), there is one “outsider”, much better known in classic numerical optimization than in neural-networks space: L-BFGS, a.k.a. Limited-memory BFGS, a memory-optimized implementation of the Broyden–Fletcher–Goldfarb–Shanno optimization algorithm (BFGS).\nBFGS is perhaps the most widely used among the so-called Quasi-Newton, second-order optimization algorithms. As opposed to the family of first-order algorithms that, in deciding on a descent direction, make use of gradient information only, second-order algorithms additionally take curvature information into account. To that end, exact Newton methods actually compute the Hessian (a costly operation), while Quasi-Newton methods avoid that cost and, instead, resort to iterative approximation.\nLooking at the contours of the Rosenbrock function, with its prolonged, narrow valley, it is not difficult to imagine that curvature information might make a difference. And, as you’ll see in a second, it really does. Before though, one note on the code. When using L-BFGS, it is necessary to wrap both function call and gradient evaluation in a closure (calc_loss(), in the below snippet), for them to be callable several times per iteration. You can convince yourself that the closure is, in fact, entered repeatedly, by inspecting this code snippet’s chatty output:\n\n\nnum_iterations <- 3\n\nx_star <- torch_tensor(c(-1, 1), requires_grad = TRUE)\n\noptimizer <- optim_lbfgs(x_star)\n\ncalc_loss <- function() {\n\n  optimizer$zero_grad()\n\n  value <- rosenbrock(x_star)\n  cat(\"Value is: \", as.numeric(value), \"\\n\")\n\n  value$backward()\n  cat(\"Gradient is: \", as.matrix(x_star$grad), \"\\n\\n\")\n  value\n\n}\n\nfor (i in 1:num_iterations) {\n  cat(\"Iteration: \", i, \"\\n\")\n  optimizer$step(calc_loss)\n}\n\n\nIteration:  1 \nValue is:  4 \nGradient is:  -4 0 \n\nValue is:  6 \nGradient is:  -2 10 \n\n...\n...\n\nValue is:  0.04880721 \nGradient is:  -0.262119 -0.1132655 \n\nValue is:  0.0302862 \nGradient is:  1.293824 -0.7403332 \n\nIteration:  2 \nValue is:  0.01697086 \nGradient is:  0.3468466 -0.3173429 \n\nValue is:  0.01124081 \nGradient is:  0.2420997 -0.2347881 \n\n...\n...\n\nValue is:  1.111701e-09 \nGradient is:  0.0002865837 -0.0001251698 \n\nValue is:  4.547474e-12 \nGradient is:  -1.907349e-05 9.536743e-06 \n\nIteration:  3 \nValue is:  4.547474e-12 \nGradient is:  -1.907349e-05 9.536743e-06 \nEven though we ran the algorithm for three iterations, the optimal value really is reached after two. Seeing how well this worked, we try L-BFGS on a more difficult function, named flower, for pretty self-evident reasons.\n(Yet) more fun with L-BFGS\nHere is the flower function.2 Mathematically, its minimum is near (0,0), but technically the function itself is undefined at (0,0), since the atan2 used in the function is not defined there.\n\n\na <- 1\nb <- 1\nc <- 4\n\nflower <- function(x) {\n  a * torch_norm(x) + b * torch_sin(c * torch_atan2(x[2], x[1]))\n}\n\n\n\n\n\nFigure 2: Flower function.\n\n\n\nWe run the same code as above, starting from (20,20) this time.\n\n\nnum_iterations <- 3\n\nx_star <- torch_tensor(c(20, 0), requires_grad = TRUE)\n\noptimizer <- optim_lbfgs(x_star)\n\ncalc_loss <- function() {\n\n  optimizer$zero_grad()\n\n  value <- flower(x_star)\n  cat(\"Value is: \", as.numeric(value), \"\\n\")\n\n  value$backward()\n  cat(\"Gradient is: \", as.matrix(x_star$grad), \"\\n\")\n  \n  cat(\"X is: \", as.matrix(x_star), \"\\n\\n\")\n  \n  value\n\n}\n\nfor (i in 1:num_iterations) {\n  cat(\"Iteration: \", i, \"\\n\")\n  optimizer$step(calc_loss)\n}\n\n\nIteration:  1 \nValue is:  28.28427 \nGradient is:  0.8071069 0.6071068 \nX is:  20 20 \n\n...\n...\n\nValue is:  19.33546 \nGradient is:  0.8100872 0.6188223 \nX is:  12.957 14.68274 \n\n...\n...\n\nValue is:  18.29546 \nGradient is:  0.8096464 0.622064 \nX is:  12.14691 14.06392 \n\n...\n...\n\nValue is:  9.853705 \nGradient is:  0.7546976 0.7025688 \nX is:  5.763702 8.895616 \n\nValue is:  2635.866 \nGradient is:  -0.7407354 -0.6717985 \nX is:  -1949.697 -1773.551 \n\nIteration:  2 \nValue is:  1333.113 \nGradient is:  -0.7413024 -0.6711776 \nX is:  -985.4553 -897.5367 \n\nValue is:  30.16862 \nGradient is:  -0.7903821 -0.6266789 \nX is:  -21.02814 -21.72296 \n\nValue is:  1281.39 \nGradient is:  0.7544561 0.6563575 \nX is:  964.0121 843.7817 \n\nValue is:  628.1306 \nGradient is:  0.7616636 0.6480014 \nX is:  475.7051 409.7372 \n\nValue is:  4965690 \nGradient is:  -0.7493951 -0.662123 \nX is:  -3721262 -3287901 \n\nValue is:  2482306 \nGradient is:  -0.7503822 -0.6610042 \nX is:  -1862675 -1640817 \n\nValue is:  8.61863e+11 \nGradient is:  0.7486113 0.6630091 \nX is:  645200412672 571423064064 \n\nValue is:  430929412096 \nGradient is:  0.7487153 0.6628917 \nX is:  322643460096 285659529216 \n\nValue is:  Inf \nGradient is:  0 0 \nX is:  -2.826342e+19 -2.503904e+19 \n\nIteration:  3 \nValue is:  Inf \nGradient is:  0 0 \nX is:  -2.826342e+19 -2.503904e+19 \nThis has been less of a success. At first, loss decreases nicely, but suddenly, the estimate dramatically overshoots, and keeps bouncing between negative and positive outer space ever after.\nLuckily, there is something we can do.\nL-BFGS with line search\nTaken in isolation, what a Quasi-Newton method like L-BFGS does is determine the best descent direction. However, as we just saw, a good direction is not enough. With the flower function, wherever we are, the optimal path leads to disaster if we stay on it long enough. Thus, we need an algorithm that carefully evaluates not only where to go, but also, how far.\nFor this reason, L-BFGS implementations commonly incorporate line search, that is, a set of rules indicating whether a proposed step length is a good one, or should be improved upon.\nSpecifically, torch’s L-BFGS optimizer implements the Strong Wolfe conditions. We re-run the above code, changing just two lines. Most importantly, the one where the optimizer is instantiated:\n\n\noptimizer <- optim_lbfgs(x_star, line_search_fn = \"strong_wolfe\")\n\n\nAnd secondly, this time I found that after the third iteration, loss continued to decrease for a while, so I let it run for five iterations. Here is the output:\nIteration:  1 \n...\n...\n\nValue is:  -0.8838741 \nGradient is:  3.742207 7.521572 \nX is:  0.09035123 -0.03220009 \n\nValue is:  -0.928809 \nGradient is:  1.464702 0.9466625 \nX is:  0.06564617 -0.026706 \n\nIteration:  2 \n...\n...\n\nValue is:  -0.9991404 \nGradient is:  39.28394 93.40318 \nX is:  0.0006493925 -0.0002656128 \n\nValue is:  -0.9992246 \nGradient is:  6.372203 12.79636 \nX is:  0.0007130796 -0.0002947929 \n\nIteration:  3 \n...\n...\n\nValue is:  -0.9997789 \nGradient is:  3.565234 5.995832 \nX is:  0.0002042478 -8.457939e-05 \n\nValue is:  -0.9998025 \nGradient is:  -4.614189 -13.74602 \nX is:  0.0001822711 -7.553725e-05 \n\nIteration:  4 \n...\n...\n\nValue is:  -0.9999917 \nGradient is:  -382.3041 -921.4625 \nX is:  -6.320081e-06 2.614706e-06 \n\nValue is:  -0.9999923 \nGradient is:  -134.0946 -321.2681 \nX is:  -6.921942e-06 2.865841e-06 \n\nIteration:  5 \n...\n...\n\nValue is:  -0.9999999 \nGradient is:  -3446.911 -8320.007 \nX is:  -7.267168e-08 3.009783e-08 \n\nValue is:  -0.9999999 \nGradient is:  -3419.361 -8253.501 \nX is:  -7.404627e-08 3.066708e-08 \nIt’s still not perfect, but a lot better.\nFinally, let’s go one step further. Can we use torch for constrained optimization?\nQuadratic penalty for constrained optimization\nIn constrained optimization, we still search for a minimum, but that minimum can’t reside just anywhere: Its location has to fulfill some number of additional conditions. In optimization lingo, it has to be feasible.\nTo illustrate, we stay with the flower function, but add on a constraint: \\(\\mathbf{x}\\) has to lie outside a circle of radius \\(sqrt(2)\\), centered at the origin. Formally, this yields the inequality constraint\n\\[\n2 - {x_1}^2 - {x_2}^2 <= 0\n\\]\nA way to minimize flower and yet, at the same time, honor the constraint is to use a penalty function. With penalty methods, the value to be minimized is a sum of two things: the target function’s output and a penalty reflecting potential constraint violation. Use of a quadratic penalty, for example, results in adding a multiple of the square of the constraint function’s output:\n\n\n# x^2 + y^2 >= 2\n# 2 - x^2 - y^2 <= 0\nconstraint <- function(x) 2 - torch_square(torch_norm(x))\n\n# quadratic penalty\npenalty <- function(x) torch_square(torch_max(constraint(x), other = 0))\n\n\nA priori, we can’t know how big that multiple has to be to enforce the constraint. Therefore, optimization proceeds iteratively. We start with a small multiplier, \\(1\\), say, and increase it for as long as the constraint is still violated:\n\n\npenalty_method <- function(f, p, x, k_max, rho = 1, gamma = 2, num_iterations = 1) {\n\n  for (k in 1:k_max) {\n    cat(\"Starting step: \", k, \", rho = \", rho, \"\\n\")\n\n    minimize(f, p, x, rho, num_iterations)\n\n    cat(\"Value: \",  as.numeric(f(x)), \"\\n\")\n    cat(\"X: \",  as.matrix(x), \"\\n\")\n    \n    current_penalty <- as.numeric(p(x))\n    cat(\"Penalty: \", current_penalty, \"\\n\")\n    if (current_penalty == 0) break\n    \n    rho <- rho * gamma\n  }\n\n}\n\n\nminimize(), called from penalty_method(), follows the usual proceedings, but now it minimizes the sum of the target and up-weighted penalty function outputs:\n\n\nminimize <- function(f, p, x, rho, num_iterations) {\n\n  calc_loss <- function() {\n    optimizer$zero_grad()\n    value <- f(x) + rho * p(x)\n    value$backward()\n    value\n  }\n\n  for (i in 1:num_iterations) {\n    cat(\"Iteration: \", i, \"\\n\")\n    optimizer$step(calc_loss)\n  }\n\n}\n\n\nThis time, we start from a low-target-loss, but unfeasible value. With yet another change to default L-BFGS (namely, a decrease in tolerance), we see the algorithm exiting successfully after twenty-two iterations, at the point (0.5411692,1.306563).\n\n\nx_star <- torch_tensor(c(0.5, 0.5), requires_grad = TRUE)\n\noptimizer <- optim_lbfgs(x_star, line_search_fn = \"strong_wolfe\", tolerance_change = 1e-20)\n\npenalty_method(flower, penalty, x_star, k_max = 30)\n\n\nStarting step:  1 , rho =  1 \nIteration:  1 \nValue:  0.3469974 \nX:  0.5154735 1.244463 \nPenalty:  0.03444662 \n\nStarting step:  2 , rho =  2 \nIteration:  1 \nValue:  0.3818618 \nX:  0.5288152 1.276674 \nPenalty:  0.008182613 \n\nStarting step:  3 , rho =  4 \nIteration:  1 \nValue:  0.3983252 \nX:  0.5351116 1.291886 \nPenalty:  0.001996888 \n\n...\n...\n\nStarting step:  20 , rho =  524288 \nIteration:  1 \nValue:  0.4142133 \nX:  0.5411959 1.306563 \nPenalty:  3.552714e-13 \n\nStarting step:  21 , rho =  1048576 \nIteration:  1 \nValue:  0.4142134 \nX:  0.5411956 1.306563 \nPenalty:  1.278977e-13 \n\nStarting step:  22 , rho =  2097152 \nIteration:  1 \nValue:  0.4142135 \nX:  0.5411962 1.306563 \nPenalty:  0 \nConclusion\nSumming up, we’ve gotten a first impression of the effectiveness of torch’s L-BFGS optimizer, especially when used with Strong-Wolfe line search. In fact, in numerical optimization – as opposed to deep learning, where computational speed is much more of an issue – there is hardly ever a reason to not use L-BFGS with line search.\nWe’ve then caught a glimpse of how to do constrained optimization, a task that arises in many real-world applications. In that regard, this post feels a lot more like a beginning than a stock-taking. There is a lot to explore, from general method fit – when is L-BFGS well suited to a problem? – via computational efficacy to applicability to different species of neural networks. Needless to say, if this inspires you to run your own experiments, and/or if you use L-BFGS in your own projects, we’d love to hear your feedback!\nThanks for reading!\nAppendix\nRosenbrock function plotting code\n\n\nlibrary(tidyverse)\n\na <- 1\nb <- 5\n\nrosenbrock <- function(x) {\n  x1 <- x[1]\n  x2 <- x[2]\n  (a - x1)^2 + b * (x2 - x1^2)^2\n}\n\ndf <- expand_grid(x1 = seq(-2, 2, by = 0.01), x2 = seq(-2, 2, by = 0.01)) %>%\n  rowwise() %>%\n  mutate(x3 = rosenbrock(c(x1, x2))) %>%\n  ungroup()\n\nggplot(data = df,\n       aes(x = x1,\n           y = x2,\n           z = x3)) +\n  geom_contour_filled(breaks = as.numeric(torch_logspace(-3, 3, steps = 50)),\n                      show.legend = FALSE) +\n  theme_minimal() +\n  scale_fill_viridis_d(direction = -1) +\n  theme(aspect.ratio = 1)\n\n\nFlower function plotting code\n\n\na <- 1\nb <- 1\nc <- 4\n\nflower <- function(x) {\n  a * torch_norm(x) + b * torch_sin(c * torch_atan2(x[2], x[1]))\n}\n\ndf <- expand_grid(x = seq(-3, 3, by = 0.05), y = seq(-3, 3, by = 0.05)) %>%\n  rowwise() %>%\n  mutate(z = flower(torch_tensor(c(x, y))) %>% as.numeric()) %>%\n  ungroup()\n\nggplot(data = df,\n       aes(x = x,\n           y = y,\n           z = z)) +\n  geom_contour_filled(show.legend = FALSE) +\n  theme_minimal() +\n  scale_fill_viridis_d(direction = -1) +\n  theme(aspect.ratio = 1)\n\n\nPhoto by Michael Trimble on Unsplash\n\nThe code used to generate this plot is found in the appendix.↩︎\nThe code to plot it is, again, found in the appendix.↩︎\n",
    "preview": "posts/2021-04-22-torch-for-optimization/images/preview.jpg",
    "last_modified": "2024-11-21T15:49:50+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-25-sparklyr-1.6.0-released/",
    "title": "sparklyr 1.6: weighted quantile summaries, power iteration clustering, spark_write_rds(), and more",
    "description": "The sparklyr 1.6 release introduces weighted quantile summaries, an R interface to power iteration clustering, spark_write_rds(), as well as a number of dplyr-related improvements.",
    "author": [
      {
        "name": "Yitao Li",
        "url": "https://github.com/yitao-li"
      }
    ],
    "date": "2021-03-25",
    "categories": [
      "R",
      "Packages/Releases",
      "Distributed Computing"
    ],
    "contents": "\nSparklyr 1.6 is now available on CRAN!\nTo install sparklyr 1.6 from CRAN, run\n\n\ninstall.packages(\"sparklyr\")\n\n\nIn this blog post, we shall highlight the following features and enhancements\nfrom sparklyr 1.6:\nWeighted quantile summaries\nPower iteration clustering\nspark_write_rds() + collect_from_rds()\nDplyr-related improvements\nWeighted quantile summaries\nApache Spark is well-known for supporting\napproximate algorithms that trade off marginal amounts of accuracy for greater\nspeed and parallelism.\nSuch algorithms are particularly beneficial for performing preliminary data\nexplorations at scale, as they enable users to quickly query certain estimated\nstatistics within a predefined error margin, while avoiding the high cost of\nexact computations.\nOne example is the Greenwald-Khanna algorithm for on-line computation of quantile\nsummaries, as described in Greenwald and Khanna (2001).\nThis algorithm was originally designed for efficient \\(\\epsilon\\)-\napproximation of quantiles within a large dataset without the notion of data\npoints carrying different weights, and the unweighted version of it has been\nimplemented as\napproxQuantile()\nsince Spark 2.0.\nHowever, the same algorithm can be generalized to handle weighted\ninputs, and as sparklyr user @Zhuk66 mentioned\nin this issue, a\nweighted version\nof this algorithm makes for a useful sparklyr feature.\nTo properly explain what weighted-quantile means, we must clarify what the\nweight of each data point signifies. For example, if we have a sequence of\nobservations \\((1, 1, 1, 1, 0, 2, -1, -1)\\), and would like to approximate the\nmedian of all data points, then we have the following two options:\nEither run the unweighted version of approxQuantile() in Spark to scan\nthrough all 8 data points\nOr alternatively, “compress” the data into 4 tuples of (value, weight):\n\\((1, 0.5), (0, 0.125), (2, 0.125), (-1, 0.25)\\), where the second component of\neach tuple represents how often a value occurs relative to the rest of the\nobserved values, and then find the median by scanning through the 4 tuples\nusing the weighted version of the Greenwald-Khanna algorithm\nWe can also run through a contrived example involving the standard normal\ndistribution to illustrate the power of weighted quantile estimation in\nsparklyr 1.6. Suppose we cannot simply run qnorm() in R to evaluate the\nquantile function\nof the standard normal distribution at \\(p = 0.25\\) and \\(p = 0.75\\), how can\nwe get some vague idea about the 1st and 3rd quantiles of this distribution?\nOne way is to sample a large number of data points from this distribution, and\nthen apply the Greenwald-Khanna algorithm to our unweighted samples, as shown\nbelow:\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n\nnum_samples <- 1e6\nsamples <- data.frame(x = rnorm(num_samples))\n\nsamples_sdf <- copy_to(sc, samples, name = random_string())\n\nsamples_sdf %>%\n  sdf_quantile(\n    column = \"x\",\n    probabilities = c(0.25, 0.75),\n    relative.error = 0.01\n  ) %>%\n  print()\n\n\n##        25%        75%\n## -0.6629242  0.6874939\nNotice that because we are working with an approximate algorithm, and have specified\nrelative.error = 0.01, the estimated value of \\(-0.6629242\\) from above\ncould be anywhere between the 24th and the 26th percentile of all samples.\nIn fact, it falls in the \\(25.36896\\)-th percentile:\n\n\npnorm(-0.6629242)\n\n\n## [1] 0.2536896\nNow how can we make use of weighted quantile estimation from sparklyr 1.6 to\nobtain similar results? Simple! We can sample a large number of \\(x\\) values\nuniformly randomly from \\((-\\infty, \\infty)\\) (or alternatively, just select a\nlarge number of values evenly spaced between \\((-M, M)\\) where \\(M\\) is\napproximately \\(\\infty\\)), and assign each \\(x\\) value a weight of\n\\(\\displaystyle \\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{x^2}{2}}\\), the standard normal\ndistribution’s probability density at \\(x\\). Finally, we run the weighted version\nof sdf_quantile() from sparklyr 1.6, as shown below:\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n\nnum_samples <- 1e6\nM <- 1000\nsamples <- tibble::tibble(\n  x = M * seq(-num_samples / 2 + 1, num_samples / 2) / num_samples,\n  weight = dnorm(x)\n)\n\nsamples_sdf <- copy_to(sc, samples, name = random_string())\n\nsamples_sdf %>%\n  sdf_quantile(\n    column = \"x\",\n    weight.column = \"weight\",\n    probabilities = c(0.25, 0.75),\n    relative.error = 0.01\n  ) %>%\n  print()\n\n\n##    25%    75%\n## -0.696  0.662\nVoilà! The estimates are not too far off from the 25th and 75th percentiles (in\nrelation to our abovementioned maximum permissible error of \\(0.01\\)):\n\n\npnorm(-0.696)\n\n\n## [1] 0.2432144\n\n\npnorm(0.662)\n\n\n## [1] 0.7460144\nPower iteration clustering\nPower iteration clustering (PIC), a simple and scalable graph clustering method\npresented in Lin and Cohen (2010), first finds a low-dimensional embedding of a dataset, using\ntruncated power iteration on a normalized pairwise-similarity matrix of all data\npoints, and then uses this embedding as the “cluster indicator,” an intermediate\nrepresentation of the dataset that leads to fast convergence when used as input\nto k-means clustering. This process is very well illustrated in figure 1\nof Lin and Cohen (2010) (reproduced below)\n\n\n\nin which the leftmost image is the visualization of a dataset consisting of 3\ncircles, with points colored in red, green, and blue indicating clustering\nresults, and the subsequent images show the power iteration process gradually\ntransforming the original set of points into what appears to be three disjoint line\nsegments, an intermediate representation that can be rapidly separated into 3\nclusters using k-means clustering with \\(k = 3\\).\nIn sparklyr 1.6, ml_power_iteration() was implemented to make the\nPIC functionality\nin Spark accessible from R. It expects as input a 3-column Spark dataframe that\nrepresents a pairwise-similarity matrix of all data points. Two of\nthe columns in this dataframe should contain 0-based row and column indices, and\nthe third column should hold the corresponding similarity measure.\nIn the example below, we will see a dataset consisting of two circles being\neasily separated into two clusters by ml_power_iteration(), with the Gaussian\nkernel being used as the similarity measure between any 2 points:\n\n\ngen_similarity_matrix <- function() {\n  # Guassian similarity measure\n  guassian_similarity <- function(pt1, pt2) {\n    exp(-sum((pt2 - pt1) ^ 2) / 2)\n  }\n  # generate evenly distributed points on a circle centered at the origin\n  gen_circle <- function(radius, num_pts) {\n    seq(0, num_pts - 1) %>%\n      purrr::map_dfr(\n        function(idx) {\n          theta <- 2 * pi * idx / num_pts\n          radius * c(x = cos(theta), y = sin(theta))\n        })\n  }\n  # generate points on both circles\n  pts <- rbind(\n    gen_circle(radius = 1, num_pts = 80),\n    gen_circle(radius = 4, num_pts = 80)\n  )\n  # populate the pairwise similarity matrix (stored as a 3-column dataframe)\n  similarity_matrix <- data.frame()\n  for (i in seq(2, nrow(pts)))\n    similarity_matrix <- similarity_matrix %>%\n      rbind(seq(i - 1L) %>%\n        purrr::map_dfr(~ list(\n          src = i - 1L, dst = .x - 1L,\n          similarity = guassian_similarity(pts[i,], pts[.x,])\n        ))\n      )\n\n  similarity_matrix\n}\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\nsdf <- copy_to(sc, gen_similarity_matrix())\nclusters <- ml_power_iteration(\n  sdf, k = 2, max_iter = 10, init_mode = \"degree\",\n  src_col = \"src\", dst_col = \"dst\", weight_col = \"similarity\"\n)\n\nclusters %>% print(n = 160)\n\n\n## # A tibble: 160 x 2\n##        id cluster\n##     <dbl>   <int>\n##   1     0       1\n##   2     1       1\n##   3     2       1\n##   4     3       1\n##   5     4       1\n##   ...\n##   157   156       0\n##   158   157       0\n##   159   158       0\n##   160   159       0\nThe output shows points from the two circles being assigned to separate clusters,\nas expected, after only a small number of PIC iterations.\nspark_write_rds() + collect_from_rds()\nspark_write_rds() and collect_from_rds() are implemented as a less memory-\nconsuming alternative to collect(). Unlike collect(), which retrieves all\nelements of a Spark dataframe through the Spark driver node, hence potentially\ncausing slowness or out-of-memory failures when collecting large amounts of data,\nspark_write_rds(), when used in conjunction with collect_from_rds(), can\nretrieve all partitions of a Spark dataframe directly from Spark workers,\nrather than through the Spark driver node.\nFirst, spark_write_rds() will\ndistribute the tasks of serializing Spark dataframe partitions in RDS version\n2 format among Spark workers. Spark workers can then process multiple partitions\nin parallel, each handling one partition at a time and persisting the RDS output\ndirectly to disk, rather than sending dataframe partitions to the Spark driver\nnode. Finally, the RDS outputs can be re-assembled to R dataframes using\ncollect_from_rds().\nShown below is an example of spark_write_rds() + collect_from_rds() usage,\nwhere RDS outputs are first saved to HDFS, then downloaded to the local\nfilesystem with hadoop fs -get, and finally, post-processed with\ncollect_from_rds():\n\n\nlibrary(sparklyr)\nlibrary(nycflights13)\n\nnum_partitions <- 10L\nsc <- spark_connect(master = \"yarn\", spark_home = \"/usr/lib/spark\")\nflights_sdf <- copy_to(sc, flights, repartition = num_partitions)\n\n# Spark workers serialize all partition in RDS format in parallel and write RDS\n# outputs to HDFS\nspark_write_rds(\n  flights_sdf,\n  dest_uri = \"hdfs://<namenode>:8020/flights-part-{partitionId}.rds\"\n)\n\n# Run `hadoop fs -get` to download RDS files from HDFS to local file system\nfor (partition in seq(num_partitions) - 1)\n  system2(\n    \"hadoop\",\n    c(\"fs\", \"-get\", sprintf(\"hdfs://<namenode>:8020/flights-part-%d.rds\", partition))\n  )\n\n# Post-process RDS outputs\npartitions <- seq(num_partitions) - 1 %>%\n  lapply(function(partition) collect_from_rds(sprintf(\"flights-part-%d.rds\", partition)))\n\n# Optionally, call `rbind()` to combine data from all partitions into a single R dataframe\nflights_df <- do.call(rbind, partitions)\n\n\nDplyr-related improvements\nSimilar to other recent sparklyr releases, sparklyr 1.6 comes with a\nnumber of dplyr-related improvements, such as\nSupport for where() predicate within select() and summarize(across(...))\noperations on Spark dataframes\nAddition of if_all() and if_any() functions\nFull compatibility with dbplyr 2.0 backend API\nselect(where(...)) and summarize(across(where(...)))\nThe dplyr where(...) construct is useful for applying a selection or\naggregation function to multiple columns that satisfy some boolean predicate.\nFor example,\n\n\nlibrary(dplyr)\n\niris %>% select(where(is.numeric))\n\n\nreturns all numeric columns from the iris dataset, and\n\n\nlibrary(dplyr)\n\niris %>% summarize(across(where(is.numeric), mean))\n\n\ncomputes the average of each numeric column.\nIn sparklyr 1.6, both types of operations can be applied to Spark dataframes, e.g.,\n\n\nlibrary(dplyr)\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\niris_sdf <- copy_to(sc, iris, name = random_string())\n\niris_sdf %>% select(where(is.numeric))\n\niris %>% summarize(across(where(is.numeric), mean))\n\n\nif_all() and if_any()\nif_all() and if_any() are two convenience functions from dplyr 1.0.4 (see\nhere for more details)\nthat effectively1\ncombine the results of applying a boolean predicate to a tidy selection of columns\nusing the logical and/or operators.\nStarting from sparklyr 1.6, if_all() and if_any() can also be applied to\nSpark dataframes, .e.g.,\n\n\nlibrary(dplyr)\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\niris_sdf <- copy_to(sc, iris, name = random_string())\n\n# Select all records with Petal.Width > 2 and Petal.Length > 2\niris_sdf %>% filter(if_all(starts_with(\"Petal\"), ~ .x > 2))\n\n# Select all records with Petal.Width > 5 or Petal.Length > 5\niris_sdf %>% filter(if_any(starts_with(\"Petal\"), ~ .x > 5))\n\n\nCompatibility with dbplyr 2.0 backend API\nSparklyr 1.6 is fully compatible with the newer dbplyr 2.0 backend API (by\nimplementing all interface changes recommended in\nhere), while still\nmaintaining backward compatibility with the previous edition of dbplyr API, so\nthat sparklyr users will not be forced to switch to any particular version of\ndbplyr.\nThis should be a mostly non-user-visible change as of now. In fact, the only\ndiscernible behavior change will be the following code\n\n\nlibrary(dbplyr)\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n\nprint(dbplyr_edition(sc))\n\n\noutputting\n[1] 2\nif sparklyr is working with dbplyr 2.0+, and\n[1] 1\nif otherwise.\nAcknowledgements\nIn chronological order, we would like to thank the following contributors for\nmaking sparklyr 1.6 awesome:\n@yitao-li\n@pgramme\n@javierluraschi\n@andrew-christianson\n@jozefhajnala\n@nathaneastwood\n@mzorko\nWe would also like to give a big shout-out to the wonderful open-source community\nbehind sparklyr, without whom we would not have benefitted from numerous\nsparklyr-related bug reports and feature suggestions.\nFinally, the author of this blog post also very much appreciates the highly\nvaluable editorial suggestions from @skeydan.\nIf you wish to learn more about sparklyr, we recommend checking out\nsparklyr.ai, spark.rstudio.com,\nand also some previous sparklyr release posts such as\nsparklyr 1.5\nand sparklyr 1.4.\nThat is all. Thanks for reading!\n\n\n\nGreenwald, Michael, and Sanjeev Khanna. 2001. “Space-Efficient Online Computation of Quantile Summaries.” SIGMOD Rec. 30 (2): 58–66. https://doi.org/10.1145/376284.375670.\n\n\nLin, Frank, and William Cohen. 2010. “Power Iteration Clustering.” In, 655–62.\n\n\nmodulo possible implementation-dependent short-circuit evaluations↩︎\n",
    "preview": "posts/2021-03-25-sparklyr-1.6.0-released/images/sparklyr-1.6.jpg",
    "last_modified": "2024-11-21T15:48:12+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-19-forecasting-time-series-with-torch_4/",
    "title": "torch time series, final episode: Attention",
    "description": "We conclude our mini-series on time-series forecasting with torch by augmenting last time's sequence-to-sequence architecture with a technique both immensely popular in natural language processing and inspired by human (and animal) cognition: attention.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2021-03-19",
    "categories": [
      "Torch",
      "R",
      "Time Series"
    ],
    "contents": "\n\nContents\nData input\nModel\nEncoder\nAttention module\nDecoder\nseq2seq module\n\nTraining\nEvaluation\n\nThis is the final post in a four-part introduction to time-series forecasting with torch. These posts have been the story of a quest for multiple-step prediction, and by now, we’ve seen three different approaches: forecasting in a loop, incorporating a multi-layer perceptron (MLP), and sequence-to-sequence models. Here’s a quick recap.\nAs one should when one sets out for an adventurous journey, we started with an in-depth study of the tools at our disposal: recurrent neural networks (RNNs). We trained a model to predict the very next observation in line, and then, thought of a clever hack: How about we use this for multi-step prediction, feeding back individual predictions in a loop? The result , it turned out, was quite acceptable.\nThen, the adventure really started. We built our first model “natively” for multi-step prediction, relieving the RNN a bit of its workload and involving a second player, a tiny-ish MLP. Now, it was the MLP’s task to project RNN output to several time points in the future. Although results were pretty satisfactory, we didn’t stop there.\nInstead, we applied to numerical time series a technique commonly used in natural language processing (NLP): sequence-to-sequence (seq2seq) prediction. While forecast performance was not much different from the previous case, we found the technique to be more intuitively appealing, since it reflects the causal relationship between successive forecasts.\nToday we’ll enrich the seq2seq approach by adding a new component: the attention module. Originally introduced around 20141, attention mechanisms have gained enormous traction, so much so that a recent paper title starts out “Attention is Not All You Need”2.\nThe idea is the following.\nIn the classic encoder-decoder setup, the decoder gets “primed” with an encoder summary just a single time: the time it starts its forecasting loop. From then on, it’s on its own. With attention, however, it gets to see the complete sequence of encoder outputs again every time it forecasts a new value. What’s more, every time, it gets to zoom in on those outputs that seem relevant for the current prediction step.\nThis is a particularly useful strategy in translation: In generating the next word, a model will need to know what part of the source sentence to focus on. How much the technique helps with numerical sequences, in contrast, will likely depend on the features of the series in question.\nData input\nAs before, we work with vic_elec, but this time, we partly deviate from the way we used to employ it. With the original, bi-hourly dataset, training the current model takes a long time, longer than readers will want to wait when experimenting. So instead, we aggregate observations by day. In order to have enough data, we train on years 2012 and 2013, reserving 2014 for validation as well as post-training inspection.\n\n\nlibrary(torch)\nlibrary(tidyverse)\nlibrary(tsibble)\nlibrary(tsibbledata)\nlibrary(lubridate)\nlibrary(fable)\nlibrary(zeallot)\n\nvic_elec_daily <- vic_elec %>%\n  select(Time, Demand) %>%\n  index_by(Date = date(Time)) %>%\n  summarise(\n    Demand = sum(Demand) / 1e3) \n\nelec_train <- vic_elec_daily %>% \n  filter(year(Date) %in% c(2012, 2013)) %>%\n  as_tibble() %>%\n  select(Demand) %>%\n  as.matrix()\n\nelec_valid <- vic_elec_daily %>% \n  filter(year(Date) == 2014) %>%\n  as_tibble() %>%\n  select(Demand) %>%\n  as.matrix()\n\nelec_test <- vic_elec_daily %>% \n  filter(year(Date) %in% c(2014), month(Date) %in% 1:4) %>%\n  as_tibble() %>%\n  select(Demand) %>%\n  as.matrix()\n\ntrain_mean <- mean(elec_train)\ntrain_sd <- sd(elec_train)\n\n\nWe’ll attempt to forecast demand up to fourteen days ahead. How long, then, should be the input sequences? This is a matter of experimentation; all the more so now that we’re adding in the attention mechanism. (I suspect that it might not handle very long sequences so well).\nBelow, we go with fourteen days for input length, too, but that may not necessarily be the best possible choice for this series.\n\n\nn_timesteps <- 7 * 2\nn_forecast <- 7 * 2\n\nelec_dataset <- dataset(\n  name = \"elec_dataset\",\n  \n  initialize = function(x, n_timesteps, sample_frac = 1) {\n    \n    self$n_timesteps <- n_timesteps\n    self$x <- torch_tensor((x - train_mean) / train_sd)\n    \n    n <- length(self$x) - self$n_timesteps - 1\n    \n    self$starts <- sort(sample.int(\n      n = n,\n      size = n * sample_frac\n    ))\n    \n  },\n  \n  .getitem = function(i) {\n    \n    start <- self$starts[i]\n    end <- start + self$n_timesteps - 1\n    lag <- 1\n    \n    list(\n      x = self$x[start:end],\n      y = self$x[(start+lag):(end+lag)]$squeeze(2)\n    )\n    \n  },\n  \n  .length = function() {\n    length(self$starts) \n  }\n)\n\nbatch_size <- 32\n\ntrain_ds <- elec_dataset(elec_train, n_timesteps)\ntrain_dl <- train_ds %>% dataloader(batch_size = batch_size, shuffle = TRUE)\n\nvalid_ds <- elec_dataset(elec_valid, n_timesteps)\nvalid_dl <- valid_ds %>% dataloader(batch_size = batch_size)\n\ntest_ds <- elec_dataset(elec_test, n_timesteps)\ntest_dl <- test_ds %>% dataloader(batch_size = 1)\n\n\nModel\nModel-wise, we again encounter the three modules familiar from the previous post: encoder, decoder, and top-level seq2seq module. However, there is an additional component: the attention module, used by the decoder to obtain attention weights.\nEncoder\nThe encoder still works the same way. It wraps an RNN, and returns the final state.\n\n\nencoder_module <- nn_module(\n  \n  initialize = function(type, input_size, hidden_size, num_layers = 1, dropout = 0) {\n    \n    self$type <- type\n    \n    self$rnn <- if (self$type == \"gru\") {\n      nn_gru(\n        input_size = input_size,\n        hidden_size = hidden_size,\n        num_layers = num_layers,\n        dropout = dropout,\n        batch_first = TRUE\n      )\n    } else {\n      nn_lstm(\n        input_size = input_size,\n        hidden_size = hidden_size,\n        num_layers = num_layers,\n        dropout = dropout,\n        batch_first = TRUE\n      )\n    }\n    \n  },\n  \n  forward = function(x) {\n    \n    # return outputs for all timesteps, as well as last-timestep states for all layers\n    x %>% self$rnn()\n    \n  }\n)\n\n\nAttention module\nIn basic seq2seq, whenever it had to generate a new value, the decoder took into account two things: its prior state, and the previous output generated. In an attention-enriched setup, the decoder additionally receives the complete output from the encoder. In deciding what subset of that output should matter, it gets help from a new agent, the attention module.\nThis, then, is the attention module’s raison d’être: Given current decoder state and well as complete encoder outputs, obtain a weighting of those outputs indicative of how relevant they are to what the decoder is currently up to. This procedure results in the so-called attention weights: a normalized score, for each time step in the encoding, that quantify their respective importance.\nAttention may be implemented in a number of different ways. Here, we show two implementation options, one additive, and one multiplicative.\nAdditive attention\nIn additive attention, encoder outputs and decoder state are commonly either added or concatenated (we choose to do the latter, below). The resulting tensor is run through a linear layer, and a softmax is applied for normalization.\n\n\nattention_module_additive <- nn_module(\n  \n  initialize = function(hidden_dim, attention_size) {\n    \n    self$attention <- nn_linear(2 * hidden_dim, attention_size)\n    \n  },\n  \n  forward = function(state, encoder_outputs) {\n    \n    # function argument shapes\n    # encoder_outputs: (bs, timesteps, hidden_dim)\n    # state: (1, bs, hidden_dim)\n    \n    # multiplex state to allow for concatenation (dimensions 1 and 2 must agree)\n    seq_len <- dim(encoder_outputs)[2]\n    # resulting shape: (bs, timesteps, hidden_dim)\n    state_rep <- state$permute(c(2, 1, 3))$repeat_interleave(seq_len, 2)\n    \n    # concatenate along feature dimension\n    concat <- torch_cat(list(state_rep, encoder_outputs), dim = 3)\n    \n    # run through linear layer with tanh\n    # resulting shape: (bs, timesteps, attention_size)\n    scores <- self$attention(concat) %>% \n      torch_tanh()\n    \n    # sum over attention dimension and normalize\n    # resulting shape: (bs, timesteps) \n    attention_weights <- scores %>%\n      torch_sum(dim = 3) %>%\n      nnf_softmax(dim = 2)\n    \n    # a normalized score for every source token\n    attention_weights\n  }\n)\n\n\nMultiplicative attention\nIn multiplicative attention, scores are obtained by computing dot products between decoder state and all of the encoder outputs. Here too, a softmax is then used for normalization.\n\n\nattention_module_multiplicative <- nn_module(\n  \n  initialize = function() {\n    \n    NULL\n    \n  },\n  \n  forward = function(state, encoder_outputs) {\n    \n    # function argument shapes\n    # encoder_outputs: (bs, timesteps, hidden_dim)\n    # state: (1, bs, hidden_dim)\n\n    # allow for matrix multiplication with encoder_outputs\n    state <- state$permute(c(2, 3, 1))\n \n    # prepare for scaling by number of features\n    d <- torch_tensor(dim(encoder_outputs)[3], dtype = torch_float())\n       \n    # scaled dot products between state and outputs\n    # resulting shape: (bs, timesteps, 1)\n    scores <- torch_bmm(encoder_outputs, state) %>%\n      torch_div(torch_sqrt(d))\n    \n    # normalize\n    # resulting shape: (bs, timesteps) \n    attention_weights <- scores$squeeze(3) %>%\n      nnf_softmax(dim = 2)\n    \n    # a normalized score for every source token\n    attention_weights\n  }\n)\n\n\nDecoder\nOnce attention weights have been computed, their actual application is handled by the decoder. Concretely, the method in question, weighted_encoder_outputs(), computes a product of weights and encoder outputs, making sure that each output will have appropriate impact.\nThe rest of the action then happens in forward(). A concatenation of weighted encoder outputs (often called “context”) and current input is run through an RNN. Then, an ensemble of RNN output, context, and input is passed to an MLP. Finally, both RNN state and current prediction are returned.\n\n\ndecoder_module <- nn_module(\n  \n  initialize = function(type, input_size, hidden_size, attention_type, attention_size = 8, num_layers = 1) {\n    \n    self$type <- type\n    \n    self$rnn <- if (self$type == \"gru\") {\n      nn_gru(\n        input_size = input_size,\n        hidden_size = hidden_size,\n        num_layers = num_layers,\n        batch_first = TRUE\n      )\n    } else {\n      nn_lstm(\n        input_size = input_size,\n        hidden_size = hidden_size,\n        num_layers = num_layers,\n        batch_first = TRUE\n      )\n    }\n    \n    self$linear <- nn_linear(2 * hidden_size + 1, 1)\n    \n    self$attention <- if (attention_type == \"multiplicative\") attention_module_multiplicative()\n      else attention_module_additive(hidden_size, attention_size)\n    \n  },\n  \n  weighted_encoder_outputs = function(state, encoder_outputs) {\n\n    # encoder_outputs is (bs, timesteps, hidden_dim)\n    # state is (1, bs, hidden_dim)\n    # resulting shape: (bs * timesteps)\n    attention_weights <- self$attention(state, encoder_outputs)\n    \n    # resulting shape: (bs, 1, seq_len)\n    attention_weights <- attention_weights$unsqueeze(2)\n    \n    # resulting shape: (bs, 1, hidden_size)\n    weighted_encoder_outputs <- torch_bmm(attention_weights, encoder_outputs)\n    \n    weighted_encoder_outputs\n    \n  },\n  \n  forward = function(x, state, encoder_outputs) {\n \n    # encoder_outputs is (bs, timesteps, hidden_dim)\n    # state is (1, bs, hidden_dim)\n    \n    # resulting shape: (bs, 1, hidden_size)\n    context <- self$weighted_encoder_outputs(state, encoder_outputs)\n    \n    # concatenate input and context\n    # NOTE: this repeating is done to compensate for the absence of an embedding module\n    # that, in NLP, would give x a higher proportion in the concatenation\n    x_rep <- x$repeat_interleave(dim(context)[3], 3) \n    rnn_input <- torch_cat(list(x_rep, context), dim = 3)\n    \n    # resulting shapes: (bs, 1, hidden_size) and (1, bs, hidden_size)\n    rnn_out <- self$rnn(rnn_input, state)\n    rnn_output <- rnn_out[[1]]\n    next_hidden <- rnn_out[[2]]\n    \n    mlp_input <- torch_cat(list(rnn_output$squeeze(2), context$squeeze(2), x$squeeze(2)), dim = 2)\n    \n    output <- self$linear(mlp_input)\n    \n    # shapes: (bs, 1) and (1, bs, hidden_size)\n    list(output, next_hidden)\n  }\n  \n)\n\n\nseq2seq module\nThe seq2seq module is basically unchanged (apart from the fact that now, it allows for attention module configuration). For a detailed explanation of what happens here, please consult the previous post.\n\n\nseq2seq_module <- nn_module(\n  \n  initialize = function(type, input_size, hidden_size, attention_type, attention_size, n_forecast, \n                        num_layers = 1, encoder_dropout = 0) {\n    \n    self$encoder <- encoder_module(type = type, input_size = input_size, hidden_size = hidden_size,\n                                   num_layers, encoder_dropout)\n    self$decoder <- decoder_module(type = type, input_size = 2 * hidden_size, hidden_size = hidden_size,\n                                   attention_type = attention_type, attention_size = attention_size, num_layers)\n    self$n_forecast <- n_forecast\n    \n  },\n  \n  forward = function(x, y, teacher_forcing_ratio) {\n    \n    outputs <- torch_zeros(dim(x)[1], self$n_forecast)\n    encoded <- self$encoder(x)\n    encoder_outputs <- encoded[[1]]\n    hidden <- encoded[[2]]\n    # list of (batch_size, 1), (1, batch_size, hidden_size)\n    out <- self$decoder(x[ , n_timesteps, , drop = FALSE], hidden, encoder_outputs)\n    # (batch_size, 1)\n    pred <- out[[1]]\n    # (1, batch_size, hidden_size)\n    state <- out[[2]]\n    outputs[ , 1] <- pred$squeeze(2)\n    \n    for (t in 2:self$n_forecast) {\n      \n      teacher_forcing <- runif(1) < teacher_forcing_ratio\n      input <- if (teacher_forcing == TRUE) y[ , t - 1, drop = FALSE] else pred\n      input <- input$unsqueeze(3)\n      out <- self$decoder(input, state, encoder_outputs)\n      pred <- out[[1]]\n      state <- out[[2]]\n      outputs[ , t] <- pred$squeeze(2)\n      \n    }\n    \n    outputs\n  }\n  \n)\n\n\nWhen instantiating the top-level model, we now have an additional choice: that between additive and multiplicative attention. In the “accuracy” sense of performance, my tests did not show any differences. However, the multiplicative variant is a lot faster.\n\n\nnet <- seq2seq_module(\"gru\", input_size = 1, hidden_size = 32, attention_type = \"multiplicative\",\n                      attention_size = 8, n_forecast = n_forecast)\n\n\nTraining\nJust like last time, in model training, we get to choose the degree of teacher forcing. Below, we go with a fraction of 0.0, that is, no forcing at all.\n\n\noptimizer <- optim_adam(net$parameters, lr = 0.001)\n\nnum_epochs <- 1000\n\ntrain_batch <- function(b, teacher_forcing_ratio) {\n  \n  optimizer$zero_grad()\n  output <- net(b$x, b$y, teacher_forcing_ratio)\n  target <- b$y\n  \n  loss <- nnf_mse_loss(output, target[ , 1:(dim(output)[2])])\n  loss$backward()\n  optimizer$step()\n  \n  loss$item()\n  \n}\n\nvalid_batch <- function(b, teacher_forcing_ratio = 0) {\n  \n  output <- net(b$x, b$y, teacher_forcing_ratio)\n  target <- b$y\n  \n  loss <- nnf_mse_loss(output, target[ , 1:(dim(output)[2])])\n  \n  loss$item()\n  \n}\n\nfor (epoch in 1:num_epochs) {\n  \n  net$train()\n  train_loss <- c()\n  \n  coro::loop(for (b in train_dl) {\n    loss <-train_batch(b, teacher_forcing_ratio = 0.0)\n    train_loss <- c(train_loss, loss)\n  })\n  \n  cat(sprintf(\"\\nEpoch %d, training: loss: %3.5f \\n\", epoch, mean(train_loss)))\n  \n  net$eval()\n  valid_loss <- c()\n  \n  coro::loop(for (b in valid_dl) {\n    loss <- valid_batch(b)\n    valid_loss <- c(valid_loss, loss)\n  })\n  \n  cat(sprintf(\"\\nEpoch %d, validation: loss: %3.5f \\n\", epoch, mean(valid_loss)))\n}\n\n\n# Epoch 1, training: loss: 0.83752 \n# Epoch 1, validation: loss: 0.83167\n\n# Epoch 2, training: loss: 0.72803 \n# Epoch 2, validation: loss: 0.80804 \n\n# ...\n# ...\n\n# Epoch 99, training: loss: 0.10385 \n# Epoch 99, validation: loss: 0.21259 \n\n# Epoch 100, training: loss: 0.10396 \n# Epoch 100, validation: loss: 0.20975 \nEvaluation\nFor visual inspection, we pick a few forecasts from the test set.\n\n\nnet$eval()\n\ntest_preds <- vector(mode = \"list\", length = length(test_dl))\n\ni <- 1\n\nvic_elec_test <- vic_elec_daily %>%\n  filter(year(Date) == 2014, month(Date) %in% 1:4)\n\n\ncoro::loop(for (b in test_dl) {\n\n  output <- net(b$x, b$y, teacher_forcing_ratio = 0)\n  preds <- as.numeric(output)\n  \n  test_preds[[i]] <- preds\n  i <<- i + 1\n  \n})\n\ntest_pred1 <- test_preds[[1]]\ntest_pred1 <- c(rep(NA, n_timesteps), test_pred1, rep(NA, nrow(vic_elec_test) - n_timesteps - n_forecast))\n\ntest_pred2 <- test_preds[[21]]\ntest_pred2 <- c(rep(NA, n_timesteps + 20), test_pred2, rep(NA, nrow(vic_elec_test) - 20 - n_timesteps - n_forecast))\n\ntest_pred3 <- test_preds[[41]]\ntest_pred3 <- c(rep(NA, n_timesteps + 40), test_pred3, rep(NA, nrow(vic_elec_test) - 40 - n_timesteps - n_forecast))\n\ntest_pred4 <- test_preds[[61]]\ntest_pred4 <- c(rep(NA, n_timesteps + 60), test_pred4, rep(NA, nrow(vic_elec_test) - 60 - n_timesteps - n_forecast))\n\ntest_pred5 <- test_preds[[81]]\ntest_pred5 <- c(rep(NA, n_timesteps + 80), test_pred5, rep(NA, nrow(vic_elec_test) - 80 - n_timesteps - n_forecast))\n\n\npreds_ts <- vic_elec_test %>%\n  select(Demand, Date) %>%\n  add_column(\n    ex_1 = test_pred1 * train_sd + train_mean,\n    ex_2 = test_pred2 * train_sd + train_mean,\n    ex_3 = test_pred3 * train_sd + train_mean,\n    ex_4 = test_pred4 * train_sd + train_mean,\n    ex_5 = test_pred5 * train_sd + train_mean) %>%\n  pivot_longer(-Date) %>%\n  update_tsibble(key = name)\n\n\npreds_ts %>%\n  autoplot() +\n  scale_color_hue(h = c(80, 300), l = 70) +\n  theme_minimal()\n\n\n\n\n\nFigure 1: A sample of two-weeks-ahead predictions for the test set, 2014.\n\n\n\nWe can’t directly compare performance here to that of previous models in our series, as we’ve pragmatically redefined the task. The main goal, however, has been to introduce the concept of attention. Specifically, how to manually implement the technique – something that, once you’ve understood the concept, you may never have to do in practice. Instead, you would likely make use of existing tools that come with torch (multi-head attention and transformer modules), tools we may introduce in a future “season” of this series.\nThanks for reading!\nPhoto by David Clode on Unsplash\n\n\n\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural Machine Translation by Jointly Learning to Align and Translate.” CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473.\n\n\nDong, Yihe, Jean-Baptiste Cordonnier, and Andreas Loukas. 2021. “Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth.” arXiv e-Prints, March, arXiv:2103.03404. https://arxiv.org/abs/2103.03404.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” arXiv e-Prints, June, arXiv:1706.03762. https://arxiv.org/abs/1706.03762.\n\n\nVinyals, Oriol, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey E. Hinton. 2014. “Grammar as a Foreign Language.” CoRR abs/1412.7449. http://arxiv.org/abs/1412.7449.\n\n\nXu, Kelvin, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.” CoRR abs/1502.03044. http://arxiv.org/abs/1502.03044.\n\n\nFor important papers see: Bahdanau, Cho, and Bengio (2014), Vinyals et al. (2014), Xu et al. (2015).↩︎\nDong, Cordonnier, and Loukas (2021), thus replying to Vaswani et al. (2017), whose 2017 paper “went viral” for stating the opposite.↩︎\n",
    "preview": "posts/2021-03-19-forecasting-time-series-with-torch_4/images/preview.jpg",
    "last_modified": "2024-11-21T15:50:38+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-16-forecasting-time-series-with-torch_3/",
    "title": "torch time series, take three: Sequence-to-sequence prediction",
    "description": "In our overview of techniques for time-series forecasting, we move on to sequence-to-sequence models. Architectures in this family are commonly used in natural language processing (NLP) tasks, such as machine translation. With NLP, however, significant pre-processing is required before proceeding to model definition and training. In staying with our familiar numerical series, we can fully concentrate on the concepts.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2021-03-16",
    "categories": [
      "Torch",
      "R",
      "Time Series"
    ],
    "contents": "\n\nContents\nData input\nModel\nEncoder\nDecoder\nseq2seq module\n\nTraining\nEvaluation\n\nToday, we continue our exploration of multi-step time-series forecasting with torch. This post is the third in a series.\nInitially, we covered basics of recurrent neural networks (RNNs), and trained a model to predict the very next value in a sequence. We also found we could forecast quite a few steps ahead by feeding back individual predictions in a loop.\nNext, we built a model “natively” for multi-step prediction. A small multi-layer-perceptron (MLP) was used to project RNN output to several time points in the future.\nOf both approaches, the latter was the more successful. But conceptually, it has an unsatisfying touch to it: When the MLP extrapolates and generates output for, say, ten consecutive points in time, there is no causal relation between those. (Imagine a weather forecast for ten days that never got updated.)\nNow, we’d like to try something more intuitively appealing. The input is a sequence; the output is a sequence. In natural language processing (NLP), this type of task is very common: It’s exactly the kind of situation we see with machine translation or summarization.\nQuite fittingly, the types of models employed to these ends are named sequence-to-sequence models (often abbreviated seq2seq). In a nutshell, they split up the task into two components: an encoding and a decoding part. The former is done just once per input-target pair. The latter is done in a loop, as in our first try. But the decoder has more information at its disposal: At each iteration, its processing is based on the previous prediction as well as previous state. That previous state will be the encoder’s when a loop is started, and its own ever thereafter.\nBefore discussing the model in detail, we need to adapt our data input mechanism.\nData input\nWe continue working with vic_elec , provided by tsibbledata.\nAgain, the dataset definition in the current post looks a bit different from the way it did before; it’s the shape of the target that differs. This time, y equals x, shifted to the left by one.\nThe reason we do this is owed to the way we are going to train the network. With seq2seq, people often use a technique called “teacher forcing” where, instead of feeding back its own prediction into the decoder module, you pass it the value it should have predicted. To be clear, this is done during training only, and to a configurable degree.\n\n\nlibrary(torch)\nlibrary(tidyverse)\nlibrary(tsibble)\nlibrary(tsibbledata)\nlibrary(lubridate)\nlibrary(fable)\nlibrary(zeallot)\n\nn_timesteps <- 7 * 24 * 2\nn_forecast <- n_timesteps\n\nvic_elec_get_year <- function(year, month = NULL) {\n  vic_elec %>%\n    filter(year(Date) == year, month(Date) == if (is.null(month)) month(Date) else month) %>%\n    as_tibble() %>%\n    select(Demand)\n}\n\nelec_train <- vic_elec_get_year(2012) %>% as.matrix()\nelec_valid <- vic_elec_get_year(2013) %>% as.matrix()\nelec_test <- vic_elec_get_year(2014, 1) %>% as.matrix()\n\ntrain_mean <- mean(elec_train)\ntrain_sd <- sd(elec_train)\n\nelec_dataset <- dataset(\n  name = \"elec_dataset\",\n  \n  initialize = function(x, n_timesteps, sample_frac = 1) {\n    \n    self$n_timesteps <- n_timesteps\n    self$x <- torch_tensor((x - train_mean) / train_sd)\n    \n    n <- length(self$x) - self$n_timesteps - 1\n    \n    self$starts <- sort(sample.int(\n      n = n,\n      size = n * sample_frac\n    ))\n    \n  },\n  \n  .getitem = function(i) {\n    \n    start <- self$starts[i]\n    end <- start + self$n_timesteps - 1\n    lag <- 1\n    \n    list(\n      x = self$x[start:end],\n      y = self$x[(start+lag):(end+lag)]$squeeze(2)\n    )\n    \n  },\n  \n  .length = function() {\n    length(self$starts) \n  }\n)\n\n\nDataset as well as dataloader instantations then can proceed as before.\n\n\nbatch_size <- 32\n\ntrain_ds <- elec_dataset(elec_train, n_timesteps, sample_frac = 0.5)\ntrain_dl <- train_ds %>% dataloader(batch_size = batch_size, shuffle = TRUE)\n\nvalid_ds <- elec_dataset(elec_valid, n_timesteps, sample_frac = 0.5)\nvalid_dl <- valid_ds %>% dataloader(batch_size = batch_size)\n\ntest_ds <- elec_dataset(elec_test, n_timesteps)\ntest_dl <- test_ds %>% dataloader(batch_size = 1)\n\n\nModel\nTechnically, the model consists of three modules: the aforementioned encoder and decoder, and the seq2seq module that orchestrates them.\nEncoder\nThe encoder takes its input and runs it through an RNN. Of the two things returned by a recurrent neural network, outputs and state, so far we’ve only been using output. This time, we do the opposite: We throw away the outputs, and only return the state.\nIf the RNN in question is a GRU (and assuming that of the outputs, we take just the final time step, which is what we’ve been doing throughout), there really is no difference: The final state equals the final output. If it’s an LSTM, however, there is a second kind of state, the “cell state”. In that case, returning the state instead of the final output will carry more information.\n\n\nencoder_module <- nn_module(\n  \n  initialize = function(type, input_size, hidden_size, num_layers = 1, dropout = 0) {\n    \n    self$type <- type\n    \n    self$rnn <- if (self$type == \"gru\") {\n      nn_gru(\n        input_size = input_size,\n        hidden_size = hidden_size,\n        num_layers = num_layers,\n        dropout = dropout,\n        batch_first = TRUE\n      )\n    } else {\n      nn_lstm(\n        input_size = input_size,\n        hidden_size = hidden_size,\n        num_layers = num_layers,\n        dropout = dropout,\n        batch_first = TRUE\n      )\n    }\n    \n  },\n  \n  forward = function(x) {\n    \n    x <- self$rnn(x)\n    \n    # return last states for all layers\n    # per layer, a single tensor for GRU, a list of 2 tensors for LSTM\n    x <- x[[2]]\n    x\n    \n  }\n  \n)\n\n\nDecoder\nIn the decoder, just like in the encoder, the main component is an RNN. In contrast to previously-shown architectures, though, it does not just return a prediction. It also reports back the RNN’s final state.\n\n\ndecoder_module <- nn_module(\n  \n  initialize = function(type, input_size, hidden_size, num_layers = 1) {\n    \n    self$type <- type\n    \n    self$rnn <- if (self$type == \"gru\") {\n      nn_gru(\n        input_size = input_size,\n        hidden_size = hidden_size,\n        num_layers = num_layers,\n        batch_first = TRUE\n      )\n    } else {\n      nn_lstm(\n        input_size = input_size,\n        hidden_size = hidden_size,\n        num_layers = num_layers,\n        batch_first = TRUE\n      )\n    }\n    \n    self$linear <- nn_linear(hidden_size, 1)\n    \n  },\n  \n  forward = function(x, state) {\n    \n    # input to forward:\n    # x is (batch_size, 1, 1)\n    # state is (1, batch_size, hidden_size)\n    x <- self$rnn(x, state)\n    \n    # break up RNN return values\n    # output is (batch_size, 1, hidden_size)\n    # next_hidden is\n    c(output, next_hidden) %<-% x\n    \n    output <- output$squeeze(2)\n    output <- self$linear(output)\n    \n    list(output, next_hidden)\n    \n  }\n  \n)\n\n\nseq2seq module\nseq2seq is where the action happens. The plan is to encode once, then call the decoder in a loop.\nIf you look back to decoder forward(), you see that it takes two arguments: x and state.\nDepending on the context, x corresponds to one of three things: final input, preceding prediction, or prior ground truth.\nThe very first time the decoder is called on an input sequence, x maps to the final input value. This is different from a task like machine translation, where you would pass in a start token. With time series, though, we’d like to continue where the actual measurements stop.\nIn further calls, we want the decoder to continue from its most recent prediction. It is only logical, thus, to pass back the preceding forecast.\nThat said, in NLP a technique called “teacher forcing” is commonly used to speed up training. With teacher forcing, instead of the forecast we pass the actual ground truth, the thing the decoder should have predicted. We do that only in a configurable fraction of cases, and – naturally – only while training. The rationale behind this technique is that without this form of re-calibration, consecutive prediction errors can quickly erase any remaining signal.\nstate, too, is polyvalent. But here, there are just two possibilities: encoder state and decoder state.\nThe first time the decoder is called, it is “seeded” with the final state from the encoder. Note how this is the only time we make use of the encoding.\nFrom then on, the decoder’s own previous state will be passed. Remember how it returns two values, forecast and state?\n\n\nseq2seq_module <- nn_module(\n  \n  initialize = function(type, input_size, hidden_size, n_forecast, num_layers = 1, encoder_dropout = 0) {\n    \n    self$encoder <- encoder_module(type = type, input_size = input_size,\n                                   hidden_size = hidden_size, num_layers, encoder_dropout)\n    self$decoder <- decoder_module(type = type, input_size = input_size,\n                                   hidden_size = hidden_size, num_layers)\n    self$n_forecast <- n_forecast\n    \n  },\n  \n  forward = function(x, y, teacher_forcing_ratio) {\n    \n    # prepare empty output\n    outputs <- torch_zeros(dim(x)[1], self$n_forecast)$to(device = device)\n    \n    # encode current input sequence\n    hidden <- self$encoder(x)\n    \n    # prime decoder with final input value and hidden state from the encoder\n    out <- self$decoder(x[ , n_timesteps, , drop = FALSE], hidden)\n    \n    # decompose into predictions and decoder state\n    # pred is (batch_size, 1)\n    # state is (1, batch_size, hidden_size)\n    c(pred, state) %<-% out\n    \n    # store first prediction\n    outputs[ , 1] <- pred$squeeze(2)\n    \n    # iterate to generate remaining forecasts\n    for (t in 2:self$n_forecast) {\n      \n      # call decoder on either ground truth or previous prediction, plus previous decoder state\n      teacher_forcing <- runif(1) < teacher_forcing_ratio\n      input <- if (teacher_forcing == TRUE) y[ , t - 1, drop = FALSE] else pred\n      input <- input$unsqueeze(3)\n      out <- self$decoder(input, state)\n      \n      # again, decompose decoder return values\n      c(pred, state) %<-% out\n      # and store current prediction\n      outputs[ , t] <- pred$squeeze(2)\n    }\n    outputs\n  }\n  \n)\n\nnet <- seq2seq_module(\"gru\", input_size = 1, hidden_size = 32, n_forecast = n_forecast)\n\n# training RNNs on the GPU currently prints a warning that may clutter \n# the console\n# see https://github.com/mlverse/torch/issues/461\n# alternatively, use \n# device <- \"cpu\"\ndevice <- torch_device(if (cuda_is_available()) \"cuda\" else \"cpu\")\n\nnet <- net$to(device = device)\n\n\nTraining\nThe training procedure is mainly unchanged. We do, however, need to decide about teacher_forcing_ratio, the proportion of input sequences we want to perform re-calibration on. In valid_batch(), this should always be 0, while in train_batch(), it’s up to us (or rather, experimentation). Here, we set it to 0.3.\n\n\noptimizer <- optim_adam(net$parameters, lr = 0.001)\n\nnum_epochs <- 50\n\ntrain_batch <- function(b, teacher_forcing_ratio) {\n  \n  optimizer$zero_grad()\n  output <- net(b$x$to(device = device), b$y$to(device = device), teacher_forcing_ratio)\n  target <- b$y$to(device = device)\n  \n  loss <- nnf_mse_loss(output, target)\n  loss$backward()\n  optimizer$step()\n  \n  loss$item()\n  \n}\n\nvalid_batch <- function(b, teacher_forcing_ratio = 0) {\n  \n  output <- net(b$x$to(device = device), b$y$to(device = device), teacher_forcing_ratio)\n  target <- b$y$to(device = device)\n  \n  loss <- nnf_mse_loss(output, target)\n  \n  loss$item()\n  \n}\n\nfor (epoch in 1:num_epochs) {\n  \n  net$train()\n  train_loss <- c()\n  \n  coro::loop(for (b in train_dl) {\n    loss <-train_batch(b, teacher_forcing_ratio = 0.3)\n    train_loss <- c(train_loss, loss)\n  })\n  \n  cat(sprintf(\"\\nEpoch %d, training: loss: %3.5f \\n\", epoch, mean(train_loss)))\n  \n  net$eval()\n  valid_loss <- c()\n  \n  coro::loop(for (b in valid_dl) {\n    loss <- valid_batch(b)\n    valid_loss <- c(valid_loss, loss)\n  })\n  \n  cat(sprintf(\"\\nEpoch %d, validation: loss: %3.5f \\n\", epoch, mean(valid_loss)))\n}\n\n\nEpoch 1, training: loss: 0.37961 \n\nEpoch 1, validation: loss: 1.10699 \n\nEpoch 2, training: loss: 0.19355 \n\nEpoch 2, validation: loss: 1.26462 \n\n# ...\n# ...\n\nEpoch 49, training: loss: 0.03233 \n\nEpoch 49, validation: loss: 0.62286 \n\nEpoch 50, training: loss: 0.03091 \n\nEpoch 50, validation: loss: 0.54457\nIt’s interesting to compare performances for different settings of teacher_forcing_ratio. With a setting of 0.5, training loss decreases a lot more slowly; the opposite is seen with a setting of 0. Validation loss, however, is not affected significantly.\nEvaluation\nThe code to inspect test-set forecasts is unchanged.\n\n\nnet$eval()\n\ntest_preds <- vector(mode = \"list\", length = length(test_dl))\n\ni <- 1\n\ncoro::loop(for (b in test_dl) {\n  \n  output <- net(b$x$to(device = device), b$y$to(device = device), teacher_forcing_ratio = 0)\n  preds <- as.numeric(output)\n  \n  test_preds[[i]] <- preds\n  i <<- i + 1\n  \n})\n\nvic_elec_jan_2014 <- vic_elec %>%\n  filter(year(Date) == 2014, month(Date) == 1)\n\ntest_pred1 <- test_preds[[1]]\ntest_pred1 <- c(rep(NA, n_timesteps), test_pred1, rep(NA, nrow(vic_elec_jan_2014) - n_timesteps - n_forecast))\n\ntest_pred2 <- test_preds[[408]]\ntest_pred2 <- c(rep(NA, n_timesteps + 407), test_pred2, rep(NA, nrow(vic_elec_jan_2014) - 407 - n_timesteps - n_forecast))\n\ntest_pred3 <- test_preds[[817]]\ntest_pred3 <- c(rep(NA, nrow(vic_elec_jan_2014) - n_forecast), test_pred3)\n\n\npreds_ts <- vic_elec_jan_2014 %>%\n  select(Demand) %>%\n  add_column(\n    mlp_ex_1 = test_pred1 * train_sd + train_mean,\n    mlp_ex_2 = test_pred2 * train_sd + train_mean,\n    mlp_ex_3 = test_pred3 * train_sd + train_mean) %>%\n  pivot_longer(-Time) %>%\n  update_tsibble(key = name)\n\n\npreds_ts %>%\n  autoplot() +\n  scale_colour_manual(values = c(\"#08c5d1\", \"#00353f\", \"#ffbf66\", \"#d46f4d\")) +\n  theme_minimal()\n\n\n\n\n\nFigure 1: One-week-ahead predictions for January, 2014.\n\n\n\nComparing this to the forecast obtained from last time’s RNN-MLP combo, we don’t see much of a difference. Is this surprising? To me it is. If asked to speculate about the reason, I would probably say this: In all of the architectures we’ve used so far, the main carrier of information has been the final hidden state of the RNN (one and only RNN in the two previous setups, encoder RNN in this one). It will be interesting to see what happens in the last part of this series, when we augment the encoder-decoder architecture by attention.\nThanks for reading!\nPhoto by Suzuha Kozuki on Unsplash\n\n\n\n",
    "preview": "posts/2021-03-16-forecasting-time-series-with-torch_3/images/preview.jpg",
    "last_modified": "2024-11-21T15:51:23+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-11-forecasting-time-series-with-torch_2/",
    "title": "torch time series continued: A first go at multi-step prediction",
    "description": "We continue our exploration of time-series forecasting with torch, moving on to architectures designed for multi-step prediction. Here, we augment the \"workhorse RNN\" by a multi-layer perceptron (MLP) to extrapolate multiple timesteps into the future.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2021-03-11",
    "categories": [
      "Torch",
      "R",
      "Time Series"
    ],
    "contents": "\n\nContents\nData input\nModel\nTraining\nEvaluation\nDiscussion\nAppendix\n\nWe pick up where the first post in this series left us: confronting the task of multi-step time-series forecasting.\nOur first attempt was a workaround of sorts. The model had been trained to deliver a single prediction, corresponding to the very next point in time. Thus, if we needed a longer forecast, all we could do is use that prediction and feed it back to the model, moving the input sequence by one value (from \\([x_{t-n}, ..., x_t]\\) to \\([x_{t-n-1}, ..., x_{t+1}]\\), say).\nIn contrast, the new model will be designed – and trained – to forecast a configurable number of observations at once. The architecture will still be basic – about as basic as possible, given the task – and thus, can serve as a baseline for later attempts.\nData input\nWe work with the same data as before, vic_elec from tsibbledata.\nCompared to last time though, the dataset class has to change. While, previously, for each batch item the target (y) was a single value, it now is a vector, just like the input, x. And just like n_timesteps was (and still is) used to specify the length of the input sequence, there is now a second parameter, n_forecast, to configure target size.\nIn our example, n_timesteps and n_forecast are set to the same value, but there is no need for this to be the case. You could equally well train on week-long sequences and then forecast developments over a single day, or a month.\nApart from the fact that .getitem() now returns a vector for y as well as x, there is not much to be said about dataset creation. Here is the complete code to set up the data input pipeline:\n\n\nn_timesteps <- 7 * 24 * 2\nn_forecast <- 7 * 24 * 2 \nbatch_size <- 32\n\nvic_elec_get_year <- function(year, month = NULL) {\n  vic_elec %>%\n    filter(year(Date) == year, month(Date) == if (is.null(month)) month(Date) else month) %>%\n    as_tibble() %>%\n    select(Demand)\n}\n\nelec_train <- vic_elec_get_year(2012) %>% as.matrix()\nelec_valid <- vic_elec_get_year(2013) %>% as.matrix()\nelec_test <- vic_elec_get_year(2014, 1) %>% as.matrix()\n\ntrain_mean <- mean(elec_train)\ntrain_sd <- sd(elec_train)\n\nelec_dataset <- dataset(\n  name = \"elec_dataset\",\n  \n  initialize = function(x, n_timesteps, n_forecast, sample_frac = 1) {\n    \n    self$n_timesteps <- n_timesteps\n    self$n_forecast <- n_forecast\n    self$x <- torch_tensor((x - train_mean) / train_sd)\n    \n    n <- length(self$x) - self$n_timesteps - self$n_forecast + 1\n    \n    self$starts <- sort(sample.int(\n      n = n,\n      size = n * sample_frac\n    ))\n    \n  },\n  \n  .getitem = function(i) {\n    \n    start <- self$starts[i]\n    end <- start + self$n_timesteps - 1\n    pred_length <- self$n_forecast\n    \n    list(\n      x = self$x[start:end],\n      y = self$x[(end + 1):(end + pred_length)]$squeeze(2)\n    )\n    \n  },\n  \n  .length = function() {\n    length(self$starts) \n  }\n)\n\ntrain_ds <- elec_dataset(elec_train, n_timesteps, n_forecast, sample_frac = 0.5)\ntrain_dl <- train_ds %>% dataloader(batch_size = batch_size, shuffle = TRUE)\n\nvalid_ds <- elec_dataset(elec_valid, n_timesteps, n_forecast, sample_frac = 0.5)\nvalid_dl <- valid_ds %>% dataloader(batch_size = batch_size)\n\ntest_ds <- elec_dataset(elec_test, n_timesteps, n_forecast)\ntest_dl <- test_ds %>% dataloader(batch_size = 1)\n\n\nModel\nThe model replaces the single linear layer that, in the previous post, had been tasked with outputting the final prediction, with a small network, complete with two linear layers and – optional – dropout.\nIn forward(), we first apply the RNN, and just like in the previous post, we make use of the outputs only; or more specifically, the output corresponding to the final time step. (See that previous post for a detailed discussion of what a torch RNN returns.)\n\n\nmodel <- nn_module(\n  \n  initialize = function(type, input_size, hidden_size, linear_size, output_size,\n                        num_layers = 1, dropout = 0, linear_dropout = 0) {\n    \n    self$type <- type\n    self$num_layers <- num_layers\n    self$linear_dropout <- linear_dropout\n    \n    self$rnn <- if (self$type == \"gru\") {\n      nn_gru(\n        input_size = input_size,\n        hidden_size = hidden_size,\n        num_layers = num_layers,\n        dropout = dropout,\n        batch_first = TRUE\n      )\n    } else {\n      nn_lstm(\n        input_size = input_size,\n        hidden_size = hidden_size,\n        num_layers = num_layers,\n        dropout = dropout,\n        batch_first = TRUE\n      )\n    }\n    \n    self$mlp <- nn_sequential(\n      nn_linear(hidden_size, linear_size),\n      nn_relu(),\n      nn_dropout(linear_dropout),\n      nn_linear(linear_size, output_size)\n    )\n    \n  },\n  \n  forward = function(x) {\n    \n    x <- self$rnn(x)\n    x[[1]][ ,-1, ..] %>% \n      self$mlp()\n    \n  }\n  \n)\n\n\nFor model instantiation, we now have an additional configuration parameter, related to the amount of dropout between the two linear layers.\n\n\nnet <- model(\n  \"gru\", input_size = 1, hidden_size = 32, linear_size = 512, output_size = n_forecast, linear_dropout = 0\n  )\n\n# training RNNs on the GPU currently prints a warning that may clutter \n# the console\n# see https://github.com/mlverse/torch/issues/461\n# alternatively, use \n# device <- \"cpu\"\ndevice <- torch_device(if (cuda_is_available()) \"cuda\" else \"cpu\")\n\nnet <- net$to(device = device)\n\n\nTraining\nThe training procedure is completely unchanged.\n\n\noptimizer <- optim_adam(net$parameters, lr = 0.001)\n\nnum_epochs <- 30\n\ntrain_batch <- function(b) {\n  \n  optimizer$zero_grad()\n  output <- net(b$x$to(device = device))\n  target <- b$y$to(device = device)\n  \n  loss <- nnf_mse_loss(output, target)\n  loss$backward()\n  optimizer$step()\n  \n  loss$item()\n}\n\nvalid_batch <- function(b) {\n  \n  output <- net(b$x$to(device = device))\n  target <- b$y$to(device = device)\n  \n  loss <- nnf_mse_loss(output, target)\n  loss$item()\n  \n}\n\nfor (epoch in 1:num_epochs) {\n  \n  net$train()\n  train_loss <- c()\n  \n  coro::loop(for (b in train_dl) {\n    loss <-train_batch(b)\n    train_loss <- c(train_loss, loss)\n  })\n  \n  cat(sprintf(\"\\nEpoch %d, training: loss: %3.5f \\n\", epoch, mean(train_loss)))\n  \n  net$eval()\n  valid_loss <- c()\n  \n  coro::loop(for (b in valid_dl) {\n    loss <- valid_batch(b)\n    valid_loss <- c(valid_loss, loss)\n  })\n  \n  cat(sprintf(\"\\nEpoch %d, validation: loss: %3.5f \\n\", epoch, mean(valid_loss)))\n}\n\n\n# Epoch 1, training: loss: 0.65737 \n# \n# Epoch 1, validation: loss: 0.54586 \n# \n# Epoch 2, training: loss: 0.43991 \n# \n# Epoch 2, validation: loss: 0.50588 \n# \n# Epoch 3, training: loss: 0.42161 \n# \n# Epoch 3, validation: loss: 0.50031 \n# \n# Epoch 4, training: loss: 0.41718 \n# \n# Epoch 4, validation: loss: 0.48703 \n# \n# Epoch 5, training: loss: 0.39498 \n# \n# Epoch 5, validation: loss: 0.49572 \n# \n# Epoch 6, training: loss: 0.38073 \n# \n# Epoch 6, validation: loss: 0.46813 \n# \n# Epoch 7, training: loss: 0.36472 \n# \n# Epoch 7, validation: loss: 0.44957 \n# \n# Epoch 8, training: loss: 0.35058 \n# \n# Epoch 8, validation: loss: 0.44440 \n# \n# Epoch 9, training: loss: 0.33880 \n# \n# Epoch 9, validation: loss: 0.41995 \n# \n# Epoch 10, training: loss: 0.32545 \n# \n# Epoch 10, validation: loss: 0.42021 \n# \n# Epoch 11, training: loss: 0.31347 \n# \n# Epoch 11, validation: loss: 0.39514 \n# \n# Epoch 12, training: loss: 0.29622 \n# \n# Epoch 12, validation: loss: 0.38146 \n# \n# Epoch 13, training: loss: 0.28006 \n# \n# Epoch 13, validation: loss: 0.37754 \n# \n# Epoch 14, training: loss: 0.27001 \n# \n# Epoch 14, validation: loss: 0.36636 \n# \n# Epoch 15, training: loss: 0.26191 \n# \n# Epoch 15, validation: loss: 0.35338 \n# \n# Epoch 16, training: loss: 0.25533 \n# \n# Epoch 16, validation: loss: 0.35453 \n# \n# Epoch 17, training: loss: 0.25085 \n# \n# Epoch 17, validation: loss: 0.34521 \n# \n# Epoch 18, training: loss: 0.24686 \n# \n# Epoch 18, validation: loss: 0.35094 \n# \n# Epoch 19, training: loss: 0.24159 \n# \n# Epoch 19, validation: loss: 0.33776 \n# \n# Epoch 20, training: loss: 0.23680 \n# \n# Epoch 20, validation: loss: 0.33974 \n# \n# Epoch 21, training: loss: 0.23070 \n# \n# Epoch 21, validation: loss: 0.34069 \n# \n# Epoch 22, training: loss: 0.22761 \n# \n# Epoch 22, validation: loss: 0.33724 \n# \n# Epoch 23, training: loss: 0.22390 \n# \n# Epoch 23, validation: loss: 0.34013 \n# \n# Epoch 24, training: loss: 0.22155 \n# \n# Epoch 24, validation: loss: 0.33460 \n# \n# Epoch 25, training: loss: 0.21820 \n# \n# Epoch 25, validation: loss: 0.33755 \n# \n# Epoch 26, training: loss: 0.22134 \n# \n# Epoch 26, validation: loss: 0.33678 \n# \n# Epoch 27, training: loss: 0.21061 \n# \n# Epoch 27, validation: loss: 0.33108 \n# \n# Epoch 28, training: loss: 0.20496 \n# \n# Epoch 28, validation: loss: 0.32769 \n# \n# Epoch 29, training: loss: 0.20223 \n# \n# Epoch 29, validation: loss: 0.32969 \n# \n# Epoch 30, training: loss: 0.20022 \n# \n# Epoch 30, validation: loss: 0.33331 \nFrom the way loss decreases on the training set, we conclude that, yes, the model is learning something. It probably would continue improving for quite some epochs still. We do, however, see less of an improvement on the validation set.\nNaturally, now we’re curious about test-set predictions. (Remember, for testing we’re choosing the “particularly hard” month of January, 2014 – particularly hard because of a heatwave that resulted in exceptionally high demand.)\nEvaluation\nWith no loop to be coded, evaluation now becomes pretty straightforward:\n\n\nnet$eval()\n\ntest_preds <- vector(mode = \"list\", length = length(test_dl))\n\ni <- 1\n\ncoro::loop(for (b in test_dl) {\n  \n  input <- b$x\n  output <- net(input$to(device = device))\n  preds <- as.numeric(output)\n  \n  test_preds[[i]] <- preds\n  i <<- i + 1\n  \n})\n\nvic_elec_jan_2014 <- vic_elec %>%\n  filter(year(Date) == 2014, month(Date) == 1)\n\ntest_pred1 <- test_preds[[1]]\ntest_pred1 <- c(rep(NA, n_timesteps), test_pred1, rep(NA, nrow(vic_elec_jan_2014) - n_timesteps - n_forecast))\n\ntest_pred2 <- test_preds[[408]]\ntest_pred2 <- c(rep(NA, n_timesteps + 407), test_pred2, rep(NA, nrow(vic_elec_jan_2014) - 407 - n_timesteps - n_forecast))\n\ntest_pred3 <- test_preds[[817]]\ntest_pred3 <- c(rep(NA, nrow(vic_elec_jan_2014) - n_forecast), test_pred3)\n\n\npreds_ts <- vic_elec_jan_2014 %>%\n  select(Demand) %>%\n  add_column(\n    mlp_ex_1 = test_pred1 * train_sd + train_mean,\n    mlp_ex_2 = test_pred2 * train_sd + train_mean,\n    mlp_ex_3 = test_pred3 * train_sd + train_mean) %>%\n  pivot_longer(-Time) %>%\n  update_tsibble(key = name)\n\n\npreds_ts %>%\n  autoplot() +\n  scale_colour_manual(values = c(\"#08c5d1\", \"#00353f\", \"#ffbf66\", \"#d46f4d\")) +\n  theme_minimal()\n\n\n\n\n\nFigure 1: One-week-ahead predictions for January, 2014.\n\n\n\nCompare this to the forecast obtained by feeding back predictions. The demand profiles over the day look a lot more realistic now. How about the phases of extreme demand? Evidently, these are not reflected in the forecast, not any more than in the “loop technique”. In fact, the forecast allows for interesting insights into this model’s personality: Apparently, it really likes fluctuating around the mean – “prime” it with inputs that oscillate around a significantly higher level, and it will quickly shift back to its comfort zone.\nDiscussion\nSeeing how, above, we provided an option to use dropout inside the MLP, you may be wondering if this would help with forecasts on the test set. Turns out it did not, in my experiments. Maybe this is not so strange either: How, absent external cues (temperature), should the network know that high demand is coming up?\nIn our analysis, we can make an additional distinction. With the first week of predictions, what we see is a failure to anticipate something that could not reasonably have been anticipated (two, or two-and-a-half, say, days of exceptionally high demand). In the second, all the network would have had to do was stay at the current, elevated level. It will be interesting to see how this is handled by the architectures we discuss next.\nFinally, an additional idea you may have had is – what if we used temperature as a second input variable? As a matter of fact, training performance indeed improved, but no performance impact was observed on the validation and test sets. Still, you may find the code useful – it is easily extended to datasets with more predictors. Therefore, we reproduce it in the appendix.\nThanks for reading!\nAppendix\n\n\n# Data input code modified to accommodate two predictors\n\nn_timesteps <- 7 * 24 * 2\nn_forecast <- 7 * 24 * 2\n\nvic_elec_get_year <- function(year, month = NULL) {\n  vic_elec %>%\n    filter(year(Date) == year, month(Date) == if (is.null(month)) month(Date) else month) %>%\n    as_tibble() %>%\n    select(Demand, Temperature)\n}\n\nelec_train <- vic_elec_get_year(2012) %>% as.matrix()\nelec_valid <- vic_elec_get_year(2013) %>% as.matrix()\nelec_test <- vic_elec_get_year(2014, 1) %>% as.matrix()\n\ntrain_mean_demand <- mean(elec_train[ , 1])\ntrain_sd_demand <- sd(elec_train[ , 1])\n\ntrain_mean_temp <- mean(elec_train[ , 2])\ntrain_sd_temp <- sd(elec_train[ , 2])\n\nelec_dataset <- dataset(\n  name = \"elec_dataset\",\n  \n  initialize = function(data, n_timesteps, n_forecast, sample_frac = 1) {\n    \n    demand <- (data[ , 1] - train_mean_demand) / train_sd_demand\n    temp <- (data[ , 2] - train_mean_temp) / train_sd_temp\n    self$x <- cbind(demand, temp) %>% torch_tensor()\n    \n    self$n_timesteps <- n_timesteps\n    self$n_forecast <- n_forecast\n    \n    n <- nrow(self$x) - self$n_timesteps - self$n_forecast + 1\n    self$starts <- sort(sample.int(\n      n = n,\n      size = n * sample_frac\n    ))\n    \n  },\n  \n  .getitem = function(i) {\n    \n    start <- self$starts[i]\n    end <- start + self$n_timesteps - 1\n    pred_length <- self$n_forecast\n    \n    list(\n      x = self$x[start:end, ],\n      y = self$x[(end + 1):(end + pred_length), 1]\n    )\n    \n  },\n  \n  .length = function() {\n    length(self$starts)\n  }\n  \n)\n\n### rest identical to single-predictor code above\n\n\nPhoto by Monica Bourgeau on Unsplash\n\n\n\n",
    "preview": "posts/2021-03-11-forecasting-time-series-with-torch_2/images/preview.jpg",
    "last_modified": "2024-11-21T15:50:02+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-10-forecasting-time-series-with-torch_1/",
    "title": "Introductory time-series forecasting with torch",
    "description": "This post is an introduction to time-series forecasting with torch. Central topics are data input, and practical usage of RNNs (GRUs/LSTMs). Upcoming posts will build on this, and introduce increasingly involved architectures.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2021-03-10",
    "categories": [
      "Torch",
      "R",
      "Time Series"
    ],
    "contents": "\n\nContents\nTime-series inspection\nData input\nModel\nTraining\nEvaluation\nConclusion\n\nThis is the first post in a series introducing time-series forecasting with torch. It does assume some prior experience with torch and/or deep learning. But as far as time series are concerned, it starts right from the beginning, using recurrent neural networks (GRU or LSTM) to predict how something develops in time.\nIn this post, we build a network that uses a sequence of observations to predict a value for the very next point in time. What if we’d like to forecast a sequence of values, corresponding to, say, a week or a month of measurements?\nOne thing we could do is feed back into the system the previously forecasted value; this is something we’ll try at the end of this post. Subsequent posts will explore other options, some of them involving significantly more complex architectures. It will be interesting to compare their performances; but the essential goal is to introduce some torch “recipes” that you can apply to your own data.\nWe start by examining the dataset used. It is a low-dimensional, but pretty polyvalent and complex one.\nTime-series inspection\nThe vic_elec dataset, available through package tsibbledata, provides three years of half-hourly electricity demand for Victoria, Australia, augmented by same-resolution temperature information and a daily holiday indicator.\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\nlibrary(tsibble) # Tidy Temporal Data Frames and Tools\nlibrary(feasts) # Feature Extraction and Statistics for Time Series\nlibrary(tsibbledata) # Diverse Datasets for 'tsibble'\n\nvic_elec %>% glimpse()\n\n\nRows: 52,608\nColumns: 5\n$ Time        <dttm> 2012-01-01 00:00:00, 2012-01-01 00:30:00, 2012-01-01 01:00:00,…\n$ Demand      <dbl> 4382.825, 4263.366, 4048.966, 3877.563, 4036.230, 3865.597, 369…\n$ Temperature <dbl> 21.40, 21.05, 20.70, 20.55, 20.40, 20.25, 20.10, 19.60, 19.10, …\n$ Date        <date> 2012-01-01, 2012-01-01, 2012-01-01, 2012-01-01, 2012-01-01, 20…\n$ Holiday     <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRU…\nDepending on what subset of variables is used, and whether and how data is temporally aggregated, these data may serve to illustrate a variety of different techniques. For example, in the third edition of Forecasting: Principles and Practice daily averages are used to teach quadratic regression with ARMA errors. In this first introductory post though, as well as in most of its successors, we’ll attempt to forecast Demand without relying on additional information, and we keep the original resolution.\nTo get an impression of how electricity demand varies over different timescales. Let’s inspect data for two months that nicely illustrate the U-shaped relationship between temperature and demand: January, 2014 and July, 2014.\nFirst, here is July.\n\n\nvic_elec_2014 <-  vic_elec %>%\n  filter(year(Date) == 2014) %>%\n  select(-c(Date, Holiday)) %>%\n  mutate(Demand = scale(Demand), Temperature = scale(Temperature)) %>%\n  pivot_longer(-Time, names_to = \"variable\") %>%\n  update_tsibble(key = variable)\n\nvic_elec_2014 %>% filter(month(Time) == 7) %>% \n  autoplot() + \n  scale_colour_manual(values = c(\"#08c5d1\", \"#00353f\")) +\n  theme_minimal()\n\n\n\n\n\nFigure 1: Temperature and electricity demand (normalized). Victoria, Australia, 07/2014.\n\n\n\nIt’s winter; temperature fluctuates below average, while electricity demand is above average (heating). There is strong variation over the course of the day; we see troughs in the demand curve corresponding to ridges in the temperature graph, and vice versa. While diurnal variation dominates, there also is variation over the days of the week. Between weeks though, we don’t see much difference.\nCompare this with the data for January:\n\n\nvic_elec_2014 %>% filter(month(Time) == 1) %>% \n  autoplot() + \n  scale_colour_manual(values = c(\"#08c5d1\", \"#00353f\")) +\n  theme_minimal()\n\n\n\n\n\nFigure 2: Temperature and electricity demand (normalized). Victoria, Australia, 01/2014.\n\n\n\nWe still see the strong circadian variation. We still see some day-of-week variation. But now it is high temperatures that cause elevated demand (cooling). Also, there are two periods of unusually high temperatures, accompanied by exceptional demand. We anticipate that in a univariate forecast, not taking into account temperature, this will be hard – or even, impossible – to forecast.\nLet’s see a concise portrait of how Demand behaves using feasts::STL(). First, here is the decomposition for July:\n\n\nvic_elec_2014 <-  vic_elec %>%\n  filter(year(Date) == 2014) %>%\n  select(-c(Date, Holiday))\n\ncmp <- vic_elec_2014 %>% filter(month(Time) == 7) %>%\n  model(STL(Demand)) %>% \n  components()\n\ncmp %>% autoplot()\n\n\n\n\n\nFigure 3: STL decomposition of electricity demand. Victoria, Australia, 07/2014.\n\n\n\nAnd here, for January:\n\n\n\nFigure 4: STL decomposition of electricity demand. Victoria, Australia, 01/2014.\n\n\n\nBoth nicely illustrate the strong circadian and weekly seasonalities (with diurnal variation substantially stronger in January). If we look closely, we can even see how the trend component is more influential in January than in July. This again hints at much stronger difficulties predicting the January than the July developments.\nNow that we have an idea what awaits us, let’s begin by creating a torch dataset.\nData input\nHere is what we intend to do. We want to start our journey into forecasting by using a sequence of observations to predict their immediate successor. In other words, the input (x) for each batch item is a vector, while the target (y) is a single value. The length of the input sequence, x, is parameterized as n_timesteps, the number of consecutive observations to extrapolate from.\nThe dataset will reflect this in its .getitem() method. When asked for the observations at index i, it will return tensors like so:\n\n\nlist(\n      x = self$x[start:end],\n      y = self$x[end+1]\n)\n\n\nwhere start:end is a vector of indices, of length n_timesteps, and end+1 is a single index.\nNow, if the dataset just iterated over its input in order, advancing the index one at a time, these lines could simply read\n\n\nlist(\n      x = self$x[i:(i + self$n_timesteps - 1)],\n      y = self$x[self$n_timesteps + i]\n)\n\n\nSince many sequences in the data are similar, we can reduce training time by making use of a fraction of the data in every epoch. This can be accomplished by (optionally) passing a sample_frac smaller than 1. In initialize(), a random set of start indices is prepared; .getitem() then just does what it normally does: look for the (x,y) pair at a given index.\nHere is the complete dataset code:\n\n\nelec_dataset <- dataset(\n  name = \"elec_dataset\",\n  \n  initialize = function(x, n_timesteps, sample_frac = 1) {\n\n    self$n_timesteps <- n_timesteps\n    self$x <- torch_tensor((x - train_mean) / train_sd)\n    \n    n <- length(self$x) - self$n_timesteps \n    \n    self$starts <- sort(sample.int(\n      n = n,\n      size = n * sample_frac\n    ))\n\n  },\n  \n  .getitem = function(i) {\n    \n    start <- self$starts[i]\n    end <- start + self$n_timesteps - 1\n    \n    list(\n      x = self$x[start:end],\n      y = self$x[end + 1]\n    )\n\n  },\n  \n  .length = function() {\n    length(self$starts) \n  }\n)\n\n\nYou may have noticed that we normalize the data by globally defined train_mean and train_sd. We yet have to calculate those.\nThe way we split the data is straightforward. We use the whole of 2012 for training, and all of 2013 for validation. For testing, we take the “difficult” month of January, 2014. You are invited to compare testing results for July that same year, and compare performances.\n\n\nvic_elec_get_year <- function(year, month = NULL) {\n  vic_elec %>%\n    filter(year(Date) == year, month(Date) == if (is.null(month)) month(Date) else month) %>%\n    as_tibble() %>%\n    select(Demand)\n}\n\nelec_train <- vic_elec_get_year(2012) %>% as.matrix()\nelec_valid <- vic_elec_get_year(2013) %>% as.matrix()\nelec_test <- vic_elec_get_year(2014, 1) %>% as.matrix() # or 2014, 7, alternatively\n\ntrain_mean <- mean(elec_train)\ntrain_sd <- sd(elec_train)\n\n\nNow, to instantiate a dataset, we still need to pick sequence length. From prior inspection, a week seems like a sensible choice.\n\n\nn_timesteps <- 7 * 24 * 2 # days * hours * half-hours\n\n\nNow we can go ahead and create a dataset for the training data. Let’s say we’ll make use of 50% of the data in each epoch:\n\n\ntrain_ds <- elec_dataset(elec_train, n_timesteps, sample_frac = 0.5)\nlength(train_ds)\n\n\n 8615\nQuick check: Are the shapes correct?\n\n\ntrain_ds[1]\n\n\n$x\ntorch_tensor\n-0.4141\n-0.5541\n[...]       ### lines removed by me\n 0.8204\n 0.9399\n... [the output was truncated (use n=-1 to disable)]\n[ CPUFloatType{336,1} ]\n\n$y\ntorch_tensor\n-0.6771\n[ CPUFloatType{1} ]\nYes: This is what we wanted to see. The input sequence has n_timesteps values in the first dimension, and a single one in the second, corresponding to the only feature present, Demand. As intended, the prediction tensor holds a single value, corresponding– as we know – to n_timesteps+1.\nThat takes care of a single input-output pair. As usual, batching is arranged for by torch’s dataloader class. We instantiate one for the training data, and immediately again verify the outcome:\n\n\nbatch_size <- 32\ntrain_dl <- train_ds %>% dataloader(batch_size = batch_size, shuffle = TRUE)\nlength(train_dl)\n\nb <- train_dl %>% dataloader_make_iter() %>% dataloader_next()\nb\n\n\n$x\ntorch_tensor\n(1,.,.) = \n  0.4805\n  0.3125\n[...]       ### lines removed by me\n -1.1756\n -0.9981\n... [the output was truncated (use n=-1 to disable)]\n[ CPUFloatType{32,336,1} ]\n\n$y\ntorch_tensor\n 0.1890\n 0.5405\n[...]       ### lines removed by me\n 2.4015\n 0.7891\n... [the output was truncated (use n=-1 to disable)]\n[ CPUFloatType{32,1} ]\nWe see the added batch dimension in front, resulting in overall shape (batch_size, n_timesteps, num_features). This is the format expected by the model, or more precisely, by its initial RNN layer.\nBefore we go on, let’s quickly create datasets and dataloaders for validation and test data, as well.\n\n\nvalid_ds <- elec_dataset(elec_valid, n_timesteps, sample_frac = 0.5)\nvalid_dl <- valid_ds %>% dataloader(batch_size = batch_size)\n\ntest_ds <- elec_dataset(elec_test, n_timesteps)\ntest_dl <- test_ds %>% dataloader(batch_size = 1)\n\n\nModel\nThe model consists of an RNN – of type GRU or LSTM, as per the user’s choice – and an output layer. The RNN does most of the work; the single-neuron linear layer that outputs the prediction compresses its vector input to a single value.\nHere, first, is the model definition.\n\n\nmodel <- nn_module(\n  \n  initialize = function(type, input_size, hidden_size, num_layers = 1, dropout = 0) {\n    \n    self$type <- type\n    self$num_layers <- num_layers\n    \n    self$rnn <- if (self$type == \"gru\") {\n      nn_gru(\n        input_size = input_size,\n        hidden_size = hidden_size,\n        num_layers = num_layers,\n        dropout = dropout,\n        batch_first = TRUE\n      )\n    } else {\n      nn_lstm(\n        input_size = input_size,\n        hidden_size = hidden_size,\n        num_layers = num_layers,\n        dropout = dropout,\n        batch_first = TRUE\n      )\n    }\n    \n    self$output <- nn_linear(hidden_size, 1)\n    \n  },\n  \n  forward = function(x) {\n    \n    # list of [output, hidden]\n    # we use the output, which is of size (batch_size, n_timesteps, hidden_size)\n    x <- self$rnn(x)[[1]]\n    \n    # from the output, we only want the final timestep\n    # shape now is (batch_size, hidden_size)\n    x <- x[ , dim(x)[2], ]\n    \n    # feed this to a single output neuron\n    # final shape then is (batch_size, 1)\n    x %>% self$output() \n  }\n  \n)\n\n\nMost importantly, this is what happens in forward().\nThe RNN returns a list. The list holds two tensors, an output, and a synopsis of hidden states. We discard the state tensor, and keep the output only. The distinction between state and output, or rather, the way it is reflected in what a torch RNN returns, deserves to be inspected more closely. We’ll do that in a second.\n\n\nx <- self$rnn(x)[[1]]\n\n\nOf the output tensor, we’re interested in only the final time-step, though.\n\n\nx <- x[ , dim(x)[2], ]\n\n\nOnly this one, thus, is passed to the output layer.\n\n\nx %>% self$output()\n\n\nFinally, the said output layer’s output is returned.\nNow, a bit more on states vs. outputs. Consider Fig. 1, from Goodfellow, Bengio, and Courville (2016).\n\n\n\nFigure 5: Source: Goodfellow et al., Deep learning. Chapter URL: https://www.deeplearningbook.org/contents/rnn.html.\n\n\n\nLet’s pretend there are three time steps only, corresponding to \\(t-1\\), \\(t\\), and \\(t+1\\). The input sequence, accordingly, is composed of \\(x_{t-1}\\), \\(x_{t}\\), and \\(x_{t+1}\\).\nAt each \\(t\\), a hidden state is generated, and so is an output. Normally, if our goal is to predict \\(y_{t+2}\\), that is, the very next observation, we want to take into account the complete input sequence. Put differently, we want to have run through the complete machinery of state updates. The logical thing to do would thus be to choose \\(o_{t+1}\\), for either direct return from forward() or for further processing.\nIndeed, return \\(o_{t+1}\\) is what a Keras LSTM or GRU would do by default.1 Not so its torch counterparts. In torch, the output tensor comprises all of \\(o\\). This is why, in step two above, we select the single time step we’re interested in – namely, the last one.\nIn later posts, we will make use of more than the last time step. Sometimes, we’ll use the sequence of hidden states (the \\(h\\)s) instead of the outputs (the \\(o\\)s). So you may feel like asking, what if we used \\(h_{t+1}\\) here instead of \\(o_{t+1}\\)? The answer is: With a GRU, this would not make a difference, as those two are identical. With LSTM though, it would, as LSTM keeps a second, namely, the “cell,” state2.\nOn to initialize(). For ease of experimentation, we instantiate either a GRU or an LSTM based on user input. Two things are worth noting:\nWe pass batch_first = TRUE when creating the RNNs. This is required with torch RNNs when we want to consistently have batch items stacked in the first dimension. And we do want that; it is arguably less confusing than a change of dimension semantics for one sub-type of module.\nnum_layers can be used to build a stacked RNN, corresponding to what you’d get in Keras when chaining two GRUs/LSTMs (the first one created with return_sequences = TRUE). This parameter, too, we’ve included for quick experimentation.\nLet’s instantiate a model for training. It will be a single-layer GRU with thirty-two units.\n\n\n# training RNNs on the GPU currently prints a warning that may clutter \n# the console\n# see https://github.com/mlverse/torch/issues/461\n# alternatively, use \n# device <- \"cpu\"\ndevice <- torch_device(if (cuda_is_available()) \"cuda\" else \"cpu\")\n\nnet <- model(\"gru\", 1, 32)\nnet <- net$to(device = device)\n\n\nTraining\nAfter all those RNN specifics, the training process is completely standard.\n\n\noptimizer <- optim_adam(net$parameters, lr = 0.001)\n\nnum_epochs <- 30\n\ntrain_batch <- function(b) {\n  \n  optimizer$zero_grad()\n  output <- net(b$x$to(device = device))\n  target <- b$y$to(device = device)\n  \n  loss <- nnf_mse_loss(output, target)\n  loss$backward()\n  optimizer$step()\n  \n  loss$item()\n}\n\nvalid_batch <- function(b) {\n  \n  output <- net(b$x$to(device = device))\n  target <- b$y$to(device = device)\n  \n  loss <- nnf_mse_loss(output, target)\n  loss$item()\n  \n}\n\nfor (epoch in 1:num_epochs) {\n  \n  net$train()\n  train_loss <- c()\n  \n  coro::loop(for (b in train_dl) {\n    loss <-train_batch(b)\n    train_loss <- c(train_loss, loss)\n  })\n  \n  cat(sprintf(\"\\nEpoch %d, training: loss: %3.5f \\n\", epoch, mean(train_loss)))\n  \n  net$eval()\n  valid_loss <- c()\n  \n  coro::loop(for (b in valid_dl) {\n    loss <- valid_batch(b)\n    valid_loss <- c(valid_loss, loss)\n  })\n  \n  cat(sprintf(\"\\nEpoch %d, validation: loss: %3.5f \\n\", epoch, mean(valid_loss)))\n}\n\n\nEpoch 1, training: loss: 0.21908 \n\nEpoch 1, validation: loss: 0.05125 \n\nEpoch 2, training: loss: 0.03245 \n\nEpoch 2, validation: loss: 0.03391 \n\nEpoch 3, training: loss: 0.02346 \n\nEpoch 3, validation: loss: 0.02321 \n\nEpoch 4, training: loss: 0.01823 \n\nEpoch 4, validation: loss: 0.01838 \n\nEpoch 5, training: loss: 0.01522 \n\nEpoch 5, validation: loss: 0.01560 \n\nEpoch 6, training: loss: 0.01315 \n\nEpoch 6, validation: loss: 0.01374 \n\nEpoch 7, training: loss: 0.01205 \n\nEpoch 7, validation: loss: 0.01200 \n\nEpoch 8, training: loss: 0.01155 \n\nEpoch 8, validation: loss: 0.01157 \n\nEpoch 9, training: loss: 0.01118 \n\nEpoch 9, validation: loss: 0.01096 \n\nEpoch 10, training: loss: 0.01070 \n\nEpoch 10, validation: loss: 0.01132 \n\nEpoch 11, training: loss: 0.01003 \n\nEpoch 11, validation: loss: 0.01150 \n\nEpoch 12, training: loss: 0.00943 \n\nEpoch 12, validation: loss: 0.01106 \n\nEpoch 13, training: loss: 0.00922 \n\nEpoch 13, validation: loss: 0.01069 \n\nEpoch 14, training: loss: 0.00862 \n\nEpoch 14, validation: loss: 0.01125 \n\nEpoch 15, training: loss: 0.00842 \n\nEpoch 15, validation: loss: 0.01095 \n\nEpoch 16, training: loss: 0.00820 \n\nEpoch 16, validation: loss: 0.00975 \n\nEpoch 17, training: loss: 0.00802 \n\nEpoch 17, validation: loss: 0.01120 \n\nEpoch 18, training: loss: 0.00781 \n\nEpoch 18, validation: loss: 0.00990 \n\nEpoch 19, training: loss: 0.00757 \n\nEpoch 19, validation: loss: 0.01017 \n\nEpoch 20, training: loss: 0.00735 \n\nEpoch 20, validation: loss: 0.00932 \n\nEpoch 21, training: loss: 0.00723 \n\nEpoch 21, validation: loss: 0.00901 \n\nEpoch 22, training: loss: 0.00708 \n\nEpoch 22, validation: loss: 0.00890 \n\nEpoch 23, training: loss: 0.00676 \n\nEpoch 23, validation: loss: 0.00914 \n\nEpoch 24, training: loss: 0.00666 \n\nEpoch 24, validation: loss: 0.00922 \n\nEpoch 25, training: loss: 0.00644 \n\nEpoch 25, validation: loss: 0.00869 \n\nEpoch 26, training: loss: 0.00620 \n\nEpoch 26, validation: loss: 0.00902 \n\nEpoch 27, training: loss: 0.00588 \n\nEpoch 27, validation: loss: 0.00896 \n\nEpoch 28, training: loss: 0.00563 \n\nEpoch 28, validation: loss: 0.00886 \n\nEpoch 29, training: loss: 0.00547 \n\nEpoch 29, validation: loss: 0.00895 \n\nEpoch 30, training: loss: 0.00523 \n\nEpoch 30, validation: loss: 0.00935 \nLoss decreases quickly, and we don’t seem to be overfitting on the validation set.\nNumbers are pretty abstract, though. So, we’ll use the test set to see how the forecast actually looks.\nEvaluation\nHere is the forecast for January, 2014, thirty minutes at a time.\n\n\nnet$eval()\n\npreds <- rep(NA, n_timesteps)\n\ncoro::loop(for (b in test_dl) {\n  output <- net(b$x$to(device = device))\n  preds <- c(preds, output %>% as.numeric())\n})\n\nvic_elec_jan_2014 <-  vic_elec %>%\n  filter(year(Date) == 2014, month(Date) == 1) %>%\n  select(Demand)\n\npreds_ts <- vic_elec_jan_2014 %>%\n  add_column(forecast = preds * train_sd + train_mean) %>%\n  pivot_longer(-Time) %>%\n  update_tsibble(key = name)\n\npreds_ts %>%\n  autoplot() +\n  scale_colour_manual(values = c(\"#08c5d1\", \"#00353f\")) +\n  theme_minimal()\n\n\n\n\n\nFigure 6: One-step-ahead predictions for January, 2014.\n\n\n\nOverall, the forecast is excellent, but it is interesting to see how the forecast “regularizes” the most extreme peaks. This kind of “regression to the mean” will be seen much more strongly in later setups, when we try to forecast further into the future.\nCan we use our current architecture for multi-step prediction? We can.\nOne thing we can do is feed back the current prediction, that is, append it to the input sequence as soon as it is available. Effectively thus, for each batch item, we obtain a sequence of predictions in a loop.\nWe’ll try to forecast 336 time steps, that is, a complete week.\n\n\nn_forecast <- 2 * 24 * 7\n\ntest_preds <- vector(mode = \"list\", length = length(test_dl))\n\ni <- 1\n\ncoro::loop(for (b in test_dl) {\n  \n  input <- b$x\n  output <- net(input$to(device = device))\n  preds <- as.numeric(output)\n  \n  for(j in 2:n_forecast) {\n    input <- torch_cat(list(input[ , 2:length(input), ], output$view(c(1, 1, 1))), dim = 2)\n    output <- net(input$to(device = device))\n    preds <- c(preds, as.numeric(output))\n  }\n  \n  test_preds[[i]] <- preds\n  i <<- i + 1\n  \n})\n\n\nFor visualization, let’s pick three non-overlapping sequences.\n\n\ntest_pred1 <- test_preds[[1]]\ntest_pred1 <- c(rep(NA, n_timesteps), test_pred1, rep(NA, nrow(vic_elec_jan_2014) - n_timesteps - n_forecast))\n\ntest_pred2 <- test_preds[[408]]\ntest_pred2 <- c(rep(NA, n_timesteps + 407), test_pred2, rep(NA, nrow(vic_elec_jan_2014) - 407 - n_timesteps - n_forecast))\n\ntest_pred3 <- test_preds[[817]]\ntest_pred3 <- c(rep(NA, nrow(vic_elec_jan_2014) - n_forecast), test_pred3)\n\n\npreds_ts <- vic_elec %>%\n  filter(year(Date) == 2014, month(Date) == 1) %>%\n  select(Demand) %>%\n  add_column(\n    iterative_ex_1 = test_pred1 * train_sd + train_mean,\n    iterative_ex_2 = test_pred2 * train_sd + train_mean,\n    iterative_ex_3 = test_pred3 * train_sd + train_mean) %>%\n  pivot_longer(-Time) %>%\n  update_tsibble(key = name)\n\npreds_ts %>%\n  autoplot() +\n  scale_colour_manual(values = c(\"#08c5d1\", \"#00353f\", \"#ffbf66\", \"#d46f4d\")) +\n  theme_minimal()\n\n\n\n\n\nFigure 7: Multi-step predictions for January, 2014, obtained in a loop.\n\n\n\nEven with this very basic forecasting technique, the diurnal rhythm is preserved, albeit in a strongly smoothed form. There even is an apparent day-of-week periodicity in the forecast. We do see, however, very strong regression to the mean, even in loop instances where the network was “primed” with a higher input sequence.\nConclusion\nHopefully this post provided a useful introduction to time series forecasting with torch. Evidently, we picked a challenging time series – challenging, that is, for at least two reasons:\nTo correctly factor in the trend, external information is needed: external information in form of a temperature forecast, which, “in reality,” would be easily obtainable.\nIn addition to the highly important trend component, the data are characterized by multiple levels of seasonality.\nOf these, the latter is less of a problem for the techniques we’re working with here. If we found that some level of seasonality went undetected, we could try to adapt the current configuration in a number of uncomplicated ways:\nUse an LSTM instead of a GRU. In theory, LSTM should better be able to capture additional lower-frequency components due to its secondary storage, the cell state.\nStack multiple layers of GRU/LSTM. In theory, this should allow for learning a hierarchy of temporal features, analogously to what we see in a convolutional neural network.\nTo address the former obstacle, bigger changes to the architecture would be needed. We may attempt to do that in a later, “bonus,” post. But in the upcoming installments, we’ll first dive into often-used techniques for sequence prediction, also porting to numerical time series things that are commonly done in natural language processing.\nThanks for reading!\nPhoto by Nick Dunn on Unsplash\n\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.\n\n\nBy default means that if the optional return_sequences argument is not passed. See here for a systematic comparison of RNN return values for both torch and Keras.↩︎\nAgain, see this post for more details.↩︎\n",
    "preview": "posts/2021-03-10-forecasting-time-series-with-torch_1/images/preview.jpg",
    "last_modified": "2024-11-21T15:52:57+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-17-survey/",
    "title": "First mlverse survey results – software, applications, and beyond",
    "description": "Last month, we conducted our first survey on mlverse software, covering topics ranging from area of application through software usage to user wishes and suggestions. In addition, the survey asked about thoughts on social impacts of AI/ML. This post presents the results, and tries to address some of the things that came up.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2021-02-17",
    "categories": [
      "Meta",
      "R"
    ],
    "contents": "\n\nContents\nThe survey\nDeep learning\nAreas and applications\nFrameworks and skills\nWishes and suggestions\n\nSpark\nAreas and applications\nFrameworks and skills\nWishes and suggestions\n\nEthics and AI in society\n“Are you worried about societal/political impacts of how AI is used in the real world?”\n“When you think of the near future, are you more afraid of AI misuse or more hopeful about positive outcomes?”\nWhy worry, and what about\n\nConclusion\n\nThank you everyone who participated in our first mlverse survey!\nWait: What even is the mlverse?\nThe mlverse originated as an abbreviation of multiverse1, which, on its part, came into being as an intended allusion to the well-known tidyverse. As such, although mlverse software aims for seamless interoperability with the tidyverse, or even integration when feasible (see our recent post featuring a wholly tidymodels-integrated torch network architecture), the priorities are probably a bit different: Often, mlverse software’s raison d’être is to allow R users to do things that are commonly known to be done with other languages, such as Python.\nAs of today, mlverse development takes place mainly in two broad areas: deep learning, and distributed computing / ML automation. By its very nature, though, it is open to changing user interests and demands. Which leads us to the topic of this post.\nThe survey\nGitHub issues and community questions are valuable feedback, but we wanted something more direct. We wanted a way to find out how you, our users, employ the software, and what for; what you think could be improved; what you wish existed but is not there (yet). To that end, we created a survey. Complementing software- and application-related questions for the above-mentioned broad areas, the survey had a third section, asking about how you perceive ethical and social implications of AI as applied in the “real world”.\nA few things upfront:\nFirstly, the survey was completely anonymous, in that we asked for neither identifiers (such as e-mail addresses) nor things that render one identifiable, such as gender or geographic location. In the same vein, we had collection of IP addresses disabled on purpose.\nSecondly, just like GitHub issues are a biased sample, this survey’s participants must be. Main venues of promotion were rstudio::global, Twitter, LinkedIn, and RStudio Community. As this was the first time we did such a thing (and under significant time constraints), not everything was planned to perfection – not wording-wise and not distribution-wise. Nevertheless, we got a lot of interesting, helpful, and often very detailed answers, – and for the next time we do this, we’ll have our lessons learned!\nThirdly, all questions were optional, naturally resulting in different numbers of valid answers per question. On the other hand, not having to select a bunch of “not applicable” boxes freed respondents to spend time on topics that mattered to them.\nAs a final pre-remark, most questions allowed for multiple answers.\nIn sum, we ended up with 138 completed2 surveys. Thanks again everyone who participated, and especially, thank you for taking the time to answer the – many – free-form questions!\nDeep learning\nAreas and applications\nOur first goal was to find out in which settings, and for what kinds of applications, deep-learning software is being used.\nOverall, 72 respondents reported using DL in their jobs in industry, followed by academia (23), studies (21), spare time (43), and not-actually-using-but-wanting-to (24).\nOf those working with DL in industry, more than twenty said they worked in consulting, finance, and healthcare (each). IT, education, retail, pharma, and transportation were each mentioned more than ten times:\n\n\n\nFigure 1: Number of users reporting to use DL in industry. Smaller groups not displayed.\n\n\n\nIn academia, dominant fields (as per survey participants) were bioinformatics, genomics, and IT, followed by biology, medicine, pharmacology, and social sciences:\n\n\n\nFigure 2: Number of users reporting to use DL in academia. Smaller groups not displayed.\n\n\n\nWhat application areas matter to larger subgroups of “our” users? Nearly a hundred (of 138!) respondents said they used DL for some kind of image-processing application (including classification, segmentation, and object detection). Next up was time-series forecasting, followed by unsupervised learning.\nThe popularity of unsupervised DL was a bit unexpected; had we anticipated this, we would have asked for more detail here. So if you’re one of the people who selected this – or if you didn’t participate, but do use DL for unsupervised learning – please let us know a bit more in the comments!\nNext, NLP was about on par with the former; followed by DL on tabular data, and anomaly detection. Bayesian deep learning, reinforcement learning, recommendation systems, and audio processing were still mentioned frequently.\n\n\n\nFigure 3: Applications deep learning is used for. Smaller groups not displayed.\n\n\n\nFrameworks and skills\nWe also asked what frameworks and languages participants were using for deep learning, and what they were planning on using in the future. Single-time mentions (e.g., deeplearning4J) are not displayed.\n\n\n\nFigure 4: Framework / language used for deep learning. Single mentions not displayed.\n\n\n\nAn important thing for any software developer or content creator to investigate is proficiency/levels of expertise present in their audiences. It (nearly) goes without saying that expertise is very different from self-reported expertise. I’d like to be very cautious, then, to interpret the below results.\nWhile with regard to R skills3, the aggregate self-ratings look plausible (to me), I would have guessed a slightly different outcome re DL. Judging from other sources (like, e.g., GitHub issues), I tend to suspect more of a bimodal distribution (a far stronger version of the bimodality we’re already seeing, that is). To me, it seems like we have rather many users who know a lot about DL. In agreement with my gut feeling, though, is the bimodality itself – as opposed to, say, a Gaussian shape.\nBut of course, sample size is moderate, and sample bias is present.\n\n\n\nFigure 5: Self-rated skills re R and deep learning.\n\n\n\nWishes and suggestions\nNow, to the free-form questions. We wanted to know what we could do better.\nI’ll address the most salient topics in order of frequency of mention.4 For DL, this is surprisingly easy (as opposed to Spark, as you’ll see).\n“No Python”\nThe number one concern with deep learning from R, for survey respondents, clearly has to do not with R but with Python. This topic appeared in various forms, the most frequent being frustration over how hard it can be, dependent on the environment, to get Python dependencies for TensorFlow/Keras correct. (It also appeared as enthusiasm for torch, which we are very happy about.)\nLet me clarify and add some context.\nTensorFlow is a Python framework (nowadays subsuming Keras, which is why I’ll be addressing both of those as “TensorFlow” for simplicity) that is made available from R through packages tensorflow and keras . As with other Python libraries, objects are imported and accessible via reticulate . While tensorflow provides the low-level access, keras brings idiomatic-feeling, nice-to-use wrappers that let you forget about the chain of dependencies involved.\nOn the other hand, torch, a recent addition to mlverse software, is an R port of PyTorch that does not delegate to Python. Instead, its R layer directly calls into libtorch, the C++ library behind PyTorch. In that way, it is like a lot of high-duty R packages, making use of C++ for performance reasons.\nNow, this is not the place for recommendations. Here are a few thoughts though.\nClearly, as one respondent remarked, as of today the torch ecosystem does not offer functionality on par with TensorFlow, and for that to change time and – hopefully! more on that below – your, the community’s, help is needed. Why? Because torch is so young, for one; but also, there is a “systemic” reason! With TensorFlow, as we can access any symbol via the tf object, it is always possible, if inelegant, to do from R what you see done in Python. Respective R wrappers nonexistent5, quite a few blog posts (see, e.g., https://blogs.rstudio.com/ai/posts/2020-04-29-encrypted_keras_with_syft/, or A first look at federated learning with TensorFlow) relied on this!\nSwitching to the topic of tensorflow’s Python dependencies causing problems with installation, my experience (from GitHub issues, as well as my own) has been that difficulties are quite system-dependent. On some OSes, complications seem to appear more often than on others; and low-control (to the individual user) environments like HPC clusters can make things especially difficult. In any case though, I have to (unfortunately) admit that when installation problems appear, they can be very tricky to solve.\ntidymodels integration\nThe second most frequent mention clearly was the wish for tighter tidymodels integration. Here, we wholeheartedly agree. As of today, there is no automated way to accomplish this for torch models generically, but it can be done for specific model implementations.\nLast week, torch, tidymodels, and high-energy physics featured the first tidymodels-integrated torch package. And there’s more to come. In fact, if you are developing a package in the torch ecosystem, why not consider doing the same? Should you run into problems, the growing torch community will be happy to help.\nDocumentation, examples, teaching materials\nThirdly, several respondents expressed the wish for more documentation, examples, and teaching materials. Here, the situation is different for TensorFlow than for torch.\nFor tensorflow, the website has a multitude of guides, tutorials, and examples. For torch, reflecting the discrepancy in respective lifecycles, materials are not that abundant (yet). However, after a recent refactoring, the website has a new, four-part Get started section addressed to both beginners in DL and experienced TensorFlow users curious to learn about torch. After this hands-on introduction, a good place to get more technical background would be the section on tensors, autograd, and neural network modules.\nTruth be told, though, nothing would be more helpful here than contributions from the community. Whenever you solve even the tiniest problem (which is often how things appear to oneself), consider creating a vignette explaining what you did. Future users will be thankful, and a growing user base means that over time, it’ll be your turn to find that some things have already been solved for you!\nCommunity, community, community\nThe remaining items discussed didn’t come up quite as often (individually), but taken together, they all have something in common: They all are wishes we happen to have, as well!\nThis definitely holds in the abstract – let me cite:\n\n“Develop more of a DL community”\n“Larger developer community and ecosystem. Rstudio has made great tools, but for applied work is has been hard to work against the momentum of working in Python.”\n\nWe wholeheartedly agree, and building a larger community is exactly what we’re trying to do. I like the formulation “a DL community” insofar it is framework-independent. In the end, frameworks are just tools, and what counts is our ability to usefully apply those tools to problems we need to solve.\nConcrete wishes include\nMore paper/model implementations (such as TabNet).\nFacilities for easy data reshaping and pre-processing (e.g., in order to pass data to RNNs or 1dd convnets in the expected 3-d format).\nProbabilistic programming for torch (analogously to TensorFlow Probability).\nA high-level library (such as fast.ai) based on torch.\nIn other words, there is a whole cosmos of useful things to create; and no small group alone can do it. This is where we hope we can build a community of people, each contributing what they’re most interested in, and to whatever extent they wish.\nSpark\nAreas and applications\nFor Spark, questions broadly paralleled those asked about deep learning.\nOverall, judging from this survey (and unsurprisingly), Spark is predominantly used in industry (n = 39). For academic staff and students (taken together), n = 8. Seventeen people reported using Spark in their spare time, while 34 said they wanted to use it in the future.\nLooking at industry sectors, we again find finance, consulting, and healthcare dominating.6\n\n\n\nFigure 6: Number of users reporting to use Spark in industry. Smaller groups not displayed.\n\n\n\nWhat do survey respondents do with Spark? Analyses of tabular data and time series dominate:\n\n\n\nFigure 7: Number of users reporting to use Spark in industry. Smaller groups not displayed.\n\n\n\nFrameworks and skills\nAs with deep learning, we wanted to know what language people use to do Spark. If you look at the below graphic, you see R appearing twice: once in connection with sparklyr, once with SparkR. What’s that about?\nBoth sparklyr and SparkR are R interfaces for Apache Spark, each designed and built with a different set of priorities and, consequently, trade-offs in mind.\nsparklyr, one the one hand, will appeal to data scientists at home in the tidyverse, as they’ll be able to use all the data manipulation interfaces they’re familiar with from packages such as dplyr, DBI, tidyr, or broom.\nSparkR, on the other hand, is a light-weight R binding for Apache Spark, and is bundled with the same. It’s an excellent choice for practitioners who are well-versed in Apache Spark and just need a thin wrapper to access various Spark functionalities from R.\n\n\n\nFigure 8: Language / language bindings used to do Spark.\n\n\n\nWhen asked to rate their expertise in R7 and Spark, respectively, respondents showed similar behavior as observed for deep learning above: Most people seem to think more of their R skills than their theoretical Spark-related knowledge. However, even more caution should be exercised here than above: The number of responses here was significantly lower.\n\n\n\nFigure 9: Self-rated skills re R and Spark.\n\n\n\nWishes and suggestions\nJust like with DL, Spark users were asked what could be improved, and what they were hoping for.\nInterestingly, answers were less “clustered” than for DL. While with DL, a few things cropped up again and again, and there were very few mentions of concrete technical features, here we see about the opposite: The great majority of wishes were concrete, technical, and often only came up once.\nProbably though, this is not a coincidence.\nLooking back at how sparklyr has evolved from 2016 until now, there is a persistent theme of it being the bridge that joins the Apache Spark ecosystem to numerous useful R interfaces, frameworks, and utilities (most notably, the tidyverse).\nMany of our users’ suggestions were essentially a continuation of this theme. This holds, for example, for two features already available as of sparklyr 1.4 and 1.2, respectively: support for the Arrow serialization format and for Databricks Connect. It also holds for tidymodels integration (a frequent wish), a simple R interface for defining Spark UDFs (frequently desired, this one too), out-of-core direct computations on Parquet files, and extended time-series functionalities.\nWe’re thankful for the feedback and will evaluate carefully what could be done in each case. In general, integrating sparklyr with some feature X is a process to be planned carefully, as modifications could, in theory, be made in various places (sparklyr; X; both sparklyr and X; or even a newly-to-be-created extension). In fact, this is a topic deserving of much more detailed coverage, and has to be left to a future post.\nEthics and AI in society\nTo start, this is probably the section that will profit most from more preparation, the next time we do this survey. Due to time pressure, some (not all!) of the questions ended up being too suggestive, possibly resulting in social-desirability bias.8\nNext time, we’ll try to avoid this, and questions in this area will likely look pretty different (more like scenarios or what-if stories)9. However, I was told by several people they’d been positively surprised by simply encountering this topic at all in the survey. So perhaps this is the main point – although there are a few results that I’m sure will be interesting by themselves!\nAnticlimactically, the most non-obvious results are presented first.10\n“Are you worried about societal/political impacts of how AI is used in the real world?”\nFor this question, we had four answer options, formulated in a way that left no real “middle ground”. (The labels in the graphic below verbatim reflect those options.)\n\n\n\nFigure 10: Number of users responding to the question ‘Are you worried about societal/political impacts of how AI is used in the real world?’ with the answer options given.\n\n\n\nThe next question is definitely one to keep for future editions, as from all questions in this section, it definitely has the highest information content.\n“When you think of the near future, are you more afraid of AI misuse or more hopeful about positive outcomes?”\nHere, the answer was to be given by moving a slider, with -100 signifying “I tend to be more pessimistic”; and 100, “I tend to be more optimistic”. Although it would have been possible to remain undecided, choosing a value close to 0, we instead see a bimodal distribution:\n\n\n\nFigure 11: When you think of the near future, are you more afraid of AI misuse or more hopeful about positive outcomes?\n\n\n\nWhy worry, and what about\nThe following two questions are those already alluded to as possibly being overly prone to social-desirability bias. They asked what applications people were worried about, and for what reasons, respectively. Both questions allowed to select however many responses one wanted, intentionally not forcing people to rank things that are not comparable (the way I see it). In both cases though, it was possible to explicitly indicate None (corresponding to “I don’t really find any of these problematic” and “I am not extensively worried”, respectively.)\n\nWhat applications of AI do you feel are most problematic?\n\n\n\n\nFigure 12: Number of users selecting the respective application in response to the question: What applications of AI do you feel are most problematic?\n\n\n\n\nIf you are worried about misuse and negative impacts, what exactly is it that worries you?\n\n\n\n\nFigure 13: Number of users selecting the respective impact in response to the question: If you are worried about misuse and negative impacts, what exactly is it that worries you?\n\n\n\nComplementing these questions, it was possible to enter further thoughts and concerns in free-form. Although I can’t cite everything that was mentioned here, recurring themes were:\nMisuse of AI to the wrong purposes, by the wrong people, and at scale.\nNot feeling responsible for how one’s algorithms are used (the I’m just a software engineer topos).\nReluctance, in AI but in society overall as well, to even discuss the topic (ethics).\nFinally, although this was mentioned just once, I’d like to relay a comment that went in a direction absent from all provided answer options, but that probably should have been there already: AI being used to construct social credit systems.\n\n“It’s also that you somehow might have to learn to game the algorithm, which will make AI application forcing us to behave in some way to be scored good. That moment scares me when the algorithm is not only learning from our behavior but we behave so that the algorithm predicts us optimally (turning every use case around).”11\n\nConclusion\nThis has become a long text. But I think that seeing how much time respondents took to answer the many questions, often including lots of detail in the free-form answers, it seemed like a matter of decency to, in the analysis and report, go into some detail as well.\nThanks again to everyone who took part! We hope to make this a recurring thing, and will strive to design the next edition in a way that makes answers even more information-rich.\nThanks for reading!\n\nCalling it an abbreviation is, in fact, itself abbreviating things. The main motivation is more practical, as in: being able to obtain not-yet-reserved URLs and not-too-frequently-used hashtags.↩︎\nThere were quite a few uncompleted ones as well, but I decided to leave them out to avoid accidental errors like double-counting. I’m sorry if that has led to valuable answers having been excluded.↩︎\nThis question was addressed only to people using R for deep learning.↩︎\nFor space reasons, I’m not able to address every single suggestion. But please be sure that everything has carefully been read and considered.↩︎\nFor TensorFlow Federated and PySyft, that is.↩︎\nIn hindsight, if you were a consultant, the survey should have asked what industry sectors you were doing consulting for/in.↩︎\nThis question only applied Spark users working in R.↩︎\nI’ll indicate below the questions for which I think this might have been the case.↩︎\nAlthough I don’t think there is an escape from social desirability bias. Even thought experiments like the Trolley problem are not free of this.↩︎\n(This is just due to my following the order of questions in the survey.)↩︎\nCited verbatim, except for typos.↩︎\n",
    "preview": "posts/2021-02-17-survey/images/preview.jpg",
    "last_modified": "2024-11-21T15:49:36+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-11-tabnet/",
    "title": "torch, tidymodels, and high-energy physics",
    "description": "Today we introduce tabnet, a torch implementation of \"TabNet: Attentive Interpretable Tabular Learning\" that is fully integrated with the tidymodels framework. Per se, already, tabnet was designed to require very little data pre-processing; thanks to tidymodels, hyperparameter tuning (so often cumbersome in deep learning) becomes convenient and even, fun!",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2021-02-11",
    "categories": [
      "Torch",
      "R",
      "Tabular Data"
    ],
    "contents": "\n\nContents\nTabNet\ntidymodels\nUsing tabnet with tidymodels\nIn the flow with TabNet\nTabNet tuning\nTabNet interpretability features\nInterpretable, explainable, …? Beyond the arbitrariness of definitions\n\nSo what’s with the clickbait (high-energy physics)? Well, it’s not just clickbait. To showcase TabNet, we will be using the Higgs dataset (Baldi, Sadowski, and Whiteson (2014)), available at UCI Machine Learning Repository. I don’t know about you, but I always enjoy using datasets that motivate me to learn more about things. But first, let’s get acquainted with the main actors of this post!\nTabNet\nTabNet was introduced in Arik and Pfister (2020). It is interesting for three reasons:\nIt claims highly competitive performance on tabular data, an area where deep learning has not gained much of a reputation yet.\nTabNet includes interpretability1 features by design.\nIt is claimed to significantly profit from self-supervised pre-training, again in an area where this is anything but undeserving of mention.\nIn this post, we won’t go into (3), but we do expand on (2), the ways TabNet allows access to its inner workings.\nHow do we use TabNet from R? The torch ecosystem includes a package – tabnet – that not only implements the model of the same name, but also allows you to make use of it as part of a tidymodels workflow.\ntidymodels\nTo many R-using data scientists, the tidymodels framework will not be a stranger. tidymodels provides a high-level, unified approach to model training, hyperparameter optimization, and inference.\ntabnet is the first (of many, we hope) torch models that let you use a tidymodels workflow all the way: from data pre-processing over hyperparameter tuning to performance evaluation and inference. While the first, as well as the last, may seem nice-to-have but not “mandatory,” the tuning experience is likely to be something you’ll won’t want to do without!\nUsing tabnet with tidymodels\nIn this post, we first showcase a tabnet-using workflow in a nutshell, making use of hyperparameter settings reported in the paper.\nThen, we initiate a tidymodels-powered hyperparameter search, focusing on the basics but also, encouraging you to dig deeper at your leisure.\nFinally, we circle back to the promise of interpretability, demonstrating what is offered by tabnet and ending in a short discussion.\nIn the flow with TabNet\nAs usual, we start by loading all required libraries. We also set a random seed, on the R as well as the torch sides. When model interpretation is part of your task, you will want to investigate the role of random initialization.\n\n\nlibrary(torch)\nlibrary(tabnet)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(finetune) # to use tuning functions from the new finetune package\nlibrary(vip) # to plot feature importances\n\nset.seed(777)\ntorch_manual_seed(777)\n\n\nNext, we load the dataset.\n\n\n# download from https://archive.ics.uci.edu/ml/datasets/HIGGS\nhiggs <- read_csv(\n  \"HIGGS.csv\",\n  col_names = c(\"class\", \"lepton_pT\", \"lepton_eta\", \"lepton_phi\", \"missing_energy_magnitude\",\n                \"missing_energy_phi\", \"jet_1_pt\", \"jet_1_eta\", \"jet_1_phi\", \"jet_1_b_tag\",\n                \"jet_2_pt\", \"jet_2_eta\", \"jet_2_phi\", \"jet_2_b_tag\", \"jet_3_pt\", \"jet_3_eta\",\n                \"jet_3_phi\", \"jet_3_b_tag\", \"jet_4_pt\", \"jet_4_eta\", \"jet_4_phi\", \"jet_4_b_tag\",\n                \"m_jj\", \"m_jjj\", \"m_lv\", \"m_jlv\", \"m_bb\", \"m_wbb\", \"m_wwbb\"),\n  col_types = \"fdddddddddddddddddddddddddddd\"\n  )\n\n\nWhat’s this about? In high-energy physics, the search for new particles takes place at powerful particle accelerators, such as (and most prominently) CERN’s Large Hadron Collider. In addition to actual experiments, simulation plays an important role. In simulations, “measurement” data are generated according to different underlying hypotheses, resulting in distributions that can be compared with each other. Given the likelihood of the simulated data, the goal then is to make inferences about the hypotheses.\nThe above dataset (Baldi, Sadowski, and Whiteson (2014)) results from just such a simulation. It explores what features could be measured assuming two different processes. In the first process, two gluons collide, and a heavy Higgs boson is produced; this is the signal process, the one we’re interested in. In the second, the collision of the gluons results in a pair of top quarks – this is the background process.\nThrough different intermediaries, both processes result in the same end products – so tracking these does not help. Instead, what the paper authors did was simulate kinematic features (momenta, specifically) of decay products, such as leptons (electrons and protons) and particle jets. In addition, they constructed a number of high-level features, features that presuppose domain knowledge. In their article, they showed that, in contrast to other machine learning methods, deep neural networks did nearly as well when presented with the low-level features (the momenta) only as with just the high-level features alone.\nCertainly, it would be interesting to double-check these results on tabnet, and then, look at the respective feature importances. However, given the size of the dataset, non-negligible computing resources (and patience) will be required.\nSpeaking of size, let’s take a look:\n\n\nhiggs %>% glimpse()\n\n\nRows: 11,000,000\nColumns: 29\n$ class                    <fct> 1.000000000000000000e+00, 1.000000…\n$ lepton_pT                <dbl> 0.8692932, 0.9075421, 0.7988347, 1…\n$ lepton_eta               <dbl> -0.6350818, 0.3291473, 1.4706388, …\n$ lepton_phi               <dbl> 0.225690261, 0.359411865, -1.63597…\n$ missing_energy_magnitude <dbl> 0.3274701, 1.4979699, 0.4537732, 1…\n$ missing_energy_phi       <dbl> -0.68999320, -0.31300953, 0.425629…\n$ jet_1_pt                 <dbl> 0.7542022, 1.0955306, 1.1048746, 1…\n$ jet_1_eta                <dbl> -0.24857314, -0.55752492, 1.282322…\n$ jet_1_phi                <dbl> -1.09206390, -1.58822978, 1.381664…\n$ jet_1_b_tag              <dbl> 0.000000, 2.173076, 0.000000, 0.00…\n$ jet_2_pt                 <dbl> 1.3749921, 0.8125812, 0.8517372, 2…\n$ jet_2_eta                <dbl> -0.6536742, -0.2136419, 1.5406590,…\n$ jet_2_phi                <dbl> 0.9303491, 1.2710146, -0.8196895, …\n$ jet_2_b_tag              <dbl> 1.107436, 2.214872, 2.214872, 2.21…\n$ jet_3_pt                 <dbl> 1.1389043, 0.4999940, 0.9934899, 1…\n$ jet_3_eta                <dbl> -1.578198314, -1.261431813, 0.3560…\n$ jet_3_phi                <dbl> -1.04698539, 0.73215616, -0.208777…\n$ jet_3_b_tag              <dbl> 0.000000, 0.000000, 2.548224, 0.00…\n$ jet_4_pt                 <dbl> 0.6579295, 0.3987009, 1.2569546, 0…\n$ jet_4_eta                <dbl> -0.01045457, -1.13893008, 1.128847…\n$ jet_4_phi                <dbl> -0.0457671694, -0.0008191102, 0.90…\n$ jet_4_btag               <dbl> 3.101961, 0.000000, 0.000000, 0.00…\n$ m_jj                     <dbl> 1.3537600, 0.3022199, 0.9097533, 0…\n$ m_jjj                    <dbl> 0.9795631, 0.8330482, 1.1083305, 1…\n$ m_lv                     <dbl> 0.9780762, 0.9856997, 0.9856922, 0…\n$ m_jlv                    <dbl> 0.9200048, 0.9780984, 0.9513313, 0…\n$ m_bb                     <dbl> 0.7216575, 0.7797322, 0.8032515, 0…\n$ m_wbb                    <dbl> 0.9887509, 0.9923558, 0.8659244, 1…\n$ m_wwbb                   <dbl> 0.8766783, 0.7983426, 0.7801176, 0…\nEleven million “observations” (kind of) – that’s a lot! Like the authors of the TabNet paper (Arik and Pfister (2020)), we’ll use 500,000 of these for validation. (Unlike them, though, we won’t be able to train for 870,000 iterations!)\nThe first variable, class, is either 1 or 0, depending on whether a Higgs boson was present or not. While in experiments, only a tiny fraction of collisions produce one of those, both classes are about equally frequent in this dataset.\nAs for the predictors, the last seven are high-level (derived). All others are “measured.”\nData loaded, we’re ready to build a tidymodels workflow, resulting in a short sequence of concise steps.\nFirst, split the data:\n\n\nn <- 11000000\nn_test <- 500000\ntest_frac <- n_test/n\n\nsplit <- initial_time_split(higgs, prop = 1 - test_frac)\ntrain <- training(split)\ntest  <- testing(split)\n\n\nSecond, create a recipe. We want to predict class from all other features present:\n\n\nrec <- recipe(class ~ ., train)\n\n\nThird, create a parsnip model specification of class tabnet. The parameters passed are those reported by the TabNet paper, for the S-sized model variant used on this dataset.2\n\n\n# hyperparameter settings (apart from epochs) as per the TabNet paper (TabNet-S)\nmod <- tabnet(epochs = 3, batch_size = 16384, decision_width = 24, attention_width = 26,\n              num_steps = 5, penalty = 0.000001, virtual_batch_size = 512, momentum = 0.6,\n              feature_reusage = 1.5, learn_rate = 0.02) %>%\n  set_engine(\"torch\", verbose = TRUE) %>%\n  set_mode(\"classification\")\n\n\nFourth, bundle recipe and model specifications in a workflow:\n\n\nwf <- workflow() %>%\n  add_model(mod) %>%\n  add_recipe(rec)\n\n\nFifth, train the model. This will take some time. Training finished, we save the trained parsnip model, so we can reuse it at a later time.\n\n\nfitted_model <- wf %>% fit(train)\n\n# access the underlying parsnip model and save it to RDS format\n# depending on when you read this, a nice wrapper may exist\n# see https://github.com/mlverse/tabnet/issues/27  \nfitted_model$fit$fit$fit %>% saveRDS(\"saved_model.rds\")\n\n\nAfter three epochs, loss was at 0.609.\nSixth – and finally – we ask the model for test-set predictions and have accuracy computed.\n\n\npreds <- test %>%\n  bind_cols(predict(fitted_model, test))\n\nyardstick::accuracy(preds, class, .pred_class)\n\n\n# A tibble: 1 x 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.672\nWe didn’t quite arrive at the accuracy reported in the TabNet paper (0.783), but then, we only trained for a tiny fraction of the time.\nIn case you’re thinking: well, that was a nice and effortless way of training a neural network! – just wait and see how easy hyperparameter tuning can get. In fact, no need to wait, we’ll take a look right now.\nTabNet tuning\nFor hyperparameter tuning, the tidymodels framework makes use of cross-validation. With a dataset of considerable size, some time and patience is needed; for the purpose of this post, I’ll use 1/1,000 of observations.\nChanges to the above workflow start at model specification. Let’s say we’ll leave most settings fixed, but vary the TabNet-specific hyperparameters decision_width, attention_width, and num_steps, as well as the learning rate:3\n\n\nmod <- tabnet(epochs = 1, batch_size = 16384, decision_width = tune(), attention_width = tune(),\n              num_steps = tune(), penalty = 0.000001, virtual_batch_size = 512, momentum = 0.6,\n              feature_reusage = 1.5, learn_rate = tune()) %>%\n  set_engine(\"torch\", verbose = TRUE) %>%\n  set_mode(\"classification\")\n\n\nWorkflow creation looks the same as before:\n\n\nwf <- workflow() %>%\n  add_model(mod) %>%\n  add_recipe(rec)\n\n\nNext, we specify the hyperparameter ranges we’re interested in, and call one of the grid construction functions from the dials package to build one for us. If it wasn’t for demonstration purposes, we’d probably want to have more than eight alternatives though, and pass a higher size to grid_max_entropy() .\n\n\ngrid <-\n  wf %>%\n  parameters() %>%\n  update(\n    decision_width = decision_width(range = c(20, 40)),\n    attention_width = attention_width(range = c(20, 40)),\n    num_steps = num_steps(range = c(4, 6)),\n    learn_rate = learn_rate(range = c(-2.5, -1))\n  ) %>%\n  grid_max_entropy(size = 8)\n\ngrid\n\n\n# A tibble: 8 x 4\n  learn_rate decision_width attention_width num_steps\n       <dbl>          <int>           <int>     <int>\n1    0.00529             28              25         5\n2    0.0858              24              34         5\n3    0.0230              38              36         4\n4    0.0968              27              23         6\n5    0.0825              26              30         4\n6    0.0286              36              25         5\n7    0.0230              31              37         5\n8    0.00341             39              23         5\nTo search the space, we use tune_race_anova() from the new finetune package, making use of five-fold cross-validation:\n\n\nctrl <- control_race(verbose_elim = TRUE)\nfolds <- vfold_cv(train, v = 5)\nset.seed(777)\n\nres <- wf %>%\n    tune_race_anova(\n    resamples = folds,\n    grid = grid,\n    control = ctrl\n  )\n\n\nWe can now extract the best hyperparameter combinations:\n\n\nres %>% show_best(\"accuracy\") %>% select(- c(.estimator, .config))\n\n\n# A tibble: 5 x 8\n  learn_rate decision_width attention_width num_steps .metric   mean     n std_err\n       <dbl>          <int>           <int>     <int> <chr>    <dbl> <int>   <dbl>\n1     0.0858             24              34         5 accuracy 0.516     5 0.00370\n2     0.0230             38              36         4 accuracy 0.510     5 0.00786\n3     0.0230             31              37         5 accuracy 0.510     5 0.00601\n4     0.0286             36              25         5 accuracy 0.510     5 0.0136\n5     0.0968             27              23         6 accuracy 0.498     5 0.00835\nIt’s hard to imagine how tuning could be more convenient!\nNow, we circle back to the original training workflow, and inspect TabNet’s interpretability features.\nTabNet interpretability features\nTabNet’s most prominent characteristic is the way – inspired by decision trees – it executes in distinct steps. At each step, it again looks at the original input features, and decides which of those to consider based on lessons learned in prior steps. Concretely, it uses an attention mechanism to learn sparse masks which are then applied to the features.\nNow, these masks being “just” model weights means we can extract them and draw conclusions about feature importance. Depending on how we proceed, we can either\naggregate mask weights over steps, resulting in global per-feature importances;\nrun the model on a few test samples and aggregate over steps, resulting in observation-wise feature importances; or\nrun the model on a few test samples and extract individual weights observation- as well as step-wise.\nThis is how to accomplish the above with tabnet.\nPer-feature importances\nWe continue with the fitted_model workflow object we ended up with at the end of part 1. vip::vip is able to display feature importances directly from the parsnip model:\n\n\nfit <- pull_workflow_fit(fitted_model)\nvip(fit) + theme_minimal()\n\n\n\n\n\nFigure 1: Global feature importances.\n\n\n\nTogether, two high-level features dominate, accounting for nearly 50% of overall attention. Along with a third high-level feature, ranked in place four, they occupy about 60% of “importance space.”\nObservation-level feature importances\nWe choose the first hundred observations in the test set to extract feature importances. Due to how TabNet enforces sparsity, we see that many features have not been made use of:\n\n\nex_fit <- tabnet_explain(fit$fit, test[1:100, ])\n\nex_fit$M_explain %>%\n  mutate(observation = row_number()) %>%\n  pivot_longer(-observation, names_to = \"variable\", values_to = \"m_agg\") %>%\n  ggplot(aes(x = observation, y = variable, fill = m_agg)) +\n  geom_tile() +\n  theme_minimal() +\n  scale_fill_viridis_c()\n\n\n\n\n\nFigure 2: Per-observation feature importances.\n\n\n\nPer-step, observation-level feature importances\nFinally and on the same selection of observations, we again inspect the masks, but this time, per decision step:\n\n\nex_fit$masks %>%\n  imap_dfr(~mutate(\n    .x,\n    step = sprintf(\"Step %d\", .y),\n    observation = row_number()\n  )) %>%\n  pivot_longer(-c(observation, step), names_to = \"variable\", values_to = \"m_agg\") %>%\n  ggplot(aes(x = observation, y = variable, fill = m_agg)) +\n  geom_tile() +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 5)) +\n  scale_fill_viridis_c() +\n  facet_wrap(~step)\n\n\n\n\n\nFigure 3: Per-observation, per-step feature importances.\n\n\n\nThis is nice: We clearly see how TabNet makes use of different features at different times.\nSo what do we make of this? It depends. Given the enormous societal importance of this topic – call it interpretability, explainability, or whatever – let’s finish this post with a short discussion.\nInterpretable, explainable, …? Beyond the arbitrariness of definitions\nAn internet search for “interpretable vs. explainable ML” immediately turns up a number of sites confidently stating “interpretable ML is …” and “explainable ML is …,” as though there were no arbitrariness in common-speech definitions. Going deeper, you find articles such as Cynthia Rudin’s “Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead” (Rudin (2018)) that present you with a clear-cut, deliberate, instrumentalizable distinction that can actually be used in real-world scenarios.\nIn a nutshell, what she decides to call explainability is: approximate a black-box model by a simpler (e.g., linear) model and, starting from the simple model, make inferences about how the black-box model works. One of the examples she gives for how this could fail is so striking I’d like to fully cite it:\n\nEven an explanation model that performs almost identically to a black box model might use completely different features, and is thus not faithful to the computation of the black box. Consider a black box model for criminal recidivism prediction, where the goal is to predict whether someone will be arrested within a certain time after being released from jail/prison. Most recidivism prediction models depend explicitly on age and criminal history, but do not explicitly depend on race. Since criminal history and age are correlated with race in all of our datasets, a fairly accurate explanation model could construct a rule such as “This person is predicted to be arrested because they are black.” This might be an accurate explanation model since it correctly mimics the predictions of the original model, but it would not be faithful to what the original model computes.\n\nWhat she calls interpretability, in contrast, is deeply related to domain knowledge:\n\nInterpretability is a domain-specific notion […] Usually, however, an interpretable machine learning model is constrained in model form so that it is either useful to someone, or obeys structural knowledge of the domain, such as monotonicity [e.g.,8], causality, structural (generative) constraints, additivity [9], or physical constraints that come from domain knowledge. Often for structured data, sparsity is a useful measure of interpretability […]. Sparse models allow a view of how variables interact jointly rather than individually. […] e.g., in some domains, sparsity is useful,and in others is it not.\n\nIf we accept these well-thought-out definitions, what can we say about TabNet? Is looking at attention masks more like constructing a post-hoc model or more like having domain knowledge incorporated? I believe Rudin would argue the former, since\nthe image-classification example she uses to point out weaknesses of explainability techniques employs saliency maps, a technical device comparable, in some ontological sense, to attention masks;\nthe sparsity enforced by TabNet is a technical, not a domain-related constraint;\nwe only know what features were used by TabNet, not how it used them.\nOn the other hand, one could disagree with Rudin (and others) about the premises. Do explanations have to be modeled after human cognition to be considered valid? Personally, I guess I’m not sure, and to cite from a post by Keith O’Rourke on just this topic of interpretability,\n\nAs with any critically-thinking inquirer, the views behind these deliberations are always subject to rethinking and revision at any time.\n\nIn any case though, we can be sure that this topic’s importance will only grow with time. While in the very early days of the GDPR (the EU General Data Protection Regulation) it was said that Article 22 (on automated decision-making) would have significant impact on how ML is used4, unfortunately the current view seems to be that its wordings are far too vague to have immediate consequences (e.g., Wachter, Mittelstadt, and Floridi (2017)). But this will be a fascinating topic to follow, from a technical as well as a political point of view.\nThanks for reading!\n\n\n\nArik, Sercan O., and Tomas Pfister. 2020. “TabNet: Attentive Interpretable Tabular Learning.” https://arxiv.org/abs/1908.07442.\n\n\nBaldi, P., P. Sadowski, and D. Whiteson. 2014. “Searching for exotic particles in high-energy physics with deep learning.” Nature Communications 5 (July): 4308. https://doi.org/10.1038/ncomms5308.\n\n\nRudin, Cynthia. 2018. “Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead.” https://arxiv.org/abs/1811.10154.\n\n\nWachter, Sandra, Brent Mittelstadt, and Luciano Floridi. 2017. “Why a Right to Explanation of Automated Decision-Making Does Not Exist in the General Data Protection Regulation.” International Data Privacy Law 7 (2): 76–99. https://doi.org/10.1093/idpl/ipx005.\n\n\nI’m using the term “naively” here. For a short discussion, see the final section, [Interpretable, explainable, you tell me – beyond arbitrary definitions].↩︎\nApart from the number of epochs, that is.↩︎\nThe number of epochs is set to one for demonstration purposes only; in reality, you will want to tune this as well.↩︎\nSee, e.g., <http://www.odbms.org/2018/07/ai-machine-learning-and-the-gdpr-are-the-wild-west-days-of-advanced-analytics-over/>.↩︎\n",
    "preview": "posts/2021-02-11-tabnet/images/d.jpg",
    "last_modified": "2024-11-21T15:49:39+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-04-simple-audio-classification-with-torch/",
    "title": "Simple audio classification with torch",
    "description": "This article translates Daniel Falbel's post on \"Simple Audio Classification\" from TensorFlow/Keras to torch/torchaudio.",
    "author": [
      {
        "name": "Athos Damiani",
        "url": {}
      }
    ],
    "date": "2021-02-04",
    "categories": [
      "Torch",
      "R",
      "Audio Processing"
    ],
    "contents": "\n\nContents\nDownloading and Importing\nClasses\nGenerator Dataloader\nModel definition\nModel fitting\nTraining loop\n\nMaking predictions\n\nThis article translates Daniel Falbel’s ‘Simple Audio Classification’ article from tensorflow/keras to torch/torchaudio. The main goal is to introduce torchaudio and illustrate its contributions to the torch ecosystem. Here, we focus on a popular dataset, the audio loader and the spectrogram transformer. An interesting side product is the parallel between torch and tensorflow, showing sometimes the differences, sometimes the similarities between them.\n\n\n\nlibrary(torch)\nlibrary(torchaudio)\n\n\nDownloading and Importing\ntorchaudio has the speechcommand_dataset built in. It filters out background_noise by default and lets us choose between versions v0.01 and v0.02.\n\n\n# set an existing folder here to cache the dataset\nDATASETS_PATH <- \"~/datasets/\"\n\n# 1.4GB download\ndf <- speechcommand_dataset(\n  root = DATASETS_PATH, \n  url = \"speech_commands_v0.01\",\n  download = TRUE\n)\n\n# expect folder: _background_noise_\ndf$EXCEPT_FOLDER\n# [1] \"_background_noise_\"\n\n# number of audio files\nlength(df)\n# [1] 64721\n\n# a sample\nsample <- df[1]\n\nsample$waveform[, 1:10]\n\n\ntorch_tensor\n0.0001 *\n 0.9155  0.3052  1.8311  1.8311 -0.3052  0.3052  2.4414  0.9155 -0.9155 -0.6104\n[ CPUFloatType{1,10} ]\n\n\nsample$sample_rate\n# 16000\nsample$label\n# bed\n\nplot(sample$waveform[1], type = \"l\", col = \"royalblue\", main = sample$label)\n\n\n\n\n\nFigure 1: A sample waveform for a ‘bed’.\n\n\n\nClasses\n\n\ndf$classes\n\n\n [1] \"bed\"    \"bird\"   \"cat\"    \"dog\"    \"down\"   \"eight\"  \"five\"  \n [8] \"four\"   \"go\"     \"happy\"  \"house\"  \"left\"   \"marvin\" \"nine\"  \n[15] \"no\"     \"off\"    \"on\"     \"one\"    \"right\"  \"seven\"  \"sheila\"\n[22] \"six\"    \"stop\"   \"three\"  \"tree\"   \"two\"    \"up\"     \"wow\"   \n[29] \"yes\"    \"zero\"  \nGenerator Dataloader\ntorch::dataloader has the same task as data_generator defined in the original article. It is responsible for preparing batches - including shuffling, padding, one-hot encoding, etc. - and for taking care of parallelism / device I/O orchestration.\nIn torch we do this by passing the train/test subset to torch::dataloader and encapsulating all the batch setup logic inside a collate_fn() function.\n\n\nset.seed(6)\nid_train <- sample(length(df), size = 0.7*length(df))\nid_test <- setdiff(seq_len(length(df)), id_train)\n# subsets\n\ntrain_subset <- torch::dataset_subset(df, id_train)\ntest_subset <- torch::dataset_subset(df, id_test)\n\n\nAt this point, dataloader(train_subset) would not work because the samples are not padded. So we need to build our own collate_fn() with the padding strategy.\nI suggest using the following approach when implementing the collate_fn():\nbegin with collate_fn <- function(batch) browser().\ninstantiate dataloader with the collate_fn()\ncreate an environment by calling enumerate(dataloader) so you can ask to retrieve a batch from dataloader.\nrun environment[[1]][[1]]. Now you should be sent inside collate_fn() with access to batch input object.\nbuild the logic.\n\n\ncollate_fn <- function(batch) {\n  browser()\n}\n\nds_train <- dataloader(\n  train_subset, \n  batch_size = 32, \n  shuffle = TRUE, \n  collate_fn = collate_fn\n)\n\nds_train_env <- enumerate(ds_train)\nds_train_env[[1]][[1]]\n\n\nThe final collate_fn() pads the waveform to length 16001 and then stacks everything up together. At this point there are no spectrograms yet. We going to make spectrogram transformation a part of model architecture.\n\n\npad_sequence <- function(batch) {\n    # Make all tensors in a batch the same length by padding with zeros\n    batch <- sapply(batch, function(x) (x$t()))\n    batch <- torch::nn_utils_rnn_pad_sequence(batch, batch_first = TRUE, padding_value = 0.)\n    return(batch$permute(c(1, 3, 2)))\n  }\n\n# Final collate_fn\ncollate_fn <- function(batch) {\n # Input structure:\n # list of 32 lists: list(waveform, sample_rate, label, speaker_id, utterance_number)\n # Transpose it\n batch <- purrr::transpose(batch)\n tensors <- batch$waveform\n targets <- batch$label_index\n\n # Group the list of tensors into a batched tensor\n tensors <- pad_sequence(tensors)\n \n # target encoding\n targets <- torch::torch_stack(targets)\n\n list(tensors = tensors, targets = targets) # (64, 1, 16001)\n}\n\n\nBatch structure is:\nbatch[[1]]: waveforms - tensor with dimension (32, 1, 16001)\nbatch[[2]]: targets - tensor with dimension (32, 1)\nAlso, torchaudio comes with 3 loaders, av_loader, tuner_loader, and audiofile_loader- more to come. set_audio_backend() is used to set one of them as the audio loader. Their performances differ based on audio format (mp3 or wav). There is no perfect world yet: tuner_loader is best for mp3, audiofile_loader is best for wav, but neither of them has the option of partially loading a sample from an audio file without bringing all the data into memory first.\nFor a given audio backend we need pass it to each worker through worker_init_fn() argument.\n\n\nds_train <- dataloader(\n  train_subset, \n  batch_size = 128, \n  shuffle = TRUE, \n  collate_fn = collate_fn,\n  num_workers = 16,\n  worker_init_fn = function(.) {torchaudio::set_audio_backend(\"audiofile_loader\")},\n  worker_globals = c(\"pad_sequence\") # pad_sequence is needed for collect_fn\n)\n\nds_test <- dataloader(\n  test_subset, \n  batch_size = 64, \n  shuffle = FALSE, \n  collate_fn = collate_fn,\n  num_workers = 8,\n  worker_globals = c(\"pad_sequence\") # pad_sequence is needed for collect_fn\n)\n\n\nModel definition\nInstead of keras::keras_model_sequential(), we are going to define a torch::nn_module(). As referenced by the original article, the model is based on this architecture for MNIST from this tutorial, and I’ll call it ‘DanielNN’.\n\n\ndan_nn <- torch::nn_module(\n  \"DanielNN\",\n  \n  initialize = function(\n    window_size_ms = 30, \n    window_stride_ms = 10\n  ) {\n    \n    # spectrogram spec\n    window_size <- as.integer(16000*window_size_ms/1000)\n    stride <- as.integer(16000*window_stride_ms/1000)\n    fft_size <- as.integer(2^trunc(log(window_size, 2) + 1))\n    n_chunks <- length(seq(0, 16000, stride))\n    \n    self$spectrogram <- torchaudio::transform_spectrogram(\n      n_fft = fft_size, \n      win_length = window_size, \n      hop_length = stride, \n      normalized = TRUE, \n      power = 2\n    )\n    \n    # convs 2D\n    self$conv1 <- torch::nn_conv2d(in_channels = 1, out_channels = 32, kernel_size = c(3,3))\n    self$conv2 <- torch::nn_conv2d(in_channels = 32, out_channels = 64, kernel_size = c(3,3))\n    self$conv3 <- torch::nn_conv2d(in_channels = 64, out_channels = 128, kernel_size = c(3,3))\n    self$conv4 <- torch::nn_conv2d(in_channels = 128, out_channels = 256, kernel_size = c(3,3))\n    \n    # denses\n    self$dense1 <- torch::nn_linear(in_features = 14336, out_features = 128)\n    self$dense2 <- torch::nn_linear(in_features = 128, out_features = 30)\n  },\n  \n  forward = function(x) {\n    x %>% # (64, 1, 16001)\n      self$spectrogram() %>% # (64, 1, 257, 101)\n      torch::torch_add(0.01) %>%\n      torch::torch_log() %>%\n      self$conv1() %>%\n      torch::nnf_relu() %>%\n      torch::nnf_max_pool2d(kernel_size = c(2,2)) %>%\n      \n      self$conv2() %>%\n      torch::nnf_relu() %>%\n      torch::nnf_max_pool2d(kernel_size = c(2,2)) %>%\n      \n      self$conv3() %>%\n      torch::nnf_relu() %>%\n      torch::nnf_max_pool2d(kernel_size = c(2,2)) %>%\n      \n      self$conv4() %>%\n      torch::nnf_relu() %>%\n      torch::nnf_max_pool2d(kernel_size = c(2,2)) %>%\n      \n      torch::nnf_dropout(p = 0.25) %>%\n      torch::torch_flatten(start_dim = 2) %>%\n      \n      self$dense1() %>%\n      torch::nnf_relu() %>%\n      torch::nnf_dropout(p = 0.5) %>%\n      self$dense2() \n  }\n)\n\nmodel <- dan_nn()\n\n\ndevice <- torch::torch_device(if(torch::cuda_is_available()) \"cuda\" else \"cpu\")\nmodel$to(device = device)\n\nprint(model)\n\n\nAn `nn_module` containing 2,226,846 parameters.\n\n── Modules ──────────────────────────────────────────────────────\n● spectrogram: <Spectrogram> #0 parameters\n● conv1: <nn_conv2d> #320 parameters\n● conv2: <nn_conv2d> #18,496 parameters\n● conv3: <nn_conv2d> #73,856 parameters\n● conv4: <nn_conv2d> #295,168 parameters\n● dense1: <nn_linear> #1,835,136 parameters\n● dense2: <nn_linear> #3,870 parameters\nModel fitting\nUnlike in tensorflow, there is no model %>% compile(...) step in torch, so we are going to set loss criterion, optimizer strategy and evaluation metrics explicitly in the training loop.\n\n\nloss_criterion <- torch::nn_cross_entropy_loss()\noptimizer <- torch::optim_adadelta(model$parameters, rho = 0.95, eps = 1e-7)\nmetrics <- list(acc = yardstick::accuracy_vec)\n\n\nTraining loop\n\n\nlibrary(glue)\nlibrary(progress)\n\npred_to_r <- function(x) {\n  classes <- factor(df$classes)\n  classes[as.numeric(x$to(device = \"cpu\"))]\n}\n\nset_progress_bar <- function(total) {\n  progress_bar$new(\n    total = total, clear = FALSE, width = 70,\n    format = \":current/:total [:bar] - :elapsed - loss: :loss - acc: :acc\"\n  )\n}\n\n\n\n\nepochs <- 20\nlosses <- c()\naccs <- c()\n\nfor(epoch in seq_len(epochs)) {\n  pb <- set_progress_bar(length(ds_train))\n  pb$message(glue(\"Epoch {epoch}/{epochs}\"))\n  coro::loop(for(batch in ds_train) {\n    optimizer$zero_grad()\n    predictions <- model(batch[[1]]$to(device = device))\n    targets <- batch[[2]]$to(device = device)\n    loss <- loss_criterion(predictions, targets)\n    loss$backward()\n    optimizer$step()\n    \n    # eval reports\n    prediction_r <- pred_to_r(predictions$argmax(dim = 2))\n    targets_r <- pred_to_r(targets)\n    acc <- metrics$acc(targets_r, prediction_r)\n    accs <- c(accs, acc)\n    loss_r <- as.numeric(loss$item())\n    losses <- c(losses, loss_r)\n    \n    pb$tick(tokens = list(loss = round(mean(losses), 4), acc = round(mean(accs), 4)))\n  })\n}\n\n\n\n# test\npredictions_r <- c()\ntargets_r <- c()\ncoro::loop(for(batch_test in ds_test) {\n  predictions <- model(batch_test[[1]]$to(device = device))\n  targets <- batch_test[[2]]$to(device = device)\n  predictions_r <- c(predictions_r, pred_to_r(predictions$argmax(dim = 2)))\n  targets_r <- c(targets_r, pred_to_r(targets))\n})\nval_acc <- metrics$acc(factor(targets_r, levels = 1:30), factor(predictions_r, levels = 1:30))\ncat(glue(\"val_acc: {val_acc}\\n\\n\"))\n\n\nEpoch 1/20                                                            \n[W SpectralOps.cpp:590] Warning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (function operator())\n354/354 [=========================] -  1m - loss: 2.6102 - acc: 0.2333\nEpoch 2/20                                                            \n354/354 [=========================] -  1m - loss: 1.9779 - acc: 0.4138\nEpoch 3/20                                                            \n354/354 [============================] -  1m - loss: 1.62 - acc: 0.519\nEpoch 4/20                                                            \n354/354 [=========================] -  1m - loss: 1.3926 - acc: 0.5859\nEpoch 5/20                                                            \n354/354 [==========================] -  1m - loss: 1.2334 - acc: 0.633\nEpoch 6/20                                                            \n354/354 [=========================] -  1m - loss: 1.1135 - acc: 0.6685\nEpoch 7/20                                                            \n354/354 [=========================] -  1m - loss: 1.0199 - acc: 0.6961\nEpoch 8/20                                                            \n354/354 [=========================] -  1m - loss: 0.9444 - acc: 0.7181\nEpoch 9/20                                                            \n354/354 [=========================] -  1m - loss: 0.8816 - acc: 0.7365\nEpoch 10/20                                                           \n354/354 [=========================] -  1m - loss: 0.8278 - acc: 0.7524\nEpoch 11/20                                                           \n354/354 [=========================] -  1m - loss: 0.7818 - acc: 0.7659\nEpoch 12/20                                                           \n354/354 [=========================] -  1m - loss: 0.7413 - acc: 0.7778\nEpoch 13/20                                                           \n354/354 [=========================] -  1m - loss: 0.7064 - acc: 0.7881\nEpoch 14/20                                                           \n354/354 [=========================] -  1m - loss: 0.6751 - acc: 0.7974\nEpoch 15/20                                                           \n354/354 [=========================] -  1m - loss: 0.6469 - acc: 0.8058\nEpoch 16/20                                                           \n354/354 [=========================] -  1m - loss: 0.6216 - acc: 0.8133\nEpoch 17/20                                                           \n354/354 [=========================] -  1m - loss: 0.5985 - acc: 0.8202\nEpoch 18/20                                                           \n354/354 [=========================] -  1m - loss: 0.5774 - acc: 0.8263\nEpoch 19/20                                                           \n354/354 [==========================] -  1m - loss: 0.5582 - acc: 0.832\nEpoch 20/20                                                           \n354/354 [=========================] -  1m - loss: 0.5403 - acc: 0.8374\nval_acc: 0.876705979296493\nMaking predictions\nWe already have all predictions calculated for test_subset, let’s recreate the alluvial plot from the original article.\n\n\nlibrary(dplyr)\nlibrary(alluvial)\ndf_validation <- data.frame(\n  pred_class = df$classes[predictions_r],\n  class = df$classes[targets_r]\n)\nx <-  df_validation %>%\n  mutate(correct = pred_class == class) %>%\n  count(pred_class, class, correct)\n\nalluvial(\n  x %>% select(class, pred_class),\n  freq = x$n,\n  col = ifelse(x$correct, \"lightblue\", \"red\"),\n  border = ifelse(x$correct, \"lightblue\", \"red\"),\n  alpha = 0.6,\n  hide = x$n < 20\n)\n\n\n\n\n\nFigure 2: Model performance: true labels <–> predicted labels.\n\n\n\nModel accuracy is 87,7%, somewhat worse than tensorflow version from the original post. Nevertheless, all conclusions from original post still hold.\n\n\n\n",
    "preview": "posts/2021-02-04-simple-audio-classification-with-torch/images/preview.jpg",
    "last_modified": "2024-11-21T15:53:37+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-02-enso-prediction/",
    "title": "Forecasting El Niño-Southern Oscillation (ENSO)",
    "description": "El Niño-Southern Oscillation (ENSO) is an atmospheric phenomenon, located in the tropical Pacific, that greatly affects ecosystems as well as human well-being on a large portion of the globe. We use the convLSTM introduced in a prior post to predict the Niño 3.4 Index from spatially-ordered sequences of sea surface temperatures.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2021-02-02",
    "categories": [
      "Torch",
      "R",
      "Image Recognition & Image Processing",
      "Time Series",
      "Spatial Data"
    ],
    "contents": "\n\nContents\nEl Niño, la Niña\nData\nInput: Sea Surface Temperatures\nTarget: Niño 3.4 Index\n\nPreprocessing\nInput\nTarget\n\nTorch dataset\nDataloaders\nModel\nTraining\nPredictions\nDiscussion\nAppendix\nA1: List of GRB files\nA2: convlstm code\n\n\nToday, we use the convLSTM introduced in a previous post to predict El Niño-Southern Oscillation (ENSO).\nEl Niño, la Niña\nENSO refers to a changing pattern of sea surface temperatures and sea-level pressures occurring in the equatorial Pacific. From its three overall states, probably the best-known is El Niño. El Niño occurs when surface water temperatures in the eastern Pacific are higher than normal, and the strong winds that normally blow from east to west are unusually weak. The opposite conditions are termed La Niña. Everything in-between is classified as normal.\nENSO has great impact on the weather worldwide, and routinely harms ecosystems and societies through storms, droughts and flooding, possibly resulting in famines and economic crises. The best societies can do is try to adapt and mitigate severe consequences. Such efforts are aided by accurate forecasts, the further ahead the better.\nHere, deep learning (DL) can potentially help: Variables like sea surface temperatures and pressures are given on a spatial grid – that of the earth – and as we know, DL is good at extracting spatial (e.g., image) features. For ENSO prediction, architectures like convolutional neural networks (Ham, Kim, and Luo (2019a)) or convolutional-recurrent hybrids1 are habitually used. One such hybrid is just our convLSTM; it operates on sequences of features given on a spatial grid. Today, thus, we’ll be training a model for ENSO forecasting. This model will have a convLSTM for its central ingredient.\nBefore we start, a note. While our model fits well with architectures described in the relevant papers, the same cannot be said for amount of training data used. For reasons of practicality, we use actual observations only; consequently, we end up with a small (relative to the task) dataset. In contrast, research papers tend to make use of climate simulations2, resulting in significantly more data to work with.\nFrom the outset, then, we don’t expect stellar performance. Nevertheless, this should make for an interesting case study, and a useful code template for our readers to apply to their own data.\nData\nWe will attempt to predict monthly average sea surface temperature in the Niño 3.4 region3, as represented by the Niño 3.4 Index, plus categorization as one of El Niño, La Niña or neutral4. Predictions will be based on prior monthly sea surface temperatures spanning a large portion of the globe.\nOn the input side, public and ready-to-use data may be downloaded from Tokyo Climate Center; as to prediction targets, we obtain index and classification here.\nInput and target data both are provided monthly. They intersect in the time period ranging from 1891-01-01 to 2020-08-01; so this is the range of dates we’ll be zooming in on.\nInput: Sea Surface Temperatures\nMonthly sea surface temperatures are provided in a latitude-longitude grid of resolution 1°. Details of how the data were processed are available here.\nData files are available in GRIB format; each file contains averages computed for a single month. We can either download individual files or generate a text file of URLs for download. In case you’d like to follow along with the post, you’ll find the contents of the text file I generated in the appendix. Once you’ve saved these URLs to a file, you can have R get the files for you like so:\n\n\npurrr::walk(\n   readLines(\"files\"),\n   function(f) download.file(url = f, destfile = basename(f))\n)\n\n\nFrom R, we can read GRIB files using stars. For example:\n\n\n# let's just quickly load all libraries we require to start with\n\nlibrary(torch)\nlibrary(tidyverse)\nlibrary(stars)\nlibrary(viridis)\nlibrary(ggthemes)\n\ntorch_manual_seed(777)\n\nread_stars(file.path(grb_dir, \"sst189101.grb\"))\n\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n sst189101.grb   \n Min.   :-274.9  \n 1st Qu.:-272.8  \n Median :-259.1  \n Mean   :-260.0  \n 3rd Qu.:-248.4  \n Max.   :-242.8  \n NA's   :21001   \ndimension(s):\n  from  to offset delta                       refsys point values    \nx    1 360      0     1 Coordinate System importe...    NA   NULL [x]\ny    1 180     90    -1 Coordinate System importe...    NA   NULL [y]\nSo in this GRIB file, we have one attribute - which we know to be sea surface temperature – on a two-dimensional grid. As to the latter, we can complement what stars tells us with additional info found in the documentation:\n\nThe east-west grid points run eastward from 0.5ºE to 0.5ºW, while the north-south grid points run northward from 89.5ºS to 89.5ºN.\n\nWe note a few things we’ll want to do with this data. For one, the temperatures seem to be given in Kelvin, but with minus signs.5 We’ll remove the minus signs and convert to degrees Celsius for convenience. We’ll also have to think about what to do with the NAs that appear for all non-maritime coordinates.\nBefore we get there though, we need to combine data from all files into a single data frame. This adds an additional dimension, time, ranging from 1891/01/01 to 2020/01/12:\n\n\ngrb <- read_stars(\n  file.path(grb_dir, map(readLines(\"files\", warn = FALSE), basename)), along = \"time\") %>%\n  st_set_dimensions(3,\n                    values = seq(as.Date(\"1891-01-01\"), as.Date(\"2020-12-01\"), by = \"months\"),\n                    names = \"time\"\n                    )\n\ngrb\n\n\nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n sst189101.grb   \n Min.   :-274.9  \n 1st Qu.:-273.3  \n Median :-258.8  \n Mean   :-260.0  \n 3rd Qu.:-247.8  \n Max.   :-242.8  \n NA's   :33724   \ndimension(s):\n     from   to offset delta                       refsys point                    values    \nx       1  360      0     1 Coordinate System importe...    NA                      NULL [x]\ny       1  180     90    -1 Coordinate System importe...    NA                      NULL [y]\ntime    1 1560     NA    NA                         Date    NA 1891-01-01,...,2020-12-01    \nLet’s visually inspect the spatial distribution of monthly temperatures for one year, 2020:\n\n\nggplot() +\n  geom_stars(data = grb %>% filter(between(time, as.Date(\"2020-01-01\"), as.Date(\"2020-12-01\"))), alpha = 0.8) +\n  facet_wrap(\"time\") +\n  scale_fill_viridis() +\n  coord_equal() +\n  theme_map() +\n  theme(legend.position = \"none\") \n\n\n\n\n\nFigure 1: Monthly sea surface temperatures, 2020/01/01 - 2020/01/12.\n\n\n\nTarget: Niño 3.4 Index\nFor the Niño 3.4 Index, we download the monthly data and, among the provided features, zoom in on two: the index itself (column NINO34_MEAN) and PHASE, which can be E (El Niño), L (La Niño) or N (neutral).\n\n\nnino <- read_table2(\"ONI_NINO34_1854-2020.txt\", skip = 9) %>%\n  mutate(month = as.Date(paste0(YEAR, \"-\", `MON/MMM`, \"-01\"))) %>%\n  select(month, NINO34_MEAN, PHASE) %>%\n  filter(between(month, as.Date(\"1891-01-01\"), as.Date(\"2020-08-01\"))) %>%\n  mutate(phase_code = as.numeric(as.factor(PHASE)))\n\nnrow(nino)\n\n\n1556\nNext, we look at how to get the data into a format convenient for training and prediction.\nPreprocessing\nInput\nFirst, we remove all input data for points in time where ground truth data are still missing.\n\n\nsst <- grb %>% filter(time <= as.Date(\"2020-08-01\"))\n\n\nNext, as is done by e.g. Ham, Kim, and Luo (2019b), we only use grid points between 55° south and 60° north. This has the additional advantage of reducing memory requirements.\n\n\nsst <- grb %>% filter(between(y,-55, 60))\n\ndim(sst)\n\n\n360, 115, 1560\nAs already alluded to, with the little data we have we can’t expect much in terms of generalization. Still, we set aside a small portion of the data for validation, since we’d like for this post to serve as a useful template to be used with bigger datasets.\n\n\nsst_train <- sst %>% filter(time < as.Date(\"1990-01-01\"))\nsst_valid <- sst %>% filter(time >= as.Date(\"1990-01-01\"))\n\n\nFrom here on, we work with R arrays.\n\n\nsst_train <- as.tbl_cube.stars(sst_train)$mets[[1]]\nsst_valid <- as.tbl_cube.stars(sst_valid)$mets[[1]]\n\n\nConversion to degrees Celsius is not strictly necessary, as initial experiments showed a slight performance increase due to normalizing the input, and we’re going to do that anyway. Still, it reads nicer to humans than Kelvin.\n\n\nsst_train <- sst_train + 273.15\nquantile(sst_train, na.rm = TRUE)\n\n\n     0%     25%     50%     75%    100% \n-1.8000 12.9975 21.8775 26.8200 34.3700 \nNot at all surprisingly, global warming is evident from inspecting temperature distribution on the validation set (which was chosen to span the last thirty-one years).\n\n\nsst_valid <- sst_valid + 273.15\nquantile(sst_valid, na.rm = TRUE)\n\n\n    0%    25%    50%    75%   100% \n-1.800 13.425 22.335 27.240 34.870 \nThe next-to-last step normalizes both sets according to training mean and variance.\n\n\ntrain_mean <- mean(sst_train, na.rm = TRUE)\ntrain_sd <- sd(sst_train, na.rm = TRUE)\n\nsst_train <- (sst_train - train_mean) / train_sd\n\nsst_valid <- (sst_valid - train_mean) / train_sd\n\n\nFinally, what should we do about the NA entries? We set them to zero, the (training set) mean. That may not be enough of an action though: It means we’re feeding the network roughly 30% misleading data. This is something we’re not done with yet.\n\n\nsst_train[is.na(sst_train)] <- 0\nsst_valid[is.na(sst_valid)] <- 0\n\n\nTarget\nThe target data are split analogously. Let’s check though: Are phases (categorizations) distributedly similarly in both sets?\n\n\nnino_train <- nino %>% filter(month < as.Date(\"1990-01-01\"))\nnino_valid <- nino %>% filter(month >= as.Date(\"1990-01-01\"))\n\nnino_train %>% group_by(phase_code, PHASE) %>% summarise(count = n(), avg = mean(NINO34_MEAN))\n\n\n# A tibble: 3 x 4\n# Groups:   phase_code [3]\n  phase_code PHASE count   avg\n       <dbl> <chr> <int> <dbl>\n1          1 E       301  27.7\n2          2 L       333  25.6\n3          3 N       554  26.7\n\n\nnino_valid %>% group_by(phase_code, PHASE) %>% summarise(count = n(), avg = mean(NINO34_MEAN))\n\n\n# A tibble: 3 x 4\n# Groups:   phase_code [3]\n  phase_code PHASE count   avg\n       <dbl> <chr> <int> <dbl>\n1          1 E        93  28.1\n2          2 L        93  25.9\n3          3 N       182  27.2\nThis doesn’t look too bad. Of course, we again see the overall rise in temperature, irrespective of phase.\nLastly, we normalize the index, same as we did for the input data.\n\n\ntrain_mean_nino <- mean(nino_train$NINO34_MEAN)\ntrain_sd_nino <- sd(nino_train$NINO34_MEAN)\n\nnino_train <- nino_train %>% mutate(NINO34_MEAN = scale(NINO34_MEAN, center = train_mean_nino, scale = train_sd_nino))\nnino_valid <- nino_valid %>% mutate(NINO34_MEAN = scale(NINO34_MEAN, center = train_mean_nino, scale = train_sd_nino))\n\n\nOn to the torch dataset.\nTorch dataset\nThe dataset is responsible for correctly matching up inputs and targets.\nOur goal is to take six months of global sea surface temperatures and predict the Niño 3.4 Index for the following month. Input-wise, the model will expect the following format semantics:\nbatch_size * timesteps * width * height * channels, where\nbatch_size is the number of observations worked on in one round of computations,\ntimesteps chains consecutive observations from adjacent months,\nwidth and height together constitute the spatial grid, and\nchannels corresponds to available visual channels in the “image.”\nIn .getitem(), we select the consecutive observations, starting at a given index, and stack them in dimension one. (One, not two, as batches will only start to exist once the dataloader comes into play.)\nNow, what about the target? Our ultimate goal was – is – predicting the Niño 3.4 Index. However, as you see we define three targets: One is the index, as expected; an additional one holds the spatially-gridded sea surface temperatures for the prediction month. Why? Our main instrument, the most prominent constituent of the model, will be a convLSTM, an architecture designed for spatial prediction. Thus, to train it efficiently, we want to give it the opportunity to predict values on a spatial grid. So far so good; but there’s one more target, the phase/category. This was added for experimentation purposes: Maybe predicting both index and phase helps in training?\nFinally, here is the code for the dataset. In our experiments, we based predictions on inputs from the preceding six months (n_timesteps <- 6). This is a parameter you might want to play with, though.\n\n\nn_timesteps <- 6\n\nenso_dataset <- dataset(\n  name = \"enso_dataset\",\n  \n  initialize = function(sst, nino, n_timesteps) {\n    self$sst <- sst\n    self$nino <- nino\n    self$n_timesteps <- n_timesteps\n  },\n  \n  .getitem = function(i) {\n    x <- torch_tensor(self$sst[, , i:(n_timesteps + i - 1)]) # (360, 115, n_timesteps)\n    x <- x$permute(c(3,1,2))$unsqueeze(2) # (n_timesteps, 1, 360, 115))\n    \n    y1 <- torch_tensor(self$sst[, , n_timesteps + i])$unsqueeze(1) # (1, 360, 115)\n    y2 <- torch_tensor(self$nino$NINO34_MEAN[n_timesteps + i])\n    y3 <- torch_tensor(self$nino$phase_code[n_timesteps + i])$squeeze()$to(torch_long())\n    list(x = x, y1 = y1, y2 = y2, y3 = y3)\n  },\n  \n  .length = function() {\n    nrow(self$nino) - n_timesteps\n  }\n  \n)\n\nvalid_ds <- enso_dataset(sst_valid, nino_valid, n_timesteps)\n\n\nDataloaders\nAfter the custom dataset, we create the – pretty typical – dataloaders, making use of a batch size of 4.\n\n\nbatch_size <- 4\n\ntrain_dl <- train_ds %>% dataloader(batch_size = batch_size, shuffle = TRUE)\n\nvalid_dl <- valid_ds %>% dataloader(batch_size = batch_size)\n\n\nNext, we proceed to model creation.\nModel\nThe model’s main ingredient is the convLSTM introduced in a prior post. For convenience, we reproduce the code in the appendix.\nBesides the convLSTM, the model makes use of three convolutional layers, a batchnorm layer and five linear layers. The logic is the following.\nFirst, the convLSTM job is to predict the next month’s sea surface temperatures on the spatial grid. For that, we almost just return its final state, - almost: We use self$conv1 to reduce the number channels to one.\nFor predicting index and phase, we then need to flatten the grid, as we require a single value each. This is where the additional conv layers come in. We do hope they’ll aid in learning, but we also want to reduce the number of parameters a bit, downsizing the grid (strides = 2 and strides = 3, resp.) a bit before the upcoming torch_flatten().\nOnce we have a flat structure, learning is shared between the tasks of index and phase prediction (self$linear), until finally their paths split (self$cont and self$cat, resp.), and they return their separate outputs.\n(The batchnorm? I’ll comment on that in the Discussion.)\n\n\nmodel <- nn_module(\n  \n  initialize = function(channels_in,\n                        convlstm_hidden,\n                        convlstm_kernel,\n                        convlstm_layers) {\n    \n    self$n_layers <- convlstm_layers\n    \n    self$convlstm <- convlstm(\n      input_dim = channels_in,\n      hidden_dims = convlstm_hidden,\n      kernel_sizes = convlstm_kernel,\n      n_layers = convlstm_layers\n    )\n    \n    self$conv1 <-\n      nn_conv2d(\n        in_channels = 32,\n        out_channels = 1,\n        kernel_size = 5,\n        padding = 2\n      )\n    \n    self$conv2 <-\n      nn_conv2d(\n        in_channels = 32,\n        out_channels = 32,\n        kernel_size = 5,\n        stride = 2\n      )\n    \n    self$conv3 <-\n      nn_conv2d(\n        in_channels = 32,\n        out_channels = 32,\n        kernel_size = 5,\n        stride = 3\n      )\n    \n    self$linear <- nn_linear(33408, 64)\n    \n    self$b1 <- nn_batch_norm1d(num_features = 64)\n        \n    self$cont <- nn_linear(64, 128)\n    self$cat <- nn_linear(64, 128)\n    \n    self$cont_output <- nn_linear(128, 1)\n    self$cat_output <- nn_linear(128, 3)\n    \n  },\n  \n  forward = function(x) {\n    \n    ret <- self$convlstm(x)\n    layer_last_states <- ret[[2]]\n    last_hidden <- layer_last_states[[self$n_layers]][[1]]\n    \n    next_sst <- last_hidden %>% self$conv1() \n    \n    c2 <- last_hidden %>% self$conv2() \n    c3 <- c2 %>% self$conv3() \n    \n    flat <- torch_flatten(c3, start_dim = 2)\n    common <- self$linear(flat) %>% self$b3() %>% nnf_relu()\n\n    next_temp <- common %>% self$cont() %>% nnf_relu() %>% self$cont_output()\n    next_nino <- common %>% self$cat() %>% nnf_relu() %>% self$cat_output()\n    \n    list(next_sst, next_temp, next_nino)\n    \n  }\n  \n)\n\n\nNext, we instantiate a pretty small-ish model. You’re more than welcome to experiment with larger models, but training time as well as GPU memory requirements will increase.\n\n\nnet <- model(\n  channels_in = 1,\n  convlstm_hidden = c(16, 16, 32),\n  convlstm_kernel = c(3, 3, 5),\n  convlstm_layers = 3\n)\n\ndevice <- torch_device(if (cuda_is_available()) \"cuda\" else \"cpu\")\n\nnet <- net$to(device = device)\nnet\n\n\nAn `nn_module` containing 2,389,605 parameters.\n\n── Modules ───────────────────────────────────────────────────────────────────────────────\n● convlstm: <nn_module> #182,080 parameters\n● conv1: <nn_conv2d> #801 parameters\n● conv2: <nn_conv2d> #25,632 parameters\n● conv3: <nn_conv2d> #25,632 parameters\n● linear: <nn_linear> #2,138,176 parameters\n● b1: <nn_batch_norm1d> #128 parameters\n● cont: <nn_linear> #8,320 parameters\n● cat: <nn_linear> #8,320 parameters\n● cont_output: <nn_linear> #129 parameters\n● cat_output: <nn_linear> #387 parameters\nTraining\nWe have three model outputs. How should we combine the losses?\nGiven that the main goal is predicting the index, and the other two outputs are essentially means to an end, I found the following combination rather effective:\n# weight for sea surface temperature prediction\nlw_sst <- 0.2\n\n# weight for prediction of El Nino 3.4 Index\nlw_temp <- 0.4\n\n# weight for phase prediction\nlw_nino <- 0.4\nThe training process follows the pattern seen in all torch posts so far: For each epoch, loop over the training set, backpropagate, check performance on validation set.\nBut, when we did the pre-processing, we were aware of an imminent problem: the missing temperatures for continental areas, which we set to zero. As a sole measure, this approach is clearly insufficient. What if we had chosen to use latitude-dependent averages? Or interpolation? Both may be better than a global average, but both have their problems as well. Let’s at least alleviate negative consequences by not using the respective pixels for spatial loss calculation. This is taken care of by the following line below:\n\n\nsst_loss <- nnf_mse_loss(sst_output[sst_target != 0], sst_target[sst_target != 0])\n\n\nHere, then, is the complete training code.\n\n\noptimizer <- optim_adam(net$parameters, lr = 0.001)\n\nnum_epochs <- 50\n\ntrain_batch <- function(b) {\n  \n  optimizer$zero_grad()\n  output <- net(b$x$to(device = device))\n  \n  sst_output <- output[[1]]\n  sst_target <- b$y1$to(device = device)\n  \n  sst_loss <- nnf_mse_loss(sst_output[sst_target != 0], sst_target[sst_target != 0])\n  temp_loss <- nnf_mse_loss(output[[2]], b$y2$to(device = device))\n  nino_loss <- nnf_cross_entropy(output[[3]], b$y3$to(device = device))\n  \n  loss <- lw_sst * sst_loss + lw_temp * temp_loss + lw_nino * nino_loss\n  loss$backward()\n  optimizer$step()\n\n  list(sst_loss$item(), temp_loss$item(), nino_loss$item(), loss$item())\n  \n}\n\nvalid_batch <- function(b) {\n  \n  output <- net(b$x$to(device = device))\n  \n  sst_output <- output[[1]]\n  sst_target <- b$y1$to(device = device)\n  \n  sst_loss <- nnf_mse_loss(sst_output[sst_target != 0], sst_target[sst_target != 0])\n  temp_loss <- nnf_mse_loss(output[[2]], b$y2$to(device = device))\n  nino_loss <- nnf_cross_entropy(output[[3]], b$y3$to(device = device))\n  \n  loss <-\n    lw_sst * sst_loss + lw_temp * temp_loss + lw_nino * nino_loss\n\n  list(sst_loss$item(),\n       temp_loss$item(),\n       nino_loss$item(),\n       loss$item())\n}\n\nfor (epoch in 1:num_epochs) {\n  \n  net$train()\n  \n  train_loss_sst <- c()\n  train_loss_temp <- c()\n  train_loss_nino <- c()\n  train_loss <- c()\n\n  coro::loop(for (b in train_dl) {\n    losses <- train_batch(b)\n    train_loss_sst <- c(train_loss_sst, losses[[1]])\n    train_loss_temp <- c(train_loss_temp, losses[[2]])\n    train_loss_nino <- c(train_loss_nino, losses[[3]])\n    train_loss <- c(train_loss, losses[[4]])\n  })\n  \n  cat(\n    sprintf(\n      \"\\nEpoch %d, training: loss: %3.3f sst: %3.3f temp: %3.3f nino: %3.3f \\n\",\n      epoch, mean(train_loss), mean(train_loss_sst), mean(train_loss_temp), mean(train_loss_nino)\n    )\n  )\n  \n  net$eval()\n  \n  valid_loss_sst <- c()\n  valid_loss_temp <- c()\n  valid_loss_nino <- c()\n  valid_loss <- c()\n\n  coro::loop(for (b in valid_dl) {\n    losses <- valid_batch(b)\n    valid_loss_sst <- c(valid_loss_sst, losses[[1]])\n    valid_loss_temp <- c(valid_loss_temp, losses[[2]])\n    valid_loss_nino <- c(valid_loss_nino, losses[[3]])\n    valid_loss <- c(valid_loss, losses[[4]])\n    \n  })\n  \n  cat(\n    sprintf(\n      \"\\nEpoch %d, validation: loss: %3.3f sst: %3.3f temp: %3.3f nino: %3.3f \\n\",\n      epoch, mean(valid_loss), mean(valid_loss_sst), mean(valid_loss_temp), mean(valid_loss_nino)\n    )\n  )\n  \n  torch_save(net, paste0(\n    \"model_\", epoch, \"_\", round(mean(train_loss), 3), \"_\", round(mean(valid_loss), 3), \".pt\"\n  ))\n  \n}\n\n\nWhen I ran this, performance on the training set decreased in a not-too-fast, but continuous way, while validation set performance kept fluctuating. For reference, total (composite) losses looked like this:\nEpoch     Training    Validation\n   \n   10        0.336         0.633\n   20        0.233         0.295\n   30        0.135         0.461\n   40        0.099         0.903\n   50        0.061         0.727\nThinking of the size of the validation set - thirty-one years, or equivalently, 372 data points – those fluctuations may not be all too surprising.\nPredictions\nNow losses tend to be abstract; let’s see what actually gets predicted. We obtain predictions for index values and phases like so …\n\n\nnet$eval()\n\npred_index <- c()\npred_phase <- c()\n\ncoro::loop(for (b in valid_dl) {\n\n  output <- net(b$x$to(device = device))\n\n  pred_index <- c(pred_index, output[[2]]$to(device = \"cpu\"))\n  pred_phase <- rbind(pred_phase, as.matrix(output[[3]]$to(device = \"cpu\")))\n\n})\n\n\n… and combine these with the ground truth, stripping off the first six rows (six was the number of timesteps used as predictors):\n\n\nvalid_perf <- data.frame(\n  actual_temp = nino_valid$NINO34_MEAN[(batch_size + 1):nrow(nino_valid)] * train_sd_nino + train_mean_nino,\n  actual_nino = factor(nino_valid$phase_code[(batch_size + 1):nrow(nino_valid)]),\n  pred_temp = pred_index * train_sd_nino + train_mean_nino,\n  pred_nino = factor(pred_phase %>% apply(1, which.max))\n)\n\n\nFor the phase, we can generate a confusion matrix:\n\n\nyardstick::conf_mat(valid_perf, actual_nino, pred_nino)\n\n\n          Truth\nPrediction   1   2   3\n         1  70   0  43\n         2   0  47  10\n         3  23  46 123\nThis looks better than expected (based on the losses). Phases 1 and 2 correspond to El Niño and La Niña, respectively, and these get sharply separated.\nWhat about the Niño 3.4 Index? Let’s plot predictions versus ground truth:\n\n\nvalid_perf <- valid_perf %>% \n  select(actual = actual_temp, predicted = pred_temp) %>% \n  add_column(month = seq(as.Date(\"1990-07-01\"), as.Date(\"2020-08-01\"), by = \"months\")) %>%\n  pivot_longer(-month, names_to = \"Index\", values_to = \"temperature\")\n\nggplot(valid_perf, aes(x = month, y = temperature, color = Index)) +\n  geom_line() +\n  scale_color_manual(values = c(\"#006D6F\", \"#B2FFFF\")) +\n  theme_classic()\n\n\n\n\n\nFigure 2: Nino 3.4 Index: Ground truth vs. predictions (validation set).\n\n\n\nThis does not look bad either. However, we need to keep in mind that we’re predicting just a single time step ahead. We probably should not overestimate the results. Which leads directly to the discussion.\nDiscussion\nWhen working with small amounts of data, a lot can be learned by quick-ish experimentation. However, when at the same time, the task is complex, one should be cautious extrapolating.\nFor example, well-established regularizers such as batchnorm and dropout, while intended to improve generalization to the validation set, may turn out to severely impede training itself. This is the story behind the one batchnorm layer I kept (I did try having more), and it is also why there is no dropout.\nOne lesson to learn from this experience then is: Make sure the amount of data matches the complexity of the task. This is what we see in the ENSO prediction papers published on arxiv.\nIf we should treat the results with caution, why even publish the post?\nFor one, it shows an application of convLSTM to real-world data, employing a reasonably complex architecture and illustrating techniques like custom losses and loss masking. Similar architectures and strategies should be applicable to a wide range of real-world tasks – basically, whenever predictors in a time-series problem are given on a spatial grid.\nSecondly, the application itself – forecasting an atmospheric phenomenon that greatly affects ecosystems as well as human well-being – seems like an excellent use of deep learning. Applications like these stand out as all the more worthwhile as the same cannot be said of everything deep learning is – and will be, barring effective regulation – used for.\nThanks for reading!\nAppendix\nA1: List of GRB files\nTo be put into a text file for use with purrr::walk( … download.file … ).\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189101.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189102.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189103.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189104.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189105.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189106.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189107.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189108.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189109.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189110.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189111.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189112.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189201.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189202.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189203.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189204.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189205.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189206.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189207.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189208.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189209.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189210.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189211.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189212.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189301.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189302.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189303.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189304.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189305.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189306.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189307.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189308.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189309.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189310.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189311.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189312.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189401.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189402.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189403.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189404.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189405.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189406.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189407.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189408.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189409.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189410.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189411.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189412.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189501.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189502.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189503.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189504.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189505.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189506.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189507.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189508.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189509.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189510.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189511.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189512.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189601.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189602.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189603.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189604.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189605.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189606.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189607.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189608.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189609.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189610.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189611.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189612.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189701.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189702.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189703.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189704.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189705.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189706.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189707.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189708.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189709.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189710.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189711.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189712.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189801.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189802.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189803.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189804.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189805.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189806.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189807.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189808.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189809.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189810.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189811.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189812.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189901.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189902.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189903.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189904.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189905.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189906.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189907.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189908.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189909.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189910.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189911.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1891-1899/sst189912.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190001.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190002.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190003.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190004.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190005.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190006.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190007.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190008.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190009.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190010.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190011.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190012.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190101.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190102.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190103.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190104.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190105.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190106.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190107.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190108.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190109.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190110.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190111.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190112.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190201.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190202.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190203.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190204.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190205.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190206.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190207.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190208.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190209.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190210.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190211.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190212.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190301.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190302.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190303.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190304.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190305.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190306.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190307.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190308.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190309.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190310.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190311.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190312.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190401.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190402.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190403.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190404.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190405.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190406.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190407.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190408.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190409.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190410.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190411.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190412.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190501.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190502.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190503.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190504.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190505.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190506.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190507.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190508.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190509.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190510.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190511.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190512.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190601.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190602.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190603.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190604.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190605.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190606.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190607.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190608.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190609.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190610.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190611.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190612.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190701.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190702.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190703.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190704.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190705.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190706.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190707.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190708.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190709.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190710.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190711.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190712.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190801.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190802.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190803.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190804.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190805.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190806.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190807.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190808.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190809.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190810.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190811.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190812.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190901.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190902.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190903.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190904.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190905.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190906.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190907.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190908.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190909.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190910.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190911.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1900-1909/sst190912.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191001.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191002.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191003.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191004.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191005.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191006.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191007.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191008.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191009.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191010.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191011.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191012.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191101.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191102.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191103.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191104.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191105.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191106.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191107.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191108.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191109.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191110.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191111.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191112.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191201.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191202.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191203.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191204.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191205.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191206.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191207.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191208.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191209.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191210.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191211.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191212.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191301.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191302.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191303.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191304.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191305.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191306.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191307.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191308.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191309.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191310.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191311.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191312.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191401.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191402.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191403.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191404.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191405.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191406.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191407.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191408.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191409.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191410.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191411.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191412.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191501.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191502.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191503.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191504.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191505.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191506.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191507.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191508.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191509.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191510.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191511.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191512.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191601.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191602.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191603.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191604.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191605.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191606.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191607.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191608.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191609.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191610.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191611.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191612.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191701.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191702.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191703.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191704.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191705.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191706.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191707.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191708.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191709.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191710.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191711.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191712.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191801.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191802.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191803.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191804.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191805.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191806.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191807.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191808.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191809.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191810.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191811.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191812.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191901.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191902.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191903.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191904.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191905.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191906.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191907.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191908.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191909.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191910.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191911.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1910-1919/sst191912.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192001.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192002.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192003.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192004.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192005.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192006.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192007.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192008.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192009.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192010.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192011.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192012.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192101.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192102.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192103.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192104.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192105.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192106.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192107.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192108.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192109.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192110.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192111.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192112.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192201.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192202.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192203.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192204.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192205.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192206.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192207.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192208.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192209.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192210.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192211.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192212.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192301.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192302.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192303.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192304.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192305.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192306.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192307.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192308.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192309.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192310.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192311.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192312.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192401.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192402.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192403.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192404.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192405.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192406.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192407.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192408.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192409.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192410.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192411.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192412.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192501.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192502.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192503.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192504.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192505.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192506.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192507.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192508.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192509.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192510.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192511.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192512.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192601.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192602.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192603.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192604.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192605.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192606.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192607.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192608.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192609.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192610.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192611.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192612.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192701.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192702.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192703.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192704.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192705.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192706.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192707.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192708.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192709.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192710.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192711.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192712.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192801.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192802.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192803.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192804.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192805.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192806.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192807.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192808.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192809.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192810.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192811.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192812.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192901.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192902.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192903.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192904.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192905.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192906.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192907.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192908.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192909.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192910.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192911.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1920-1929/sst192912.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193001.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193002.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193003.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193004.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193005.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193006.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193007.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193008.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193009.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193010.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193011.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193012.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193101.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193102.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193103.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193104.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193105.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193106.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193107.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193108.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193109.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193110.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193111.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193112.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193201.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193202.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193203.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193204.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193205.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193206.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193207.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193208.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193209.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193210.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193211.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193212.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193301.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193302.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193303.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193304.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193305.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193306.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193307.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193308.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193309.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193310.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193311.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193312.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193401.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193402.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193403.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193404.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193405.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193406.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193407.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193408.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193409.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193410.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193411.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193412.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193501.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193502.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193503.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193504.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193505.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193506.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193507.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193508.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193509.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193510.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193511.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193512.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193601.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193602.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193603.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193604.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193605.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193606.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193607.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193608.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193609.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193610.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193611.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193612.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193701.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193702.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193703.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193704.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193705.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193706.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193707.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193708.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193709.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193710.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193711.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193712.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193801.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193802.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193803.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193804.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193805.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193806.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193807.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193808.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193809.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193810.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193811.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193812.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193901.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193902.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193903.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193904.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193905.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193906.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193907.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193908.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193909.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193910.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193911.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1930-1939/sst193912.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194001.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194002.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194003.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194004.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194005.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194006.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194007.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194008.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194009.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194010.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194011.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194012.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194101.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194102.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194103.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194104.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194105.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194106.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194107.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194108.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194109.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194110.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194111.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194112.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194201.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194202.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194203.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194204.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194205.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194206.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194207.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194208.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194209.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194210.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194211.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194212.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194301.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194302.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194303.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194304.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194305.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194306.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194307.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194308.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194309.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194310.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194311.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194312.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194401.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194402.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194403.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194404.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194405.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194406.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194407.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194408.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194409.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194410.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194411.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194412.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194501.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194502.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194503.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194504.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194505.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194506.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194507.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194508.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194509.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194510.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194511.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194512.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194601.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194602.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194603.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194604.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194605.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194606.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194607.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194608.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194609.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194610.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194611.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194612.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194701.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194702.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194703.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194704.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194705.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194706.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194707.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194708.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194709.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194710.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194711.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194712.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194801.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194802.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194803.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194804.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194805.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194806.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194807.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194808.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194809.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194810.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194811.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194812.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194901.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194902.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194903.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194904.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194905.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194906.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194907.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194908.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194909.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194910.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194911.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1940-1949/sst194912.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195001.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195002.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195003.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195004.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195005.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195006.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195007.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195008.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195009.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195010.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195011.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195012.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195101.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195102.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195103.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195104.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195105.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195106.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195107.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195108.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195109.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195110.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195111.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195112.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195201.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195202.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195203.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195204.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195205.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195206.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195207.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195208.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195209.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195210.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195211.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195212.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195301.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195302.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195303.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195304.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195305.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195306.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195307.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195308.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195309.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195310.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195311.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195312.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195401.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195402.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195403.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195404.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195405.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195406.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195407.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195408.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195409.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195410.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195411.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195412.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195501.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195502.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195503.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195504.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195505.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195506.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195507.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195508.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195509.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195510.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195511.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195512.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195601.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195602.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195603.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195604.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195605.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195606.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195607.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195608.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195609.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195610.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195611.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195612.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195701.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195702.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195703.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195704.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195705.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195706.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195707.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195708.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195709.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195710.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195711.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195712.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195801.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195802.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195803.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195804.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195805.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195806.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195807.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195808.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195809.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195810.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195811.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195812.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195901.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195902.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195903.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195904.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195905.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195906.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195907.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195908.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195909.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195910.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195911.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1950-1959/sst195912.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196001.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196002.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196003.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196004.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196005.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196006.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196007.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196008.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196009.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196010.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196011.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196012.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196101.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196102.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196103.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196104.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196105.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196106.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196107.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196108.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196109.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196110.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196111.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196112.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196201.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196202.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196203.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196204.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196205.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196206.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196207.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196208.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196209.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196210.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196211.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196212.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196301.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196302.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196303.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196304.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196305.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196306.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196307.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196308.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196309.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196310.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196311.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196312.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196401.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196402.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196403.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196404.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196405.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196406.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196407.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196408.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196409.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196410.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196411.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196412.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196501.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196502.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196503.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196504.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196505.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196506.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196507.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196508.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196509.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196510.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196511.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196512.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196601.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196602.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196603.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196604.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196605.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196606.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196607.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196608.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196609.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196610.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196611.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196612.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196701.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196702.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196703.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196704.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196705.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196706.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196707.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196708.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196709.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196710.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196711.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196712.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196801.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196802.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196803.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196804.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196805.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196806.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196807.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196808.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196809.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196810.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196811.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196812.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196901.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196902.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196903.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196904.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196905.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196906.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196907.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196908.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196909.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196910.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196911.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1960-1969/sst196912.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197001.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197002.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197003.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197004.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197005.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197006.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197007.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197008.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197009.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197010.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197011.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197012.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197101.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197102.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197103.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197104.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197105.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197106.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197107.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197108.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197109.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197110.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197111.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197112.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197201.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197202.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197203.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197204.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197205.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197206.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197207.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197208.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197209.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197210.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197211.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197212.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197301.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197302.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197303.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197304.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197305.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197306.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197307.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197308.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197309.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197310.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197311.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197312.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197401.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197402.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197403.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197404.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197405.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197406.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197407.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197408.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197409.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197410.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197411.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197412.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197501.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197502.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197503.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197504.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197505.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197506.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197507.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197508.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197509.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197510.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197511.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197512.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197601.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197602.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197603.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197604.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197605.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197606.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197607.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197608.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197609.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197610.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197611.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197612.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197701.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197702.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197703.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197704.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197705.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197706.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197707.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197708.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197709.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197710.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197711.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197712.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197801.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197802.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197803.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197804.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197805.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197806.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197807.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197808.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197809.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197810.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197811.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197812.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197901.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197902.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197903.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197904.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197905.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197906.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197907.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197908.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197909.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197910.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197911.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1970-1979/sst197912.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198001.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198002.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198003.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198004.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198005.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198006.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198007.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198008.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198009.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198010.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198011.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198012.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198101.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198102.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198103.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198104.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198105.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198106.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198107.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198108.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198109.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198110.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198111.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198112.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198201.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198202.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198203.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198204.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198205.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198206.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198207.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198208.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198209.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198210.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198211.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198212.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198301.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198302.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198303.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198304.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198305.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198306.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198307.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198308.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198309.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198310.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198311.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198312.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198401.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198402.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198403.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198404.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198405.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198406.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198407.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198408.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198409.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198410.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198411.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198412.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198501.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198502.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198503.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198504.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198505.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198506.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198507.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198508.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198509.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198510.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198511.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198512.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198601.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198602.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198603.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198604.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198605.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198606.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198607.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198608.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198609.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198610.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198611.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198612.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198701.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198702.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198703.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198704.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198705.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198706.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198707.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198708.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198709.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198710.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198711.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198712.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198801.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198802.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198803.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198804.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198805.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198806.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198807.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198808.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198809.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198810.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198811.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198812.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198901.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198902.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198903.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198904.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198905.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198906.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198907.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198908.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198909.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198910.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198911.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1980-1989/sst198912.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199001.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199002.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199003.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199004.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199005.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199006.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199007.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199008.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199009.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199010.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199011.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199012.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199101.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199102.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199103.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199104.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199105.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199106.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199107.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199108.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199109.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199110.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199111.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199112.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199201.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199202.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199203.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199204.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199205.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199206.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199207.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199208.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199209.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199210.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199211.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199212.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199301.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199302.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199303.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199304.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199305.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199306.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199307.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199308.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199309.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199310.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199311.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199312.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199401.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199402.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199403.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199404.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199405.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199406.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199407.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199408.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199409.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199410.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199411.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199412.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199501.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199502.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199503.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199504.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199505.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199506.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199507.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199508.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199509.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199510.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199511.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199512.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199601.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199602.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199603.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199604.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199605.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199606.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199607.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199608.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199609.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199610.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199611.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199612.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199701.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199702.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199703.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199704.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199705.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199706.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199707.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199708.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199709.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199710.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199711.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199712.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199801.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199802.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199803.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199804.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199805.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199806.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199807.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199808.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199809.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199810.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199811.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199812.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199901.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199902.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199903.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199904.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199905.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199906.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199907.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199908.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199909.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199910.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199911.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/1990-1999/sst199912.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200001.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200002.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200003.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200004.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200005.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200006.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200007.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200008.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200009.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200010.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200011.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200012.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200101.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200102.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200103.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200104.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200105.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200106.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200107.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200108.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200109.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200110.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200111.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200112.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200201.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200202.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200203.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200204.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200205.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200206.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200207.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200208.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200209.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200210.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200211.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200212.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200301.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200302.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200303.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200304.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200305.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200306.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200307.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200308.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200309.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200310.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200311.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200312.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200401.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200402.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200403.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200404.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200405.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200406.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200407.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200408.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200409.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200410.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200411.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200412.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200501.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200502.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200503.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200504.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200505.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200506.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200507.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200508.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200509.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200510.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200511.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200512.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200601.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200602.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200603.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200604.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200605.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200606.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200607.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200608.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200609.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200610.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200611.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200612.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200701.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200702.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200703.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200704.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200705.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200706.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200707.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200708.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200709.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200710.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200711.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200712.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200801.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200802.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200803.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200804.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200805.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200806.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200807.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200808.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200809.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200810.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200811.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200812.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200901.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200902.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200903.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200904.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200905.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200906.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200907.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200908.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200909.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200910.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200911.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2000-2009/sst200912.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201001.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201002.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201003.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201004.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201005.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201006.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201007.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201008.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201009.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201010.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201011.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201012.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201101.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201102.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201103.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201104.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201105.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201106.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201107.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201108.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201109.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201110.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201111.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201112.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201201.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201202.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201203.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201204.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201205.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201206.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201207.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201208.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201209.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201210.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201211.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201212.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201301.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201302.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201303.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201304.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201305.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201306.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201307.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201308.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201309.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201310.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201311.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201312.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201401.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201402.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201403.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201404.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201405.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201406.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201407.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201408.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201409.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201410.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201411.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201412.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201501.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201502.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201503.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201504.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201505.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201506.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201507.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201508.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201509.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201510.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201511.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201512.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201601.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201602.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201603.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201604.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201605.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201606.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201607.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201608.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201609.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201610.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201611.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201612.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201701.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201702.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201703.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201704.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201705.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201706.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201707.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201708.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201709.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201710.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201711.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201712.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201801.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201802.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201803.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201804.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201805.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201806.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201807.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201808.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201809.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201810.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201811.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201812.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201901.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201902.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201903.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201904.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201905.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201906.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201907.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201908.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201909.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201910.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201911.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2010-2019/sst201912.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2020-2029/sst202001.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2020-2029/sst202002.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2020-2029/sst202003.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2020-2029/sst202004.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2020-2029/sst202005.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2020-2029/sst202006.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2020-2029/sst202007.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2020-2029/sst202008.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2020-2029/sst202009.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2020-2029/sst202010.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2020-2029/sst202011.grb\nhttps://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/gpvdata/2020-2029/sst202012.grb\nA2: convlstm code\nFor an in-depth explanation of convlstm, see the blog post.\n\n\nlibrary(torch)\nlibrary(torchvision)\n\nconvlstm_cell <- nn_module(\n  \n  initialize = function(input_dim, hidden_dim, kernel_size, bias) {\n    \n    self$hidden_dim <- hidden_dim\n    \n    padding <- kernel_size %/% 2\n    \n    self$conv <- nn_conv2d(\n      in_channels = input_dim + self$hidden_dim,\n      # for each of input, forget, output, and cell gates\n      out_channels = 4 * self$hidden_dim,\n      kernel_size = kernel_size,\n      padding = padding,\n      bias = bias\n    )\n  },\n  \n  forward = function(x, prev_states) {\n\n    h_prev <- prev_states[[1]]\n    c_prev <- prev_states[[2]]\n    \n    combined <- torch_cat(list(x, h_prev), dim = 2)  # concatenate along channel axis\n    combined_conv <- self$conv(combined)\n    gate_convs <- torch_split(combined_conv, self$hidden_dim, dim = 2)\n    cc_i <- gate_convs[[1]]\n    cc_f <- gate_convs[[2]]\n    cc_o <- gate_convs[[3]]\n    cc_g <- gate_convs[[4]]\n    \n    # input, forget, output, and cell gates (corresponding to torch's LSTM)\n    i <- torch_sigmoid(cc_i)\n    f <- torch_sigmoid(cc_f)\n    o <- torch_sigmoid(cc_o)\n    g <- torch_tanh(cc_g)\n    \n    # cell state\n    c_next <- f * c_prev + i * g\n    # hidden state\n    h_next <- o * torch_tanh(c_next)\n    \n    list(h_next, c_next)\n    \n  },\n  \n  init_hidden = function(batch_size, height, width) {\n    list(torch_zeros(batch_size, self$hidden_dim, height, width, device = self$conv$weight$device),\n         torch_zeros(batch_size, self$hidden_dim, height, width, device = self$conv$weight$device))\n  }\n)\n\nconvlstm <- nn_module(\n  \n  initialize = function(input_dim, hidden_dims, kernel_sizes, n_layers, bias = TRUE) {\n \n    self$n_layers <- n_layers\n    \n    self$cell_list <- nn_module_list()\n    \n    for (i in 1:n_layers) {\n      cur_input_dim <- if (i == 1) input_dim else hidden_dims[i - 1]\n      self$cell_list$append(convlstm_cell(cur_input_dim, hidden_dims[i], kernel_sizes[i], bias))\n    }\n  },\n  \n  # we always assume batch-first\n  forward = function(x) {\n    \n    batch_size <- x$size()[1]\n    seq_len <- x$size()[2]\n    height <- x$size()[4]\n    width <- x$size()[5]\n   \n    # initialize hidden states\n    init_hidden <- vector(mode = \"list\", length = self$n_layers)\n    for (i in 1:self$n_layers) {\n      init_hidden[[i]] <- self$cell_list[[i]]$init_hidden(batch_size, height, width)\n    }\n    \n    # list containing the outputs, of length seq_len, for each layer\n    # this is the same as h, at each step in the sequence\n    layer_output_list <- vector(mode = \"list\", length = self$n_layers)\n    \n    # list containing the last states (h, c) for each layer\n    layer_state_list <- vector(mode = \"list\", length = self$n_layers)\n\n    cur_layer_input <- x\n    hidden_states <- init_hidden\n    \n    # loop over layers\n    for (i in 1:self$n_layers) {\n      \n      # every layer's hidden state starts from 0 (non-stateful)\n      h_c <- hidden_states[[i]]\n      h <- h_c[[1]]\n      c <- h_c[[2]]\n      # outputs, of length seq_len, for this layer\n      # equivalently, list of h states for each time step\n      output_sequence <- vector(mode = \"list\", length = seq_len)\n      \n      # loop over timesteps\n      for (t in 1:seq_len) {\n        h_c <- self$cell_list[[i]](cur_layer_input[ , t, , , ], list(h, c))\n        h <- h_c[[1]]\n        c <- h_c[[2]]\n        # keep track of output (h) for every timestep\n        # h has dim (batch_size, hidden_size, height, width)\n        output_sequence[[t]] <- h\n      }\n\n      # stack hs for all timesteps over seq_len dimension\n      # stacked_outputs has dim (batch_size, seq_len, hidden_size, height, width)\n      # same as input to forward (x)\n      stacked_outputs <- torch_stack(output_sequence, dim = 2)\n      \n      # pass the list of outputs (hs) to next layer\n      cur_layer_input <- stacked_outputs\n      \n      # keep track of list of outputs or this layer\n      layer_output_list[[i]] <- stacked_outputs\n      # keep track of last state for this layer\n      layer_state_list[[i]] <- list(h, c)\n    }\n \n    list(layer_output_list, layer_state_list)\n      \n  }\n    \n)\n\n\n\n\n\nHam, Yoo-Geun, Jeong-Hwan Kim, and Jing-Jia Luo. 2019b. “Deep learning for multi-year ENSO forecasts” 573 (7775): 568–72. https://doi.org/10.1038/s41586-019-1559-7.\n\n\n———. 2019a. “Deep learning for multi-year ENSO forecasts” 573 (7775): 568–72. https://doi.org/10.1038/s41586-019-1559-7.\n\n\nE.g., Forecasting El Niño with Convolutional and Recurrent Neural Networks.↩︎\nE.g., CMIP5, CNRM-CM5, or HADGEM2-ES.↩︎\nThis region extends over latitudes from 5° south to 5° north and longitudes from 120° west to 170° west.↩︎\nThat classification is based on ONI (Oceanic Niño Index), a measure representing 3-month average anomalies in the Niño 3.4 Index.↩︎\nThis may also be an artifact produced by the software stack involved in reading the file.↩︎\n",
    "preview": "posts/2021-02-02-enso-prediction/images/pic.jpg",
    "last_modified": "2024-11-21T15:48:32+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-17-torch-convlstm/",
    "title": "Convolutional LSTM for spatial forecasting",
    "description": "In forecasting spatially-determined phenomena (the weather, say, or the next frame in a movie), we want to model temporal evolution, ideally using recurrence relations. At the same time, we'd like to efficiently extract spatial features, something that is normally done with convolutional filters. Ideally then, we'd have at our disposal an architecture that is both recurrent and convolutional. In this post, we build a convolutional LSTM with torch.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-12-17",
    "categories": [
      "Torch",
      "R",
      "Image Recognition & Image Processing",
      "Time Series",
      "Spatial Data"
    ],
    "contents": "\n\nContents\nWhat to expect from this post\nA torch convLSTM\nInput and output\nInterlude: Outputs, states, hidden values … what’s what?\nconvLSTM, the plan\nA single step: convlstm_cell\nIteration over time steps: convlstm\nCalling the convlstm\nSanity-checking the convlstm\n\nAppendix\nKeras\ntorch\n\n\nThis post is the first in a loose series exploring forecasting of spatially-determined data over time. By spatially-determined I mean that whatever the quantities we’re trying to predict – be they univariate or multivariate time series, of spatial dimensionality or not – the input data are given on a spatial grid.\nFor example, the input could be atmospheric measurements, such as sea surface temperature or pressure, given at some set of latitudes and longitudes. The target to be predicted could then span that same (or another) grid. Alternatively, it could be a univariate time series, like a meteorological index.\nBut wait a second, you may be thinking. For time-series prediction, we have that time-honored set of recurrent architectures (e.g., LSTM, GRU), right? Right. We do; but, once we feed spatial data to an RNN, treating different locations as different input features, we lose an essential structural relationship. Importantly, we need to operate in both space and time. We want both: recurrence relations and convolutional filters. Enter convolutional RNNs.\nWhat to expect from this post\nToday, we won’t jump into real-world applications just yet. Instead, we’ll take our time to build a convolutional LSTM (henceforth: convLSTM) in torch. For one, we have to – there is no official PyTorch implementation.\n\nKeras, on the other hand, has one. If you’re interested in quickly playing around with a Keras convLSTM, check out this nice example.\nWhat’s more, this post can serve as an introduction to building your own modules. This is something you may be familiar with from Keras or not – depending on whether you’ve used custom models or rather, preferred the declarative define -> compile -> fit style. (Yes, I’m implying there’s some transfer going on if one comes to torch from Keras custom training. Syntactic and semantic details may be different, but both share the object-oriented style that allows for great flexibility and control.)\nLast but not least, we’ll also use this as a hands-on experience with RNN architectures (the LSTM, specifically). While the general concept of recurrence may be easy to grasp, it is not necessarily self-evident how those architectures should, or could, be coded. Personally, I find that independent of the framework used, RNN-related documentation leaves me confused. What exactly is being returned from calling an LSTM, or a GRU? (In Keras this depends on how you’ve defined the layer in question.) I suspect that once we’ve decided what we want to return, the actual code won’t be that complicated. Consequently, we’ll take a detour clarifying what it is that torch and Keras are giving us. Implementing our convLSTM will be a lot more straightforward thereafter.\nA torch convLSTM\nThe code discussed here may be found on GitHub. (Depending on when you’re reading this, the code in that repository may have evolved though.)\nMy starting point was one of the PyTorch implementations found on the net, namely, this one. If you search for “PyTorch convGRU” or “PyTorch convLSTM”, you will find stunning discrepancies in how these are realized – discrepancies not just in syntax and/or engineering ambition, but on the semantic level, right at the center of what the architectures may be expected to do. As they say, let the buyer beware. (Regarding the implementation I ended up porting, I am confident that while numerous optimizations will be possible, the basic mechanism matches my expectations.)\nWhat do I expect? Let’s approach this task in a top-down way.\nInput and output\nThe convLSTM’s input will be a time series of spatial data, each observation being of size (time steps, channels, height, width).\nCompare this with the usual RNN input format, be it in torch or Keras. In both frameworks, RNNs expect tensors of size (timesteps, input_dim)1. input_dim is \\(1\\) for univariate time series and greater than \\(1\\) for multivariate ones. Conceptually, we may match this to convLSTM’s channels dimension: There could be a single channel, for temperature, say – or there could be several, such as for pressure, temperature, and humidity. The two additional dimensions found in convLSTM, height and width, are spatial indexes into the data.\nIn sum, we want to be able to pass data that:\nconsist of one or more features,\nevolve in time, and\nare indexed in two spatial dimensions.\nHow about the output? We want to be able to return forecasts for as many time steps as we have in the input sequence. This is something that torch RNNs do by default, while Keras equivalents do not. (You have to pass return_sequences = TRUE to obtain that effect.) If we’re interested in predictions for just a single point in time, we can always pick the last time step in the output tensor.\nHowever, with RNNs, it is not all about outputs. RNN architectures also carry through hidden states.\nWhat are hidden states? I carefully phrased that sentence to be as general as possible – deliberately circling around the confusion that, in my view, often arises at this point. We’ll attempt to clear up some of that confusion in a second, but let’s first finish our high-level requirements specification.\nWe want our convLSTM to be usable in different contexts and applications. Various architectures exist that make use of hidden states, most prominently perhaps, encoder-decoder architectures. Thus, we want our convLSTM to return those as well. Again, this is something a torch LSTM does by default, while in Keras it is achieved using return_state = TRUE.\nNow though, it really is time for that interlude. We’ll sort out the ways things are called by both torch and Keras, and inspect what you get back from their respective GRUs and LSTMs.\nInterlude: Outputs, states, hidden values … what’s what?\nFor this to remain an interlude, I summarize findings on a high level. The code snippets in the appendix show how to arrive at these results. Heavily commented, they probe return values from both Keras and torch GRUs and LSTMs. Running these will make the upcoming summaries seem a lot less abstract.\nFirst, let’s look at the ways you create an LSTM in both frameworks. (I will generally use LSTM as the “prototypical RNN example”, and just mention GRUs when there are differences significant in the context in question.)\nIn Keras, to create an LSTM you may write something like this:\n\n\nlstm <- layer_lstm(units = 1)\n\n\nThe torch equivalent would be:\n\n\nlstm <- nn_lstm(\n  input_size = 2, # number of input features\n  hidden_size = 1 # number of hidden (and output!) features\n)\n\n\nDon’t focus on torch‘s input_size parameter for this discussion. (It’s the number of features in the input tensor.) The parallel occurs between Keras’ units and torch’s hidden_size. If you’ve been using Keras, you’re probably thinking of units as the thing that determines output size (equivalently, the number of features in the output). So when torch lets us arrive at the same result using hidden_size, what does that mean? It means that somehow we’re specifying the same thing, using different terminology. And it does make sense, since at every time step current input and previous hidden state are added2:\n\\[\n\\mathbf{h}_t = \\mathbf{W}_{x}\\mathbf{x}_t + \\mathbf{W}_{h}\\mathbf{h}_{t-1}\n\\]\nNow, about those hidden states.\nWhen a Keras LSTM is defined with return_state = TRUE, its return value is a structure of three entities called output, memory state, and carry state. In torch, the same entities are referred to as output, hidden state, and cell state. (In torch, we always get all of them.)\nSo are we dealing with three different types of entities? We are not.\nThe cell, or carry state is that special thing that sets apart LSTMs from GRUs deemed responsible for the “long” in “long short-term memory”. Technically, it could be reported to the user at all points in time; as we’ll see shortly though, it is not.\nWhat about outputs and hidden, or memory states? Confusingly, these really are the same thing. Recall that for each input item in the input sequence, we’re combining it with the previous state, resulting in a new state, to be made used of in the next step3:\n\\[\n\\mathbf{h}_t = \\mathbf{W}_{x}\\mathbf{x}_t + \\mathbf{W}_{h}\\mathbf{h}_{t-1}\n\\]\nNow, say that we’re interested in looking at just the final time step – that is, the default output of a Keras LSTM. From that point of view, we can consider those intermediate computations as “hidden”. Seen like that, output and hidden states feel different.\nHowever, we can also request to see the outputs for every time step. If we do so, there is no difference – the outputs (plural) equal the hidden states. This can be verified using the code in the appendix.\nThus, of the three things returned by an LSTM, two are really the same. How about the GRU, then? As there is no “cell state”, we really have just one type of thing left over – call it outputs or hidden states.\nLet’s summarize this in a table.\n\n\nTable 1: RNN terminology. Comparing torch-speak and Keras-speak. In row 1, the terms are parameter names. In rows 2 and 3, they are pulled from current documentation.\nReferring to this entity:\ntorch says:\nKeras says:\nNumber of features in the output\nThis determines both how many output features there are and the dimensionality of the hidden states.\nhidden_size\nunits\nPer-time-step output; latent state; intermediate state …\nThis could be named “public state” in the sense that we, the users, are able to obtain all values.\nhidden state\nmemory state\nCell state; inner state … (LSTM only)\nThis could be named “private state” in that we are able to obtain a value only for the last time step. More on that in a second.\ncell state\ncarry state\n\n\nNow, about that public vs. private distinction. In both frameworks, we can obtain outputs (hidden states) for every time step. The cell state, however, we can access only for the very last time step. This is purely an implementation decision. As we’ll see when building our own recurrent module, there are no obstacles inherent in keeping track of cell states and passing them back to the user.\nIf you dislike the pragmatism of this distinction, you can always go with the math. When a new cell state has been computed (based on prior cell state, input, forget, and cell gates – the specifics of which we are not going to get into here), it is transformed to the hidden (a.k.a. output) state making use of yet another, namely, the output gate:\n\\[\nh_t = o_t \\odot \\tanh(c_t)\n\\]\nDefinitely, then, hidden state (output, resp.) builds on cell state, adding additional modeling power.\nNow it is time to get back to our original goal and build that convLSTM. First though, let’s summarize the return values obtainable from torch and Keras.\n\n\nTable 2: Contrasting ways of obtaining various return values in torch vs. Keras. Cf. the appendix for complete examples.\nTo achieve this goal:\nin torch do:\nin Keras do:\naccess all intermediate outputs ( = per-time-step outputs)\nret[[1]]\nreturn_sequences = TRUE\naccess both “hidden state” (output) and “cell state” from final time step (only!)\nret[[2]]\nreturn_state = TRUE\naccess all intermediate outputs and the final “cell state”\nboth of the above\nreturn_sequences = TRUE, return_state = TRUE\naccess all intermediate outputs and “cell states” from all time steps\nno way\nno way\n\n\nconvLSTM, the plan\nIn both torch and Keras RNN architectures, single time steps are processed by corresponding Cell classes: There is an LSTM Cell matching the LSTM, a GRU Cell matching the GRU, and so on. We do the same for ConvLSTM. In convlstm_cell(), we first define what should happen to a single observation; then in convlstm(), we build up the recurrence logic.\nOnce we’re done, we create a dummy dataset, as reduced-to-the-essentials as can be. With more complex datasets, even artificial ones, chances are that if we don’t see any training progress, there are hundreds of possible explanations. We want a sanity check that, if failed, leaves no excuses. Realistic applications are left to future posts.\nA single step: convlstm_cell\nOur convlstm_cell’s constructor takes arguments input_dim , hidden_dim, and bias, just like a torch LSTM Cell.\nBut we’re processing two-dimensional input data. Instead of the usual affine combination of new input and previous state, we use a convolution of kernel size kernel_size. Inside convlstm_cell, it is self$conv that takes care of this.\nNote how the channels dimension, which in the original input data would correspond to different variables, is creatively used to consolidate four convolutions into one: Each channel output will be passed to just one of the four cell gates. Once in possession of the convolution output, forward() applies the gate logic, resulting in the two types of states it needs to send back to the caller.\n\n\nlibrary(torch)\nlibrary(zeallot)\n\nconvlstm_cell <- nn_module(\n  \n  initialize = function(input_dim, hidden_dim, kernel_size, bias) {\n    \n    self$hidden_dim <- hidden_dim\n    \n    padding <- kernel_size %/% 2\n    \n    self$conv <- nn_conv2d(\n      in_channels = input_dim + self$hidden_dim,\n      # for each of input, forget, output, and cell gates\n      out_channels = 4 * self$hidden_dim,\n      kernel_size = kernel_size,\n      padding = padding,\n      bias = bias\n    )\n  },\n  \n  forward = function(x, prev_states) {\n\n    c(h_prev, c_prev) %<-% prev_states\n    \n    combined <- torch_cat(list(x, h_prev), dim = 2)  # concatenate along channel axis\n    combined_conv <- self$conv(combined)\n    c(cc_i, cc_f, cc_o, cc_g) %<-% torch_split(combined_conv, self$hidden_dim, dim = 2)\n    \n    # input, forget, output, and cell gates (corresponding to torch's LSTM)\n    i <- torch_sigmoid(cc_i)\n    f <- torch_sigmoid(cc_f)\n    o <- torch_sigmoid(cc_o)\n    g <- torch_tanh(cc_g)\n    \n    # cell state\n    c_next <- f * c_prev + i * g\n    # hidden state\n    h_next <- o * torch_tanh(c_next)\n    \n    list(h_next, c_next)\n  },\n  \n  init_hidden = function(batch_size, height, width) {\n    \n    list(\n      torch_zeros(batch_size, self$hidden_dim, height, width, device = self$conv$weight$device),\n      torch_zeros(batch_size, self$hidden_dim, height, width, device = self$conv$weight$device))\n  }\n)\n\n\nNow convlstm_cell has to be called for every time step. This is done by convlstm.\nIteration over time steps: convlstm\nA convlstm may consist of several layers, just like a torch LSTM. For each layer, we are able to specify hidden and kernel sizes individually.\nDuring initialization, each layer gets its own convlstm_cell. On call, convlstm executes two loops. The outer one iterates over layers. At the end of each iteration, we store the final pair (hidden state, cell state) for later reporting. The inner loop runs over input sequences, calling convlstm_cell at each time step.\nWe also keep track of intermediate outputs, so we’ll be able to return the complete list of hidden_states seen during the process. Unlike a torch LSTM, we do this for every layer.\n\n\nconvlstm <- nn_module(\n  \n  # hidden_dims and kernel_sizes are vectors, with one element for each layer in n_layers\n  initialize = function(input_dim, hidden_dims, kernel_sizes, n_layers, bias = TRUE) {\n \n    self$n_layers <- n_layers\n    \n    self$cell_list <- nn_module_list()\n    \n    for (i in 1:n_layers) {\n      cur_input_dim <- if (i == 1) input_dim else hidden_dims[i - 1]\n      self$cell_list$append(convlstm_cell(cur_input_dim, hidden_dims[i], kernel_sizes[i], bias))\n    }\n  },\n  \n  # we always assume batch-first\n  forward = function(x) {\n    \n    c(batch_size, seq_len, num_channels, height, width) %<-% x$size()\n   \n    # initialize hidden states\n    init_hidden <- vector(mode = \"list\", length = self$n_layers)\n    for (i in 1:self$n_layers) {\n      init_hidden[[i]] <- self$cell_list[[i]]$init_hidden(batch_size, height, width)\n    }\n    \n    # list containing the outputs, of length seq_len, for each layer\n    # this is the same as h, at each step in the sequence\n    layer_output_list <- vector(mode = \"list\", length = self$n_layers)\n    \n    # list containing the last states (h, c) for each layer\n    layer_state_list <- vector(mode = \"list\", length = self$n_layers)\n\n    cur_layer_input <- x\n    hidden_states <- init_hidden\n    \n    # loop over layers\n    for (i in 1:self$n_layers) {\n      \n      # every layer's hidden state starts from 0 (non-stateful)\n      c(h, c) %<-% hidden_states[[i]]\n      # outputs, of length seq_len, for this layer\n      # equivalently, list of h states for each time step\n      output_sequence <- vector(mode = \"list\", length = seq_len)\n      \n      # loop over time steps\n      for (t in 1:seq_len) {\n        c(h, c) %<-% self$cell_list[[i]](cur_layer_input[ , t, , , ], list(h, c))\n        # keep track of output (h) for every time step\n        # h has dim (batch_size, hidden_size, height, width)\n        output_sequence[[t]] <- h\n      }\n\n      # stack hs for all time steps over seq_len dimension\n      # stacked_outputs has dim (batch_size, seq_len, hidden_size, height, width)\n      # same as input to forward (x)\n      stacked_outputs <- torch_stack(output_sequence, dim = 2)\n      \n      # pass the list of outputs (hs) to next layer\n      cur_layer_input <- stacked_outputs\n      \n      # keep track of list of outputs or this layer\n      layer_output_list[[i]] <- stacked_outputs\n      # keep track of last state for this layer\n      layer_state_list[[i]] <- list(h, c)\n    }\n \n    list(layer_output_list, layer_state_list)\n  }\n    \n)\n\n\nCalling the convlstm\nLet’s see the input format expected by convlstm, and how to access its different outputs.\nHere is a suitable input tensor.\n\n\n# batch_size, seq_len, channels, height, width\nx <- torch_rand(c(2, 4, 3, 16, 16))\n\n\nFirst we make use of a single layer.\n\n\nmodel <- convlstm(input_dim = 3, hidden_dims = 5, kernel_sizes = 3, n_layers = 1)\n\nc(layer_outputs, layer_last_states) %<-% model(x)\n\n\nWe get back a list of length two, which we immediately split up into the two types of output returned: intermediate outputs from all layers, and final states (of both types) for the last layer.\nWith just a single layer, layer_outputs[[1]]holds all of the layer’s intermediate outputs, stacked on dimension two.\n\n\ndim(layer_outputs[[1]])\n# [1]  2  4  5 16 16\n\n\nlayer_last_states[[1]]is a list of tensors, the first of which holds the single layer’s final hidden state, and the second, its final cell state.\n\n\ndim(layer_last_states[[1]][[1]])\n# [1]  2  5 16 16\ndim(layer_last_states[[1]][[2]])\n# [1]  2  5 16 16\n\n\nFor comparison, this is how return values look for a multi-layer architecture.\n\n\nmodel <- convlstm(input_dim = 3, hidden_dims = c(5, 5, 1), kernel_sizes = rep(3, 3), n_layers = 3)\nc(layer_outputs, layer_last_states) %<-% model(x)\n\n# for each layer, tensor of size (batch_size, seq_len, hidden_size, height, width)\ndim(layer_outputs[[1]])\n# 2  4  5 16 16\ndim(layer_outputs[[3]])\n# 2  4  1 16 16\n\n# list of 2 tensors for each layer\nstr(layer_last_states)\n# List of 3\n#  $ :List of 2\n#   ..$ :Float [1:2, 1:5, 1:16, 1:16]\n#   ..$ :Float [1:2, 1:5, 1:16, 1:16]\n#  $ :List of 2\n#   ..$ :Float [1:2, 1:5, 1:16, 1:16]\n#   ..$ :Float [1:2, 1:5, 1:16, 1:16]\n#  $ :List of 2\n#   ..$ :Float [1:2, 1:1, 1:16, 1:16]\n#   ..$ :Float [1:2, 1:1, 1:16, 1:16]\n\n# h, of size (batch_size, hidden_size, height, width)\ndim(layer_last_states[[3]][[1]])\n# 2  1 16 16\n\n# c, of size (batch_size, hidden_size, height, width)\ndim(layer_last_states[[3]][[2]])\n# 2  1 16 16\n\n\nNow we want to sanity-check this module with the simplest-possible dummy data.\nSanity-checking the convlstm\nWe generate black-and-white “movies” of diagonal beams successively translated in space.\nEach sequence consists of six time steps, and each beam of six pixels. Just a single sequence is created manually. To create that one sequence, we start from a single beam:\n\n\nlibrary(torchvision)\n\nbeams <- vector(mode = \"list\", length = 6)\nbeam <- torch_eye(6) %>% nnf_pad(c(6, 12, 12, 6)) # left, right, top, bottom\nbeams[[1]] <- beam\n\n\nUsing torch_roll() , we create a pattern where this beam moves up diagonally, and stack the individual tensors along the timesteps dimension.\n\n\nfor (i in 2:6) {\n  beams[[i]] <- torch_roll(beam, c(-(i-1),i-1), c(1, 2))\n}\n\ninit_sequence <- torch_stack(beams, dim = 1)\n\n\nThat’s a single sequence. Thanks to torchvision::transform_random_affine(), we almost effortlessly produce a dataset of a hundred sequences. Moving beams start at random points in the spatial frame, but they all share that upward-diagonal motion.\n\n\nsequences <- vector(mode = \"list\", length = 100)\nsequences[[1]] <- init_sequence\n\nfor (i in 2:100) {\n  sequences[[i]] <- transform_random_affine(init_sequence, degrees = 0, translate = c(0.5, 0.5))\n}\n\ninput <- torch_stack(sequences, dim = 1)\n\n# add channels dimension\ninput <- input$unsqueeze(3)\ndim(input)\n# [1] 100   6  1  24  24\n\n\nThat’s it for the raw data. Now we still need a dataset and a dataloader. Of the six time steps, we use the first five as input and try to predict the last one.\n\n\ndummy_ds <- dataset(\n  \n  initialize = function(data) {\n    self$data <- data\n  },\n  \n  .getitem = function(i) {\n    list(x = self$data[i, 1:5, ..], y = self$data[i, 6, ..])\n  },\n  \n  .length = function() {\n    nrow(self$data)\n  }\n)\n\nds <- dummy_ds(input)\ndl <- dataloader(ds, batch_size = 100)\n\n\nHere is a tiny-ish convLSTM, trained for motion prediction:\n\n\nmodel <- convlstm(input_dim = 1, hidden_dims = c(64, 1), kernel_sizes = c(3, 3), n_layers = 2)\n\noptimizer <- optim_adam(model$parameters)\n\nnum_epochs <- 100\n\nfor (epoch in 1:num_epochs) {\n  \n  model$train()\n  batch_losses <- c()\n  \n  for (b in enumerate(dl)) {\n    \n    optimizer$zero_grad()\n    \n    # last-time-step output from last layer\n    preds <- model(b$x)[[2]][[2]][[1]]\n  \n    loss <- nnf_mse_loss(preds, b$y)\n    batch_losses <- c(batch_losses, loss$item())\n    \n    loss$backward()\n    optimizer$step()\n  }\n  \n  if (epoch %% 10 == 0)\n    cat(sprintf(\"\\nEpoch %d, training loss:%3f\\n\", epoch, mean(batch_losses)))\n}\n\n\nEpoch 10, training loss:0.008522\n\nEpoch 20, training loss:0.008079\n\nEpoch 30, training loss:0.006187\n\nEpoch 40, training loss:0.003828\n\nEpoch 50, training loss:0.002322\n\nEpoch 60, training loss:0.001594\n\nEpoch 70, training loss:0.001376\n\nEpoch 80, training loss:0.001258\n\nEpoch 90, training loss:0.001218\n\nEpoch 100, training loss:0.001171\nLoss decreases, but that in itself is not a guarantee the model has learned anything. Has it? Let’s inspect its forecast for the very first sequence and see.\nFor printing, I’m zooming in on the relevant region in the 24x24-pixel frame. Here is the ground truth for time step six:\n\n\nb$y[1, 1, 6:15, 10:19]\n\n\n0  0  0  0  0  0  0  0  0  0\n0  0  0  0  0  0  0  0  0  0\n0  0  1  0  0  0  0  0  0  0\n0  0  0  1  0  0  0  0  0  0\n0  0  0  0  1  0  0  0  0  0\n0  0  0  0  0  1  0  0  0  0\n0  0  0  0  0  0  1  0  0  0\n0  0  0  0  0  0  0  1  0  0\n0  0  0  0  0  0  0  0  0  0\n0  0  0  0  0  0  0  0  0  0\nAnd here is the forecast. This does not look bad at all, given there was neither experimentation nor tuning involved.\n\n\nround(as.matrix(preds[1, 1, 6:15, 10:19]), 2)\n\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]\n [1,]  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00     0\n [2,] -0.02  0.36  0.01  0.06  0.00  0.00  0.00  0.00  0.00     0\n [3,]  0.00 -0.01  0.71  0.01  0.06  0.00  0.00  0.00  0.00     0\n [4,] -0.01  0.04  0.00  0.75  0.01  0.06  0.00  0.00  0.00     0\n [5,]  0.00 -0.01 -0.01 -0.01  0.75  0.01  0.06  0.00  0.00     0\n [6,]  0.00  0.01  0.00 -0.07 -0.01  0.75  0.01  0.06  0.00     0\n [7,]  0.00  0.01 -0.01 -0.01 -0.07 -0.01  0.75  0.01  0.06     0\n [8,]  0.00  0.00  0.01  0.00  0.00 -0.01  0.00  0.71  0.00     0\n [9,]  0.00  0.00  0.00  0.01  0.01  0.00  0.03 -0.01  0.37     0\n[10,]  0.00  0.00  0.00  0.00  0.00  0.00 -0.01 -0.01 -0.01     0\nThis should suffice for a sanity check. If you made it till the end, thanks for your patience! In the best case, you’ll be able to apply this architecture (or a similar one) to your own data – but even if not, I hope you’ve enjoyed learning about torch model coding and/or RNN weirdness ;-)\nI, for one, am certainly looking forward to exploring convLSTMs on real-world problems in the near future. Thanks for reading!\nAppendix\nThis appendix contains the code used to create tables 1 and 2 above.\nKeras\nLSTM\n\n\nlibrary(keras)\n\n# batch of 3, with 4 time steps each and a single feature\ninput <- k_random_normal(shape = c(3L, 4L, 1L))\ninput\n\n# default args\n# return shape = (batch_size, units)\nlstm <- layer_lstm(\n  units = 1,\n  kernel_initializer = initializer_constant(value = 1),\n  recurrent_initializer = initializer_constant(value = 1)\n)\nlstm(input)\n\n# return_sequences = TRUE\n# return shape = (batch_size, time steps, units)\n#\n# note how for each item in the batch, the value for time step 4 equals that obtained above\nlstm <- layer_lstm(\n  units = 1,\n  return_sequences = TRUE,\n  kernel_initializer = initializer_constant(value = 1),\n  recurrent_initializer = initializer_constant(value = 1)\n  # bias is by default initialized to 0\n)\nlstm(input)\n\n# return_state = TRUE\n# return shape = list of:\n#                - outputs, of shape: (batch_size, units)\n#                - \"memory states\" for the last time step, of shape: (batch_size, units)\n#                - \"carry states\" for the last time step, of shape: (batch_size, units)\n#\n# note how the first and second list items are identical!\nlstm <- layer_lstm(\n  units = 1,\n  return_state = TRUE,\n  kernel_initializer = initializer_constant(value = 1),\n  recurrent_initializer = initializer_constant(value = 1)\n)\nlstm(input)\n\n# return_state = TRUE, return_sequences = TRUE\n# return shape = list of:\n#                - outputs, of shape: (batch_size, time steps, units)\n#                - \"memory\" states for the last time step, of shape: (batch_size, units)\n#                - \"carry states\" for the last time step, of shape: (batch_size, units)\n#\n# note how again, the \"memory\" state found in list item 2 matches the final-time step outputs reported in item 1\nlstm <- layer_lstm(\n  units = 1,\n  return_sequences = TRUE,\n  return_state = TRUE,\n  kernel_initializer = initializer_constant(value = 1),\n  recurrent_initializer = initializer_constant(value = 1)\n)\nlstm(input)\n\n\nGRU\n\n\n# default args\n# return shape = (batch_size, units)\ngru <- layer_gru(\n  units = 1,\n  kernel_initializer = initializer_constant(value = 1),\n  recurrent_initializer = initializer_constant(value = 1)\n)\ngru(input)\n\n# return_sequences = TRUE\n# return shape = (batch_size, time steps, units)\n#\n# note how for each item in the batch, the value for time step 4 equals that obtained above\ngru <- layer_gru(\n  units = 1,\n  return_sequences = TRUE,\n  kernel_initializer = initializer_constant(value = 1),\n  recurrent_initializer = initializer_constant(value = 1)\n)\ngru(input)\n\n# return_state = TRUE\n# return shape = list of:\n#    - outputs, of shape: (batch_size, units)\n#    - \"memory\" states for the last time step, of shape: (batch_size, units)\n#\n# note how the list items are identical!\ngru <- layer_gru(\n  units = 1,\n  return_state = TRUE,\n  kernel_initializer = initializer_constant(value = 1),\n  recurrent_initializer = initializer_constant(value = 1)\n)\ngru(input)\n\n# return_state = TRUE, return_sequences = TRUE\n# return shape = list of:\n#    - outputs, of shape: (batch_size, time steps, units)\n#    - \"memory states\" for the last time step, of shape: (batch_size, units)\n#\n# note how again, the \"memory state\" found in list item 2 matches the final-time-step outputs reported in item 1\ngru <- layer_gru(\n  units = 1,\n  return_sequences = TRUE,\n  return_state = TRUE,\n  kernel_initializer = initializer_constant(value = 1),\n  recurrent_initializer = initializer_constant(value = 1)\n)\ngru(input)\n\n\ntorch\nLSTM (non-stacked architecture)\n\n\nlibrary(torch)\n\n# batch of 3, with 4 time steps each and a single feature\n# we will specify batch_first = TRUE when creating the LSTM\ninput <- torch_randn(c(3, 4, 1))\ninput\n\n# default args\n# return shape = (batch_size, units)\n#\n# note: there is an additional argument num_layers that we could use to specify a stacked LSTM - effectively composing two LSTM modules\n# default for num_layers is 1 though \nlstm <- nn_lstm(\n  input_size = 1, # number of input features\n  hidden_size = 1, # number of hidden (and output!) features\n  batch_first = TRUE # for easy comparability with Keras\n)\n\nnn_init_constant_(lstm$weight_ih_l1, 1)\nnn_init_constant_(lstm$weight_hh_l1, 1)\nnn_init_constant_(lstm$bias_ih_l1, 0)\nnn_init_constant_(lstm$bias_hh_l1, 0)\n\n# returns a list of length 2, namely\n#   - outputs, of shape (batch_size, time steps, hidden_size) - given we specified batch_first\n#       Note 1: If this is a stacked LSTM, these are the outputs from the last layer only.\n#               For our current purpose, this is irrelevant, as we're restricting ourselves to single-layer LSTMs.\n#       Note 2: hidden_size here is equivalent to units in Keras - both specify number of features\n#  - list of:\n#    - hidden state for the last time step, of shape (num_layers, batch_size, hidden_size)\n#    - cell state for the last time step, of shape (num_layers, batch_size, hidden_size)\n#      Note 3: For a single-layer LSTM, the hidden states are already provided in the first list item.\n\nlstm(input)\n\n\nGRU (non-stacked architecture)\n\n\n# default args\n# return shape = (batch_size, units)\n#\n# note: there is an additional argument num_layers that we could use to specify a stacked GRU - effectively composing two GRU modules\n# default for num_layers is 1 though \ngru <- nn_gru(\n  input_size = 1, # number of input features\n  hidden_size = 1, # number of hidden (and output!) features\n  batch_first = TRUE # for easy comparability with Keras\n)\n\nnn_init_constant_(gru$weight_ih_l1, 1)\nnn_init_constant_(gru$weight_hh_l1, 1)\nnn_init_constant_(gru$bias_ih_l1, 0)\nnn_init_constant_(gru$bias_hh_l1, 0)\n\n# returns a list of length 2, namely\n#   - outputs, of shape (batch_size, time steps, hidden_size) - given we specified batch_first\n#       Note 1: If this is a stacked GRU, these are the outputs from the last layer only.\n#               For our current purpose, this is irrelevant, as we're restricting ourselves to single-layer GRUs.\n#       Note 2: hidden_size here is equivalent to units in Keras - both specify number of features\n#  - list of:\n#    - hidden state for the last time step, of shape (num_layers, batch_size, hidden_size)\n#    - cell state for the last time step, of shape (num_layers, batch_size, hidden_size)\n#       Note 3: For a single-layer GRU, these values are already provided in the first list item.\ngru(input)\n\n\n\nLeaving aside the batch dimension in this discussion.↩︎\nIn theory, it would be possible for them to be of different sizes if the respective weight matrices transformed their operands to the same output size.↩︎\nYes, this is the same formula as above.↩︎\n",
    "preview": "posts/2020-12-17-torch-convlstm/images/preview.jpeg",
    "last_modified": "2024-11-21T15:49:56+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-15-torch-0.2.0-released/",
    "title": "torch 0.2.0 - Initial JIT support and many bug fixes",
    "description": "The torch 0.2.0 release includes many bug fixes and some nice new features like initial JIT support, multi-worker dataloaders, new optimizers and a new print method for  nn_modules.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": {}
      }
    ],
    "date": "2020-12-15",
    "categories": [
      "Torch",
      "R",
      "Packages/Releases"
    ],
    "contents": "\n\nContents\nMulti-worker dataloaders\nInitial JIT support\nNew print method for nn_modules\ntorchaudio\nOther features and bug fixes\n\nWe are happy to announce that the version 0.2.0 of torch\njust landed on CRAN.\nThis release includes many bug fixes and some nice new features\nthat we will present in this blog post. You can see the full changelog\nin the NEWS.md file.\nThe features that we will discuss in detail are:\nInitial support for JIT tracing\nMulti-worker dataloaders\nPrint methods for nn_modules\nMulti-worker dataloaders\ndataloaders now respond to the num_workers argument and\nwill run the pre-processing in parallel workers.\nFor example, say we have the following dummy dataset that does\na long computation:\n\n\nlibrary(torch)\ndat <- dataset(\n  \"mydataset\",\n  initialize = function(time, len = 10) {\n    self$time <- time\n    self$len <- len\n  },\n  .getitem = function(i) {\n    Sys.sleep(self$time)\n    torch_randn(1)\n  },\n  .length = function() {\n    self$len\n  }\n)\nds <- dat(1)\nsystem.time(ds[1])\n\n\n   user  system elapsed \n  0.029   0.005   1.027 \nWe will now create two dataloaders, one that executes\nsequentially and another executing in parallel.\n\n\nseq_dl <- dataloader(ds, batch_size = 5)\npar_dl <- dataloader(ds, batch_size = 5, num_workers = 2)\n\n\nWe can now compare the time it takes to process two batches sequentially to\nthe time it takes in parallel:\n\n\nseq_it <- dataloader_make_iter(seq_dl)\npar_it <- dataloader_make_iter(par_dl)\n\ntwo_batches <- function(it) {\n  dataloader_next(it)\n  dataloader_next(it)\n  \"ok\"\n}\n\nsystem.time(two_batches(seq_it))\nsystem.time(two_batches(par_it))\n\n\n   user  system elapsed \n  0.098   0.032  10.086 \n   user  system elapsed \n  0.065   0.008   5.134 \nNote that it is batches that are obtained in parallel, not individual observations. Like that, we will be able to support\ndatasets with variable batch sizes in the future.\nUsing multiple workers is not necessarily faster than serial execution because there’s a considerable overhead\nwhen passing tensors from a worker to the main session as\nwell as when initializing the workers.\nThis feature is enabled by the powerful callr package\nand works in all operating systems supported by torch. callr let’s\nus create persistent R sessions, and thus, we only pay once the overhead of transferring potentially large dataset\nobjects to workers.\nIn the process of implementing this feature we have made\ndataloaders behave like coro iterators.\nThis means that you can now use coro’s syntax\nfor looping through the dataloaders:\n\n\ncoro::loop(for(batch in par_dl) {\n  print(batch$shape)\n})\n\n\n[1] 5 1\n[1] 5 1\nThis is the first torch release including the multi-worker\ndataloaders feature, and you might run into edge cases when\nusing it. Do let us know if you find any problems.\nInitial JIT support\nPrograms that make use of the torch package are inevitably\nR programs and thus, they always need an R installation in order\nto execute.\nAs of version 0.2.0, torch allows users to JIT trace\ntorch R functions into TorchScript. JIT (Just in time) tracing will invoke\nan R function with example inputs, record all operations that\noccured when the function was run and return a script_function object\ncontaining the TorchScript representation.\nThe nice thing about this is that TorchScript programs are easily\nserializable, optimizable, and they can be loaded by another\nprogram written in PyTorch or LibTorch without requiring any R\ndependency.\nSuppose you have the following R function that takes a tensor,\nand does a matrix multiplication with a fixed weight matrix and\nthen adds a bias term:\n\n\nw <- torch_randn(10, 1)\nb <- torch_randn(1)\nfn <- function(x) {\n  a <- torch_mm(x, w)\n  a + b\n}\n\n\nThis function can be JIT-traced into TorchScript with jit_trace by passing the function and example inputs:\n\n\nx <- torch_ones(2, 10)\ntr_fn <- jit_trace(fn, x)\ntr_fn(x)\n\n\ntorch_tensor\n-0.6880\n-0.6880\n[ CPUFloatType{2,1} ]\nNow all torch operations that happened when computing the result of\nthis function were traced and transformed into a graph:\n\n\ntr_fn$graph\n\n\ngraph(%0 : Float(2:10, 10:1, requires_grad=0, device=cpu)):\n  %1 : Float(10:1, 1:1, requires_grad=0, device=cpu) = prim::Constant[value=-0.3532  0.6490 -0.9255  0.9452 -1.2844  0.3011  0.4590 -0.2026 -1.2983  1.5800 [ CPUFloatType{10,1} ]]()\n  %2 : Float(2:1, 1:1, requires_grad=0, device=cpu) = aten::mm(%0, %1)\n  %3 : Float(1:1, requires_grad=0, device=cpu) = prim::Constant[value={-0.558343}]()\n  %4 : int = prim::Constant[value=1]()\n  %5 : Float(2:1, 1:1, requires_grad=0, device=cpu) = aten::add(%2, %3, %4)\n  return (%5)\nThe traced function can be serialized with jit_save:\n\n\njit_save(tr_fn, \"linear.pt\")\n\n\nIt can be reloaded in R with jit_load, but it can also be reloaded in Python\nwith torch.jit.load:\n\nimport torch\nfn = torch.jit.load(\"linear.pt\")\nfn(torch.ones(2, 10))\n\ntensor([[-0.6880],\n        [-0.6880]])\nHow cool is that?!\nThis is just the initial support for JIT in R. We will continue developing\nthis. Specifically, in the next version of torch we plan to support tracing nn_modules directly. Currently, you need to detach all parameters before\ntracing them; see an example here. This will allow you also to take benefit of TorchScript to make your models\nrun faster!\nAlso note that tracing has some limitations, especially when your code has loops\nor control flow statements that depend on tensor data. See ?jit_trace to\nlearn more.\nNew print method for nn_modules\nIn this release we have also improved the nn_module printing methods in order\nto make it easier to understand what’s inside.\nFor example, if you create an instance of an nn_linear module you will\nsee:\n\n\nnn_linear(10, 1)\n\n\nAn `nn_module` containing 11 parameters.\n\n── Parameters ──────────────────────────────────────────────────────────────────\n● weight: Float [1:1, 1:10]\n● bias: Float [1:1]\nYou immediately see the total number of parameters in the module as well as\ntheir names and shapes.\nThis also works for custom modules (possibly including sub-modules). For example:\n\n\nmy_module <- nn_module(\n  initialize = function() {\n    self$linear <- nn_linear(10, 1)\n    self$param <- nn_parameter(torch_randn(5,1))\n    self$buff <- nn_buffer(torch_randn(5))\n  }\n)\nmy_module()\n\n\nAn `nn_module` containing 16 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────\n● linear: <nn_linear> #11 parameters\n\n── Parameters ──────────────────────────────────────────────────────────────────\n● param: Float [1:5, 1:1]\n\n── Buffers ─────────────────────────────────────────────────────────────────────\n● buff: Float [1:5]\nWe hope this makes it easier to understand nn_module objects.\nWe have also improved autocomplete support for nn_modules and we will now\nshow all sub-modules, parameters and buffers while you type.\ntorchaudio\ntorchaudio is an extension for torch developed by Athos Damiani (@athospd), providing audio loading, transformations, common architectures for signal processing, pre-trained weights and access to commonly used datasets. An almost literal translation from PyTorch’s Torchaudio library to R.\ntorchaudio is not yet on CRAN, but you can already try the development version\navailable here.\nYou can also visit the pkgdown website for examples and reference documentation.\nOther features and bug fixes\nThanks to community contributions we have found and fixed many bugs in torch.\nWe have also added new features including:\nelement_size and bool Tensor methods by @dirkschumacher\nchecking the MD5 hashes of downloaded LibTorch binaries by @dirkschumacher\ninitial development for the Distributions module by @krzjoa\nthe nn_batch_norm3d module implemented by @mattwarkentin\na Dockerfile with GPU support as well as an installation guide by @y-vectorfield\nYou can see the full list of changes in the NEWS.md file.\nThanks very much for reading this blog post, and feel free to reach out on GitHub for help or discussions!\nThe photo used in this post preview is by Oleg Illarionov on Unsplash\n\n\n\n",
    "preview": "posts/2020-12-15-torch-0.2.0-released/images/torch.jpg",
    "last_modified": "2024-11-21T15:52:14+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-14-sparklyr-1.5.0-released/",
    "title": "sparklyr 1.5: better dplyr interface, more sdf_* functions, and RDS-based serialization routines",
    "description": "Unlike all three previous sparklyr releases, the recent release of sparklyr 1.5 placed much more emphasis on enhancing existing sparklyr features rather than creating new ones. As a result, many valuable suggestions from sparklyr users were taken into account and were successfully addressed in a long list of bug fixes and improvements.",
    "author": [
      {
        "name": "Yitao Li",
        "url": "https://github.com/yitao-li"
      }
    ],
    "date": "2020-12-14",
    "categories": [
      "R",
      "Packages/Releases",
      "Distributed Computing"
    ],
    "contents": "\nWe are thrilled to announce sparklyr 1.5 is now\navailable on CRAN!\nTo install sparklyr 1.5 from CRAN, run\n\n\ninstall.packages(\"sparklyr\")\n\n\nIn this blog post, we will highlight the following aspects of sparklyr 1.5:\nBetter dplyr interface\n4 useful additions to the sdf_* family of functions\nNew RDS-based serialization routines along with several serialization-related improvements and bug fixes\nBetter dplyr interface\nA large fraction of pull requests that went into the sparklyr 1.5 release were focused on making\nSpark dataframes work with various dplyr verbs in the same way that R dataframes do.\nThe full list of dplyr-related bugs and feature requests that were resolved in\nsparklyr 1.5 can be found in here.\nIn this section, we will showcase three new dplyr functionalities that were shipped with sparklyr 1.5.\nStratified sampling\nStratified sampling on an R dataframe can be accomplished with a combination of dplyr::group_by() followed by\ndplyr::sample_n() or dplyr::sample_frac(), where the grouping variables specified in the dplyr::group_by()\nstep are the ones that define each stratum. For instance, the following query will group mtcars by number\nof cylinders and return a weighted random sample of size two from each group, without replacement, and weighted by\nthe mpg column:\n\n\nmtcars %>%\n  dplyr::group_by(cyl) %>%\n  dplyr::sample_n(size = 2, weight = mpg, replace = FALSE) %>%\n  print()\n\n\n## # A tibble: 6 x 11\n## # Groups:   cyl [3]\n##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1  33.9     4  71.1    65  4.22  1.84  19.9     1     1     4     1\n## 2  22.8     4 108      93  3.85  2.32  18.6     1     1     4     1\n## 3  21.4     6 258     110  3.08  3.22  19.4     1     0     3     1\n## 4  21       6 160     110  3.9   2.62  16.5     0     1     4     4\n## 5  15.5     8 318     150  2.76  3.52  16.9     0     0     3     2\n## 6  19.2     8 400     175  3.08  3.84  17.0     0     0     3     2\nStarting from sparklyr 1.5, the same can also be done for Spark dataframes with Spark 3.0 or above, e.g.,:\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\", version = \"3.0.0\")\nmtcars_sdf <- copy_to(sc, mtcars, replace = TRUE, repartition = 3)\n\nmtcars_sdf %>%\n  dplyr::group_by(cyl) %>%\n  dplyr::sample_n(size = 2, weight = mpg, replace = FALSE) %>%\n  print()\n\n\n# Source: spark<?> [?? x 11]\n# Groups: cyl\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  21       6 160     110  3.9   2.62  16.5     0     1     4     4\n2  21.4     6 258     110  3.08  3.22  19.4     1     0     3     1\n3  27.3     4  79      66  4.08  1.94  18.9     1     1     4     1\n4  32.4     4  78.7    66  4.08  2.2   19.5     1     1     4     1\n5  16.4     8 276.    180  3.07  4.07  17.4     0     0     3     3\n6  18.7     8 360     175  3.15  3.44  17.0     0     0     3     2\nor\n\n\nmtcars_sdf %>%\n  dplyr::group_by(cyl) %>%\n  dplyr::sample_frac(size = 0.2, weight = mpg, replace = FALSE) %>%\n  print()\n\n\n## # Source: spark<?> [?? x 11]\n## # Groups: cyl\n##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1  21       6 160     110  3.9   2.62  16.5     0     1     4     4\n## 2  21.4     6 258     110  3.08  3.22  19.4     1     0     3     1\n## 3  22.8     4 141.     95  3.92  3.15  22.9     1     0     4     2\n## 4  33.9     4  71.1    65  4.22  1.84  19.9     1     1     4     1\n## 5  30.4     4  95.1   113  3.77  1.51  16.9     1     1     5     2\n## 6  15.5     8 318     150  2.76  3.52  16.9     0     0     3     2\n## 7  18.7     8 360     175  3.15  3.44  17.0     0     0     3     2\n## 8  16.4     8 276.    180  3.07  4.07  17.4     0     0     3     3\nRow sums\nThe rowSums() functionality offered by dplyr is handy when one needs to sum up\na large number of columns within an R dataframe that are impractical to be enumerated\nindividually.\nFor example, here we have a six-column dataframe of random real numbers, where the\npartial_sum column in the result contains the sum of columns b through d within\neach row:\n\n\nncols <- 6\nnums <- seq(ncols) %>% lapply(function(x) runif(5))\nnames(nums) <- letters[1:ncols]\ntbl <- tibble::as_tibble(nums)\n\ntbl %>%\n  dplyr::mutate(partial_sum = rowSums(.[2:5])) %>%\n  print()\n\n\n## # A tibble: 5 x 7\n##         a     b     c      d     e      f partial_sum\n##     <dbl> <dbl> <dbl>  <dbl> <dbl>  <dbl>       <dbl>\n## 1 0.781   0.801 0.157 0.0293 0.169 0.0978        1.16\n## 2 0.696   0.412 0.221 0.941  0.697 0.675         2.27\n## 3 0.802   0.410 0.516 0.923  0.190 0.904         2.04\n## 4 0.200   0.590 0.755 0.494  0.273 0.807         2.11\n## 5 0.00149 0.711 0.286 0.297  0.107 0.425         1.40\nBeginning with sparklyr 1.5, the same operation can be performed with Spark dataframes:\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\nsdf <- copy_to(sc, tbl, overwrite = TRUE)\n\nsdf %>%\n  dplyr::mutate(partial_sum = rowSums(.[2:5])) %>%\n  print()\n\n\n## # Source: spark<?> [?? x 7]\n##         a     b     c      d     e      f partial_sum\n##     <dbl> <dbl> <dbl>  <dbl> <dbl>  <dbl>       <dbl>\n## 1 0.781   0.801 0.157 0.0293 0.169 0.0978        1.16\n## 2 0.696   0.412 0.221 0.941  0.697 0.675         2.27\n## 3 0.802   0.410 0.516 0.923  0.190 0.904         2.04\n## 4 0.200   0.590 0.755 0.494  0.273 0.807         2.11\n## 5 0.00149 0.711 0.286 0.297  0.107 0.425         1.40\nAs a bonus from implementing the rowSums feature for Spark dataframes,\nsparklyr 1.5 now also offers limited support for the column-subsetting\noperator on Spark dataframes.\nFor example, all code snippets below will return some subset of columns from\nthe dataframe named sdf:\n\n\n# select columns `b` through `e`\nsdf[2:5]\n\n\n\n\n# select columns `b` and `c`\nsdf[c(\"b\", \"c\")]\n\n\n\n\n# drop the first and third columns and return the rest\nsdf[c(-1, -3)]\n\n\nWeighted-mean summarizer\nSimilar to the two dplyr functions mentioned above, the weighted.mean() summarizer is another\nuseful function that has become part of the dplyr interface for Spark dataframes in sparklyr 1.5.\nOne can see it in action by, for example, comparing the output from the following\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n\nmtcars_sdf <- copy_to(sc, mtcars, replace = TRUE)\nmtcars_sdf %>%\n  dplyr::group_by(cyl) %>%\n  dplyr::summarize(mpg_wm = weighted.mean(mpg, wt)) %>%\n  print()\n\n\nwith output from the equivalent operation on mtcars in R:\n\n\nmtcars %>%\n  dplyr::group_by(cyl) %>%\n  dplyr::summarize(mpg_wm = weighted.mean(mpg, wt)) %>%\n  print()\n\n\nboth of them should evaluate to the following:\n##     cyl mpg_wm\n##   <dbl>  <dbl>\n## 1     4   25.9\n## 2     6   19.6\n## 3     8   14.8\nNew additions to the sdf_* family of functions\nsparklyr provides a large number of convenience functions for working with Spark dataframes,\nand all of them have names starting with the sdf_ prefix.\nIn this section we will briefly mention four new additions\nand show some example scenarios in which those functions are useful.\nsdf_expand_grid()\nAs the name suggests, sdf_expand_grid() is simply the Spark equivalent of expand.grid().\nRather than running expand.grid() in R and importing the resulting R dataframe to Spark, one\ncan now run sdf_expand_grid(), which accepts both R vectors and Spark dataframes and supports\nhints for broadcast hash joins. The example below shows sdf_expand_grid() creating a\n100-by-100-by-10-by-10 grid in Spark over 1000 Spark partitions, with broadcast hash join hints\non variables with small cardinalities:\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n\ngrid_sdf <- sdf_expand_grid(\n  sc,\n  var1 = seq(100),\n  var2 = seq(100),\n  var3 = seq(10),\n  var4 = seq(10),\n  broadcast_vars = c(var3, var4),\n  repartition = 1000\n)\n\ngrid_sdf %>% sdf_nrow() %>% print()\n\n\n## [1] 1e+06\nsdf_partition_sizes()\nAs sparklyr user @sbottelli suggested here,\none thing that would be great to have in sparklyr is an efficient way to query partition sizes of a Spark dataframe.\nIn sparklyr 1.5, sdf_partition_sizes() does exactly that:\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n\nsdf_len(sc, 1000, repartition = 5) %>%\n  sdf_partition_sizes() %>%\n  print(row.names = FALSE)\n\n\n##  partition_index partition_size\n##                0            200\n##                1            200\n##                2            200\n##                3            200\n##                4            200\nsdf_unnest_longer() and sdf_unnest_wider()\nsdf_unnest_longer() and sdf_unnest_wider() are the equivalents of\ntidyr::unnest_longer() and tidyr::unnest_wider() for Spark dataframes.\nsdf_unnest_longer() expands all elements in a struct column into multiple rows, and\nsdf_unnest_wider() expands them into multiple columns. As illustrated with an example\ndataframe below,\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\nsdf <- copy_to(\n  sc,\n  tibble::tibble(\n    id = seq(3),\n    attribute = list(\n      list(name = \"Alice\", grade = \"A\"),\n      list(name = \"Bob\", grade = \"B\"),\n      list(name = \"Carol\", grade = \"C\")\n    )\n  )\n)\n\n\n\n\nsdf %>%\n  sdf_unnest_longer(col = record, indices_to = \"key\", values_to = \"value\") %>%\n  print()\n\n\nevaluates to\n## # Source: spark<?> [?? x 3]\n##      id value key\n##   <int> <chr> <chr>\n## 1     1 A     grade\n## 2     1 Alice name\n## 3     2 B     grade\n## 4     2 Bob   name\n## 5     3 C     grade\n## 6     3 Carol name\nwhereas\n\n\nsdf %>%\n  sdf_unnest_wider(col = record) %>%\n  print()\n\n\nevaluates to\n## # Source: spark<?> [?? x 3]\n##      id grade name\n##   <int> <chr> <chr>\n## 1     1 A     Alice\n## 2     2 B     Bob\n## 3     3 C     Carol\nRDS-based serialization routines\nSome readers must be wondering why a brand new serialization format would need to be implemented in sparklyr at all.\nLong story short, the reason is that RDS serialization is a strictly better replacement for its CSV predecessor.\nIt possesses all desirable attributes the CSV format has,\nwhile avoiding a number of disadvantages that are common among text-based data formats.\nIn this section, we will briefly outline why sparklyr should support at least one serialization format other than arrow,\ndeep-dive into issues with CSV-based serialization,\nand then show how the new RDS-based serialization is free from those issues.\nWhy arrow is not for everyone?\nTo transfer data between Spark and R correctly and efficiently, sparklyr must rely on some data serialization\nformat that is well-supported by both Spark and R.\nUnfortunately, not many serialization formats satisfy this requirement,\nand among the ones that do are text-based formats such as CSV and JSON,\nand binary formats such as Apache Arrow, Protobuf, and as of recent, a small subset of RDS version 2.\nFurther complicating the matter is the additional consideration that\nsparklyr should support at least one serialization format whose implementation can be fully self-contained within the sparklyr code base,\ni.e., such serialization should not depend on any external R package or system library,\nso that it can accommodate users who want to use sparklyr but who do not necessarily have the required C++ compiler tool chain and\nother system dependencies for setting up R packages such as arrow or\nprotolite.\nPrior to sparklyr 1.5, CSV-based serialization was the default alternative to fallback to when users do not have the arrow package installed or\nwhen the type of data being transported from R to Spark is unsupported by the version of arrow available.\nWhy is the CSV format not ideal?\nThere are at least three reasons to believe CSV format is not the best choice when it comes to exporting data from R to Spark.\nOne reason is efficiency. For example, a double-precision floating point number such as .Machine$double.eps needs to\nbe expressed as \"2.22044604925031e-16\" in CSV format in order to not incur any loss of precision, thus taking up 20 bytes\nrather than 8 bytes.\nBut more important than efficiency are correctness concerns. In a R dataframe, one can store both NA_real_ and\nNaN in a column of floating point numbers. NA_real_ should ideally translate to null within a Spark dataframe, whereas\nNaN should continue to be NaN when transported from R to Spark. Unfortunately, NA_real_ in R becomes indistinguishable\nfrom NaN once serialized in CSV format, as evident from a quick demo shown below:\n\n\noriginal_df <- data.frame(x = c(NA_real_, NaN))\noriginal_df %>% dplyr::mutate(is_nan = is.nan(x)) %>% print()\n\n\n##     x is_nan\n## 1  NA  FALSE\n## 2 NaN   TRUE\n\n\ncsv_file <- \"/tmp/data.csv\"\nwrite.csv(original_df, file = csv_file, row.names = FALSE)\ndeserialized_df <- read.csv(csv_file)\ndeserialized_df %>% dplyr::mutate(is_nan = is.nan(x)) %>% print()\n\n\n##    x is_nan\n## 1 NA  FALSE\n## 2 NA  FALSE\nAnother correctness issue very much similar to the one above was the fact that\n\"NA\" and NA within a string column of an R dataframe become indistinguishable\nonce serialized in CSV format, as correctly pointed out in\nthis Github issue\nby @caewok and others.\nRDS to the rescue!\nRDS format is one of the most widely used binary formats for serializing R objects.\nIt is described in some detail in chapter 1, section 8 of\nthis document.\nAmong advantages of the RDS format are efficiency and accuracy: it has a reasonably\nefficient implementation in base R, and supports all R data types.\nAlso worth noticing is the fact that when an R dataframe containing only data types\nwith sensible equivalents in Apache Spark (e.g., RAWSXP, LGLSXP, CHARSXP, REALSXP, etc)\nis saved using RDS version 2,\n(e.g., serialize(mtcars, connection = NULL, version = 2L, xdr = TRUE)),\nonly a tiny subset of the RDS format will be involved in the serialization process,\nand implementing deserialization routines in Scala capable of decoding such a restricted\nsubset of RDS constructs is in fact a reasonably simple and straightforward task\n(as shown in\nhere\n).\nLast but not least, because RDS is a binary format, it allows NA_character_, \"NA\",\nNA_real_, and NaN to all be encoded in an unambiguous manner, hence allowing sparklyr\n1.5 to avoid all correctness issues detailed above in non-arrow serialization use cases.\nOther benefits of RDS serialization\nIn addition to correctness guarantees, RDS format also offers quite a few other advantages.\nOne advantage is of course performance: for example, importing a non-trivially-sized dataset\nsuch as nycflights13::flights from R to Spark using the RDS format in sparklyr 1.5 is\nroughly 40%-50% faster compared to CSV-based serialization in sparklyr 1.4. The\ncurrent RDS-based implementation is still nowhere as fast as arrow-based serialization\nthough (arrow is about 3-4x faster), so for performance-sensitive tasks involving\nheavy serialization, arrow should still be the top choice.\nAnother advantage is that with RDS serialization, sparklyr can import R dataframes containing\nraw columns directly into binary columns in Spark. Thus, use cases such as the one below\nwill work in sparklyr 1.5\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n\ntbl <- tibble::tibble(\n  x = list(serialize(\"sparklyr\", NULL), serialize(c(123456, 789), NULL))\n)\nsdf <- copy_to(sc, tbl)\n\n\nWhile most sparklyr users probably won’t find this capability of importing binary columns\nto Spark immediately useful in their typical sparklyr::copy_to() or sparklyr::collect()\nusages, it does play a crucial role in reducing serialization overheads in the Spark-based\nforeach parallel backend that\nwas first introduced in sparklyr 1.2.\nThis is because Spark workers can directly fetch the serialized R closures to be computed\nfrom a binary Spark column instead of extracting those serialized bytes from intermediate\nrepresentations such as base64-encoded strings.\nSimilarly, the R results from executing worker closures will be directly available in RDS\nformat which can be efficiently deserialized in R, rather than being delivered in other\nless efficient formats.\nAcknowledgement\nIn chronological order, we would like to thank the following contributors for making their pull\nrequests part of sparklyr 1.5:\n@wkdavis\n@yitao-li\n@falaki\n@nathaneastwood\n@pgramme\nWe would also like to express our gratitude towards numerous bug reports and feature requests for\nsparklyr from a fantastic open-source community.\nFinally, the author of this blog post is indebted to\n@javierluraschi,\n@batpigandme,\nand @skeydan for their valuable editorial inputs.\nIf you wish to learn more about sparklyr, check out sparklyr.ai,\nspark.rstudio.com, and some of the previous release posts such as\nsparklyr 1.4 and\nsparklyr 1.3.\nThanks for reading!\n\n\n\n",
    "preview": "posts/2020-12-14-sparklyr-1.5.0-released/images/sparklyr-1.5.jpg",
    "last_modified": "2024-11-21T15:51:52+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-11-30-torch-brain-segmentation/",
    "title": "Brain image segmentation with torch",
    "description": "The need to segment images arises in various sciences and their applications, many of which are vital to human (and animal) life. In this introductory post, we train a U-Net to mark lesioned regions on MRI brain scans.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-11-30",
    "categories": [
      "Torch",
      "R",
      "Image Recognition & Image Processing"
    ],
    "contents": "\n\nContents\nWhen what is not enough\nU-Net\nBrain image segmentation\nData\nDataset\nModel\nOptimization\nTraining\nEvaluation\nWrapup\n\nWhen what is not enough\nTrue, sometimes it’s vital to distinguish between different kinds of objects. Is that a car speeding towards me, in which case I’d better jump out of the way? Or is it a huge Doberman (in which case I’d probably do the same)? Often in real life though, instead of coarse-grained classification, what is needed is fine-grained segmentation.\nZooming in on images, we’re not looking for a single label; instead, we want to classify every pixel according to some criterion:\nIn medicine, we may want to distinguish between different cell types, or identify tumors.\nIn various earth sciences, satellite data are used to segment terrestrial surfaces.\nTo enable use of custom backgrounds, video-conferencing software has to be able to tell foreground from background.\nImage segmentation is a form of supervised learning: Some kind of ground truth is needed. Here, it comes in form of a mask – an image, of spatial resolution identical to that of the input data, that designates the true class for every pixel. Accordingly, classification loss is calculated pixel-wise; losses are then summed up to yield an aggregate to be used in optimization.\nThe “canonical” architecture for image segmentation is U-Net (around since 2015).\nU-Net\nHere is the prototypical U-Net, as depicted in the original Rönneberger et al. paper (Ronneberger, Fischer, and Brox 2015).\nOf this architecture, numerous variants exist. You could use different layer sizes, activations, ways to achieve downsizing and upsizing, and more. However, there is one defining characteristic: the U-shape, stabilized by the “bridges” crossing over horizontally at all levels.\n\nIn a nutshell, the left-hand side of the U resembles the convolutional architectures used in image classification. It successively reduces spatial resolution. At the same time, another dimension – the channels dimension – is used to build up a hierarchy of features, ranging from very basic to very specialized.\nUnlike in classification, however, the output should have the same spatial resolution as the input. Thus, we need to upsize again – this is taken care of by the right-hand side of the U. But, how are we going to arrive at a good per-pixel classification, now that so much spatial information has been lost?\nThis is what the “bridges” are for: At each level, the input to an upsampling layer is a concatenation of the previous layer’s output – which went through the whole compression/decompression routine – and some preserved intermediate representation from the downsizing phase. In this way, a U-Net architecture combines attention to detail with feature extraction.\nBrain image segmentation\nWith U-Net, domain applicability is as broad as the architecture is flexible. Here, we want to detect abnormalities in brain scans. The dataset, used in Buda, Saha, and Mazurowski (2019), contains MRI images together with manually created FLAIR abnormality segmentation masks. It is available on Kaggle.\nNicely, the paper is accompanied by a GitHub repository. Below, we closely follow (though not exactly replicate) the authors’ preprocessing and data augmentation code.\nAs is often the case in medical imaging, there is notable class imbalance in the data. For every patient, sections have been taken at multiple positions. (Number of sections per patient varies.) Most sections do not exhibit any lesions; the corresponding masks are colored black everywhere.\nHere are three examples where the masks do indicate abnormalities:\n\nLet’s see if we can build a U-Net that generates such masks for us.\nData\nBefore you start typing, here is a Colaboratory notebook to conveniently follow along.\nWe use pins to obtain the data. Please see this introduction if you haven’t used that package before.\n\n\n# deep learning (incl. dependencies)\nlibrary(torch)\nlibrary(torchvision)\n\n# data wrangling\nlibrary(tidyverse)\nlibrary(zeallot)\n\n# image processing and visualization\nlibrary(magick)\nlibrary(cowplot)\n\n# dataset loading \nlibrary(pins)\nlibrary(zip)\n\ntorch_manual_seed(777)\nset.seed(777)\n\n# use your own kaggle.json here\npins::board_register_kaggle(token = \"~/kaggle.json\")\n\nfiles <- pins::pin_get(\"mateuszbuda/lgg-mri-segmentation\", board = \"kaggle\",  extract = FALSE)\n\n\nThe dataset is not that big – it includes scans from 110 different patients – so we’ll have to do with just a training and a validation set. (Don’t do this in real life, as you’ll inevitably end up fine-tuning on the latter.)\n\n\ntrain_dir <- \"data/mri_train\"\nvalid_dir <- \"data/mri_valid\"\n\nif(dir.exists(train_dir)) unlink(train_dir, recursive = TRUE, force = TRUE)\nif(dir.exists(valid_dir)) unlink(valid_dir, recursive = TRUE, force = TRUE)\n\nzip::unzip(files, exdir = \"data\")\n\nfile.rename(\"data/kaggle_3m\", train_dir)\n\n# this is a duplicate, again containing kaggle_3m (evidently a packaging error on Kaggle)\n# we just remove it\nunlink(\"data/lgg-mri-segmentation\", recursive = TRUE)\n\ndir.create(valid_dir)\n\n\nOf those 110 patients, we keep 30 for validation. Some more file manipulations, and we’re set up with a nice hierarchical structure, with train_dir and valid_dir holding their per-patient sub-directories, respectively.\n\n\nvalid_indices <- sample(1:length(patients), 30)\n\npatients <- list.dirs(train_dir, recursive = FALSE)\n\nfor (i in valid_indices) {\n  dir.create(file.path(valid_dir, basename(patients[i])))\n  for (f in list.files(patients[i])) {    \n    file.rename(file.path(train_dir, basename(patients[i]), f), file.path(valid_dir, basename(patients[i]), f))    \n  }\n  unlink(file.path(train_dir, basename(patients[i])), recursive = TRUE)\n}\n\n\nWe now need a dataset that knows what to do with these files.\nDataset\nLike every torch dataset, this one has initialize() and .getitem() methods. initialize() creates an inventory of scan and mask file names, to be used by .getitem() when it actually reads those files. In contrast to what we’ve seen in previous posts, though , .getitem() does not simply return input-target pairs in order. Instead, whenever the parameter random_sampling is true, it will perform weighted sampling, preferring items with sizable lesions. This option will be used for the training set, to counter the class imbalance mentioned above.\nThe other way training and validation sets will differ is use of data augmentation. Training images/masks may be flipped, re-sized, and rotated; probabilities and amounts are configurable.\nAn instance of brainseg_dataset encapsulates all this functionality:\n\n\nbrainseg_dataset <- dataset(\n  name = \"brainseg_dataset\",\n  \n  initialize = function(img_dir,\n                        augmentation_params = NULL,\n                        random_sampling = FALSE) {\n    self$images <- tibble(\n      img = grep(\n        list.files(\n          img_dir,\n          full.names = TRUE,\n          pattern = \"tif\",\n          recursive = TRUE\n        ),\n        pattern = 'mask',\n        invert = TRUE,\n        value = TRUE\n      ),\n      mask = grep(\n        list.files(\n          img_dir,\n          full.names = TRUE,\n          pattern = \"tif\",\n          recursive = TRUE\n        ),\n        pattern = 'mask',\n        value = TRUE\n      )\n    )\n    self$slice_weights <- self$calc_slice_weights(self$images$mask)\n    self$augmentation_params <- augmentation_params\n    self$random_sampling <- random_sampling\n  },\n  \n  .getitem = function(i) {\n    index <-\n      if (self$random_sampling == TRUE)\n        sample(1:self$.length(), 1, prob = self$slice_weights)\n    else\n      i\n    \n    img <- self$images$img[index] %>%\n      image_read() %>%\n      transform_to_tensor() \n    mask <- self$images$mask[index] %>%\n      image_read() %>%\n      transform_to_tensor() %>%\n      transform_rgb_to_grayscale() %>%\n      torch_unsqueeze(1)\n    \n    img <- self$min_max_scale(img)\n    \n    if (!is.null(self$augmentation_params)) {\n      scale_param <- self$augmentation_params[1]\n      c(img, mask) %<-% self$resize(img, mask, scale_param)\n      \n      rot_param <- self$augmentation_params[2]\n      c(img, mask) %<-% self$rotate(img, mask, rot_param)\n      \n      flip_param <- self$augmentation_params[3]\n      c(img, mask) %<-% self$flip(img, mask, flip_param)\n      \n    }\n    list(img = img, mask = mask)\n  },\n  \n  .length = function() {\n    nrow(self$images)\n  },\n  \n  calc_slice_weights = function(masks) {\n    weights <- map_dbl(masks, function(m) {\n      img <-\n        as.integer(magick::image_data(image_read(m), channels = \"gray\"))\n      sum(img / 255)\n    })\n    \n    sum_weights <- sum(weights)\n    num_weights <- length(weights)\n    \n    weights <- weights %>% map_dbl(function(w) {\n      w <- (w + sum_weights * 0.1 / num_weights) / (sum_weights * 1.1)\n    })\n    weights\n  },\n  \n  min_max_scale = function(x) {\n    min = x$min()$item()\n    max = x$max()$item()\n    x$clamp_(min = min, max = max)\n    x$add_(-min)$div_(max - min + 1e-5)\n    x\n  },\n  \n  resize = function(img, mask, scale_param) {\n    img_size <- dim(img)[2]\n    rnd_scale <- runif(1, 1 - scale_param, 1 + scale_param)\n    img <- transform_resize(img, size = rnd_scale * img_size)\n    mask <- transform_resize(mask, size = rnd_scale * img_size)\n    diff <- dim(img)[2] - img_size\n    if (diff > 0) {\n      top <- ceiling(diff / 2)\n      left <- ceiling(diff / 2)\n      img <- transform_crop(img, top, left, img_size, img_size)\n      mask <- transform_crop(mask, top, left, img_size, img_size)\n    } else {\n      img <- transform_pad(img,\n                           padding = -c(\n                             ceiling(diff / 2),\n                             floor(diff / 2),\n                             ceiling(diff / 2),\n                             floor(diff / 2)\n                           ))\n      mask <- transform_pad(mask, padding = -c(\n        ceiling(diff / 2),\n        floor(diff /\n                2),\n        ceiling(diff /\n                  2),\n        floor(diff /\n                2)\n      ))\n    }\n    list(img, mask)\n  },\n  \n  rotate = function(img, mask, rot_param) {\n    rnd_rot <- runif(1, 1 - rot_param, 1 + rot_param)\n    img <- transform_rotate(img, angle = rnd_rot)\n    mask <- transform_rotate(mask, angle = rnd_rot)\n    \n    list(img, mask)\n  },\n  \n  flip = function(img, mask, flip_param) {\n    rnd_flip <- runif(1)\n    if (rnd_flip > flip_param) {\n      img <- transform_hflip(img)\n      mask <- transform_hflip(mask)\n    }\n    \n    list(img, mask)\n  }\n)\n\n\nAfter instantiation, we see we have 2977 training pairs and 952 validation pairs, respectively:\n\n\ntrain_ds <- brainseg_dataset(\n  train_dir,\n  augmentation_params = c(0.05, 15, 0.5),\n  random_sampling = TRUE\n)\n\nlength(train_ds)\n# 2977\n\nvalid_ds <- brainseg_dataset(\n  valid_dir,\n  augmentation_params = NULL,\n  random_sampling = FALSE\n)\n\nlength(valid_ds)\n# 952\n\n\nAs a correctness check, let’s plot an image and associated mask:\n\n\npar(mfrow = c(1, 2), mar = c(0, 1, 0, 1))\n\nimg_and_mask <- valid_ds[27]\nimg <- img_and_mask[[1]]\nmask <- img_and_mask[[2]]\n\nimg$permute(c(2, 3, 1)) %>% as.array() %>% as.raster() %>% plot()\nmask$squeeze() %>% as.array() %>% as.raster() %>% plot()\n\n\n\n\n\nWith torch, it is straightforward to inspect what happens when you change augmentation-related parameters. We just pick a pair from the validation set, which has not had any augmentation applied as yet, and call valid_ds$<augmentation_func()> directly. Just for fun, let’s use more “extreme” parameters here than we do in actual training. (Actual training uses the settings from Mateusz’ GitHub repository, which we assume have been carefully chosen for optimal performance.1)\n\n\nimg_and_mask <- valid_ds[77]\nimg <- img_and_mask[[1]]\nmask <- img_and_mask[[2]]\n\nimgs <- map (1:24, function(i) {\n  \n  # scale factor; train_ds really uses 0.05\n  c(img, mask) %<-% valid_ds$resize(img, mask, 0.2) \n  c(img, mask) %<-% valid_ds$flip(img, mask, 0.5)\n  # rotation angle; train_ds really uses 15\n  c(img, mask) %<-% valid_ds$rotate(img, mask, 90) \n  img %>%\n    transform_rgb_to_grayscale() %>%\n    as.array() %>%\n    as_tibble() %>%\n    rowid_to_column(var = \"Y\") %>%\n    gather(key = \"X\", value = \"value\", -Y) %>%\n    mutate(X = as.numeric(gsub(\"V\", \"\", X))) %>%\n    ggplot(aes(X, Y, fill = value)) +\n    geom_raster() +\n    theme_void() +\n    theme(legend.position = \"none\") +\n    theme(aspect.ratio = 1)\n  \n})\n\nplot_grid(plotlist = imgs, nrow = 4)\n\n\n\n\n\nNow we still need the data loaders, and then, nothing keeps us from proceeding to the next big task: building the model.\n\n\nbatch_size <- 4\ntrain_dl <- dataloader(train_ds, batch_size)\nvalid_dl <- dataloader(valid_ds, batch_size)\n\n\nModel\nOur model nicely illustrates the kind of modular code that comes “naturally” with torch. We approach things top-down, starting with the U-Net container itself.\nunet takes care of the global composition – how far “down” do we go, shrinking the image while incrementing the number of filters, and then how do we go “up” again?\nImportantly, it is also in the system’s memory. In forward(), it keeps track of layer outputs seen going “down,” to be added back in going “up.”\n\n\nunet <- nn_module(\n  \"unet\",\n  \n  initialize = function(channels_in = 3,\n                        n_classes = 1,\n                        depth = 5,\n                        n_filters = 6) {\n    \n    self$down_path <- nn_module_list()\n    \n    prev_channels <- channels_in\n    for (i in 1:depth) {\n      self$down_path$append(down_block(prev_channels, 2 ^ (n_filters + i - 1)))\n      prev_channels <- 2 ^ (n_filters + i -1)\n    }\n    \n    self$up_path <- nn_module_list()\n    \n    for (i in ((depth - 1):1)) {\n      self$up_path$append(up_block(prev_channels, 2 ^ (n_filters + i - 1)))\n      prev_channels <- 2 ^ (n_filters + i - 1)\n    }\n    \n    self$last = nn_conv2d(prev_channels, n_classes, kernel_size = 1)\n  },\n  \n  forward = function(x) {\n    \n    blocks <- list()\n    \n    for (i in 1:length(self$down_path)) {\n      x <- self$down_path[[i]](x)\n      if (i != length(self$down_path)) {\n        blocks <- c(blocks, x)\n        x <- nnf_max_pool2d(x, 2)\n      }\n    }\n    \n    for (i in 1:length(self$up_path)) {  \n      x <- self$up_path[[i]](x, blocks[[length(blocks) - i + 1]]$to(device = device))\n    }\n    \n    torch_sigmoid(self$last(x))\n  }\n)\n\n\nunet delegates to two containers just below it in the hierarchy: down_block and up_block. While down_block is “just” there for aesthetic reasons (it immediately delegates to its own workhorse, conv_block), in up_block we see the U-Net “bridges” in action.\n\n\ndown_block <- nn_module(\n  \"down_block\",\n  \n  initialize = function(in_size, out_size) {\n    self$conv_block <- conv_block(in_size, out_size)\n  },\n  \n  forward = function(x) {\n    self$conv_block(x)\n  }\n)\n\nup_block <- nn_module(\n  \"up_block\",\n  \n  initialize = function(in_size, out_size) {\n    \n    self$up = nn_conv_transpose2d(in_size,\n                                  out_size,\n                                  kernel_size = 2,\n                                  stride = 2)\n    self$conv_block = conv_block(in_size, out_size)\n  },\n  \n  forward = function(x, bridge) {\n    \n    up <- self$up(x)\n    torch_cat(list(up, bridge), 2) %>%\n      self$conv_block()\n  }\n)\n\n\nFinally, a conv_block is a sequential structure containing convolutional, ReLU, and dropout layers.\n\n\nconv_block <- nn_module( \n  \"conv_block\",\n  \n  initialize = function(in_size, out_size) {\n    \n    self$conv_block <- nn_sequential(\n      nn_conv2d(in_size, out_size, kernel_size = 3, padding = 1),\n      nn_relu(),\n      nn_dropout(0.6),\n      nn_conv2d(out_size, out_size, kernel_size = 3, padding = 1),\n      nn_relu()\n    )\n  },\n  \n  forward = function(x){\n    self$conv_block(x)\n  }\n)\n\n\nNow instantiate the model, and possibly, move it to the GPU:\n\n\ndevice <- torch_device(if(cuda_is_available()) \"cuda\" else \"cpu\")\nmodel <- unet(depth = 5)$to(device = device)\n\n\nOptimization\nWe train our model with a combination of cross entropy and dice loss.\nThe latter, though not shipped with torch, may be implemented manually:\n\n\ncalc_dice_loss <- function(y_pred, y_true) {\n  \n  smooth <- 1\n  y_pred <- y_pred$view(-1)\n  y_true <- y_true$view(-1)\n  intersection <- (y_pred * y_true)$sum()\n  \n  1 - ((2 * intersection + smooth) / (y_pred$sum() + y_true$sum() + smooth))\n}\n\ndice_weight <- 0.3\n\n\nOptimization uses stochastic gradient descent (SGD), together with the one-cycle learning rate scheduler introduced in the context of image classification with torch.\n\n\noptimizer <- optim_sgd(model$parameters, lr = 0.1, momentum = 0.9)\n\nnum_epochs <- 20\n\nscheduler <- lr_one_cycle(\n  optimizer,\n  max_lr = 0.1,\n  steps_per_epoch = length(train_dl),\n  epochs = num_epochs\n)\n\n\nTraining\nThe training loop then follows the usual scheme. One thing to note: Every epoch, we save the model (using torch_save()), so we can later pick the best one, should performance have degraded thereafter.\n\n\ntrain_batch <- function(b) {\n  \n  optimizer$zero_grad()\n  output <- model(b[[1]]$to(device = device))\n  target <- b[[2]]$to(device = device)\n  \n  bce_loss <- nnf_binary_cross_entropy(output, target)\n  dice_loss <- calc_dice_loss(output, target)\n  loss <-  dice_weight * dice_loss + (1 - dice_weight) * bce_loss\n  \n  loss$backward()\n  optimizer$step()\n  scheduler$step()\n\n  list(bce_loss$item(), dice_loss$item(), loss$item())\n  \n}\n\nvalid_batch <- function(b) {\n  \n  output <- model(b[[1]]$to(device = device))\n  target <- b[[2]]$to(device = device)\n\n  bce_loss <- nnf_binary_cross_entropy(output, target)\n  dice_loss <- calc_dice_loss(output, target)\n  loss <-  dice_weight * dice_loss + (1 - dice_weight) * bce_loss\n  \n  list(bce_loss$item(), dice_loss$item(), loss$item())\n  \n}\n\nfor (epoch in 1:num_epochs) {\n  \n  model$train()\n  train_bce <- c()\n  train_dice <- c()\n  train_loss <- c()\n  \n  coro::loop(for (b in train_dl) {\n    c(bce_loss, dice_loss, loss) %<-% train_batch(b)\n    train_bce <- c(train_bce, bce_loss)\n    train_dice <- c(train_dice, dice_loss)\n    train_loss <- c(train_loss, loss)\n  })\n  \n  torch_save(model, paste0(\"model_\", epoch, \".pt\"))\n  \n  cat(sprintf(\"\\nEpoch %d, training: loss:%3f, bce: %3f, dice: %3f\\n\",\n              epoch, mean(train_loss), mean(train_bce), mean(train_dice)))\n  \n  model$eval()\n  valid_bce <- c()\n  valid_dice <- c()\n  valid_loss <- c()\n  \n  i <- 0\n  coro::loop(for (b in tvalid_dl) {\n    \n    i <<- i + 1\n    c(bce_loss, dice_loss, loss) %<-% valid_batch(b)\n    valid_bce <- c(valid_bce, bce_loss)\n    valid_dice <- c(valid_dice, dice_loss)\n    valid_loss <- c(valid_loss, loss)\n    \n  })\n  \n  cat(sprintf(\"\\nEpoch %d, validation: loss:%3f, bce: %3f, dice: %3f\\n\",\n              epoch, mean(valid_loss), mean(valid_bce), mean(valid_dice)))\n}\n\n\nEpoch 1, training: loss:0.304232, bce: 0.148578, dice: 0.667423\nEpoch 1, validation: loss:0.333961, bce: 0.127171, dice: 0.816471\n\nEpoch 2, training: loss:0.194665, bce: 0.101973, dice: 0.410945\nEpoch 2, validation: loss:0.341121, bce: 0.117465, dice: 0.862983\n\n[...]\n\nEpoch 19, training: loss:0.073863, bce: 0.038559, dice: 0.156236\nEpoch 19, validation: loss:0.302878, bce: 0.109721, dice: 0.753577\n\nEpoch 20, training: loss:0.070621, bce: 0.036578, dice: 0.150055\nEpoch 20, validation: loss:0.295852, bce: 0.101750, dice: 0.748757\nEvaluation\nIn this run, it is the final model that performs best on the validation set. Still, we’d like to show how to load a saved model, using torch_load() .\nOnce loaded, put the model into eval mode:\n\n\nsaved_model <- torch_load(\"model_20.pt\") \n\nmodel <- saved_model\nmodel$eval()\n\n\nNow, since we don’t have a separate test set, we already know the average out-of-sample metrics; but in the end, what we care about are the generated masks. Let’s view some, displaying ground truth and MRI scans for comparison.\n\n\n# without random sampling, we'd mainly see lesion-free patches\neval_ds <- brainseg_dataset(valid_dir, augmentation_params = NULL, random_sampling = TRUE)\neval_dl <- dataloader(eval_ds, batch_size = 8)\n\nbatch <- eval_dl %>% dataloader_make_iter() %>% dataloader_next()\n\npar(mfcol = c(3, 8), mar = c(0, 1, 0, 1))\n\nfor (i in 1:8) {\n  \n  img <- batch[[1]][i, .., drop = FALSE]\n  inferred_mask <- model(img$to(device = device))\n  true_mask <- batch[[2]][i, .., drop = FALSE]$to(device = device)\n  \n  bce <- nnf_binary_cross_entropy(inferred_mask, true_mask)$to(device = \"cpu\") %>%\n    as.numeric()\n  dc <- calc_dice_loss(inferred_mask, true_mask)$to(device = \"cpu\") %>% as.numeric()\n  cat(sprintf(\"\\nSample %d, bce: %3f, dice: %3f\\n\", i, bce, dc))\n  \n\n  inferred_mask <- inferred_mask$to(device = \"cpu\") %>% as.array() %>% .[1, 1, , ]\n  \n  inferred_mask <- ifelse(inferred_mask > 0.5, 1, 0)\n  \n  img[1, 1, ,] %>% as.array() %>% as.raster() %>% plot()\n  true_mask$to(device = \"cpu\")[1, 1, ,] %>% as.array() %>% as.raster() %>% plot()\n  inferred_mask %>% as.raster() %>% plot()\n}\n\n\nWe also print the individual cross entropy and dice losses; relating those to the generated masks might yield useful information for model tuning.\nSample 1, bce: 0.088406, dice: 0.387786}\n\nSample 2, bce: 0.026839, dice: 0.205724\n\nSample 3, bce: 0.042575, dice: 0.187884\n\nSample 4, bce: 0.094989, dice: 0.273895\n\nSample 5, bce: 0.026839, dice: 0.205724\n\nSample 6, bce: 0.020917, dice: 0.139484\n\nSample 7, bce: 0.094989, dice: 0.273895\n\nSample 8, bce: 2.310956, dice: 0.999824\n\n\n\nWhile far from perfect, most of these masks aren’t that bad – a nice result given the small dataset!\nWrapup\nThis has been our most complex torch post so far; however, we hope you’ve found the time well spent. For one, among applications of deep learning, medical image segmentation stands out as highly societally useful. Secondly, U-Net-like architectures are employed in many other areas. And finally, we once more saw torch’s flexibility and intuitive behavior in action.\nThanks for reading!\n\n\n\nBuda, Mateusz, Ashirbani Saha, and Maciej A. Mazurowski. 2019. “Association of Genomic Subtypes of Lower-Grade Gliomas with Shape Features Automatically Extracted by a Deep Learning Algorithm.” Computers in Biology and Medicine 109: 218–25. https://doi.org/https://doi.org/10.1016/j.compbiomed.2019.05.002.\n\n\nRonneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” CoRR abs/1505.04597. http://arxiv.org/abs/1505.04597.\n\n\nYes, we did a few experiments, confirming that more augmentation isn’t better … what did I say about inevitably ending up doing optimization on the validation set …?↩︎\n",
    "preview": "posts/2020-11-30-torch-brain-segmentation/images/scans.png",
    "last_modified": "2024-11-21T15:54:03+00:00",
    "input_file": {},
    "preview_width": 798,
    "preview_height": 542
  },
  {
    "path": "posts/2020-11-03-torch-tabular/",
    "title": "torch for tabular data",
    "description": "How not to die from poisonous mushrooms. Also: How to use torch for deep learning on tabular data, including a mix of categorical and numerical features.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-11-03",
    "categories": [
      "Torch",
      "R",
      "Tabular Data"
    ],
    "contents": "\n\nContents\nAgenda\nDataset\nModel\nTraining\nEvaluation\nMaking the task harder\nA look at the hidden representations\n\nMachine learning on image-like data can be many things: fun (dogs vs. cats), societally useful (medical imaging), or societally harmful (surveillance). In comparison, tabular data – the bread and butter of data science – may seem more mundane.\nWhat’s more, if you’re particularly interested in deep learning (DL), and looking for the extra benefits to be gained from big data, big architectures, and big compute, you’re much more likely to build an impressive showcase on the former instead of the latter.\nSo for tabular data, why not just go with random forests, or gradient boosting, or other classical methods? I can think of at least a few reasons to learn about DL for tabular data:\nEven if all your features are interval-scale or ordinal, thus requiring “just” some form of (not necessarily linear) regression, applying DL may result in performance benefits due to sophisticated optimization algorithms, activation functions, layer depth, and more (plus interactions of all of these).\nIf, in addition, there are categorical features, DL models may profit from embedding those in continuous space, discovering similarities and relationships that go unnoticed in one-hot encoded representations.\nWhat if most features are numeric or categorical, but there’s also text in column F and an image in column G? With DL, different modalities can be worked on by different modules that feed their outputs into a common module, to take over from there.\nAgenda\nIn this introductory post, we keep the architecture straightforward. We don’t experiment with fancy optimizers or nonlinearities. Nor do we add in text or image processing. However, we do make use of embeddings, and pretty prominently at that. Thus from the above bullet list, we’ll shed a light on the second, while leaving the other two for future posts.\nIn a nutshell, what we’ll see is\nHow to create a custom dataset, tailored to the specific data you have.\nHow to handle a mix of numeric and categorical data.\nHow to extract continuous-space representations from the embedding modules.\nDataset\nThe dataset, Mushrooms, was chosen for its abundance of categorical columns. It is an unusual dataset to use in DL: It was designed for machine learning models to infer logical rules, as in: IF a AND NOT b OR c […], then it’s an x.\nMushrooms are classified into two groups: edible and non-edible. The dataset description lists five possible rules with their resulting accuracies. While the least we want to go into here is the hotly debated topic of whether DL is suited to, or how it could be made more suited to rule learning, we’ll allow ourselves some curiosity and check out what happens if we successively remove all columns used to construct those five rules.\nOh, and before you start copy-pasting: Here is the example in a Google Colaboratory notebook.\n\n\nlibrary(torch)\nlibrary(purrr)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ggrepel)\n\ndownload.file(\n  \"https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\",\n  destfile = \"agaricus-lepiota.data\"\n)\n\nmushroom_data <- read_csv(\n  \"agaricus-lepiota.data\",\n  col_names = c(\n    \"poisonous\",\n    \"cap-shape\",\n    \"cap-surface\",\n    \"cap-color\",\n    \"bruises\",\n    \"odor\",\n    \"gill-attachment\",\n    \"gill-spacing\",\n    \"gill-size\",\n    \"gill-color\",\n    \"stalk-shape\",\n    \"stalk-root\",\n    \"stalk-surface-above-ring\",\n    \"stalk-surface-below-ring\",\n    \"stalk-color-above-ring\",\n    \"stalk-color-below-ring\",\n    \"veil-type\",\n    \"veil-color\",\n    \"ring-type\",\n    \"ring-number\",\n    \"spore-print-color\",\n    \"population\",\n    \"habitat\"\n  ),\n  col_types = rep(\"c\", 23) %>% paste(collapse = \"\")\n) %>%\n  # can as well remove because there's just 1 unique value\n  select(-`veil-type`)\n\n\nIn torch, dataset() creates an R6 class. As with most R6 classes, there will usually be a need for an initialize() method. Below, we use initialize() to preprocess the data and store it in convenient pieces. More on that in a minute. Prior to that, please note the two other methods a dataset has to implement:\n.getitem(i) . This is the whole purpose of a dataset: Retrieve and return the observation located at some index it is asked for. Which index? That’s to be decided by the caller, a dataloader. During training, usually we want to permute the order in which observations are used, while not caring about order in case of validation or test data.\n.length(). This method, again for use of a dataloader, indicates how many observations there are.\nIn our example, both methods are straightforward to implement. .getitem(i) directly uses its argument to index into the data, and .length() returns the number of observations:\n\n\nmushroom_dataset <- dataset(\n  name = \"mushroom_dataset\",\n\n  initialize = function(indices) {\n    data <- self$prepare_mushroom_data(mushroom_data[indices, ])\n    self$xcat <- data[[1]][[1]]\n    self$xnum <- data[[1]][[2]]\n    self$y <- data[[2]]\n  },\n\n  .getitem = function(i) {\n    xcat <- self$xcat[i, ]\n    xnum <- self$xnum[i, ]\n    y <- self$y[i, ]\n    \n    list(x = list(xcat, xnum), y = y)\n  },\n  \n  .length = function() {\n    dim(self$y)[1]\n  },\n  \n  prepare_mushroom_data = function(input) {\n    \n    input <- input %>%\n      mutate(across(.fns = as.factor)) \n    \n    target_col <- input$poisonous %>% \n      as.integer() %>%\n      `-`(1) %>%\n      as.matrix()\n    \n    categorical_cols <- input %>% \n      select(-poisonous) %>%\n      select(where(function(x) nlevels(x) != 2)) %>%\n      mutate(across(.fns = as.integer)) %>%\n      as.matrix()\n\n    numerical_cols <- input %>%\n      select(-poisonous) %>%\n      select(where(function(x) nlevels(x) == 2)) %>%\n      mutate(across(.fns = as.integer)) %>%\n      as.matrix()\n    \n    list(list(torch_tensor(categorical_cols), torch_tensor(numerical_cols)),\n         torch_tensor(target_col))\n  }\n)\n\n\nAs for data storage, there is a field for the target, self$y, but instead of the expected self$x we see separate fields for numerical features (self$xnum) and categorical ones (self$xcat). This is just for convenience: The latter will be passed into embedding modules, which require its inputs to be of type torch_long(), as opposed to most other modules that, by default, work with torch_float().\nAccordingly, then, all prepare_mushroom_data() does is break apart the data into those three parts.\nIndispensable aside: In this dataset, really all features happen to be categorical – it’s just that for some, there are but two types. Technically, we could just have treated them the same as the non-binary features. But since normally in DL, we just leave binary features the way they are, we use this as an occasion to show how to handle a mix of various data types.\nOur custom dataset defined, we create instances for training and validation; each gets its companion dataloader:\n\n\ntrain_indices <- sample(1:nrow(mushroom_data), size = floor(0.8 * nrow(mushroom_data)))\nvalid_indices <- setdiff(1:nrow(mushroom_data), train_indices)\n\ntrain_ds <- mushroom_dataset(train_indices)\ntrain_dl <- train_ds %>% dataloader(batch_size = 256, shuffle = TRUE)\n\nvalid_ds <- mushroom_dataset(valid_indices)\nvalid_dl <- valid_ds %>% dataloader(batch_size = 256, shuffle = FALSE)\n\n\nModel\nIn torch, how much you modularize your models is up to you. Often, high degrees of modularization enhance readability and help with troubleshooting.\nHere we factor out the embedding functionality. An embedding_module, to be passed the categorical features only, will call torch’s nn_embedding() on each of them:\n\n\nembedding_module <- nn_module(\n  \n  initialize = function(cardinalities) {\n    self$embeddings = nn_module_list(lapply(cardinalities, function(x) nn_embedding(num_embeddings = x, embedding_dim = ceiling(x/2))))\n  },\n  \n  forward = function(x) {\n    embedded <- vector(mode = \"list\", length = length(self$embeddings))\n    for (i in 1:length(self$embeddings)) {\n      embedded[[i]] <- self$embeddings[[i]](x[ , i])\n    }\n    torch_cat(embedded, dim = 2)\n  }\n)\n\n\nThe main model, when called, starts by embedding the categorical features, then appends the numerical input and continues processing:\n\n\nnet <- nn_module(\n  \"mushroom_net\",\n\n  initialize = function(cardinalities,\n                        num_numerical,\n                        fc1_dim,\n                        fc2_dim) {\n    self$embedder <- embedding_module(cardinalities)\n    self$fc1 <- nn_linear(sum(map(cardinalities, function(x) ceiling(x/2)) %>% unlist()) + num_numerical, fc1_dim)\n    self$fc2 <- nn_linear(fc1_dim, fc2_dim)\n    self$output <- nn_linear(fc2_dim, 1)\n  },\n\n  forward = function(xcat, xnum) {\n    embedded <- self$embedder(xcat)\n    all <- torch_cat(list(embedded, xnum$to(dtype = torch_float())), dim = 2)\n    all %>% self$fc1() %>%\n      nnf_relu() %>%\n      self$fc2() %>%\n      self$output() %>%\n      nnf_sigmoid()\n  }\n)\n\n\nNow instantiate this model, passing in, on the one hand, output sizes for the linear layers, and on the other, feature cardinalities. The latter will be used by the embedding modules to determine their output sizes, following a simple rule “embed into a space of size half the number of input values”:\n\n\ncardinalities <- map(\n  mushroom_data[ , 2:ncol(mushroom_data)], compose(nlevels, as.factor)) %>%\n  keep(function(x) x > 2) %>%\n  unlist() %>%\n  unname()\n\nnum_numerical <- ncol(mushroom_data) - length(cardinalities) - 1\n\nfc1_dim <- 16\nfc2_dim <- 16\n\nmodel <- net(\n  cardinalities,\n  num_numerical,\n  fc1_dim,\n  fc2_dim\n)\n\ndevice <- if (cuda_is_available()) torch_device(\"cuda:0\") else \"cpu\"\n\nmodel <- model$to(device = device)\n\n\nTraining\nThe training loop now is “business as usual”:\n\n\noptimizer <- optim_adam(model$parameters, lr = 0.1)\n\nfor (epoch in 1:20) {\n\n  model$train()\n  train_losses <- c()  \n\n  coro::loop(for (b in train_dl) {\n    optimizer$zero_grad()\n    output <- model(b$x[[1]]$to(device = device), b$x[[2]]$to(device = device))\n    loss <- nnf_binary_cross_entropy(output, b$y$to(dtype = torch_float(), device = device))\n    loss$backward()\n    optimizer$step()\n    train_losses <- c(train_losses, loss$item())\n  })\n\n  model$eval()\n  valid_losses <- c()\n\n  coro::loop(for (b in valid_dl) {\n    output <- model(b$x[[1]]$to(device = device), b$x[[2]]$to(device = device))\n    loss <- nnf_binary_cross_entropy(output, b$y$to(dtype = torch_float(), device = device))\n    valid_losses <- c(valid_losses, loss$item())\n  })\n\n  cat(sprintf(\"Loss at epoch %d: training: %3f, validation: %3f\\n\", epoch, mean(train_losses), mean(valid_losses)))\n}\n\n\nLoss at epoch 1: training: 0.274634, validation: 0.111689\nLoss at epoch 2: training: 0.057177, validation: 0.036074\nLoss at epoch 3: training: 0.025018, validation: 0.016698\nLoss at epoch 4: training: 0.010819, validation: 0.010996\nLoss at epoch 5: training: 0.005467, validation: 0.002849\nLoss at epoch 6: training: 0.002026, validation: 0.000959\nLoss at epoch 7: training: 0.000458, validation: 0.000282\nLoss at epoch 8: training: 0.000231, validation: 0.000190\nLoss at epoch 9: training: 0.000172, validation: 0.000144\nLoss at epoch 10: training: 0.000120, validation: 0.000110\nLoss at epoch 11: training: 0.000098, validation: 0.000090\nLoss at epoch 12: training: 0.000079, validation: 0.000074\nLoss at epoch 13: training: 0.000066, validation: 0.000064\nLoss at epoch 14: training: 0.000058, validation: 0.000055\nLoss at epoch 15: training: 0.000052, validation: 0.000048\nLoss at epoch 16: training: 0.000043, validation: 0.000042\nLoss at epoch 17: training: 0.000038, validation: 0.000038\nLoss at epoch 18: training: 0.000034, validation: 0.000034\nLoss at epoch 19: training: 0.000032, validation: 0.000031\nLoss at epoch 20: training: 0.000028, validation: 0.000027\nWhile loss on the validation set is still decreasing, we’ll soon see that the network has learned enough to obtain an accuracy of 100%.\nEvaluation\nTo check classification accuracy, we re-use the validation set, seeing how we haven’t employed it for tuning anyway.\n\n\nmodel$eval()\n\ntest_dl <- valid_ds %>% dataloader(batch_size = valid_ds$.length(), shuffle = FALSE)\niter <- test_dl$.iter()\nb <- iter$.next()\n\noutput <- model(b$x[[1]]$to(device = device), b$x[[2]]$to(device = device))\npreds <- output$to(device = \"cpu\") %>% as.array()\npreds <- ifelse(preds > 0.5, 1, 0)\n\ncomp_df <- data.frame(preds = preds, y = b[[2]] %>% as_array())\nnum_correct <- sum(comp_df$preds == comp_df$y)\nnum_total <- nrow(comp_df)\naccuracy <- num_correct/num_total\naccuracy\n\n\n1\nPhew. No embarrassing failure for the DL approach on a task where straightforward rules are sufficient. Plus, we’ve really been parsimonious as to network size.\nBefore concluding with an inspection of the learned embeddings, let’s have some fun obscuring things.\nMaking the task harder\nThe following rules (with accompanying accuracies) are reported in the dataset description.\nDisjunctive rules for poisonous mushrooms, from most general\n    to most specific:\n\n    P_1) odor=NOT(almond.OR.anise.OR.none)\n         120 poisonous cases missed, 98.52% accuracy\n\n    P_2) spore-print-color=green\n         48 cases missed, 99.41% accuracy\n         \n    P_3) odor=none.AND.stalk-surface-below-ring=scaly.AND.\n              (stalk-color-above-ring=NOT.brown) \n         8 cases missed, 99.90% accuracy\n         \n    P_4) habitat=leaves.AND.cap-color=white\n             100% accuracy     \n\n    Rule P_4) may also be\n\n    P_4') population=clustered.AND.cap_color=white\n\n    These rule involve 6 attributes (out of 22). \nEvidently, there’s no distinction being made between training and test sets; but we’ll stay with our 80:20 split anyway. We’ll successively remove all mentioned attributes, starting with the three that enabled 100% accuracy, and continuing our way up. Here are the results I obtained seeding the random number generator like so:\n\n\ntorch_manual_seed(777)\n\n\nwithout\naccuracy\ncap-color, population, habitat\n0.9938\ncap-color, population, habitat, stalk-surface-below-ring, stalk-color-above-ring\n1\ncap-color, population, habitat, stalk-surface-below-ring, stalk-color-above-ring, spore-print-color\n0.9994\ncap-color, population, habitat, stalk-surface-below-ring, stalk-color-above-ring, spore-print-color, odor\n0.9526\nStill 95% correct … While experiments like this are fun, it looks like they can also tell us something serious: Imagine the case of so-called “debiasing” by removing features like race, gender, or income. How many proxy variables may still be left that allow for inferring the masked attributes?\nA look at the hidden representations\nLooking at the weight matrix of an embedding module, what we see are the learned representations of a feature’s values. The first categorical column was cap-shape; let’s extract its corresponding embeddings:\n\n\nembedding_weights <- vector(mode = \"list\")\nfor (i in 1: length(model$embedder$embeddings)) {\n  embedding_weights[[i]] <- model$embedder$embeddings[[i]]$parameters$weight$to(device = \"cpu\")\n}\n\ncap_shape_repr <- embedding_weights[[1]]\ncap_shape_repr\n\n\ntorch_tensor\n-0.0025 -0.1271  1.8077\n-0.2367 -2.6165 -0.3363\n-0.5264 -0.9455 -0.6702\n 0.3057 -1.8139  0.3762\n-0.8583 -0.7752  1.0954\n 0.2740 -0.7513  0.4879\n[ CPUFloatType{6,3} ]\nThe number of columns is three, since that’s what we chose when creating the embedding layer. The number of rows is six, matching the number of available categories. We may look up per-feature categories in the dataset description (agaricus-lepiota.names):\n\n\ncap_shapes <- c(\"bell\", \"conical\", \"convex\", \"flat\", \"knobbed\", \"sunken\")\n\n\nFor visualization, it’s convenient to do principal components analysis (but there are other options, like t-SNE). Here are the six cap shapes in two-dimensional space:\n\n\npca <- prcomp(cap_shape_repr, center = TRUE, scale. = TRUE, rank = 2)$x[, c(\"PC1\", \"PC2\")]\n\npca %>%\n  as.data.frame() %>%\n  mutate(class = cap_shapes) %>%\n  ggplot(aes(x = PC1, y = PC2)) +\n  geom_point() +\n  geom_label_repel(aes(label = class)) + \n  coord_cartesian(xlim = c(-2, 2), ylim = c(-2, 2)) +\n  theme(aspect.ratio = 1) +\n  theme_classic()\n\n\n\n\n\nNaturally, how interesting you find the results depends on how much you care about the hidden representation of a variable. Analyses like these may quickly turn into an activity where extreme caution is to be applied, as any biases in the data will immediately translate into biased representations. Moreover, reduction to two-dimensional space may or may not be adequate.\nThis concludes our introduction to torch for tabular data. While the conceptual focus was on categorical features, and how to make use of them in combination with numerical ones, we’ve taken care to also provide background on something that will come up time and again: defining a dataset tailored to the task at hand.\nThanks for reading!\n\n\n\n",
    "preview": "posts/2020-11-03-torch-tabular/images/preview.jpeg",
    "last_modified": "2024-11-21T15:51:58+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-10-19-torch-image-classification/",
    "title": "Classifying images with torch",
    "description": "We learn about transfer learning, input pipelines, and learning rate schedulers, all while using torch to tell apart species of beautiful birds.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-10-19",
    "categories": [
      "Torch",
      "R",
      "Image Recognition & Image Processing"
    ],
    "contents": "\n\nContents\nData loading and preprocessing\nModel\nTraining\nTest set accuracy\nWrapup\n\nIn recent posts, we’ve been exploring essential torch functionality: tensors, the sine qua non of every deep learning framework; autograd, torch’s implementation of reverse-mode automatic differentiation; modules, composable building blocks of neural networks; and optimizers, the – well – optimization algorithms that torch provides.\nBut we haven’t really had our “hello world” moment yet, at least not if by “hello world” you mean the inevitable deep learning experience of classifying pets. Cat or dog? Beagle or boxer? Chinook or Chihuahua? We’ll distinguish ourselves by asking a (slightly) different question: What kind of bird?\nTopics we’ll address on our way:\nThe core roles of torch datasets and data loaders, respectively.\nHow to apply transforms, both for image preprocessing and data augmentation.\nHow to use Resnet (He et al. 2015), a pre-trained model that comes with torchvision, for transfer learning.\nHow to use learning rate schedulers, and in particular, the one-cycle learning rate algorithm [@abs-1708-07120].\nHow to find a good initial learning rate.\nFor convenience, the code is available on Google Colaboratory – no copy-pasting required.\nData loading and preprocessing\nThe example dataset used here is available on Kaggle.\nConveniently, it may be obtained using torchdatasets, which uses pins for authentication, retrieval and storage. To enable pins to manage your Kaggle downloads, please follow the instructions here.\nThis dataset is very “clean,” unlike the images we may be used to from, e.g., ImageNet. To help with generalization, we introduce noise during training – in other words, we perform data augmentation. In torchvision, data augmentation is part of an image processing pipeline that first converts an image to a tensor, and then applies any transformations such as resizing, cropping, normalization, or various forms of distorsion.\nBelow are the transformations performed on the training set. Note how most of them are for data augmentation, while normalization is done to comply with what’s expected by ResNet.\nImage preprocessing pipeline\n\n\nlibrary(torch)\nlibrary(torchvision)\nlibrary(torchdatasets)\n\nlibrary(dplyr)\nlibrary(pins)\nlibrary(ggplot2)\n\ndevice <- if (cuda_is_available()) torch_device(\"cuda:0\") else \"cpu\"\n\ntrain_transforms <- function(img) {\n  img %>%\n    # first convert image to tensor\n    transform_to_tensor() %>%\n    # then move to the GPU (if available)\n    (function(x) x$to(device = device)) %>%\n    # data augmentation\n    transform_random_resized_crop(size = c(224, 224)) %>%\n    # data augmentation\n    transform_color_jitter() %>%\n    # data augmentation\n    transform_random_horizontal_flip() %>%\n    # normalize according to what is expected by resnet\n    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225))\n}\n\n\nOn the validation set, we don’t want to introduce noise, but still need to resize, crop, and normalize the images. The test set should be treated identically.\n\n\nvalid_transforms <- function(img) {\n  img %>%\n    transform_to_tensor() %>%\n    (function(x) x$to(device = device)) %>%\n    transform_resize(256) %>%\n    transform_center_crop(224) %>%\n    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225))\n}\n\ntest_transforms <- valid_transforms\n\n\nAnd now, let’s get the data, nicely divided into training, validation and test sets. Additionally, we tell the corresponding R objects what transformations they’re expected to apply:1\n\n\ntrain_ds <- bird_species_dataset(\"data\", download = TRUE, transform = train_transforms)\n\nvalid_ds <- bird_species_dataset(\"data\", split = \"valid\", transform = valid_transforms)\n\ntest_ds <- bird_species_dataset(\"data\", split = \"test\", transform = test_transforms)\n\n\nTwo things to note. First, transformations are part of the dataset concept, as opposed to the data loader we’ll encounter shortly. Second, let’s take a look at how the images have been stored on disk. The overall directory structure (starting from data, which we specified as the root directory to be used) is this:\ndata/bird_species/train\ndata/bird_species/valid\ndata/bird_species/test\nIn the train, valid, and test directories, different classes of images reside in their own folders. For example, here is the directory layout for the first three classes in the test set:\ndata/bird_species/test/ALBATROSS/\n - data/bird_species/test/ALBATROSS/1.jpg\n - data/bird_species/test/ALBATROSS/2.jpg\n - data/bird_species/test/ALBATROSS/3.jpg\n - data/bird_species/test/ALBATROSS/4.jpg\n - data/bird_species/test/ALBATROSS/5.jpg\n \ndata/test/'ALEXANDRINE PARAKEET'/\n - data/bird_species/test/'ALEXANDRINE PARAKEET'/1.jpg\n - data/bird_species/test/'ALEXANDRINE PARAKEET'/2.jpg\n - data/bird_species/test/'ALEXANDRINE PARAKEET'/3.jpg\n - data/bird_species/test/'ALEXANDRINE PARAKEET'/4.jpg\n - data/bird_species/test/'ALEXANDRINE PARAKEET'/5.jpg\n \n data/test/'AMERICAN BITTERN'/\n - data/bird_species/test/'AMERICAN BITTERN'/1.jpg\n - data/bird_species/test/'AMERICAN BITTERN'/2.jpg\n - data/bird_species/test/'AMERICAN BITTERN'/3.jpg\n - data/bird_species/test/'AMERICAN BITTERN'/4.jpg\n - data/bird_species/test/'AMERICAN BITTERN'/5.jpg\nThis is exactly the kind of layout expected by torchs image_folder_dataset() – and really bird_species_dataset() instantiates a subtype of this class. Had we downloaded the data manually, respecting the required directory structure, we could have created the datasets like so:\n\n\n# e.g.\ntrain_ds <- image_folder_dataset(\n  file.path(data_dir, \"train\"),\n  transform = train_transforms)\n\n\nNow that we got the data, let’s see how many items there are in each set.\n\n\ntrain_ds$.length()\nvalid_ds$.length()\ntest_ds$.length()\n\n\n31316\n1125\n1125\nThat training set is really big! It’s thus recommended to run this on GPU, or just play around with the provided Colab notebook.\nWith so many samples, we’re curious how many classes there are.\n\n\nclass_names <- test_ds$classes\nlength(class_names)\n\n\n225\nSo we do have a substantial training set, but the task is formidable as well: We’re going to tell apart no less than 225 different bird species.\nData loaders\nWhile datasets know what to do with each single item, data loaders know how to treat them collectively. How many samples make up a batch? Do we want to feed them in the same order always, or instead, have a different order chosen for every epoch?\n\n\nbatch_size <- 64\n\ntrain_dl <- dataloader(train_ds, batch_size = batch_size, shuffle = TRUE)\nvalid_dl <- dataloader(valid_ds, batch_size = batch_size)\ntest_dl <- dataloader(test_ds, batch_size = batch_size)\n\n\nData loaders, too, may be queried for their length. Now length means: How many batches?\n\n\ntrain_dl$.length() \nvalid_dl$.length() \ntest_dl$.length()  \n\n\n490\n18\n18\nSome birds\nNext, let’s view a few images from the test set. We can retrieve the first batch – images and corresponding classes – by creating an iterator from the dataloader and calling next() on it:\n\n\n# for display purposes, here we are actually using a batch_size of 24\nbatch <- train_dl$.iter()$.next()\n\n\nbatch is a list, the first item being the image tensors:\n\n\nbatch[[1]]$size()\n\n\n[1]  24   3 224 224\nAnd the second, the classes:\n\n\nbatch[[2]]$size()\n\n\n[1] 24\nClasses are coded as integers, to be used as indices in a vector of class names. We’ll use those for labeling the images.\n\n\nclasses <- batch[[2]]\nclasses\n\n\ntorch_tensor \n 1\n 1\n 1\n 1\n 1\n 2\n 2\n 2\n 2\n 2\n 3\n 3\n 3\n 3\n 3\n 4\n 4\n 4\n 4\n 4\n 5\n 5\n 5\n 5\n[ GPULongType{24} ]\nThe image tensors have shape batch_size x num_channels x height x width. For plotting using as.raster(), we need to reshape the images such that channels come last. We also undo the normalization applied by the dataloader.\nHere are the first twenty-four images:\n\n\nlibrary(dplyr)\n\nimages <- as_array(batch[[1]]) %>% aperm(perm = c(1, 3, 4, 2))\nmean <- c(0.485, 0.456, 0.406)\nstd <- c(0.229, 0.224, 0.225)\nimages <- std * images + mean\nimages <- images * 255\nimages[images > 255] <- 255\nimages[images < 0] <- 0\n\npar(mfcol = c(4,6), mar = rep(1, 4))\n\nimages %>%\n  purrr::array_tree(1) %>%\n  purrr::set_names(class_names[as_array(classes)]) %>%\n  purrr::map(as.raster, max = 255) %>%\n  purrr::iwalk(~{plot(.x); title(.y)})\n\n\n\n\n\nModel\nThe backbone of our model is a pre-trained instance of ResNet.\n\n\nmodel <- model_resnet18(pretrained = TRUE)\n\n\nBut we want to distinguish among our 225 bird species, while ResNet was trained on 1000 different classes. What can we do? We simply replace the output layer.\nThe new output layer is also the only one whose weights we are going to train – leaving all other ResNet parameters the way they are. Technically, we could perform backpropagation through the complete model, striving to fine-tune ResNet’s weights as well. However, this would slow down training significantly. In fact, the choice is not all-or-none: It is up to us how many of the original parameters to keep fixed, and how many to “set free” for fine tuning. For the task at hand, we’ll be content to just train the newly added output layer: With the abundance of animals, including birds, in ImageNet, we expect the trained ResNet to know a lot about them!\n\n\nmodel$parameters %>% purrr::walk(function(param) param$requires_grad_(FALSE))\n\n\nTo replace the output layer, the model is modified in-place:\n\n\nnum_features <- model$fc$in_features\n\nmodel$fc <- nn_linear(in_features = num_features, out_features = length(class_names))\n\n\nNow put the modified model on the GPU (if available):\n\n\nmodel <- model$to(device = device)\n\n\nTraining\nFor optimization, we use cross entropy loss and stochastic gradient descent.\n\n\ncriterion <- nn_cross_entropy_loss()\n\noptimizer <- optim_sgd(model$parameters, lr = 0.1, momentum = 0.9)\n\n\nFinding an optimally efficient learning rate\nWe set the learning rate to 0.1, but that is just a formality. As has become widely known due to the excellent lectures by fast.ai, it makes sense to spend some time upfront to determine an efficient learning rate. While out-of-the-box, torch does not provide a tool like fast.ai’s learning rate finder, the logic is straightforward to implement. Here’s how to find a good learning rate, as translated to R from Sylvain Gugger’s post:\n\n\n# ported from: https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html\n\nlosses <- c()\nlog_lrs <- c()\n\nfind_lr <- function(init_value = 1e-8, final_value = 10, beta = 0.98) {\n\n  num <- train_dl$.length()\n  mult = (final_value/init_value)^(1/num)\n  lr <- init_value\n  optimizer$param_groups[[1]]$lr <- lr\n  avg_loss <- 0\n  best_loss <- 0\n  batch_num <- 0\n\n  coro::loop(for (b in train_dl) {\n\n    batch_num <- batch_num + 1\n    optimizer$zero_grad()\n    output <- model(b[[1]]$to(device = device))\n    loss <- criterion(output, b[[2]]$to(device = device))\n\n    #Compute the smoothed loss\n    avg_loss <- beta * avg_loss + (1-beta) * loss$item()\n    smoothed_loss <- avg_loss / (1 - beta^batch_num)\n    #Stop if the loss is exploding\n    if (batch_num > 1 && smoothed_loss > 4 * best_loss) break\n    #Record the best loss\n    if (smoothed_loss < best_loss || batch_num == 1) best_loss <- smoothed_loss\n\n    #Store the values\n    losses <<- c(losses, smoothed_loss)\n    log_lrs <<- c(log_lrs, (log(lr, 10)))\n\n    loss$backward()\n    optimizer$step()\n\n    #Update the lr for the next step\n    lr <- lr * mult\n    optimizer$param_groups[[1]]$lr <- lr\n  })\n}\n\nfind_lr()\n\ndf <- data.frame(log_lrs = log_lrs, losses = losses)\nggplot(df, aes(log_lrs, losses)) + geom_point(size = 1) + theme_classic()\n\n\n\n\n\nThe best learning rate is not the exact one where loss is at a minimum. Instead, it should be picked somewhat earlier on the curve, while loss is still decreasing. 0.05 looks like a sensible choice.\nThis value is nothing but an anchor, however. Learning rate schedulers allow learning rates to evolve according to some proven algorithm. Among others, torch implements one-cycle learning [@abs-1708-07120], cyclical learning rates (Smith 2015), and cosine annealing with warm restarts (Loshchilov and Hutter 2016).\nHere, we use lr_one_cycle(), passing in our newly found, optimally efficient, hopefully, value 0.05 as a maximum learning rate. lr_one_cycle() will start with a low rate, then gradually ramp up until it reaches the allowed maximum. After that, the learning rate will slowly, continuously decrease, until it falls slightly below its initial value.\nAll this happens not per epoch, but exactly once, which is why the name has one_cycle in it. Here’s how the evolution of learning rates looks in our example:\n\n\n\nBefore we start training, let’s quickly re-initialize the model, so as to start from a clean slate:\n\n\nmodel <- model_resnet18(pretrained = TRUE)\nmodel$parameters %>% purrr::walk(function(param) param$requires_grad_(FALSE))\n\nnum_features <- model$fc$in_features\n\nmodel$fc <- nn_linear(in_features = num_features, out_features = length(class_names))\n\nmodel <- model$to(device = device)\n\ncriterion <- nn_cross_entropy_loss()\n\noptimizer <- optim_sgd(model$parameters, lr = 0.05, momentum = 0.9)\n\n\nAnd instantiate the scheduler:\n\n\nnum_epochs = 10\n\nscheduler <- optimizer %>% \n  lr_one_cycle(max_lr = 0.05, epochs = num_epochs, steps_per_epoch = train_dl$.length())\n\n\nTraining loop\nNow we train for ten epochs. For every training batch, we call scheduler$step() to adjust the learning rate. Notably, this has to be done after optimizer$step().\n\n\ntrain_batch <- function(b) {\n\n  optimizer$zero_grad()\n  output <- model(b[[1]])\n  loss <- criterion(output, b[[2]]$to(device = device))\n  loss$backward()\n  optimizer$step()\n  scheduler$step()\n  loss$item()\n\n}\n\nvalid_batch <- function(b) {\n\n  output <- model(b[[1]])\n  loss <- criterion(output, b[[2]]$to(device = device))\n  loss$item()\n}\n\nfor (epoch in 1:num_epochs) {\n\n  model$train()\n  train_losses <- c()\n\n  coro::loop(for (b in train_dl) {\n    loss <- train_batch(b)\n    train_losses <- c(train_losses, loss)\n  })\n\n  model$eval()\n  valid_losses <- c()\n\n  coro::loop(for (b in valid_dl) {\n    loss <- valid_batch(b)\n    valid_losses <- c(valid_losses, loss)\n  })\n\n  cat(sprintf(\"\\nLoss at epoch %d: training: %3f, validation: %3f\\n\", epoch, mean(train_losses), mean(valid_losses)))\n}\n\n\nLoss at epoch 1: training: 2.662901, validation: 0.790769\n\nLoss at epoch 2: training: 1.543315, validation: 1.014409\n\nLoss at epoch 3: training: 1.376392, validation: 0.565186\n\nLoss at epoch 4: training: 1.127091, validation: 0.575583\n\nLoss at epoch 5: training: 0.916446, validation: 0.281600\n\nLoss at epoch 6: training: 0.775241, validation: 0.215212\n\nLoss at epoch 7: training: 0.639521, validation: 0.151283\n\nLoss at epoch 8: training: 0.538825, validation: 0.106301\n\nLoss at epoch 9: training: 0.407440, validation: 0.083270\n\nLoss at epoch 10: training: 0.354659, validation: 0.080389\nIt looks like the model made good progress, but we don’t yet know anything about classification accuracy in absolute terms. We’ll check that out on the test set.\nTest set accuracy\nFinally, we calculate accuracy on the test set:\n\n\nmodel$eval()\n\ntest_batch <- function(b) {\n\n  output <- model(b[[1]])\n  labels <- b[[2]]$to(device = device)\n  loss <- criterion(output, labels)\n  \n  test_losses <<- c(test_losses, loss$item())\n  # torch_max returns a list, with position 1 containing the values\n  # and position 2 containing the respective indices\n  predicted <- torch_max(output$data(), dim = 2)[[2]]\n  total <<- total + labels$size(1)\n  # add number of correct classifications in this batch to the aggregate\n  correct <<- correct + (predicted == labels)$sum()$item()\n\n}\n\ntest_losses <- c()\ntotal <- 0\ncorrect <- 0\n\nfor (b in enumerate(test_dl)) {\n  test_batch(b)\n}\n\nmean(test_losses)\n\n\n[1] 0.03719\n\n\ntest_accuracy <-  correct/total\ntest_accuracy\n\n\n[1] 0.98756\nAn impressive result, given how many different species there are!\nWrapup\nHopefully, this has been a useful introduction to classifying images with torch, as well as to its non-domain-specific architectural elements, like datasets, data loaders, and learning-rate schedulers. Future posts will explore other domains, as well as move on beyond “hello world” in image recognition. Thanks for reading!\n\n\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Deep Residual Learning for Image Recognition.” CoRR abs/1512.03385. http://arxiv.org/abs/1512.03385.\n\n\nLoshchilov, Ilya, and Frank Hutter. 2016. “SGDR: Stochastic Gradient Descent with Restarts.” CoRR abs/1608.03983. http://arxiv.org/abs/1608.03983.\n\n\nSmith, Leslie N. 2015. “No More Pesky Learning Rate Guessing Games.” CoRR abs/1506.01186. http://arxiv.org/abs/1506.01186.\n\n\nPhysically, the dataset consists of a single zip file; so it is really the first instruction that downloads all the data. The remaining two function calls perform semantic mappings only.↩︎\n",
    "preview": "posts/2020-10-19-torch-image-classification/images/image_classif_birds.png",
    "last_modified": "2024-11-21T15:51:20+00:00",
    "input_file": {},
    "preview_width": 500,
    "preview_height": 333
  },
  {
    "path": "posts/2020-10-12-sparklyr-flint-0.2.0-released/",
    "title": "sparklyr.flint 0.2: ASOF Joins, OLS Regression, and additional summarizers",
    "description": "We are excited to announce a number of powerful, new functionalities and improvements which are now part of sparklyr.flint 0.2!",
    "author": [
      {
        "name": "Yitao Li",
        "url": "https://github.com/yitao-li"
      }
    ],
    "date": "2020-10-12",
    "categories": [
      "R",
      "Packages/Releases",
      "Time Series"
    ],
    "contents": "\nSince sparklyr.flint, a sparklyr extension for leveraging Flint time series functionalities through sparklyr, was introduced in September, we have made a number of enhancements to it, and have successfully submitted sparklyr.flint 0.2 to CRAN.\nIn this blog post, we highlight the following new features and improvements from sparklyr.flint 0.2:\nASOF Joins of Timeseries RDDs\nOLS Regression\nAdditional Summarizers\nBetter Integration With sparklyr\nASOF Joins\nFor those unfamiliar with the term, ASOF joins are temporal join operations based on inexact matching of timestamps. Within the context of Apache Spark, a join operation, loosely speaking, matches records from two data frames (let’s call them left and right) based on some criteria. A temporal join implies matching records in left and right based on timestamps, and with inexact matching of timestamps permitted, it is typically useful to join left and right along one of the following temporal directions:\nLooking behind: if a record from left has timestamp t, then it gets matched with ones from right having the most recent timestamp less than or equal to t.\nLooking ahead: if a record from left has timestamp t, then it gets matched with ones from right having the smallest timestamp greater than or equal to (or alternatively, strictly greater than) t.\nHowever, oftentimes it is not useful to consider two timestamps as “matching” if they are too far apart. Therefore, an additional constraint on the maximum amount of time to look behind or look ahead is usually also part of an ASOF join operation.\nIn sparklyr.flint 0.2, all ASOF join functionalities of Flint are accessible via the asof_join() method. For example, given 2 timeseries RDDs left and right:\nlibrary(sparklyr)\nlibrary(sparklyr.flint)\n\nsc <- spark_connect(master = \"local\")\nleft <- copy_to(sc, tibble::tibble(t = seq(10), u = seq(10))) %>%\n  from_sdf(is_sorted = TRUE, time_unit = \"SECONDS\", time_column = \"t\")\nright <- copy_to(sc, tibble::tibble(t = seq(10) + 1, v = seq(10) + 1L)) %>%\n  from_sdf(is_sorted = TRUE, time_unit = \"SECONDS\", time_column = \"t\")\nThe following prints the result of matching each record from left with the most recent record(s) from right that are at most 1 second behind.\nprint(asof_join(left, right, tol = \"1s\", direction = \">=\") %>% to_sdf())\n\n## # Source: spark<?> [?? x 3]\n##    time                    u     v\n##    <dttm>              <int> <int>\n##  1 1970-01-01 00:00:01     1    NA\n##  2 1970-01-01 00:00:02     2     2\n##  3 1970-01-01 00:00:03     3     3\n##  4 1970-01-01 00:00:04     4     4\n##  5 1970-01-01 00:00:05     5     5\n##  6 1970-01-01 00:00:06     6     6\n##  7 1970-01-01 00:00:07     7     7\n##  8 1970-01-01 00:00:08     8     8\n##  9 1970-01-01 00:00:09     9     9\n## 10 1970-01-01 00:00:10    10    10\nWhereas if we change the temporal direction to “<”, then each record from left will be matched with any record(s) from right that is strictly in the future and is at most 1 second ahead of the current record from left:\nprint(asof_join(left, right, tol = \"1s\", direction = \"<\") %>% to_sdf())\n\n## # Source: spark<?> [?? x 3]\n##    time                    u     v\n##    <dttm>              <int> <int>\n##  1 1970-01-01 00:00:01     1     2\n##  2 1970-01-01 00:00:02     2     3\n##  3 1970-01-01 00:00:03     3     4\n##  4 1970-01-01 00:00:04     4     5\n##  5 1970-01-01 00:00:05     5     6\n##  6 1970-01-01 00:00:06     6     7\n##  7 1970-01-01 00:00:07     7     8\n##  8 1970-01-01 00:00:08     8     9\n##  9 1970-01-01 00:00:09     9    10\n## 10 1970-01-01 00:00:10    10    11\nNotice regardless of which temporal direction is selected, an outer-left join is always performed (i.e., all timestamp values and u values of left from above will always be present in the output, and the v column in the output will contain NA whenever there is no record from right that meets the matching criteria).\nOLS Regression\nYou might be wondering whether the version of this functionality in Flint is more or less identical to lm() in R. Turns out it has much more to offer than lm() does. An OLS regression in Flint will compute useful metrics such as Akaike information criterion and Bayesian information criterion, both of which are useful for model selection purposes, and the calculations of both are parallelized by Flint to fully utilize computational power available in a Spark cluster. In addition, Flint supports ignoring regressors that are constant or nearly constant, which becomes useful when an intercept term is included. To see why this is the case, we need to briefly examine the goal of the OLS regression, which is to find some column vector of coefficients \\(\\mathbf{\\beta}\\) that minimizes \\(\\|\\mathbf{y} - \\mathbf{X} \\mathbf{\\beta}\\|^2\\), where \\(\\mathbf{y}\\) is the column vector of response variables, and \\(\\mathbf{X}\\) is a matrix consisting of columns of regressors plus an entire column of \\(1\\)s representing the intercept terms. The solution to this problem is \\(\\mathbf{\\beta} = (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\mathbf{y}\\), assuming the Gram matrix \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) is non-singular. However, if \\(\\mathbf{X}\\) contains a column of all \\(1\\)s of intercept terms, and another column formed by a regressor that is constant (or nearly so), then columns of \\(\\mathbf{X}\\) will be linearly dependent (or nearly so) and \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) will be singular (or nearly so), which presents an issue computation-wise. However, if a regressor is constant, then it essentially plays the same role as the intercept terms do. So simply excluding such a constant regressor in \\(\\mathbf{X}\\) solves the problem. Also, speaking of inverting the Gram matrix, readers remembering the concept of “condition number” from numerical analysis must be thinking to themselves how computing \\(\\mathbf{\\beta} = (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\mathbf{y}\\) could be numerically unstable if \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) has a large condition number. This is why Flint also outputs the condition number of the Gram matrix in the OLS regression result, so that one can sanity-check the underlying quadratic minimization problem being solved is well-conditioned.\nSo, to summarize, the OLS regression functionality implemented in Flint not only outputs the solution to the problem, but also calculates useful metrics that help data scientists assess the sanity and predictive quality of the resulting model.\nTo see OLS regression in action with sparklyr.flint, one can run the following example:\nmtcars_sdf <- copy_to(sc, mtcars, overwrite = TRUE) %>%\n  dplyr::mutate(time = 0L)\nmtcars_ts <- from_sdf(mtcars_sdf, is_sorted = TRUE, time_unit = \"SECONDS\")\nmodel <- ols_regression(mtcars_ts, mpg ~ hp + wt) %>% to_sdf()\n\nprint(model %>% dplyr::select(akaikeIC, bayesIC, cond))\n\n## # Source: spark<?> [?? x 3]\n##   akaikeIC bayesIC    cond\n##      <dbl>   <dbl>   <dbl>\n## 1     155.    159. 345403.\n\n# ^ output says condition number of the Gram matrix was within reason\nand obtain \\(\\mathbf{\\beta}\\), the vector of optimal coefficients, with the following:\nprint(model %>% dplyr::pull(beta))\n\n## [[1]]\n## [1] -0.03177295 -3.87783074\nAdditional Summarizers\nThe EWMA (Exponential Weighted Moving Average), EMA half-life, and the standardized moment summarizers (namely, skewness and kurtosis) along with a few others which were missing in sparklyr.flint 0.1 are now fully supported in sparklyr.flint 0.2.\nBetter Integration With sparklyr\nWhile sparklyr.flint 0.1 included a collect() method for exporting data from a Flint time-series RDD to an R data frame, it did not have a similar method for extracting the underlying Spark data frame from a Flint time-series RDD. This was clearly an oversight. In sparklyr.flint 0.2, one can call to_sdf() on a timeseries RDD to get back a Spark data frame that is usable in sparklyr (e.g., as shown by model %>% to_sdf() %>% dplyr::select(...) examples from above). One can also get to the underlying Spark data frame JVM object reference by calling spark_dataframe() on a Flint time-series RDD (this is usually unnecessary in vast majority of sparklyr use cases though).\nConclusion\nWe have presented a number of new features and improvements introduced in sparklyr.flint 0.2 and deep-dived into some of them in this blog post. We hope you are as excited about them as we are.\nThanks for reading!\nAcknowledgement\nThe author would like to thank Mara (@batpigandme), Sigrid (@skeydan), and Javier (@javierluraschi) for their fantastic editorial inputs on this blog post!\n\n\n\n",
    "preview": "posts/2020-10-12-sparklyr-flint-0.2.0-released/images/sparklyr-flint-0.2.jpg",
    "last_modified": "2024-11-21T15:52:35+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-10-09-torch-optim/",
    "title": "Optimizers in torch",
    "description": "Today, we wrap up our mini-series on torch basics, adding to our toolset two abstractions: loss functions and optimizers.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-10-09",
    "categories": [
      "Torch",
      "R"
    ],
    "contents": "\n\nContents\nLosses and loss functions\nOptimizers\nSimple network: final version\n\n\n\n\nThis is the fourth and last installment in a series introducing torch basics. Initially, we focused on tensors. To illustrate their power, we coded a complete (if toy-size) neural network from scratch. We didn’t make use of any of torch’s higher-level capabilities – not even autograd, its automatic-differentiation feature.\nThis changed in the follow-up post. No more thinking about derivatives and the chain rule; a single call to backward() did it all.\nIn the third post, the code again saw a major simplification. Instead of tediously assembling a DAG1 by hand, we let modules take care of the logic.\nBased on that last state, there are just two more things to do. For one, we still compute the loss by hand. And secondly, even though we get the gradients all nicely computed from autograd, we still loop over the model’s parameters, updating them all ourselves. You won’t be surprised to hear that none of this is necessary.\nLosses and loss functions\ntorch comes with all the usual loss functions, such as mean squared error, cross entropy, Kullback-Leibler divergence, and the like. In general, there are two usage modes.\nTake the example of calculating mean squared error. One way is to call nnf_mse_loss() directly on the prediction and ground truth tensors. For example:\n\n\nx <- torch_randn(c(3, 2, 3))\ny <- torch_zeros(c(3, 2, 3))\n\nnnf_mse_loss(x, y)\n\n\ntorch_tensor \n0.682362\n[ CPUFloatType{} ]\nOther loss functions designed to be called directly start with nnf_ as well: nnf_binary_cross_entropy(), nnf_nll_loss(), nnf_kl_div() … and so on.2\nThe second way is to define the algorithm in advance and call it at some later time. Here, respective constructors all start with nn_ and end in _loss. For example: nn_bce_loss(), nn_nll_loss(), nn_kl_div_loss() …3\n\n\nloss <- nn_mse_loss()\n\nloss(x, y)\n\n\ntorch_tensor \n0.682362\n[ CPUFloatType{} ]\nThis method may be preferable when one and the same algorithm should be applied to more than one pair of tensors.\nOptimizers\nSo far, we’ve been updating model parameters following a simple strategy: The gradients told us which direction on the loss curve was downward; the learning rate told us how big of a step to take. What we did was a straightforward implementation of gradient descent.\nHowever, optimization algorithms used in deep learning get a lot more sophisticated than that. Below, we’ll see how to replace our manual updates using optim_adam(), torch’s implementation of the Adam algorithm (Kingma and Ba 2017). First though, let’s take a quick look at how torch optimizers work.\nHere is a very simple network, consisting of just one linear layer, to be called on a single data point.\n\n\ndata <- torch_randn(1, 3)\n\nmodel <- nn_linear(3, 1)\nmodel$parameters\n\n\n$weight\ntorch_tensor \n-0.0385  0.1412 -0.5436\n[ CPUFloatType{1,3} ]\n\n$bias\ntorch_tensor \n-0.1950\n[ CPUFloatType{1} ]\nWhen we create an optimizer, we tell it what parameters it is supposed to work on.\n\n\noptimizer <- optim_adam(model$parameters, lr = 0.01)\noptimizer\n\n\n<optim_adam>\n  Inherits from: <torch_Optimizer>\n  Public:\n    add_param_group: function (param_group) \n    clone: function (deep = FALSE) \n    defaults: list\n    initialize: function (params, lr = 0.001, betas = c(0.9, 0.999), eps = 1e-08, \n    param_groups: list\n    state: list\n    step: function (closure = NULL) \n    zero_grad: function () \nAt any time, we can inspect those parameters:\n\n\noptimizer$param_groups[[1]]$params\n\n\n$weight\ntorch_tensor \n-0.0385  0.1412 -0.5436\n[ CPUFloatType{1,3} ]\n\n$bias\ntorch_tensor \n-0.1950\n[ CPUFloatType{1} ]\nNow we perform the forward and backward passes. The backward pass calculates the gradients, but does not update the parameters, as we can see both from the model and the optimizer objects:\n\n\nout <- model(data)\nout$backward()\n\noptimizer$param_groups[[1]]$params\nmodel$parameters\n\n\n$weight\ntorch_tensor \n-0.0385  0.1412 -0.5436\n[ CPUFloatType{1,3} ]\n\n$bias\ntorch_tensor \n-0.1950\n[ CPUFloatType{1} ]\n\n$weight\ntorch_tensor \n-0.0385  0.1412 -0.5436\n[ CPUFloatType{1,3} ]\n\n$bias\ntorch_tensor \n-0.1950\n[ CPUFloatType{1} ]\nCalling step() on the optimizer actually performs the updates. Again, let’s check that both model and optimizer now hold the updated values:\n\n\noptimizer$step()\n\noptimizer$param_groups[[1]]$params\nmodel$parameters\n\n\nNULL\n$weight\ntorch_tensor \n-0.0285  0.1312 -0.5536\n[ CPUFloatType{1,3} ]\n\n$bias\ntorch_tensor \n-0.2050\n[ CPUFloatType{1} ]\n\n$weight\ntorch_tensor \n-0.0285  0.1312 -0.5536\n[ CPUFloatType{1,3} ]\n\n$bias\ntorch_tensor \n-0.2050\n[ CPUFloatType{1} ]\nIf we perform optimization in a loop, we need to make sure to call optimizer$zero_grad() on every step, as otherwise gradients would be accumulated. You can see this in our final version of the network.\nSimple network: final version\n\n\nlibrary(torch)\n\n### generate training data -----------------------------------------------------\n\n# input dimensionality (number of input features)\nd_in <- 3\n# output dimensionality (number of predicted features)\nd_out <- 1\n# number of observations in training set\nn <- 100\n\n\n# create random data\nx <- torch_randn(n, d_in)\ny <- x[, 1, NULL] * 0.2 - x[, 2, NULL] * 1.3 - x[, 3, NULL] * 0.5 + torch_randn(n, 1)\n\n\n\n### define the network ---------------------------------------------------------\n\n# dimensionality of hidden layer\nd_hidden <- 32\n\nmodel <- nn_sequential(\n  nn_linear(d_in, d_hidden),\n  nn_relu(),\n  nn_linear(d_hidden, d_out)\n)\n\n### network parameters ---------------------------------------------------------\n\n# for adam, need to choose a much higher learning rate in this problem\nlearning_rate <- 0.08\n\noptimizer <- optim_adam(model$parameters, lr = learning_rate)\n\n### training loop --------------------------------------------------------------\n\nfor (t in 1:200) {\n  \n  ### -------- Forward pass -------- \n  \n  y_pred <- model(x)\n  \n  ### -------- compute loss -------- \n  loss <- nnf_mse_loss(y_pred, y, reduction = \"sum\")\n  if (t %% 10 == 0)\n    cat(\"Epoch: \", t, \"   Loss: \", loss$item(), \"\\n\")\n  \n  ### -------- Backpropagation -------- \n  \n  # Still need to zero out the gradients before the backward pass, only this time,\n  # on the optimizer object\n  optimizer$zero_grad()\n  \n  # gradients are still computed on the loss tensor (no change here)\n  loss$backward()\n  \n  ### -------- Update weights -------- \n  \n  # use the optimizer to update model parameters\n  optimizer$step()\n}\n\n\nAnd that’s it! We’ve seen all the major actors on stage: tensors, autograd, modules, loss functions, and optimizers. In future posts, we’ll explore how to use torch for standard deep learning tasks involving images, text, tabular data, and more. Thanks for reading!\n\n\n\nKingma, Diederik P., and Jimmy Ba. 2017. “Adam: A Method for Stochastic Optimization.” https://arxiv.org/abs/1412.6980.\n\n\ndirected acyclic graph↩︎\nThe prefix nnf_ was chosen because in PyTorch, the corresponding functions live in torch.nn.functional.↩︎\nThis time, the corresponding PyTorch module is torch.nn.↩︎\n",
    "preview": "posts/2020-10-09-torch-optim/images/preview.jpg",
    "last_modified": "2024-11-21T15:52:30+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-10-07-torch-modules/",
    "title": "Using torch modules",
    "description": "In this third installment of our mini-series introducing torch basics, we replace hand-coded matrix operations by modules, considerably simplifying our toy network's code.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-10-07",
    "categories": [
      "Torch",
      "R"
    ],
    "contents": "\n\nContents\nModules\nBase modules (“layers”)\nContainer modules (“models”)\n\nSimple network using modules\n\nInitially,\nwe started learning about torch basics by coding a simple neural\nnetwork from scratch, making use of just a single of torch’s features:\ntensors.\nThen,\nwe immensely simplified the task, replacing manual backpropagation with\nautograd. Today, we modularize the network - in both the habitual\nand a very literal sense: Low-level matrix operations are swapped out\nfor torch modules.\nModules\nFrom other frameworks (Keras, say), you may be used to distinguishing\nbetween models and layers. In torch, both are instances of\nnn_Module(), and thus, have some methods in common. For those thinking\nin terms of “models” and “layers”, I’m artificially splitting up this\nsection into two parts. In reality though, there is no dichotomy: New\nmodules may be composed of existing ones up to arbitrary levels of\nrecursion.\nBase modules (“layers”)\nInstead of writing out an affine operation by hand – x$mm(w1) + b1,\nsay –, as we’ve been doing so far, we can create a linear module. The\nfollowing snippet instantiates a linear layer that expects three-feature\ninputs and returns a single output per observation:\n\n\nlibrary(torch)\nl <- nn_linear(3, 1)\n\n\nThe module has two parameters, “weight” and “bias”. Both now come\npre-initialized:\n\n\nl$parameters\n\n\n$weight\ntorch_tensor \n-0.0385  0.1412 -0.5436\n[ CPUFloatType{1,3} ]\n\n$bias\ntorch_tensor \n-0.1950\n[ CPUFloatType{1} ]\nModules are callable; calling a module executes its forward() method,\nwhich, for a linear layer, matrix-multiplies input and weights, and adds\nthe bias.\nLet’s try this:\n\n\ndata  <- torch_randn(10, 3)\nout <- l(data)\n\n\nUnsurprisingly, out now holds some data:\n\n\nout$data()\n\n\ntorch_tensor \n 0.2711\n-1.8151\n-0.0073\n 0.1876\n-0.0930\n 0.7498\n-0.2332\n-0.0428\n 0.3849\n-0.2618\n[ CPUFloatType{10,1} ]\nIn addition though, this tensor knows what will need to be done, should\never it be asked to calculate gradients:\n\n\nout$grad_fn\n\n\nAddmmBackward\nNote the difference between tensors returned by modules and self-created\nones. When creating tensors ourselves, we need to pass\nrequires_grad = TRUE to trigger gradient calculation. With modules,\ntorch correctly assumes that we’ll want to perform backpropagation at\nsome point.\nBy now though, we haven’t called backward() yet. Thus, no gradients\nhave yet been computed:\n\n\nl$weight$grad\nl$bias$grad\n\n\ntorch_tensor \n[ Tensor (undefined) ]\ntorch_tensor \n[ Tensor (undefined) ]\nLet’s change this:\n\n\nout$backward()\n\n\nError in (function (self, gradient, keep_graph, create_graph)  : \n  grad can be implicitly created only for scalar outputs (_make_grads at ../torch/csrc/autograd/autograd.cpp:47)\nWhy the error? Autograd expects the output tensor to be a scalar,\nwhile in our example, we have a tensor of size (10, 1). This error\nwon’t often occur in practice, where we work with batches of inputs\n(sometimes, just a single batch). But still, it’s interesting to see how\nto resolve this.\nTo make the example work, we introduce a – virtual – final aggregation\nstep – taking the mean, say. Let’s call it avg. If such a mean were\ntaken, its gradient with respect to l$weight would be obtained via the\nchain rule:\n\\[\n\\begin{equation*} \n \\frac{\\partial \\ avg}{\\partial w} = \\frac{\\partial \\ avg}{\\partial \\ out}  \\ \\frac{\\partial \\ out}{\\partial w}\n\\end{equation*}\n\\]\nOf the quantities on the right side, we’re interested in the second. We\nneed to provide the first one, the way it would look if really we were\ntaking the mean:\n\n\nd_avg_d_out <- torch_tensor(10)$`repeat`(10)$unsqueeze(1)$t()\nout$backward(gradient = d_avg_d_out)\n\n\nNow, l$weight$grad and l$bias$grad do contain gradients:\n\n\nl$weight$grad\nl$bias$grad\n\n\ntorch_tensor \n 1.3410  6.4343 -30.7135\n[ CPUFloatType{1,3} ]\ntorch_tensor \n 100\n[ CPUFloatType{1} ]\nIn addition to nn_linear() , torch provides pretty much all the\ncommon layers you might hope for. But few tasks are solved by a single\nlayer. How do you combine them? Or, in the usual lingo: How do you build\nmodels?\nContainer modules (“models”)\nNow, models are just modules that contain other modules. For example,\nif all inputs are supposed to flow through the same nodes and along the\nsame edges, then nn_sequential() can be used to build a simple graph.\nFor example:\n\n\nmodel <- nn_sequential(\n    nn_linear(3, 16),\n    nn_relu(),\n    nn_linear(16, 1)\n)\n\n\nWe can use the same technique as above to get an overview of all model\nparameters (two weight matrices and two bias vectors):\n\n\nmodel$parameters\n\n\n$`0.weight`\ntorch_tensor \n-0.1968 -0.1127 -0.0504\n 0.0083  0.3125  0.0013\n 0.4784 -0.2757  0.2535\n-0.0898 -0.4706 -0.0733\n-0.0654  0.5016  0.0242\n 0.4855 -0.3980 -0.3434\n-0.3609  0.1859 -0.4039\n 0.2851  0.2809 -0.3114\n-0.0542 -0.0754 -0.2252\n-0.3175  0.2107 -0.2954\n-0.3733  0.3931  0.3466\n 0.5616 -0.3793 -0.4872\n 0.0062  0.4168 -0.5580\n 0.3174 -0.4867  0.0904\n-0.0981 -0.0084  0.3580\n 0.3187 -0.2954 -0.5181\n[ CPUFloatType{16,3} ]\n\n$`0.bias`\ntorch_tensor \n-0.3714\n 0.5603\n-0.3791\n 0.4372\n-0.1793\n-0.3329\n 0.5588\n 0.1370\n 0.4467\n 0.2937\n 0.1436\n 0.1986\n 0.4967\n 0.1554\n-0.3219\n-0.0266\n[ CPUFloatType{16} ]\n\n$`2.weight`\ntorch_tensor \nColumns 1 to 10-0.0908 -0.1786  0.0812 -0.0414 -0.0251 -0.1961  0.2326  0.0943 -0.0246  0.0748\n\nColumns 11 to 16 0.2111 -0.1801 -0.0102 -0.0244  0.1223 -0.1958\n[ CPUFloatType{1,16} ]\n\n$`2.bias`\ntorch_tensor \n 0.2470\n[ CPUFloatType{1} ]\nTo inspect an individual parameter, make use of its position in the\nsequential model. For example:\n\n\nmodel[[1]]$bias\n\n\ntorch_tensor \n-0.3714\n 0.5603\n-0.3791\n 0.4372\n-0.1793\n-0.3329\n 0.5588\n 0.1370\n 0.4467\n 0.2937\n 0.1436\n 0.1986\n 0.4967\n 0.1554\n-0.3219\n-0.0266\n[ CPUFloatType{16} ]\nAnd just like nn_linear() above, this module can be called directly on\ndata:\n\n\nout <- model(data)\n\n\nOn a composite module like this one, calling backward() will\nbackpropagate through all the layers:\n\n\nout$backward(gradient = torch_tensor(10)$`repeat`(10)$unsqueeze(1)$t())\n\n# e.g.\nmodel[[1]]$bias$grad\n\n\ntorch_tensor \n  0.0000\n-17.8578\n  1.6246\n -3.7258\n -0.2515\n -5.8825\n 23.2624\n  8.4903\n -2.4604\n  6.7286\n 14.7760\n-14.4064\n -1.0206\n -1.7058\n  0.0000\n -9.7897\n[ CPUFloatType{16} ]\nAnd placing the composite module on the GPU will move all tensors there:\n\n\nmodel$cuda()\nmodel[[1]]$bias$grad\n\n\ntorch_tensor \n  0.0000\n-17.8578\n  1.6246\n -3.7258\n -0.2515\n -5.8825\n 23.2624\n  8.4903\n -2.4604\n  6.7286\n 14.7760\n-14.4064\n -1.0206\n -1.7058\n  0.0000\n -9.7897\n[ CUDAFloatType{16} ]\nNow let’s see how using nn_sequential() can simplify our example\nnetwork.\nSimple network using modules\n\n\n### generate training data -----------------------------------------------------\n\n# input dimensionality (number of input features)\nd_in <- 3\n# output dimensionality (number of predicted features)\nd_out <- 1\n# number of observations in training set\nn <- 100\n\n\n# create random data\nx <- torch_randn(n, d_in)\ny <- x[, 1, NULL] * 0.2 - x[, 2, NULL] * 1.3 - x[, 3, NULL] * 0.5 + torch_randn(n, 1)\n\n\n### define the network ---------------------------------------------------------\n\n# dimensionality of hidden layer\nd_hidden <- 32\n\nmodel <- nn_sequential(\n  nn_linear(d_in, d_hidden),\n  nn_relu(),\n  nn_linear(d_hidden, d_out)\n)\n\n### network parameters ---------------------------------------------------------\n\nlearning_rate <- 1e-4\n\n### training loop --------------------------------------------------------------\n\nfor (t in 1:200) {\n  \n  ### -------- Forward pass -------- \n  \n  y_pred <- model(x)\n  \n  ### -------- compute loss -------- \n  loss <- (y_pred - y)$pow(2)$sum()\n  if (t %% 10 == 0)\n    cat(\"Epoch: \", t, \"   Loss: \", loss$item(), \"\\n\")\n  \n  ### -------- Backpropagation -------- \n  \n  # Zero the gradients before running the backward pass.\n  model$zero_grad()\n  \n  # compute gradient of the loss w.r.t. all learnable parameters of the model\n  loss$backward()\n  \n  ### -------- Update weights -------- \n  \n  # Wrap in with_no_grad() because this is a part we DON'T want to record\n  # for automatic gradient computation\n  # Update each parameter by its `grad`\n  \n  with_no_grad({\n    model$parameters %>% purrr::walk(function(param) param$sub_(learning_rate * param$grad))\n  })\n  \n}\n\n\nThe forward pass looks a lot better now; however, we still loop through\nthe model’s parameters and update each one by hand. Furthermore, you may\nbe already be suspecting that torch provides abstractions for common\nloss functions. In the next and last installment of this series, we’ll\naddress both points, making use of torch losses and optimizers. See\nyou then!\n\n\n\n",
    "preview": "posts/2020-10-07-torch-modules/images/preview.jpg",
    "last_modified": "2024-11-21T15:51:55+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-10-05-torch-network-with-autograd/",
    "title": "Introducing torch autograd",
    "description": "With torch, there is hardly ever a reason to code backpropagation from scratch. Its automatic differentiation feature, called autograd, keeps track of operations that need their gradients computed, as well as how to compute them. In this second post of a four-part series, we update our simple, hand-coded network to make use of autograd.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-10-05",
    "categories": [
      "Torch",
      "R"
    ],
    "contents": "\n\nContents\nAutomatic differentiation with autograd\nThe simple network, now using autograd\nOutlook\n\nLast week, we saw how to code a simple network from\nscratch,\nusing nothing but torch tensors. Predictions, loss, gradients,\nweight updates – all these things we’ve been computing ourselves.\nToday, we make a significant change: Namely, we spare ourselves the\ncumbersome calculation of gradients, and have torch do it for us.\nPrior to that though, let’s get some background.\nAutomatic differentiation with autograd\ntorch uses a module called autograd to\nrecord operations performed on tensors, and\nstore what will have to be done to obtain the corresponding\ngradients, once we’re entering the backward pass.\nThese prospective actions are stored internally as functions, and when\nit’s time to compute the gradients, these functions are applied in\norder: Application starts from the output node, and calculated gradients\nare successively propagated back through the network. This is a form\nof reverse mode automatic differentiation.\nAutograd basics\nAs users, we can see a bit of the implementation. As a prerequisite for\nthis “recording” to happen, tensors have to be created with\nrequires_grad = TRUE. For example:\n\n\nlibrary(torch)\n\nx <- torch_ones(2, 2, requires_grad = TRUE)\n\n\nTo be clear, x now is a tensor with respect to which gradients have\nto be calculated – normally, a tensor representing a weight or a bias,\nnot the input data 1. If we subsequently perform some operation on\nthat tensor, assigning the result to y,\n\n\ny <- x$mean()\n\n\nwe find that y now has a non-empty grad_fn that tells torch how to\ncompute the gradient of y with respect to x:\n\n\ny$grad_fn\n\n\nMeanBackward0\nActual computation of gradients is triggered by calling backward()\non the output tensor.\n\n\ny$backward()\n\n\nAfter backward() has been called, x has a non-null field termed\ngrad that stores the gradient of y with respect to x:\n\n\nx$grad\n\n\ntorch_tensor \n 0.2500  0.2500\n 0.2500  0.2500\n[ CPUFloatType{2,2} ]\nWith longer chains of computations, we can take a glance at how torch\nbuilds up a graph of backward operations. Here is a slightly more\ncomplex example – feel free to skip if you’re not the type who just\nhas to peek into things for them to make sense.\nDigging deeper\nWe build up a simple graph of tensors, with inputs x1 and x2 being\nconnected to output out by intermediaries y and z.\n\n\nx1 <- torch_ones(2, 2, requires_grad = TRUE)\nx2 <- torch_tensor(1.1, requires_grad = TRUE)\n\ny <- x1 * (x2 + 2)\n\nz <- y$pow(2) * 3\n\nout <- z$mean()\n\n\nTo save memory, intermediate gradients are normally not being stored.\nCalling retain_grad() on a tensor allows one to deviate from this\ndefault. Let’s do this here, for the sake of demonstration:\n\n\ny$retain_grad()\n\nz$retain_grad()\n\n\nNow we can go backwards through the graph and inspect torch’s action\nplan for backprop, starting from out$grad_fn, like so:\n\n\n# how to compute the gradient for mean, the last operation executed\nout$grad_fn\n\n\nMeanBackward0\n\n\n# how to compute the gradient for the multiplication by 3 in z = y.pow(2) * 3\nout$grad_fn$next_functions\n\n\n[[1]]\nMulBackward1\n\n\n# how to compute the gradient for pow in z = y.pow(2) * 3\nout$grad_fn$next_functions[[1]]$next_functions\n\n\n[[1]]\nPowBackward0\n\n\n# how to compute the gradient for the multiplication in y = x * (x + 2)\nout$grad_fn$next_functions[[1]]$next_functions[[1]]$next_functions\n\n\n[[1]]\nMulBackward0\n\n\n# how to compute the gradient for the two branches of y = x * (x + 2),\n# where the left branch is a leaf node (AccumulateGrad for x1)\nout$grad_fn$next_functions[[1]]$next_functions[[1]]$next_functions[[1]]$next_functions\n\n\n[[1]]\ntorch::autograd::AccumulateGrad\n[[2]]\nAddBackward1\n\n\n# here we arrive at the other leaf node (AccumulateGrad for x2)\nout$grad_fn$next_functions[[1]]$next_functions[[1]]$next_functions[[1]]$next_functions[[2]]$next_functions\n\n\n[[1]]\ntorch::autograd::AccumulateGrad\nIf we now call out$backward(), all tensors in the graph will have\ntheir respective gradients calculated.\n\n\nout$backward()\n\nz$grad\ny$grad\nx2$grad\nx1$grad\n\n\ntorch_tensor \n 0.2500  0.2500\n 0.2500  0.2500\n[ CPUFloatType{2,2} ]\ntorch_tensor \n 4.6500  4.6500\n 4.6500  4.6500\n[ CPUFloatType{2,2} ]\ntorch_tensor \n 18.6000\n[ CPUFloatType{1} ]\ntorch_tensor \n 14.4150  14.4150\n 14.4150  14.4150\n[ CPUFloatType{2,2} ]\nAfter this nerdy excursion, let’s see how autograd makes our network\nsimpler.\nThe simple network, now using autograd\nThanks to autograd, we say good-bye to the tedious, error-prone\nprocess of coding backpropagation ourselves. A single method call does\nit all: loss$backward().\nWith torch keeping track of operations as required, we don’t even have\nto explicitly name the intermediate tensors any more. We can code\nforward pass, loss calculation, and backward pass in just three lines:\n\n\ny_pred <- x$mm(w1)$add(b1)$clamp(min = 0)$mm(w2)$add(b2)\n  \nloss <- (y_pred - y)$pow(2)$sum()\n\nloss$backward()\n\n\nHere is the complete code. We’re at an intermediate stage: We still\nmanually compute the forward pass and the loss, and we still manually\nupdate the weights. Due to the latter, there is something I need to\nexplain. But I’ll let you check out the new version first:\n\n\nlibrary(torch)\n\n### generate training data -----------------------------------------------------\n\n# input dimensionality (number of input features)\nd_in <- 3\n# output dimensionality (number of predicted features)\nd_out <- 1\n# number of observations in training set\nn <- 100\n\n\n# create random data\nx <- torch_randn(n, d_in)\ny <- x[, 1, NULL] * 0.2 - x[, 2, NULL] * 1.3 - x[, 3, NULL] * 0.5 + torch_randn(n, 1)\n\n\n### initialize weights ---------------------------------------------------------\n\n# dimensionality of hidden layer\nd_hidden <- 32\n# weights connecting input to hidden layer\nw1 <- torch_randn(d_in, d_hidden, requires_grad = TRUE)\n# weights connecting hidden to output layer\nw2 <- torch_randn(d_hidden, d_out, requires_grad = TRUE)\n\n# hidden layer bias\nb1 <- torch_zeros(1, d_hidden, requires_grad = TRUE)\n# output layer bias\nb2 <- torch_zeros(1, d_out, requires_grad = TRUE)\n\n### network parameters ---------------------------------------------------------\n\nlearning_rate <- 1e-4\n\n### training loop --------------------------------------------------------------\n\nfor (t in 1:200) {\n  ### -------- Forward pass --------\n  \n  y_pred <- x$mm(w1)$add(b1)$clamp(min = 0)$mm(w2)$add(b2)\n  \n  ### -------- compute loss -------- \n  loss <- (y_pred - y)$pow(2)$sum()\n  if (t %% 10 == 0)\n    cat(\"Epoch: \", t, \"   Loss: \", loss$item(), \"\\n\")\n  \n  ### -------- Backpropagation --------\n  \n  # compute gradient of loss w.r.t. all tensors with requires_grad = TRUE\n  loss$backward()\n  \n  ### -------- Update weights -------- \n  \n  # Wrap in with_no_grad() because this is a part we DON'T \n  # want to record for automatic gradient computation\n   with_no_grad({\n     w1 <- w1$sub_(learning_rate * w1$grad)\n     w2 <- w2$sub_(learning_rate * w2$grad)\n     b1 <- b1$sub_(learning_rate * b1$grad)\n     b2 <- b2$sub_(learning_rate * b2$grad)  \n     \n     # Zero gradients after every pass, as they'd accumulate otherwise\n     w1$grad$zero_()\n     w2$grad$zero_()\n     b1$grad$zero_()\n     b2$grad$zero_()  \n   })\n\n}\n\n\nAs explained above, after some_tensor$backward(), all tensors\npreceding it in the graph2 will have their grad fields populated.\nWe make use of these fields to update the weights. But now that\nautograd is “on”, whenever we execute an operation we don’t want\nrecorded for backprop, we need to explicitly exempt it: This is why we\nwrap the weight updates in a call to with_no_grad().\nWhile this is something you may file under “nice to know” – after all,\nonce we arrive at the last post in the series, this manual updating of\nweights will be gone – the idiom of zeroing gradients is here to\nstay: Values stored in grad fields accumulate; whenever we’re done\nusing them, we need to zero them out before reuse.\nOutlook\nSo where do we stand? We started out coding a network completely from\nscratch, making use of nothing but torch tensors. Today, we got\nsignificant help from autograd.\nBut we’re still manually updating the weights, – and aren’t deep\nlearning frameworks known to provide abstractions (“layers”, or:\n“modules”) on top of tensor computations …?\nWe address both issues in the follow-up installments. Thanks for\nreading!\n\nUnless we want to change the data, as when generating\nadversarial examples.↩︎\nAll that have requires_grad set to TRUE, to be precise.↩︎\n",
    "preview": "posts/2020-10-05-torch-network-with-autograd/images/preview.jpg",
    "last_modified": "2024-11-21T15:48:55+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-10-01-torch-network-from-scratch/",
    "title": "Getting familiar with torch tensors",
    "description": "In this first installment of a four-part miniseries, we present the main things you will want to know about torch tensors. As an illustrative example, we'll code a simple neural network from scratch.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-10-01",
    "categories": [
      "Torch",
      "R"
    ],
    "contents": "\n\nContents\nTensors\nCreation\nConversion to built-in R data types\nIndexing and slicing tensors\nReshaping tensors\nOperations on tensors\n\nRunning on GPU\nBroadcasting\nA simple neural network using torch tensors\n\nTwo days ago, I introduced torch, an R package that provides the native functionality that is brought to Python users by PyTorch. In that post, I assumed basic familiarity with TensorFlow/Keras. Consequently, I portrayed torch in a way I figured would be helpful to someone who “grew up” with the Keras way of training a model: Aiming to focus on differences, yet not lose sight of the overall process.\nThis post now changes perspective. We code a simple neural network “from scratch”, making use of just one of torch’s building blocks: tensors. This network will be as “raw” (low-level) as can be. (For the less math-inclined people among us, it may serve as a refresher of what’s actually going on beneath all those convenience tools they built for us. But the real purpose is to illustrate what can be done with tensors alone.)\nSubsequently, three posts will progressively show how to reduce the effort – noticeably right from the start, enormously once we finish. At the end of this mini-series, you will have seen how automatic differentiation works in torch, how to use modules (layers, in keras speak, and compositions thereof), and optimizers. By then, you’ll have a lot of the background desirable when applying torch to real-world tasks.\nThis post will be the longest, since there is a lot to learn about tensors: How to create them; how to manipulate their contents and/or modify their shapes; how to convert them to R arrays, matrices or vectors; and of course, given the omnipresent need for speed: how to get all those operations executed on the GPU. Once we’ve cleared that agenda, we code the aforementioned little network, seeing all those aspects in action.\nTensors\nCreation\nTensors may be created by specifying individual values. Here we create two one-dimensional tensors (vectors), of types float and bool, respectively:\n\n\nlibrary(torch)\n# a 1d vector of length 2\nt <- torch_tensor(c(1, 2))\nt\n\n# also 1d, but of type boolean\nt <- torch_tensor(c(TRUE, FALSE))\nt\n\n\ntorch_tensor \n 1\n 2\n[ CPUFloatType{2} ]\n\ntorch_tensor \n 1\n 0\n[ CPUBoolType{2} ]\nAnd here are two ways to create two-dimensional tensors (matrices). Note how in the second approach, you need to specify byrow = TRUE in the call to matrix() to get values arranged in row-major order.\n\n\n# a 3x3 tensor (matrix)\nt <- torch_tensor(rbind(c(1,2,0), c(3,0,0), c(4,5,6)))\nt\n\n# also 3x3\nt <- torch_tensor(matrix(1:9, ncol = 3, byrow = TRUE))\nt\n\n\ntorch_tensor \n 1  2  0\n 3  0  0\n 4  5  6\n[ CPUFloatType{3,3} ]\n\ntorch_tensor \n 1  2  3\n 4  5  6\n 7  8  9\n[ CPULongType{3,3} ]\nIn higher dimensions especially, it can be easier to specify the type of tensor abstractly, as in: “give me a tensor of <…> of shape n1 x n2”, where <…> could be “zeros”; or “ones”; or, say, “values drawn from a standard normal distribution”:\n\n\n# a 3x3 tensor of standard-normally distributed values\nt <- torch_randn(3, 3)\nt\n\n# a 4x2x2 (3d) tensor of zeroes\nt <- torch_zeros(4, 2, 2)\nt\n\n\ntorch_tensor \n-2.1563  1.7085  0.5245\n 0.8955 -0.6854  0.2418\n 0.4193 -0.7742 -1.0399\n[ CPUFloatType{3,3} ]\n\ntorch_tensor \n(1,.,.) = \n  0  0\n  0  0\n\n(2,.,.) = \n  0  0\n  0  0\n\n(3,.,.) = \n  0  0\n  0  0\n\n(4,.,.) = \n  0  0\n  0  0\n[ CPUFloatType{4,2,2} ]\nMany similar functions exist, including, e.g., torch_arange() to create a tensor holding a sequence of evenly spaced values, torch_eye() which returns an identity matrix, and torch_logspace() which fills a specified range with a list of values spaced logarithmically.\nIf no dtype argument is specified, torch will infer the data type from the passed-in value(s). For example:\n\n\nt <- torch_tensor(c(3, 5, 7))\nt$dtype\n\nt <- torch_tensor(1L)\nt$dtype\n\n\ntorch_Float\ntorch_Long\nBut we can explicitly request a different dtype if we want:\n\n\nt <- torch_tensor(2, dtype = torch_double())\nt$dtype\n\n\ntorch_Double\ntorch tensors live on a device. By default, this will be the CPU:\n\n\nt$device\n\n\ntorch_device(type='cpu')\nBut we could also define a tensor to live on the GPU:\n\n\nt <- torch_tensor(2, device = \"cuda\")\nt$device\n\n\ntorch_device(type='cuda', index=0)\nWe’ll talk more about devices below.\nThere is another very important parameter to the tensor-creation functions: requires_grad. Here though, I need to ask for your patience: This one will prominently figure in the follow-up post.\nConversion to built-in R data types\nTo convert torch tensors to R, use as_array():\n\n\nt <- torch_tensor(matrix(1:9, ncol = 3, byrow = TRUE))\nas_array(t)\n\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\nDepending on whether the tensor is one-, two-, or three-dimensional, the resulting R object will be a vector, a matrix, or an array:\n\n\nt <- torch_tensor(c(1, 2, 3))\nas_array(t) %>% class()\n\nt <- torch_ones(c(2, 2))\nas_array(t) %>% class()\n\nt <- torch_ones(c(2, 2, 2))\nas_array(t) %>% class()\n\n\n[1] \"numeric\"\n\n[1] \"matrix\" \"array\" \n\n[1] \"array\"\nFor one-dimensional and two-dimensional tensors, it is also possible to use as.integer() / as.matrix(). (One reason you might want to do this is to have more self-documenting code.)\nIf a tensor currently lives on the GPU, you need to move it to the CPU first:\n\n\nt <- torch_tensor(2, device = \"cuda\")\nas.integer(t$cpu())\n\n\n[1] 2\nIndexing and slicing tensors\nOften, we want to retrieve not a complete tensor, but only some of the values it holds, or even just a single value. In these cases, we talk about slicing and indexing, respectively.\nIn R, these operations are 1-based, meaning that when we specify offsets, we assume for the very first element in an array to reside at offset 1. The same behavior was implemented for torch. Thus, a lot of the functionality described in this section should feel intuitive.\nThe way I’m organizing this section is the following. We’ll inspect the intuitive parts first, where by intuitive I mean: intuitive to the R user who has not yet worked with Python’s NumPy. Then come things which, to this user, may look more surprising, but will turn out to be pretty useful.\nIndexing and slicing: the R-like part\nNone of these should be overly surprising:\n\n\nt <- torch_tensor(rbind(c(1,2,3), c(4,5,6)))\nt\n\n# a single value\nt[1, 1]\n\n# first row, all columns\nt[1, ]\n\n# first row, a subset of columns\nt[1, 1:2]\n\n\ntorch_tensor \n 1  2  3\n 4  5  6\n[ CPUFloatType{2,3} ]\n\ntorch_tensor \n1\n[ CPUFloatType{} ]\n\ntorch_tensor \n 1\n 2\n 3\n[ CPUFloatType{3} ]\n\ntorch_tensor \n 1\n 2\n[ CPUFloatType{2} ]\nNote how, just as in R, singleton dimensions are dropped:\n\n\nt <- torch_tensor(rbind(c(1,2,3), c(4,5,6)))\n\n# 2x3\nt$size() \n\n# just a single row: will be returned as a vector\nt[1, 1:2]$size() \n\n# a single element\nt[1, 1]$size()\n\n\n[1] 2 3\n\n[1] 2\n\ninteger(0)\nAnd just like in R, you can specify drop = FALSE to keep those dimensions:\n\n\nt[1, 1:2, drop = FALSE]$size()\n\nt[1, 1, drop = FALSE]$size()\n\n\n[1] 1 2\n\n[1] 1 1\nIndexing and slicing: What to look out for\nWhereas R uses negative numbers to remove elements at specified positions, in torch negative values indicate that we start counting from the end of a tensor – with -1 pointing to its last element:\n\n\nt <- torch_tensor(rbind(c(1,2,3), c(4,5,6)))\n\nt[1, -1]\n\nt[ , -2:-1] \n\n\ntorch_tensor \n3\n[ CPUFloatType{} ]\n\ntorch_tensor \n 2  3\n 5  6\n[ CPUFloatType{2,2} ]\nThis is a feature you might know from NumPy. Same with the following.\nWhen the slicing expression m:n is augmented by another colon and a third number – m:n:o –, we will take every oth item from the range specified by m and n:\n\n\nt <- torch_tensor(1:10)\nt[2:10:2]\n\n\ntorch_tensor \n  2\n  4\n  6\n  8\n 10\n[ CPULongType{5} ]\nSometimes we don’t know how many dimensions a tensor has, but we do know what to do with the final dimension, or the first one. To subsume all others, we can use ..:\n\n\nt <- torch_randint(-7, 7, size = c(2, 2, 2))\nt\n\nt[.., 1]\n\nt[2, ..]\n\n\ntorch_tensor \n(1,.,.) = \n  2 -2\n -5  4\n\n(2,.,.) = \n  0  4\n -3 -1\n[ CPUFloatType{2,2,2} ]\n\ntorch_tensor \n 2 -5\n 0 -3\n[ CPUFloatType{2,2} ]\n\ntorch_tensor \n 0  4\n-3 -1\n[ CPUFloatType{2,2} ]\nNow we move on to a topic that, in practice, is just as indispensable as slicing: changing tensor shapes.\nReshaping tensors\nChanges in shape can occur in two fundamentally different ways. Seeing how “reshape” really means: keep the values but modify their layout, we could either alter how they’re arranged physically, or keep the physical structure as-is and just change the “mapping” (a semantic change, as it were).\nIn the first case, storage will have to be allocated for two tensors, source and target, and elements will be copied from the latter to the former. In the second, physically there will be just a single tensor, referenced by two logical entities with distinct metadata.\nNot surprisingly, for performance reasons, the second operation is preferred.\nZero-copy reshaping\nWe start with zero-copy methods, as we’ll want to use them whenever we can.\nA special case often seen in practice is adding or removing a singleton dimension.\nunsqueeze() adds a dimension of size 1 at a position specified by dim:\n\n\nt1 <- torch_randint(low = 3, high = 7, size = c(3, 3, 3))\nt1$size()\n\nt2 <- t1$unsqueeze(dim = 1)\nt2$size()\n\nt3 <- t1$unsqueeze(dim = 2)\nt3$size()\n\n\n[1] 3 3 3\n\n[1] 1 3 3 3\n\n[1] 3 1 3 3\nConversely, squeeze() removes singleton dimensions:\n\n\nt4 <- t3$squeeze()\nt4$size()\n\n\n[1] 3 3 3\nThe same could be accomplished with view(). view(), however, is much more general, in that it allows you to reshape the data to any valid dimensionality. (Valid meaning: The number of elements stays the same.)\nHere we have a 3x2 tensor that is reshaped to size 2x3:\n\n\nt1 <- torch_tensor(rbind(c(1, 2), c(3, 4), c(5, 6)))\nt1\n\nt2 <- t1$view(c(2, 3))\nt2\n\n\ntorch_tensor \n 1  2\n 3  4\n 5  6\n[ CPUFloatType{3,2} ]\n\ntorch_tensor \n 1  2  3\n 4  5  6\n[ CPUFloatType{2,3} ]\n(Note how this is different from matrix transposition.)\nInstead of going from two to three dimensions, we can flatten the matrix to a vector.\n\n\nt4 <- t1$view(c(-1, 6))\n\nt4$size()\n\nt4\n\n\n[1] 1 6\n\ntorch_tensor \n 1  2  3  4  5  6\n[ CPUFloatType{1,6} ]\nIn contrast to indexing operations, this does not drop dimensions.\nLike we said above, operations like squeeze() or view() do not make copies. Or, put differently: The output tensor shares storage with the input tensor. We can in fact verify this ourselves:\n\n\nt1$storage()$data_ptr()\n\nt2$storage()$data_ptr()\n\n\n[1] \"0x5648d02ac800\"\n\n[1] \"0x5648d02ac800\"\nWhat’s different is the storage metadata torch keeps about both tensors. Here, the relevant information is the stride:\nA tensor’s stride() method tracks, for every dimension, how many elements have to be traversed to arrive at its next element (row or column, in two dimensions). For t1 above, of shape 3x2, we have to skip over 2 items to arrive at the next row. To arrive at the next column though, in every row we just have to skip a single entry:\n\n\nt1$stride()\n\n\n[1] 2 1\nFor t2, of shape 3x2, the distance between column elements is the same, but the distance between rows is now 3:\n\n\nt2$stride()\n\n\n[1] 3 1\nWhile zero-copy operations are optimal, there are cases where they won’t work.\nWith view(), this can happen when a tensor was obtained via an operation – other than view() itself – that itself has already modified the stride. One example would be transpose():\n\n\nt1 <- torch_tensor(rbind(c(1, 2), c(3, 4), c(5, 6)))\nt1\nt1$stride()\n\nt2 <- t1$t()\nt2\nt2$stride()\n\n\ntorch_tensor \n 1  2\n 3  4\n 5  6\n[ CPUFloatType{3,2} ]\n\n[1] 2 1\n\ntorch_tensor \n 1  3  5\n 2  4  6\n[ CPUFloatType{2,3} ]\n\n[1] 1 2\nIn torch lingo, tensors – like t2 – that re-use existing storage (and just read it differently), are said not to be “contiguous”1. One way to reshape them is to use contiguous() on them before. We’ll see this in the next subsection.\nReshape with copy\nIn the following snippet, trying to reshape t2 using view() fails, as it already carries information indicating that the underlying data should not be read in physical order.\n\n\nt1 <- torch_tensor(rbind(c(1, 2), c(3, 4), c(5, 6)))\n\nt2 <- t1$t()\n\nt2$view(6) # error!\n\n\nError in (function (self, size)  : \n  view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces).\n  Use .reshape(...) instead. (view at ../aten/src/ATen/native/TensorShape.cpp:1364)\nHowever, if we first call contiguous() on it, a new tensor is created, which may then be (virtually) reshaped using view().2\n\n\nt3 <- t2$contiguous()\n\nt3$view(6)\n\n\ntorch_tensor \n 1\n 3\n 5\n 2\n 4\n 6\n[ CPUFloatType{6} ]\nAlternatively, we can use reshape(). reshape() defaults to view()-like behavior if possible; otherwise it will create a physical copy.\n\n\nt2$storage()$data_ptr()\n\nt4 <- t2$reshape(6)\n\nt4$storage()$data_ptr()\n\n\n[1] \"0x5648d49b4f40\"\n\n[1] \"0x5648d2752980\"\nOperations on tensors\nUnsurprisingly, torch provides a bunch of mathematical operations on tensors; we’ll see some of them in the network code below, and you’ll encounter lots more when you continue your torch journey. Here, we quickly take a look at the overall tensor method semantics.\nTensor methods normally return references to new objects. Here, we add to t1 a clone of itself:\n\n\nt1 <- torch_tensor(rbind(c(1, 2), c(3, 4), c(5, 6)))\nt2 <- t1$clone()\n\nt1$add(t2)\n\n\ntorch_tensor \n  2   4\n  6   8\n 10  12\n[ CPUFloatType{3,2} ]\nIn this process, t1 has not been modified:\n\n\nt1\n\n\ntorch_tensor \n 1  2\n 3  4\n 5  6\n[ CPUFloatType{3,2} ]\nMany tensor methods have variants for mutating operations. These all carry a trailing underscore:\n\n\nt1$add_(t1)\n\n# now t1 has been modified\nt1\n\n\ntorch_tensor \n  4   8\n 12  16\n 20  24\n[ CPUFloatType{3,2} ]\n\ntorch_tensor \n  4   8\n 12  16\n 20  24\n[ CPUFloatType{3,2} ]\nAlternatively, you can of course assign the new object to a new reference variable:\n\n\nt3 <- t1$add(t1)\n\nt3\n\n\ntorch_tensor \n  8  16\n 24  32\n 40  48\n[ CPUFloatType{3,2} ]\nThere is one thing we need to discuss before we wrap up our introduction to tensors: How can we have all those operations executed on the GPU?\nRunning on GPU\nTo check if your GPU(s) is/are visible to torch, run\n\n\ncuda_is_available()\n\ncuda_device_count()\n\n\n[1] TRUE\n\n[1] 1\nTensors may be requested to live on the GPU right at creation:\n\n\ndevice <- torch_device(\"cuda\")\n\nt <- torch_ones(c(2, 2), device = device) \n\n\nAlternatively, they can be moved between devices at any time:\n\n\nt2 <- t$cuda()\nt2$device\n\n\ntorch_device(type='cuda', index=0)\n\n\nt3 <- t2$cpu()\nt3$device\n\n\ntorch_device(type='cpu')\nThat’s it for our discussion on tensors — almost. There is one torch feature that, although related to tensor operations, deserves special mention. It is called broadcasting, and “bilingual” (R + Python) users will know it from NumPy.\nBroadcasting\nWe often have to perform operations on tensors with shapes that don’t match exactly.\nUnsurprisingly, we can add a scalar to a tensor:\n\n\nt1 <- torch_randn(c(3,5))\n\nt1 + 22\n\n\ntorch_tensor \n 23.1097  21.4425  22.7732  22.2973  21.4128\n 22.6936  21.8829  21.1463  21.6781  21.0827\n 22.5672  21.2210  21.2344  23.1154  20.5004\n[ CPUFloatType{3,5} ]\nThe same will work if we add tensor of size 1:\n\n\nt1 <- torch_randn(c(3,5))\n\nt1 + torch_tensor(c(22))\n\n\nAdding tensors of different sizes normally won’t work:\n\n\nt1 <- torch_randn(c(3,5))\nt2 <- torch_randn(c(5,5))\n\nt1$add(t2) # error\n\n\nError in (function (self, other, alpha)  : \n  The size of tensor a (2) must match the size of tensor b (5) at non-singleton dimension 1 (infer_size at ../aten/src/ATen/ExpandUtils.cpp:24)\nHowever, under certain conditions, one or both tensors may be virtually expanded so both tensors line up. This behavior is what is meant by broadcasting. The way it works in torch is not just inspired by, but actually identical to that of NumPy.\nThe rules are:\nWe align array shapes, starting from the right.\nSay we have two tensors, one of size 8x1x6x1, the other of size 7x1x5.\nHere they are, right-aligned:\n# t1, shape:     8  1  6  1\n# t2, shape:        7  1  5\nStarting to look from the right, the sizes along aligned axes either have to match exactly, or one of them has to be equal to 1: in which case the latter is broadcast to the larger one.\nIn the above example, this is the case for the second-from-last dimension. This now gives\n# t1, shape:     8  1  6  1\n# t2, shape:        7  6  5\n, with broadcasting happening in t2.\nIf on the left, one of the arrays has an additional axis (or more than one), the other is virtually expanded to have a size of 1 in that place, in which case broadcasting will happen as stated in (2).\nThis is the case with t1’s leftmost dimension. First, there is a virtual expansion\n# t1, shape:     8  1  6  1\n# t2, shape:     1  7  1  5\nand then, broadcasting happens:\n# t1, shape:     8  1  6  1\n# t2, shape:     8  7  1  5\nAccording to these rules, our above example\n\n\nt1 <- torch_randn(c(3,5))\nt2 <- torch_randn(c(5,5))\n\nt1$add(t2)\n\n\ncould be modified in various ways that would allow for adding two tensors.\nFor example, if t2 were 1x5, it would only need to get broadcast to size 3x5 before the addition operation:\n\n\nt1 <- torch_randn(c(3,5))\nt2 <- torch_randn(c(1,5))\n\nt1$add(t2)\n\n\ntorch_tensor \n-1.0505  1.5811  1.1956 -0.0445  0.5373\n 0.0779  2.4273  2.1518 -0.6136  2.6295\n 0.1386 -0.6107 -1.2527 -1.3256 -0.1009\n[ CPUFloatType{3,5} ]\nIf it were of size 5, a virtual leading dimension would be added, and then, the same broadcasting would take place as in the previous case.\n\n\nt1 <- torch_randn(c(3,5))\nt2 <- torch_randn(c(5))\n\nt1$add(t2)\n\n\ntorch_tensor \n-1.4123  2.1392 -0.9891  1.1636 -1.4960\n 0.8147  1.0368 -2.6144  0.6075 -2.0776\n-2.3502  1.4165  0.4651 -0.8816 -1.0685\n[ CPUFloatType{3,5} ]\nHere is a more complex example. Broadcasting how happens both in t1 and in t2:\n\n\nt1 <- torch_randn(c(1,5))\nt2 <- torch_randn(c(3,1))\n\nt1$add(t2)\n\n\ntorch_tensor \n 1.2274  1.1880  0.8531  1.8511 -0.0627\n 0.2639  0.2246 -0.1103  0.8877 -1.0262\n-1.5951 -1.6344 -1.9693 -0.9713 -2.8852\n[ CPUFloatType{3,5} ]\nAs a nice concluding example, through broadcasting an outer product can be computed like so:\n\n\nt1 <- torch_tensor(c(0, 10, 20, 30))\n\nt2 <- torch_tensor(c(1, 2, 3))\n\nt1$view(c(4,1)) * t2\n\n\ntorch_tensor \n  0   0   0\n 10  20  30\n 20  40  60\n 30  60  90\n[ CPUFloatType{4,3} ]\nAnd now, we really get to implementing that neural network!\nA simple neural network using torch tensors\nOur task, which we approach in a low-level way today but considerably simplify in upcoming installments, consists of regressing a single target datum based on three input variables.\nWe directly use torch to simulate some data.\nToy data\n\n\nlibrary(torch)\n\n# input dimensionality (number of input features)\nd_in <- 3\n# output dimensionality (number of predicted features)\nd_out <- 1\n# number of observations in training set\nn <- 100\n\n\n# create random data\n# input\nx <- torch_randn(n, d_in)\n# target\ny <- x[, 1, drop = FALSE] * 0.2 -\n  x[, 2, drop = FALSE] * 1.3 -\n  x[, 3, drop = FALSE] * 0.5 +\n  torch_randn(n, 1)\n\n\nNext, we need to initialize the network’s weights. We’ll have one hidden layer, with 32 units. The output layer’s size, being determined by the task, is equal to 1.\nInitialize weights\n\n\n# dimensionality of hidden layer\nd_hidden <- 32\n\n# weights connecting input to hidden layer\nw1 <- torch_randn(d_in, d_hidden)\n# weights connecting hidden to output layer\nw2 <- torch_randn(d_hidden, d_out)\n\n# hidden layer bias\nb1 <- torch_zeros(1, d_hidden)\n# output layer bias\nb2 <- torch_zeros(1, d_out)\n\n\nNow for the training loop proper. The training loop here really is the network.\nTraining loop\nIn each iteration (“epoch”), the training loop does four things:\nruns through the network, computing predictions (forward pass)\ncompares those predictions to the ground truth and quantify the loss\nruns backwards through the network, computing the gradients that indicate how the weights should be changed\nupdates the weights, making use of the requested learning rate.\nHere is the template we’re going to fill:\n\n\nfor (t in 1:200) {\n    \n    ### -------- Forward pass -------- \n    \n    # here we'll compute the prediction\n    \n    \n    ### -------- compute loss -------- \n    \n    # here we'll compute the sum of squared errors\n    \n\n    ### -------- Backpropagation -------- \n    \n    # here we'll pass through the network, calculating the required gradients\n    \n\n    ### -------- Update weights -------- \n    \n    # here we'll update the weights, subtracting portion of the gradients \n}\n\n\nThe forward pass effectuates two affine transformations, one each for the hidden and output layers. In-between, ReLU activation is applied:\n\n\n  # compute pre-activations of hidden layers (dim: 100 x 32)\n  # torch_mm does matrix multiplication\n  h <- x$mm(w1) + b1\n  \n  # apply activation function (dim: 100 x 32)\n  # torch_clamp cuts off values below/above given thresholds\n  h_relu <- h$clamp(min = 0)\n  \n  # compute output (dim: 100 x 1)\n  y_pred <- h_relu$mm(w2) + b2\n\n\nOur loss here is mean squared error:\n\n\n  loss <- as.numeric((y_pred - y)$pow(2)$sum())\n\n\nCalculating gradients the manual way is a bit tedious3, but it can be done:\n\n\n  # gradient of loss w.r.t. prediction (dim: 100 x 1)\n  grad_y_pred <- 2 * (y_pred - y)\n  # gradient of loss w.r.t. w2 (dim: 32 x 1)\n  grad_w2 <- h_relu$t()$mm(grad_y_pred)\n  # gradient of loss w.r.t. hidden activation (dim: 100 x 32)\n  grad_h_relu <- grad_y_pred$mm(w2$t())\n  # gradient of loss w.r.t. hidden pre-activation (dim: 100 x 32)\n  grad_h <- grad_h_relu$clone()\n  \n  grad_h[h < 0] <- 0\n  \n  # gradient of loss w.r.t. b2 (shape: ())\n  grad_b2 <- grad_y_pred$sum()\n  \n  # gradient of loss w.r.t. w1 (dim: 3 x 32)\n  grad_w1 <- x$t()$mm(grad_h)\n  # gradient of loss w.r.t. b1 (shape: (32, ))\n  grad_b1 <- grad_h$sum(dim = 1)\n\n\nThe final step then uses the calculated gradients to update the weights:\n\n\n  learning_rate <- 1e-4\n  \n  w2 <- w2 - learning_rate * grad_w2\n  b2 <- b2 - learning_rate * grad_b2\n  w1 <- w1 - learning_rate * grad_w1\n  b1 <- b1 - learning_rate * grad_b1\n\n\nLet’s use these snippets to fill in the gaps in the above template, and give it a try!\nPutting it all together\n\n\nlibrary(torch)\n\n### generate training data -----------------------------------------------------\n\n# input dimensionality (number of input features)\nd_in <- 3\n# output dimensionality (number of predicted features)\nd_out <- 1\n# number of observations in training set\nn <- 100\n\n\n# create random data\nx <- torch_randn(n, d_in)\ny <-\n  x[, 1, NULL] * 0.2 - x[, 2, NULL] * 1.3 - x[, 3, NULL] * 0.5 + torch_randn(n, 1)\n\n\n### initialize weights ---------------------------------------------------------\n\n# dimensionality of hidden layer\nd_hidden <- 32\n# weights connecting input to hidden layer\nw1 <- torch_randn(d_in, d_hidden)\n# weights connecting hidden to output layer\nw2 <- torch_randn(d_hidden, d_out)\n\n# hidden layer bias\nb1 <- torch_zeros(1, d_hidden)\n# output layer bias\nb2 <- torch_zeros(1, d_out)\n\n### network parameters ---------------------------------------------------------\n\nlearning_rate <- 1e-4\n\n### training loop --------------------------------------------------------------\n\nfor (t in 1:200) {\n  ### -------- Forward pass --------\n  \n  # compute pre-activations of hidden layers (dim: 100 x 32)\n  h <- x$mm(w1) + b1\n  # apply activation function (dim: 100 x 32)\n  h_relu <- h$clamp(min = 0)\n  # compute output (dim: 100 x 1)\n  y_pred <- h_relu$mm(w2) + b2\n  \n  ### -------- compute loss --------\n\n  loss <- as.numeric((y_pred - y)$pow(2)$sum())\n  \n  if (t %% 10 == 0)\n    cat(\"Epoch: \", t, \"   Loss: \", loss, \"\\n\")\n  \n  ### -------- Backpropagation --------\n  \n  # gradient of loss w.r.t. prediction (dim: 100 x 1)\n  grad_y_pred <- 2 * (y_pred - y)\n  # gradient of loss w.r.t. w2 (dim: 32 x 1)\n  grad_w2 <- h_relu$t()$mm(grad_y_pred)\n  # gradient of loss w.r.t. hidden activation (dim: 100 x 32)\n  grad_h_relu <- grad_y_pred$mm(\n    w2$t())\n  # gradient of loss w.r.t. hidden pre-activation (dim: 100 x 32)\n  grad_h <- grad_h_relu$clone()\n  \n  grad_h[h < 0] <- 0\n  \n  # gradient of loss w.r.t. b2 (shape: ())\n  grad_b2 <- grad_y_pred$sum()\n  \n  # gradient of loss w.r.t. w1 (dim: 3 x 32)\n  grad_w1 <- x$t()$mm(grad_h)\n  # gradient of loss w.r.t. b1 (shape: (32, ))\n  grad_b1 <- grad_h$sum(dim = 1)\n  \n  ### -------- Update weights --------\n  \n  w2 <- w2 - learning_rate * grad_w2\n  b2 <- b2 - learning_rate * grad_b2\n  w1 <- w1 - learning_rate * grad_w1\n  b1 <- b1 - learning_rate * grad_b1\n  \n}\n\n\nEpoch:  10     Loss:  352.3585 \nEpoch:  20     Loss:  219.3624 \nEpoch:  30     Loss:  155.2307 \nEpoch:  40     Loss:  124.5716 \nEpoch:  50     Loss:  109.2687 \nEpoch:  60     Loss:  100.1543 \nEpoch:  70     Loss:  94.77817 \nEpoch:  80     Loss:  91.57003 \nEpoch:  90     Loss:  89.37974 \nEpoch:  100    Loss:  87.64617 \nEpoch:  110    Loss:  86.3077 \nEpoch:  120    Loss:  85.25118 \nEpoch:  130    Loss:  84.37959 \nEpoch:  140    Loss:  83.44133 \nEpoch:  150    Loss:  82.60386 \nEpoch:  160    Loss:  81.85324 \nEpoch:  170    Loss:  81.23454 \nEpoch:  180    Loss:  80.68679 \nEpoch:  190    Loss:  80.16555 \nEpoch:  200    Loss:  79.67953 \nThis looks like it worked pretty well! It also should have fulfilled its purpose: Showing what you can achieve using torch tensors alone. In case you didn’t feel like going through the backprop logic with too much enthusiasm, don’t worry: In the next installment, this will get significantly less cumbersome. See you then!\n\nAlthough the assumption may be tempting, “contiguous” does not correspond to what we’d call “contiguous in memory” in casual language.↩︎\nFor correctness’ sake, contiguous() will only make a copy if the tensor it is called on is not contiguous already.↩︎\nJust to avoid any misunderstandings: In the next installment, this will be very first thing rendered obsolete by torch’s automatic differentiation capabilities.↩︎\n",
    "preview": "posts/2020-10-01-torch-network-from-scratch/images/pic.jpg",
    "last_modified": "2024-11-21T15:52:48+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-09-30-sparklyr-1.4.0-released/",
    "title": "sparklyr 1.4: Weighted Sampling, Tidyr Verbs, Robust Scaler, RAPIDS, and more",
    "description": "Sparklyr 1.4 is now available! This release comes with delightful new features such as weighted sampling and tidyr verbs support for Spark dataframes, robust scaler for standardizing data based on median and interquartile range, spark_connect interface for RAPIDS GPU acceleration plugin, as well as a number of dplyr-related improvements.",
    "author": [
      {
        "name": "Yitao Li",
        "url": "https://github.com/yitao-li"
      }
    ],
    "date": "2020-09-30",
    "categories": [
      "R",
      "Packages/Releases",
      "Distributed Computing"
    ],
    "contents": "\nsparklyr 1.4 is now available on CRAN! To install sparklyr 1.4 from CRAN, run\n\n\ninstall.packages(\"sparklyr\")\n\n\nIn this blog post, we will showcase the following much-anticipated new functionalities from the sparklyr 1.4 release:\nParallelized Weighted Sampling with Spark\nSupport for Tidyr Verbs on Spark Dataframes\nft_robust_scaler as the R interface for RobustScaler from Spark 3.0\nOption for enabling RAPIDS GPU acceleration plugin in spark_connect()\nHigher-order functions and dplyr-related improvements\nParallelized Weighted Sampling\nReaders familiar with dplyr::sample_n() and dplyr::sample_frac() functions may have noticed that both of them support weighted-sampling use cases on R dataframes, e.g.,\n\n\ndplyr::sample_n(mtcars, size = 3, weight = mpg, replace = FALSE)\n\n\n               mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nFiat 128      32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nMerc 280C     17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMazda RX4 Wag 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nand\n\n\ndplyr::sample_frac(mtcars, size = 0.1, weight = mpg, replace = FALSE)\n\n\n             mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nHonda Civic 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nMerc 450SE  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nFiat X1-9   27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nwill select some random subset of mtcars using the mpg attribute as the sampling weight for each row. If replace = FALSE is set, then a row is removed from the sampling population once it gets selected, whereas when setting replace = TRUE, each row will always stay in the sampling population and can be selected multiple times.\nNow the exact same use cases are supported for Spark dataframes in sparklyr 1.4! For example:\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\nmtcars_sdf <- copy_to(sc, mtcars, repartition = 4L)\n\ndplyr::sample_n(mtcars_sdf, size = 5, weight = mpg, replace = FALSE)\n\n\nwill return a random subset of size 5 from the Spark dataframe mtcars_sdf.\nMore importantly, the sampling algorithm implemented in sparklyr 1.4 is something that fits perfectly into the MapReduce paradigm: as we have split our mtcars data into 4 partitions of mtcars_sdf by specifying repartition = 4L, the algorithm will first process each partition independently and in parallel, selecting a sample set of size up to 5 from each, and then reduce all 4 sample sets into a final sample set of size 5 by choosing records having the top 5 highest sampling priorities among all.\nHow is such parallelization possible, especially for the sampling without replacement scenario, where the desired result is defined as the outcome of a sequential process? A detailed answer to this question is in this blog post, which includes a definition of the problem (in particular, the exact meaning of sampling weights in term of probabilities), a high-level explanation of the current solution and the motivation behind it, and also, some mathematical details all hidden in one link to a PDF file, so that non-math-oriented readers can get the gist of everything else without getting scared away, while math-oriented readers can enjoy working out all the integrals themselves before peeking at the answer.\nTidyr Verbs\nThe specialized implementations of the following tidyr verbs that work efficiently with Spark dataframes were included as part of sparklyr 1.4:\ntidyr::fill\ntidyr::nest\ntidyr::unnest\ntidyr::pivot_wider\ntidyr::pivot_longer\ntidyr::separate\ntidyr::unite\nWe can demonstrate how those verbs are useful for tidying data through some examples.\nLet’s say we are given mtcars_sdf, a Spark dataframe containing all rows from mtcars plus the name of each row:\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\nmtcars_sdf <- cbind(\n  data.frame(model = rownames(mtcars)),\n  data.frame(mtcars, row.names = NULL)\n) %>%\n  copy_to(sc, ., repartition = 4L)\n\nprint(mtcars_sdf, n = 5)\n\n\n# Source: spark<?> [?? x 12]\n  model          mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <chr>        <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 Mazda RX4     21       6   160   110  3.9   2.62  16.5     0     1     4     4\n2 Mazda RX4 W…  21       6   160   110  3.9   2.88  17.0     0     1     4     4\n3 Datsun 710    22.8     4   108    93  3.85  2.32  18.6     1     1     4     1\n4 Hornet 4 Dr…  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1\n5 Hornet Spor…  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2\n# … with more rows\nand we would like to turn all numeric attributes in mtcar_sdf (in other words, all columns other than the model column) into key-value pairs stored in 2 columns, with the key column storing the name of each attribute, and the value column storing each attribute’s numeric value. One way to accomplish that with tidyr is by utilizing the tidyr::pivot_longer functionality:\n\n\nmtcars_kv_sdf <- mtcars_sdf %>%\n  tidyr::pivot_longer(cols = -model, names_to = \"key\", values_to = \"value\")\nprint(mtcars_kv_sdf, n = 5)\n\n\n# Source: spark<?> [?? x 3]\n  model     key   value\n  <chr>     <chr> <dbl>\n1 Mazda RX4 am      1\n2 Mazda RX4 carb    4\n3 Mazda RX4 cyl     6\n4 Mazda RX4 disp  160\n5 Mazda RX4 drat    3.9\n# … with more rows\nTo undo the effect of tidyr::pivot_longer, we can apply tidyr::pivot_wider to our mtcars_kv_sdf Spark dataframe, and get back the original data that was present in mtcars_sdf:\n\n\ntbl <- mtcars_kv_sdf %>%\n  tidyr::pivot_wider(names_from = key, values_from = value)\nprint(tbl, n = 5)\n\n\n# Source: spark<?> [?? x 12]\n  model         carb   cyl  drat    hp   mpg    vs    wt    am  disp  gear  qsec\n  <chr>        <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 Mazda RX4        4     6  3.9    110  21       0  2.62     1  160      4  16.5\n2 Hornet 4 Dr…     1     6  3.08   110  21.4     1  3.22     0  258      3  19.4\n3 Hornet Spor…     2     8  3.15   175  18.7     0  3.44     0  360      3  17.0\n4 Merc 280C        4     6  3.92   123  17.8     1  3.44     0  168.     4  18.9\n5 Merc 450SLC      3     8  3.07   180  15.2     0  3.78     0  276.     3  18\n# … with more rows\nAnother way to reduce many columns into fewer ones is by using tidyr::nest to move some columns into nested tables. For instance, we can create a nested table perf encapsulating all performance-related attributes from mtcars (namely, hp, mpg, disp, and qsec). However, unlike R dataframes, Spark Dataframes do not have the concept of nested tables, and the closest to nested tables we can get is a perf column containing named structs with hp, mpg, disp, and qsec attributes:\n\n\nmtcars_nested_sdf <- mtcars_sdf %>%\n  tidyr::nest(perf = c(hp, mpg, disp, qsec))\n\n\nWe can then inspect the type of perf column in mtcars_nested_sdf:\n\n\nsdf_schema(mtcars_nested_sdf)$perf$type\n\n\n[1] \"ArrayType(StructType(StructField(hp,DoubleType,true), StructField(mpg,DoubleType,true), StructField(disp,DoubleType,true), StructField(qsec,DoubleType,true)),true)\"\nand inspect individual struct elements within perf:\n\n\nperf <- mtcars_nested_sdf %>% dplyr::pull(perf)\nunlist(perf[[1]])\n\n\n    hp    mpg   disp   qsec\n110.00  21.00 160.00  16.46\nFinally, we can also use tidyr::unnest to undo the effects of tidyr::nest:\n\n\nmtcars_unnested_sdf <- mtcars_nested_sdf %>%\n  tidyr::unnest(col = perf)\nprint(mtcars_unnested_sdf, n = 5)\n\n\n# Source: spark<?> [?? x 12]\n  model          cyl  drat    wt    vs    am  gear  carb    hp   mpg  disp  qsec\n  <chr>        <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 Mazda RX4        6  3.9   2.62     0     1     4     4   110  21    160   16.5\n2 Hornet 4 Dr…     6  3.08  3.22     1     0     3     1   110  21.4  258   19.4\n3 Duster 360       8  3.21  3.57     0     0     3     4   245  14.3  360   15.8\n4 Merc 280         6  3.92  3.44     1     0     4     4   123  19.2  168.  18.3\n5 Lincoln Con…     8  3     5.42     0     0     3     4   215  10.4  460   17.8\n# … with more rows\nRobust Scaler\nRobustScaler is a new functionality introduced in Spark 3.0 (SPARK-28399). Thanks to a pull request by @zero323, an R interface for RobustScaler, namely, the ft_robust_scaler() function, is now part of sparklyr.\nIt is often observed that many machine learning algorithms perform better on numeric inputs that are standardized. Many of us have learned in stats 101 that given a random variable \\(X\\), we can compute its mean \\(\\mu = E[X]\\), standard deviation \\(\\sigma = \\sqrt{E[X^2] - (E[X])^2}\\), and then obtain a standard score \\(z = \\frac{X - \\mu}{\\sigma}\\) which has mean of 0 and standard deviation of 1.\nHowever, notice both \\(E[X]\\) and \\(E[X^2]\\) from above are quantities that can be easily skewed by extreme outliers in \\(X\\), causing distortions in \\(z\\). A particular bad case of it would be if all non-outliers among \\(X\\) are very close to \\(0\\), hence making \\(E[X]\\) close to \\(0\\), while extreme outliers are all far in the negative direction, hence dragging down \\(E[X]\\) while skewing \\(E[X^2]\\) upwards.\nAn alternative way of standardizing \\(X\\) based on its median, 1st quartile, and 3rd quartile values, all of which are robust against outliers, would be the following:\n\\(\\displaystyle z = \\frac{X - \\text{Median}(X)}{\\text{P75}(X) - \\text{P25}(X)}\\)\nand this is precisely what RobustScaler offers.\nTo see ft_robust_scaler() in action and demonstrate its usefulness, we can go through a contrived example consisting of the following steps:\nDraw 500 random samples from the standard normal distribution\n\n\nsample_values <- rnorm(500)\nprint(sample_values)\n\n\n  [1] -0.626453811  0.183643324 -0.835628612  1.595280802  0.329507772\n  [6] -0.820468384  0.487429052  0.738324705  0.575781352 -0.305388387\n  ...\nInspect the minimal and maximal values among the \\(500\\) random samples:\n\n\nprint(min(sample_values))\n\n\n  [1] -3.008049\n\n\nprint(max(sample_values))\n\n\n  [1] 3.810277\nNow create \\(10\\) other values that are extreme outliers compared to the \\(500\\) random samples above. Given that we know all \\(500\\) samples are within the range of \\((-4, 4)\\), we can choose \\(-501, -502, \\ldots, -509, -510\\) as our \\(10\\) outliers:\n\n\noutliers <- -500L - seq(10)\n\n\nCopy all \\(510\\) values into a Spark dataframe named sdf\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\", version = \"3.0.0\")\nsdf <- copy_to(sc, data.frame(value = c(sample_values, outliers)))\n\n\nWe can then apply ft_robust_scaler() to obtain the standardized value for each input:\n\n\nscaled <- sdf %>%\n  ft_vector_assembler(\"value\", \"input\") %>%\n  ft_robust_scaler(\"input\", \"scaled\") %>%\n  dplyr::pull(scaled) %>%\n  unlist()\n\n\nPlotting the result shows the non-outlier data points being scaled to values that still more or less form a bell-shaped distribution centered around \\(0\\), as expected, so the scaling is robust against influence of the outliers:\n\n\nlibrary(ggplot2)\n\nggplot(data.frame(scaled = scaled), aes(x = scaled)) +\n  xlim(-7, 7) +\n  geom_histogram(binwidth = 0.2)\n\n\n\nFinally, we can compare the distribution of the scaled values above with the distribution of z-scores of all input values, and notice how scaling the input with only mean and standard deviation would have caused noticeable skewness – which the robust scaler has successfully avoided:\n\n\nall_values <- c(sample_values, outliers)\nz_scores <- (all_values - mean(all_values)) / sd(all_values)\nggplot(data.frame(scaled = z_scores), aes(x = scaled)) +\n  xlim(-0.05, 0.2) +\n  geom_histogram(binwidth = 0.005)\n\n\n\nFrom the 2 plots above, one can observe while both standardization processes produced some distributions that were still bell-shaped, the one produced by ft_robust_scaler() is centered around \\(0\\), correctly indicating the average among all non-outlier values, while the z-score distribution is clearly not centered around \\(0\\) as its center has been noticeably shifted by the \\(10\\) outlier values.\nRAPIDS\nReaders following Apache Spark releases closely probably have noticed the recent addition of RAPIDS GPU acceleration support in Spark 3.0. Catching up with this recent development, an option to enable RAPIDS in Spark connections was also created in sparklyr and shipped in sparklyr 1.4. On a host with RAPIDS-capable hardware (e.g., an Amazon EC2 instance of type ‘p3.2xlarge’), one can install sparklyr 1.4 and observe RAPIDS hardware acceleration being reflected in Spark SQL physical query plans:\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\", version = \"3.0.0\", packages = \"rapids\")\ndplyr::db_explain(sc, \"SELECT 4\")\n\n\n== Physical Plan ==\n*(2) GpuColumnarToRow false\n+- GpuProject [4 AS 4#45]\n   +- GpuRowToColumnar TargetSize(2147483647)\n      +- *(1) Scan OneRowRelation[]\nHigher-Order Functions and dplyr-Related Improvements\nAll newly introduced higher-order functions from Spark 3.0, such as array_sort() with custom comparator, transform_keys(), transform_values(), and map_zip_with(), are supported by sparklyr 1.4.\nIn addition, all higher-order functions can now be accessed directly through dplyr rather than their hof_* counterparts in sparklyr. This means, for example, that we can run the following dplyr queries to calculate the square of all array elements in column x of sdf, and then sort them in descending order:\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\", version = \"3.0.0\")\nsdf <- copy_to(sc, tibble::tibble(x = list(c(-3, -2, 1, 5), c(6, -7, 5, 8))))\n\nsq_desc <- sdf %>%\n  dplyr::mutate(x = transform(x, ~ .x * .x)) %>%\n  dplyr::mutate(x = array_sort(x, ~ as.integer(sign(.y - .x)))) %>%\n  dplyr::pull(x)\n\nprint(sq_desc)\n\n\n[[1]]\n[1] 25  9  4  1\n\n[[2]]\n[1] 64 49 36 25\nAcknowledgement\nIn chronological order, we would like to thank the following individuals for their contributions to sparklyr 1.4:\n@javierluraschi\n@nealrichardson\n@yitao-li\n@wkdavis\n@Loquats\n@zero323\nWe also appreciate bug reports, feature requests, and valuable other feedback about sparklyr from our awesome open-source community (e.g., the weighted sampling feature in sparklyr 1.4 was largely motivated by this Github issue filed by @ajing, and some dplyr-related bug fixes in this release were initiated in #2648 and completed with this pull request by @wkdavis).\nLast but not least, the author of this blog post is extremely grateful for fantastic editorial suggestions from @javierluraschi, @batpigandme, and @skeydan.\nIf you wish to learn more about sparklyr, we recommend checking out sparklyr.ai, spark.rstudio.com, and also some of the previous release posts such as sparklyr 1.3 and sparklyr 1.2.\nThanks for reading!\n\n\n\n",
    "preview": "posts/2020-09-30-sparklyr-1.4.0-released/images/sparklyr-1.4.jpg",
    "last_modified": "2024-11-21T15:51:29+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-09-29-introducing-torch-for-r/",
    "title": "Please allow me to introduce myself: Torch for R",
    "description": "Today, we are excited to introduce torch, an R package that allows you to use PyTorch-like functionality natively from R. No Python installation is required: torch is built directly on top of libtorch, a C++ library that provides the tensor-computation and automatic-differentiation capabilities essential to building neural networks.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-09-29",
    "categories": [
      "Packages/Releases",
      "Torch",
      "R"
    ],
    "contents": "\n\nContents\nInstallation\nData loading and pre-processing\nNetwork\nTraining\nEvaluation\nLearn\nWe need you\n\nLast January at rstudio::conf, in that distant past when conferences still used to take place at some physical location, my colleague Daniel gave a talk introducing new features and ongoing development in the tensorflow ecosystem. In the Q&A part, he was asked something unexpected: Were we going to build support for PyTorch? He hesitated; that was in fact the plan, and he had already played around with natively implementing torch tensors at a prior time, but he was not completely certain how well “it” would work.\n“It,” that is an implementation which does not bind to Python Torch, meaning, we don’t install the PyTorch wheel and import it via reticulate. Instead, we delegate to the underlying C++ library libtorch for tensor computations and automatic differentiation, while neural network features – layers, activations, optimizers – are implemented directly in R. Removing the intermediary has at least two benefits: For one, the leaner software stack means fewer possible problems in installation and fewer places to look when troubleshooting. Secondly, through its non-dependence on Python, torch does not require users to install and maintain a suitable Python environment. Depending on operating system and context, this can make an enormous difference: For example, in many organizations employees are not allowed to manipulate privileged software installations on their laptops.\nSo why did Daniel hesitate, and, if I recall correctly, give a not-too-conclusive answer? On the one hand, it was not clear whether compilation against libtorch would, on some operating systems, pose severe difficulties. (It did, but difficulties turned out to be surmountable.)1 On the other, the sheer amount of work involved in re-implementing – not all, but a big amount of – PyTorch in R seemed intimidating. Today, there is still lots of work to be done (we’ll pick up that thread at the end), but the main obstacles have been ovecome, and enough components are available that torch can be useful to the R community. Thus, without further ado, let’s train a neural network.\nYou’re not at your laptop now? Just follow along in the companion notebook on Colaboratory.\nInstallation\ntorch\nInstalling torch is as straightforward as typing\n\n\ninstall.packages(\"torch\")\n\n\nThis will detect whether you have CUDA installed, and either download the CPU or the GPU version of libtorch. Then, it will install the R package from CRAN. To make use of the very newest features, you can install the development version from GitHub:\n\n\ndevtools::install_github(\"mlverse/torch\")\n\n\nTo quickly check the installation, and whether GPU support works fine (assuming that there is a CUDA-capable NVidia GPU), create a tensor on the CUDA device:\n\n\ntorch_tensor(1, device = \"cuda\")\n\n\ntorch_tensor \n 1\n[ CUDAFloatType{1} ]\nIf all our hello torch example did was run a network on, say, simulated data, we could stop here. As we’ll do image classification, however, we need to install another package: torchvision.\ntorchvision\n\n\n\nWhereas torch is where tensors, network modules, and generic data loading functionality live, datatype-specific capabilities are – or will be – provided by dedicated packages. In general, these capabilities comprise three types of things: datasets, tools for pre-processing and data loading, and pre-trained models.\nAs of this writing, PyTorch has dedicated libraries for three domain areas: vision, text, and audio. In R, we plan to proceed analogously – “plan,” because torchtext and torchaudio are yet to be created. Right now, torchvision is all we need:\n\n\ndevtools::install_github(\"mlverse/torchvision\")\n\n\nAnd we’re ready to load the data.\nData loading and pre-processing\n\n\nlibrary(torch)\nlibrary(torchvision)\n\n\nThe list of vision datasets bundled with PyTorch is long, and they’re continually being added to torchvision.\nThe one we need right now is available already, and it’s – MNIST? … not quite: It’s my favorite “MNIST dropin,” Kuzushiji-MNIST (Clanuwat et al. 2018). Like other datasets explicitly created to replace MNIST, it has ten classes – characters, in this case, depicted as grayscale images of resolution 28x28.\nHere are the first 32 characters:\n\n\n\nFigure 1: Kuzushiji MNIST.\n\n\n\nDataset\nThe following code will download the data separately for training and test sets.\n\n\ntrain_ds <- kmnist_dataset(\n  \".\",\n  download = TRUE,\n  train = TRUE,\n  transform = transform_to_tensor\n)\n\ntest_ds <- kmnist_dataset(\n  \".\",\n  download = TRUE,\n  train = FALSE,\n  transform = transform_to_tensor\n)\n\n\nNote the transform argument. transform_to_tensor takes an image and applies two transformations: First, it normalizes the pixels to the range between 0 and 1. Then, it adds another dimension in front. Why?\nContrary to what you might expect – if until now, you’ve been using keras – the additional dimension is not the batch dimension. Batching will be taken care of by the dataloader, to be introduced next. Instead, this is the channels dimension that in torch, is found before the width and height dimensions by default.\nOne thing I’ve found to be extremely useful about torch is how easy it is to inspect objects. Even though we’re dealing with a dataset, a custom object, and not an R array or even a torch tensor, we can easily peek at what’s inside. Indexing in torch is 1-based, conforming to the R user’s intuitions. Consequently,\n\n\ntrain_ds[1]\n\n\ngives us the first element in the dataset, an R list of two tensors corresponding to input and target, respectively. (We don’t reproduce the output here, but you can see for yourself in the notebook.)\nLet’s inspect the shape of the input tensor:\n\n\ntrain_ds[1][[1]]$size()\n\n\n[1]  1 28 28\nNow that we have the data, we need someone to feed them to a deep learning model, nicely batched and all. In torch, this is the task of data loaders.\nData loader\nEach of the training and test sets gets their own data loader:\n\n\ntrain_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE)\ntest_dl <- dataloader(test_ds, batch_size = 32)\n\n\nAgain, torch makes it easy to verify we did the correct thing. To take a look at the content of the first batch, do\n\n\ntrain_iter <- train_dl$.iter()\ntrain_iter$.next()\n\n\nFunctionality like this may not seem indispensable when working with a well-known dataset, but it will turn out to be very useful when a lot of domain-specific pre-processing is required.\nNow that we’ve seen how to load data, all prerequisites are fulfilled for visualizing them. Here is the code that was used to display the first batch of characters, above:\n\n\npar(mfrow = c(4,8), mar = rep(0, 4))\nimages <- train_dl$.iter()$.next()[[1]][1:32, 1, , ] \nimages %>%\n  purrr::array_tree(1) %>%\n  purrr::map(as.raster) %>%\n  purrr::iwalk(~{plot(.x)})\n\n\nWe’re ready to define our network – a simple convnet.\nNetwork\nIf you’ve been using keras custom models (or have some experience with PyTorch), the following way of defining a network may not look too surprising.\nYou use nn_module() to define an R6 class that will hold the network’s components. Its layers are created in initialize(); forward() describes what happens during the network’s forward pass. One thing on terminology: In torch, layers are called modules, as are networks. This makes sense: The design is truly modular in that any module can be used as a component in a larger one.\n\n\nnet <- nn_module(\n  \n  \"KMNIST-CNN\",\n  \n  initialize = function() {\n    # in_channels, out_channels, kernel_size, stride = 1, padding = 0\n    self$conv1 <- nn_conv2d(1, 32, 3)\n    self$conv2 <- nn_conv2d(32, 64, 3)\n    self$dropout1 <- nn_dropout2d(0.25)\n    self$dropout2 <- nn_dropout2d(0.5)\n    self$fc1 <- nn_linear(9216, 128)\n    self$fc2 <- nn_linear(128, 10)\n  },\n  \n  forward = function(x) {\n    x %>% \n      self$conv1() %>%\n      nnf_relu() %>%\n      self$conv2() %>%\n      nnf_relu() %>%\n      nnf_max_pool2d(2) %>%\n      self$dropout1() %>%\n      torch_flatten(start_dim = 2) %>%\n      self$fc1() %>%\n      nnf_relu() %>%\n      self$dropout2() %>%\n      self$fc2()\n  }\n)\n\n\nThe layers – apologies: modules – themselves may look familiar. Unsurprisingly, nn_conv2d() performs two-dimensional convolution; nn_linear() multiplies by a weight matrix and adds a vector of biases. But what are those numbers: nn_linear(128, 10), say?\nIn torch, instead of the number of units in a layer, you specify input and output dimensionalities of the “data” that run through it. Thus, nn_linear(128, 10) has 128 input connections and outputs 10 values – one for every class. In some cases, such as this one, specifying dimensions is easy – we know how many input edges there are (namely, the same as the number of output edges from the previous layer), and we know how many output values we need. But how about the previous module? How do we arrive at 9216 input connections?\nHere, a bit of calculation is necessary. We go through all actions that happen in forward() – if they affect shapes, we keep track of the transformation; if they don’t, we ignore them.\nSo, we start with input tensors of shape batch_size x 1 x 28 x 28. Then,\nnn_conv2d(1, 32, 3) , or equivalently, nn_conv2d(in_channels = 1, out_channels = 32, kernel_size = 3),applies a convolution with kernel size 3, stride 1 (the default), and no padding (the default). We can consult the documentation to look up the resulting output size, or just intuitively reason that with a kernel of size 3 and no padding, the image will shrink by one pixel in each direction, resulting in a spatial resolution of 26 x 26. Per channel, that is. Thus, the actual output shape is batch_size x 32 x 26 x 26 . Next,\nnnf_relu() applies ReLU activation, in no way touching the shape. Next is\nnn_conv2d(32, 64, 3), another convolution with zero padding and kernel size 3. Output size now is batch_size x 64 x 24 x 24 . Now, the second\nnnf_relu() again does nothing to the output shape, but\nnnf_max_pool2d(2) (equivalently: nnf_max_pool2d(kernel_size = 2)) does: It applies max pooling over regions of extension 2 x 2, thus downsizing the output to a format of batch_size x 64 x 12 x 12 . Now,\nnn_dropout2d(0.25) is a no-op, shape-wise, but if we want to apply a linear layer later, we need to merge all of the channels, height and width axes into a single dimension. This is done in\ntorch_flatten(start_dim = 2). Output shape is now batch_size * 9216 , since 64 * 12 * 12 = 9216 . Thus here we have the 9216 input connections fed into the\nnn_linear(9216, 128) discussed above. Again,\nnnf_relu() and nn_dropout2d(0.5) leave dimensions as they are, and finally,\nnn_linear(128, 10) gives us the desired output scores, one for each of the ten classes.\nNow you’ll be thinking, – what if my network is more complicated? Calculations could become pretty cumbersome. Luckily, with torch’s flexibility, there is another way. Since every layer is callable in isolation, we can just … create some sample data and see what happens!\nHere is a sample “image” – or more precisely, a one-item batch containing it:\n\n\nx <- torch_randn(c(1, 1, 28, 28))\n\n\nWhat if we call the first conv2d module on it?\n\n\nconv1 <- nn_conv2d(1, 32, 3)\nconv1(x)$size()\n\n\n[1]  1 32 26 26\nOr both conv2d modules?\n\n\nconv2 <- nn_conv2d(32, 64, 3)\n(conv1(x) %>% conv2())$size()\n\n\n[1]  1 64 24 24\nAnd so on. This is just one example illustrating how torchs flexibility makes developing neural nets easier.\nBack to the main thread. We instantiate the model, and we ask torch to allocate its weights (parameters) on the GPU:\n\n\nmodel <- net()\nmodel$to(device = \"cuda\")\n\n\nWe’ll do the same for the input and output data – that is, we’ll move them to the GPU. This is done in the training loop, which we’ll inspect next.\nTraining\nIn torch, when creating an optimizer, we tell it what to operate on, namely, the model’s parameters:\n\n\noptimizer <- optim_adam(model$parameters)\n\n\nWhat about the loss function? For classification with more than two classes, we use cross entropy, in torch: nnf_cross_entropy(prediction, ground_truth):\n\n\n# this will be called for every batch, see training loop below\nloss <- nnf_cross_entropy(output, b[[2]]$to(device = \"cuda\"))\n\n\nUnlike categorical cross entropy in keras , which would expect prediction to contain probabilities, as obtained by applying a softmax activation, torch’s nnf_cross_entropy() works with the raw outputs (the logits). This is why the network’s last linear layer was not followed by any activation.\nThe training loop, in fact, is a double one: It loops over epochs and batches. For every batch, it calls the model on the input, calculates the loss, and has the optimizer update the weights:\n\n\nfor (epoch in 1:5) {\n\n  l <- c()\n\n  coro::loop(for (b in train_dl) {\n    # make sure each batch's gradient updates are calculated from a fresh start\n    optimizer$zero_grad()\n    # get model predictions\n    output <- model(b[[1]]$to(device = \"cuda\"))\n    # calculate loss\n    loss <- nnf_cross_entropy(output, b[[2]]$to(device = \"cuda\"))\n    # calculate gradient\n    loss$backward()\n    # apply weight updates\n    optimizer$step()\n    # track losses\n    l <- c(l, loss$item())\n  })\n\n  cat(sprintf(\"Loss at epoch %d: %3f\\n\", epoch, mean(l)))\n}\n\n\nLoss at epoch 1: 1.795564\nLoss at epoch 2: 1.540063\nLoss at epoch 3: 1.495343\nLoss at epoch 4: 1.461649\nLoss at epoch 5: 1.446628\nAlthough there is a lot more that could be done – calculate metrics or evaluate performance on a validation set, for example – the above is a typical (if simple) template for a torch training loop.\nThe optimizer-related idioms in particular\n\n\noptimizer$zero_grad()\n# ...\nloss$backward()\n# ...\noptimizer$step()\n\n\nyou’ll keep encountering over and over.\nFinally, let’s evaluate model performance on the test set.\nEvaluation\nPutting a model in eval mode tells torch not to calculate gradients and perform backprop during the operations that follow:\n\n\nmodel$eval()\n\n\nWe iterate over the test set, keeping track of losses and accuracies obtained on the batches.\n\n\ntest_losses <- c()\ntotal <- 0\ncorrect <- 0\n\ncoro::loop(for (b in test_dl) {\n  output <- model(b[[1]]$to(device = \"cuda\"))\n  labels <- b[[2]]$to(device = \"cuda\")\n  loss <- nnf_cross_entropy(output, labels)\n  test_losses <- c(test_losses, loss$item())\n  # torch_max returns a list, with position 1 containing the values \n  # and position 2 containing the respective indices\n  predicted <- torch_max(output$data(), dim = 2)[[2]]\n  total <- total + labels$size(1)\n  # add number of correct classifications in this batch to the aggregate\n  correct <- correct + (predicted == labels)$sum()$item()\n})\n\nmean(test_losses)\n\n\n[1] 1.53784480643349\nHere is mean accuracy, computed as proportion of correct classifications:\n\n\ntest_accuracy <-  correct/total\ntest_accuracy\n\n\n[1] 0.9449\nThat’s it for our first torch example. Where to from here?\nLearn\nTo learn more, check out our vignettes on the torch website. To begin, you may want to check out these in particular:\n“Getting started” series: Build a simple neural network from scratch, starting from low-level tensor manipulation and gradually adding in higher-level features like automatic differentiation and network modules.\nMore on tensors: Tensor creation and indexing\nBackpropagation in torch: autograd\nIf you have questions, or run into problems, please feel free to ask on GitHub or on the RStudio community forum.\nWe need you\nWe very much hope that the R community will find the new functionality useful. But that’s not all. We hope that you, many of you, will take part in the journey.\nThere is not just a whole framework to be built, including many specialized modules, activation functions, optimizers and schedulers, with more of each being added continuously, on the Python side.\nThere is not just that whole “bag of data types” to be taken care of (images, text, audio…), each of which demand their own pre-processing and data-loading functionality. As everyone knows from experience, ease of data preparation is a, perhaps the essential factor in how usable a framework is.\nThen, there is the ever-expanding ecosystem of libraries built on top of PyTorch: PySyft and CrypTen for privacy-preserving machine learning, PyTorch Geometric for deep learning on manifolds, and Pyro for probabilistic programming, to name just a few.\nAll this is much more than can be done by one or two people: We need your help! Contributions are greatly welcomed at absolutely any scale:\nAdd or improve documentation, add introductory examples\nImplement missing layers (modules), activations, helper functions…\nImplement model architectures\nPort some of the PyTorch ecosystem\nOne component that should be of special interest to the R community is Torch distributions, the basis for probabilistic computation. This package is built upon by e.g. the aforementioned Pyro; at the same time, the distributions that live there are used in probabilistic neural networks or normalizing flows.\nTo reiterate, participation from the R community is greatly encouraged (more than that – fervently hoped for!). Have fun with torch, and thanks for reading!\n\n\n\nClanuwat, Tarin, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. 2018. “Deep Learning for Classical Japanese Literature.” December 3, 2018. https://arxiv.org/abs/cs.CV/1812.01718.\n\n\nIn a nutshell, Javier had the idea of wrapping libtorch into lantern, a C interface to libtorch, thus avoiding cross-compiler issues between MinGW and Visual Studio.↩︎\n",
    "preview": "posts/2020-09-29-introducing-torch-for-r/images/pt.png",
    "last_modified": "2024-11-21T15:54:00+00:00",
    "input_file": {},
    "preview_width": 919,
    "preview_height": 264
  },
  {
    "path": "posts/2020-09-07-sparklyr-flint/",
    "title": "Introducing sparklyr.flint: A time-series extension for sparklyr",
    "description": "We are pleased to announce that sparklyr.flint, a sparklyr extension for analyzing time series at scale with Flint, is now available on CRAN. Flint is an open-source library for working with time-series in Apache Spark which supports aggregates and joins on time-series datasets.",
    "author": [
      {
        "name": "Yitao Li",
        "url": {}
      }
    ],
    "date": "2020-09-07",
    "categories": [
      "R",
      "Time Series"
    ],
    "contents": "\nIn this blog post, we will showcase sparklyr.flint, a brand new sparklyr extension providing a simple and intuitive R interface to the Flint time series library. sparklyr.flint is available on CRAN today and can be installed as follows:\ninstall.packages(\"sparklyr.flint\")\nThe first two sections of this post will be a quick bird’s eye view on sparklyr and Flint, which will ensure readers unfamiliar with sparklyr or Flint can see both of them as essential building blocks for sparklyr.flint. After that, we will feature sparklyr.flint’s design philosophy, current state, example usages, and last but not least, its future directions as an open-source project in the subsequent sections.\nQuick Intro to sparklyr\nsparklyr is an open-source R interface that integrates the power of distributed computing from Apache Spark with the familiar idioms, tools, and paradigms for data transformation and data modelling in R. It allows data pipelines working well with non-distributed data in R to be easily transformed into analogous ones that can process large-scale, distributed data in Apache Spark.\nInstead of summarizing everything sparklyr has to offer in a few sentences, which is impossible to do, this section will solely focus on a small subset of sparklyr functionalities that are relevant to connecting to Apache Spark from R, importing time series data from external data sources to Spark, and also simple transformations which are typically part of data pre-processing steps.\nConnecting to an Apache Spark cluster\nThe first step in using sparklyr is to connect to Apache Spark. Usually this means one of the following:\nRunning Apache Spark locally on your machine, and connecting to it to test, debug, or to execute quick demos that don’t require a multi-node Spark cluster:\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\", version = \"2.4.4\")\n\n\nConnecting to a multi-node Apache Spark cluster that is managed by a cluster manager such as YARN, e.g.,\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"yarn-client\", spark_home = \"/usr/lib/spark\")\n\n\nImporting external data to Spark\nMaking external data available in Spark is easy with sparklyr given the large number of data sources sparklyr supports. For example, given an R dataframe, such as\n\n\ndat <- data.frame(id = seq(10), value = rnorm(10))\n\n\nthe command to copy it to a Spark dataframe with 3 partitions is simply\n\n\nsdf <- copy_to(sc, dat, name = \"unique_name_of_my_spark_dataframe\", repartition = 3L)\n\n\nSimilarly, there are options for ingesting data in CSV, JSON, ORC, AVRO, and many other well-known formats into Spark as well:\n\n\nsdf_csv <- spark_read_csv(sc, name = \"another_spark_dataframe\", path = \"file:///tmp/file.csv\", repartition = 3L)\n  # or\n  sdf_json <- spark_read_json(sc, name = \"yet_another_one\", path = \"file:///tmp/file.json\", repartition = 3L)\n  # or spark_read_orc, spark_read_avro, etc\n\n\nTransforming a Spark dataframe\nWith sparklyr, the simplest and most readable way to transformation a Spark dataframe is by using dplyr verbs and the pipe operator (%>%) from magrittr.\nSparklyr supports a large number of dplyr verbs. For example,\n\n\nsdf <- sdf %>%\n    dplyr::filter(!is.null(id)) %>%\n    dplyr::mutate(value = value ^ 2)\n\n\nEnsures sdf only contains rows with non-null IDs, and then squares the value column of each row.\nThat’s about it for a quick intro to sparklyr. You can learn more in sparklyr.ai, where you will find links to reference material, books, communities, sponsors, and much more.\nWhat is Flint?\nFlint is a powerful open-source library for working with time-series data in Apache Spark. First of all, it supports efficient computation of aggregate statistics on time-series data points having the same timestamp (a.k.a summarizeCycles in Flint nomenclature), within a given time window (a.k.a., summarizeWindows), or within some given time intervals (a.k.a summarizeIntervals). It can also join two or more time-series datasets based on inexact match of timestamps using asof join functions such as LeftJoin and FutureLeftJoin. The author of Flint has outlined many more of Flint’s major functionalities in this article, which I found to be extremely helpful when working out how to build sparklyr.flint as a simple and straightforward R interface for such functionalities.\nReaders wanting some direct hands-on experience with Flint and Apache Spark can go through the following steps to run a minimal example of using Flint to analyze time-series data:\nFirst, install Apache Spark locally, and then for convenience reasons, define the SPARK_HOME environment variable. In this example, we will run Flint with Apache Spark 2.4.4 installed at ~/spark, so:\n\nexport SPARK_HOME=~/spark/spark-2.4.4-bin-hadoop2.7\n\nLaunch Spark shell and instruct it to download Flint and its Maven dependencies:\n\n\"${SPARK_HOME}\"/bin/spark-shell --packages=com.twosigma:flint:0.6.0\n\nCreate a simple Spark dataframe containing some time-series data:\n\nimport spark.implicits._\n\nval ts_sdf = Seq((1L, 1), (2L, 4), (3L, 9), (4L, 16)).toDF(\"time\", \"value\")\n\nImport the dataframe along with additional metadata such as time unit and name of the timestamp column into a TimeSeriesRDD, so that Flint can interpret the time-series data unambiguously:\n\nimport com.twosigma.flint.timeseries.TimeSeriesRDD\n\nval ts_rdd = TimeSeriesRDD.fromDF(\n  ts_sdf\n)(\n  isSorted = true, // rows are already sorted by time\n  timeUnit = java.util.concurrent.TimeUnit.SECONDS,\n  timeColumn = \"time\"\n)\n\nFinally, after all the hard work above, we can leverage various time-series functionalities provided by Flint to analyze ts_rdd. For example, the following will produce a new column named value_sum. For each row, value_sum will contain the summation of values that occurred within the past 2 seconds from the timestamp of that row:\n\nimport com.twosigma.flint.timeseries.Windows\nimport com.twosigma.flint.timeseries.Summarizers\n\nval window = Windows.pastAbsoluteTime(\"2s\")\nval summarizer = Summarizers.sum(\"value\")\nval result = ts_rdd.summarizeWindows(window, summarizer)\n\nresult.toDF.show()\n\n    +-------------------+-----+---------+\n    |               time|value|value_sum|\n    +-------------------+-----+---------+\n    |1970-01-01 00:00:01|    1|      1.0|\n    |1970-01-01 00:00:02|    4|      5.0|\n    |1970-01-01 00:00:03|    9|     14.0|\n    |1970-01-01 00:00:04|   16|     29.0|\n    +-------------------+-----+---------+\n     In other words, given a timestamp t and a row in the result having time equal to t, one can notice the value_sum column of that row contains sum of values within the time window of [t - 2, t] from ts_rdd.\nIntro to sparklyr.flint\nThe purpose of sparklyr.flint is to make time-series functionalities of Flint easily accessible from sparklyr. To see sparklyr.flint in action, one can skim through the example in the previous section, go through the following to produce the exact R-equivalent of each step in that example, and then obtain the same summarization as the final result:\nFirst of all, install sparklyr and sparklyr.flint if you haven’t done so already.\n\n\ninstall.packages(\"sparklyr\")\ninstall.packages(\"sparklyr.flint\")\n\n\nConnect to Apache Spark that is running locally from sparklyr, but remember to attach sparklyr.flint before running sparklyr::spark_connect, and then import our example time-series data to Spark:\n\n\nlibrary(sparklyr)\nlibrary(sparklyr.flint)\n\nsc <- spark_connect(master = \"local\", version = \"2.4\")\nsdf <- copy_to(sc, data.frame(time = seq(4), value = seq(4)^2))\n\n\nConvert sdf above into a TimeSeriesRDD\n\n\nts_rdd <- fromSDF(sdf, is_sorted = TRUE, time_unit = \"SECONDS\", time_column = \"time\")\n\n\nAnd finally, run the ‘sum’ summarizer to obtain a summation of values in all past-2-second time windows:\n\n\nresult <- summarize_sum(ts_rdd, column = \"value\", window = in_past(\"2s\"))\n\nprint(result %>% collect())\n\n\n\n\n## # A tibble: 4 x 3\n##   time                value value_sum\n##   <dttm>              <dbl>     <dbl>\n## 1 1970-01-01 00:00:01     1         1\n## 2 1970-01-01 00:00:02     4         5\n## 3 1970-01-01 00:00:03     9        14\n## 4 1970-01-01 00:00:04    16        29\n\n\nWhy create a sparklyr extension?\nThe alternative to making sparklyr.flint a sparklyr extension is to bundle all time-series functionalities it provides with sparklyr itself. We decided that this would not be a good idea because of the following reasons:\nNot all sparklyr users will need those time-series functionalities\ncom.twosigma:flint:0.6.0 and all Maven packages it transitively relies on are quite heavy dependency-wise\nImplementing an intuitive R interface for Flint also takes a non-trivial number of R source files, and making all of that part of sparklyr itself would be too much\nSo, considering all of the above, building sparklyr.flint as an extension of sparklyr seems to be a much more reasonable choice.\nCurrent state of sparklyr.flint and its future directions\nRecently sparklyr.flint has had its first successful release on CRAN. At the moment, sparklyr.flint only supports the summarizeCycle and summarizeWindow functionalities of Flint, and does not yet support asof join and other useful time-series operations. While sparklyr.flint contains R interfaces to most of the summarizers in Flint (one can find the list of summarizers currently supported by sparklyr.flint in here), there are still a few of them missing (e.g., the support for OLSRegressionSummarizer, among others).\nIn general, the goal of building sparklyr.flint is for it to be a thin “translation layer” between sparklyr and Flint. It should be as simple and intuitive as possibly can be, while supporting a rich set of Flint time-series functionalities.\nWe cordially welcome any open-source contribution towards sparklyr.flint. Please visit https://github.com/r-spark/sparklyr.flint/issues if you would like to initiate discussions, report bugs, or propose new features related to sparklyr.flint, and https://github.com/r-spark/sparklyr.flint/pulls if you would like to send pull requests.\nAcknowledgement\nFirst and foremost, the author wishes to thank Javier (@javierluraschi) for proposing the idea of creating sparklyr.flint as the R interface for Flint, and for his guidance on how to build it as an extension to sparklyr.\nBoth Javier (@javierluraschi) and Daniel (@dfalbel) have offered numerous helpful tips on making the initial submission of sparklyr.flint to CRAN successful.\nWe really appreciate the enthusiasm from sparklyr users who were willing to give sparklyr.flint a try shortly after it was released on CRAN (and there were quite a few downloads of sparklyr.flint in the past week according to CRAN stats, which was quite encouraging for us to see). We hope you enjoy using sparklyr.flint.\nThe author is also grateful for valuable editorial suggestions from Mara (@batpigandme), Sigrid (@skeydan), and Javier (@javierluraschi) on this blog post.\nThanks for reading!\n\n\n\n",
    "preview": "posts/2020-09-07-sparklyr-flint/images/thumb.png",
    "last_modified": "2024-11-21T15:50:52+00:00",
    "input_file": {},
    "preview_width": 126,
    "preview_height": 77
  },
  {
    "path": "posts/2020-09-01-weather-prediction/",
    "title": "An introduction to weather forecasting with deep learning",
    "description": "A few weeks ago, we showed how to forecast chaotic dynamical systems with deep learning, augmented by a custom constraint derived from domain-specific insight. Global weather is a chaotic system, but of much higher complexity than many tasks commonly addressed with machine and/or deep learning. In this post, we provide a practical introduction featuring a simple deep learning baseline for atmospheric forecasting. While far away from being competitive, it serves to illustrate how more sophisticated and compute-intensive models may approach that formidable task by means of methods situated on the \"black-box end\" of the continuum.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-09-01",
    "categories": [
      "R",
      "TensorFlow/Keras",
      "Time Series",
      "Spatial Data"
    ],
    "contents": "\nWith all that is going on in the world these days, is it frivolous to talk about weather prediction? Asked in the 21st\ncentury, this is bound to be a rhetorical question. In the 1930s, when German poet Bertolt Brecht wrote the famous lines:\n\nWas sind das für Zeiten, wo\nEin Gespräch über Bäume fast ein Verbrechen ist\nWeil es ein Schweigen über so viele Untaten einschließt!1\n\n(“What kind of times are these, where a conversation about trees is almost a crime, for it means silence about so many\natrocities!”),\nhe couldn’t have anticipated the responses he would get in the second half of that century, with trees symbolizing, as well as\nliterally falling victim to, environmental pollution and climate change.\nToday, no lengthy justification is needed as to why prediction of atmospheric states is vital: Due to global warming,\nfrequency and intensity of severe weather conditions – droughts, wildfires, hurricanes, heatwaves – have risen and will\ncontinue to rise. And while accurate forecasts don’t change those events per se, they constitute essential information in\nmitigating their consequences. This goes for atmospheric forecasts on all scales: from so-called “nowcasting” (operating on a\nrange of about six hours), over medium-range (three to five days) and sub-seasonal (weekly/monthly), to climate forecasts\n(concerned with years and decades). Medium-range forecasts especially are extremely important in acute disaster prevention.\nThis post will show how deep learning (DL) methods can be used to generate atmospheric forecasts, using a newly published\nbenchmark dataset(Rasp et al. 2020). Future posts may refine the model used here\nand/or discuss the role of DL (“AI”) in mitigating climate change – and its implications – more globally.\nThat said, let’s put the current endeavor in context. In a way, we have here the usual dejà vu of using DL as a\nblack-box-like, magic instrument on a task where human knowledge used to be required. Of course, this characterization is\noverly dichotomizing; many choices are made in creating DL models, and performance is necessarily constrained by available\nalgorithms – which may, or may not, fit the domain to be modeled to a sufficient degree.\nIf you’ve started learning about image recognition rather recently, you may well have been using DL methods from the outset,\nand not have heard much about the rich set of feature engineering methods developed in pre-DL image recognition. In the\ncontext of atmospheric prediction, then, let’s begin by asking: How in the world did they do that before?\nNumerical weather prediction in a nutshell\nIt is not like machine learning and/or statistics are not already used in numerical weather prediction – on the contrary. For\nexample, every model has to start from somewhere; but raw observations are not suited to direct use as initial conditions.\nInstead, they have to be assimilated to the four-dimensional grid2 over which model computations are performed. At the\nother end, namely, model output, statistical post-processing is used to refine the predictions. And very importantly, ensemble\nforecasts are employed to determine uncertainty.\nThat said, the model core, the part that extrapolates into the future atmospheric conditions observed today, is based on a\nset of differential3 equations, the so-called primitive equations,\nthat are due to the conservation laws of momentum,\nenergy, and\nmass. These differential equations cannot be solved analytically;\nrather, they have to be solved numerically, and that on a grid of resolution as high as possible. In that light, even deep\nlearning could appear as just “moderately resource-intensive” (dependent, though, on the model in question). So how, then,\ncould a DL approach look?\nDeep learning models for weather prediction\nAccompanying the benchmark dataset they created, Rasp et al.(Rasp et al. 2020) provide a set of notebooks, including one\ndemonstrating the use of a simple convolutional neural network to predict two of the available atmospheric variables, 500hPa\ngeopotential and 850hPa temperature. Here 850hPa temperature is the (spatially varying) temperature at a fix atmospheric\nheight of 850hPa (~ 1.5 kms)4 ; 500hPa geopotential is proportional to the (again, spatially varying) altitude\nassociated with the pressure level in question (500hPa).\nFor this task, two-dimensional convnets, as usually employed in image processing, are a natural fit: Image width and height\nmap to longitude and latitude of the spatial grid, respectively; target variables appear as channels. In this architecture,\nthe time series character of the data is essentially lost: Every sample stands alone, without dependency on either past or\npresent. In this respect, as well as given its size and simplicity, the convnet presented below is only a toy model, meant to\nintroduce the approach as well as the application overall. It may also serve as a deep learning baseline, along with two\nother types of baseline commonly used in numerical weather prediction introduced below.\nDirections on how to improve on that baseline are given by recent publications. Weyn et al.(Weyn, Durran, and Caruana, n.d.), in addition to applying\nmore geometrically-adequate spatial preprocessing, use a U-Net-based architecture instead of a plain convnet. Rasp and Thuerey\n(Rasp and Thuerey 2020), building on a fully convolutional, high-capacity ResNet architecture, add a key new procedural ingredient:\npre-training on climate models. With their method, they are able to not just compete with physical models, but also, show\nevidence of the network learning about physical structure and dependencies. Unfortunately, compute facilities of this order\nare not available to the average individual, which is why we’ll content ourselves with demonstrating a simple toy model.\nStill, having seen a simple model in action, as well as the type of data it works on, should help a lot in understanding how\nDL can be used for weather prediction.\nDataset\nWeatherbench was explicitly created as a benchmark dataset and thus, as is\ncommon for this species, hides a lot of preprocessing and standardization effort from the user. Atmospheric data are available\non an hourly basis, ranging from 1979 to 2018, at different spatial resolutions. Depending on resolution, there are about 15\nto 20 measured variables, including temperature, geopotential, wind speed, and humidity. Of these variables, some are\navailable at several pressure levels. Thus, our example makes use of a small subset of available “channels.” To save storage,\nnetwork and computational resources, it also operates at the smallest available resolution.\nThis post is accompanied by executable code on Google\nColaboratory, which should not just\nrender unnecessary any copy-pasting of code snippets but also, allow for uncomplicated modification and experimentation.\nTo read in and extract the data, stored as NetCDF files, we use\ntidync, a high-level package built on top of\nncdf4 and RNetCDF. Otherwise,\navailability of the usual “TensorFlow family” as well as a subset of tidyverse packages is assumed.\n\n\nlibrary(reticulate)\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(tfdatasets)\nlibrary(tfautograph)\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(lubridate)\n\nlibrary(tidync)\n\n\nAs already alluded to, our example makes use of two spatio-temporal series: 500hPa geopotential and 850hPa temperature. The\nfollowing commands will download and unpack the respective sets of by-year files, for a spatial resolution of 5.625 degrees:\n\n\ndownload.file(\"https://dataserv.ub.tum.de/s/m1524895/download?path=%2F5.625deg%2Ftemperature_850&files=temperature_850_5.625deg.zip\",\n              \"temperature_850_5.625deg.zip\")\nunzip(\"temperature_850_5.625deg.zip\", exdir = \"temperature_850\")\n\ndownload.file(\"https://dataserv.ub.tum.de/s/m1524895/download?path=%2F5.625deg%2Fgeopotential_500&files=geopotential_500_5.625deg.zip\",\n              \"geopotential_500_5.625deg.zip\")\nunzip(\"geopotential_500_5.625deg.zip\", exdir = \"geopotential_500\")\n\n\nInspecting one of those files’ contents, we see that its data array is structured along three dimensions, longitude (64\ndifferent values), latitude (32) and time (8760). The data itself is z, the geopotential.\n\n\ntidync(\"geopotential_500/geopotential_500hPa_2015_5.625deg.nc\") %>% hyper_array()\n\n\nClass: tidync_data (list of tidync data arrays)\nVariables (1): 'z'\nDimension (3): lon,lat,time (64, 32, 8760)\nSource: /[...]/geopotential_500/geopotential_500hPa_2015_5.625deg.nc\nExtraction of the data array is as easy as telling tidync to read the first in the list of arrays:\n\n\nz500_2015 <- (tidync(\"geopotential_500/geopotential_500hPa_2015_5.625deg.nc\") %>%\n                hyper_array())[[1]]\n\ndim(z500_2015)\n\n\n[1] 64 32 8760\nWhile we delegate further introduction to tidync to a comprehensive blog\npost on the ROpenSci website, let’s at least look at a quick visualization, for\nwhich we pick the very first time point. (Extraction and visualization code is analogous for 850hPa temperature.)\n\n\nimage(z500_2015[ , , 1],\n      col = hcl.colors(20, \"viridis\"), # for temperature, the color scheme used is YlOrRd \n      xaxt = 'n',\n      yaxt = 'n',\n      main = \"500hPa geopotential\"\n)\n\n\nThe maps show how pressure5 and temperature strongly depend on latitude. Furthermore, it’s easy to spot the atmospheric\nwaves:\n\n\n\nFigure 1: Spatial distribution of 500hPa geopotential and 850 hPa temperature for 2015/01/01 0:00h.\n\n\n\nFor training, validation and testing, we choose consecutive years: 2015, 2016, and 2017, respectively.\n\n\nz500_train <- (tidync(\"geopotential_500/geopotential_500hPa_2015_5.625deg.nc\") %>% hyper_array())[[1]]\n\nt850_train <- (tidync(\"temperature_850/temperature_850hPa_2015_5.625deg.nc\") %>% hyper_array())[[1]]\n\nz500_valid <- (tidync(\"geopotential_500/geopotential_500hPa_2016_5.625deg.nc\") %>% hyper_array())[[1]]\n\nt850_valid <- (tidync(\"temperature_850/temperature_850hPa_2016_5.625deg.nc\") %>% hyper_array())[[1]]\n\nz500_test <- (tidync(\"geopotential_500/geopotential_500hPa_2017_5.625deg.nc\") %>% hyper_array())[[1]]\n\nt850_test <- (tidync(\"temperature_850/temperature_850hPa_2017_5.625deg.nc\") %>% hyper_array())[[1]]\n\n\nSince geopotential and temperature will be treated as channels, we concatenate the corresponding arrays. To transform the data\ninto the format needed for images, a permutation is necessary:\n\n\ntrain_all <- abind::abind(z500_train, t850_train, along = 4)\ntrain_all <- aperm(train_all, perm = c(3, 2, 1, 4))\ndim(train_all)\n\n\n[1] 8760 32 64 2\nAll data will be standardized according to mean and standard deviation as obtained from the training set:\n\n\nlevel_means <- apply(train_all, 4, mean)\nlevel_sds <- apply(train_all, 4, sd)\n\nround(level_means, 2)\n\n\n54124.91  274.8\nIn words, the mean geopotential height (see footnote 5 for more on this term), as measured at an isobaric surface of 500hPa,\namounts to about 5400 metres6, while the mean temperature at the 850hPa level approximates 275 Kelvin (about 2 degrees\nCelsius).\n\n\ntrain <- train_all\ntrain[, , , 1] <- (train[, , , 1] - level_means[1]) / level_sds[1]\ntrain[, , , 2] <- (train[, , , 2] - level_means[2]) / level_sds[2]\n\nvalid_all <- abind::abind(z500_valid, t850_valid, along = 4)\nvalid_all <- aperm(valid_all, perm = c(3, 2, 1, 4))\n\nvalid <- valid_all\nvalid[, , , 1] <- (valid[, , , 1] - level_means[1]) / level_sds[1]\nvalid[, , , 2] <- (valid[, , , 2] - level_means[2]) / level_sds[2]\n\ntest_all <- abind::abind(z500_test, t850_test, along = 4)\ntest_all <- aperm(test_all, perm = c(3, 2, 1, 4))\n\ntest <- test_all\ntest[, , , 1] <- (test[, , , 1] - level_means[1]) / level_sds[1]\ntest[, , , 2] <- (test[, , , 2] - level_means[2]) / level_sds[2]\n\n\nWe’ll attempt to predict three days ahead.\n\n\nlead_time <- 3 * 24 # 3d\n\n\nNow all that remains to be done is construct the actual datasets.\n\n\nbatch_size <- 32\n\ntrain_x <- train %>%\n  tensor_slices_dataset() %>%\n  dataset_take(dim(train)[1] - lead_time)\n\ntrain_y <- train %>%\n  tensor_slices_dataset() %>%\n  dataset_skip(lead_time)\n\ntrain_ds <- zip_datasets(train_x, train_y) %>%\n  dataset_shuffle(buffer_size = dim(train)[1] - lead_time) %>%\n  dataset_batch(batch_size = batch_size, drop_remainder = TRUE)\n\nvalid_x <- valid %>%\n  tensor_slices_dataset() %>%\n  dataset_take(dim(valid)[1] - lead_time)\n\nvalid_y <- valid %>%\n  tensor_slices_dataset() %>%\n  dataset_skip(lead_time)\n\nvalid_ds <- zip_datasets(valid_x, valid_y) %>%\n  dataset_batch(batch_size = batch_size, drop_remainder = TRUE)\n\ntest_x <- test %>%\n  tensor_slices_dataset() %>%\n  dataset_take(dim(test)[1] - lead_time)\n\ntest_y <- test %>%\n  tensor_slices_dataset() %>%\n  dataset_skip(lead_time)\n\ntest_ds <- zip_datasets(test_x, test_y) %>%\n  dataset_batch(batch_size = batch_size, drop_remainder = TRUE)\n\n\nLet’s proceed to defining the model.\nBasic CNN with periodic convolutions\nThe model is a straightforward convnet, with one exception: Instead of plain convolutions, it uses slightly more sophisticated\nones that “wrap around” longitudinally.\n\n\nperiodic_padding_2d <- function(pad_width,\n                                name = NULL) {\n  \n  keras_model_custom(name = name, function(self) {\n    self$pad_width <- pad_width\n    \n    function (x, mask = NULL) {\n      x <- if (self$pad_width == 0) {\n        x\n      } else {\n        lon_dim <- dim(x)[3]\n        pad_width <- tf$cast(self$pad_width, tf$int32)\n        # wrap around for longitude\n        tf$concat(list(x[, ,-pad_width:lon_dim,],\n                       x,\n                       x[, , 1:pad_width,]),\n                  axis = 2L) %>%\n          tf$pad(list(\n            list(0L, 0L),\n            # zero-pad for latitude\n            list(pad_width, pad_width),\n            list(0L, 0L),\n            list(0L, 0L)\n          ))\n      }\n    }\n  })\n}\n\nperiodic_conv_2d <- function(filters,\n                             kernel_size,\n                             name = NULL) {\n  \n  keras_model_custom(name = name, function(self) {\n    self$padding <- periodic_padding_2d(pad_width = (kernel_size - 1) / 2)\n    self$conv <-\n      layer_conv_2d(filters = filters,\n                    kernel_size = kernel_size,\n                    padding = 'valid')\n    \n    function (x, mask = NULL) {\n      x %>% self$padding() %>% self$conv()\n    }\n  })\n}\n\n\nFor our purposes of establishing a deep-learning baseline that is fast to train, CNN architecture and parameter defaults are\nchosen to be simple and moderate, respectively:7\n\n\nperiodic_cnn <- function(filters = c(64, 64, 64, 64, 2),\n                         kernel_size = c(5, 5, 5, 5, 5),\n                         dropout = rep(0.2, 5),\n                         name = NULL) {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$conv1 <-\n      periodic_conv_2d(filters = filters[1], kernel_size = kernel_size[1])\n    self$act1 <- layer_activation_leaky_relu()\n    self$drop1 <- layer_dropout(rate = dropout[1])\n    self$conv2 <-\n      periodic_conv_2d(filters = filters[2], kernel_size = kernel_size[2])\n    self$act2 <- layer_activation_leaky_relu()\n    self$drop2 <- layer_dropout(rate =dropout[2])\n    self$conv3 <-\n      periodic_conv_2d(filters = filters[3], kernel_size = kernel_size[3])\n    self$act3 <- layer_activation_leaky_relu()\n    self$drop3 <- layer_dropout(rate = dropout[3])\n    self$conv4 <-\n      periodic_conv_2d(filters = filters[4], kernel_size = kernel_size[4])\n    self$act4 <- layer_activation_leaky_relu()\n    self$drop4 <- layer_dropout(rate = dropout[4])\n    self$conv5 <-\n      periodic_conv_2d(filters = filters[5], kernel_size = kernel_size[5])\n    \n    function (x, mask = NULL) {\n      x %>%\n        self$conv1() %>%\n        self$act1() %>%\n        self$drop1() %>%\n        self$conv2() %>%\n        self$act2() %>%\n        self$drop2() %>%\n        self$conv3() %>%\n        self$act3() %>%\n        self$drop3() %>%\n        self$conv4() %>%\n        self$act4() %>%\n        self$drop4() %>%\n        self$conv5()\n    }\n  })\n}\n\nmodel <- periodic_cnn()\n\n\nTraining\nIn that same spirit of “default-ness,” we train with MSE loss and Adam optimizer.\n\n\nloss <- tf$keras$losses$MeanSquaredError(reduction = tf$keras$losses$Reduction$SUM)\noptimizer <- optimizer_adam()\n\ntrain_loss <- tf$keras$metrics$Mean(name='train_loss')\n\nvalid_loss <- tf$keras$metrics$Mean(name='test_loss')\n\ntrain_step <- function(train_batch) {\n\n  with (tf$GradientTape() %as% tape, {\n    predictions <- model(train_batch[[1]])\n    l <- loss(train_batch[[2]], predictions)\n  })\n\n  gradients <- tape$gradient(l, model$trainable_variables)\n  optimizer$apply_gradients(purrr::transpose(list(\n    gradients, model$trainable_variables\n  )))\n\n  train_loss(l)\n\n}\n\nvalid_step <- function(valid_batch) {\n  predictions <- model(valid_batch[[1]])\n  l <- loss(valid_batch[[2]], predictions)\n  \n  valid_loss(l)\n}\n\ntraining_loop <- tf_function(autograph(function(train_ds, valid_ds, epoch) {\n  \n  for (train_batch in train_ds) {\n    train_step(train_batch)\n  }\n  \n  for (valid_batch in valid_ds) {\n    valid_step(valid_batch)\n  }\n  \n  tf$print(\"MSE: train: \", train_loss$result(), \", validation: \", valid_loss$result()) \n    \n}))\n\n\nDepicted graphically, we see that the model trains well, but extrapolation does not surpass a certain threshold (which is\nreached early, after training for just two epochs).\n\n\n\nFigure 2: MSE per epoch on training and validation sets.\n\n\n\nThis is not too surprising though, given the model’s architectural simplicity and modest size.\nEvaluation\nHere, we first present two other baselines, which – given a highly complex and chaotic system like the atmosphere – may\nsound irritatingly simple and yet, be pretty hard to beat. The metric used for comparison is latitudinally weighted\nroot-mean-square error. Latitudinal weighting up-weights the lower latitudes and down-weights the upper ones.\n\n\ndeg2rad <- function(d) {\n  (d / 180) * pi\n}\n\nlats <- tidync(\"geopotential_500/geopotential_500hPa_2015_5.625deg.nc\")$transforms$lat %>%\n  select(lat) %>%\n  pull()\n\nlat_weights <- cos(deg2rad(lats))\nlat_weights <- lat_weights / mean(lat_weights)\n\nweighted_rmse <- function(forecast, ground_truth) {\n  error <- (forecast - ground_truth) ^ 2\n  for (i in seq_along(lat_weights)) {\n    error[, i, ,] <- error[, i, ,] * lat_weights[i]\n  }\n  apply(error, 4, mean) %>% sqrt()\n}\n\n\nBaseline 1: Weekly climatology\nIn general, climatology refers to long-term averages computed over defined time ranges. Here, we first calculate weekly\naverages based on the training set. These averages are then used to forecast the variables in question for the time period\nused as test set.\nStep one makes use of tidync, ncmeta, RNetCDF and lubridate to compute weekly averages for 2015, following the ISO\nweek date system.\n\n\ntrain_file <- \"geopotential_500/geopotential_500hPa_2015_5.625deg.nc\"\n\ntimes_train <- (tidync(train_file) %>% activate(\"D2\") %>% hyper_array())$time\n\ntime_unit_train <- ncmeta::nc_atts(train_file, \"time\") %>%\n  tidyr::unnest(cols = c(value)) %>%\n  dplyr::filter(name == \"units\")\n\ntime_parts_train <- RNetCDF::utcal.nc(time_unit_train$value, times_train)\n\niso_train <- ISOdate(\n  time_parts_train[, \"year\"],\n  time_parts_train[, \"month\"],\n  time_parts_train[, \"day\"],\n  time_parts_train[, \"hour\"],\n  time_parts_train[, \"minute\"],\n  time_parts_train[, \"second\"]\n)\n\nisoweeks_train <- map(iso_train, isoweek) %>% unlist()\n\ntrain_by_week <- apply(train_all, c(2, 3, 4), function(x) {\n  tapply(x, isoweeks_train, function(y) {\n    mean(y)\n  })\n})\n\ndim(train_by_week)\n\n\n53 32 64 2\nStep two then runs through the test set, mapping dates to corresponding ISO weeks and associating the weekly averages from the\ntraining set:\n\n\ntest_file <- \"geopotential_500/geopotential_500hPa_2017_5.625deg.nc\"\n\ntimes_test <- (tidync(test_file) %>% activate(\"D2\") %>% hyper_array())$time\n\ntime_unit_test <- ncmeta::nc_atts(test_file, \"time\") %>%\n  tidyr::unnest(cols = c(value)) %>%\n  dplyr::filter(name == \"units\")\n\ntime_parts_test <- RNetCDF::utcal.nc(time_unit_test$value, times_test)\n\niso_test <- ISOdate(\n  time_parts_test[, \"year\"],\n  time_parts_test[, \"month\"],\n  time_parts_test[, \"day\"],\n  time_parts_test[, \"hour\"],\n  time_parts_test[, \"minute\"],\n  time_parts_test[, \"second\"]\n)\n\nisoweeks_test <- map(iso_test, isoweek) %>% unlist()\n\nclimatology_forecast <- test_all\n\nfor (i in 1:dim(climatology_forecast)[1]) {\n  week <- isoweeks_test[i]\n  lookup <- train_by_week[week, , , ]\n  climatology_forecast[i, , ,] <- lookup\n}\n\n\nFor this baseline, the latitudinally-weighted RMSE amounts to roughly 975 for geopotential and 4 for temperature.\n\n\nwrmse <- weighted_rmse(climatology_forecast, test_all)\nround(wrmse, 2)\n\n\n974.50   4.09\nBaseline 2: Persistence forecast\nThe second baseline commonly used makes a straightforward assumption: Tomorrow’s weather is today’s weather, or, in our case:\nIn three days, things will be just like they are now.\nComputation for this metric is almost a one-liner. And as it turns out, for the given lead time (three days), performance is\nnot too dissimilar from obtained by means of weekly climatology:\n\n\npersistence_forecast <- test_all[1:(dim(test_all)[1] - lead_time), , ,]\n\ntest_period <- test_all[(lead_time + 1):dim(test_all)[1], , ,]\n\nwrmse <- weighted_rmse(persistence_forecast, test_period)\n\nround(wrmse, 2)\n\n\n937.55  4.31\nBaseline 3: Simple convnet\nHow does the simple deep learning model stack up against those two?\nTo answer that question, we first need to obtain predictions on the test set.\n\n\ntest_wrmses <- data.frame()\n\ntest_loss <- tf$keras$metrics$Mean(name = 'test_loss')\n\ntest_step <- function(test_batch, batch_index) {\n  predictions <- model(test_batch[[1]])\n  l <- loss(test_batch[[2]], predictions)\n  \n  predictions <- predictions %>% as.array()\n  predictions[, , , 1] <- predictions[, , , 1] * level_sds[1] + level_means[1]\n  predictions[, , , 2] <- predictions[, , , 2] * level_sds[2] + level_means[2]\n  \n  wrmse <- weighted_rmse(predictions, test_all[batch_index:(batch_index + 31), , ,])\n  test_wrmses <<- test_wrmses %>% bind_rows(c(z = wrmse[1], temp = wrmse[2]))\n\n  test_loss(l)\n}\n\ntest_iterator <- as_iterator(test_ds)\n\nbatch_index <- 0\nwhile (TRUE) {\n  test_batch <- test_iterator %>% iter_next()\n  if (is.null(test_batch))\n    break\n  batch_index <- batch_index + 1\n  test_step(test_batch, as.integer(batch_index))\n}\n\ntest_loss$result() %>% as.numeric()\n\n\n3821.016\nThus, average loss on the test set parallels that seen on the validation set. As to latitudinally weighted RMSE, it turns out\nto be higher for the DL baseline than for the other two:\n\n\napply(test_wrmses, 2, mean) %>% round(2)\n\n\n      z    temp \n1521.47    7.70 \nConclusion\nAt first glance, seeing the DL baseline perform worse than the others might feel anticlimactic. But if you think about it,\nthere is no need to be disappointed.\nFor one, given the enormous complexity of the task, these heuristics are not as easy to outsmart. Take persistence: Depending\non lead time - how far into the future we’re forecasting - the wisest guess may actually be that everything will stay the\nsame. What would you guess the weather will look like in five minutes? — Same with weekly climatology: Looking back at how\nwarm it was, at a given location, that same week two years ago, does not in general sound like a bad strategy.\nSecond, the DL baseline shown is as basic as it can get, architecture- as well as parameter-wise. More sophisticated and\npowerful architectures have been developed that not just by far surpass the baselines, but can even compete with physical\nmodels (cf. especially Rasp and Thuerey (Rasp and Thuerey 2020) already mentioned above). Unfortunately, models like that need to be\ntrained on a lot of data.\nHowever, other weather-related applications (other than medium-range forecasting, that is) may be more in reach for\nindividuals interested in the topic. For those, we hope we have given a useful introduction. Thanks for reading!\n\n\n\nRasp, Stephan, Peter D. Dueben, Sebastian Scher, Jonathan A. Weyn, Soukayna Mouatadid, and Nils Thuerey. 2020. “WeatherBench: A benchmark dataset for data-driven weather forecasting.” arXiv e-Prints, February, arXiv:2002.00469. https://arxiv.org/abs/2002.00469.\n\n\nRasp, Stephan, and Nils Thuerey. 2020. “Purely Data-Driven Medium-Range Weather Forecasting Achieves Comparable Skill to Physical Models at Similar Resolution.” https://arxiv.org/abs/2008.08626.\n\n\nWeyn, Jonathan A., Dale R. Durran, and Rich Caruana. n.d. “Improving Data-Driven Global Weather Prediction Using Deep Convolutional Neural Networks on a Cubed Sphere.” Journal of Advances in Modeling Earth Systems n/a (n/a): e2020MS002109. https://doi.org/10.1029/2020MS002109.\n\n\nAn die Nachgeborenen, 1934-38. The atrocities referred to are those of Nazi Germany.↩︎\nFour, because in addition to three spatial dimensions, there is the time dimension.↩︎\nmostly↩︎\nPressure and altitude are related by the barometric equation. On\nweather maps, pressure is often used to represent the vertical dimension.↩︎\nWhereas we normally might think of atmospheric pressure as varying at a fixed height (for example, at sea level),\nmeteorologists like to display things the other way round, displaying variable heights at fixed constant-pressure\n(isobaric) surfaces. Still, intuitively these may be read in the same way: High pressure at a given location means that\nsome predefined pressure level is attained at higher altitude than in some low-pressure location. To be precise, these\nkinds of “inverted pressure maps” normally display geopotential height, measured in metres, not geopotential, the\nvariable we’re dealing with here. Geopotential (without the “height”) refers to gravitational potential energy per unit\nmass; it is obtained by multiplying by gravitational acceleration, and is measured in metres squared per second squared.\nThe measures are not a hundred percent equivalent, because gravitational acceleration varies with latitude and longitude\nas well as elevation.↩︎\nAs explained in the previous footnote, geopotential height is geopotential divided by standard gravitational\nacceleration, roughly, 9.8 metres per seconds squared.↩︎\nThese are the same filter and kernel sizes as employed in Rasp et al.'s simple CNN example on\ngithub.↩︎\n",
    "preview": "posts/2020-09-01-weather-prediction/images/thumb.png",
    "last_modified": "2024-11-21T15:51:34+00:00",
    "input_file": {},
    "preview_width": 600,
    "preview_height": 332
  },
  {
    "path": "posts/2020-08-24-training-imagenet-with-r/",
    "title": "Training ImageNet with R",
    "description": "This post explores how to train large datasets with TensorFlow and R. Specifically, we present how to download and repartition ImageNet, followed by training ImageNet across multiple GPUs in distributed environments using TensorFlow and Apache Spark.",
    "author": [
      {
        "name": "Javier Luraschi",
        "url": {}
      }
    ],
    "date": "2020-08-24",
    "categories": [
      "R",
      "TensorFlow/Keras",
      "Distributed Computing",
      "Data Management"
    ],
    "contents": "\nImageNet (Deng et al. 2009) is an image database organized according to the WordNet (Miller 1995) hierarchy which, historically, has been used in computer vision benchmarks and research. However, it was not until AlexNet (Krizhevsky, Sutskever, and Hinton 2012) demonstrated the efficiency of deep learning using convolutional neural networks on GPUs that the computer-vision discipline turned to deep learning to achieve state-of-the-art models that revolutionized their field. Given the importance of ImageNet and AlexNet, this post introduces tools and techniques to consider when training ImageNet and other large-scale datasets with R.\nNow, in order to process ImageNet, we will first have to divide and conquer, partitioning the dataset into several manageable subsets. Afterwards, we will train ImageNet using AlexNet across multiple GPUs and compute instances. Preprocessing ImageNet and distributed training are the two topics that this post will present and discuss, starting with preprocessing ImageNet.\nPreprocessing ImageNet\nWhen dealing with large datasets, even simple tasks like downloading or reading a dataset can be much harder than what you would expect. For instance, since ImageNet is roughly 300GB in size, you will need to make sure to have at least 600GB of free space to leave some room for download and decompression. But no worries, you can always borrow computers with huge disk drives from your favorite cloud provider. While you are at it, you should also request compute instances with multiple GPUs, Solid State Drives (SSDs), and a reasonable amount of CPUs and memory. If you want to use the exact configuration we used, take a look at the mlverse/imagenet repo, which contains a Docker image and configuration commands required to provision reasonable computing resources for this task. In summary, make sure you have access to sufficient compute resources.\nNow that we have resources capable of working with ImageNet, we need to find a place to download ImageNet from. The easiest way is to use a variation of ImageNet used in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which contains a subset of about 250GB of data and can be easily downloaded from many Kaggle competitions, like the ImageNet Object Localization Challenge.\nIf you’ve read some of our previous posts, you might be already thinking of using the pins package, which you can use to: cache, discover and share resources from many services, including Kaggle. You can learn more about data retrieval from Kaggle in the Using Kaggle Boards article; in the meantime, let’s assume you are already familiar with this package.\nAll we need to do now is register the Kaggle board, retrieve ImageNet as a pin, and decompress this file. Warning, the following code requires you to stare at a progress bar for, potentially, over an hour.\n\n\nlibrary(pins)\nboard_register(\"kaggle\", token = \"kaggle.json\")\n\npin_get(\"c/imagenet-object-localization-challenge\", board = \"kaggle\")[1] %>%\n  untar(exdir = \"/localssd/imagenet/\")\n\n\nIf we are going to be training this model over and over using multiple GPUs and even multiple compute instances, we want to make sure we don’t waste too much time downloading ImageNet every single time.\nThe first improvement to consider is getting a faster hard drive. In our case, we locally-mounted an array of SSDs into the /localssd path. We then used /localssd to extract ImageNet and configured R’s temp path and pins cache to use the SSDs as well. Consult your cloud provider’s documentation to configure SSDs, or take a look at mlverse/imagenet.\nNext, a well-known approach we can follow is to partition ImageNet into chunks that can be individually downloaded to perform distributed training later on.\nIn addition, it is also faster to download ImageNet from a nearby location, ideally from a URL stored within the same data center where our cloud instance is located. For this, we can also use pins to register a board with our cloud provider and then re-upload each partition. Since ImageNet is already partitioned by category, we can easily split ImageNet into multiple zip files and re-upload to our closest data center as follows. Make sure the storage bucket is created in the same region as your computing instances.\n\n\nboard_register(\"<board>\", name = \"imagenet\", bucket = \"r-imagenet\")\n\ntrain_path <- \"/localssd/imagenet/ILSVRC/Data/CLS-LOC/train/\"\nfor (path in dir(train_path, full.names = TRUE)) {\n  dir(path, full.names = TRUE) %>%\n    pin(name = basename(path), board = \"imagenet\", zip = TRUE)\n}\n\n\nWe can now retrieve a subset of ImageNet quite efficiently. If you are motivated to do so and have about one gigabyte to spare, feel free to follow along executing this code. Notice that ImageNet contains lots of JPEG images for each WordNet category.\n\n\nboard_register(\"https://storage.googleapis.com/r-imagenet/\", \"imagenet\")\n\ncategories <- pin_get(\"categories\", board = \"imagenet\")\npin_get(categories$id[1], board = \"imagenet\", extract = TRUE) %>%\n  tibble::as_tibble()\n\n\n# A tibble: 1,300 x 1\n   value                                                           \n   <chr>                                                           \n 1 /localssd/pins/storage/n01440764/n01440764_10026.JPEG\n 2 /localssd/pins/storage/n01440764/n01440764_10027.JPEG\n 3 /localssd/pins/storage/n01440764/n01440764_10029.JPEG\n 4 /localssd/pins/storage/n01440764/n01440764_10040.JPEG\n 5 /localssd/pins/storage/n01440764/n01440764_10042.JPEG\n 6 /localssd/pins/storage/n01440764/n01440764_10043.JPEG\n 7 /localssd/pins/storage/n01440764/n01440764_10048.JPEG\n 8 /localssd/pins/storage/n01440764/n01440764_10066.JPEG\n 9 /localssd/pins/storage/n01440764/n01440764_10074.JPEG\n10 /localssd/pins/storage/n01440764/n01440764_1009.JPEG \n# … with 1,290 more rows\nWhen doing distributed training over ImageNet, we can now let a single compute instance process a partition of ImageNet with ease. Say, 1/16 of ImageNet can be retrieved and extracted, in under a minute, using parallel downloads with the callr package:\n\n\ncategories <- pin_get(\"categories\", board = \"imagenet\")\ncategories <- categories$id[1:(length(categories$id) / 16)]\n\nprocs <- lapply(categories, function(cat)\n  callr::r_bg(function(cat) {\n    library(pins)\n    board_register(\"https://storage.googleapis.com/r-imagenet/\", \"imagenet\")\n    \n    pin_get(cat, board = \"imagenet\", extract = TRUE)\n  }, args = list(cat))\n)\n  \nwhile (any(sapply(procs, function(p) p$is_alive()))) Sys.sleep(1)\n\n\nWe can wrap this up partition in a list containing a map of images and categories, which we will later use in our AlexNet model through tfdatasets.\n\n\ndata <- list(\n    image = unlist(lapply(categories, function(cat) {\n        pin_get(cat, board = \"imagenet\", download = FALSE)\n    })),\n    category = unlist(lapply(categories, function(cat) {\n        rep(cat, length(pin_get(cat, board = \"imagenet\", download = FALSE)))\n    })),\n    categories = categories\n)\n\n\nGreat! We are halfway there training ImageNet. The next section will focus on introducing distributed training using multiple GPUs.\nDistributed Training\nNow that we have broken down ImageNet into manageable parts, we can forget for a second about the size of ImageNet and focus on training a deep learning model for this dataset. However, any model we choose is likely to require a GPU, even for a 1/16 subset of ImageNet. So make sure your GPUs are properly configured by running is_gpu_available(). If you need help getting a GPU configured, the Using GPUs with TensorFlow and Docker video can help you get up to speed.\n\n\nlibrary(tensorflow)\ntf$test$is_gpu_available()\n\n\n[1] TRUE\nWe can now decide which deep learning model would best be suited for ImageNet classification tasks. Instead, for this post, we will go back in time to the glory days of AlexNet and use the r-tensorflow/alexnet repo instead. This repo contains a port of AlexNet to R, but please notice that this port has not been tested and is not ready for any real use cases. In fact, we would appreciate PRs to improve it if someone feels inclined to do so. Regardless, the focus of this post is on workflows and tools, not about achieving state-of-the-art image classification scores. So by all means, feel free to use more appropriate models.\nOnce we’ve chosen a model, we will want to me make sure that it properly trains on a subset of ImageNet:\n\n\nremotes::install_github(\"r-tensorflow/alexnet\")\nalexnet::alexnet_train(data = data)\n\n\nEpoch 1/2\n 103/2269 [>...............] - ETA: 5:52 - loss: 72306.4531 - accuracy: 0.9748\nSo far so good! However, this post is about enabling large-scale training across multiple GPUs, so we want to make sure we are using as many as we can. Unfortunately, running nvidia-smi will show that only one GPU currently being used:\n\nnvidia-smi\n\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 418.152.00   Driver Version: 418.152.00   CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   48C    P0    89W / 149W |  10935MiB / 11441MiB |     28%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           Off  | 00000000:00:06.0 Off |                    0 |\n| N/A   74C    P0    74W / 149W |     71MiB / 11441MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\nIn order to train across multiple GPUs, we need to define a distributed-processing strategy. If this is a new concept, it might be a good time to take a look at the Distributed Training with Keras tutorial and the distributed training with TensorFlow docs. Or, if you allow us to oversimplify the process, all you have to do is define and compile your model under the right scope. A step-by-step explanation is available in the Distributed Deep Learning with TensorFlow and R video. In this case, the alexnet model already supports a strategy parameter, so all we have to do is pass it along.\n\n\nlibrary(tensorflow)\nstrategy <- tf$distribute$MirroredStrategy(\n  cross_device_ops = tf$distribute$ReductionToOneDevice())\n\nalexnet::alexnet_train(data = data, strategy = strategy, parallel = 6)\n\n\nNotice also parallel = 6 which configures tfdatasets to make use of multiple CPUs when loading data into our GPUs, see Parallel Mapping for details.\nWe can now re-run nvidia-smi to validate all our GPUs are being used:\n\nnvidia-smi\n\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 418.152.00   Driver Version: 418.152.00   CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   49C    P0    94W / 149W |  10936MiB / 11441MiB |     53%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           Off  | 00000000:00:06.0 Off |                    0 |\n| N/A   76C    P0   114W / 149W |  10936MiB / 11441MiB |     26%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\nThe MirroredStrategy can help us scale up to about 8 GPUs per compute instance; however, we are likely to need 16 instances with 8 GPUs each to train ImageNet in a reasonable time (see Jeremy Howard’s post on Training Imagenet in 18 Minutes). So where do we go from here?\nWelcome to MultiWorkerMirroredStrategy: This strategy can use not only multiple GPUs, but also multiple GPUs across multiple computers. To configure them, all we have to do is define a TF_CONFIG environment variable with the right addresses and run the exact same code in each compute instance.\n\n\nlibrary(tensorflow)\n\npartition <- 0\nSys.setenv(TF_CONFIG = jsonlite::toJSON(list(\n    cluster = list(\n        worker = c(\"10.100.10.100:10090\", \"10.100.10.101:10090\")\n    ),\n    task = list(type = 'worker', index = partition)\n), auto_unbox = TRUE))\n\nstrategy <- tf$distribute$MultiWorkerMirroredStrategy(\n  cross_device_ops = tf$distribute$ReductionToOneDevice())\n\nalexnet::imagenet_partition(partition = partition) %>%\n  alexnet::alexnet_train(strategy = strategy, parallel = 6)\n\n\nPlease note that partition must change for each compute instance to uniquely identify it, and that the IP addresses also need to be adjusted. In addition, data should point to a different partition of ImageNet, which we can retrieve with pins; although, for convenience, alexnet contains similar code under alexnet::imagenet_partition(). Other than that, the code that you need to run in each compute instance is exactly the same.\nHowever, if we were to use 16 machines with 8 GPUs each to train ImageNet, it would be quite time-consuming and error-prone to manually run code in each R session. So instead, we should think of making use of cluster-computing frameworks, like Apache Spark with barrier execution. If you are new to Spark, there are many resources available at sparklyr.ai. To learn just about running Spark and TensorFlow together, watch our Deep Learning with Spark, TensorFlow and R video.\nPutting it all together, training ImageNet in R with TensorFlow and Spark looks as follows:\n\n\nlibrary(sparklyr)\nsc <- spark_connect(\"yarn|mesos|etc\", config = list(\"sparklyr.shell.num-executors\" = 16))\n\nsdf_len(sc, 16, repartition = 16) %>%\n  spark_apply(function(df, barrier) {\n      library(tensorflow)\n\n      Sys.setenv(TF_CONFIG = jsonlite::toJSON(list(\n        cluster = list(\n          worker = paste(\n            gsub(\":[0-9]+$\", \"\", barrier$address),\n            8000 + seq_along(barrier$address), sep = \":\")),\n        task = list(type = 'worker', index = barrier$partition)\n      ), auto_unbox = TRUE))\n      \n      if (is.null(tf_version())) install_tensorflow()\n      \n      strategy <- tf$distribute$MultiWorkerMirroredStrategy()\n    \n      result <- alexnet::imagenet_partition(partition = barrier$partition) %>%\n        alexnet::alexnet_train(strategy = strategy, epochs = 10, parallel = 6)\n      \n      result$metrics$accuracy\n  }, barrier = TRUE, columns = c(accuracy = \"numeric\"))\n\n\nWe hope this post gave you a reasonable overview of what training large-datasets in R looks like – thanks for reading along!\n\n\n\nDeng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. “Imagenet: A Large-Scale Hierarchical Image Database.” In 2009 IEEE Conference on Computer Vision and Pattern Recognition, 248–55. Ieee.\n\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. “Imagenet Classification with Deep Convolutional Neural Networks.” In Advances in Neural Information Processing Systems, 1097–1105.\n\n\nMiller, George A. 1995. “WordNet: A Lexical Database for English.” Communications of the ACM 38 (11): 39–41.\n\n\n\n\n",
    "preview": "posts/2020-08-24-training-imagenet-with-r/images/fishing-net.jpg",
    "last_modified": "2024-11-21T15:51:17+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-08-18-deepfake/",
    "title": "Deepfake detection challenge from R",
    "description": "A couple of months ago, Amazon, Facebook, Microsoft, and other contributors initiated a challenge consisting of telling apart real and AI-generated (\"fake\") videos. We show how to approach this challenge from R.",
    "author": [
      {
        "name": "Turgut Abdullayev",
        "url": "https://github.com/henry090"
      }
    ],
    "date": "2020-08-18",
    "categories": [
      "Image Recognition & Image Processing"
    ],
    "contents": "\n\nContents\nIntroduction\nData exploration\nFrame extraction\nFace detection\nFace extraction\nDeep learning model\nConclusion\n\nIntroduction\nWorking with video datasets, particularly with respect to detection of AI-based fake objects, is very challenging due to proper frame selection and face detection. To approach this challenge from R, one can make use of capabilities offered by OpenCV, magick, and keras.\nOur approach consists of the following consequent steps:\nread all the videos\ncapture and extract images from the videos\ndetect faces from the extracted images\ncrop the faces\nbuild an image classification model with Keras\nLet’s quickly introduce the non-deep-learning libraries we’re using. OpenCV is a computer vision library that includes:\nFacial recognition technology\nMotion tracking\nAugmented reality\nand more.\nOn the other hand, magick is the open-source image-processing library that will help to read and extract useful features from video datasets:\nRead video files\nExtract images per second from the video\nCrop the faces from the images\nBefore we go into a detailed explanation, readers should know that there is no need to copy-paste code chunks. Because at the end of the post one can find a link to Google Colab with GPU acceleration. This kernel allows everyone to run and reproduce the same results.\nData exploration\nThe dataset that we are going to analyze is provided by AWS, Facebook, Microsoft, the Partnership on AI’s Media Integrity Steering Committee, and various academics.\nIt contains both real and AI-generated fake videos. The total size is over 470 GB. However, the sample 4 GB dataset is separately available.\nFrame extraction\nThe videos in the folders are in the format of mp4 and have various lengths. Our task is to determine the number of images to capture per second of a video. We usually took 1-3 fps for every video.\n\nNote: Set fps to NULL if you want to extract all frames.\n\n\n\nvideo = magick::image_read_video(\"aagfhgtpmv.mp4\",fps = 2)\nvid_1 = video[[1]]\nvid_1 = magick::image_read(vid_1) %>% image_resize('1000x1000')\n\n\n\n\n\n\nFigure 1: Deepfake detection challenge\n\n\n\n\nWe saw just the first frame. What about the rest of them?\n\n\n\nFigure 2: Deepfake detection challenge\n\n\n\nLooking at the gif one can observe that some fakes are very easy to differentiate, but a small fraction looks pretty realistic. This is another challenge during data preparation.\nFace detection\nAt first, face locations need to be determined via bounding boxes, using OpenCV. Then, magick is used to automatically extract them from all images.\n\n\n# get face location and calculate bounding box\nlibrary(opencv)\nunconf <- ocv_read('frame_1.jpg')\nfaces <- ocv_face(unconf)\nfacemask <- ocv_facemask(unconf)\ndf = attr(facemask, 'faces')\nrectX = (df$x - df$radius) \nrectY = (df$y - df$radius)\nx = (df$x + df$radius) \ny = (df$y + df$radius)\n\n# draw with red dashed line the box\nimh  = image_draw(image_read('frame_1.jpg'))\nrect(rectX, rectY, x, y, border = \"red\", \n     lty = \"dashed\", lwd = 2)\ndev.off()\n\n\n\n\n\n\nFigure 3: Deepfake detection challenge\n\n\n\n\nFace extraction\nIf face locations are found, then it is very easy to extract them all.\n\n\nedited = image_crop(imh, \"49x49+66+34\")\nedited = image_crop(imh, paste(x-rectX+1,'x',x-rectX+1,'+',rectX, '+',rectY,sep = ''))\nedited\n\n\n\n\n\n\nFigure 4: Deepfake detection challenge\n\n\n\n\nDeep learning model\nAfter dataset preparation, it is time to build a deep learning model with Keras. We can quickly place all the images into folders and, using image generators, feed faces to a pre-trained Keras model.\n\n\ntrain_dir = 'fakes_reals'\nwidth = 150L\nheight = 150L\nepochs = 10\n\ntrain_datagen = image_data_generator(\n  rescale = 1/255,\n  rotation_range = 40,\n  width_shift_range = 0.2,\n  height_shift_range = 0.2,\n  shear_range = 0.2,\n  zoom_range = 0.2,\n  horizontal_flip = TRUE,\n  fill_mode = \"nearest\",\n  validation_split=0.2\n)\n\n\ntrain_generator <- flow_images_from_directory(\n  train_dir,                  \n  train_datagen,             \n  target_size = c(width,height), \n  batch_size = 10,\n  class_mode = \"binary\"\n)\n\n# Build the model ---------------------------------------------------------\n\nconv_base <- application_vgg16(\n  weights = \"imagenet\",\n  include_top = FALSE,\n  input_shape = c(width, height, 3)\n)\n\nmodel <- keras_model_sequential() %>% \n  conv_base %>% \n  layer_flatten() %>% \n  layer_dense(units = 256, activation = \"relu\") %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %>% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = optimizer_rmsprop(lr = 2e-5),\n  metrics = c(\"accuracy\")\n)\n\nhistory <- model %>% fit_generator(\n  train_generator,\n  steps_per_epoch = ceiling(train_generator$samples/train_generator$batch_size),\n  epochs = 10\n)\n\n\n\nReproduce in a Notebook\n\nConclusion\nThis post shows how to do video classification from R. The steps were:\nRead videos and extract images from the dataset\nApply OpenCV to detect faces\nExtract faces via bounding boxes\nBuild a deep learning model\nHowever, readers should know that the implementation of the following steps may drastically improve model performance:\nextract all of the frames from the video files\nload different pre-trained weights, or use different pre-trained models\nuse another technology to detect faces – e.g., “MTCNN face detector”\nFeel free to try these options on the Deepfake detection challenge and share your results in the comments section!\nThanks for reading!\n\n\n\n",
    "preview": "posts/2020-08-18-deepfake/files/frame_2.jpg",
    "last_modified": "2024-11-21T15:48:39+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-07-31-fnn-vae-for-noisy-timeseries/",
    "title": "FNN-VAE for noisy time series forecasting",
    "description": "In the last part of this mini-series on forecasting with false nearest neighbors (FNN) loss, we replace the LSTM autoencoder from the previous post by a convolutional VAE, resulting in equivalent prediction performance but significantly lower training time. In addition, we find that FNN regularization is of great help when an underlying deterministic process is obscured by substantial noise.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-07-31",
    "categories": [
      "R",
      "TensorFlow/Keras",
      "Time Series",
      "Generative Models"
    ],
    "contents": "\nThis post did not end up quite the way I’d imagined. A quick follow-up on the recent Time series prediction with\nFNN-LSTM, it was supposed to demonstrate how noisy time series (so common in\npractice) could profit from a change in architecture: Instead of FNN-LSTM, an LSTM autoencoder regularized by false nearest\nneighbors (FNN) loss, use FNN-VAE, a variational autoencoder constrained by the same. However, FNN-VAE did not seem to handle\nnoise better than FNN-LSTM. No plot, no post, then?\nOn the other hand – this is not a scientific study, with hypothesis and experimental setup all preregistered; all that really\nmatters is if there’s something useful to report. And it looks like there is.\nFirstly, FNN-VAE, while on par performance-wise with FNN-LSTM, is far superior in that other meaning of “performance”:\nTraining goes a lot faster for FNN-VAE.\nSecondly, while we don’t see much difference between FNN-LSTM and FNN-VAE, we do see a clear impact of using FNN loss. Adding in FNN loss strongly reduces mean squared error with respect to the underlying (denoised) series – especially in the case of VAE, but for LSTM as well. This is of particular interest with VAE, as it comes with a regularizer\nout-of-the-box – namely, Kullback-Leibler (KL) divergence.\nOf course, we don’t claim that similar results will always be obtained on other noisy series; nor did we tune any of\nthe models “to death.” For what could be the intent of such a post but to show our readers interesting (and promising) ideas\nto pursue in their own experimentation?\nThe context\nThis post is the third in a mini-series.\nIn Deep attractors: Where deep learning meets chaos, we\nexplained, with a substantial detour into chaos theory, the idea of FNN loss, introduced in (Gilpin 2020). Please consult\nthat first post for theoretical background and intuitions behind the technique.\nThe subsequent post, Time series prediction with FNN-LSTM, showed\nhow to use an LSTM autoencoder, constrained by FNN loss, for forecasting (as opposed to reconstructing an attractor). The results were stunning: In multi-step prediction (12-120 steps, with that number varying by\ndataset), the short-term forecasts were drastically improved by adding in FNN regularization. See that second post for\nexperimental setup and results on four very different, non-synthetic datasets.\nToday, we show how to replace the LSTM autoencoder by a – convolutional – VAE. In light of the experimentation results,\nalready hinted at above, it is completely plausible that the “variational” part is not even so important here – that a\nconvolutional autoencoder with just MSE loss would have performed just as well on those data. In fact, to find out, it’s\nenough to remove the call to reparameterize() and multiply the KL component of the loss by 0. (We leave this to the\ninterested reader, to keep the post at reasonable length.)\nOne last piece of context, in case you haven’t read the two previous posts and would like to jump in here directly. We’re\ndoing time series forecasting; so why this talk of autoencoders? Shouldn’t we just be comparing an LSTM (or some other type of\nRNN, for that matter) to a convnet? In fact, the necessity of a latent representation is due to the very idea of FNN: The\nlatent code is supposed to reflect the true attractor of a dynamical system. That is, if the attractor of the underlying\nsystem is roughly two-dimensional, we hope to find that just two of the latent variables have considerable variance. (This\nreasoning is explained in a lot of detail in the previous posts.)\nFNN-VAE\nSo, let’s start with the code for our new model.\nThe encoder takes the time series, of format batch_size x num_timesteps x num_features just like in the LSTM case, and\nproduces a flat, 10-dimensional output: the latent code, which FNN loss is computed on.\n\n\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(tfdatasets)\nlibrary(tfautograph)\nlibrary(reticulate)\nlibrary(purrr)\n\nvae_encoder_model <- function(n_timesteps,\n                               n_features,\n                               n_latent,\n                               name = NULL) {\n  keras_model_custom(name = name, function(self) {\n    self$conv1 <- layer_conv_1d(kernel_size = 3,\n                                filters = 16,\n                                strides = 2)\n    self$act1 <- layer_activation_leaky_relu()\n    self$batchnorm1 <- layer_batch_normalization()\n    self$conv2 <- layer_conv_1d(kernel_size = 7,\n                                filters = 32,\n                                strides = 2)\n    self$act2 <- layer_activation_leaky_relu()\n    self$batchnorm2 <- layer_batch_normalization()\n    self$conv3 <- layer_conv_1d(kernel_size = 9,\n                                filters = 64,\n                                strides = 2)\n    self$act3 <- layer_activation_leaky_relu()\n    self$batchnorm3 <- layer_batch_normalization()\n    self$conv4 <- layer_conv_1d(\n      kernel_size = 9,\n      filters = n_latent,\n      strides = 2,\n      activation = \"linear\" \n    )\n    self$batchnorm4 <- layer_batch_normalization()\n    self$flat <- layer_flatten()\n    \n    function (x, mask = NULL) {\n      x %>%\n        self$conv1() %>%\n        self$act1() %>%\n        self$batchnorm1() %>%\n        self$conv2() %>%\n        self$act2() %>%\n        self$batchnorm2() %>%\n        self$conv3() %>%\n        self$act3() %>%\n        self$batchnorm3() %>%\n        self$conv4() %>%\n        self$batchnorm4() %>%\n        self$flat()\n    }\n  })\n}\n\n\nThe decoder starts from this – flat – representation and decompresses it into a time sequence. In both encoder and decoder\n(de-)conv layers, parameters are chosen to handle a sequence length (num_timesteps) of 120, which is what we’ll use for\nprediction below.\n\n\nvae_decoder_model <- function(n_timesteps,\n                               n_features,\n                               n_latent,\n                               name = NULL) {\n  keras_model_custom(name = name, function(self) {\n    self$reshape <- layer_reshape(target_shape = c(1, n_latent))\n    self$conv1 <- layer_conv_1d_transpose(kernel_size = 15,\n                                          filters = 64,\n                                          strides = 3)\n    self$act1 <- layer_activation_leaky_relu()\n    self$batchnorm1 <- layer_batch_normalization()\n    self$conv2 <- layer_conv_1d_transpose(kernel_size = 11,\n                                          filters = 32,\n                                          strides = 3)\n    self$act2 <- layer_activation_leaky_relu()\n    self$batchnorm2 <- layer_batch_normalization()\n    self$conv3 <- layer_conv_1d_transpose(\n      kernel_size = 9,\n      filters = 16,\n      strides = 2,\n      output_padding = 1\n    )\n    self$act3 <- layer_activation_leaky_relu()\n    self$batchnorm3 <- layer_batch_normalization()\n    self$conv4 <- layer_conv_1d_transpose(\n      kernel_size = 7,\n      filters = 1,\n      strides = 1,\n      activation = \"linear\"\n    )\n    self$batchnorm4 <- layer_batch_normalization()\n    \n    function (x, mask = NULL) {\n      x %>%\n        self$reshape() %>%\n        self$conv1() %>%\n        self$act1() %>%\n        self$batchnorm1() %>%\n        self$conv2() %>%\n        self$act2() %>%\n        self$batchnorm2() %>%\n        self$conv3() %>%\n        self$act3() %>%\n        self$batchnorm3() %>%\n        self$conv4() %>%\n        self$batchnorm4()\n    }\n  })\n}\n\n\nNote that even though we called these constructors vae_encoder_model() and vae_decoder_model(), there is nothing\nvariational to these models per se; they are really just an encoder and a decoder, respectively. Metamorphosis into a VAE will\nhappen in the training procedure; in fact, the only two things that will make this a VAE are going to be the\nreparameterization of the latent layer and the added-in KL loss.\nSpeaking of training, these are the routines we’ll call. The function to compute FNN loss, loss_false_nn(), can be found in\nboth of the abovementioned predecessor posts; we kindly ask the reader to copy it from one of these places.\n\n\n# to reparameterize encoder output before calling decoder\nreparameterize <- function(mean, logvar = 0) {\n  eps <- k_random_normal(shape = n_latent)\n  eps * k_exp(logvar * 0.5) + mean\n}\n\n# loss has 3 components: NLL, KL, and FNN\n# otherwise, this is just normal TF2-style training \ntrain_step_vae <- function(batch) {\n  with (tf$GradientTape(persistent = TRUE) %as% tape, {\n    code <- encoder(batch[[1]])\n    z <- reparameterize(code)\n    prediction <- decoder(z)\n    \n    l_mse <- mse_loss(batch[[2]], prediction)\n    # see loss_false_nn in 2 previous posts\n    l_fnn <- loss_false_nn(code)\n    # KL divergence to a standard normal\n    l_kl <- -0.5 * k_mean(1 - k_square(z))\n    # overall loss is a weighted sum of all 3 components\n    loss <- l_mse + fnn_weight * l_fnn + kl_weight * l_kl\n  })\n  \n  encoder_gradients <-\n    tape$gradient(loss, encoder$trainable_variables)\n  decoder_gradients <-\n    tape$gradient(loss, decoder$trainable_variables)\n  \n  optimizer$apply_gradients(purrr::transpose(list(\n    encoder_gradients, encoder$trainable_variables\n  )))\n  optimizer$apply_gradients(purrr::transpose(list(\n    decoder_gradients, decoder$trainable_variables\n  )))\n  \n  train_loss(loss)\n  train_mse(l_mse)\n  train_fnn(l_fnn)\n  train_kl(l_kl)\n}\n\n# wrap it all in autograph\ntraining_loop_vae <- tf_function(autograph(function(ds_train) {\n  \n  for (batch in ds_train) {\n    train_step_vae(batch) \n  }\n  \n  tf$print(\"Loss: \", train_loss$result())\n  tf$print(\"MSE: \", train_mse$result())\n  tf$print(\"FNN loss: \", train_fnn$result())\n  tf$print(\"KL loss: \", train_kl$result())\n  \n  train_loss$reset_states()\n  train_mse$reset_states()\n  train_fnn$reset_states()\n  train_kl$reset_states()\n  \n}))\n\n\nTo finish up the model section, here is the actual training code. This is nearly identical to what we did for FNN-LSTM before.\n\n\nn_latent <- 10L\nn_features <- 1\n\nencoder <- vae_encoder_model(n_timesteps,\n                         n_features,\n                         n_latent)\n\ndecoder <- vae_decoder_model(n_timesteps,\n                         n_features,\n                         n_latent)\nmse_loss <-\n  tf$keras$losses$MeanSquaredError(reduction = tf$keras$losses$Reduction$SUM)\n\ntrain_loss <- tf$keras$metrics$Mean(name = 'train_loss')\ntrain_fnn <- tf$keras$metrics$Mean(name = 'train_fnn')\ntrain_mse <-  tf$keras$metrics$Mean(name = 'train_mse')\ntrain_kl <-  tf$keras$metrics$Mean(name = 'train_kl')\n\nfnn_multiplier <- 1 # default value used in nearly all cases (see text)\nfnn_weight <- fnn_multiplier * nrow(x_train)/batch_size\n\nkl_weight <- 1\n\noptimizer <- optimizer_adam(lr = 1e-3)\n\nfor (epoch in 1:100) {\n  cat(\"Epoch: \", epoch, \" -----------\\n\")\n  training_loop_vae(ds_train)\n \n  test_batch <- as_iterator(ds_test) %>% iter_next()\n  encoded <- encoder(test_batch[[1]][1:1000])\n  test_var <- tf$math$reduce_variance(encoded, axis = 0L)\n  print(test_var %>% as.numeric() %>% round(5))\n}\n\n\nExperimental setup and data\nThe idea was to add white noise to a deterministic series. This time, the Roessler\nsystem was chosen, mainly for the prettiness of its attractor, apparent\neven in its two-dimensional projections:\n\n\n\nFigure 1: Roessler attractor, two-dimensional projections.\n\n\n\nLike we did for the Lorenz system in the first part of this series, we use deSolve to generate data from the Roessler\nequations.\n\n\nlibrary(deSolve)\n\nparameters <- c(a = .2,\n                b = .2,\n                c = 5.7)\n\ninitial_state <-\n  c(x = 1,\n    y = 1,\n    z = 1.05)\n\nroessler <- function(t, state, parameters) {\n  with(as.list(c(state, parameters)), {\n    dx <- -y - z\n    dy <- x + a * y\n    dz = b + z * (x - c)\n    \n    list(c(dx, dy, dz))\n  })\n}\n\ntimes <- seq(0, 2500, length.out = 20000)\n\nroessler_ts <-\n  ode(\n    y = initial_state,\n    times = times,\n    func = roessler,\n    parms = parameters,\n    method = \"lsoda\"\n  ) %>% unclass() %>% as_tibble()\n\nn <- 10000\nroessler <- roessler_ts$x[1:n]\n\nroessler <- scale(roessler)\n\n\nThen, noise is added, to the desired degree, by drawing from a normal distribution, centered at zero, with standard deviations\nvarying between 1 and 2.5.\n\n\n# add noise\nnoise <- 1 # also used 1.5, 2, 2.5\nroessler <- roessler + rnorm(10000, mean = 0, sd = noise)\n\n\nHere you can compare effects of not adding any noise (left), standard deviation-1 (middle), and standard deviation-2.5 Gaussian noise:\n\n\n\nFigure 2: Roessler series with added noise. Top: none. Middle: SD = 1. Bottom: SD = 2.5.\n\n\n\nOtherwise, preprocessing proceeds as in the previous posts. In the upcoming results section, we’ll compare forecasts not just\nto the “real,” after noise addition, test split of the data, but also to the underlying Roessler system – that is, the thing\nwe’re really interested in. (Just that in the real world, we can’t do that check.) This second test set is prepared for\nforecasting just like the other one; to avoid duplication we don’t reproduce the code.\n\n\nn_timesteps <- 120\nbatch_size <- 32\n\ngen_timesteps <- function(x, n_timesteps) {\n  do.call(rbind,\n          purrr::map(seq_along(x),\n                     function(i) {\n                       start <- i\n                       end <- i + n_timesteps - 1\n                       out <- x[start:end]\n                       out\n                     })\n  ) %>%\n    na.omit()\n}\n\ntrain <- gen_timesteps(roessler[1:(n/2)], 2 * n_timesteps)\ntest <- gen_timesteps(roessler[(n/2):n], 2 * n_timesteps) \n\ndim(train) <- c(dim(train), 1)\ndim(test) <- c(dim(test), 1)\n\nx_train <- train[ , 1:n_timesteps, , drop = FALSE]\ny_train <- train[ , (n_timesteps + 1):(2*n_timesteps), , drop = FALSE]\n\nds_train <- tensor_slices_dataset(list(x_train, y_train)) %>%\n  dataset_shuffle(nrow(x_train)) %>%\n  dataset_batch(batch_size)\n\nx_test <- test[ , 1:n_timesteps, , drop = FALSE]\ny_test <- test[ , (n_timesteps + 1):(2*n_timesteps), , drop = FALSE]\n\nds_test <- tensor_slices_dataset(list(x_test, y_test)) %>%\n  dataset_batch(nrow(x_test))\n\n\nResults\nThe LSTM used for comparison with the VAE described above is identical to the architecture employed in the previous post.\nWhile with the VAE, an fnn_multiplier of 1 yielded sufficient regularization for all noise levels, some more experimentation\nwas needed for the LSTM: At noise levels 2 and 2.5, that multiplier was set to 5.\nAs a result, in all cases, there was one latent variable with high variance and a second one of minor importance. For all\nothers, variance was close to 0.\nIn all cases here means: In all cases where FNN regularization was used. As already hinted at in the introduction, the main\nregularizing factor providing robustness to noise here seems to be FNN loss, not KL divergence. So for all noise levels,\nbesides FNN-regularized LSTM and VAE models we also tested their non-constrained counterparts.\nLow noise\nSeeing how all models did superbly on the original deterministic series, a noise level of 1 can almost be treated as\na baseline. Here you see sixteen 120-timestep predictions from both regularized models, FNN-VAE (dark blue), and FNN-LSTM\n(orange). The noisy test data, both input (x, 120 steps) and output (y, 120 steps) are displayed in (blue-ish) grey. In\ngreen, also spanning the whole sequence, we have the original Roessler data, the way they would look had no noise been added.\n\n\n\nFigure 3: Roessler series with added Gaussian noise of standard deviation 1. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from FNN-LSTM. Dark blue: Predictions from FNN-VAE.\n\n\n\nDespite the noise, forecasts from both models look excellent. Is this due to the FNN regularizer?\nLooking at forecasts from their unregularized counterparts, we have to admit these do not look any worse. (For better\ncomparability, the sixteen sequences to forecast were initiallly picked at random, but used to test all models and\nconditions.)\n\n\n\nFigure 4: Roessler series with added Gaussian noise of standard deviation 1. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from unregularized LSTM. Dark blue: Predictions from unregularized VAE.\n\n\n\nWhat happens when we start to add noise?\nSubstantial noise\nBetween noise levels 1.5 and 2, something changed, or became noticeable from visual inspection. Let’s jump directly to the\nhighest-used level though: 2.5.\nHere first are predictions obtained from the unregularized models.\n\n\n\nFigure 5: Roessler series with added Gaussian noise of standard deviation 2.5. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from unregularized LSTM. Dark blue: Predictions from unregularized VAE.\n\n\n\nBoth LSTM and VAE get “distracted” a bit too much by the noise, the latter to an even higher degree. This leads to cases\nwhere predictions strongly “overshoot” the underlying non-noisy rhythm. This is not surprising, of course: They were trained\non the noisy version; predict fluctuations is what they learned.\nDo we see the same with the FNN models?\n\n\n\nFigure 6: Roessler series with added Gaussian noise of standard deviation 2.5. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from FNN-LSTM. Dark blue: Predictions from FNN-VAE.\n\n\n\nInterestingly, we see a much better fit to the underlying Roessler system now! Especially the VAE model, FNN-VAE, surprises\nwith a whole new smoothness of predictions; but FNN-LSTM turns up much smoother forecasts as well.\n“Smooth, fitting the system…” – by now you may be wondering, when are we going to come up with more quantitative\nassertions? If quantitative implies “mean squared error” (MSE), and if MSE is taken to be some divergence between forecasts\nand the true target from the test set, the answer is that this MSE doesn’t differ much between any of the four architectures.\nPut differently, it is mostly a function of noise level.\nHowever, we could argue that what we’re really interested in is how well a model forecasts the underlying process. And there,\nwe see differences.\nIn the following plot, we contrast MSEs obtained for the four model types (grey: VAE; orange: LSTM; dark blue: FNN-VAE; green:\nFNN-LSTM). The rows reflect noise levels (1, 1.5, 2, 2.5); the columns represent MSE in relation to the noisy(“real”) target\n(left) on the one hand, and in relation to the underlying system on the other (right). For better visibility of the effect,\nMSEs have been normalized as fractions of the maximum MSE in a category.\nSo, if we want to predict signal plus noise (left), it is not extremely critical whether we use FNN or not. But if we want to\npredict the signal only (right), with increasing noise in the data FNN loss becomes increasingly effective. This effect is far\nstronger for VAE vs. FNN-VAE than for LSTM vs. FNN-LSTM: The distance between the grey line (VAE) and the dark blue one\n(FNN-VAE) becomes larger and larger as we add more noise.\n\n\n\nFigure 7: Normalized MSEs obtained for the four model types (grey: VAE; orange: LSTM; dark blue: FNN-VAE; green: FNN-LSTM). Rows are noise levels (1, 1.5, 2, 2.5); columns are MSE as related to the real target (left) and the underlying system (right).\n\n\n\nSumming up\nOur experiments show that when noise is likely to obscure measurements from an underlying deterministic system, FNN\nregularization can strongly improve forecasts. This is the case especially for convolutional VAEs, and probably convolutional\nautoencoders in general. And if an FNN-constrained VAE performs as well, for time series prediction, as an LSTM, there is a\nstrong incentive to use the convolutional model: It trains significantly faster.\nWith that, we conclude our mini-series on FNN-regularized models. As always, we’d love to hear from you if you were able to\nmake use of this in your own work!\nThanks for reading!\n\n\n\nGilpin, William. 2020. “Deep Reconstruction of Strange Attractors from Time Series.” https://arxiv.org/abs/2002.05909.\n\n\n\n\n",
    "preview": "posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/kb.jpg",
    "last_modified": "2024-11-21T15:49:47+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-07-30-state-of-the-art-nlp-models-from-r/",
    "title": "State-of-the-art NLP models from R",
    "description": "Nowadays, Microsoft, Google, Facebook, and OpenAI are sharing lots of state-of-the-art models in the field of Natural Language Processing. However, fewer materials exist how to use these models from R. In this post, we will show how R users can access and benefit from these models as well.",
    "author": [
      {
        "name": "Turgut Abdullayev",
        "url": "https://github.com/henry090"
      }
    ],
    "date": "2020-07-30",
    "categories": [
      "Natural Language Processing"
    ],
    "contents": "\n\nContents\nIntroduction\nPrerequisites\nTemplate\nData preparation\nData input for Keras\nConclusion\n\nIntroduction\nThe Transformers repository from “Hugging Face” contains a lot of ready to use, state-of-the-art models, which are straightforward to download and fine-tune with Tensorflow & Keras.\nFor this purpose the users usually need to get:\nThe model itself (e.g. Bert, Albert, RoBerta, GPT-2 and etc.)\nThe tokenizer object\nThe weights of the model\nIn this post, we will work on a classic binary classification task and train our dataset on 3 models:\nGPT-2 from Open AI\nRoBERTa from Facebook\nElectra from Google Research/Stanford University\nHowever, readers should know that one can work with transformers on a variety of down-stream tasks, such as:\nfeature extraction\nsentiment analysis\ntext classification\nquestion answering\nsummarization\ntranslation and many more.\nPrerequisites\nOur first job is to install the transformers package via reticulate.\n\n\nreticulate::py_install('transformers', pip = TRUE)\n\n\nThen, as usual, load standard ‘Keras’, ‘TensorFlow’ >= 2.0 and some classic libraries from R.\n\n\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(dplyr)\nlibrary(tfdatasets)\n\ntransformer = reticulate::import('transformers')\n\n\nNote that if running TensorFlow on GPU one could specify the following parameters in order to avoid memory issues.\n\n\nphysical_devices = tf$config$list_physical_devices('GPU')\ntf$config$experimental$set_memory_growth(physical_devices[[1]],TRUE)\n\ntf$keras$backend$set_floatx('float32')\n\n\nTemplate\nWe already mentioned that to train a data on the specific model, users should download the model, its tokenizer object and weights. For example, to get a RoBERTa model one has to do the following:\n\n\n# get Tokenizer\ntransformer$RobertaTokenizer$from_pretrained('roberta-base', do_lower_case=TRUE)\n\n# get Model with weights\ntransformer$TFRobertaModel$from_pretrained('roberta-base')\n\n\nData preparation\nA dataset for binary classification is provided in text2vec package. Let’s load the dataset and take a sample for fast model training.\n\n\nlibrary(text2vec)\ndata(\"movie_review\")\ndf = movie_review %>% rename(target = sentiment, comment_text = review) %>% \n  sample_n(2000) %>% \n  data.table::as.data.table()\n\n\nSplit our data into 2 parts:\n\n\nidx_train = sample.int(nrow(df)*0.8)\n\ntrain = df[idx_train,]\ntest = df[!idx_train,]\n\n\nData input for Keras\nUntil now, we’ve just covered data import and train-test split. To feed input to the network we have to turn our raw text into indices via the imported tokenizer. And then adapt the model to do binary classification by adding a dense layer with a single unit at the end.\nHowever, we want to train our data for 3 models GPT-2, RoBERTa, and Electra. We need to write a loop for that.\n\nNote: one model in general requires 500-700 MB\n\n\n\n# list of 3 models\nai_m = list(\n  c('TFGPT2Model',       'GPT2Tokenizer',       'gpt2'),\n   c('TFRobertaModel',    'RobertaTokenizer',    'roberta-base'),\n   c('TFElectraModel',    'ElectraTokenizer',    'google/electra-small-generator')\n)\n\n# parameters\nmax_len = 50L\nepochs = 2\nbatch_size = 10\n\n# create a list for model results\ngather_history = list()\n\nfor (i in 1:length(ai_m)) {\n  \n  # tokenizer\n  tokenizer = glue::glue(\"transformer${ai_m[[i]][2]}$from_pretrained('{ai_m[[i]][3]}',\n                         do_lower_case=TRUE)\") %>% \n    rlang::parse_expr() %>% eval()\n  \n  # model\n  model_ = glue::glue(\"transformer${ai_m[[i]][1]}$from_pretrained('{ai_m[[i]][3]}')\") %>% \n    rlang::parse_expr() %>% eval()\n  \n  # inputs\n  text = list()\n  # outputs\n  label = list()\n  \n  data_prep = function(data) {\n    for (i in 1:nrow(data)) {\n      \n      txt = tokenizer$encode(data[['comment_text']][i],max_length = max_len, \n                             truncation=T) %>% \n        t() %>% \n        as.matrix() %>% list()\n      lbl = data[['target']][i] %>% t()\n      \n      text = text %>% append(txt)\n      label = label %>% append(lbl)\n    }\n    list(do.call(plyr::rbind.fill.matrix,text), do.call(plyr::rbind.fill.matrix,label))\n  }\n  \n  train_ = data_prep(train)\n  test_ = data_prep(test)\n  \n  # slice dataset\n  tf_train = tensor_slices_dataset(list(train_[[1]],train_[[2]])) %>% \n    dataset_batch(batch_size = batch_size, drop_remainder = TRUE) %>% \n    dataset_shuffle(128) %>% dataset_repeat(epochs) %>% \n    dataset_prefetch(tf$data$experimental$AUTOTUNE)\n  \n  tf_test = tensor_slices_dataset(list(test_[[1]],test_[[2]])) %>% \n    dataset_batch(batch_size = batch_size)\n  \n  # create an input layer\n  input = layer_input(shape=c(max_len), dtype='int32')\n  hidden_mean = tf$reduce_mean(model_(input)[[1]], axis=1L) %>% \n    layer_dense(64,activation = 'relu')\n  # create an output layer for binary classification\n  output = hidden_mean %>% layer_dense(units=1, activation='sigmoid')\n  model = keras_model(inputs=input, outputs = output)\n  \n  # compile with AUC score\n  model %>% compile(optimizer= tf$keras$optimizers$Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0),\n                    loss = tf$losses$BinaryCrossentropy(from_logits=F),\n                    metrics = tf$metrics$AUC())\n  \n  print(glue::glue('{ai_m[[i]][1]}'))\n  # train the model\n  history = model %>% keras::fit(tf_train, epochs=epochs, #steps_per_epoch=len/batch_size,\n                validation_data=tf_test)\n  gather_history[[i]]<- history\n  names(gather_history)[i] = ai_m[[i]][1]\n}\n\n\n\nReproduce in a           Notebook\n\n\nExtract results to see the benchmarks:\n\n\nres = sapply(1:3, function(x) {\n  do.call(rbind,gather_history[[x]][[\"metrics\"]]) %>% \n    as.data.frame() %>% \n    tibble::rownames_to_column() %>% \n    mutate(model_names = names(gather_history[x])) \n}, simplify = F) %>% do.call(plyr::rbind.fill,.) %>% \n  mutate(rowname = stringr::str_extract(rowname, 'loss|val_loss|auc|val_auc')) %>% \n  rename(epoch_1 = V1, epoch_2 = V2)\n\n\n\n\n\nBoth the RoBERTa and Electra models show some additional improvements after 2 epochs of training, which cannot be said of GPT-2. In this case, it is clear that it can be enough to train a state-of-the-art model even for a single epoch.\nConclusion\nIn this post, we showed how to use state-of-the-art NLP models from R.\nTo understand how to apply them to more complex tasks, it is highly recommended to review the transformers tutorial.\nWe encourage readers to try out these models and share their results below in the comments section!\n\n\n\n",
    "preview": "posts/2020-07-30-state-of-the-art-nlp-models-from-r/files/dino.jpg",
    "last_modified": "2024-11-21T15:50:32+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-07-29-parallelized-sampling/",
    "title": "Parallelized sampling using exponential variates",
    "description": "How can the seemingly iterative process of weighted sampling without replacement be transformed into something highly parallelizable? Turns out a well-known technique based on exponential variates accomplishes exactly that.",
    "author": [
      {
        "name": "Yitao Li",
        "url": {}
      }
    ],
    "date": "2020-07-29",
    "categories": [
      "Concepts",
      "Distributed Computing"
    ],
    "contents": "\nAs part of our recent work to support weighted sampling of Spark data frames in sparklyr, we embarked on a journey searching for algorithms that can perform weighted sampling, especially sampling without replacement, in efficient and scalable ways within a distributed cluster-computing framework, such as Apache Spark.\nIn the interest of brevity, “weighted sampling without replacement” shall be shortened into SWoR for the remainder of this blog post.\nIn the following sections, we will explain and illustrate what SWoR means probability-wise, briefly outline some alternative solutions we have considered but were not completely satisfied with, and then deep-dive into exponential variates, a simple mathematical construct that made the ideal solution for this problem possible.\nIf you cannot wait to jump into action, there is also a section in which we showcase example usages of sdf_weighted_sample() in sparklyr. In addition, you can examine the implementation detail of sparklyr::sdf_weighted_sample() in this pull request.\nHow it all started\nOur journey started from a Github issue inquiring about the possibility of supporting the equivalent of dplyr::sample_frac(..., weight = <weight_column>) for Spark data frames in sparklyr. For example,\n\n\ndplyr::sample_frac(mtcars, 0.25, weight = gear, replace = FALSE)\n\n\n##                    mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Merc 280C         17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\n## Chrysler Imperial 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\n## Fiat X1-9         27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n## Hornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\n## Porsche 914-2     26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n## Maserati Bora     15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n## Ferrari Dino      19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nwill randomly select one-fourth of all rows from a R data frame named “mtcars” without replacement, using mtcars$gear as weights. We were unable to find any function implementing the weighted versions of dplyr::sample_frac among Spark SQL built-in functions in Spark 3.0 or in earlier versions, which means a future version of sparklyr will need to run its own weighted sampling algorithm to support such use cases.\nWhat exactly is SWoR\nThe purpose of this section is to mathematically describe the probability distribution generated by SWoR in terms of \\(w_1, \\dotsc, w_N\\), so that readers can clearly see that the exponential-variate based algorithm presented in a subsequent section in fact samples from precisely the same probability distribution. Readers already having a crystal-clear mental picture of what SWoR entails should probably skip most of this section. The key take-away here is given \\(N\\) rows \\(r_1, \\dotsc, r_N\\) and their weights \\(w_1, \\dotsc, w_N\\) and a desired sample size \\(n\\), the probability of SWoR selecting \\((r_1, \\dotsc, r_n)\\) is \\(\\prod\\limits_{j = 1}^{n} \\left( {w_j} \\middle/ {\\sum\\limits_{k = j}^{N}{w_k}} \\right)\\).\nSWOR is conceptually equivalent to a \\(n\\)-step process of selecting 1 out of \\((n - j + 1)\\) remaining rows in the \\(j\\)-th step for \\(j \\in \\{1, \\dotsc, n\\}\\), with each remaining row’s likelihood of getting selected being linearly proportional to its weight in any of the steps, i.e.,\nsamples := {}\npopulation := {r[1], ..., r[N]}\n\nfor j = 1 to n\n  select r[x] from population with probability\n    (w[x] / TotalWeight(population))\n  samples := samples + {r[x]}\n  population := population - {r[x]}\nNotice the outcome of a SWoR process is in fact order-significant, which is why in this post it will always be represented as an ordered tuple of elements.\nIntuitively, SWoR is analogous to throwing darts at a bunch of tiles. For example, let’s say the size of our sample space is 5:\nImagine \\(r_1, r_2, \\dotsc, r_5\\) as 5 rectangular tiles laid out contiguously on a wall with widths \\(w_1, w_2, \\dotsc, w_5\\), with \\(r_1\\) covering \\([0, w_1)\\), \\(r_2\\) covering \\([w_1, w_1 + w_2)\\), …, and \\(r_5\\) covering \\(\\left[\\sum\\limits_{j = 1}^{4} w_j, \\sum\\limits_{j = 1}^{5} w_j\\right)\\)\nEquate drawing a random sample in each step to throwing a dart uniformly randomly within the interval covered by all tiles that are not hit yet\nAfter a tile is hit, it gets taken out and remaining tiles are re-arranged so that they continue to cover a contiguous interval without overlapping\nIf our sample size is 3, then we shall ask ourselves what is the probability of the dart hitting \\((r_1, r_2, r_3)\\) in that order?\nIn step \\(j = 1\\), the dart will hit \\(r_1\\) with probability \\(\\left. w_1 \\middle/ \\left(\\sum\\limits_{k = 1}^{N}w_k\\right) \\right.\\)\n .\nAfter deleting \\(r_1\\) from the sample space after it’s hit, step \\(j = 2\\) will look like this:\n ,\nand the probability of the dart hitting \\(r_2\\) in step 2 is \\(\\left. w_2 \\middle/ \\left(\\sum\\limits_{k = 2}^{N}w_k\\right) \\right.\\) .\nFinally, moving on to step \\(j = 3\\), we have:\n ,\nwith the probability of the dart hitting \\(r_3\\) being \\(\\left. w_3 \\middle/ \\left(\\sum\\limits_{k = 3}^{N}w_k\\right) \\right.\\).\nSo, combining all of the above, the overall probability of selecting \\((r_1, r_2, r_3)\\) is \\(\\prod\\limits_{j = 1}^{3} \\left( {w_j} \\middle/ {\\sum\\limits_{k = j}^{N}{w_k}} \\right)\\).\nNaive approaches for implementing SWoR\nThis section outlines some possible approaches that were briefly under consideration. Because none of these approaches scales well to a large number of rows or a non-trivial number of partitions in a Spark data frame, we decided to avoid all of them in sparklyr.\nA tree-base approach\nOne possible way to accomplish SWoR is to have a mutable data structure keeping track of the sample space at each step.\nContinuing with the dart-throwing analogy from the previous section, let us say initially, none of the tiles has been taken out yet, and a dart has landed at some point \\(x \\in \\left[0, \\sum\\limits_{k = 1}^{N} w_k\\right)\\). Which tile did it hit? This can be answered efficiently if we have a binary tree, pictured as the following (or in general, some \\(b\\)-ary tree for integer \\(b \\ge 2\\))\n.To find the tile that was hit given the dart’s position \\(x\\), we simply need to traverse down the tree, going through the box containing \\(x\\) in each level, incurring a \\(O(\\log(N))\\) cost in time complexity for each sample. To take a tile out of the picture, we update the width of the tile to \\(0\\) and propagate this change upwards from leaf level to root of the tree, again incurring a \\(O(\\log(N))\\) cost in time complexity, making the overall time complexity of selecting \\(n\\) samples \\(O(n \\cdot \\log(N))\\), which is not so great for large data sets, and also, not parallelizable across multiple partitions of a Spark data frame.\nRejection sampling\nAnother possible approach is to use rejection sampling. In term of the previously mentioned dart-throwing analogy, that means not removing any tile that is hit, hence avoiding the performance cost of keeping the sample space up-to-date, but then having to re-throw the dart in each of the subsequent rounds until the dart lands on a tile that was not hit previously. This approach, just like the previous one, would not be performant, and would not be parallelizable across multiple partitions of a Spark data frame either.\nExponential variates to the rescue\nA solution that has proven to be much better than any of the naive approaches turns out to be a numerical stable variant of the algorithm described in “Weighted Random Sampling” (Efraimidis and Spirakis 2016) by Pavlos S. Efraimidis and Paul G. Spirakis.\nA version of this sampling algorithm implemented by sparklyr does the following to sample \\(n\\) out of \\(N\\) rows from a Spark data frame \\(X\\):\nFor each row \\(r_j \\in X\\), draw a random number \\(u_j\\) independently and uniformly randomly from \\((0, 1)\\) and compute the key of \\(r_j\\) as \\(k_j = \\ln(u_j) / w_j\\), where \\(w_j\\) is the weight of \\(r_j\\). Perform this calulation in parallel across all partitions of \\(X\\).\nSelect \\(n\\) rows with largest keys and return them as the result. This step is also mostly parallelizable: for each partition of \\(X\\), one can select up to \\(n\\) rows having largest keys within that partition as candidates, and after selecting candidates from all partitions in parallel, simply extract the top \\(n\\) rows among all candidates, and return them as the \\(n\\) chosen samples.\nThere are at least 4 reasons why this solution is highly appealing and was chosen to be implemented in sparklyr:\nIt is a one-pass algorithm (i.e., only need to iterate through all rows of a data frame exactly once).\nIts computational overhead is quite low (as selecting top \\(n\\) rows at any stage only requires a bounded priority queue of max size \\(n\\), which costs \\(O(\\log(n))\\) per update in time complexity).\nMore importantly, most of its required computations can be performed in parallel. In fact, the only non-parallelizable step is the very last stage of combining top candidates from all partitions and choosing the top \\(n\\) rows among those candidates. So, it fits very well into the world of Spark / MapReduce, and has drastically better horizontal scalability compared to the naive approaches.\nBonus: It is also suitable for weighted reservoir sampling (i.e., can sample \\(n\\) out of a possibly infinite stream of rows according to their weights such that at any moment the \\(n\\) samples will be a weighted representation of all rows that have been processed so far).\nWhy does this algorithm work\nAs an interesting aside, some readers have probably seen this technique presented in a slightly different form under another name. It is in fact equivalent to a generalized version of the Gumbel-max trick which is commonly referred to as the Gumbel-top-k trick. Readers familiar with properties of the Gumbel distribution will no doubt have an easy time convincing themselves the algorithm above works as expected.\nIn this section, we will also present a proof of correctness for this algorithm based on elementary properties of probability density function (shortened as PDF from now on), cumulative distribution function (shortened as CDF from now on), and basic calculus.\nFirst of all, to make sense of all the \\(\\ln(u_j) / w_j\\) calculations in this algorithm, one has to understand inverse transform sampling. For each \\(j \\in \\{1, \\dotsc, N\\}\\), consider the probability distribution defined on \\((-\\infty, 0)\\) with CDF \\(F_j(x) = e^{w_j \\cdot x}\\). In order to pluck out a value \\(y\\) from this distribution, we first sample a value \\(u_j\\) uniformly randomly out of \\((0, 1)\\) that determines the percentile of \\(y\\) (i.e., how our \\(y\\) value ranks relative to all possible \\(y\\) values, a.k.a, the “overall population,” from this distribution), and then apply \\(F_j^{-1}\\) to \\(u_j\\) to find \\(y\\), so, \\(y = F_j^{-1}(u_j) = \\ln(u_j) / w_j\\).\nSecondly, after defining all the required CDF functions \\(F_j(x) = e^{w_j \\cdot x}\\) for \\(j \\in \\{1, \\dotsc, N\\}\\), we can also easily derive their corresponding PDF functions \\(f_j\\): \\[f_j(x) = \\frac{d F_j(x)}{dx} = w_j e^{w_j \\cdot x}\\].\nFinally, with a clear understanding of the family of probability distributions involved, one can prove the probability of this algorithm selecting a given sequence of rows \\((r_1, \\dotsc, r_n)\\) is equal to \\(\\prod\\limits_{j = 1}^{n} \\left( {w_j} \\middle/ {\\sum\\limits_{k = j}^{N}{w_k}} \\right)\\), identical to the probability previously mentioned in the “What exactly is SWoR” section, which implies the possible outcomes of this algorithm will follow exactly the same probability distribution as that of a \\(n\\)-step SWoR.\nIn order to not deprive our dear readers the pleasure of completing this proof by themselves, we have decided to not inline the rest of the proof (which boils down to a calculus exercise) within this blog post, but it is available in this file.\nWeighted sampling with replacement\nWhile all previous sections focused entirely on weighted sampling without replacement, this section will briefly discuss how the exponential-variate approach can also benefit the weighted-sampling-with-replacement use case (which will be shortened as SWR from now on).\nAlthough SWR with sample size \\(n\\) can be carried out by \\(n\\) independent processes each selecting \\(1\\) sample, parallelizing a SWR workload across all partitions of a Spark data frame (let’s call it \\(X\\)) will still be more performant if the number of partitions is much larger than \\(n\\) and more than \\(n\\) executors are available in a Spark cluster.\nAn initial solution we had in mind was to run SWR with sample size \\(n\\) in parallel on each partition of \\(X\\), and then re-sample the results based on relative total weights of each partition. Despite sounding deceptively simple when summarized in words, implementing such a solution in practice would be a moderately complicated task. First, one has to apply the alias method or similar in order to perform weighted sampling efficiently on each partition of \\(X\\), and on top of that, implementing the re-sampling logic across all partitions correctly and verifying the correctness of such procedure will also require considerable effort.\nIn comparison, with the help of exponential variates, a SWR carried out as \\(n\\) independent SWoR processes each selecting \\(1\\) sample is much simpler to implement, while still being comparable to our initial solution in terms of efficiency and scalability. An example implementation of it (which takes fewer than 60 lines of Scala) is presented in samplingutils.scala.\nVisualization\nHow do we know sparklyr::sdf_weighted_sample() is working as expected? While the rigorous answer to this question is presented in full in the testing section, we thought it would also be useful to first show some histograms that will help readers visualize what that test plan is. Therefore in this section, we will do the following:\nRun dplyr::slice_sample() multiple times on a small sample space, with each run using a different PRNG seed (sample size will be reduced to \\(2\\) here so that there will fewer than 100 possible outcomes and visualization will be easier)\nDo the same for sdf_weighted_sample()\nUse histograms to visualize the distribution of sampling outcomes\nThroughout this section, we will sample \\(2\\) elements out of \\(\\{0, \\dotsc, 7\\}\\) without replacement according to some weights, so, the first step is to set up the following in R:\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n\n# `octs` will be our sample space\nocts <- data.frame(\n  x = seq(0, 7),\n  weight = c(1, 4, 2, 8, 5, 7, 1, 4)\n)\n# `octs_sdf` will be our sample space copied into a Spark data frame\nocts_sdf <- copy_to(sc, octs)\n\nsample_size <- 2\n\n\nIn order to tally up and visualize the sampling outcomes efficiently, we shall map each possible outcome to an octal number (e.g., (6, 7) gets mapped to \\(6 \\cdot 8^0 + 7 \\cdot 8^1\\)) using a helper function to_oct in R:\n\n\nto_oct <- function(sample) sum(8 ^ seq(0, sample_sz - 1) * sample$x)\n\n\nWe also need to tally up sampling outcomes from dplyr::slice_sample() and sparklyr::sdf_weighted_sample() in 2 separate arrays:\n\n\nmax_possible_outcome <- to_oct(list(x = seq(8 - sample_sz, 7)))\n\nsdf_weighted_sample_outcomes <- rep(0, max_possible_outcome)\ndplyr_slice_sample_outcomes <- rep(0, max_possible_outcome)\n\n\nFinally, we can run both dplyr::slice_sample() and sparklyr::sdf_weighted_sample() for arbitrary number of iterations and compare tallied outcomes from both:\n\n\nnum_sampling_iters <- 1000  # actually we will vary this value from 500 to 5000\n\nfor (x in seq(num_sampling_iters)) {\n  sample1 <- octs_sdf %>%\n    sdf_weighted_sample(\n      k = sample_size, weight_col = \"weight\", replacement = FALSE, seed = seed\n    ) %>%\n    collect() %>%\n    to_oct()\n  sdf_weighted_sample_outcomes[[sample1]] <-\n      sdf_weighted_sample_outcomes[[sample1]] + 1\n\n  seed <- x * 97\n  set.seed(seed) # set random seed for dplyr::sample_slice()\n  sample2 <- octs %>%\n    dplyr::slice_sample(\n      n = sample_size, weight_by = weight, replace = FALSE\n    ) %>%\n    to_oct()\n  dplyr_slice_sample_outcomes[[sample2]] <-\n      dplyr_slice_sample_outcomes[[sample2]] + 1\n}\n\n\nAfter all the hard work above, we can now enjoy plotting the sampling outcomes from dplyr::slice_sample() and those from sparklyr::sdf_weighted_sample() after 500, 1000, and 5000 iterations and observe how the distributions of both start converging after a large number of iterations.\nSampling outcomes after 500, 1000, and 5000 iterations, shown in 3 histograms:\n\n(you will most probably need to view it in a separate tab to see everything clearly)\nTesting\nWhile parallelized sampling based on exponential variates looks fantastic on paper, there are still plenty of potential pitfalls when it comes to translating such idea into code, and as usual, a good testing plan is necessary to ensure implementation correctness.\nFor instance, numerical instability issues from floating point numbers arise if \\(\\ln(u_j) / w_j\\) were replaced by \\(u_j ^ {1 / w_j}\\) in the aforementioned computations.\nAnother more subtle source of error is the usage of PRNG seeds. For example, consider the following:\n  def sampleWithoutReplacement(\n    rdd: RDD[Row],\n    weightColumn: String,\n    sampleSize: Int,\n    seed: Long\n  ): RDD[Row] = {\n    val sc = rdd.context\n    if (0 == sampleSize) {\n      sc.emptyRDD\n    } else {\n      val random = new Random(seed)\n      val mapRDDs = rdd.mapPartitions { iter =>\n        for (row <- iter) {\n          val weight = row.getAs[Double](weightColumn)\n          val key = scala.math.log(random.nextDouble) / weight\n          <and then make sampling decision for `row` based on its `key`,\n           as described in the previous section>\n        }\n        ...\n      }\n      ...\n    }\n  }\nEven though it might look OK upon first glance, rdd.mapPartitions(...) from the above will cause the same sequence of pseudorandom numbers to be applied to multiple partitions of the input Spark data frame, which will cause undesired bias (i.e., sampling outcomes from one partition will have non-trivial correlation with those from another partition when such correlation should be negligible in a correct implementation).\nThe code snippet below is an example implementation in which each partition of the input Spark data frame is sampled using a different sequence of pseudorandom numbers:\n  def sampleWithoutReplacement(\n    rdd: RDD[Row],\n    weightColumn: String,\n    sampleSize: Int,\n    seed: Long\n  ): RDD[Row] = {\n    val sc = rdd.context\n    if (0 == sampleSize) {\n      sc.emptyRDD\n    } else {\n      val mapRDDs = rdd.mapPartitionsWithIndex { (index, iter) =>\n        val random = new Random(seed + index)\n\n        for (row <- iter) {\n          val weight = row.getAs[Double](weightColumn)\n          val key = scala.math.log(random.nextDouble) / weight\n          <and then make sampling decision for `row` based on its `key`,\n           as described in the previous section>\n        }\n\n        ...\n      }\n    ...\n  }\n}\nAn example test case in which a two-sided Kolmogorov-Smirnov test is used to compare distribution of sampling outcomes from dplyr::slice_sample() with that from sparklyr::sdf_weighted_sample() is shown in this file. Such tests have proven to be effective in surfacing non-obvious implementation errors such as the ones mentioned above.\nExample Usages\nPlease note the sparklyr::sdf_weighted_sample() functionality is not included in any official release of sparklyr yet. We are aiming to ship it as part of sparklyr 1.4 in about 2 to 3 months from now.\nIn the meanwhile, you can try it out with the following steps:\nFirst, make sure remotes is installed, and then run\n\n\nremotes::install_github(\"sparklyr/sparklyr\", ref = \"master\")\n\n\nto install sparklyr from source.\nNext, create a test data frame with numeric weight column consisting of non-negative weight for each row, and then copy it to Spark (see code snippet below as an example):\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n\nexample_df <- data.frame(\n  x = seq(100),\n  weight = c(\n    rep(1, 50),\n    rep(2, 25),\n    rep(4, 10),\n    rep(8, 10),\n    rep(16, 5)\n  )\n)\nexample_sdf <- copy_to(sc, example_df, repartition = 5, overwrite = TRUE)\n\n\nFinally, run sparklyr::sdf_weighted_sample() on example_sdf:\n\n\nsample_size <- 5\n\nsamples_without_replacement <- example_sdf %>%\n  sdf_weighted_sample(\n    weight_col = \"weight\",\n    k = sample_size,\n    replacement = FALSE\n  )\n\nsamples_without_replacement %>% print(n = sample_size)\n\n\n## # Source: spark<?> [?? x 2]\n##       x weight\n##   <int>  <dbl>\n## 1    48      1\n## 2    22      1\n## 3    78      4\n## 4    56      2\n## 5   100     16\n\n\nsamples_with_replacement <- example_sdf %>%\n  sdf_weighted_sample(\n    weight_col = \"weight\",\n    k = sample_size,\n    replacement = TRUE\n  )\n\nsamples_with_replacement %>% print(n = sample_size)\n\n\n## # Source: spark<?> [?? x 2]\n##       x weight\n##   <int>  <dbl>\n## 1    86      8\n## 2    97     16\n## 3    91      8\n## 4   100     16\n## 5    65      2\nAcknowledgement\nFirst and foremost, the author wishes to thank @ajing for reporting the weighted sampling use cases were not properly supported yet in sparklyr 1.3 and suggesting it should be part of some future version of sparklyr in this Github issue.\nSpecial thanks also goes to Javier (@javierluraschi) for reviewing the implementation of all exponential-variate based sampling algorithms in sparklyr, and to Mara (@batpigandme), Sigrid (@Sigrid), and Javier (@javierluraschi) for their valuable editorial suggestions.\nWe hope you have enjoyed reading this blog post! If you wish to learn more about sparklyr, we recommend visiting sparklyr.ai, spark.rstudio.com, and some of the previous release posts such as sparklyr 1.3 and sparklyr 1.2. Also, your contributions to sparklyr are more than welcome. Please send your pull requests through here and file any bug report or feature request in here.\nThanks for reading!\n\n\n\nEfraimidis, Pavlos, and Paul (Pavlos) Spirakis. 2016. “Weighted Random Sampling.” In Encyclopedia of Algorithms, edited by Ming-Yang Kao, 2365–67. New York, NY: Springer New York. https://doi.org/10.1007/978-1-4939-2864-4_478.\n\n\n\n\n",
    "preview": "posts/2020-07-29-parallelized-sampling/images/dice.jpg",
    "last_modified": "2024-11-21T15:49:33+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-07-20-fnn-lstm/",
    "title": "Time series prediction with FNN-LSTM",
    "description": "In a recent post, we showed how an LSTM autoencoder, regularized by false nearest neighbors (FNN) loss, can be used to reconstruct the attractor of a nonlinear, chaotic dynamical system. Here, we explore how that same technique assists in prediction. Matched up with a comparable, capacity-wise, \"vanilla LSTM\", FNN-LSTM improves performance on a set of very different, real-world datasets, especially for the initial steps in a multi-step forecast.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-07-20",
    "categories": [
      "R",
      "TensorFlow/Keras",
      "Time Series",
      "Generative Models"
    ],
    "contents": "\nToday, we pick up on the plan alluded to in the conclusion of the recent Deep attractors: Where deep learning meets\nchaos: employ that same technique to generate forecasts for\nempirical time series data.\n“That same technique,” which for conciseness, I’ll take the liberty of referring to as FNN-LSTM, is due to William Gilpin’s\n2020 paper “Deep reconstruction of strange attractors from time series” (Gilpin 2020).\nIn a nutshell1, the problem addressed is as follows: A system, known or assumed to be nonlinear and highly dependent on\ninitial conditions, is observed, resulting in a scalar series of measurements. The measurements are not just – inevitably –\nnoisy, but in addition, they are – at best – a projection of a multidimensional state space onto a line.\nClassically in nonlinear time series analysis, such scalar series of observations are augmented by supplementing, at every\npoint in time, delayed measurements of that same series – a technique called delay coordinate embedding (Sauer, Yorke, and Casdagli 1991). For\nexample, instead of just a single vector X1, we could have a matrix of vectors X1, X2, and X3, with X2 containing\nthe same values as X1, but starting from the third observation, and X3, from the fifth. In this case, the delay would be\n2, and the embedding dimension, 3. Various theorems state that if these\nparameters are chosen adequately, it is possible to reconstruct the complete state space. There is a problem though: The\ntheorems assume that the dimensionality of the true state space is known, which in many real-world applications, won’t be the\ncase.\nThis is where Gilpin’s idea comes in: Train an autoencoder, whose intermediate representation encapsulates the system’s\nattractor. Not just any MSE-optimized autoencoder though. The latent representation is regularized by false nearest\nneighbors (FNN) loss, a technique commonly used with delay coordinate embedding to determine an adequate embedding dimension.\nFalse neighbors are those who are close in n-dimensional space, but significantly farther apart in n+1-dimensional space.\nIn the aforementioned introductory post, we showed how this\ntechnique allowed to reconstruct the attractor of the (synthetic) Lorenz system. Now, we want to move on to prediction.\nWe first describe the setup, including model definitions, training procedures, and data preparation. Then, we tell you how it\nwent.\nSetup\nFrom reconstruction to forecasting, and branching out into the real world\nIn the previous post, we trained an LSTM autoencoder to generate a compressed code, representing the attractor of the system.\nAs usual with autoencoders, the target when training is the same as the input, meaning that overall loss consisted of two\ncomponents: The FNN loss, computed on the latent representation only, and the mean-squared-error loss between input and\noutput. Now for prediction, the target consists of future values, as many as we wish to predict. Put differently: The\narchitecture stays the same, but instead of reconstruction we perform prediction, in the standard RNN way. Where the usual RNN\nsetup would just directly chain the desired number of LSTMs, we have an LSTM encoder that outputs a (timestep-less) latent\ncode, and an LSTM decoder that starting from that code, repeated as many times as required, forecasts the required number of\nfuture values.\nThis of course means that to evaluate forecast performance, we need to compare against an LSTM-only setup. This is exactly\nwhat we’ll do, and comparison will turn out to be interesting not just quantitatively, but qualitatively as well.\nWe perform these comparisons on the four datasets Gilpin chose to demonstrate attractor reconstruction on observational\ndata. While all of these, as is evident from the images\nin that notebook, exhibit nice attractors, we’ll see that not all of them are equally suited to forecasting using simple\nRNN-based architectures – with or without FNN regularization. But even those that clearly demand a different approach allow\nfor interesting observations as to the impact of FNN loss.\nModel definitions and training setup\nIn all four experiments, we use the same model definitions and training procedures, the only differing parameter being the\nnumber of timesteps used in the LSTMs (for reasons that will become evident when we introduce the individual datasets).\nBoth architectures were chosen to be straightforward, and about comparable in number of parameters – both basically consist\nof two LSTMs with 32 units (n_recurrent will be set to 32 for all experiments).2\nFNN-LSTM\nFNN-LSTM looks nearly like in the previous post, apart from the fact that we split up the encoder LSTM into two, to uncouple\ncapacity (n_recurrent) from maximal latent state dimensionality (n_latent, kept at 10 just like before).\n\n\n# DL-related packages\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(tfdatasets)\nlibrary(tfautograph)\nlibrary(reticulate)\n\n# going to need those later\nlibrary(tidyverse)\nlibrary(cowplot)\n\nencoder_model <- function(n_timesteps,\n                          n_features,\n                          n_recurrent,\n                          n_latent,\n                          name = NULL) {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$noise <- layer_gaussian_noise(stddev = 0.5)\n    self$lstm1 <-  layer_lstm(\n      units = n_recurrent,\n      input_shape = c(n_timesteps, n_features),\n      return_sequences = TRUE\n    ) \n    self$batchnorm1 <- layer_batch_normalization()\n    self$lstm2 <-  layer_lstm(\n      units = n_latent,\n      return_sequences = FALSE\n    ) \n    self$batchnorm2 <- layer_batch_normalization()\n    \n    function (x, mask = NULL) {\n      x %>%\n        self$noise() %>%\n        self$lstm1() %>%\n        self$batchnorm1() %>%\n        self$lstm2() %>%\n        self$batchnorm2() \n    }\n  })\n}\n\ndecoder_model <- function(n_timesteps,\n                          n_features,\n                          n_recurrent,\n                          n_latent,\n                          name = NULL) {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$repeat_vector <- layer_repeat_vector(n = n_timesteps)\n    self$noise <- layer_gaussian_noise(stddev = 0.5)\n    self$lstm <- layer_lstm(\n      units = n_recurrent,\n      return_sequences = TRUE,\n      go_backwards = TRUE\n    ) \n    self$batchnorm <- layer_batch_normalization()\n    self$elu <- layer_activation_elu() \n    self$time_distributed <- time_distributed(layer = layer_dense(units = n_features))\n    \n    function (x, mask = NULL) {\n      x %>%\n        self$repeat_vector() %>%\n        self$noise() %>%\n        self$lstm() %>%\n        self$batchnorm() %>%\n        self$elu() %>%\n        self$time_distributed()\n    }\n  })\n}\n\nn_latent <- 10L\nn_features <- 1\nn_hidden <- 32\n\nencoder <- encoder_model(n_timesteps,\n                         n_features,\n                         n_hidden,\n                         n_latent)\n\ndecoder <- decoder_model(n_timesteps,\n                         n_features,\n                         n_hidden,\n                         n_latent)\n\n\nThe regularizer, FNN loss, is unchanged:\n\n\nloss_false_nn <- function(x) {\n  \n  # changing these parameters is equivalent to\n  # changing the strength of the regularizer, so we keep these fixed (these values\n  # correspond to the original values used in Kennel et al 1992).\n  rtol <- 10 \n  atol <- 2\n  k_frac <- 0.01\n  \n  k <- max(1, floor(k_frac * batch_size))\n  \n  ## Vectorized version of distance matrix calculation\n  tri_mask <-\n    tf$linalg$band_part(\n      tf$ones(\n        shape = c(tf$cast(n_latent, tf$int32), tf$cast(n_latent, tf$int32)),\n        dtype = tf$float32\n      ),\n      num_lower = -1L,\n      num_upper = 0L\n    )\n  \n  # latent x batch_size x latent\n  batch_masked <-\n    tf$multiply(tri_mask[, tf$newaxis,], x[tf$newaxis, reticulate::py_ellipsis()])\n  \n  # latent x batch_size x 1\n  x_squared <-\n    tf$reduce_sum(batch_masked * batch_masked,\n                  axis = 2L,\n                  keepdims = TRUE)\n  \n  # latent x batch_size x batch_size\n  pdist_vector <- x_squared + tf$transpose(x_squared, perm = c(0L, 2L, 1L)) -\n    2 * tf$matmul(batch_masked, tf$transpose(batch_masked, perm = c(0L, 2L, 1L)))\n  \n  #(latent, batch_size, batch_size)\n  all_dists <- pdist_vector\n  # latent\n  all_ra <-\n    tf$sqrt((1 / (\n      batch_size * tf$range(1, 1 + n_latent, dtype = tf$float32)\n    )) *\n      tf$reduce_sum(tf$square(\n        batch_masked - tf$reduce_mean(batch_masked, axis = 1L, keepdims = TRUE)\n      ), axis = c(1L, 2L)))\n  \n  # Avoid singularity in the case of zeros\n  #(latent, batch_size, batch_size)\n  all_dists <-\n    tf$clip_by_value(all_dists, 1e-14, tf$reduce_max(all_dists))\n  \n  #inds = tf.argsort(all_dists, axis=-1)\n  top_k <- tf$math$top_k(-all_dists, tf$cast(k + 1, tf$int32))\n  # (#(latent, batch_size, batch_size)\n  top_indices <- top_k[[1]]\n  \n  #(latent, batch_size, batch_size)\n  neighbor_dists_d <-\n    tf$gather(all_dists, top_indices, batch_dims = -1L)\n  #(latent - 1, batch_size, batch_size)\n  neighbor_new_dists <-\n    tf$gather(all_dists[2:-1, , ],\n              top_indices[1:-2, , ],\n              batch_dims = -1L)\n  \n  # Eq. 4 of Kennel et al.\n  #(latent - 1, batch_size, batch_size)\n  scaled_dist <- tf$sqrt((\n    tf$square(neighbor_new_dists) -\n      # (9, 8, 2)\n      tf$square(neighbor_dists_d[1:-2, , ])) /\n      # (9, 8, 2)\n      tf$square(neighbor_dists_d[1:-2, , ])\n  )\n  \n  # Kennel condition #1\n  #(latent - 1, batch_size, batch_size)\n  is_false_change <- (scaled_dist > rtol)\n  # Kennel condition 2\n  #(latent - 1, batch_size, batch_size)\n  is_large_jump <-\n    (neighbor_new_dists > atol * all_ra[1:-2, tf$newaxis, tf$newaxis])\n  \n  is_false_neighbor <-\n    tf$math$logical_or(is_false_change, is_large_jump)\n  #(latent - 1, batch_size, 1)\n  total_false_neighbors <-\n    tf$cast(is_false_neighbor, tf$int32)[reticulate::py_ellipsis(), 2:(k + 2)]\n  \n  # Pad zero to match dimensionality of latent space\n  # (latent - 1)\n  reg_weights <-\n    1 - tf$reduce_mean(tf$cast(total_false_neighbors, tf$float32), axis = c(1L, 2L))\n  # (latent,)\n  reg_weights <- tf$pad(reg_weights, list(list(1L, 0L)))\n  \n  # Find batch average activity\n  \n  # L2 Activity regularization\n  activations_batch_averaged <-\n    tf$sqrt(tf$reduce_mean(tf$square(x), axis = 0L))\n  \n  loss <- tf$reduce_sum(tf$multiply(reg_weights, activations_batch_averaged))\n  loss\n  \n}\n\n\nTraining is unchanged as well, apart from the fact that now, we continually output latent variable variances in addition to\nthe losses. This is because with FNN-LSTM, we have to choose an adequate weight for the FNN loss component. An “adequate\nweight” is one where the variance drops sharply after the first n variables, with n thought to correspond to attractor\ndimensionality. For the Lorenz system discussed in the previous post, this is how these variances looked:\n     V1       V2        V3        V4        V5        V6        V7        V8        V9       V10\n 0.0739   0.0582   1.12e-6   3.13e-4   1.43e-5   1.52e-8   1.35e-6   1.86e-4   1.67e-4   4.39e-5\nIf we take variance as an indicator of importance, the first two variables are clearly more important than the rest. This\nfinding nicely corresponds to “official” estimates of Lorenz attractor dimensionality. For example, the correlation dimension\nis estimated to lie around 2.05 (Grassberger and Procaccia 1983).\nThus, here we have the training routine:\n\n\ntrain_step <- function(batch) {\n  with (tf$GradientTape(persistent = TRUE) %as% tape, {\n    code <- encoder(batch[[1]])\n    prediction <- decoder(code)\n    \n    l_mse <- mse_loss(batch[[2]], prediction)\n    l_fnn <- loss_false_nn(code)\n    loss <- l_mse + fnn_weight * l_fnn\n  })\n  \n  encoder_gradients <-\n    tape$gradient(loss, encoder$trainable_variables)\n  decoder_gradients <-\n    tape$gradient(loss, decoder$trainable_variables)\n  \n  optimizer$apply_gradients(purrr::transpose(list(\n    encoder_gradients, encoder$trainable_variables\n  )))\n  optimizer$apply_gradients(purrr::transpose(list(\n    decoder_gradients, decoder$trainable_variables\n  )))\n  \n  train_loss(loss)\n  train_mse(l_mse)\n  train_fnn(l_fnn)\n  \n  \n}\n\ntraining_loop <- tf_function(autograph(function(ds_train) {\n  for (batch in ds_train) {\n    train_step(batch)\n  }\n  \n  tf$print(\"Loss: \", train_loss$result())\n  tf$print(\"MSE: \", train_mse$result())\n  tf$print(\"FNN loss: \", train_fnn$result())\n  \n  train_loss$reset_states()\n  train_mse$reset_states()\n  train_fnn$reset_states()\n  \n}))\n\n\nmse_loss <-\n  tf$keras$losses$MeanSquaredError(reduction = tf$keras$losses$Reduction$SUM)\n\ntrain_loss <- tf$keras$metrics$Mean(name = 'train_loss')\ntrain_fnn <- tf$keras$metrics$Mean(name = 'train_fnn')\ntrain_mse <-  tf$keras$metrics$Mean(name = 'train_mse')\n\n# fnn_multiplier should be chosen individually per dataset\n# this is the value we used on the geyser dataset\nfnn_multiplier <- 0.7\nfnn_weight <- fnn_multiplier * nrow(x_train)/batch_size\n\n# learning rate may also need adjustment\noptimizer <- optimizer_adam(lr = 1e-3)\n\nfor (epoch in 1:200) {\n cat(\"Epoch: \", epoch, \" -----------\\n\")\n training_loop(ds_train)\n \n test_batch <- as_iterator(ds_test) %>% iter_next()\n encoded <- encoder(test_batch[[1]]) \n test_var <- tf$math$reduce_variance(encoded, axis = 0L)\n print(test_var %>% as.numeric() %>% round(5))\n}\n\n\nOn to what we’ll use as a baseline for comparison.\nVanilla LSTM\nHere is the vanilla LSTM, stacking two layers, each, again, of size 32. Dropout and recurrent dropout were chosen individually\nper dataset, as was the learning rate.\n\n\nlstm <- function(n_latent, n_timesteps, n_features, n_recurrent, dropout, recurrent_dropout,\n                 optimizer = optimizer_adam(lr =  1e-3)) {\n  \n  model <- keras_model_sequential() %>%\n    layer_lstm(\n      units = n_recurrent,\n      input_shape = c(n_timesteps, n_features),\n      dropout = dropout, \n      recurrent_dropout = recurrent_dropout,\n      return_sequences = TRUE\n    ) %>% \n    layer_lstm(\n      units = n_recurrent,\n      dropout = dropout,\n      recurrent_dropout = recurrent_dropout,\n      return_sequences = TRUE\n    ) %>% \n    time_distributed(layer_dense(units = 1))\n  \n  model %>%\n    compile(\n      loss = \"mse\",\n      optimizer = optimizer\n    )\n  model\n  \n}\n\nmodel <- lstm(n_latent, n_timesteps, n_features, n_hidden, dropout = 0.2, recurrent_dropout = 0.2)\n\n\nData preparation\nFor all experiments, data were prepared in the same way.\nIn every case, we used the first 10000 measurements available in the respective .pkl files provided by Gilpin in his GitHub\nrepository. To save on file size and not depend on an external\ndata source, we extracted those first 10000 entries to .csv files downloadable directly from this blog’s repo:\n\n\ngeyser <- download.file(\n  \"https://raw.githubusercontent.com/rstudio/ai-blog/master/docs/posts/2020-07-20-fnn-lstm/data/geyser.csv\",\n  \"data/geyser.csv\")\n\nelectricity <- download.file(\n  \"https://raw.githubusercontent.com/rstudio/ai-blog/master/docs/posts/2020-07-20-fnn-lstm/data/electricity.csv\",\n  \"data/electricity.csv\")\n\necg <- download.file(\n  \"https://raw.githubusercontent.com/rstudio/ai-blog/master/docs/posts/2020-07-20-fnn-lstm/data/ecg.csv\",\n  \"data/ecg.csv\")\n\nmouse <- download.file(\n  \"https://raw.githubusercontent.com/rstudio/ai-blog/master/docs/posts/2020-07-20-fnn-lstm/data/mouse.csv\",\n  \"data/mouse.csv\")\n\n\nShould you want to access the complete time series (of considerably greater lengths), just download them from Gilpin’s repo\nand load them using reticulate:\n\n\n# e.g.\ngeyser <- reticulate::py_load_object(\"geyser_train_test.pkl\")\n\n\nHere is the data preparation code for the first dataset, geyser - all other datasets were treated the same way.\n\n\n# the first 10000 measurements from the compilation provided by Gilpin\ngeyser <- read_csv(\"geyser.csv\", col_names = FALSE) %>% select(X1) %>% pull() %>% unclass()\n\n# standardize\ngeyser <- scale(geyser)\n\n# varies per dataset; see below \nn_timesteps <- 60\nbatch_size <- 32\n\n# transform into [batch_size, timesteps, features] format required by RNNs\ngen_timesteps <- function(x, n_timesteps) {\n  do.call(rbind,\n          purrr::map(seq_along(x),\n                     function(i) {\n                       start <- i\n                       end <- i + n_timesteps - 1\n                       out <- x[start:end]\n                       out\n                     })\n  ) %>%\n    na.omit()\n}\n\nn <- 10000\ntrain <- gen_timesteps(geyser[1:(n/2)], 2 * n_timesteps)\ntest <- gen_timesteps(geyser[(n/2):n], 2 * n_timesteps) \n\ndim(train) <- c(dim(train), 1)\ndim(test) <- c(dim(test), 1)\n\n# split into input and target  \nx_train <- train[ , 1:n_timesteps, , drop = FALSE]\ny_train <- train[ , (n_timesteps + 1):(2*n_timesteps), , drop = FALSE]\n\nx_test <- test[ , 1:n_timesteps, , drop = FALSE]\ny_test <- test[ , (n_timesteps + 1):(2*n_timesteps), , drop = FALSE]\n\n# create tfdatasets\nds_train <- tensor_slices_dataset(list(x_train, y_train)) %>%\n  dataset_shuffle(nrow(x_train)) %>%\n  dataset_batch(batch_size)\n\nds_test <- tensor_slices_dataset(list(x_test, y_test)) %>%\n  dataset_batch(nrow(x_test))\n\n\nNow we’re ready to look at how forecasting goes on our four datasets.\nExperiments\nGeyser dataset\nPeople working with time series may have heard of Old Faithful, a geyser in\nWyoming, US that has continually been erupting every 44 minutes to two hours since the year 2004. For the subset of data\nGilpin extracted3,\n\ngeyser_train_test.pkl corresponds to detrended temperature readings from the main runoff pool of the Old Faithful geyser\nin Yellowstone National Park, downloaded from the GeyserTimes database. Temperature measurements\nstart on April 13, 2015 and occur in one-minute increments.\n\nLike we said above, geyser.csv is a subset of these measurements, comprising the first 10000 data points. To choose an\nadequate timestep for the LSTMs, we inspect the series at various resolutions:\n\n\n\nFigure 1: Geyer dataset. Top: First 1000 observations. Bottom: Zooming in on the first 200.\n\n\n\nIt seems like the behavior is periodic with a period of about 40-50; a timestep of 60 thus seemed like a good try.\nHaving trained both FNN-LSTM and the vanilla LSTM for 200 epochs, we first inspect the variances of the latent variables on\nthe test set. The value of fnn_multiplier corresponding to this run was 0.7.\n\n\ntest_batch <- as_iterator(ds_test) %>% iter_next()\nencoded <- encoder(test_batch[[1]]) %>%\n  as.array() %>%\n  as_tibble()\n\nencoded %>% summarise_all(var)\n\n\n   V1     V2        V3          V4       V5       V6       V7       V8       V9      V10\n0.258 0.0262 0.0000627 0.000000600 0.000533 0.000362 0.000238 0.000121 0.000518 0.000365\nThere is a drop in importance between the first two variables and the rest; however, unlike in the Lorenz system, V1 and\nV2 variances also differ by an order of magnitude.\nNow, it’s interesting to compare prediction errors for both models. We are going to make an observation that will carry\nthrough to all three datasets to come.\nKeeping up the suspense for a while, here is the code used to compute per-timestep prediction errors from both models. The\nsame code will be used for all other datasets.\n\n\ncalc_mse <- function(df, y_true, y_pred) {\n  (sum((df[[y_true]] - df[[y_pred]])^2))/nrow(df)\n}\n\nget_mse <- function(test_batch, prediction) {\n  \n  comp_df <- \n    data.frame(\n      test_batch[[2]][, , 1] %>%\n        as.array()) %>%\n        rename_with(function(name) paste0(name, \"_true\")) %>%\n    bind_cols(\n      data.frame(\n        prediction[, , 1] %>%\n          as.array()) %>%\n          rename_with(function(name) paste0(name, \"_pred\")))\n  \n  mse <- purrr::map(1:dim(prediction)[2],\n                        function(varno)\n                          calc_mse(comp_df,\n                                   paste0(\"X\", varno, \"_true\"),\n                                   paste0(\"X\", varno, \"_pred\"))) %>%\n    unlist()\n  \n  mse\n}\n\nprediction_fnn <- decoder(encoder(test_batch[[1]]))\nmse_fnn <- get_mse(test_batch, prediction_fnn)\n\nprediction_lstm <- model %>% predict(ds_test)\nmse_lstm <- get_mse(test_batch, prediction_lstm)\n\nmses <- data.frame(timestep = 1:n_timesteps, fnn = mse_fnn, lstm = mse_lstm) %>%\n  gather(key = \"type\", value = \"mse\", -timestep)\n\nggplot(mses, aes(timestep, mse, color = type)) +\n  geom_point() +\n  scale_color_manual(values = c(\"#00008B\", \"#3CB371\")) +\n  theme_classic() +\n  theme(legend.position = \"none\") \n\n\nAnd here is the actual comparison. One thing especially jumps to the eye: FNN-LSTM forecast error is significantly lower for\ninitial timesteps, first and foremost, for the very first prediction, which from this graph we expect to be pretty good!\n\n\n\nFigure 2: Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM.\n\n\n\nInterestingly, we see “jumps” in prediction error, for FNN-LSTM, between the very first forecast and the second, and then\nbetween the second and the ensuing ones, reminding of the similar jumps in variable importance for the latent code! After the\nfirst ten timesteps, vanilla LSTM has caught up with FNN-LSTM, and we won’t interpret further development of the losses based\non just a single run’s output.\nInstead, let’s inspect actual predictions. We randomly pick sequences from the test set, and ask both FNN-LSTM and vanilla\nLSTM for a forecast. The same procedure will be followed for the other datasets.\n\n\ngiven <- data.frame(as.array(tf$concat(list(\n  test_batch[[1]][, , 1], test_batch[[2]][, , 1]\n),\naxis = 1L)) %>% t()) %>%\n  add_column(type = \"given\") %>%\n  add_column(num = 1:(2 * n_timesteps))\n\nfnn <- data.frame(as.array(prediction_fnn[, , 1]) %>%\n                    t()) %>%\n  add_column(type = \"fnn\") %>%\n  add_column(num = (n_timesteps  + 1):(2 * n_timesteps))\n\nlstm <- data.frame(as.array(prediction_lstm[, , 1]) %>%\n                     t()) %>%\n  add_column(type = \"lstm\") %>%\n  add_column(num = (n_timesteps + 1):(2 * n_timesteps))\n\ncompare_preds_df <- bind_rows(given, lstm, fnn)\n\nplots <- \n  purrr::map(sample(1:dim(compare_preds_df)[2], 16),\n             function(v) {\n               ggplot(compare_preds_df, aes(num, .data[[paste0(\"X\", v)]], color = type)) +\n                 geom_line() +\n                 theme_classic() +\n                 theme(legend.position = \"none\", axis.title = element_blank()) +\n                 scale_color_manual(values = c(\"#00008B\", \"#DB7093\", \"#3CB371\"))\n             })\n\nplot_grid(plotlist = plots, ncol = 4)\n\n\nHere are sixteen random picks of predictions on the test set. The ground truth is displayed in pink; blue forecasts are from\nFNN-LSTM, green ones from vanilla LSTM.\n\n\n\nFigure 3: 60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth.\n\n\n\nWhat we expect from the error inspection comes true: FNN-LSTM yields significantly better predictions for immediate\ncontinuations of a given sequence.\nLet’s move on to the second dataset on our list.\nElectricity dataset\nThis is a dataset on power consumption, aggregated over 321 different households and fifteen-minute-intervals.\n\nelectricity_train_test.pkl corresponds to average power consumption by 321 Portuguese households between 2012 and 2014, in\nunits of kilowatts consumed in fifteen minute increments. This dataset is from the UCI machine learning\ndatabase.4\n\nHere, we see a very regular pattern:\n\n\n\nFigure 4: Electricity dataset. Top: First 2000 observations. Bottom: Zooming in on 500 observations, skipping the very beginning of the series.\n\n\n\nWith such regular behavior, we immediately tried to predict a higher number of timesteps (120) – and didn’t have to retract\nbehind that aspiration.\nFor an fnn_multiplier of 0.5, latent variable variances look like this:\nV1          V2            V3       V4       V5            V6       V7         V8      V9     V10\n0.390 0.000637 0.00000000288 1.48e-10 2.10e-11 0.00000000119 6.61e-11 0.00000115 1.11e-4 1.40e-4\nWe definitely see a sharp drop already after the first variable.\nHow do prediction errors compare on the two architectures?\n\n\n\nFigure 5: Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM.\n\n\n\nHere, FNN-LSTM performs better over a long range of timesteps, but again, the difference is most visible for immediate\npredictions. Will an inspection of actual predictions confirm this view?\n\n\n\nFigure 6: 60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth.\n\n\n\nIt does! In fact, forecasts from FNN-LSTM are very impressive on all time scales.\nNow that we’ve seen the easy and predictable, let’s approach the weird and difficult.\nECG dataset\nSays Gilpin,\n\necg_train.pkl and ecg_test.pkl correspond to ECG measurements for two different patients, taken from the PhysioNet QT\ndatabase.5\n\nHow do these look?\n\n\n\nFigure 7: ECG dataset. Top: First 1000 observations. Bottom: Zooming in on the first 400 observations.\n\n\n\nTo the layperson that I am, these do not look nearly as regular as expected. First experiments showed that both architectures\nare not capable of dealing with a high number of timesteps. In every try, FNN-LSTM performed better for the very first\ntimestep.\nThis is also the case for n_timesteps = 12, the final try (after 120, 60 and 30). With an fnn_multiplier of 1, the\nlatent variances obtained amounted to the following:\n     V1        V2          V3        V4         V5       V6       V7         V8         V9       V10\n  0.110  1.16e-11     3.78e-9 0.0000992    9.63e-9  4.65e-5  1.21e-4    9.91e-9    3.81e-9   2.71e-8\nThere is a gap between the first variable and all other ones; but not much variance is explained by V1 either.\nApart from the very first prediction, vanilla LSTM shows lower forecast errors this time; however, we have to add that this\nwas not consistently observed when experimenting with other timestep settings.\n\n\n\nFigure 8: Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM.\n\n\n\nLooking at actual predictions, both architectures perform best when a persistence forecast is adequate – in fact, they\nproduce one even when it is not.\n\n\n\nFigure 9: 60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth.\n\n\n\nOn this dataset, we certainly would want to explore other architectures better able to capture the presence of high and low\nfrequencies in the data, such as mixture models. But – were we forced to stay with one of these, and could do a\none-step-ahead, rolling forecast, we’d go with FNN-LSTM.\nSpeaking of mixed frequencies – we haven’t seen the extremes yet …\nMouse dataset\n“Mouse,” that’s spike rates recorded from a mouse thalamus.\n\nmouse.pkl A time series of spiking rates for a neuron in a mouse thalamus. Raw spike data was obtained from\nCRCNS and processed with the authors' code in order to generate a\nspike rate time series.6\n\n\n\n\nFigure 10: Mouse dataset. Top: First 2000 observations. Bottom: Zooming in on the first 500 observations.\n\n\n\nObviously, this dataset will be very hard to predict. How, after “long” silence, do you know that a neuron is going to fire?\nAs usual, we inspect latent code variances (fnn_multiplier was set to 0.4):\n\n     V1       V2        V3         V4       V5       V6        V7      V8       V9        V10\n 0.0796  0.00246  0.000214    2.26e-7   .71e-9  4.22e-8  6.45e-10 1.61e-4 2.63e-10    2.05e-8\n>\n\nAgain, we don’t see the first variable explaining much variance. Still, interestingly, when inspecting forecast errors we get\na picture very similar to the one obtained on our first, geyser, dataset:\n\n\n\nFigure 11: Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM.\n\n\n\nSo here, the latent code definitely seems to help! With every timestep “more” that we try to predict, prediction performance\ngoes down continuously – or put the other way round, short-time predictions are expected to be pretty good!\nLet’s see:\n\n\n\nFigure 12: 60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth.\n\n\n\nIn fact on this dataset, the difference in behavior between both architectures is striking. When nothing is “supposed to\nhappen,” vanilla LSTM produces “flat” curves at about the mean of the data, while FNN-LSTM takes the effort to “stay on track”\nas long as possible before also converging to the mean. Choosing FNN-LSTM – had we to choose one of these two – would be an\nobvious decision with this dataset.\nDiscussion\nWhen, in timeseries forecasting, would we consider FNN-LSTM? Judging by the above experiments, conducted on four very different\ndatasets: Whenever we consider a deep learning approach. Of course, this has been a casual exploration – and it was meant to\nbe, as – hopefully – was evident from the nonchalant and bloomy (sometimes) writing style.\nThroughout the text, we’ve emphasized utility – how could this technique be used to improve predictions? But, looking at\nthe above results, a number of interesting questions come to mind. We already speculated (though in an indirect way) whether\nthe number of high-variance variables in the latent code was relatable to how far we could sensibly forecast into the future.\nHowever, even more intriguing is the question of how characteristics of the dataset itself affect FNN efficiency.\nSuch characteristics could be:\nHow nonlinear is the dataset? (Put differently, how incompatible, as indicated by some form of test algorithm, is it with\nthe hypothesis that the data generation mechanism was a linear one?)\nTo what degree does the system appear to be sensitively dependent on initial conditions? In other words, what is the value\nof its (estimated, from the observations) highest Lyapunov exponent?\nWhat is its (estimated) dimensionality, for example, in terms of correlation\ndimension?\nWhile it is easy to obtain those estimates, using, for instance, the\nnonlinearTseries package explicitly modeled after practices\ndescribed in Kantz & Schreiber’s classic (Kantz and Schreiber 2004), we don’t want to extrapolate from our tiny sample of datasets, and leave\nsuch explorations and analyses to further posts, and/or the interested reader’s ventures :-). In any case, we hope you enjoyed\nthe demonstration of practical usability of an approach that in the preceding post, was mainly introduced in terms of its\nconceptual attractivity.\nThanks for reading!\n\n\n\nGilpin, William. 2020. “Deep Reconstruction of Strange Attractors from Time Series.” https://arxiv.org/abs/2002.05909.\n\n\nGrassberger, Peter, and Itamar Procaccia. 1983. “Measuring the Strangeness of Strange Attractors.” Physica D: Nonlinear Phenomena 9 (1): 189–208. https://doi.org/https://doi.org/10.1016/0167-2789(83)90298-1.\n\n\nKantz, Holger, and Thomas Schreiber. 2004. Nonlinear Time Series Analysis. Cambridge University Press.\n\n\nSauer, Tim, James A. Yorke, and Martin Casdagli. 1991. “Embedology.” Journal of Statistical Physics 65 (3-4): 579–616. https://doi.org/10.1007/BF01053745.\n\n\nPlease refer to the aforementioned predecessor post for a detailed introduction.↩︎\n“Basically” because FNN-LSTM technically has three LSTMs – the third one, with n_latent = 10 units, being used to\nstore the latent code.↩︎\nsee dataset descriptions in the repository's README↩︎\nagain, citing from Gilpin’s repository’s README.↩︎\nagain, citing from Gilpin’s repository’s README.↩︎\nagain, citing from Gilpin’s repository’s README.↩︎\n",
    "preview": "posts/2020-07-20-fnn-lstm/images/old_faithful.jpg",
    "last_modified": "2024-11-21T15:48:28+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-07-16-sparklyr-1.3.0-released/",
    "title": "sparklyr 1.3: Higher-order Functions, Avro and Custom Serializers",
    "description": "Sparklyr 1.3 is now available, featuring exciting new functionalities such as integration of Spark higher-order functions and data import/export in Avro and in user-defined serialization formats.",
    "author": [
      {
        "name": "Yitao Li",
        "url": "https://github.com/yl790"
      }
    ],
    "date": "2020-07-16",
    "categories": [
      "Packages/Releases",
      "Distributed Computing"
    ],
    "contents": "\nsparklyr 1.3 is now available on CRAN, with the following major new features:\nHigher-order Functions to easily manipulate arrays and structs\nSupport for Apache Avro, a row-oriented data serialization framework\nCustom Serialization using R functions to read and write any data format\nOther Improvements such as compatibility with EMR 6.0 & Spark 3.0, and initial support for Flint time series library\nTo install sparklyr 1.3 from CRAN, run\n\n\ninstall.packages(\"sparklyr\")\n\n\nIn this post, we shall highlight some major new features introduced in sparklyr 1.3, and showcase scenarios where such features come in handy. While a number of enhancements and bug fixes (especially those related to spark_apply(), Apache Arrow, and secondary Spark connections) were also an important part of this release, they will not be the topic of this post, and it will be an easy exercise for the reader to find out more about them from the sparklyr NEWS file.\nHigher-order Functions\nHigher-order functions are built-in Spark SQL constructs that allow user-defined lambda expressions to be applied efficiently to complex data types such as arrays and structs. As a quick demo to see why higher-order functions are useful, let’s say one day Scrooge McDuck dove into his huge vault of money and found large quantities of pennies, nickels, dimes, and quarters. Having an impeccable taste in data structures, he decided to store the quantities and face values of everything into two Spark SQL array columns:\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\", version = \"2.4.5\")\ncoins_tbl <- copy_to(\n  sc,\n  tibble::tibble(\n    quantities = list(c(4000, 3000, 2000, 1000)),\n    values = list(c(1, 5, 10, 25))\n  )\n)\n\n\nThus declaring his net worth of 4k pennies, 3k nickels, 2k dimes, and 1k quarters. To help Scrooge McDuck calculate the total value of each type of coin in sparklyr 1.3 or above, we can apply hof_zip_with(), the sparklyr equivalent of ZIP_WITH, to quantities column and values column, combining pairs of elements from arrays in both columns. As you might have guessed, we also need to specify how to combine those elements, and what better way to accomplish that than a concise one-sided formula   ~ .x * .y   in R, which says we want (quantity * value) for each type of coin? So, we have the following:\n\n\nresult_tbl <- coins_tbl %>%\n  hof_zip_with(~ .x * .y, dest_col = total_values) %>%\n  dplyr::select(total_values)\n\nresult_tbl %>% dplyr::pull(total_values)\n\n\n[1]  4000 15000 20000 25000\nWith the result 4000 15000 20000 25000 telling us there are in total $40 dollars worth of pennies, $150 dollars worth of nickels, $200 dollars worth of dimes, and $250 dollars worth of quarters, as expected.\nUsing another sparklyr function named hof_aggregate(), which performs an AGGREGATE operation in Spark, we can then compute the net worth of Scrooge McDuck based on result_tbl, storing the result in a new column named total. Notice for this aggregate operation to work, we need to ensure the starting value of aggregation has data type (namely, BIGINT) that is consistent with the data type of total_values (which is ARRAY<BIGINT>), as shown below:\n\n\nresult_tbl %>%\n  dplyr::mutate(zero = dplyr::sql(\"CAST (0 AS BIGINT)\")) %>%\n  hof_aggregate(start = zero, ~ .x + .y, expr = total_values, dest_col = total) %>%\n  dplyr::select(total) %>%\n  dplyr::pull(total)\n\n\n[1] 64000\nSo Scrooge McDuck’s net worth is $640 dollars.\nOther higher-order functions supported by Spark SQL so far include transform, filter, and exists, as documented in here, and similar to the example above, their counterparts (namely, hof_transform(), hof_filter(), and hof_exists()) all exist in sparklyr 1.3, so that they can be integrated with other dplyr verbs in an idiomatic manner in R.\nAvro\nAnother highlight of the sparklyr 1.3 release is its built-in support for Avro data sources. Apache Avro is a widely used data serialization protocol that combines the efficiency of a binary data format with the flexibility of JSON schema definitions. To make working with Avro data sources simpler, in sparklyr 1.3, as soon as a Spark connection is instantiated with spark_connect(..., package = \"avro\"), sparklyr will automatically figure out which version of spark-avro package to use with that connection, saving a lot of potential headaches for sparklyr users trying to determine the correct version of spark-avro by themselves. Similar to how spark_read_csv() and spark_write_csv() are in place to work with CSV data, spark_read_avro() and spark_write_avro() methods were implemented in sparklyr 1.3 to facilitate reading and writing Avro files through an Avro-capable Spark connection, as illustrated in the example below:\n\n\nlibrary(sparklyr)\n\n# The `package = \"avro\"` option is only supported in Spark 2.4 or higher\nsc <- spark_connect(master = \"local\", version = \"2.4.5\", package = \"avro\")\n\nsdf <- sdf_copy_to(\n  sc,\n  tibble::tibble(\n    a = c(1, NaN, 3, 4, NaN),\n    b = c(-2L, 0L, 1L, 3L, 2L),\n    c = c(\"a\", \"b\", \"c\", \"\", \"d\")\n  )\n)\n\n# This example Avro schema is a JSON string that essentially says all columns\n# (\"a\", \"b\", \"c\") of `sdf` are nullable.\navro_schema <- jsonlite::toJSON(list(\n  type = \"record\",\n  name = \"topLevelRecord\",\n  fields = list(\n    list(name = \"a\", type = list(\"double\", \"null\")),\n    list(name = \"b\", type = list(\"int\", \"null\")),\n    list(name = \"c\", type = list(\"string\", \"null\"))\n  )\n), auto_unbox = TRUE)\n\n# persist the Spark data frame from above in Avro format\nspark_write_avro(sdf, \"/tmp/data.avro\", as.character(avro_schema))\n\n# and then read the same data frame back\nspark_read_avro(sc, \"/tmp/data.avro\")\n\n\n# Source: spark<data> [?? x 3]\n      a     b c\n  <dbl> <int> <chr>\n  1     1    -2 \"a\"\n  2   NaN     0 \"b\"\n  3     3     1 \"c\"\n  4     4     3 \"\"\n  5   NaN     2 \"d\"\n\nCustom Serialization\nIn addition to commonly used data serialization formats such as CSV, JSON, Parquet, and Avro, starting from sparklyr 1.3, customized data frame serialization and deserialization procedures implemented in R can also be run on Spark workers via the newly implemented spark_read() and spark_write() methods. We can see both of them in action through a quick example below, where saveRDS() is called from a user-defined writer function to save all rows within a Spark data frame into 2 RDS files on disk, and readRDS() is called from a user-defined reader function to read the data from the RDS files back to Spark:\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\nsdf <- sdf_len(sc, 7)\npaths <- c(\"/tmp/file1.RDS\", \"/tmp/file2.RDS\")\n\nspark_write(sdf, writer = function(df, path) saveRDS(df, path), paths = paths)\nspark_read(sc, paths, reader = function(path) readRDS(path), columns = c(id = \"integer\"))\n\n\n# Source: spark<?> [?? x 1]\n     id\n  <int>\n1     1\n2     2\n3     3\n4     4\n5     5\n6     6\n7     7\nOther Improvements\nSparklyr.flint\nSparklyr.flint is a sparklyr extension that aims to make functionalities from the Flint time-series library easily accessible from R. It is currently under active development. One piece of good news is that, while the original Flint library was designed to work with Spark 2.x, a slightly modified fork of it will work well with Spark 3.0, and within the existing sparklyr extension framework. sparklyr.flint can automatically determine which version of the Flint library to load based on the version of Spark it’s connected to. Another bit of good news is, as previously mentioned, sparklyr.flint doesn’t know too much about its own destiny yet. Maybe you can play an active part in shaping its future!\nEMR 6.0\nThis release also features a small but important change that allows sparklyr to correctly connect to the version of Spark 2.4 that is included in Amazon EMR 6.0.\nPreviously, sparklyr automatically assumed any Spark 2.x it was connecting to was built with Scala 2.11 and attempted to load any required Scala artifacts built with Scala 2.11 as well. This became problematic when connecting to Spark 2.4 from Amazon EMR 6.0, which is built with Scala 2.12. Starting from sparklyr 1.3, such problem can be fixed by simply specifying scala_version = \"2.12\" when calling spark_connect() (e.g., spark_connect(master = \"yarn-client\", scala_version = \"2.12\")).\nSpark 3.0\nLast but not least, it is worthwhile to mention sparklyr 1.3.0 is known to be fully compatible with the recently released Spark 3.0. We highly recommend upgrading your copy of sparklyr to 1.3.0 if you plan to have Spark 3.0 as part of your data workflow in future.\nAcknowledgement\nIn chronological order, we want to thank the following individuals for submitting pull requests towards sparklyr 1.3:\nJozef Hajnala\nHossein Falaki\nSamuel Macêdo\nYitao Li\nAndy Zhang\nJavier Luraschi\nNeal Richardson\nWe are also grateful for valuable input on the sparklyr 1.3 roadmap, #2434, and #2551 from [@javierluraschi](https://github.com/javierluraschi), and great spiritual advice on #1773 and #2514 from @mattpollock and @benmwhite.\nPlease note if you believe you are missing from the acknowledgement above, it may be because your contribution has been considered part of the next sparklyr release rather than part of the current release. We do make every effort to ensure all contributors are mentioned in this section. In case you believe there is a mistake, please feel free to contact the author of this blog post via e-mail (yitao at rstudio dot com) and request a correction.\nIf you wish to learn more about sparklyr, we recommend visiting sparklyr.ai, spark.rstudio.com, and some of the previous release posts such as sparklyr 1.2 and sparklyr 1.1.\nThanks for reading!\n\n\n\n",
    "preview": "posts/2020-07-16-sparklyr-1.3.0-released/images/sparklyr-1.3.jpg",
    "last_modified": "2024-11-21T15:52:11+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-06-24-deep-attractors/",
    "title": "Deep attractors: Where deep learning meets chaos",
    "description": "In nonlinear dynamics, when the state space is thought to be multidimensional but all we have for data is just a univariate time series, one may attempt to reconstruct the true space via delay coordinate embeddings. However, it is not clear a priori how to choose dimensionality and time lag of the reconstruction space. In this post, we show how to use an autoencoder architecture to circumvent the problem: Given just a scalar series of observations, the autoencoder directly learns to represent attractors of chaotic systems in adequate dimensionality.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-06-24",
    "categories": [
      "R",
      "TensorFlow/Keras",
      "Time Series",
      "Generative Models"
    ],
    "contents": "\nFor us deep learning practitioners, the world is – not flat, but – linear, mostly. Or piecewise linear.1 Like other\nlinear approximations, or maybe even more so, deep learning can be incredibly successful at making predictions. But let’s\nadmit it – sometimes we just miss the thrill of the nonlinear, of good, old, deterministic-yet-unpredictable chaos. Can we\nhave both? It looks like we can. In this post, we’ll see an application of deep learning (DL) to nonlinear time series\nprediction – or rather, the essential step that predates it: reconstructing the attractor underlying its dynamics. While this\npost is an introduction, presenting the topic from scratch, further posts will build on this and extrapolate to observational\ndatasets.\nWhat to expect from this post\nIn his 2020 paper Deep reconstruction of strange attractors from time series (Gilpin 2020), William Gilpin uses an\nautoencoder architecture, combined with a regularizer implementing the false nearest neighbors statistic\n(Kennel, Brown, and Abarbanel 1992), to reconstruct attractors from univariate observations of multivariate, nonlinear dynamical systems. If\nyou feel you completely understand the sentence you just read, you may as well directly jump to the paper – come back for the\ncode though2. If, on the other hand, you’re more familiar with the chaos on your desk (extrapolating … apologies) than\nchaos theory chaos, read on. Here, we’ll first go into what it’s all about3, and then, show an example application,\nfeaturing Edward Lorenz’s famous butterfly attractor. While this initial post is primarily supposed to be a fun introduction\nto a fascinating topic, we hope to follow up with applications to real-world datasets in the future.\nRabbits, butterflies, and low-dimensional projections: Our problem statement in context\nIn curious misalignment with how we use “chaos” in day-to-day language, chaos, the technical concept, is very different from\nstochasticity, or randomness. Chaos may emerge from purely deterministic processes - very simplistic ones, even. Let’s see\nhow; with rabbits.\nRabbits, or: Sensitive dependence on initial conditions\nYou may be familiar with the logistic equation, used as a toy model for population growth. Often it’s written like this –\nwith \\(x\\) being the size of the population, expressed as a fraction of the maximal size (a fraction of possible rabbits, thus),\nand \\(r\\) being the growth rate (the rate at which rabbits reproduce):\n\\[\nx_{n + 1} = r \\ x_n \\ (1 - x_n)\n\\]\nThis equation describes an iterated map over discrete timesteps \\(n\\). Its repeated application results in a trajectory\ndescribing how the population of rabbits evolves. Maps can have fixed points, states where further function application goes\non producing the same result forever. Example-wise, say the growth rate amounts to \\(2.1\\), and we start at two (pretty\ndifferent!) initial values, \\(0.3\\) and \\(0.8\\). Both trajectories arrive at a fixed point – the same fixed point – in fewer\nthan 10 iterations. Were we asked to predict the population size after a hundred iterations, we could make a very confident\nguess, whatever the of starting value. (If the initial value is \\(0\\), we stay at \\(0\\), but we can be pretty certain of that as\nwell.)\n\n\n\nFigure 1: Trajectory of the logistic map for r = 2.1 and two different initial values.\n\n\n\nWhat if the growth rate were somewhat higher, at \\(3.3\\), say? Again, we immediately compare trajectories resulting from initial\nvalues \\(0.3\\) and \\(0.9\\):\n\n\n\nFigure 2: Trajectory of the logistic map for r = 3.3 and two different initial values.\n\n\n\nThis time, don’t see a single fixed point, but a two-cycle: As the trajectories stabilize, population size inevitably is at\none of two possible values – either too many rabbits or too few, you could say. The two trajectories are phase-shifted, but\nagain, the attracting values – the attractor – is shared by both initial conditions. So still, predictability is pretty\nhigh. But we haven’t seen everything yet.\nLet’s again enhance the growth rate some. Now this (literally) is chaos:\n\n\n\nFigure 3: Trajectory of the logistic map for r = 3.6 and two different initial values, 0.3 and 0.9.\n\n\n\nEven after a hundred iterations, there is no set of values the trajectories recur to. We can’t be confident about any\nprediction we might make.\nOr can we? After all, we have the governing equation, which is deterministic. So we should be able to calculate the size of\nthe population at, say, time \\(150\\)? In principle, yes; but this presupposes we have an accurate measurement for the starting\nstate.\nHow accurate? Let’s compare trajectories for initial values \\(0.3\\) and \\(0.301\\):\n\n\n\nFigure 4: Trajectory of the logistic map for r = 3.6 and two different initial values, 0.3 and 0.301.\n\n\n\nAt first, trajectories seem to jump around in unison; but during the second dozen iterations already, they dissociate more and\nmore, and increasingly, all bets are off. What if initial values are really close, as in, \\(0.3\\) vs. \\(0.30000001\\)?\nIt just takes a bit longer for the disassociation to surface.\n\n\n\nFigure 5: Trajectory of the logistic map for r = 3.6 and two different initial values, 0.3 and 0.30000001.\n\n\n\nWhat we’re seeing here is sensitive dependence on initial conditions, an essential precondition for a system to be chaotic.\nIn an nutshell: Chaos arises when a deterministic system shows sensitive dependence on initial conditions. Or as Edward\nLorenz is said to have put it,\n\nWhen the present determines the future, but the approximate present does not approximately determine the future.\n\nNow if these unstructured, random-looking point clouds constitute chaos, what with the all-but-amorphous butterfly (to be\ndisplayed very soon)?\nButterflies, or: Attractors and strange attractors\nActually, in the context of chaos theory, the term butterfly may be encountered in different contexts.\nFirstly, as so-called “butterfly effect,” it is an instantiation of the templatic phrase “the flap of a butterfly’s wing in\n_________ profoundly affects the course of the weather in _________.”4 In this usage, it is mostly a\nmetaphor for sensitive dependence on initial conditions.\nSecondly, the existence of this metaphor led to a Rorschach-test-like identification with two-dimensional visualizations of\nattractors of the Lorenz system. The Lorenz system is a set of three first-order differential equations designed to describe\natmospheric convection:\n\\[\n\\begin{aligned}\n& \\frac{dx}{dt} = \\sigma (y - x)\\\\\n& \\frac{dy}{dt} = \\rho x - x z - y\\\\\n& \\frac{dz}{dt} = x y - \\beta z\n\\end{aligned}\n\\]\nThis set of equations is nonlinear, as required for chaotic behavior to appear. It also has the required dimensionality, which\nfor smooth, continuous systems, is at least 35. Whether we actually see chaotic attractors – among which, the butterfly –\ndepends on the settings of the parameters \\(\\sigma\\), \\(\\rho\\) and \\(\\beta\\). For the values conventionally chosen, \\(\\sigma=10\\),\n\\(\\rho=28\\), and \\(\\beta=8/3\\) , we see it when projecting the trajectory on the \\(x\\) and \\(z\\) axes:\n\n\n\nFigure 6: Two-dimensional projections of the Lorenz attractor for sigma = 10, rho = 28, beta = 8 / 3. On the right: the butterfly.\n\n\n\nThe butterfly is an attractor (as are the other two projections), but it is neither a point nor a cycle. It is an attractor\nin the sense that starting from a variety of different initial values, we end up in some sub-region of the state space, and we\ndon’t get to escape no more. This is easier to see when watching evolution over time, as in this animation:\n\n\n<img src=“images/x_z.gif” alt=“How the Lorenz attractor traces out the famous”butterfly” shape.” />\n\nFigure 7: How the Lorenz attractor traces out the famous “butterfly” shape.\n\n\n\nNow, to plot the attractor in two dimensions, we threw away the third. But in “real life,” we don’t usually have too much\ninformation (although it may sometimes seem like we had). We might have a lot of measurements, but these don’t usually reflect\nthe actual state variables we’re interested in. In these cases, we may want to actually add information.\nEmbeddings (as a non-DL term), or: Undoing the projection\nAssume that instead of all three variables of the Lorenz system, we had measured just one: \\(x\\), the rate of convection. Often\nin nonlinear dynamics, the technique of delay coordinate embedding (Sauer, Yorke, and Casdagli 1991) is used to enhance a series of univariate\nmeasurements.\nIn this method – or family of methods – the univariate series is augmented by time-shifted copies of itself. There are two\ndecisions to be made: How many copies to add, and how big the delay should be. To illustrate, if we had a scalar series,\n1 2 3 4 5 6 7 8 9 10 11 ...\na three-dimensional embedding with time delay 2 would look like this:\n1 3 5\n2 4 6\n3 5 7\n4 6 8\n5 7 9\n6 8 10\n7 9 11\n...\nOf the two decisions to be made – number of shifted series and time lag – the first is a decision on the dimensionality of\nthe reconstruction space. Various theorems, such as Taken's theorem,\nindicate bounds on the number of dimensions required, provided the dimensionality of the true state space is known – which,\nin real-world applications, often is not the case.The second has been of little interest to mathematicians, but is important\nin practice. In fact, Kantz and Schreiber (Kantz and Schreiber 2004) argue that in practice, it is the product of both parameters that matters,\nas it indicates the time span represented by an embedding vector.\nHow are these parameters chosen? Regarding reconstruction dimensionality, the reasoning goes that even in chaotic systems,\npoints that are close in state space at time \\(t\\) should still be close at time \\(t + \\Delta t\\), provided \\(\\Delta t\\) is very\nsmall. So say we have two points that are close, by some metric, when represented in two-dimensional space. But in three\ndimensions, that is, if we don’t “project away” the third dimension, they are a lot more distant. As illustrated in\n(Gilpin 2020):\n\n\n\nFigure 8: In the two-dimensional projection on axes x and y, the red points are close neighbors. In 3d, however, they are separate. Compare with the blue points, which stay close even in higher-dimensional space. Figure from Gilpin (2020).\n\n\n\nIf this happens, then projecting down has eliminated some essential information. In 2d, the points were false neighbors. The\nfalse nearest neighbors (FNN) statistic can be used to determine an adequate embedding size, like this:\nFor each point, take its closest neighbor in \\(m\\) dimensions, and compute the ratio of their distances in \\(m\\) and \\(m+1\\)\ndimensions. If the ratio is larger than some threshold \\(t\\), the neighbor was false. Sum the number of false neighbors over all\npoints. Do this for different \\(m\\) and \\(t\\), and inspect the resulting curves.\nAt this point, let’s look ahead at the autoencoder approach. The autoencoder will use that same FNN statistic as a\nregularizer, in addition to the usual autoencoder reconstruction loss. This will result in a new heuristic regarding embedding\ndimensionality that involves fewer decisions.\nGoing back to the classic method for an instant, the second parameter, the time lag, is even more difficult to sort out\n(Kantz and Schreiber 2004). Usually, mutual information is plotted for different delays and then, the first delay where it falls below some\nthreshold is chosen. We don’t further elaborate on this question as it is rendered obsolete in the neural network approach.\nWhich we’ll see now.\nLearning the Lorenz attractor\nOur code closely follows the architecture, parameter settings, and data setup used in the reference\nimplementation William provided. The loss function, especially, has been ported\none-to-one.\nThe general idea is the following. An autoencoder – for example, an LSTM autoencoder as presented here – is used to compress\nthe univariate time series into a latent representation of some dimensionality, which will constitute an upper bound on the\ndimensionality of the learned attractor. In addition to mean squared error between input and reconstructions, there will be a\nsecond loss term, applying the FNN regularizer. This results in the latent units being roughly ordered by importance, as\nmeasured by their variance. It is expected that somewhere in the listing of variances, a sharp drop will appear. The units\nbefore the drop are then assumed to encode the attractor of the system in question.\nIn this setup, there is still a choice to be made: how to weight the FNN loss. One would run training for different weights\n\\(\\lambda\\) and look for the drop. Surely, this could in principle be automated, but given the newness of the method – the\npaper was published this year – it makes sense to focus on thorough analysis first.\nData generation\nWe use the deSolve package to generate data from the Lorenz equations.\n\n\nlibrary(deSolve)\nlibrary(tidyverse)\n\nparameters <- c(sigma = 10,\n                rho = 28,\n                beta = 8/3)\n\ninitial_state <-\n  c(x = -8.60632853,\n    y = -14.85273055,\n    z = 15.53352487)\n\nlorenz <- function(t, state, parameters) {\n  with(as.list(c(state, parameters)), {\n    dx <- sigma * (y - x)\n    dy <- x * (rho - z) - y\n    dz <- x * y - beta * z\n    \n    list(c(dx, dy, dz))\n  })\n}\n\ntimes <- seq(0, 500, length.out = 125000)\n\nlorenz_ts <-\n  ode(\n    y = initial_state,\n    times = times,\n    func = lorenz,\n    parms = parameters,\n    method = \"lsoda\"\n  ) %>% as_tibble()\n\nlorenz_ts[1:10,]\n\n\n# A tibble: 10 x 4\n      time      x     y     z\n     <dbl>  <dbl> <dbl> <dbl>\n 1 0        -8.61 -14.9  15.5\n 2 0.00400  -8.86 -15.2  15.9\n 3 0.00800  -9.12 -15.6  16.3\n 4 0.0120   -9.38 -16.0  16.7\n 5 0.0160   -9.64 -16.3  17.1\n 6 0.0200   -9.91 -16.7  17.6\n 7 0.0240  -10.2  -17.0  18.1\n 8 0.0280  -10.5  -17.3  18.6\n 9 0.0320  -10.7  -17.7  19.1\n10 0.0360  -11.0  -18.0  19.7\nWe’ve already seen the attractor, or rather, its three two-dimensional projections, in figure 6 above. But now our scenario is\ndifferent. We only have access to \\(x\\), a univariate time series. As the time interval used to numerically integrate the\ndifferential equations was rather tiny, we just use every tenth observation.\n\n\nobs <- lorenz_ts %>%\n  select(time, x) %>%\n  filter(row_number() %% 10 == 0)\n\nggplot(obs, aes(time, x)) +\n  geom_line() +\n  coord_cartesian(xlim = c(0, 100)) +\n  theme_classic()\n\n\n\n\n\nFigure 9: Convection rates as a univariate time series.\n\n\n\nPreprocessing\nThe first half of the series is used for training. The data is scaled and transformed into the three-dimensional form expected\nby recurrent layers.\n\n\nlibrary(keras)\nlibrary(tfdatasets)\nlibrary(tfautograph)\nlibrary(reticulate)\nlibrary(purrr)\n\n# scale observations\nobs <- obs %>% mutate(\n  x = scale(x)\n)\n\n# generate timesteps\nn <- nrow(obs)\nn_timesteps <- 10\n\ngen_timesteps <- function(x, n_timesteps) {\n  do.call(rbind,\n          purrr::map(seq_along(x),\n             function(i) {\n               start <- i\n               end <- i + n_timesteps - 1\n               out <- x[start:end]\n               out\n             })\n  ) %>%\n    na.omit()\n}\n\n# train with start of time series, test with end of time series \nx_train <- gen_timesteps(as.matrix(obs$x)[1:(n/2)], n_timesteps)\nx_test <- gen_timesteps(as.matrix(obs$x)[(n/2):n], n_timesteps) \n\n# add required dimension for features (we have one)\ndim(x_train) <- c(dim(x_train), 1)\ndim(x_test) <- c(dim(x_test), 1)\n\n# some batch size (value not crucial)\nbatch_size <- 100\n\n# transform to datasets so we can use custom training\nds_train <- tensor_slices_dataset(x_train) %>%\n  dataset_batch(batch_size)\n\nds_test <- tensor_slices_dataset(x_test) %>%\n  dataset_batch(nrow(x_test))\n\n\nAutoencoder\nWith newer versions of TensorFlow (>= 2.0, certainly if >= 2.2), autoencoder-like models are best coded as custom models,\nand trained in an “autographed” loop.6\nThe encoder is centered around a single LSTM layer, whose size determines the maximum dimensionality of the attractor. The\ndecoder then undoes the compression – again, mainly using a single LSTM.\n\n\n# size of the latent code\nn_latent <- 10L\nn_features <- 1\n\nencoder_model <- function(n_timesteps,\n                          n_features,\n                          n_latent,\n                          name = NULL) {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$noise <- layer_gaussian_noise(stddev = 0.5)\n    self$lstm <-  layer_lstm(\n      units = n_latent,\n      input_shape = c(n_timesteps, n_features),\n      return_sequences = FALSE\n    ) \n    self$batchnorm <- layer_batch_normalization()\n    \n    function (x, mask = NULL) {\n      x %>%\n        self$noise() %>%\n        self$lstm() %>%\n        self$batchnorm() \n    }\n  })\n}\n\ndecoder_model <- function(n_timesteps,\n                          n_features,\n                          n_latent,\n                          name = NULL) {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$repeat_vector <- layer_repeat_vector(n = n_timesteps)\n    self$noise <- layer_gaussian_noise(stddev = 0.5)\n    self$lstm <- layer_lstm(\n        units = n_latent,\n        return_sequences = TRUE,\n        go_backwards = TRUE\n      ) \n    self$batchnorm <- layer_batch_normalization()\n    self$elu <- layer_activation_elu() \n    self$time_distributed <- time_distributed(layer = layer_dense(units = n_features))\n    \n    function (x, mask = NULL) {\n      x %>%\n        self$repeat_vector() %>%\n        self$noise() %>%\n        self$lstm() %>%\n        self$batchnorm() %>%\n        self$elu() %>%\n        self$time_distributed()\n    }\n  })\n}\n\n\nencoder <- encoder_model(n_timesteps, n_features, n_latent)\ndecoder <- decoder_model(n_timesteps, n_features, n_latent)\n\n\nLoss\nAs already explained above, the loss function we train with is twofold. On the one hand, we compare the original inputs with\nthe decoder outputs (the reconstruction), using mean squared error:\n\n\nmse_loss <- tf$keras$losses$MeanSquaredError(\n  reduction = tf$keras$losses$Reduction$SUM)\n\n\nIn addition, we try to keep the number of false neighbors small, by means of the following regularizer.7\n\n\nloss_false_nn <- function(x) {\n \n  # original values used in Kennel et al. (1992)\n  rtol <- 10 \n  atol <- 2\n  k_frac <- 0.01\n  \n  k <- max(1, floor(k_frac * batch_size))\n  \n  tri_mask <-\n    tf$linalg$band_part(\n      tf$ones(\n        shape = c(n_latent, n_latent),\n        dtype = tf$float32\n      ),\n      num_lower = -1L,\n      num_upper = 0L\n    )\n  \n   batch_masked <- tf$multiply(\n     tri_mask[, tf$newaxis,], x[tf$newaxis, reticulate::py_ellipsis()]\n   )\n  \n  x_squared <- tf$reduce_sum(\n    batch_masked * batch_masked,\n    axis = 2L,\n    keepdims = TRUE\n  )\n\n  pdist_vector <- x_squared +\n  tf$transpose(\n    x_squared, perm = c(0L, 2L, 1L)\n  ) -\n  2 * tf$matmul(\n    batch_masked,\n    tf$transpose(batch_masked, perm = c(0L, 2L, 1L))\n  )\n\n  all_dists <- pdist_vector\n  all_ra <-\n    tf$sqrt((1 / (\n      batch_size * tf$range(1, 1 + n_latent, dtype = tf$float32)\n    )) *\n      tf$reduce_sum(tf$square(\n        batch_masked - tf$reduce_mean(batch_masked, axis = 1L, keepdims = TRUE)\n      ), axis = c(1L, 2L)))\n  \n  all_dists <- tf$clip_by_value(all_dists, 1e-14, tf$reduce_max(all_dists))\n\n  top_k <- tf$math$top_k(-all_dists, tf$cast(k + 1, tf$int32))\n  top_indices <- top_k[[1]]\n\n  neighbor_dists_d <- tf$gather(all_dists, top_indices, batch_dims = -1L)\n  \n  neighbor_new_dists <- tf$gather(\n    all_dists[2:-1, , ],\n    top_indices[1:-2, , ],\n    batch_dims = -1L\n  )\n  \n  # Eq. 4 of Kennel et al. (1992)\n  scaled_dist <- tf$sqrt((\n    tf$square(neighbor_new_dists) -\n      tf$square(neighbor_dists_d[1:-2, , ])) /\n      tf$square(neighbor_dists_d[1:-2, , ])\n  )\n  \n  # Kennel condition #1\n  is_false_change <- (scaled_dist > rtol)\n  # Kennel condition #2\n  is_large_jump <-\n    (neighbor_new_dists > atol * all_ra[1:-2, tf$newaxis, tf$newaxis])\n  \n  is_false_neighbor <-\n    tf$math$logical_or(is_false_change, is_large_jump)\n  \n  total_false_neighbors <-\n    tf$cast(is_false_neighbor, tf$int32)[reticulate::py_ellipsis(), 2:(k + 2)]\n  \n  reg_weights <- 1 -\n    tf$reduce_mean(tf$cast(total_false_neighbors, tf$float32), axis = c(1L, 2L))\n  reg_weights <- tf$pad(reg_weights, list(list(1L, 0L)))\n  \n  activations_batch_averaged <-\n    tf$sqrt(tf$reduce_mean(tf$square(x), axis = 0L))\n  \n  loss <- tf$reduce_sum(tf$multiply(reg_weights, activations_batch_averaged))\n  loss\n  \n}\n\n\nMSE and FNN are added , with FNN loss weighted according to the essential hyperparameter of this model:\n\n\nfnn_weight <- 10\n\n\nThis value was experimentally chosen as the one best conforming to our look-for-the-highest-drop heuristic.\nModel training\nThe training loop closely follows the aforementioned recipe on how to\ntrain with custom models and tfautograph.\n\n\ntrain_loss <- tf$keras$metrics$Mean(name='train_loss')\ntrain_fnn <- tf$keras$metrics$Mean(name='train_fnn')\ntrain_mse <-  tf$keras$metrics$Mean(name='train_mse')\n\ntrain_step <- function(batch) {\n  \n  with (tf$GradientTape(persistent = TRUE) %as% tape, {\n    \n    code <- encoder(batch)\n    reconstructed <- decoder(code)\n    \n    l_mse <- mse_loss(batch, reconstructed)\n    l_fnn <- loss_false_nn(code)\n    loss <- l_mse + fnn_weight * l_fnn\n    \n  })\n  \n  encoder_gradients <- tape$gradient(loss, encoder$trainable_variables)\n  decoder_gradients <- tape$gradient(loss, decoder$trainable_variables)\n  \n  optimizer$apply_gradients(\n    purrr::transpose(list(encoder_gradients, encoder$trainable_variables))\n  )\n  optimizer$apply_gradients(\n    purrr::transpose(list(decoder_gradients, decoder$trainable_variables))\n  )\n  \n  train_loss(loss)\n  train_mse(l_mse)\n  train_fnn(l_fnn)\n}\n\ntraining_loop <- tf_function(autograph(function(ds_train) {\n  \n  for (batch in ds_train) {\n    train_step(batch)\n  }\n  \n  tf$print(\"Loss: \", train_loss$result())\n  tf$print(\"MSE: \", train_mse$result())\n  tf$print(\"FNN loss: \", train_fnn$result())\n  \n  train_loss$reset_states()\n  train_mse$reset_states()\n  train_fnn$reset_states()\n  \n}))\n\noptimizer <- optimizer_adam(lr = 1e-3)\n\nfor (epoch in 1:200) {\n  cat(\"Epoch: \", epoch, \" -----------\\n\")\n  training_loop(ds_train)  \n}\n\n\nAfter two hundred epochs, overall loss is at 2.67, with the MSE component at 1.8 and FNN at 0.09.\nObtaining the attractor from the test set\nWe use the test set to inspect the latent code:\n\n\ntest_batch <- as_iterator(ds_test) %>% iter_next()\npredicted <- encoder(test_batch) %>%\n  as.array(predicted) %>%\n  as_tibble()\n\npredicted\n\n\n# A tibble: 6,242 x 10\n      V1    V2         V3        V4        V5         V6        V7        V8       V9       V10\n   <dbl> <dbl>      <dbl>     <dbl>     <dbl>      <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n 1 0.439 0.401 -0.000614  -0.0258   -0.00176  -0.0000276  0.000276  0.00677  -0.0239   0.00906 \n 2 0.415 0.504  0.0000481 -0.0279   -0.00435  -0.0000970  0.000921  0.00509  -0.0214   0.00921 \n 3 0.389 0.619  0.000848  -0.0240   -0.00661  -0.000171   0.00106   0.00454  -0.0150   0.00794 \n 4 0.363 0.729  0.00137   -0.0143   -0.00652  -0.000244   0.000523  0.00450  -0.00594  0.00476 \n 5 0.335 0.809  0.00128   -0.000450 -0.00338  -0.000307  -0.000561  0.00407   0.00394 -0.000127\n 6 0.304 0.828  0.000631   0.0126    0.000889 -0.000351  -0.00167   0.00250   0.0115  -0.00487 \n 7 0.274 0.769 -0.000202   0.0195    0.00403  -0.000367  -0.00220  -0.000308  0.0145  -0.00726 \n 8 0.246 0.657 -0.000865   0.0196    0.00558  -0.000359  -0.00208  -0.00376   0.0134  -0.00709 \n 9 0.224 0.535 -0.00121    0.0162    0.00608  -0.000335  -0.00169  -0.00697   0.0106  -0.00576 \n10 0.211 0.434 -0.00129    0.0129    0.00606  -0.000306  -0.00134  -0.00927   0.00820 -0.00447 \n# … with 6,232 more rows\nAs a result of the FNN regularizer, the latent code units should be ordered roughly by decreasing variance, with a sharp drop\nappearing some place (if the FNN weight has been chosen adequately).\nFor an fnn_weight of 10, we do see a drop after the first two units:\n\n\npredicted %>% summarise_all(var)\n\n\n# A tibble: 1 x 10\n      V1     V2      V3      V4      V5      V6      V7      V8      V9     V10\n   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 0.0739 0.0582 1.12e-6 3.13e-4 1.43e-5 1.52e-8 1.35e-6 1.86e-4 1.67e-4 4.39e-5\nSo the model indicates that the Lorenz attractor can be represented in two dimensions. If we nonetheless want to plot the\ncomplete (reconstructed) state space of three dimensions, we should reorder the remaining variables by magnitude of\nvariance8. Here, this results in three projections of the set V1, V2 and V4:\n\n\n\nFigure 10: Attractors as predicted from the latent code (test set). The three highest-variance variables were chosen.\n\n\n\nWrapping up (for this time)\nAt this point, we’ve seen how to reconstruct the Lorenz attractor from data we did not train on (the test set), using an\nautoencoder regularized by a custom false nearest neighbors loss. It is important to stress that at no point was the network\npresented with the expected solution (attractor) – training was purely unsupervised.\nThis is a fascinating result. Of course, thinking practically, the next step is to obtain predictions on heldout data. Given\nhow long this text has become already, we reserve that for a follow-up post. And again of course, we’re thinking about other\ndatasets, especially ones where the true state space is not known beforehand. What about measurement noise? What about\ndatasets that are not completely deterministic9? There is a lot to explore, stay tuned – and as always, thanks for\nreading!\n\n\nGilpin, William. 2020. “Deep Reconstruction of Strange Attractors from Time Series.” https://arxiv.org/abs/2002.05909.\n\n\nKantz, Holger, and Thomas Schreiber. 2004. Nonlinear Time Series Analysis. Cambridge University Press.\n\n\nKennel, Matthew B., Reggie Brown, and Henry D. I. Abarbanel. 1992. “Determining Embedding Dimension for Phase-Space Reconstruction Using a Geometrical Construction.” Phys. Rev. A 45 (March): 3403–11. https://doi.org/10.1103/PhysRevA.45.3403.\n\n\nSauer, Tim, James A. Yorke, and Martin Casdagli. 1991. “Embedology.” Journal of Statistical Physics 65 (3-4): 579–616. https://doi.org/10.1007/BF01053745.\n\n\nStrang, Gilbert. 2019. Linear Algebra and Learning from Data. Wellesley Cambridge Press.\n\n\nStrogatz, Steven. 2015. Nonlinear Dynamics and Chaos: With Applications to Physics, Biology, Chemistry, and Engineering. Westview Press.\n\n\nFor many popular activation functions at least (such as ReLU). See e.g. (Strang 2019).↩︎\nThe paper is also accompanied by a Python implementation.↩︎\nTo people who want to learn more about this topic, the usual recommendation is (Strogatz 2015). Personally I prefer another\nsource, which I can’t recommend highly enough: Santa Fe Institute’s Nonlinear Dynamics: Mathematical and Computational\nApproaches,\ntaught by Liz Bradley.↩︎\nSee e.g. Wikipedia for some history and links to sources.↩︎\nIn discrete systems, like the logistic map, a single dimension is enough.↩︎\nSee the custom training tutorial for a blueprint.↩︎\nSee the appendix of (Gilpin 2020) for a pseudocode-like documentation.↩︎\nAs per author recommendation (personal communication).↩︎\nSee (Kantz and Schreiber 2004) for detailed discussions on using methodology from nonlinear deterministic systems analysis for noisy\nand/or partly-stochastic data.\n\n↩︎\n",
    "preview": "posts/2020-06-24-deep-attractors/images/x_z.gif",
    "last_modified": "2024-11-21T15:49:02+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-05-29-pixelcnn/",
    "title": "Easy PixelCNN with tfprobability",
    "description": "PixelCNN is a deep learning architecture - or bundle of architectures - designed to generate highly realistic-looking images. To use it, no reverse-engineering of arXiv papers or search for reference implementations is required: TensorFlow Probability and its R wrapper, tfprobability, now include a PixelCNN distribution that can be used to train a straightforwardly-defined neural network in a parameterizable way.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-05-29",
    "categories": [
      "R",
      "Image Recognition & Image Processing",
      "TensorFlow/Keras",
      "Probabilistic ML/DL",
      "Generative Models"
    ],
    "contents": "\nWe’ve seen quite a few examples of unsupervised learning (or self-supervised learning, to choose the more correct but less\npopular term) on this blog.\nOften, these involved Variational Autoencoders (VAEs), whose appeal lies in them allowing to model a latent space of\nunderlying, independent (preferably) factors that determine the visible features. A possible downside can be the inferior\nquality of generated samples. Generative Adversarial Networks (GANs) are another popular approach. Conceptually, these are\nhighly attractive due to their game-theoretic framing. However, they can be difficult to train. PixelCNN variants, on the\nother hand – we’ll subsume them all here under PixelCNN – are generally known for their good results. They seem to involve\nsome more alchemy1 though. Under those circumstances, what could be more welcome than an easy way of experimenting with\nthem? Through TensorFlow Probability (TFP) and its R wrapper, tfprobability, we now have\nsuch a way.\nThis post first gives an introduction to PixelCNN, concentrating on high-level concepts (leaving the details for the curious\nto look them up in the respective papers). We’ll then show an example of using tfprobability to experiment with the TFP\nimplementation.\nPixelCNN principles\nAutoregressivity, or: We need (some) order\nThe basic idea in PixelCNN is autoregressivity. Each pixel is modeled as depending on all prior pixels. Formally:\n\\[p(\\mathbf{x}) = \\prod_{i}p(x_i|x_0, x_1, ..., x_{i-1})\\]\nNow wait a second - what even are prior pixels? Last I saw one images were two-dimensional. So this means we have to impose\nan order on the pixels. Commonly this will be raster scan order: row after row, from left to right. But when dealing with\ncolor images, there’s something else: At each position, we actually have three intensity values, one for each of red, green,\nand blue. The original PixelCNN paper(Oord, Kalchbrenner, and Kavukcuoglu 2016) carried through autoregressivity here as well, with a pixel’s intensity for\nred depending on just prior pixels, those for green depending on these same prior pixels but additionally, the current value\nfor red, and those for blue depending on the prior pixels as well as the current values for red and green.\n\\[p(x_i|\\mathbf{x}<i) = p(x_{i,R}|\\mathbf{x}<i)\\ p(x_{i,G}|\\mathbf{x}<i, x_{i,R})\\ p(x_{i,B}|\\mathbf{x}<i, x_{i,R}, x_{i,G})\\]\nHere, the variant implemented in TFP, PixelCNN++(Salimans et al. 2017) , introduces a simplification; it factorizes the joint\ndistribution in a less compute-intensive way.2\nTechnically, then, we know how autoregressivity is realized; intuitively, it may still seem surprising that imposing a raster\nscan order “just works” (to me, at least, it is). Maybe this is one of those points where compute power successfully\ncompensates for lack of an equivalent of a cognitive prior.\nMasking, or: Where not to look\nNow, PixelCNN ends in “CNN” for a reason – as usual in image processing, convolutional layers (or blocks thereof) are\ninvolved. But – is it not the very nature of a convolution that it computes an average of some sorts, looking, for each\noutput pixel, not just at the corresponding input but also, at its spatial (or temporal) surroundings? How does that rhyme\nwith the look-at-just-prior-pixels strategy?\nSurprisingly, this problem is easier to solve than it sounds. When applying the convolutional kernel, just multiply with a\nmask that zeroes out any “forbidden pixels” – like in this example for a 5x5 kernel, where we’re about to compute the\nconvolved value for row 3, column 3:\n\\[\\left[\\begin{array}\n{rrr}\n1 & 1 & 1 & 1 & 1\\\\\n1 & 1 & 1 & 1 & 1\\\\\n1 & 1 & 1 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0\\\\\n\\end{array}\\right]\n\\]\nThis makes the algorithm honest, but introduces a different problem: With each successive convolutional layer consuming its\npredecessor’s output, there is a continuously growing blind spot (so-called in analogy to the blind spot on the retina, but\nlocated in the top right) of pixels that are never seen by the algorithm. Van den Oord et al. (2016)(Oord et al. 2016) fix this\nby using two different convolutional stacks, one proceeding from top to bottom, the other from left to right3.\nFig. 1: Left: Blind spot, growing over layers. Right: Using two different stacks (a vertical and a horizontal one) solves\nthe problem. Source: van den Oord et al., 2016.Conditioning, or: Show me a kitten\nSo far, we’ve always talked about “generating images” in a purely generic way. But the real attraction lies in creating\nsamples of some specified type – one of the classes we’ve been training on, or orthogonal information fed into the network.\nThis is where PixelCNN becomes Conditional PixelCNN(Oord et al. 2016), and it is also where that feeling of magic resurfaces.\nAgain, as “general math” it’s not hard to conceive. Here, \\(\\mathbf{h}\\) is the additional input we’re conditioning on:\n\\[p(\\mathbf{x}| \\mathbf{h}) = \\prod_{i}p(x_i|x_0, x_1, ..., x_{i-1}, \\mathbf{h})\\]\nBut how does this translate into neural network operations? It’s just another matrix multiplication (\\(V^T \\mathbf{h}\\)) added\nto the convolutional outputs (\\(W \\mathbf{x}\\)).\n\\[\\mathbf{y} = tanh(W_{k,f} \\mathbf{x} + V^T_{k,f} \\mathbf{h}) \\odot \\sigma(W_{k,g} \\mathbf{x} + V^T_{k,g} \\mathbf{h})\\]\n(If you’re wondering about the second part on the right, after the Hadamard product sign – we won’t go into details, but in a\nnutshell, it’s another modification introduced by (Oord et al. 2016), a transfer of the “gating” principle from recurrent neural\nnetworks, such as GRUs and LSTMs, to the convolutional setting.)\nSo we see what goes into the decision of a pixel value to sample. But how is that decision actually made?\nLogistic mixture likelihood , or: No pixel is an island\nAgain, this is where the TFP implementation does not follow the original paper, but the latter PixelCNN++ one. Originally,\npixels were modeled as discrete values, decided on by a softmax over 256 (0-255) possible values. (That this actually worked\nseems like another instance of deep learning magic. Imagine: In this model, 254 is as far from 255 as it is from 0.)\nIn contrast, PixelCNN++ assumes an underlying continuous distribution of color intensity, and rounds to the nearest integer.\nThat underlying distribution is a mixture of logistic distributions, thus allowing for multimodality:\n\\[\\nu \\sim \\sum_{i} \\pi_i \\ logistic(\\mu_i, \\sigma_i)\\]\nOverall architecture and the PixelCNN distribution\nOverall, PixelCNN++, as described in (Salimans et al. 2017), consists of six blocks. The blocks together make up a UNet-like\nstructure, successively downsizing the input and then, upsampling again:\nFig. 2: Overall structure of PixelCNN++. From: Salimans et al., 2017.In TFP’s PixelCNN distribution, the number of blocks is configurable as num_hierarchies, the default being 3.\nEach block consists of a customizable number of layers, called ResNet layers due to the residual connection (visible on the\nright) complementing the convolutional operations in the horizontal stack:\nFig. 3: One so-called \"ResNet layer\", featuring both a vertical and a horizontal convolutional stack. Source: van den Oord\net al., 2017.In TFP, the number of these layers per block is configurable as num_resnet.\nnum_resnet and num_hierarchies are the parameters you’re most likely to experiment with, but there are a few more you can\ncheck out in the documentation. The number of logistic\ndistributions in the mixture is also configurable, but from my experiments it’s best to keep that number rather low to avoid\nproducing NaNs during training.\nLet’s now see a complete example.\nEnd-to-end example\nOur playground will be QuickDraw, a dataset – still growing –\nobtained by asking people to draw some object in at most twenty seconds, using the mouse. (To see for yourself, just check out\nthe website). As of today, there are more than a fifty million instances, from 345\ndifferent classes.\nFirst and foremost, these data were chosen to take a break from MNIST and its variants. But just like those (and many more!),\nQuickDraw can be obtained, in tfdatasets-ready form, via tfds, the R wrapper to\nTensorFlow datasets. In contrast to the MNIST “family” though, the “real samples” are themselves highly irregular, and often\neven missing essential parts. So to anchor judgment, when displaying generated samples we always show eight actual drawings\nwith them.\nPreparing the data\nThe dataset being gigantic, we instruct tfds to load the first 500,000 drawings “only.”\n\n\nlibrary(reticulate)\n\n# >= 2.2 required\nlibrary(tensorflow)\nlibrary(keras)\n\n# make sure to use at least version 0.10\nlibrary(tfprobability)\n\nlibrary(tfdatasets)\n# currently to be installed from github\nlibrary(tfds)\n\n# load just the first 500,000 images\n# nonetheless, initially the complete dataset will be downloaded and unpacked\n# ... be prepared for this to take some time\ntrain_ds <- tfds_load(\"quickdraw_bitmap\", split='train[:500000]')\n\n\nTo speed up training further, we then zoom in on twenty classes. This effectively leaves us with ~ 1,100 - 1,500 drawings per\nclass.\n\n\n# bee, bicycle, broccoli, butterfly, cactus,\n# frog, guitar, lightning, penguin, pizza,\n# rollerskates, sea turtle, sheep, snowflake, sun,\n# swan, The Eiffel Tower, tractor, train, tree\nclasses <- c(26, 29, 43, 49, 50,\n             125, 134, 172, 218, 225,\n             246, 255, 258, 271, 295,\n             296, 308, 320, 322, 323\n)\n\nclasses_tensor <- tf$cast(classes, tf$int64)\n\ntrain_ds <- train_ds %>%\n  dataset_filter(\n    function(record) tf$reduce_any(tf$equal(classes_tensor, record$label), -1L)\n  )\n\n\nThe PixelCNN distribution expects values in the range from 0 to 255 – no normalization required. Preprocessing then consists\nof just casting pixels and labels each to float:\n\n\npreprocess <- function(record) {\n  record$image <- tf$cast(record$image, tf$float32) \n  record$label <- tf$cast(record$label, tf$float32)\n  list(tuple(record$image, record$label))\n}\n\nbatch_size <- 32\n\ntrain <- train_ds %>%\n  dataset_map(preprocess) %>%\n  dataset_shuffle(10000) %>%\n  dataset_batch(batch_size)\n\n\nCreating the model\nWe now use tfd_pixel_cnn to define what will be the\nloglikelihood used by the model.\n\n\ndist <- tfd_pixel_cnn(\n  image_shape = c(28, 28, 1),\n  conditional_shape = list(),\n  num_resnet = 5,\n  num_hierarchies = 3,\n  num_filters = 128,\n  num_logistic_mix = 5,\n  dropout_p =.5\n)\n\nimage_input <- layer_input(shape = c(28, 28, 1))\nlabel_input <- layer_input(shape = list())\nlog_prob <- dist %>% tfd_log_prob(image_input, conditional_input = label_input)\n\n\nThis custom loglikelihood is added as a loss to the model, and then, the model is compiled with just an optimizer\nspecification only. During training, loss first decreased quickly, but improvements from later epochs were smaller.\n\n\nmodel <- keras_model(inputs = list(image_input, label_input), outputs = log_prob)\nmodel$add_loss(-tf$reduce_mean(log_prob))\nmodel$compile(optimizer = optimizer_adam(lr = .001))\n\nmodel %>% fit(train, epochs = 10)\n\n\nTo jointly display real and fake images:\n\n\nfor (i in classes) {\n  \n  real_images <- train_ds %>%\n    dataset_filter(\n      function(record) record$label == tf$cast(i, tf$int64)\n    ) %>% \n    dataset_take(8) %>%\n    dataset_batch(8)\n  it <- as_iterator(real_images)\n  real_images <- iter_next(it)\n  real_images <- real_images$image %>% as.array()\n  real_images <- real_images[ , , , 1]/255\n  \n  generated_images <- dist %>% tfd_sample(8, conditional_input = i)\n  generated_images <- generated_images %>% as.array()\n  generated_images <- generated_images[ , , , 1]/255\n  \n  images <- abind::abind(real_images, generated_images, along = 1)\n  png(paste0(\"draw_\", i, \".png\"), width = 8 * 28 * 10, height = 2 * 28 * 10)\n  par(mfrow = c(2, 8), mar = c(0, 0, 0, 0))\n  images %>%\n    purrr::array_tree(1) %>%\n    purrr::map(as.raster) %>%\n    purrr::iwalk(plot)\n  dev.off()\n}\n\n\nFrom our twenty classes, here’s a choice of six, each showing real drawings in the top row, and fake ones below.\nFig. 4: Bicycles, drawn by people (top row) and the network (bottom row).Fig. 5: Broccoli, drawn by people (top row) and the network (bottom row).Fig. 6: Butterflies, drawn by people (top row) and the network (bottom row).Fig. 7: Guitars, drawn by people (top row) and the network (bottom row).Fig. 8: Penguins, drawn by people (top row) and the network (bottom row).Fig. 9: Roller skates, drawn by people (top row) and the network (bottom row).We probably wouldn’t confuse the first and second rows, but then, the actual human drawings exhibit enormous variation, too.\nAnd no one ever said PixelCNN was an architecture for concept learning. Feel free to play around with other datasets of your\nchoice – TFP’s PixelCNN distribution makes it easy.\nWrapping up\nIn this post, we had tfprobability / TFP do all the heavy lifting for us, and so, could focus on the underlying concepts.\nDepending on your inclinations, this can be an ideal situation – you don’t lose sight of the forest for the trees. On the\nother hand: Should you find that changing the provided parameters doesn’t achieve what you want, you have a reference\nimplementation to start from. So whatever the outcome, the addition of such higher-level functionality to TFP is a win for the\nusers. (If you’re a TFP developer reading this: Yes, we’d like more :-)).\nTo everyone though, thanks for reading!\n\n\n\nOord, Aaron van den, Nal Kalchbrenner, and Koray Kavukcuoglu. 2016. “Pixel Recurrent Neural Networks.” CoRR abs/1601.06759. http://arxiv.org/abs/1601.06759.\n\n\nOord, Aaron van den, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. 2016. “Conditional Image Generation with PixelCNN Decoders.” CoRR abs/1606.05328. http://arxiv.org/abs/1606.05328.\n\n\nSalimans, Tim, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. 2017. “PixelCNN++: A PixelCNN Implementation with Discretized Logistic Mixture Likelihood and Other Modifications.” In ICLR.\n\n\nAlluding to Ali Rahimi’s (in)famous “deep learning is alchemy” talk at\nNeurIPS 2017. I would suspect that to some degree, that statement resonates with many DL practitioners – although one\nneed not agree that more mathematical rigor is the solution.↩︎\nFor details, see (Salimans et al. 2017).↩︎\n.For details, see (Oord et al. 2016).↩︎\n",
    "preview": "posts/2020-05-29-pixelcnn/images/thumb.png",
    "last_modified": "2024-11-21T15:52:28+00:00",
    "input_file": {},
    "preview_width": 400,
    "preview_height": 203
  },
  {
    "path": "posts/2020-05-15-model-inversion-attacks/",
    "title": "Hacking deep learning: model inversion attack by example",
    "description": "Compared to other applications, deep learning models might not seem too likely as victims of privacy attacks. However, methods exist to determine whether an entity was used in the training set (an adversarial attack called member inference), and techniques subsumed under \"model inversion\" allow to reconstruct raw data input given just model output (and sometimes, context information). This post shows an end-to-end example of model inversion, and explores mitigation strategies using TensorFlow Privacy.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-05-15",
    "categories": [
      "R",
      "Privacy & Security",
      "TensorFlow/Keras"
    ],
    "contents": "\nHow private are individual data in the context of machine learning models? The data used to train the model, say. There are\ntypes of models where the answer is simple. Take k-nearest-neighbors, for example. There is not even a model without the\ncomplete dataset. Or support vector machines. There is no model without the support vectors. But neural networks? They’re just\nsome composition of functions, – no data included.\nThe same is true for data fed to a deployed deep-learning model. It’s pretty unlikely one could invert the final softmax\noutput from a big ResNet and get back the raw input data.\nIn theory, then, “hacking” a standard neural net to spy on input data sounds illusory. In practice, however, there is always\nsome real-world context. The context may be other datasets, publicly available, that can be linked to the “private” data in\nquestion. This is a popular showcase used in advocating for differential privacy(Dwork et al. 2006): Take an “anonymized” dataset,\ndig up complementary information from public sources, and de-anonymize records ad libitum. Some context in that sense will\noften be used in “black-box” attacks, ones that presuppose no insider information about the model to be hacked.\nBut context can also be structural, such as in the scenario demonstrated in this post. For example, assume a distributed\nmodel, where sets of layers run on different devices – embedded devices or mobile phones, for example. (A scenario like that\nis sometimes seen as “white-box”(Wu et al. 2016), but in common understanding, white-box attacks probably presuppose some more\ninsider knowledge, such as access to model architecture or even, weights. I’d therefore prefer calling this white-ish at\nmost.) — Now assume that in this context, it is possible to intercept, and interact with, a system that executes the deeper\nlayers of the model. Based on that system’s intermediate-level output, it is possible to perform model inversion(Fredrikson et al. 2014),\nthat is, to reconstruct the input data fed into the system.\nIn this post, we’ll demonstrate such a model inversion attack, basically porting the approach given in a\nnotebook\nfound in the PySyft repository. We then experiment with different levels of\n\\(\\epsilon\\)-privacy, exploring impact on reconstruction success. This second part will make use of TensorFlow Privacy,\nintroduced in a previous blog post.\nPart 1: Model inversion in action\nExample dataset: All the world’s letters1\nThe overall process of model inversion used here is the following. With no, or scarcely any, insider knowledge about a model,\n– but given opportunities to repeatedly query it –, I want to learn how to reconstruct unknown inputs based on just model\noutputs . Independently of original model training, this, too, is a training process; however, in general it will not involve\nthe original data, as those won’t be publicly available. Still, for best success, the attacker model is trained with data as\nsimilar as possible to the original training data assumed. Thinking of images, for example, and presupposing the popular view\nof successive layers representing successively coarse-grained features, we want that the surrogate data to share as many\nrepresentation spaces with the real data as possible – up to the very highest layers before final classification, ideally.\nIf we wanted to use classical MNIST as an example, one thing we could do is to only use some of the digits for training the\n“real” model; and the rest, for training the adversary. Let’s try something different though, something that might make the\nundertaking harder as well as easier at the same time. Harder, because the dataset features exemplars more complex than MNIST\ndigits; easier because of the same reason: More could possibly be learned, by the adversary, from a complex task.\nOriginally designed to develop a machine model of concept learning and generalization (Lake, Salakhutdinov, and Tenenbaum 2015), the\nOmniGlot dataset incorporates characters from fifty alphabets, split into two\ndisjoint groups of thirty and twenty alphabets each. We’ll use the group of twenty to train our target model. Here is a\nsample:\n\n\n\nFigure 1: Sample from the twenty-alphabet set used to train the target model (originally: ‘evaluation set’)\n\n\n\nThe group of thirty we don’t use; instead, we’ll employ two small five-alphabet collections to train the adversary and to test\nreconstruction, respectively. (These small subsets of the original “big” thirty-alphabet set are again disjoint.)\nHere first is a sample from the set used to train the adversary.\n\n\n\nFigure 2: Sample from the five-alphabet set used to train the adversary (originally: ‘background small 1’)\n\n\n\nThe other small subset will be used to test the adversary’s spying capabilities after training. Let’s peek at this one, too:\n\n\n\nFigure 3: Sample from the five-alphabet set used to test the adversary after training(originally: ‘background small 2’)\n\n\n\nConveniently, we can use tfds, the R wrapper to TensorFlow Datasets, to load those subsets:\n\n\nlibrary(reticulate)\nlibrary(keras)\nlibrary(tfdatasets)\nlibrary(tfautograph)\nlibrary(tfds)\n\nlibrary(purrr)\n\n# we'll use this to train the target model\n# n = 13180\nomni_train <- tfds$load(\"omniglot\", split = \"test\")\n\n# this is used to train the adversary\n# n = 2720\nomni_spy <- tfds$load(\"omniglot\", split = \"small1\")\n\n# this we'll use for testing\n# n = 3120\nomni_test <- tfds$load(\"omniglot\", split = \"small2\")\n\n\nNow first, we train the target model.\nTrain target model\nThe dataset originally has four columns: the image, of size 105 x 105; an alphabet id and a within-dataset character id; and a\nlabel. For our use case, we’re not really interested in the task the target model was/is used for; we just want to get at the\ndata. Basically, whatever task we choose, it is not much more than a dummy task. So, let’s just say we train the target to\nclassify characters by alphabet.\nWe thus throw out all unneeded features, keeping just the alphabet id and the image itself:\n\n\n# normalize and work with a single channel (images are black-and-white anyway)\npreprocess_image <- function(image) {\n  image %>%\n    tf$cast(dtype = tf$float32) %>%\n    tf$truediv(y = 255) %>%\n    tf$image$rgb_to_grayscale()\n}\n\n# use the first 11000 images for training\ntrain_ds <- omni_train %>% \n  dataset_take(11000) %>%\n  dataset_map(function(record) {\n    record$image <- preprocess_image(record$image)\n    list(record$image, record$alphabet)}) %>%\n  dataset_shuffle(1000) %>% \n  dataset_batch(32)\n\n# use the remaining 2180 records for validation\nval_ds <- omni_train %>% \n  dataset_skip(11000) %>%\n  dataset_map(function(record) {\n    record$image <- preprocess_image(record$image)\n    list(record$image, record$alphabet)}) %>%\n  dataset_batch(32)\n\n\nThe model consists of two parts. The first is imagined to run in a distributed fashion; for example, on mobile devices (stage\none). These devices then send model outputs to a central server, where final results are computed (stage two). Sure, you will\nbe thinking, this is a convenient setup for our scenario: If we intercept stage one results, we – most probably – gain\naccess to richer information than what is contained in a model’s final output layer. — That is correct, but the scenario is\nless contrived than one might assume. Just like federated learning (McMahan et al. 2016), it fulfills important desiderata: Actual\ntraining data never leaves the devices, thus staying (in theory!) private; at the same time, ingoing traffic to the server is\nsignificantly reduced.\nIn our example setup, the on-device model is a convnet, while the server model is a simple feedforward network.\nWe link both together as a TargetModel that when called normally, will run both steps in succession. However, we’ll be able\nto call target_model$mobile_step() separately, thereby intercepting intermediate results.\n\n\non_device_model <- keras_model_sequential() %>%\n  layer_conv_2d(filters = 32, kernel_size = c(7, 7),\n                input_shape = c(105, 105, 1), activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_max_pooling_2d(pool_size = c(3, 3), strides = 3) %>%\n  layer_dropout(0.2) %>%\n  layer_conv_2d(filters = 32, kernel_size = c(7, 7), activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_max_pooling_2d(pool_size = c(3, 3), strides = 2) %>%\n  layer_dropout(0.2) %>%\n  layer_conv_2d(filters = 32, kernel_size = c(5, 5), activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_max_pooling_2d(pool_size = c(2, 2), strides = 2) %>%\n  layer_dropout(0.2) %>%\n  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_max_pooling_2d(pool_size = c(2, 2), strides = 2) %>%\n  layer_dropout(0.2) \n\nserver_model <- keras_model_sequential() %>%\n  layer_dense(units = 256, activation = \"relu\") %>%\n  layer_flatten() %>%\n  layer_dropout(0.2) %>% \n  # we have just 20 different ids, but they are not in lexicographic order\n  layer_dense(units = 50, activation = \"softmax\")\n\ntarget_model <- function() {\n  keras_model_custom(name = \"TargetModel\", function(self) {\n    \n    self$on_device_model <-on_device_model\n    self$server_model <- server_model\n    self$mobile_step <- function(inputs) \n      self$on_device_model(inputs)\n    self$server_step <- function(inputs)\n      self$server_model(inputs)\n\n    function(inputs, mask = NULL) {\n      inputs %>% \n        self$mobile_step() %>%\n        self$server_step()\n    }\n  })\n  \n}\n\nmodel <- target_model()\n\n\nThe overall model is a Keras custom model, so we train it TensorFlow 2.x -\nstyle. After ten epochs, training and validation accuracy are at ~0.84\nand ~0.73, respectively – not bad at all for a 20-class discrimination task.\n\n\nloss <- loss_sparse_categorical_crossentropy\noptimizer <- optimizer_adam()\n\ntrain_loss <- tf$keras$metrics$Mean(name='train_loss')\ntrain_accuracy <-  tf$keras$metrics$SparseCategoricalAccuracy(name='train_accuracy')\n\nval_loss <- tf$keras$metrics$Mean(name='val_loss')\nval_accuracy <-  tf$keras$metrics$SparseCategoricalAccuracy(name='val_accuracy')\n\ntrain_step <- function(images, labels) {\n  with (tf$GradientTape() %as% tape, {\n    predictions <- model(images)\n    l <- loss(labels, predictions)\n  })\n  gradients <- tape$gradient(l, model$trainable_variables)\n  optimizer$apply_gradients(purrr::transpose(list(\n    gradients, model$trainable_variables\n  )))\n  train_loss(l)\n  train_accuracy(labels, predictions)\n}\n\nval_step <- function(images, labels) {\n  predictions <- model(images)\n  l <- loss(labels, predictions)\n  val_loss(l)\n  val_accuracy(labels, predictions)\n}\n\n\ntraining_loop <- tf_function(autograph(function(train_ds, val_ds) {\n  for (b1 in train_ds) {\n    train_step(b1[[1]], b1[[2]])\n  }\n  for (b2 in val_ds) {\n    val_step(b2[[1]], b2[[2]])\n  }\n  \n  tf$print(\"Train accuracy\", train_accuracy$result(),\n           \"    Validation Accuracy\", val_accuracy$result())\n  \n  train_loss$reset_states()\n  train_accuracy$reset_states()\n  val_loss$reset_states()\n  val_accuracy$reset_states()\n}))\n\n\nfor (epoch in 1:10) {\n  cat(\"Epoch: \", epoch, \" -----------\\n\")\n  training_loop(train_ds, val_ds)  \n}\n\n\nEpoch:  1  -----------\nTrain accuracy 0.195090905     Validation Accuracy 0.376605511\nEpoch:  2  -----------\nTrain accuracy 0.472272724     Validation Accuracy 0.5243119\n...\n...\nEpoch:  9  -----------\nTrain accuracy 0.821454525     Validation Accuracy 0.720183492\nEpoch:  10  -----------\nTrain accuracy 0.840454519     Validation Accuracy 0.726605475\nNow, we train the adversary.\nTrain adversary\nThe adversary’s general strategy will be:\nFeed its small, surrogate dataset to the on-device model. The output received can be regarded as a (highly)\ncompressed version of the original images.\nPass that “compressed” version as input to its own model, which tries to reconstruct the original images from the\nsparse code.\nCompare original images (those from the surrogate dataset) to the reconstruction pixel-wise. The goal is to minimize\nthe mean (squared, say) error.\nDoesn’t this sound a lot like the decoding side of an autoencoder? No wonder the attacker model is a deconvolutional network.\nIts input – equivalently, the on-device model’s output – is of size batch_size x 1 x 1 x 32. That is, the information is\nencoded in 32 channels, but the spatial resolution is 1. Just like in an autoencoder operating on images, we need to\nupsample until we arrive at the original resolution of 105 x 105.\nThis is exactly what’s happening in the attacker model:\n\n\nattack_model <- function() {\n  \n  keras_model_custom(name = \"AttackModel\", function(self) {\n    \n    self$conv1 <-layer_conv_2d_transpose(filters = 32, kernel_size = 9,\n                                         padding = \"valid\",\n                                         strides = 1, activation = \"relu\")\n    self$conv2 <- layer_conv_2d_transpose(filters = 32, kernel_size = 7,\n                                          padding = \"valid\",\n                                          strides = 2, activation = \"relu\") \n    self$conv3 <- layer_conv_2d_transpose(filters = 1, kernel_size = 7,\n                                          padding = \"valid\",\n                                          strides = 2, activation = \"relu\")  \n    self$conv4 <- layer_conv_2d_transpose(filters = 1, kernel_size = 5,\n                                          padding = \"valid\",\n                                          strides = 2, activation = \"relu\")\n    \n    function(inputs, mask = NULL) {\n      inputs %>% \n        # bs * 9 * 9 * 32\n        # output = strides * (input - 1) + kernel_size - 2 * padding\n        self$conv1() %>%\n        # bs * 23 * 23 * 32\n        self$conv2() %>%\n        # bs * 51 * 51 * 1\n        self$conv3() %>%\n        # bs * 105 * 105 * 1\n        self$conv4()\n    }\n  })\n  \n}\n\nattacker = attack_model()\n\n\nTo train the adversary, we use one of the small (five-alphabet) subsets. To reiterate what was said above, there is no overlap\nwith the data used to train the target model.\n\n\nattacker_ds <- omni_spy %>% \ndataset_map(function(record) {\n    record$image <- preprocess_image(record$image)\n    list(record$image, record$alphabet)}) %>%\n  dataset_batch(32)\n\n\nHere, then, is the attacker training loop, striving to refine the decoding process over a hundred – short – epochs:\n\n\nattacker_criterion <- loss_mean_squared_error\nattacker_optimizer <- optimizer_adam()\nattacker_loss <- tf$keras$metrics$Mean(name='attacker_loss')\nattacker_mse <-  tf$keras$metrics$MeanSquaredError(name='attacker_mse')\n\nattacker_step <- function(images) {\n  \n  attack_input <- model$mobile_step(images)\n  \n  with (tf$GradientTape() %as% tape, {\n    generated <- attacker(attack_input)\n    l <- attacker_criterion(images, generated)\n  })\n  gradients <- tape$gradient(l, attacker$trainable_variables)\n  attacker_optimizer$apply_gradients(purrr::transpose(list(\n    gradients, attacker$trainable_variables\n  )))\n  attacker_loss(l)\n  attacker_mse(images, generated)\n}\n\n\nattacker_training_loop <- tf_function(autograph(function(attacker_ds) {\n  for (b in attacker_ds) {\n    attacker_step(b[[1]])\n  }\n  \n  tf$print(\"mse: \", attacker_mse$result())\n  \n  attacker_loss$reset_states()\n  attacker_mse$reset_states()\n}))\n\nfor (epoch in 1:100) {\n  cat(\"Epoch: \", epoch, \" -----------\\n\")\n  attacker_training_loop(attacker_ds)  \n}\n\n\nEpoch:  1  -----------\n  mse:  0.530902684\nEpoch:  2  -----------\n  mse:  0.201351956\n...\n...\nEpoch:  99  -----------\n  mse:  0.0413453057\nEpoch:  100  -----------\n  mse:  0.0413028933\nThe question now is, – does it work? Has the attacker really learned to infer actual data from (stage one) model output?\nTest adversary\nTo test the adversary, we use the third dataset we downloaded, containing images from five yet-unseen alphabets. For display,\nwe select just the first sixteen records – a completely arbitrary decision, of course.\n\n\ntest_ds <- omni_test %>% \n  dataset_map(function(record) {\n    record$image <- preprocess_image(record$image)\n    list(record$image, record$alphabet)}) %>%\n  dataset_take(16) %>%\n  dataset_batch(16)\n\nbatch <- as_iterator(test_ds) %>% iterator_get_next()\nimages <- batch[[1]]\n\nattack_input <- model$mobile_step(images)\ngenerated <- attacker(attack_input) %>% as.array()\n\ngenerated[generated > 1] <- 1\ngenerated <- generated[ , , , 1]\ngenerated %>%\n  purrr::array_tree(1) %>%\n  purrr::map(as.raster) %>%\n  purrr::iwalk(~{plot(.x)})\n\n\nJust like during the training process, the adversary queries the target model (stage one), obtains the compressed\nrepresentation, and attempts to reconstruct the original image. (Of course, in the real world, the setup would be different in\nthat the attacker would not be able to simply inspect the images, as is the case here. There would thus have to be some way\nto intercept, and make sense of, network traffic.)\n\n\nattack_input <- model$mobile_step(images)\ngenerated <- attacker(attack_input) %>% as.array()\n\ngenerated[generated > 1] <- 1\ngenerated <- generated[ , , , 1]\ngenerated %>%\n  purrr::array_tree(1) %>%\n  purrr::map(as.raster) %>%\n  purrr::iwalk(~{plot(.x)})\n\n\nTo allow for easier comparison (and increase suspense …!), here again are the actual images, which we displayed already when\nintroducing the dataset:\n\n\n\nFigure 4: First images from the test set, the way they really look.\n\n\n\nAnd here is the reconstruction:\n\n\n\nFigure 5: First images from the test set, as reconstructed by the adversary.\n\n\n\nOf course, it is hard to say how revealing these “guesses” are. There definitely seems to be a connection to character\ncomplexity; overall, it seems like the Greek and Roman letters, which are the least complex, are also the ones most easily\nreconstructed. Still, in the end, how much privacy is lost will very much depend on contextual factors.\nFirst and foremost, do the exemplars in the dataset represent individuals or classes of individuals? If – as in reality\n– the character X represents a class, it might not be so grave if we were able to reconstruct “some X” here: There are many\nXs in the dataset, all pretty similar to each other; we’re unlikely to exactly to have reconstructed one special, individual\nX. If, however, this was a dataset of individual people, with all Xs being photographs of Alex, then in reconstructing an\nX we have effectively reconstructed Alex.\nSecond, in less obvious scenarios, evaluating the degree of privacy breach will likely surpass computation of quantitative\nmetrics, and involve the judgment of domain experts.\nSpeaking of quantitative metrics though – our example seems like a perfect use case to experiment with differential\nprivacy. Differential privacy is measured by \\(\\epsilon\\) (lower is better), the main idea being that answers to queries to a\nsystem should depend as little as possible on the presence or absence of a single (any single) datapoint.\nSo, we will repeat the above experiment, using TensorFlow Privacy (TFP) to add noise, as well as clip gradients, during\noptimization of the target model. We’ll try three different conditions, resulting in three different values for \\(\\epsilon\\)s,\nand for each condition, inspect the images reconstructed by the adversary.\nPart 2: Differential privacy to the rescue\nUnfortunately, the setup for this part of the experiment requires a little workaround. Making use of the flexibility afforded\nby TensorFlow 2.x, our target model has been a custom model, joining two distinct stages (“mobile” and “server”) that could be\ncalled independently.\nTFP, however, does still not work with TensorFlow 2.x, meaning we have to use old-style, non-eager model definitions and\ntraining. Luckily, the workaround will be easy.\nFirst, load (and possibly, install) libraries, taking care to disable TensorFlow V2 behavior.\n\n\nlibrary(keras)\nlibrary(tensorflow)\n# still necessary when working with TensorFlow Privacy, as of this writing\ntf$compat$v1$disable_v2_behavior()\n\n# if you don't have it installed:\n# reticulate::py_install(\"tensorflow_privacy\")\ntfp <- import(\"tensorflow_privacy\")\n\nlibrary(tfdatasets)\nlibrary(tfds)\n\nlibrary(purrr)\n\n\nThe training set is loaded, preprocessed and batched (nearly) as before.\n\n\nomni_train <- tfds$load(\"omniglot\", split = \"test\")\n\nbatch_size <- 32\n\ntrain_ds <- omni_train %>%\n  dataset_take(11000) %>%\n  dataset_map(function(record) {\n    record$image <- preprocess_image(record$image)\n    list(record$image, record$alphabet)}) %>%\n  dataset_shuffle(1000) %>%\n  # need dataset_repeat() when not eager\n  dataset_repeat() %>%\n  dataset_batch(batch_size)\n\n\nTrain target model – with TensorFlow Privacy\nTo train the target, we put the layers from both stages – “mobile” and “server” – into one sequential model. Note how we\nremove the dropout. This is because noise will be added during optimization anyway.\n\n\ncomplete_model <- keras_model_sequential() %>%\n  layer_conv_2d(filters = 32, kernel_size = c(7, 7),\n                input_shape = c(105, 105, 1),\n                activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_max_pooling_2d(pool_size = c(3, 3), strides = 3) %>%\n  #layer_dropout(0.2) %>%\n  layer_conv_2d(filters = 32, kernel_size = c(7, 7), activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_max_pooling_2d(pool_size = c(3, 3), strides = 2) %>%\n  #layer_dropout(0.2) %>%\n  layer_conv_2d(filters = 32, kernel_size = c(5, 5), activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_max_pooling_2d(pool_size = c(2, 2), strides = 2) %>%\n  #layer_dropout(0.2) %>%\n  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_max_pooling_2d(pool_size = c(2, 2), strides = 2, name = \"mobile_output\") %>%\n  #layer_dropout(0.2) %>%\n  layer_dense(units = 256, activation = \"relu\") %>%\n  layer_flatten() %>%\n  #layer_dropout(0.2) %>%\n  layer_dense(units = 50, activation = \"softmax\")\n\n\nUsing TFP mainly means using a TFP optimizer, one that clips gradients according to some defined magnitude and adds noise of\ndefined size. noise_multiplier is the parameter we are going to vary to arrive at different \\(\\epsilon\\)s:\n\n\nl2_norm_clip <- 1\n\n# ratio of the standard deviation to the clipping norm\n# we run training for each of the three values\nnoise_multiplier <- 0.7\nnoise_multiplier <- 0.5\nnoise_multiplier <- 0.3\n\n# same as batch size\nnum_microbatches <- k_cast(batch_size, \"int32\")\nlearning_rate <- 0.005\n\noptimizer <- tfp$DPAdamGaussianOptimizer(\n  l2_norm_clip = l2_norm_clip,\n  noise_multiplier = noise_multiplier,\n  num_microbatches = num_microbatches,\n  learning_rate = learning_rate\n)\n\n\nIn training the model, the second important change for TFP we need to make is to have loss and gradients computed on the\nindividual level.\n\n\n# need to add noise to every individual contribution\nloss <- tf$keras$losses$SparseCategoricalCrossentropy(reduction =   tf$keras$losses$Reduction$NONE)\n\ncomplete_model %>% compile(loss = loss, optimizer = optimizer, metrics = \"sparse_categorical_accuracy\")\n\nnum_epochs <- 20\n\nn_train <- 13180\n\nhistory <- complete_model %>% fit(\n  train_ds,\n  # need steps_per_epoch when not in eager mode\n  steps_per_epoch = n_train/batch_size,\n  epochs = num_epochs)\n\n\nTo test three different \\(\\epsilon\\)s, we run this thrice, each time with a different noise_multiplier. Each time we arrive at\na different final accuracy.\nHere is a synopsis, where \\(\\epsilon\\) was computed like so:\n\n\ncompute_priv <- tfp$privacy$analysis$compute_dp_sgd_privacy\n\ncompute_priv$compute_dp_sgd_privacy(\n  # number of records in training set\n  n_train,\n  batch_size,\n  # noise_multiplier\n  0.7, # or 0.5, or 0.3\n  # number of epochs\n  20,\n  # delta - should not exceed 1/number of examples in training set\n  1e-5)\n\n\nnoise multiplier\nepsilon\nfinal acc. (training set)\n0.7\n4.0\n0.37\n0.5\n12.5\n0.45\n0.3\n84.7\n0.56\nNow, as the adversary won’t call the complete model, we need to “cut off” the second-stage layers. This leaves us with a model\nthat executes stage-one logic only. We save its weights, so we can later call it from the adversary:\n\n\nintercepted <- keras_model(\n  complete_model$input,\n  complete_model$get_layer(\"mobile_output\")$output\n)\n\nintercepted %>% save_model_hdf5(\"./intercepted.hdf5\")\n\n\nTrain adversary (against differentially private target)\nIn training the adversary, we can keep most of the original code – meaning, we’re back to TF-2 style. Even the definition of\nthe target model is the same as before:\n\non_device_model <- keras_model_sequential() %>%\n  [...]\n\nserver_model <- keras_model_sequential() %>%\n  [...]\n\ntarget_model <- function() {\n  keras_model_custom(name = \"TargetModel\", function(self) {\n    \n    self$on_device_model <-on_device_model\n    self$server_model <- server_model\n    self$mobile_step <- function(inputs) \n      self$on_device_model(inputs)\n    self$server_step <- function(inputs)\n      self$server_model(inputs)\n    \n    function(inputs, mask = NULL) {\n      inputs %>% \n        self$mobile_step() %>%\n        self$server_step()\n    }\n  })\n}\n\nintercepted <- target_model()\n\nBut now, we load the trained target’s weights into the freshly defined model’s “mobile stage”:\n\n\nintercepted$on_device_model$load_weights(\"intercepted.hdf5\")\n\n\nAnd now, we’re back to the old training routine. Testing setup is the same as before, as well.\nSo how well does the adversary perform with differential privacy added to the picture?\nTest adversary (against differentially private target)\nHere, ordered by decreasing \\(\\epsilon\\), are the reconstructions. Again, we refrain from judging the results, for the same\nreasons as before: In real-world applications, whether privacy is preserved “well enough” will depend on the context.\nHere, first, are reconstructions from the run where the least noise was added.\n\n\n\nFigure 6: Reconstruction attempts from a setup where the target model was trained with an epsilon of 84.7.\n\n\n\nOn to the next level of privacy protection:\n\n\n\nFigure 7: Reconstruction attempts from a setup where the target model was trained with an epsilon of 12.5.\n\n\n\nAnd the highest-\\(\\epsilon\\) one:\n\n\n\nFigure 8: Reconstruction attempts from a setup where the target model was trained with an epsilon of 4.0.\n\n\n\nConclusion\nThroughout this post, we’ve refrained from “over-commenting” on results, and focused on the why-and-how instead. This is\nbecause in an artificial setup, chosen to facilitate exposition of concepts and methods, there really is no objective frame of\nreference. What is a good reconstruction? What is a good \\(\\epsilon\\)? What constitutes a data breach? No-one knows.\nIn the real world, there is a context to everything – there are people involved, the people whose data we’re talking about.\nThere are organizations, regulations, laws. There are abstract principles, and there are implementations; different\nimplementations of the same “idea” can differ.\nAs in machine learning overall, research papers on privacy-, ethics- or otherwise society-related topics are full of LaTeX\nformulae. Amid the math, let’s not forget the people.\nThanks for reading!\n\n\n\nDwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. “Calibrating Noise to Sensitivity in Private Data Analysis.” In Proceedings of the Third Conference on Theory of Cryptography, 265–84. TCC’06. Berlin, Heidelberg: Springer-Verlag. https://doi.org/10.1007/11681878_14.\n\n\nFredrikson, Matthew, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas Ristenpart. 2014. “Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing.” In Proceedings of the 23rd USENIX Conference on Security Symposium, 17–32. SEC’14. USA: USENIX Association.\n\n\nLake, Brenden M., Ruslan Salakhutdinov, and Joshua B. Tenenbaum. 2015. “Human-Level Concept Learning Through Probabilistic Program Induction.” Science 350 (6266): 1332–38. https://doi.org/10.1126/science.aab3050.\n\n\nMcMahan, H. Brendan, Eider Moore, Daniel Ramage, and Blaise Agüera y Arcas. 2016. “Federated Learning of Deep Networks Using Model Averaging.” CoRR abs/1602.05629. http://arxiv.org/abs/1602.05629.\n\n\nWu, X., M. Fredrikson, S. Jha, and J. F. Naughton. 2016. “A Methodology for Formalizing Model-Inversion Attacks.” In 2016 IEEE 29th Computer Security Foundations Symposium (CSF), 355–70.\n\n\nDon’t take all literally please; it’s just a nice phrase.↩︎\n",
    "preview": "posts/2020-05-15-model-inversion-attacks/images/results.png",
    "last_modified": "2024-11-21T15:53:29+00:00",
    "input_file": {},
    "preview_width": 600,
    "preview_height": 394
  },
  {
    "path": "posts/2020-04-29-encrypted_keras_with_syft/",
    "title": "Towards privacy: Encrypted deep learning with Syft and Keras",
    "description": "Deep learning need not be irreconcilable with privacy protection. Federated learning enables on-device, distributed model training; encryption keeps model and gradient updates private; differential privacy prevents the training data from leaking. As of today, private and secure deep learning is an emerging technology. In this post, we introduce Syft, an open-source framework that integrates with PyTorch as well as TensorFlow. In an example use case, we obtain private predictions from a Keras model.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-04-29",
    "categories": [
      "R",
      "Privacy & Security",
      "TensorFlow/Keras"
    ],
    "contents": "\nThe word privacy, in the context of deep learning (or machine learning, or “AI”), and especially when combined with things\nlike security, sounds like it could be part of a catch phrase: privacy, safety, security – like liberté, fraternité,\négalité. In fact, there should probably be a mantra like that. But that’s another topic, and like with the other catch phrase\njust cited, not everyone interprets these terms in the same way.\nSo let’s think about privacy, narrowed down to its role in training or using deep learning models, in a more technical way.\nSince privacy – or rather, its violations – may appear in various ways, different violations will demand different\ncountermeasures. Of course, in the end, we’d like to see them all integrated – but re privacy-related technologies, the field\nis really just starting out on a journey. The most important thing we can do, then, is to learn about the concepts,\ninvestigate the landscape of implementations under development, and – perhaps – decide to join the effort.\nThis post tries to do a tiny little bit of all of those.\nAspects of privacy in deep learning\nSay you work at a hospital, and would be interested in training a deep learning model to help diagnose some disease from brain\nscans. Where you work, you don’t have many patients with this disease; moreover, they tend to mostly be affected by the same\nsubtypes: Your training set, were you to create one, would not reflect the overall distribution very well. It would, thus,\nmake sense to cooperate with other hospitals; but that isn’t so easy, as the data collected is protected by privacy\nregulations. So, the first requirement is: The data has to stay where it is; e.g., it may not be sent to a central server.\nFederated learning\nThis first sine qua non is addressed by federated\nlearning (McMahan et al. 2016). Federated learning is\nnot “just” desirable for privacy reasons. On the contrary, in many use cases, it may be the only viable way (like with\nsmartphones or sensors, which collect gigantic amounts of data). In federated learning, each participant receives a copy of\nthe model, trains on their own data, and sends back the gradients obtained to the central server, where gradients are averaged\nand applied to the model.\nThis is good insofar as the data never leaves the individual devices; however, a lot of information can still be extracted\nfrom plain-text gradients. Imagine a smartphone app that provides trainable auto-completion for text messages. Even if\ngradient updates from many iterations are averaged, their distributions will greatly vary between individuals. Some form of\nencryption is needed. But then how is the server going to make sense of the encrypted gradients?\nOne way to accomplish this relies on secure multi-party computation (SMPC).\nSecure multi-party computation\nIn SMPC, we need a system of several agents who collaborate to provide a result no single agent could provide alone: “normal”\ncomputations (like addition, multiplication …) on “secret” (encrypted) data. The assumption is that these agents are “honest\nbut curious” – honest, because they won’t tamper with their share of data; curious in the sense that if they were (curious,\nthat is), they wouldn’t be able to inspect the data because it’s encrypted.\nThe principle behind this is secret sharing. A single piece of data – a salary, say – is “split up” into meaningless\n(hence, encrypted) parts which, when put together again, yield the original data. Here is an example.\nSay the parties involved are Julia, Greg, and me. The below function encrypts a single value, assigning to each of us their\n“meaningless” share:\n\n\n# a big prime number\n# all computations are performed in a finite field, for example, the integers modulo that prime\nQ <- 78090573363827\n \nencrypt <- function(x) {\n  # all but the very last share are random \n  julias <- runif(1, min = -Q, max = Q)\n  gregs <- runif(1, min = -Q, max = Q)\n  mine <- (x - julias - gregs) %% Q\n  list (julias, gregs, mine)\n}\n\n# some top secret value no-one may get to see\nvalue <- 77777\n\nencrypted <- encrypt(value)\nencrypted\n\n\n[[1]]\n[1] 7467283737857\n\n[[2]]\n[1] 36307804406429\n\n[[3]]\n[1] 34315485297318\nOnce the three of us put our shares together, getting back the plain value is straightforward:\n\n\ndecrypt <- function(shares) {\n  Reduce(sum, shares) %% Q  \n}\n\ndecrypt(encrypted)\n\n\n77777\nAs an example of how to compute on encrypted data, here’s addition. (Other operations will be a lot less straightforward.) To\nadd two numbers, just have everyone add their respective shares:\n\n\nadd <- function(x, y) {\n  list(\n    # julia\n    (x[[1]] + y[[1]]) %% Q,\n    # greg\n    (x[[2]] + y[[2]]) %% Q,\n    # me\n    (x[[3]] + y[[3]]) %% Q\n  )\n}\n  \nx <- encrypt(11)\ny <- encrypt(122)\n\ndecrypt(add(x, y))\n\n\n133\nBack to the setting of deep learning and the current task to be solved: Have the server apply gradient updates without ever\nseeing them. With secret sharing, it would work like this:\nJulia, Greg and me each want to train on our own private data. Together, we will be responsible for gradient averaging, that\nis, we’ll form a cluster of workers united in that task. Now, the model owner secret shares the model, and we start\ntraining, each on their own data. After some number of iterations, we use secure averaging to combine our respective\ngradients. Then, all the server gets to see is the mean gradient, and there is no way to determine our respective\ncontributions.\nBeyond private gradients\nAmazingly, it is even possible to train on encrypted data – amongst others, using that same technique of secret sharing. Of\ncourse, this has to negatively affect training speed. But it’s good to know that if one’s use case were to demand it, it would\nbe feasible. (One possible use case is when training on one party’s data alone doesn’t make any sense, but data is sensitive,\nso others won’t let you access their data unless encrypted.)\nSo with encryption available on an all-you-need basis, are we completely safe, privacy-wise? The answer is no. The model can\nstill leak information. For example, in some cases it is possible to perform model inversion [@abs-1805-04049], that is,\nwith just black-box access to a model, train an attack model that allows reconstructing some of the original training data.\nNeedless to say, this kind of leakage has to be avoided. Differential\nprivacy (Dwork et al. 2006), (Dwork 2006)\ndemands that results obtained from querying a model be independent from the presence or absence, in the dataset employed for\ntraining, of a single individual. In general, this is ensured by adding noise to the answer to every query. In training deep\nlearning models, we add noise to the gradients, as well as clip them according to some chosen norm.\nAt some point, then, we will want all of those in combination: federated learning, encryption, and differential privacy.\nSyft is a very promising, very actively developed framework that aims for providing all of them. Instead of “aims for,” I\nshould perhaps have written “provides” – it depends. We need some more context.\nIntroducing Syft\nSyft – also known as PySyft, since as of today, its most mature implementation is\nwritten in and for Python – is maintained by OpenMined, an open source community dedicated to\nenabling privacy-preserving AI. It’s worth it reproducing their mission statement here:\n\nIndustry standard tools for artificial intelligence have been designed with several assumptions: data is centralized into a\nsingle compute cluster, the cluster exists in a secure cloud, and the resulting models will be owned by a central authority.\nWe envision a world in which we are not restricted to this scenario - a world in which AI tools treat privacy, security, and\nmulti-owner governance as first class citizens. […] The mission of the OpenMined community is to create an accessible\necosystem of tools for private, secure, multi-owner governed AI.\n\nWhile far from being the only one, PySyft is their most maturely developed framework. Its role is to provide secure federated\nlearning, including encryption and differential privacy. For deep learning, it relies on existing frameworks.\nPyTorch integration seems the most mature, as of today; with PyTorch, encrypted and differentially private training are\nalready available. Integration with TensorFlow is a bit more involved; it does not yet include TensorFlow Federated and\nTensorFlow Privacy. For encryption, it relies on TensorFlow Encrypted (TFE),\nwhich as of this writing is not an official TensorFlow subproject.\nHowever, even now it is already possible to secret share Keras models and administer private predictions. Let’s see how.\nPrivate predictions with Syft, TensorFlow Encrypted and Keras\nOur introductory example will show how to use an externally-provided model to classify private data – without the model owner\never seeing that data, and without the user ever getting hold of (e.g., downloading) the model. (Think about the model owner\nwanting to keep the fruits of their labour hidden, as well.)\nPut differently: The model is encrypted, and the data is, too. As you might imagine, this involves a cluster of agents,\ntogether performing secure multi-party computation.\nThis use case presupposing an already trained model, we start by quickly creating one. There is nothing special going on here.\nPrelude: Train a simple model on MNIST\n\n\n# create_model.R\n\nlibrary(tensorflow)\nlibrary(keras)\n\nmnist <- dataset_mnist()\nmnist$train$x <- mnist$train$x/255\nmnist$test$x <- mnist$test$x/255\n\ndim(mnist$train$x) <- c(dim(mnist$train$x), 1)\ndim(mnist$test$x) <- c(dim(mnist$test$x), 1)\n\ninput_shape <- c(28, 28, 1)\n\nmodel <- keras_model_sequential() %>%\n  layer_conv_2d(filters = 16, kernel_size = c(3, 3), input_shape = input_shape) %>%\n  layer_average_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_activation(\"relu\") %>%\n  layer_conv_2d(filters = 32, kernel_size = c(3, 3)) %>%\n  layer_average_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_activation(\"relu\") %>%\n  layer_conv_2d(filters = 64, kernel_size = c(3, 3)) %>%\n  layer_average_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_activation(\"relu\") %>%\n  layer_flatten() %>%\n  layer_dense(units = 10, activation = \"linear\")\n  \n\nmodel %>% compile(\n  loss = \"sparse_categorical_crossentropy\",\n  optimizer = \"adam\",\n  metrics = \"accuracy\"\n)\n\nmodel %>% fit(\n    x = mnist$train$x,\n    y = mnist$train$y,\n    epochs = 1,\n    validation_split = 0.3,\n    verbose = 2\n)\n\nmodel$save(filepath = \"model.hdf5\")\n\n\nSet up cluster and serve model\nThe easiest way to get all required packages is to install the ensemble OpenMined put together for their Udacity\nCourse that introduces federated learning and differential\nprivacy with PySyft. This will install TensorFlow 1.15 and TensorFlow Encrypted, amongst others.\nThe following lines of code should all be put together in a single file. I found it practical to “source” this script from an\nR process running in a console tab.\nTo begin, we again define the model, two things being different now. First, for technical reasons, we need to pass in\nbatch_input_shape instead of input_shape. Second, the final layer is “missing” the softmax activation. This is not an\noversight – SMPC softmax has not been implemented yet. (Depending on when you read this, that statement may no longer be\ntrue.) Were we training this model in secret sharing mode, this would of course be a problem; for classification though, all\nwe care about is the maximum score.\nAfter model definition, we load the actual weights from the model we trained in the previous step. Then, the action begins. We\ncreate an ensemble of TFE workers that together run a distributed TensorFlow cluster. The model is secret shared with the\nworkers, that is, model weights are split up into shares that, each inspected alone, are unusable. Finally, the model is\nserved, i.e., made available to clients requesting predictions.\nHow can a Keras model be shared and served? These are not methods provided by Keras itself. The magic comes from Syft\nhooking into Keras, extending the model object: cf. hook <- sy$KerasHook(tf$keras) right after we import Syft.\n\n\n# serve.R\n# you could start R on the console and \"source\" this file\n\n# do this just once\nreticulate::py_install(\"syft[udacity]\")\n\nlibrary(tensorflow)\nlibrary(keras)\n\nsy <- reticulate::import((\"syft\"))\nhook <- sy$KerasHook(tf$keras)\n\nbatch_input_shape <- c(1, 28, 28, 1)\n\nmodel <- keras_model_sequential() %>%\n layer_conv_2d(filters = 16, kernel_size = c(3, 3), batch_input_shape = batch_input_shape) %>%\n layer_average_pooling_2d(pool_size = c(2, 2)) %>%\n layer_activation(\"relu\") %>%\n layer_conv_2d(filters = 32, kernel_size = c(3, 3)) %>%\n layer_average_pooling_2d(pool_size = c(2, 2)) %>%\n layer_activation(\"relu\") %>%\n layer_conv_2d(filters = 64, kernel_size = c(3, 3)) %>%\n layer_average_pooling_2d(pool_size = c(2, 2)) %>%\n layer_activation(\"relu\") %>%\n layer_flatten() %>%\n layer_dense(units = 10) \n \npre_trained_weights <- \"model.hdf5\"\nmodel$load_weights(pre_trained_weights)\n\n# create and start TFE cluster\nAUTO <- TRUE\njulia <- sy$TFEWorker(host = 'localhost:4000', auto_managed = AUTO)\ngreg <- sy$TFEWorker(host = 'localhost:4001', auto_managed = AUTO)\nme <- sy$TFEWorker(host = 'localhost:4002', auto_managed = AUTO)\ncluster <- sy$TFECluster(julia, greg, me)\ncluster$start()\n\n# split up model weights into shares \nmodel$share(cluster)\n\n# serve model (limiting number of requests)\nmodel$serve(num_requests = 3L)\n\n\nOnce the desired number of requests have been served, we can go to this R process, stop model sharing, and shut down the\ncluster:\n\n\n# stop model sharing\nmodel$stop()\n\n# stop cluster\ncluster$stop()\n\n\nNow, on to the client(s).\nRequest predictions on private data\nIn our example, we have one client. The client is a TFE worker, just like the agents that make up the cluster.\nWe define the cluster here, client-side, as well; create the client; and connect the client to the model. This will set up a\nqueueing server that takes care of secret sharing all input data before submitting them for prediction.\nFinally, we have the client asking for classification of the first three MNIST images.\nWith the server running in some different R process, we can conveniently run this in RStudio:\n\n\n# client.R\n\nlibrary(tensorflow)\nlibrary(keras)\n\nsy <- reticulate::import((\"syft\"))\nhook <- sy$KerasHook(tf$keras)\n\nmnist <- dataset_mnist()\nmnist$train$x <- mnist$train$x/255\nmnist$test$x <- mnist$test$x/255\n\ndim(mnist$train$x) <- c(dim(mnist$train$x), 1)\ndim(mnist$test$x) <- c(dim(mnist$test$x), 1)\n\nbatch_input_shape <- c(1, 28, 28, 1)\nbatch_output_shape <- c(1, 10)\n\n# define the same TFE cluster\nAUTO <- TRUE\njulia <- sy$TFEWorker(host = 'localhost:4000', auto_managed = AUTO)\ngreg <- sy$TFEWorker(host = 'localhost:4001', auto_managed = AUTO)\nme <- sy$TFEWorker(host = 'localhost:4002', auto_managed = AUTO)\ncluster <- sy$TFECluster(julia, greg, me)\n\n# create the client\nclient <- sy$TFEWorker()\n\n# create a queueing server on the client that secret shares the data \n# before submitting a prediction request\nclient$connect_to_model(batch_input_shape, batch_output_shape, cluster)\n\nnum_tests <- 3\nimages <- mnist$test$x[1: num_tests, , , , drop = FALSE]\nexpected_labels <- mnist$test$y[1: num_tests]\n\nfor (i in 1:num_tests) {\n  res <- client$query_model(images[i, , , , drop = FALSE])\n  predicted_label <- which.max(res) - 1\n  cat(\"Actual: \", expected_labels[i], \", predicted: \", predicted_label)\n}\n\n\nActual:  7 , predicted:  7 \nActual:  2 , predicted:  2 \nActual:  1 , predicted:  1 \nThere we go. Both model and data did remain secret, yet we were able to classify our data.\nLet’s wrap up.\nConclusion\nOur example use case has not been too ambitious – we started with a trained model, thus leaving aside federated learning.\nKeeping the setup simple, we were able to focus on underlying principles: Secret sharing as a means of encryption, and\nsetting up a Syft/TFE cluster of workers that together, provide the infrastructure for encrypting model weights as well as\nclient data.\nIn case you’ve read our previous post on TensorFlow\nFederated – that, too, a framework under\ndevelopment – you may have gotten an impression similar to the one I got: Setting up Syft was a lot more straightforward,\nconcepts were easy to grasp, and surprisingly little code was required. As we may gather from a recent blog\npost, integration of Syft with TensorFlow Federated and TensorFlow\nPrivacy are on the roadmap. I am looking forward a lot for this to happen.\nThanks for reading!\n\n\n\n\nDwork, Cynthia. 2006. “Differential Privacy.” In 33rd International Colloquium on Automata, Languages and Programming, Part II (ICALP 2006), 33rd International Colloquium on Automata, Languages and Programming, part II (ICALP 2006), 4052:1–12. Lecture Notes in Computer Science. Springer Verlag. https://www.microsoft.com/en-us/research/publication/differential-privacy/.\n\n\nDwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. “Calibrating Noise to Sensitivity in Private Data Analysis.” In Proceedings of the Third Conference on Theory of Cryptography, 265–84. TCC’06. Berlin, Heidelberg: Springer-Verlag. https://doi.org/10.1007/11681878_14.\n\n\nMcMahan, H. Brendan, Eider Moore, Daniel Ramage, and Blaise Agüera y Arcas. 2016. “Federated Learning of Deep Networks Using Model Averaging.” CoRR abs/1602.05629. http://arxiv.org/abs/1602.05629.\n\n\n\n\n",
    "preview": "posts/2020-04-29-encrypted_keras_with_syft/images/thumb.jpg",
    "last_modified": "2024-11-21T15:54:18+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-04-21-sparklyr-1.2.0-released/",
    "title": "sparklyr 1.2: Foreach, Spark 3.0 and Databricks Connect",
    "description": "A new sparklyr release is now available. This sparklyr 1.2 release features new functionalities such as support for Databricks Connect, a Spark backend for the 'foreach' package, inter-op improvements for working with Spark 3.0 preview, as well as a number of bug fixes and improvements addressing user-visible pain points.",
    "author": [
      {
        "name": "Yitao Li",
        "url": "https://github.com/yl790"
      }
    ],
    "date": "2020-04-21",
    "categories": [
      "R",
      "Packages/Releases",
      "Distributed Computing"
    ],
    "contents": "\nBehold the glory that is sparklyr 1.2! In this release, the following new hotnesses have emerged into spotlight:\nA registerDoSpark method to create a foreach parallel backend powered by Spark that enables hundreds of existing R packages to run in Spark.\nSupport for Databricks Connect, allowing sparklyr to connect to remote Databricks clusters.\nImproved support for Spark structures when collecting and querying their nested attributes with dplyr.\nA number of inter-op issues observed with sparklyr and Spark 3.0 preview were also addressed recently, in hope that by the time Spark 3.0 officially graces us with its presence, sparklyr will be fully ready to work with it. Most notably, key features such as spark_submit, sdf_bind_rows, and standalone connections are now finally working with Spark 3.0 preview.\nTo install sparklyr 1.2 from CRAN run,\n\n\ninstall.packages(\"sparklyr\")\n\n\nThe full list of changes are available in the sparklyr NEWS file.\nForeach\nThe foreach package provides the %dopar% operator to iterate over elements in a collection in parallel. Using sparklyr 1.2, you can now register Spark as a backend using registerDoSpark() and then easily iterate over R objects using Spark:\n\n\nlibrary(sparklyr)\nlibrary(foreach)\n\nsc <- spark_connect(master = \"local\", version = \"2.4\")\n\nregisterDoSpark(sc)\nforeach(i = 1:3, .combine = 'c') %dopar% {\n  sqrt(i)\n}\n\n\n[1] 1.000000 1.414214 1.732051\nSince many R packages are based on foreach to perform parallel computation, we can now make use of all those great packages in Spark as well!\nFor instance, we can use parsnip and the tune package with data from mlbench to perform hyperparameter tuning in Spark with ease:\n\n\nlibrary(tune)\nlibrary(parsnip)\nlibrary(mlbench)\n\ndata(Ionosphere)\nsvm_rbf(cost = tune(), rbf_sigma = tune()) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kernlab\") %>%\n  tune_grid(Class ~ .,\n    resamples = rsample::bootstraps(dplyr::select(Ionosphere, -V2), times = 30),\n    control = control_grid(verbose = FALSE))\n\n\n# Bootstrap sampling\n# A tibble: 30 x 4\n   splits            id          .metrics          .notes\n * <list>            <chr>       <list>            <list>\n 1 <split [351/124]> Bootstrap01 <tibble [10 × 5]> <tibble [0 × 1]>\n 2 <split [351/126]> Bootstrap02 <tibble [10 × 5]> <tibble [0 × 1]>\n 3 <split [351/125]> Bootstrap03 <tibble [10 × 5]> <tibble [0 × 1]>\n 4 <split [351/135]> Bootstrap04 <tibble [10 × 5]> <tibble [0 × 1]>\n 5 <split [351/127]> Bootstrap05 <tibble [10 × 5]> <tibble [0 × 1]>\n 6 <split [351/131]> Bootstrap06 <tibble [10 × 5]> <tibble [0 × 1]>\n 7 <split [351/141]> Bootstrap07 <tibble [10 × 5]> <tibble [0 × 1]>\n 8 <split [351/123]> Bootstrap08 <tibble [10 × 5]> <tibble [0 × 1]>\n 9 <split [351/118]> Bootstrap09 <tibble [10 × 5]> <tibble [0 × 1]>\n10 <split [351/136]> Bootstrap10 <tibble [10 × 5]> <tibble [0 × 1]>\n# … with 20 more rows\nThe Spark connection was already registered, so the code ran in Spark without any additional changes. We can verify this was the case by navigating to the Spark web interface:\n\nDatabricks Connect\nDatabricks Connect allows you to connect your favorite IDE (like RStudio!) to a Spark Databricks cluster.\nYou will first have to install the databricks-connect package as described in our README and start a Databricks cluster, but once that’s ready, connecting to the remote cluster is as easy as running:\n\n\nsc <- spark_connect(\n  method = \"databricks\",\n  spark_home = system2(\"databricks-connect\", \"get-spark-home\", stdout = TRUE))\n\n\n\nThat’s about it, you are now remotely connected to a Databricks cluster from your local R session.\nStructures\nIf you previously used collect to deserialize structurally complex Spark dataframes into their equivalents in R, you likely have noticed Spark SQL struct columns were only mapped into JSON strings in R, which was non-ideal. You might also have run into a much dreaded java.lang.IllegalArgumentException: Invalid type list error when using dplyr to query nested attributes from any struct column of a Spark dataframe in sparklyr.\nUnfortunately, often times in real-world Spark use cases, data describing entities comprising of sub-entities (e.g., a product catalog of all hardware components of some computers) needs to be denormalized / shaped in an object-oriented manner in the form of Spark SQL structs to allow efficient read queries. When sparklyr had the limitations mentioned above, users often had to invent their own workarounds when querying Spark struct columns, which explained why there was a mass popular demand for sparklyr to have better support for such use cases.\nThe good news is with sparklyr 1.2, those limitations no longer exist any more when working running with Spark 2.4 or above.\nAs a concrete example, consider the following catalog of computers:\n\n\nlibrary(dplyr)\n\ncomputers <- tibble::tibble(\n  id = seq(1, 2),\n  attributes = list(\n    list(\n      processor = list(freq = 2.4, num_cores = 256),\n      price = 100\n   ),\n   list(\n     processor = list(freq = 1.6, num_cores = 512),\n     price = 133\n   )\n  )\n)\n\ncomputers <- copy_to(sc, computers, overwrite = TRUE)\n\n\nA typical dplyr use case involving computers would be the following:\n\n\nhigh_freq_computers <- computers %>%\n                       filter(attributes.processor.freq >= 2) %>%\n                       collect()\n\n\nAs previously mentioned, before sparklyr 1.2, such query would fail with Error: java.lang.IllegalArgumentException: Invalid type list.\nWhereas with sparklyr 1.2, the expected result is returned in the following form:\n# A tibble: 1 x 2\n     id attributes\n  <int> <list>\n1     1 <named list [2]>\nwhere high_freq_computers$attributes is what we would expect:\n[[1]]\n[[1]]$price\n[1] 100\n\n[[1]]$processor\n[[1]]$processor$freq\n[1] 2.4\n\n[[1]]$processor$num_cores\n[1] 256\nAnd More!\nLast but not least, we heard about a number of pain points sparklyr users have run into, and have addressed many of them in this release as well. For example:\nDate type in R is now correctly serialized into Spark SQL date type by copy_to\n<spark dataframe> %>% print(n = 20) now actually prints 20 rows as expected instead of 10\nspark_connect(master = \"local\") will emit a more informative error message if it’s failing because the loopback interface is not up\n… to just name a few. We want to thank the open source community for their continuous feedback on sparklyr, and are looking forward to incorporating more of that feedback to make sparklyr even better in the future.\nFinally, in chronological order, we wish to thank the following individuals for contributing to sparklyr 1.2: zero323, Andy Zhang, Yitao Li,\nJavier Luraschi, Hossein Falaki, Lu Wang, Samuel Macedo and Jozef Hajnala. Great job everyone!\nIf you need to catch up on sparklyr, please visit sparklyr.ai, spark.rstudio.com, or some of the previous release posts: sparklyr 1.1 and sparklyr 1.0.\nThank you for reading this post.\n\n\n\n",
    "preview": "posts/2020-04-21-sparklyr-1.2.0-released/images/sparklyr.png",
    "last_modified": "2024-11-21T15:51:49+00:00",
    "input_file": {},
    "preview_width": 1241,
    "preview_height": 307
  },
  {
    "path": "posts/2020-04-13-pins-04/",
    "title": "pins 0.4: Versioning",
    "description": "A new release of pins is available on CRAN today. This release adds support to time travel across dataset versions, which improves collaboration and protects your code from breaking when remote resources change unexpectedly.",
    "author": [
      {
        "name": "Javier Luraschi",
        "url": {}
      }
    ],
    "date": "2020-04-13",
    "categories": [
      "R",
      "Packages/Releases",
      "Data Management"
    ],
    "contents": "\nA new version of pins is available on CRAN today, which adds support for versioning your datasets and DigitalOcean Spaces boards!\nAs a quick recap, the pins package allows you to cache, discover and share resources. You can use pins in a wide range of situations, from downloading a dataset from a URL to creating complex automation workflows (learn more at pins.rstudio.com). You can also use pins in combination with TensorFlow and Keras; for instance, use cloudml to train models in cloud GPUs, but rather than manually copying files into the GPU instance, you can store them as pins directly from R.\nTo install this new version of pins from CRAN, simply run:\ninstall.packages(\"pins\")\nYou can find a detailed list of improvements in the pins NEWS file.\nVersioning\nTo illustrate the new versioning functionality, let’s start by downloading and caching a remote dataset with pins. For this example, we will download the weather in London, this happens to be in JSON format and requires jsonlite to be parsed:\nlibrary(pins)\n\nweather_url <- \"https://samples.openweathermap.org/data/2.5/weather?q=London,uk&appid=b6907d289e10d714a6e88b30761fae22\"\n\npin(weather_url, \"weather\") %>%\n  jsonlite::read_json() %>%\n  as.data.frame()\n  coord.lon coord.lat weather.id weather.main     weather.description weather.icon\n1     -0.13     51.51        300      Drizzle light intensity drizzle          09d\nOne advantage of using pins is that, even if the URL or your internet connection becomes unavailable, the above code will still work.\nBut back to pins 0.4! The new signature parameter in pin_info() allows you to retrieve the “version” of this dataset:\npin_info(\"weather\", signature = TRUE)\n# Source: local<weather> [files]\n# Signature: 624cca260666c6f090b93c37fd76878e3a12a79b\n# Properties:\n#   - path: weather\nYou can then validate the remote dataset has not changed by specifying its signature:\npin(weather_url, \"weather\", signature = \"624cca260666c6f090b93c37fd76878e3a12a79b\") %>%\n  jsonlite::read_json()\nIf the remote dataset changes, pin() will fail and you can take the appropriate steps to accept the changes by updating the signature or properly updating your code. The previous example is useful as a way of detecting version changes, but we might also want to retrieve specific versions even when the dataset changes.\npins 0.4 allows you to display and retrieve versions from services like GitHub, Kaggle and RStudio Connect. Even in boards that don’t support versioning natively, you can opt-in by registering a board with versions = TRUE.\nTo keep this simple, let’s focus on GitHub first. We will register a GitHub board and pin a dataset to it. Notice that you can also specify the commit parameter in GitHub boards as the commit message for this change.\nboard_register_github(repo = \"javierluraschi/datasets\", branch = \"datasets\")\n\npin(iris, name = \"versioned\", board = \"github\", commit = \"use iris as the main dataset\")\nNow suppose that a colleague comes along and updates this dataset as well:\npin(mtcars, name = \"versioned\", board = \"github\", commit = \"slight preference to mtcars\")\nFrom now on, your code could be broken or, even worse, produce incorrect results!\nHowever, since GitHub was designed as a version control system and pins 0.4 adds support for pin_versions(), we can now explore particular versions of this dataset:\npin_versions(\"versioned\", board = \"github\")\n# A tibble: 2 x 4\n  version created              author         message                     \n  <chr>   <chr>                <chr>          <chr>                       \n1 6e6c320 2020-04-02T21:28:07Z javierluraschi slight preference to mtcars \n2 01f8ddf 2020-04-02T21:27:59Z javierluraschi use iris as the main dataset\nYou can then retrieve the version you are interested in as follows:\npin_get(\"versioned\", version = \"01f8ddf\", board = \"github\")\n# A tibble: 150 x 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 140 more rows\nYou can follow similar steps for RStudio Connect and Kaggle boards, even for existing pins! Other boards like Amazon S3, Google Cloud, Digital Ocean and Microsoft Azure require you explicitly enable versioning when registering your boards.\nDigitalOcean\nTo try out the new DigitalOcean Spaces board, first you will have to register this board and enable versioning by setting versions to TRUE:\nlibrary(pins)\nboard_register_dospace(space = \"pinstest\",\n                       key = \"AAAAAAAAAAAAAAAAAAAA\",\n                       secret = \"ABCABCABCABCABCABCABCABCABCABCABCABCABCA==\",\n                       datacenter = \"sfo2\",\n                       versions = TRUE)\nYou can then use all the functionality pins provides, including versioning:\n# create pin and replace content in digitalocean\npin(iris, name = \"versioned\", board = \"pinstest\")\npin(mtcars, name = \"versioned\", board = \"pinstest\")\n\n# retrieve versions from digitalocean\npin_versions(name = \"versioned\", board = \"pinstest\")\n# A tibble: 2 x 1\n  version\n  <chr>  \n1 c35da04\n2 d9034cd\nNotice that enabling versions in cloud services requires additional storage space for each version of the dataset being stored:\n\nTo learn more visit the Versioning and DigitalOcean articles. To catch up with previous releases:\npins 0.3: Azure, GCloud and S3\npins 0.2: Pin, Discover and Share Resources\nThanks for reading along!\n\n\n\n",
    "preview": "posts/2020-04-13-pins-04/images/thumb.jpg",
    "last_modified": "2024-11-21T15:49:48+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-04-08-tf-federated-intro/",
    "title": "A first look at federated learning with TensorFlow",
    "description": "The term \"federated learning\" was coined to describe a form of distributed model training where the data remains on client devices, i.e., is never shipped to the coordinating server. In this post, we introduce central concepts and run first experiments with TensorFlow Federated, using R.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-04-08",
    "categories": [
      "Privacy & Security",
      "TensorFlow/Keras"
    ],
    "contents": "\nHere, stereotypically, is the process of applied deep learning: Gather/get data;\niteratively train and evaluate; deploy. Repeat (or have it all automated as a\ncontinuous workflow). We often discuss training and evaluation;\ndeployment matters to varying degrees, depending on the circumstances. But the\ndata often is just assumed to be there: All together, in one place (on your\nlaptop; on a central server; in some cluster in the cloud.) In real life though,\ndata could be all over the world: on smartphones for example, or on IoT devices.\nThere are a lot of reasons why we don’t want to ship all that data to some central\nlocation: Privacy, of course (why should some third party get to know about what\nyou texted your friend?); but also, sheer mass (and this latter aspect is bound\nto become more influential all the time).\nA solution is that data on client devices stays on client devices, yet\nparticipates in training a global model. How? In so-called federated\nlearning(McMahan et al. 2016), there is a central coordinator (“server”), as well as\na potentially huge number of clients (e.g., phones) who participate in learning\non an “as-fits” basis: e.g., if plugged in and on a high-speed connection.\nWhenever they’re ready to train, clients are passed the current model weights,\nand perform some number of training iterations on their own data. They then send\nback gradient information to the server (more on that soon), whose job is to\nupdate the weights accordingly. Federated learning is not the only conceivable\nprotocol to jointly train a deep learning model while keeping the data private:\nA fully decentralized alternative could be gossip learning (Blot et al. 2016),\nfollowing the gossip protocol .\nAs of today, however, I am not aware of existing implementations in any of the\nmajor deep learning frameworks.\nIn fact, even TensorFlow Federated (TFF), the library used in this post, was\nofficially introduced just about a year ago. Meaning, all this is pretty new\ntechnology, somewhere inbetween proof-of-concept state and production readiness.\nSo, let’s set expectations as to what you might get out of this post.\nWhat to expect from this post\nWe start with quick glance at federated learning in the context of privacy\noverall. Subsequently, we introduce, by example, some of TFF’s basic building\nblocks. Finally, we show a complete image classification example using Keras –\nfrom R.\nWhile this sounds like “business as usual,” it’s not – or not quite. With no R\npackage existing, as of this writing, that would wrap TFF, we’re accessing its\nfunctionality using $-syntax – not in itself a big problem. But there’s\nsomething else.\nTFF, while providing a Python API, itself is not written in Python. Instead, it\nis an internal language designed specifically for serializability and\ndistributed computation. One of the consequences is that TensorFlow (that is: TF\nas opposed to TFF) code has to be wrapped in calls to tf.function, triggering\nstatic-graph construction. However, as I write this, the TFF documentation\ncautions:\n“Currently, TensorFlow does not fully support serializing and deserializing\neager-mode TensorFlow.” Now when we call TFF from R, we add another layer of\ncomplexity, and are more likely to run into corner cases.\nTherefore, at the current\nstage, when using TFF from R it’s advisable to play around with high-level\nfunctionality – using Keras models – instead of, e.g., translating to R the\nlow-level functionality shown in the second TFF Core\ntutorial.\nOne final remark before we get started: As of this writing, there is no\ndocumentation on how to actually run federated training on “real clients.” There is, however, a\ndocument\nthat describes how to run TFF on Google Kubernetes Engine, and\ndeployment-related documentation is visibly and steadily growing.)\nThat said, now how does federated learning relate to privacy, and how does it\nlook in TFF?\nFederated learning in context\nIn federated learning, client data never leaves the device. So in an immediate\nsense, computations are private. However, gradient updates are sent to a central\nserver, and this is where privacy guarantees may be violated. In some cases, it\nmay be easy to reconstruct the actual data from the gradients – in an NLP task,\nfor example, when the vocabulary is known on the server, and gradient updates\nare sent for small pieces of text.\nThis may sound like a special case, but general methods have been demonstrated\nthat work regardless of circumstances. For example, Zhu et\nal. (Zhu, Liu, and Han 2019) use a “generative” approach, with the server starting\nfrom randomly generated fake data (resulting in fake gradients) and then,\niteratively updating that data to obtain gradients more and more like the real\nones – at which point the real data has been reconstructed.\nComparable attacks would not be feasible were gradients not sent in clear text.\nHowever, the server needs to actually use them to update the model – so it must\nbe able to “see” them, right? As hopeless as this sounds, there are ways out\nof the dilemma. For example, homomorphic\nencryption, a technique\nthat enables computation on encrypted data. Or secure multi-party\naggregation,\noften achieved through secret\nsharing, where individual pieces\nof data (e.g.: individual salaries) are split up into “shares,” exchanged and\ncombined with random data in various ways, until finally the desired global\nresult (e.g.: mean salary) is computed. (These are extremely fascinating topics\nthat unfortunately, by far surpass the scope of this post.)\nNow, with the server prevented from actually “seeing” the gradients, a problem\nstill remains. The model – especially a high-capacity one, with many parameters\n– could still memorize individual training data. Here is where differential\nprivacy comes into play. In differential privacy, noise is added to the\ngradients to decouple them from actual training examples. (This\npost\ngives an introduction to differential privacy with TensorFlow, from R.)\nAs of this writing, TFF’s federal averaging mechanism (McMahan et al. 2016) does not\nyet include these additional privacy-preserving techniques. But research papers\nexist that outline algorithms for integrating both secure aggregation\n(Bonawitz et al. 2016) and differential privacy (McMahan et al. 2017) .\nClient-side and server-side computations\nLike we said above, at this point it is advisable to mainly stick with\nhigh-level computations using TFF from R. (Presumably that is what we’d be interested in\nin many cases, anyway.) But it’s instructive to look at a few building blocks\nfrom a high-level, functional point of view.\nIn federated learning, model training happens on the clients. Clients each\ncompute their local gradients, as well as local metrics. The server, on the other hand,\ncalculates global gradient updates, as well as global metrics.\nLet’s say the metric is accuracy. Then clients and server both compute averages: local\naverages and a global average, respectively. All the server will need to know to\ndetermine the global averages are the local ones and the respective sample\nsizes.\nLet’s see how TFF would calculate a simple average.\nThe code in this post was run with the current TensorFlow release 2.1 and TFF\nversion 0.13.1. We use reticulate to install and import TFF.\n\n\nlibrary(tensorflow)\nlibrary(reticulate)\nlibrary(tfdatasets)\n\npy_install(\"tensorflow-federated\")\n\ntff <- import(\"tensorflow_federated\")\n\n\nFirst, we need every client to be able to compute their own local averages.\nHere is a function that reduces a list of values to their sum and count, both\nat the same time, and then returns their quotient.\nThe function contains only TensorFlow operations, not computations described in R\ndirectly; if there were any, they would have to be wrapped in calls to\ntf_function, calling for construction of a static graph. (The same would apply\nto raw (non-TF) Python code.)\nNow, this function will still have to be wrapped (we’re getting to that in an\ninstant), as TFF expects functions that make use of TF operations to be\ndecorated by calls to tff$tf_computation. Before we do that, one comment on\nthe use of dataset_reduce: Inside tff$tf_computation, the data that is\npassed in behaves like a dataset, so we can perform tfdatasets operations\nlike dataset_map, dataset_filter etc. on it.\n\n\nget_local_temperature_average <- function(local_temperatures) {\n  sum_and_count <- local_temperatures %>% \n    dataset_reduce(tuple(0, 0), function(x, y) tuple(x[[1]] + y, x[[2]] + 1))\n  sum_and_count[[1]] / tf$cast(sum_and_count[[2]], tf$float32)\n}\n\n\nNext is the call to tff$tf_computation we already alluded to, wrapping\nget_local_temperature_average. We also need to indicate the\nargument’s TFF-level type.\n(In the context of this post, TFF datatypes are\ndefinitely out-of-scope, but the TFF documentation has lots of detailed\ninformation in that regard. All we need to know right now is that we will be able to pass the data\nas a list.)\n\n\nget_local_temperature_average <- tff$tf_computation(get_local_temperature_average, tff$SequenceType(tf$float32))\n\n\nLet’s test this function:\n\n\nget_local_temperature_average(list(1, 2, 3))\n\n\n[1] 2\nSo that’s a local average, but we originally set out to compute a global one.\nTime to move on to server side (code-wise).\nNon-local computations are called federated (not too surprisingly). Individual\noperations start with federated_; and these have to be wrapped in\ntff$federated_computation:\n\n\nget_global_temperature_average <- function(sensor_readings) {\n  tff$federated_mean(tff$federated_map(get_local_temperature_average, sensor_readings))\n}\n\nget_global_temperature_average <- tff$federated_computation(\n  get_global_temperature_average, tff$FederatedType(tff$SequenceType(tf$float32), tff$CLIENTS))\n\n\nCalling this on a list of lists – each sub-list presumedly representing client data – will display the global (non-weighted) average:\n\n\nget_global_temperature_average(list(list(1, 1, 1), list(13)))\n\n\n[1] 7\nNow that we’ve gotten a bit of a feeling for “low-level TFF,” let’s train a\nKeras model the federated way.\nFederated Keras\nThe setup for this example looks a bit more Pythonian1 than usual. We need the\ncollections module from Python to make use of OrderedDicts, and we want them to be passed to Python without\nintermediate conversion to R – that’s why we import the module with convert\nset to FALSE.\n\n\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(tfds)\nlibrary(reticulate)\nlibrary(tfdatasets)\nlibrary(dplyr)\n\ntff <- import(\"tensorflow_federated\")\ncollections <- import(\"collections\", convert = FALSE)\nnp <- import(\"numpy\")\n\n\nFor this example, we use Kuzushiji-MNIST\n(Clanuwat et al. 2018), which may conveniently be obtained through\ntfds, the R wrapper for TensorFlow\nDatasets.\nThe 10 classes of Kuzushiji-MNIST, with the first column showing each\ncharacter's modern hiragana counterpart. From:\nhttps://github.com/rois-codh/kmnistTensorFlow datasets come as – well – datasets, which normally would be just\nfine; here however, we want to simulate different clients each with their own\ndata. The following code splits up the dataset into ten arbitrary – sequential,\nfor convenience – ranges and, for each range (that is: client), creates a list of\nOrderedDicts that have the images as their x, and the labels as their y\ncomponent:\n\n\nn_train <- 60000\nn_test <- 10000\n\ns <- seq(0, 90, by = 10)\ntrain_ranges <- paste0(\"train[\", s, \"%:\", s + 10, \"%]\") %>% as.list()\ntrain_splits <- purrr::map(train_ranges, function(r) tfds_load(\"kmnist\", split = r))\n\ntest_ranges <- paste0(\"test[\", s, \"%:\", s + 10, \"%]\") %>% as.list()\ntest_splits <- purrr::map(test_ranges, function(r) tfds_load(\"kmnist\", split = r))\n\nbatch_size <- 100\n\ncreate_client_dataset <- function(source, n_total, batch_size) {\n  iter <- as_iterator(source %>% dataset_batch(batch_size))\n  output_sequence <- vector(mode = \"list\", length = n_total/10/batch_size)\n  i <- 1\n  while (TRUE) {\n    item <- iter_next(iter)\n    if (is.null(item)) break\n    x <- tf$reshape(tf$cast(item$image, tf$float32), list(100L,784L))/255\n    y <- item$label\n    output_sequence[[i]] <-\n      collections$OrderedDict(\"x\" = np_array(x$numpy(), np$float32), \"y\" = y$numpy())\n     i <- i + 1\n  }\n  output_sequence\n}\n\nfederated_train_data <- purrr::map(\n  train_splits, function(split) create_client_dataset(split, n_train, batch_size))\n\n\nAs a quick check, the following are the labels for the first batch of images for\nclient 5:\n\n\nfederated_train_data[[5]][[1]][['y']]\n\n\n> [0. 9. 8. 3. 1. 6. 2. 8. 8. 2. 5. 7. 1. 6. 1. 0. 3. 8. 5. 0. 5. 6. 6. 5.\n 2. 9. 5. 0. 3. 1. 0. 0. 6. 3. 6. 8. 2. 8. 9. 8. 5. 2. 9. 0. 2. 8. 7. 9.\n 2. 5. 1. 7. 1. 9. 1. 6. 0. 8. 6. 0. 5. 1. 3. 5. 4. 5. 3. 1. 3. 5. 3. 1.\n 0. 2. 7. 9. 6. 2. 8. 8. 4. 9. 4. 2. 9. 5. 7. 6. 5. 2. 0. 3. 4. 7. 8. 1.\n 8. 2. 7. 9.]\nThe model is a simple, one-layer sequential Keras model. For TFF to have full\ncontrol over graph construction, it has to be defined inside a function. The\nblueprint for creation is passed to tff$learning$from_keras_model, together\nwith a “dummy” batch that exemplifies how the training data will look:\n\n\nsample_batch = federated_train_data[[5]][[1]]\n\ncreate_keras_model <- function() {\n  keras_model_sequential() %>%\n    layer_dense(input_shape = 784,\n                units = 10,\n                kernel_initializer = \"zeros\",\n                activation = \"softmax\") \n}\n\nmodel_fn <- function() {\n  keras_model <- create_keras_model()\n  tff$learning$from_keras_model(\n    keras_model,\n    dummy_batch = sample_batch,\n    loss = tf$keras$losses$SparseCategoricalCrossentropy(),\n    metrics = list(tf$keras$metrics$SparseCategoricalAccuracy()))\n}\n\n\nTraining is a stateful process that keeps updating model weights (and if\napplicable, optimizer states). It is created via\ntff$learning$build_federated_averaging_process …\n\n\niterative_process <- tff$learning$build_federated_averaging_process(\n  model_fn,\n  client_optimizer_fn = function() tf$keras$optimizers$SGD(learning_rate = 0.02),\n  server_optimizer_fn = function() tf$keras$optimizers$SGD(learning_rate = 1.0))\n\n\n… and on initialization, produces a starting state:\n\n\nstate <- iterative_process$initialize()\nstate\n\n\n<model=<trainable=<[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]],[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]>,non_trainable=<>>,optimizer_state=<0>,delta_aggregate_state=<>,model_broadcast_state=<>>\nThus before training, all the state does is reflect our zero-initialized model\nweights.\nNow, state transitions are accomplished via calls to next(). After one round\nof training, the state then comprises the “state proper” (weights, optimizer\nparameters …) as well as the current training metrics:\n\n\nstate_and_metrics <- iterative_process$`next`(state, federated_train_data)\n\nstate <- state_and_metrics[0]\nstate\n\n\n<model=<trainable=<[[ 9.9695253e-06 -8.5083229e-05 -8.9266898e-05 ... -7.7834651e-05\n  -9.4819807e-05  3.4227365e-04]\n [-5.4778640e-05 -1.5390900e-04 -1.7912561e-04 ... -1.4122366e-04\n  -2.4614178e-04  7.7663612e-04]\n [-1.9177950e-04 -9.0706220e-05 -2.9841764e-04 ... -2.2249141e-04\n  -4.1685964e-04  1.1348884e-03]\n ...\n [-1.3832574e-03 -5.3664664e-04 -3.6622395e-04 ... -9.0854493e-04\n   4.9618416e-04  2.6899918e-03]\n [-7.7253254e-04 -2.4583895e-04 -8.3220737e-05 ... -4.5274393e-04\n   2.6396243e-04  1.7454443e-03]\n [-2.4157032e-04 -1.3836231e-05  5.0371520e-05 ... -1.0652864e-04\n   1.5947431e-04  4.5250656e-04]],[-0.01264258  0.00974309  0.00814162  0.00846065 -0.0162328   0.01627758\n -0.00445857 -0.01607843  0.00563046  0.00115899]>,non_trainable=<>>,optimizer_state=<1>,delta_aggregate_state=<>,model_broadcast_state=<>>\n\n\nmetrics <- state_and_metrics[1]\nmetrics\n\n\n<sparse_categorical_accuracy=0.5710999965667725,loss=1.8662642240524292,keras_training_time_client_sum_sec=0.0>\nLet’s train for a few more epochs, keeping track of accuracy:\n\n\nnum_rounds <- 20\n\nfor (round_num in (2:num_rounds)) {\n  state_and_metrics <- iterative_process$`next`(state, federated_train_data)\n  state <- state_and_metrics[0]\n  metrics <- state_and_metrics[1]\n  cat(\"round: \", round_num, \"  accuracy: \", round(metrics$sparse_categorical_accuracy, 4), \"\\n\")\n}\n\n\nround:  2    accuracy:  0.6949 \nround:  3    accuracy:  0.7132 \nround:  4    accuracy:  0.7231 \nround:  5    accuracy:  0.7319 \nround:  6    accuracy:  0.7404 \nround:  7    accuracy:  0.7484 \nround:  8    accuracy:  0.7557 \nround:  9    accuracy:  0.7617 \nround:  10   accuracy:  0.7661 \nround:  11   accuracy:  0.7695 \nround:  12   accuracy:  0.7728 \nround:  13   accuracy:  0.7764 \nround:  14   accuracy:  0.7788 \nround:  15   accuracy:  0.7814 \nround:  16   accuracy:  0.7836 \nround:  17   accuracy:  0.7855 \nround:  18   accuracy:  0.7872 \nround:  19   accuracy:  0.7885 \nround:  20   accuracy:  0.7902 \nTraining accuracy is increasing continuously. These values represent averages of\nlocal accuracy measurements, so in the real world, they might well be overly\noptimistic (with each client overfitting on their respective data). So\nsupplementing federated training, a federated evaluation process would need to\nbe built in order to get a realistic view on performance. This is a topic to\ncome back to when more related TFF documentation is available.\nConclusion\nWe hope you’ve enjoyed this first introduction to TFF using R. Certainly at this\ntime, it is too early for use in production; and for application in research (e.g., adversarial attacks on federated learning)\nfamiliarity with “lowish”-level implementation code is required – regardless\nwhether you use R or Python.\nHowever, judging from activity on GitHub, TFF is under very active development right now (including new documentation being added!), so we’re looking forward\nto what’s to come. In the meantime, it’s never too early to start learning the\nconcepts…\nThanks for reading!\n\n\n\nBlot, Michael, David Picard, Matthieu Cord, and Nicolas Thome. 2016. “Gossip Training for Deep Learning.” CoRR abs/1611.09726. http://arxiv.org/abs/1611.09726.\n\n\nBonawitz, Keith, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. 2016. “Practical Secure Aggregation for Federated Learning on User-Held Data.” CoRR abs/1611.04482. http://arxiv.org/abs/1611.04482.\n\n\nClanuwat, Tarin, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. 2018. “Deep Learning for Classical Japanese Literature.” December 3, 2018. https://arxiv.org/abs/cs.CV/1812.01718.\n\n\nMcMahan, H. Brendan, Eider Moore, Daniel Ramage, and Blaise Agüera y Arcas. 2016. “Federated Learning of Deep Networks Using Model Averaging.” CoRR abs/1602.05629. http://arxiv.org/abs/1602.05629.\n\n\nMcMahan, H. Brendan, Daniel Ramage, Kunal Talwar, and Li Zhang. 2017. “Learning Differentially Private Language Models Without Losing Accuracy.” CoRR abs/1710.06963. http://arxiv.org/abs/1710.06963.\n\n\nZhu, Ligeng, Zhijian Liu, and Song Han. 2019. “Deep Leakage from Gradients.” CoRR abs/1906.08935. http://arxiv.org/abs/1906.08935.\n\n\nnot Pythonic :-)↩︎\n",
    "preview": "posts/2020-04-08-tf-federated-intro/images/federated_learning.png",
    "last_modified": "2024-11-21T15:49:27+00:00",
    "input_file": {},
    "preview_width": 1122,
    "preview_height": 570
  },
  {
    "path": "posts/2020-04-01-rstudio-ai-blog/",
    "title": "Introducing: The RStudio AI Blog",
    "description": "This blog just got a new title: RStudio AI Blog. We explain why.",
    "author": [
      {
        "name": "The Multiverse Team",
        "url": {}
      }
    ],
    "date": "2020-03-30",
    "categories": [
      "Meta"
    ],
    "contents": "\nWhy the new name, RStudio AI Blog? There is a straightforward reason. The\nprevious title, “TensorFlow for R Blog”, was a good match for the content\nwe covered so far: technical or applied aspects of performing deep\nlearning with TensorFlow and Keras. Yet, our team (the Multiverse Team) is not\nworking exclusively in those areas; instead, enabling distributed computing from\nR (sparklyr), integrating automated\nmachine learning workflows (mlflow), and\noptimizing data ingestion (pins) are\nsubstantial aspects of what we do. We would like to have a platform we can use\nto tell you about our work in these areas as well. Furthermore, regarding the\nhitherto dominant topic on this blog, deep learning, we might also want to\nreflect about it in a less technical way, focussing on impacts on\nsociety, ethics, or even “just” epistemic questions.\nConsequently, we needed a new name, but why “AI”? Maybe “data science” would work\nas well – however, the science in data science brings up connotations of formality and\ntheoretic ambitions which we would rather avoid. Instead, AI appeared to be a more rigorous\ndefinition, understood as outlined in an article by Michael\nJordan. Jordan envisions AI as a\nnew engineering discipline that builds on existing knowledge about inference,\noptimization, computation, and data processing the way that chemical engineering\nand civil engineering built upon chemistry and physics, respectively.\nSupplementing those building blocks (from mathematics, statistics, computer\nscience), the goal of this new discipline is to include guidance from the social sciences\nand the humanities.\nBy the way, as of this writing, the Multiverse Team consists of Daniel\nFalbel, Sigrid\nKeydana, Yitao\nLi, and Javier\nLuraschi.\nYou can find us on Twitter under the\n#mlverse\ntag, or pass by our new mlverse\nchannel on\nYouTube. Thank you for your support!\n\n\n\n",
    "preview": "posts/2020-04-01-rstudio-ai-blog/images/thumb.jpg",
    "last_modified": "2024-11-21T15:53:21+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-02-19-kl-divergence/",
    "title": "Infinite surprise - the iridescent personality of Kullback-Leibler divergence",
    "description": "Kullback-Leibler divergence is not just used to train variational autoencoders or Bayesian networks (and not just a hard-to-pronounce thing). It is a fundamental concept in information theory, put to use in a vast range of applications. Most interestingly, it's not always about constraint, regularization or compression. Quite on the contrary, sometimes it is about novelty, discovery and surprise.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-02-19",
    "categories": [
      "Probabilistic ML/DL",
      "Concepts"
    ],
    "contents": "\nAmong deep learning practitioners, Kullback-Leibler divergence (KL divergence) is perhaps best known for its role in training variational autoencoders (VAEs).1 To learn an informative latent space, we don’t just optimize for good reconstruction. Rather, we also impose a prior on the latent distribution, and aim to keep them close – often, by minimizing KL divergence.\nIn this role, KL divergence acts like a watchdog; it is a constraining, regularizing factor, and if anthropomorphized, would seem stern and severe. If we leave it at that, however, we’ve seen just one side of its character, and are missing out on its complement, a picture of playfulness, adventure, and curiosity. In this post, we’ll take a look at that other side.\nWhile being inspired by a series of tweets by Simon de Deo, enumerating applications of KL divergence in a vast number of disciplines,\n\nwe don’t aspire to provide a comprehensive write-up here – as mentioned in the initial tweet, the topic could easily fill a whole semester of study.\nThe much more modest goals of this post, then, are\nto quickly recap the role of KL divergence in training VAEs, and mention similar-in-character applications;\nto illustrate that more playful, adventurous “other side” of its character;2 and\nin a not-so-entertaining, but – hopefully – useful manner, differentiate KL divergence from related concepts such as cross entropy, mutual information, or free energy.\nBefore though, we start with a definition and some terminology.\nKL divergence in a nutshell\nKL divergence is the expected value of the logarithmic difference in probabilities according to two distributions, \\(p\\) and \\(q\\). Here it is in its discrete-probabilities variant:\n\\[\\begin{equation}\nD_{KL}(p||q) = \\sum\\limits_{x} p(x) log(\\frac{p(x)}{q(x)})\n \\tag{1}\n\\end{equation}\\]\nNotably, it is asymmetric; that is, \\(D_{KL}(p||q)\\) is not the same as \\(D_{KL}(q||p)\\). (Which is why it is a divergence, not a distance.) This aspect will play an important role in section 2 dedicated to the “other side.”\nTo stress this asymmetry, KL divergence is sometimes called relative information (as in “information of \\(p\\) relative to \\(q\\)”), or information gain. We agree with one of our sources3 that because of its universality and importance, KL divergence would probably have deserved a more informative name; such as, precisely, information gain. (Which is less ambiguous pronunciation-wise, as well.)\nKL divergence, “villain”\nIn many machine learning algorithms, KL divergence appears in the context of variational inference. Often, for realistic data, exact computation of the posterior distribution is infeasible. Thus, some form of approximation is required. In variational inference, the true posterior \\(p^*\\) is approximated by a simpler distribution, \\(q\\), from some tractable family.\nTo ensure we have a good approximation, we minimize – in theory, at least – the KL divergence of \\(q\\) relative to \\(p^*\\), thus replacing inference by optimization.\nIn practice, again for reasons of intractability, the KL divergence minimized is that of \\(q\\) relative to an unnormalized distribution \\(\\widetilde{p}\\)\n\\[\\begin{equation}\nJ(q)\\ = D_{KL}(q||\\widetilde{p})\n \\tag{2}\n\\end{equation}\\]\nwhere \\(\\widetilde{p}\\) is the joint distribution of parameters and data:\n\\[\\begin{equation}\n\\widetilde{p}(\\mathbf{x}) = p(\\mathbf{x}, \\mathcal{D}) = p^*(\\mathbf{x}) \\ p(\\mathcal{D})\n \\tag{3}\n\\end{equation}\\]\nand \\(p^*\\) is the true posterior:\n\\[\\begin{equation}\np^*(\\mathbf{x}) = p(\\mathbf{x}|\\mathcal{D})\n \\tag{4}\n\\end{equation}\\]\nEquivalent to that formulation (eq. (2)) – for a derivation see (Murphy 2012) – is this, which shows the optimization objective to be an upper bound on the negative log-likelihood (NLL):\n\\[\\begin{equation}\nJ(q)\\ =  D_{KL}(q||p^*) - log \\ p(D)\n \\tag{5}\n\\end{equation}\\]\nYet another formulation – again, see (Murphy 2012) for details – is the one we actually use when training (e.g.) VAEs. This one corresponds to the expected NLL plus the KL divergence between the approximation \\(q\\) and the imposed prior \\(p\\):\n\\[\\begin{equation}\nJ(q)\\ =  D_{KL}(q||p) - E_q[- log \\ p(\\mathcal{D}|\\mathbf{x})]\n \\tag{6}\n\\end{equation}\\]\nNegated, this formulation is also called the ELBO, for evidence lower bound. In the VAE post cited above, the ELBO was written\n\\[\\begin{equation}\nELBO\\ = \\ E[log\\ p(x|z)]\\ -\\ KL(q(z)||p(z))\n \\tag{7}\n\\end{equation}\\]\nwith \\(z\\) denoting the latent variables (\\(q(z)\\) being the approximation, \\(p(z)\\) the prior, often a multivariate normal).\nBeyond VAEs\nGeneralizing this “conservative” action pattern of KL divergence beyond VAEs, we can say that it expresses the quality of approximations. An important area where approximation takes place is (lossy) compression. KL divergence provides a way to quantify how much information is lost when we compress data.\nSumming up, in these and similar applications, KL divergence is “bad” – although we don’t want it to be zero (or else, why bother using the algorithm?), we certainly want to keep it low. So now, let’s see the other side.\nKL divergence, good guy\nIn a second category of applications, KL divergence is not something to be minimized.4 In these domains, KL divergence is indicative of surprise, disagreement, exploratory behavior, or learning: This truly is the perspective of information gain.\nSurprise\nOne domain where surprise, not information per se, governs behavior is perception. For example, eyetracking studies (e.g., (Itti and Baldi 2005)) showed that surprise, as measured by KL divergence, was a better predictor of visual attention than information, measured by entropy.5 While these studies seem to have popularized the expression “Bayesian surprise,” this compound is – I think – not the most informative one, as neither part adds much information to the other. In Bayesian updating, the magnitude of the difference between prior and posterior reflects the degree of surprise brought about by the data – surprise is an integral part of the concept.\nThus, with KL divergence linked to surprise, and surprise rooted in the fundamental process of Bayesian updating, a process that could be used to describe the course of life itself, KL divergence itself becomes fundamental. We could get tempted to see it everywhere. Accordingly, it has been used in many fields to quantify unidirectional divergence.\nFor example, (Zanardo 2017) have applied it in trading, measuring how much a person disagrees with the market belief. Higher disagreement then corresponds to higher expected gains from betting against the market.\nCloser to the area of deep learning, it is used in intrinsically motivated reinforcement learning (e.g., (Sun, Gomez, and Schmidhuber 2011)), where an optimal policy should maximize the long-term information gain. This is possible because like entropy, KL divergence is additive.\nAlthough its asymmetry is relevant whether you use KL divergence for regularization (section 1) or surprise (this section), it becomes especially evident when used for learning and surprise.\nAsymmetry in action\nLooking again at the KL formula\n\\[\\begin{equation}\nD_{KL}(p||q) = \\sum\\limits_{x} p(x) log(\\frac{p(x)}{q(x)})\n \\tag{1}\n\\end{equation}\\]\nthe roles of \\(p\\) and \\(q\\) are fundamentally different. For one, the expectation is computed over the first distribution (\\(p\\) in (1)). This aspect is important because the “order” (the respective roles) of \\(p\\) and \\(q\\) may have to be chosen according to tractability (which distribution are we able to average over).\nSecondly, the fraction inside the \\(log\\) means that if \\(q\\) is ever zero at a point where \\(p\\) isn’t, the KL divergence will “blow up.” What this means for distribution estimation in general is nicely detailed in Murphy (2012). In the context of surprise, it means that if I learn something I used to think had probability zero, I will be “infinitely surprised.”\nTo avoid infinite surprise, we can make sure our prior probability is never zero. But even then, the interesting thing is that how much information we gain in any one event depends on how much information I had before. Let’s see a simple example.\nAssume that in my current understanding of the world, black swans probably don’t exist, but they could … maybe 1 percent of them is black. Put differently, my prior belief of a swan, should I encounter one, being black is \\(q = 0.01\\).\nNow in fact I do encounter one, and it’s black.\nThe information I’ve gained is:\n\\[\\begin{equation}\nl(p,q) = 0 * log(\\frac{0}{0.99}) + 1 * log(\\frac{1}{0.01}) = 6.6 \\ bits\n \\tag{8}\n\\end{equation}\\]\nConversely, suppose I’d been much more undecided before; say I’d have thought the odds were 50:50.\nOn seeing a black swan, I get a lot less information:\n\\[\\begin{equation}\nl(p,q) = 0 * log(\\frac{0}{0.5}) + 1 * log(\\frac{1}{0.5}) = 1 \\ bit\n \\tag{9}\n\\end{equation}\\]\nThis view of KL divergence, in terms of surprise and learning, is inspiring – it could lead one to seeing it in action everywhere. However, we still have the third and final task to handle: quickly compare KL divergence to other concepts in the area.\nRelated concepts\nEntropy\nIt all starts with entropy, or uncertainty, or information, as formulated by Claude Shannon.\nEntropy is the average log probability of a distribution:\n\\[\\begin{equation}\nH(X) = - \\sum\\limits_{x=1}^n p(x_i) log(p(x_i))\n \\tag{10}\n\\end{equation}\\]\nAs nicely described in (DeDeo 2016), this formulation was chosen to satisfy four criteria, one of which is what we commonly picture as its “essence,” and one of which is especially interesting.\nAs to the former, if there are \\(n\\) possible states, entropy is maximal when all states are equiprobable. E.g., for a coin flip uncertainty is highest when coin bias is 0.5.\nThe latter has to do with coarse-graining, a change in “resolution” of the state space. Say we have 16 possible states, but we don’t really care at that level of detail. We do care about 3 individual states, but all the rest are basically the same to us. Then entropy decomposes additively; total (fine-grained) entropy is the entropy of the coarse-grained space, plus the entropy of the “lumped-together” group, weighted by their probabilities.6\nSubjectively, entropy reflects our uncertainty whether an event will happen. Interestingly though, it exists in the physical world as well: For example, when ice melts, it becomes more uncertain where individual particles are. As reported by (DeDeo 2016), the number of bits released when one gram of ice melts is about 100 billion terabytes!\nAs fascinating as it is, information per se may, in many cases, not be the best means of characterizing human behavior. Going back to the eyetracking example, it is completely intuitive that people look at surprising parts of images, not at white noise areas, which are the maximum you could get in terms of entropy.\nAs a deep learning practitioner, you’ve probably been waiting for the point at which we’d mention cross entropy – the most commonly used loss function in categorization.\nCross entropy\nThe cross entropy between distributions \\(p\\) and \\(q\\) is the entropy of \\(p\\) plus the KL divergence of \\(p\\) relative to \\(q\\). If you’ve ever implemented your own classification network, you probably recognize the sum on the very right:\n\\[\\begin{equation}\nH(p,q) = H(p) + D_{KL}(p||q) = - \\sum p \\ log(q)\n \\tag{11}\n\\end{equation}\\]\nIn information theory-speak, \\(H(p,q)\\) is the expected message length per datum when \\(q\\) is assumed but \\(p\\) is true.\nCloser to the world of machine learning, for fixed \\(p\\), minimizing cross entropy is equivalent to minimizing KL divergence.\nMutual information\nAnother extremely important quantity, used in many contexts and applications, is mutual information. Again citing DeDeo, “you can think of it as the most general form of correlation coefficient that you can measure.”\nWith two variables \\(X\\) and \\(Y\\), we can ask: How much do we learn about \\(X\\) when we learn about an individual \\(y\\), \\(Y=y\\)? Averaged over all \\(y\\), this is the conditional entropy:\n\\[\\begin{equation}\nH(X|Y) = - \\sum\\limits_{i} P(y_i) log(H(X|y_i))\n \\tag{12}\n\\end{equation}\\]\nNow mutual information is entropy minus conditional entropy:\n\\[\\begin{equation}\nI(X, Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)\n \\tag{13}\n\\end{equation}\\]\nThis quantity – as required for a measure representing something like correlation – is symmetric: If two variables \\(X\\) and \\(Y\\) are related, the amount of information \\(X\\) gives you about \\(Y\\) is equal to that \\(Y\\) gives you about \\(X\\).\nKL divergence is part of a family of divergences, called f-divergences, used to measure directed difference between probability distributions. Let’s also quickly look another information-theoretic measure that unlike those, is a distance.\nJensen-Shannon distance\nIn math, a distance, or metric, besides being non-negative has to satisfy two other criteria: It must be symmetric, and it must obey the triangle inequality.\nBoth criteria are met by the Jensen-Shannon distance. With \\(m\\) a mixture distribution:\n\\[\\begin{equation}\nm_i = \\frac{1}{2}(p_i + q_i)\n \\tag{14}\n\\end{equation}\\]\nthe Jensen-Shannon distance is an average of KL divergences, one of \\(m\\) relative to \\(p\\), the other of \\(m\\) relative to \\(q\\):\n\\[\\begin{equation}\nJSD = \\frac{1}{2}(KL(m||p) + KL(m||q))\n \\tag{15}\n\\end{equation}\\]\nThis would be an ideal candidate to use were we interested in (undirected) distance between, not directed surprise caused by, distributions.\nFinally, let’s wrap up with a last term, restricting ourselves to a quick glimpse at something whole books could be written about.\n(Variational) Free Energy\nReading papers on variational inference, you’re pretty likely to hear people talking not “just” about KL divergence and/or the ELBO (which as soon as you know what it stands for, is just what it is), but also, something mysteriously called free energy (or: variational free energy, in that context).\nFor practical purposes, it suffices to know that variational free energy is negative the ELBO, that is, corresponds to equation (2). But for those interested, there is free energy as a central concept in thermodynamics.\nIn this post, we’re mainly interested in how concepts are related to KL divergence, and for this, we follow the characterization John Baez gives in his aforementioned talk.\nFree energy, that is, energy in useful form, is the expected energy minus temperature times entropy:\n\\[\\begin{equation}\nF = [E] -T \\ H\n \\tag{16}\n\\end{equation}\\]\nThen, the extra free energy of a system \\(Q\\) – compared to a system in equilibrium \\(P\\) – is proportional to their KL divergence, that is, the information of \\(Q\\) relative to \\(P\\):7\n\\[\\begin{equation}\nF(Q) - F(P) = k \\ T \\ KL(q||p)\n \\tag{17}\n\\end{equation}\\]\nSpeaking of free energy, there’s also the – not uncontroversial – free energy principle posited in neuroscience..8 But at some point, we have to stop, and we do it here.\nConclusion\nWrapping up, this post has tried to do three things: Having in mind a reader with background mainly in deep learning, start with the “habitual” use in training variational autoencoders; then show the – probably less familiar – “other side”; and finally, provide a synopsis of related terms and their applications.\nIf you’re interested in digging deeper into the many various applications, in a range of different fields, no better place to start than from the Twitter thread, mentioned above, that gave rise to this post. Thanks for reading!\n\n\n\nDeDeo, Simon. 2016. “Information Theory for Intelligent People.”\n\n\nFriston, Karl. 2010. “Friston, k.j.: The Free-Energy Principle: A Unified Brain Theory? Nat. Rev. Neurosci. 11, 127-138.” Nature Reviews. Neuroscience 11 (February): 127–38. https://doi.org/10.1038/nrn2787.\n\n\nItti, Laurent, and Pierre Baldi. 2005. “Bayesian Surprise Attracts Human Attention.” In Advances in Neural Information Processing Systems 18 [Neural Information Processing Systems, NIPS 2005, December 5-8, 2005, Vancouver, British Columbia, Canada], 547–54. http://papers.nips.cc/paper/2822-bayesian-surprise-attracts-human-attention.\n\n\nMurphy, Kevin. 2012. Machine Learning: A Probabilistic Perspective. MIT Press.\n\n\nSun, Yi, Faustino J. Gomez, and Juergen Schmidhuber. 2011. “Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments.” CoRR abs/1103.5708. http://arxiv.org/abs/1103.5708.\n\n\nZanardo, Enrico. 2017. “HOW TO MEASURE DISAGREEMENT ?” In.\n\n\nSee Representation learning with MMD-VAE for an introduction.↩︎\nAs you probably guessed, these epitheta are not to be taken entirely seriously…↩︎\nJohn Baez, whom we cite below when discussing free energy.↩︎\nNot, by contrast, something to be maximized either. Rather, depending on the domain, there will probably be an “optimal” amount of KL divergence for the related behavior to ensue.↩︎\nWe discuss entropy in section 3.↩︎\nSee DeDeo (2016) for details.↩︎\nHere k is the Boltzmann constant.↩︎\nSee, e.g., (Friston 2010)↩︎\n",
    "preview": "posts/2020-02-19-kl-divergence/images/ultimatemachine.jpg",
    "last_modified": "2024-11-21T15:52:36+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-01-24-numpy-broadcasting/",
    "title": "NumPy-style broadcasting for R TensorFlow users",
    "description": "Broadcasting, as done by Python's scientific computing library NumPy, involves dynamically extending shapes so that arrays of different sizes may be passed to operations that expect conformity - such as adding or multiplying elementwise. In NumPy, the way broadcasting works is specified exactly; the same rules apply to TensorFlow operations. For anyone who finds herself, occasionally, consulting Python code, this post strives to explain.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-01-24",
    "categories": [
      "TensorFlow/Keras",
      "Concepts"
    ],
    "contents": "\nWe develop, train, and deploy TensorFlow models from R. But that doesn’t mean we don’t make use of documentation, blog posts, and examples written in Python. We look up specific functionality in the official TensorFlow API docs; we get inspiration from other people’s code.\nDepending on how comfortable you are with Python, there’s a problem. For example: You’re supposed to know how broadcasting works. And perhaps, you’d say you’re vaguely familiar with it: So when arrays have different shapes, some elements get duplicated until their shapes match and … and isn’t R vectorized anyway?\nWhile such a global notion may work in general, like when skimming a blog post, it’s not enough to understand, say, examples in the TensorFlow API docs. In this post, we’ll try to arrive at a more exact understanding, and check it on concrete examples.\nSpeaking of examples, here are two motivating ones.\nBroadcasting in action\nThe first uses TensorFlow’s matmul to multiply two tensors. Would you like to guess the result – not the numbers, but how it comes about in general? Does this even run without error – shouldn’t matrices be two-dimensional (rank-2 tensors, in TensorFlow speak)?\n\n\na <- tf$constant(keras::array_reshape(1:12, dim = c(2, 2, 3)))\na \n# tf.Tensor(\n# [[[ 1.  2.  3.]\n#   [ 4.  5.  6.]]\n# \n#  [[ 7.  8.  9.]\n#   [10. 11. 12.]]], shape=(2, 2, 3), dtype=float64)\n\nb <- tf$constant(keras::array_reshape(101:106, dim = c(1, 3, 2)))\nb  \n# tf.Tensor(\n# [[[101. 102.]\n#   [103. 104.]\n#   [105. 106.]]], shape=(1, 3, 2), dtype=float64)\n\nc <- tf$matmul(a, b)\n\n\nSecond, here is a “real example” from a TensorFlow Probability (TFP) github issue. (Translated to R, but keeping the semantics).\nIn TFP, we can have batches of distributions. That, per se, is not surprising. But look at this:\n\n\nlibrary(tfprobability)\nd <- tfd_normal(loc = c(0, 1), scale = matrix(1.5:4.5, ncol = 2, byrow = TRUE))\nd\n# tfp.distributions.Normal(\"Normal\", batch_shape=[2, 2], event_shape=[], dtype=float64)\n\n\nWe create a batch of four normal distributions: each with a different scale (1.5, 2.5, 3.5, 4.5). But wait: there are only two location parameters given. So what are their scales, respectively?\nThankfully, TFP developers Brian Patton and Chris Suter explained how it works: TFP actually does broadcasting – with distributions – just like with tensors!\nWe get back to both examples at the end of this post. Our main focus will be to explain broadcasting as done in NumPy, as NumPy-style broadcasting is what numerous other frameworks have adopted (e.g., TensorFlow).\nBefore though, let’s quickly review a few basics about NumPy arrays: How to index or slice them (indexing normally referring to single-element extraction, while slicing would yield – well – slices containing several elements); how to parse their shapes; some terminology and related background.\nThough not complicated per se, these are the kinds of things that can be confusing to infrequent Python users; yet they’re often a prerequisite to successfully making use of Python documentation.\nStated upfront, we’ll really restrict ourselves to the basics here; for example, we won’t touch advanced indexing which – just like lots more –, can be looked up in detail in the NumPy documentation.\nFew facts about NumPy\nBasic slicing\nFor simplicity, we’ll use the terms indexing and slicing more or less synonymously from now on. The basic device here is a slice, namely, a start:stop 1 structure indicating, for a single dimension, which range of elements to include in the selection.\nIn contrast to R, Python indexing is zero-based, and the end index is exclusive:\n\nimport numpy as np\nx = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\nx[1:7] \n# array([1, 2, 3, 4, 5, 6])\n\nMinus, to R users, is a false friend; it means we start counting from the end (the last element being -1):\n\nx[-2:10] \n# array([8, 9])\n\nLeaving out start (stop, resp.) selects all elements from the start (till the end).\nThis may feel so convenient that Python users might miss it in R:\n\nx[5:] \n# array([5, 6, 7, 8, 9])\n\nx[:7]\n# array([0, 1, 2, 3, 4, 5, 6])\n\nJust to make a point about the syntax, we could leave out both the start and the stop indices, in this one-dimensional case effectively resulting in a no-op:\n\nx[:] \narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\nGoing on to two dimensions – without commenting on array creation just yet –, we can immediately apply the “semicolon trick” here too. This will select the second row with all its columns:\n\nx = np.array([[1, 2], [3, 4], [5, 6]])\nx\n# array([[1, 2],\n#        [3, 4],\n#        [5, 6]])\n\nx[1, :] \n# array([3, 4])\n\nWhile this, arguably, makes for the easiest way to achieve that result and thus, would be the way you’d write it yourself, it’s good to know that these are two alternative ways that do the same:\n\nx[1] \n# array([3, 4])\n\nx[1, ] \n# array([3, 4])\n\nWhile the second one sure looks a bit like R, the mechanism is different. Technically, these start:stop things are parts of a Python tuple – that list-like, but immutable data structure that can be written with or without parentheses, e.g., 1,2 or (1,2) –, and whenever we have more dimensions in the array than elements in the tuple NumPy will assume we meant : for that dimension: Just select everything.\nWe can see that moving on to three dimensions. Here is a 2 x 3 x 1-dimensional array:\n\nx = np.array([[[1],[2],[3]], [[4],[5],[6]]])\nx\n# array([[[1],\n#         [2],\n#         [3]],\n# \n#        [[4],\n#         [5],\n#         [6]]])\n\nx.shape\n# (2, 3, 1)\n\nIn R, this would throw an error, while in Python it works:\n\nx[0,]\n#array([[1],\n#       [2],\n#       [3]])\n\nIn such a case, for enhanced readability we could instead use the so-called Ellipsis, explicitly asking Python to “use up all dimensions required to make this work”:\n\nx[0, ...]\n#array([[1],\n#       [2],\n#       [3]])\n\nWe stop here with our selection of essential (yet confusing, possibly, to infrequent Python users) Numpy indexing features; re. “possibly confusing” though, here are a few remarks about array creation.\nSyntax for array creation\nCreating a more-dimensional NumPy array is not that hard – depending on how you do it. The trick is to use reshape to tell NumPy exactly what shape you want. For example, to create an array of all zeros, of dimensions 3 x 4 x 2:\n\nnp.zeros(24).reshape(4, 3, 2)\n\nBut we also want to understand what others might write. And then, you might see things like these:\n\nc1 = np.array([[[0, 0, 0]]])\nc2 = np.array([[[0], [0], [0]]]) \nc3 = np.array([[[0]], [[0]], [[0]]])\n\nThese are all 3-dimensional, and all have three elements, so their shapes must be 1 x 1 x 3, 1 x 3 x 1, and 3 x 1 x 1, in some order. Of course, shape is there to tell us:\n\nc1.shape # (1, 1, 3)\nc2.shape # (1, 3, 1)\nc3.shape # (3, 1, 1) \n\nbut we’d like to be able to “parse” internally without executing the code. One way to think about it would be processing the brackets like a state machine, every opening bracket moving one axis to the right and every closing bracket moving back left by one axis. Let us know if you can think of other – possibly more helpful – mnemonics!\nIn the very last sentence, we on purpose used “left” and “right” referring to the array axes; “out there” though, you’ll also hear “outmost” and “innermost”. Which, then, is which?\nA bit of terminology\nIn common Python (TensorFlow, for example) usage, when talking of an array shape like (2, 6, 7), outmost is left and innermost is right. Why?\nLet’s take a simpler, two-dimensional example of shape (2, 3).\n\na = np.array([[1, 2, 3], [4, 5, 6]])\na\n# array([[1, 2, 3],\n#        [4, 5, 6]])\n\nComputer memory is conceptually one-dimensional, a sequence of locations; so when we create arrays in a high-level programming language, their contents are effectively “flattened” into a vector. That flattening could occur “by row” (row-major, C-style, the default in NumPy), resulting in the above array ending up like this\n1 2 3 4 5 6\nor “by column” (column-major, Fortran-style, the ordering used in R), yielding\n1 4 2 5 3 6\n\nfor the above example.\nNow if we see “outmost” as the axis whose index varies the least often, and “innermost” as the one that changes most quickly, in row-major ordering the left axis is “outer”, and the right one is “inner”.\nJust as a (cool!) aside, NumPy arrays have an attribute called strides that stores how many bytes have to be traversed, for each axis, to arrive at its next element. For our above example:\n\nc1 = np.array([[[0, 0, 0]]])\nc1.shape   # (1, 1, 3)\nc1.strides # (24, 24, 8)\n\nc2 = np.array([[[0], [0], [0]]]) \nc2.shape   # (1, 3, 1)\nc2.strides # (24, 8, 8)\n\nc3 = np.array([[[0]], [[0]], [[0]]])\nc3.shape   # (3, 1, 1) \nc3.strides # (8, 8, 8)\n\nFor array c3, every element is on its own on the outmost level; so for axis 0, to jump from one element to the next, it’s just 8 bytes. For c2 and c1 though, everything is “squished” in the first element of axis 0 (there is just a single element there). So if we wanted to jump to another, nonexisting-as-yet, outmost item, it’d take us 3 * 8 = 24 bytes.\nAt this point, we’re ready to talk about broadcasting. We first stay with NumPy and then, examine some TensorFlow examples.\nNumPy Broadcasting\nWhat happens if we add a scalar to an array? This won’t be surprising for R users:\n\na = np.array([1,2,3])\nb = 1\na + b\n\narray([2, 3, 4])\nTechnically, this is already broadcasting in action; b is virtually (not physically!) expanded to shape (3,) in order to match the shape of a.\nHow about two arrays, one of shape (2, 3) – two rows, three columns –, the other one-dimensional, of shape (3,)?\n\na = np.array([1,2,3])\nb = np.array([[1,2,3], [4,5,6]])\na + b\n\narray([[2, 4, 6],\n       [5, 7, 9]])\nThe one-dimensional array gets added to both rows. If a were length-two instead, would it get added to every column?\n\na = np.array([1,2,3])\nb = np.array([[1,2,3], [4,5,6]])\na + b\n\nValueError: operands could not be broadcast together with shapes (2,) (2,3) \nSo now it is time for the broadcasting rule. For broadcasting (virtual expansion) to happen, the following is required.\nWe align array shapes, starting from the right.\n   # array 1, shape:     8  1  6  1\n   # array 2, shape:        7  1  5\nStarting to look from the right, the sizes along aligned axes either have to match exactly, or one of them has to be 1: In which case the latter is broadcast to the one not equal to 1.\nIf on the left, one of the arrays has an additional axis (or more than one), the other is virtually expanded to have a 1 in that place, in which case broadcasting will happen as stated in (2).\nStated like this, it probably sounds incredibly simple. Maybe it is, and it only seems complicated because it presupposes correct parsing of array shapes (which as shown above, can be confusing)?\nHere again is a quick example to test our understanding:\n\na = np.zeros([2, 3]) # shape (2, 3)\nb = np.zeros([2])    # shape (2,)\nc = np.zeros([3])    # shape (3,)\n\na + b # error\n\na + c\n# array([[0., 0., 0.],\n#        [0., 0., 0.]])\n\nAll in accord with the rules. Maybe there’s something else that makes it confusing?\nFrom linear algebra, we are used to thinking in terms of column vectors (often seen as the default) and row vectors (accordingly, seen as their transposes). What now is\n\nnp.array([0, 0])\n\n, of shape – as we’ve seen a few times by now – (2,)? Really it’s neither, it’s just some one-dimensional array structure. We can create row vectors and column vectors though, in the sense of 1 x n and n x 1 matrices, by explicitly adding a second axis. Any of these would create a column vector:\n\n# start with the above \"non-vector\"\nc = np.array([0, 0])\nc.shape\n# (2,)\n\n# way 1: reshape\nc.reshape(2, 1).shape\n# (2, 1)\n\n# np.newaxis inserts new axis\nc[ :, np.newaxis].shape\n# (2, 1)\n\n# None does the same\nc[ :, None].shape\n# (2, 1)\n\n# or construct directly as (2, 1), paying attention to the parentheses...\nc = np.array([[0], [0]])\nc.shape\n# (2, 1)\n\nAnd analogously for row vectors. Now these “more explicit”, to a human reader, shapes should make it easier to assess where broadcasting will work, and where it won’t.\n\nc = np.array([[0], [0]])\nc.shape\n# (2, 1)\n\na = np.zeros([2, 3])\na.shape\n# (2, 3)\na + c\n# array([[0., 0., 0.],\n#       [0., 0., 0.]])\n\na = np.zeros([3, 2])\na.shape\n# (3, 2)\na + c\n# ValueError: operands could not be broadcast together with shapes (3,2) (2,1) \n\nBefore we jump to TensorFlow, let’s see a simple practical application: computing an outer product.\n\na = np.array([0.0, 10.0, 20.0, 30.0])\na.shape\n# (4,)\n\nb = np.array([1.0, 2.0, 3.0])\nb.shape\n# (3,)\n\na[:, np.newaxis] * b\n# array([[ 0.,  0.,  0.],\n#        [10., 20., 30.],\n#        [20., 40., 60.],\n#        [30., 60., 90.]])\n\nTensorFlow\nIf by now, you’re feeling less than enthusiastic about hearing a detailed exposition of how TensorFlow broadcasting differs from NumPy’s, there is good news: Basically, the rules are the same. However, when matrix operations work on batches – as in the case of matmul and friends – , things may still get complicated; the best advice here probably is to carefully read the documentation (and as always, try things out).\nBefore revisiting our introductory matmul example, we quickly check that really, things work just like in NumPy. Thanks to the tensorflow R package, there is no reason to do this in Python; so at this point, we switch to R – attention, it’s 1-based indexing from here.\nFirst check – (4, 1) added to (4,) should yield (4, 4):\n\n\na <- tf$ones(shape = c(4L, 1L))\na\n# tf.Tensor(\n# [[1.]\n#  [1.]\n#  [1.]\n#  [1.]], shape=(4, 1), dtype=float32)\n\nb <- tf$constant(c(1, 2, 3, 4))\nb\n# tf.Tensor([1. 2. 3. 4.], shape=(4,), dtype=float32)\n\na + b\n# tf.Tensor(\n# [[2. 3. 4. 5.]\n# [2. 3. 4. 5.]\n# [2. 3. 4. 5.]\n# [2. 3. 4. 5.]], shape=(4, 4), dtype=float32)\n\n\nAnd second, when we add tensors with shapes (3, 3) and (3,), the 1-d tensor should get added to every row (not every column):\n\n\na <- tf$constant(matrix(1:9, ncol = 3, byrow = TRUE), dtype = tf$float32)\na\n# tf.Tensor(\n# [[1. 2. 3.]\n#  [4. 5. 6.]\n#  [7. 8. 9.]], shape=(3, 3), dtype=float32)\n\nb <- tf$constant(c(100, 200, 300))\nb\n# tf.Tensor([100. 200. 300.], shape=(3,), dtype=float32)\n\na + b\n# tf.Tensor(\n# [[101. 202. 303.]\n#  [104. 205. 306.]\n#  [107. 208. 309.]], shape=(3, 3), dtype=float32)\n\n\nNow back to the initial matmul example.\nBack to the puzzles\nThe documentation for matmul says,\n\nThe inputs must, following any transpositions, be tensors of rank >= 2 where the inner 2 dimensions specify valid matrix multiplication dimensions, and any further outer dimensions specify matching batch size.\n\nSo here (see code just below), the inner two dimensions look good – (2, 3) and (3, 2) – while the one (one and only, in this case) batch dimension shows mismatching values 2 and 1, respectively.\nA case for broadcasting thus: Both “batches” of a get matrix-multiplied with b.\n\n\na <- tf$constant(keras::array_reshape(1:12, dim = c(2, 2, 3)))\na \n# tf.Tensor(\n# [[[ 1.  2.  3.]\n#   [ 4.  5.  6.]]\n# \n#  [[ 7.  8.  9.]\n#   [10. 11. 12.]]], shape=(2, 2, 3), dtype=float64)\n\nb <- tf$constant(keras::array_reshape(101:106, dim = c(1, 3, 2)))\nb  \n# tf.Tensor(\n# [[[101. 102.]\n#   [103. 104.]\n#   [105. 106.]]], shape=(1, 3, 2), dtype=float64)\n\nc <- tf$matmul(a, b)\nc\n# tf.Tensor(\n# [[[ 622.  628.]\n#   [1549. 1564.]]\n# \n#  [[2476. 2500.]\n#   [3403. 3436.]]], shape=(2, 2, 2), dtype=float64) \n\n\nLet’s quickly check this really is what happens, by multiplying both batches separately:\n\n\ntf$matmul(a[1, , ], b)\n# tf.Tensor(\n# [[[ 622.  628.]\n#   [1549. 1564.]]], shape=(1, 2, 2), dtype=float64)\n\ntf$matmul(a[2, , ], b)\n# tf.Tensor(\n# [[[2476. 2500.]\n#   [3403. 3436.]]], shape=(1, 2, 2), dtype=float64)\n\n\nIs it too weird to be wondering if broadcasting would also happen for matrix dimensions? E.g., could we try matmuling tensors of shapes (2, 4, 1) and (2, 3, 1), where the 4 x 1 matrix would be broadcast to 4 x 3? – A quick test shows that no.\nTo see how really, when dealing with TensorFlow operations, it pays off overcoming one’s initial reluctance and actually consult the documentation, let’s try another one.\nIn the documentation for matvec, we are told:\n\nMultiplies matrix a by vector b, producing a * b.\nThe matrix a must, following any transpositions, be a tensor of rank >= 2, with shape(a)[-1] == shape(b)[-1], and shape(a)[:-2] able to broadcast with shape(b)[:-1].\n\nIn our understanding, given input tensors of shapes (2, 2, 3) and (2, 3), matvec should perform two matrix-vector multiplications: once for each batch, as indexed by each input’s leftmost dimension. Let’s check this – so far, there is no broadcasting involved:\n\n\n# two matrices\na <- tf$constant(keras::array_reshape(1:12, dim = c(2, 2, 3)))\na\n# tf.Tensor(\n# [[[ 1.  2.  3.]\n#   [ 4.  5.  6.]]\n# \n#  [[ 7.  8.  9.]\n#   [10. 11. 12.]]], shape=(2, 2, 3), dtype=float64)\n\nb = tf$constant(keras::array_reshape(101:106, dim = c(2, 3)))\nb\n# tf.Tensor(\n# [[101. 102. 103.]\n#  [104. 105. 106.]], shape=(2, 3), dtype=float64)\n\nc <- tf$linalg$matvec(a, b)\nc\n# tf.Tensor(\n# [[ 614. 1532.]\n#  [2522. 3467.]], shape=(2, 2), dtype=float64)\n\n\nDoublechecking, we manually multiply the corresponding matrices and vectors, and get:\n\n\ntf$linalg$matvec(a[1,  , ], b[1, ])\n# tf.Tensor([ 614. 1532.], shape=(2,), dtype=float64)\n\ntf$linalg$matvec(a[2,  , ], b[2, ])\n# tf.Tensor([2522. 3467.], shape=(2,), dtype=float64)\n\n\nThe same. Now, will we see broadcasting if b has just a single batch?\n\n\nb = tf$constant(keras::array_reshape(101:103, dim = c(1, 3)))\nb\n# tf.Tensor([[101. 102. 103.]], shape=(1, 3), dtype=float64)\n\nc <- tf$linalg$matvec(a, b)\nc\n# tf.Tensor(\n# [[ 614. 1532.]\n#  [2450. 3368.]], shape=(2, 2), dtype=float64)\n\n\nMultiplying every batch of a with b, for comparison:\n\n\ntf$linalg$matvec(a[1,  , ], b)\n# tf.Tensor([ 614. 1532.], shape=(2,), dtype=float64)\n\ntf$linalg$matvec(a[2,  , ], b)\n# tf.Tensor([[2450. 3368.]], shape=(1, 2), dtype=float64)\n\n\nIt worked!\nNow, on to the other motivating example, using tfprobability.\nBroadcasting everywhere\nHere again is the setup:\n\n\nlibrary(tfprobability)\nd <- tfd_normal(loc = c(0, 1), scale = matrix(1.5:4.5, ncol = 2, byrow = TRUE))\nd\n# tfp.distributions.Normal(\"Normal\", batch_shape=[2, 2], event_shape=[], dtype=float64)\n\n\nWhat is going on? Let’s inspect location and scale separately:\n\n\nd$loc\n# tf.Tensor([0. 1.], shape=(2,), dtype=float64)\n\nd$scale\n# tf.Tensor(\n# [[1.5 2.5]\n#  [3.5 4.5]], shape=(2, 2), dtype=float64)\n\n\nJust focusing on these tensors and their shapes, and having been told that there’s broadcasting going on, we can reason like this: Aligning both shapes on the right and extending loc’s shape by 1 (on the left), we have (1, 2) which may be broadcast with (2,2) - in matrix-speak, loc is treated as a row and duplicated.\nMeaning: We have two distributions with mean \\(0\\) (one of scale \\(1.5\\), the other of scale \\(3.5\\)), and also two with mean \\(1\\) (corresponding scales being \\(2.5\\) and \\(4.5\\)).\nHere’s a more direct way to see this:\n\n\nd$mean()\n# tf.Tensor(\n# [[0. 1.]\n#  [0. 1.]], shape=(2, 2), dtype=float64)\n\nd$stddev()\n# tf.Tensor(\n# [[1.5 2.5]\n#  [3.5 4.5]], shape=(2, 2), dtype=float64)\n\n\nPuzzle solved!\nSumming up, broadcasting is simple “in theory” (its rules are), but may need some practicing to get it right. Especially in conjunction with the fact that functions / operators do have their own views on which parts of its inputs should broadcast, and which shouldn’t. Really, there is no way around looking up the actual behaviors in the documentation.\nHopefully though, you’ve found this post to be a good start into the topic. Maybe, like the author, you feel like you might see broadcasting going on anywhere in the world now. Thanks for reading!\n\nor start:stop:step, if applicable↩︎\n",
    "preview": "posts/2020-01-24-numpy-broadcasting/images/thumb.jpg",
    "last_modified": "2024-11-21T15:49:42+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-01-13-mixed-precision-training/",
    "title": "First experiments with TensorFlow mixed-precision training",
    "description": "TensorFlow 2.1, released last week, allows for mixed-precision training, making use of the Tensor Cores available in the most recent NVidia GPUs. In this post, we report first experimental results and provide some background on what this is all about.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-01-13",
    "categories": [
      "TensorFlow/Keras"
    ],
    "contents": "\nStarting from its - very - recent 2.1 release, TensorFlow supports what is called mixed-precision training (in the following: MPT) for Keras. In this post, we experiment with MPT and provide some background. Stated upfront: On a Tesla V100 GPU, our CNN-based experiment did not reveal substantial reductions in execution time. In a case like this, it is hard to decide whether to actually write a post or not. You could argue that just like in science, null results are results. Or, more practically: They open up a discussion that may lead to bug discovery, clarification of usage instructions, and further experimentation, among others.\nIn addition, the topic itself is interesting enough to deserve some background explanations – even if the results are not quite there yet.\nSo to start, let’s hear some context on MPT.\nThis is not just about saving memory\nOne way to describe MPT in TensorFlow could go like this: MPT lets you train models where the weights are of type float32 or float64, as usual (for reasons of numeric stability), but the data – the tensors pushed between operations – have lower precision, namely, 16bit (float16).\nThis sentence would probably do fine as a TLDR; 1\nfor the new-ish MPT documentation page, also available for R on the TensorFlow for R website. And based on this sentence, you might be lead to think “oh sure, so this is about saving memory”. Less memory usage would then imply you could run larger batch sizes without getting out-of-memory errors.\nThis is of course correct, and you’ll see it happening in the experimentation results.\nBut it’s only part of the story. The other part is related to GPU architecture and parallel (not just parallel on-GPU, as we’ll see) computing.\nAVX & co.\nGPUs are all about parallelization. But for CPUs as well, the last ten years have seen important developments in architecture and instruction sets. SIMD (Single Instruction Multiple Data) operations perform one instruction over a bunch of data at once. For example, two 128-bit operands could hold two 64-bit integers each, and these could be added pairwise. Conceptually, this reminds of vector addition in R (it’s just an analogue though!):\n\n\n# picture these as 64-bit integers\nc(1, 2) + c(3, 4)\n\n\nOr, those operands could contain four 32-bit integers each, in which case we could symbolically write\n\n\n# picture these as 32-bit integers\nc(1, 2, 3, 4) + c(5, 6, 7, 8)\n\n\nWith 16-bit integers, we could again double the number of elements operated upon:\n\n\n# picture these as 16-bit integers\nc(1, 2, 3, 4, 5, 6, 7, 8) + c(9, 10, 11, 12, 13, 14, 15, 16)\n\n\nOver the last decade, the major SIMD-related X-86 assembly language extensions have been AVX (Advanced Vector Extensions), AVX2, AVX-512, and FMA (more on FMA soon).\nDo any of these ring a bell?\nYour CPU supports instructions that this TensorFlow binary was not compiled to use:\nAVX2 FMA\nThis is a line you are likely to see if you are using a pre-built TensorFlow binary, as opposed to compiling from source. (Later, when reporting experimentation results, we will also indicate on-CPU execution times, to provide some context for the GPU execution times we’re interested in – and just for fun, we’ll also do a – very superficial – comparison between a TensorFlow binary installed from PyPi and one that was compiled manually.)\nWhile all those AVXes are (basically) about an extension of vector processing to larger and larger data types, FMA is different, and it’s an interesting thing to know about in itself – for anyone doing signal processing or using neural networks.\nFused Multiply-Add (FMA)\nFused Multiply-Add is a type of multiply-accumulate operation. In multiply-accumulate, operands are multiplied and then added to accumulator keeping track of the running sum. If “fused”, the whole multiply-then-add operation is performed with a single rounding at the end (as opposed to rounding once after the multiplication, and then again after the addition). Usually, this results in higher accuracy.\nFor CPUs, FMA was introduced concurrently with AVX2. FMA can be performed on scalars or on vectors, “packed” in the way described in the previous paragraph.\nWhy did we say this was so interesting to data scientists? Well, a lot of operations – dot products, matrix multiplications, convolutions – involve multiplications followed by additions. “Matrix multiplication” here actually has us leave the realm of CPUs and jump to GPUs instead, because what MPT does is make use of the new-ish NVidia Tensor Cores that extend FMA from scalars/vectors to matrices.\nTensor Cores\nAs documented, MPT requires GPUs with compute capability >= 7.0. The respective GPUs, in addition to the usual Cuda Cores, have so called “Tensor Cores” that perform FMA on matrices:\n\n\n\nFigure 1: Source: https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf.\n\n\n\nThe operation takes place on 4x4 matrices; multiplications happen on 16-bit operands while the final result could be 16-bit or 32-bit.\n\n\n\nFigure 2: Source: https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf\n\n\n\nWe can see how this is immediately relevant to the operations involved in deep learning; the details, however, are not necessarily clear.\nLeaving those internals to the experts, we now proceed to the actual experiment.\nExperiments\nDataset\nWith their 28x28px / 32x32px sized images, neither MNIST nor CIFAR seemed particularly suited to challenge the GPU. Instead, we chose Imagenette, the “little ImageNet” created by the fast.ai folks, consisting of 10 classes: tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, and parachute. 2 Here are a few examples, taken from the 320px version:\n\n\n\nFigure 3: Examples of the 10 classes of Imagenette.\n\n\n\nThese images have been resized - keeping the aspect ratio - such that the larger dimension has length 320px. As part of preprocessing, we’ll further resize to 256x256px, to work with a nice power of 2. 3\nThe dataset may conveniently be obtained via using tfds, the R interface to TensorFlow Datasets.\n\n\nlibrary(keras)\n# needs version 2.1\nlibrary(tensorflow)\nlibrary(tfdatasets)\n# available from github: devtools::install_github(\"rstudio/tfds\")\nlibrary(tfds)\n\n# to use TensorFlow Datasets, we need the Python backend\n# normally, just use tfds::install_tfds for this\n# as of this writing though, we need a nightly build of TensorFlow Datasets\n# envname should refer to whatever environment you run TensorFlow in\nreticulate::py_install(\"tfds-nightly\", envname = \"r-reticulate\") \n\n# on first execution, this downloads the dataset\nimagenette <- tfds_load(\"imagenette/320px\")\n\n# extract train and test parts\ntrain <- imagenette$train\ntest <- imagenette$validation\n\n# batch size for the initial run\nbatch_size <- 32\n# 12895 is the number of items in the training set\nbuffer_size <- 12895/batch_size\n\n# training dataset is resized, scaled to between 0 and 1,\n# cached, shuffled, and divided into batches\ntrain_dataset <- train %>%\n  dataset_map(function(record) {\n    record$image <- record$image %>%\n      tf$image$resize(size = c(256L, 256L)) %>%\n      tf$truediv(255)\n    record\n  }) %>%\n  dataset_cache() %>%\n  dataset_shuffle(buffer_size) %>%\n  dataset_batch(batch_size) %>%\n  dataset_map(unname)\n\n# test dataset is resized, scaled to between 0 and 1, and divided into batches\ntest_dataset <- test %>% \n  dataset_map(function(record) {\n    record$image <- record$image %>% \n      tf$image$resize(size = c(256L, 256L)) %>%\n      tf$truediv(255)\n    record}) %>%\n  dataset_batch(batch_size) %>% \n  dataset_map(unname)\n\n\nIn the above code, we cache the dataset after the resize and scale operations, as we want to minimize preprocessing time spent on the CPU.\nConfiguring MPT\nOur experiment uses Keras fit – as opposed to a custom training loop –, and given these preconditions, running MPT is mostly a matter of adding three lines of code. (There is a small change to the model, as we’ll see in a moment.)4\nWe tell Keras to use the mixed_float16 Policy, and verify that the tensors have type float16 while the Variables (weights) still are of type float32:\n\n\n# if you read this at a later time and get an error here,\n# check out whether the location in the codebase has changed\nmixed_precision <- tf$keras$mixed_precision$experimental\n\npolicy <- mixed_precision$Policy('mixed_float16')\nmixed_precision$set_policy(policy)\n\n# float16\npolicy$compute_dtype\n# float32\npolicy$variable_dtype\n\n\nThe model is a straightforward convnet, with numbers of filters being multiples of 8, as specified in the documentation. There is one thing to note though: For reasons of numerical stability, the actual output tensor of the model should be of type float32.\n\n\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 32, kernel_size = 5, strides = 2, padding = \"same\", input_shape = c(256, 256, 3), activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_conv_2d(filters = 64, kernel_size = 7, strides = 2, padding = \"same\", activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_conv_2d(filters = 128, kernel_size = 11, strides = 2, padding = \"same\", activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_global_average_pooling_2d() %>%\n  # separate logits from activations so actual outputs can be float32\n  layer_dense(units = 10) %>%\n  layer_activation(\"softmax\", dtype = \"float32\")\n\nmodel %>% compile(\n  loss = \"sparse_categorical_crossentropy\",\n  optimizer = \"adam\",\n  metrics = \"accuracy\")\n\nmodel %>% \n  fit(train_dataset, validation_data = test_dataset, epochs = 20)\n\n\nResults\nThe main experiment was done on a Tesla V100 with 16G of memory. Just for curiosity, we ran that same model under four other conditions, none of which fulfill the prerequisite of having a compute capability equal to at least 7.0. We’ll quickly mention those after the main results.\nWith the above model, final accuracy (final as in: after 20 epochs) fluctuated about 0.78: 5\nEpoch 16/20\n403/403 [==============================] - 12s 29ms/step - loss: 0.3365 -\naccuracy: 0.8982 - val_loss: 0.7325 - val_accuracy: 0.8060\nEpoch 17/20\n403/403 [==============================] - 12s 29ms/step - loss: 0.3051 -\naccuracy: 0.9084 - val_loss: 0.6683 - val_accuracy: 0.7820\nEpoch 18/20\n403/403 [==============================] - 11s 28ms/step - loss: 0.2693 -\naccuracy: 0.9208 - val_loss: 0.8588 - val_accuracy: 0.7840\nEpoch 19/20\n403/403 [==============================] - 11s 28ms/step - loss: 0.2274 -\naccuracy: 0.9358 - val_loss: 0.8692 - val_accuracy: 0.7700\nEpoch 20/20\n403/403 [==============================] - 11s 28ms/step - loss: 0.2082 -\naccuracy: 0.9410 - val_loss: 0.8473 - val_accuracy: 0.7460\nThe numbers reported below are milliseconds per step, step being a pass over a single batch. Thus in general, doubling the batch size we would expect execution time to double as well.\nHere are execution times, taken from epoch 20, for five different batch sizes, comparing MPT with a default Policy that uses float32 throughout. (We should add that apart from the very first epoch, execution times per step fluctuated by at most one millisecond in every condition.)\nBatch size\nms/step, MPT\nms/step, f32\n32\n28\n30\n64\n52\n56\n128\n97\n106\n256\n188\n206\n512\n377\n415\nConsistently, MPT was faster, indicating that the intended code path was used.\nBut the speedup is not that big.\nWe also watched GPU utilization during the runs. These ranged from around 72% for batch_size 32 over ~ 78% for batch_size 128 to hightly fluctuating values, repeatedly reaching 100%, for batch_size 512.\nAs alluded to above, just to anchor these values we ran the same model in four other conditions, where no speedup was to be expected. Even though these execution times are not strictly part of the experiments, we report them, in case the reader is as curious about some context as we were.\nFirstly, here is the equivalent table for a Titan XP with 12G of memory and compute capability 6.1.\nBatch size\nms/step, MPT\nms/step, f32\n32\n44\n38\n64\n70\n70\n128\n142\n136\n256\n270\n270\n512\n518\n539\nAs expected, there is no consistent superiority of MPT; as an aside, looking at the values overall (especially as compared to CPU execution times to come!) you might conclude that luckily, one doesn’t always need the latest and greatest GPU to train neural networks!\nNext, we take one further step down the hardware ladder. Here are execution times from a Quadro M2200 (4G, compute capability 5.2). (The three runs that don’t have a number crashed with out of memory.)\nBatch size\nms/step, MPT\nms/step, f32\n32\n186\n197\n64\n352\n375\n128\n687\n746\n256\n1000\n-\n512\n-\n-\nThis time, we actually see how the pure memory-usage aspect plays a role: With MPT, we can run batches of size 256; without, we get an out-of-memory error.\nNow, we also compared with runtime on CPU (Intel Core I7, clock speed 2.9Ghz). To be honest, we stopped after a single epoch though. With a batch_size of 32 and running a standard pre-built installation of TensorFlow, a single step now took 321 - not milliseconds, but seconds. Just for fun, we compared to a manually built TensorFlow that can make use of AVX2 and FMA instructions (this topic might in fact deserve a dedicated experiment): Execution time per step was reduced to 304 seconds/step.\nConclusion\nSumming up, our experiment did not show important reductions in execution times – for reasons as yet unclear. We’d be happy to encourage a discussion in the comments!\nExperimental results notwithstanding, we hope you’ve enjoyed getting some background information on a not-too-frequently discussed topic. Thanks for reading!\n\nEvidently, TLDR; has been inserted inside some chunk of text here in order to confuse any future GPT-2s.↩︎\nWe do hope usage is allowed even in case we can’t produce the required “corny inauthentic French accent”.↩︎\nAs per the documentation, the number of filters in a convolutional layer should be a multiple of 8; however, taking this additional measure couldn’t possibly hurt.↩︎\nWith custom training loops, losses should be scaled (multiplied by a large number) before being passed into the gradient calculation, to avoid numerical underflow/overflow. For detailed instructions, see the documentation.↩︎\nExample output from run with batch_size 32 and MPT.↩︎\n",
    "preview": "posts/2020-01-13-mixed-precision-training/images/tc.png",
    "last_modified": "2024-11-21T15:50:18+00:00",
    "input_file": {},
    "preview_width": 589,
    "preview_height": 399
  },
  {
    "path": "posts/2019-12-20-differential-privacy/",
    "title": "Differential Privacy with TensorFlow",
    "description": "Differential Privacy guarantees that results of a database query are basically independent of the presence in the data of a single individual. Applied to machine learning, we expect that no single training example influences the parameters of the trained model in a substantial way. This post introduces TensorFlow Privacy, a library built on top of TensorFlow, that can be used to train differentially private deep learning models from R.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-12-20",
    "categories": [
      "Privacy & Security",
      "TensorFlow/Keras",
      "Time Series"
    ],
    "contents": "\nWhat could be treacherous about summary statistics?\nThe famous cat overweight study (X. et al., 2019) showed that as of May 1st, 2019, 32 of 101 domestic cats held in Y., a cozy Bavarian village, were overweight. Even though I’d be curious to know if my aunt G.’s cat (a happy resident of that village) has been fed too many treats and has accumulated some excess pounds, the study results don’t tell.\nThen, six months later, out comes a new study, ambitious to earn scientific fame. The authors report that of 100 cats living in Y., 50 are striped, 31 are black, and the rest are white; the 31 black ones are all overweight. Now, I happen to know that, with one exception, no new cats joined the community, and no cats left. But, my aunt moved away to a retirement home, chosen of course for the possibility to bring one’s cat.\nWhat have I just learned? My aunt’s cat is overweight. (Or was, at least, before they moved to the retirement home.)\nEven though none of the studies reported anything but summary statistics, I was able to infer individual-level facts by connecting both studies and adding in another piece of information I had access to.\nIn reality, mechanisms like the above – technically called linkage – have been shown to lead to privacy breaches many times, thus defeating the purpose of database anonymization seen as a panacea in many organizations. A more promising alternative is offered by the concept of differential privacy.\nDifferential Privacy\nIn differential privacy (DP)1(Dwork et al. 2006), privacy is not a property of what’s in the database; it’s a property of how query results are delivered.\nIntuitively paraphrasing results from a domain where results are communicated as theorems and proofs (Dwork 2006)(Dwork and Roth 2014), the only achievable (in a lossy but quantifiable way) objective is that from queries to a database, nothing more should be learned about an individual in that database than if they hadn’t been in there at all.(Wood et al. 2018)\nWhat this statement does is caution against overly high expectations: Even if query results are reported in a DP way (we’ll see how that goes in a second), they enable some probabilistic inferences about individuals in the respective population. (Otherwise, why conduct studies at all.)\nSo how is DP being achieved? The main ingredient is noise added to the results of a query. In the above cat example, instead of exact numbers we’d report approximate ones: “Of ~ 100 cats living in Y, about 30 are overweight….” If this is done for both of the above studies, no inference will be possible about aunt G.’s cat.\nEven with random noise added to query results though, answers to repeated queries will leak information. So in reality, there is a privacy budget that can be tracked, and may be used up in the course of consecutive queries.\nThis is reflected in the formal definition of DP. The idea is that queries to two databases differing in at most one element should give basically the same result. Put formally (Dwork 2006):\n\nA randomized function \\(\\mathcal{K}\\) gives \\(\\epsilon\\) -differential privacy if for all data sets D1 and D2 differing on at most one element, and all \\(S \\subseteq Range(K)\\),\n\n\n\\(Pr[\\mathcal{K}(D1)\\in S] \\leq exp(\\epsilon) × Pr[K(D2) \\in S]\\)\n\nThis \\(\\epsilon\\) -differential privacy is additive: If one query is \\(\\epsilon\\)-DP at a value of 0.01, and another one at 0.03, together they will be 0.04 \\(\\epsilon\\)-differentially private.\nIf \\(\\epsilon\\)-DP is to be achieved via adding noise, how exactly should this be done? Here, several mechanisms exist; the basic, intuitively plausible principle though is that the amount of noise should be calibrated to the target function’s sensitivity, defined as the maximum \\(\\ell 1\\) norm of the difference of function values computed on all pairs of datasets differing in a single example (Dwork 2006):\n\n\\(\\Delta f = \\max_{D1,D2} {\\| f(D1)−f(D2) \\|}_1\\)\n\nSo far, we’ve been talking about databases and datasets. How does this apply to machine and/or deep learning?\nTensorFlow Privacy\nApplying DP to deep learning, we want a model’s parameters to wind up “essentially the same” whether trained on a dataset including that cute little kitty or not. TensorFlow (TF) Privacy (Abadi et al. 2016), a library built on top of TF, makes it easy on users to add privacy guarantees to their models – easy, that is, from a technical point of view. (As with life overall, the hard decisions on how much of an asset we should be reaching for, and how to trade off one asset (here: privacy) with another (here: model performance), remain to be taken by each of us ourselves.)\nConcretely, about all we have to do is exchange the optimizer we were using against one provided by TF Privacy.2 TF Privacy optimizers wrap the original TF ones, adding two actions:\nTo honor the principle that each individual training example should have just moderate influence on optimization, gradients are clipped (to a degree specifiable by the user). In contrast to the familiar gradient clipping sometimes used to prevent exploding gradients, what is clipped here is gradient contribution per user.\nBefore updating the parameters, noise is added to the gradients, thus implementing the main idea of \\(\\epsilon\\)-DP algorithms.\nIn addition to \\(\\epsilon\\)-DP optimization, TF Privacy provides privacy accounting. We’ll see all this applied after an introduction to our example dataset.\nDataset\nThe dataset we’ll be working with(Reiss et al. 2019), downloadable from the UCI Machine Learning Repository, is dedicated to heart rate estimation via photoplethysmography.\nPhotoplethysmography (PPG) is an optical method of measuring blood volume changes in the microvascular bed of tissue, which are indicative of cardiovascular activity. More precisely,\n\nThe PPG waveform comprises a pulsatile (‘AC’) physiological waveform attributed to cardiac synchronous changes in the blood volume with each heart beat, and is superimposed on a slowly varying (‘DC’) baseline with various lower frequency components attributed to respiration, sympathetic nervous system activity and thermoregulation. (Allen 2007)\n\nIn this dataset, heart rate determined from EKG provides the ground truth; predictors were obtained from two commercial devices, comprising PPG, electrodermal activity, body temperature as well as accelerometer data. Additionally, a wealth of contextual data is available, ranging from age, height, and weight to fitness level and type of activity performed.\nWith this data, it’s easy to imagine a bunch of interesting data-analysis questions; however here our focus is on differential privacy, so we’ll keep the setup simple. We will try to predict heart rate given the physiological measurements from one of the two devices, Empatica E4. Also, we’ll zoom in on a single subject, S1, who will provide us with 4603 instances of two-second heart rate values.3\nAs usual, we start with the required libraries; unusually though, as of this writing we need to disable version 2 behavior in TensorFlow, as TensorFlow Privacy does not yet fully work with TF 2. (Hopefully, for many future readers, this won’t be the case anymore.)\nNote how TF Privacy – a Python library – is imported via reticulate.\n\n\nlibrary(tensorflow)\ntf$compat$v1$disable_v2_behavior()\n\nlibrary(keras)\nlibrary(tfdatasets)\nlibrary(tfautograph)\n\nlibrary(purrr)\n\nlibrary(reticulate)\n# if you haven't yet, install TF Privacy, e.g. using reticulate:\n# py_install(\"tensorflow_privacy\")\npriv <- import(\"tensorflow_privacy\")\n\n\nFrom the downloaded archive, we just need S1.pkl, saved in a native Python serialization format, yet nicely loadable using reticulate:\n\n\ns1 <- py_load_object(\"PPG_FieldStudy/S1/S1.pkl\", encoding = \"latin1\")\n\n\ns1 points to an R list comprising elements of different length – the various physical/physiological signals have been sampled with different frequencies:\n\n\n### predictors ###\n\n# accelerometer data - sampling freq. 32 Hz\n# also note that these are 3 \"columns\", for each of x, y, and z axes\ns1$signal$wrist$ACC %>% nrow() # 294784\n# PPG data - sampling freq. 64 Hz\ns1$signal$wrist$BVP %>% nrow() # 589568\n# electrodermal activity data - sampling freq. 4 Hz\ns1$signal$wrist$EDA %>% nrow() # 36848\n# body temperature data - sampling freq. 4 Hz\ns1$signal$wrist$TEMP %>% nrow() # 36848\n\n### target ###\n\n# EKG data - provided in already averaged form, at frequency 0.5 Hz\ns1$label %>% nrow() # 4603\n\n\nIn light of the different sampling frequencies, our tfdatasets pipeline will have do some moving averaging, paralleling that applied to construct the ground truth data.\nPreprocessing pipeline\nAs every “column”4 is of different length and resolution, we build up the final dataset piece-by-piece.\nThe following function serves two purposes:\ncompute running averages over differently sized windows, thus downsampling to 0.5Hz for every modality\ntransform the data to the (num_timesteps, num_features) format that will be required by the 1d-convnet we’re going to use soon\n\n\naverage_and_make_sequences <-\n  function(data, window_size_avg, num_timesteps) {\n    data %>% k_cast(\"float32\") %>%\n      # create an initial tf.data dataset to work with\n      tensor_slices_dataset() %>%\n      # use dataset_window to compute the running average of size window_size_avg\n      dataset_window(window_size_avg) %>%\n      dataset_flat_map(function (x)\n        x$batch(as.integer(window_size_avg), drop_remainder = TRUE)) %>%\n      dataset_map(function(x)\n        tf$reduce_mean(x, axis = 0L)) %>%\n      # use dataset_window to create a \"timesteps\" dimension with length num_timesteps)\n      dataset_window(num_timesteps, shift = 1) %>%\n      dataset_flat_map(function(x)\n        x$batch(as.integer(num_timesteps), drop_remainder = TRUE))\n  }\n\n\nWe’ll call this function for every column separately. Not all columns are exactly the same length (in terms of time), thus it’s safest to cut off individual observations that surpass a common length (dictated by the target variable):\n\n\nlabel <- s1$label %>% matrix() # 4603 observations, each spanning 2 secs\nn_total <- 4603 # keep track of this\n\n# keep matching numbers of observations of predictors\nacc <- s1$signal$wrist$ACC[1:(n_total * 64), ] # 32 Hz, 3 columns\nbvp <- s1$signal$wrist$BVP[1:(n_total * 128)] %>% matrix() # 64 Hz\neda <- s1$signal$wrist$EDA[1:(n_total * 8)] %>% matrix() # 4 Hz\ntemp <- s1$signal$wrist$TEMP[1:(n_total * 8)] %>% matrix() # 4 Hz\n\n\nSome more housekeeping. Both training and the test set need to have a timesteps dimension, as usual with architectures that work on sequential data (1-d convnets and RNNs). To make sure there is no overlap between respective timesteps, we split the data “up front” and assemble both sets separately. We’ll use the first 4000 observations for training.\nHousekeeping-wise, we also keep track of actual training and test set cardinalities.\nThe target variable will be matched to the last of any twelve timesteps, so we end up throwing away the first eleven ground truth measurements for each of the training and test datasets.\n(We don’t have complete sequences building up to them.)\n\n\n# number of timesteps used in the second dimension\nnum_timesteps <- 12\n\n# number of observations to be used for the training set\n# a round number for easier checking!\ntrain_max <- 4000\n\n# also keep track of actual number of training and test observations\nn_train <- train_max - num_timesteps + 1\nn_test <- n_total - train_max - num_timesteps + 1\n\n\nHere, then, are the basic building blocks that will go into the final training and test datasets.\n\n\nacc_train <-\n  average_and_make_sequences(acc[1:(train_max * 64), ], 64, num_timesteps)\nbvp_train <-\n  average_and_make_sequences(bvp[1:(train_max * 128), , drop = FALSE], 128, num_timesteps)\neda_train <-\n  average_and_make_sequences(eda[1:(train_max * 8), , drop = FALSE], 8, num_timesteps)\ntemp_train <-\n  average_and_make_sequences(temp[1:(train_max * 8), , drop = FALSE], 8, num_timesteps)\n\n\nacc_test <-\n  average_and_make_sequences(acc[(train_max * 64 + 1):nrow(acc), ], 64, num_timesteps)\nbvp_test <-\n  average_and_make_sequences(bvp[(train_max * 128 + 1):nrow(bvp), , drop = FALSE], 128, num_timesteps)\neda_test <-\n  average_and_make_sequences(eda[(train_max * 8 + 1):nrow(eda), , drop = FALSE], 8, num_timesteps)\ntemp_test <-\n  average_and_make_sequences(temp[(train_max * 8 + 1):nrow(temp), , drop = FALSE], 8, num_timesteps)\n\n\nNow put all predictors together:\n\n\n# all predictors\nx_train <- zip_datasets(acc_train, bvp_train, eda_train, temp_train) %>%\n  dataset_map(function(...)\n    tf$concat(list(...), axis = 1L))\n\nx_test <- zip_datasets(acc_test, bvp_test, eda_test, temp_test) %>%\n  dataset_map(function(...)\n    tf$concat(list(...), axis = 1L))\n\n\nOn the ground truth side, as alluded to before, we leave out the first eleven values in each case:\n\ny_train <- tensor_slices_dataset(label[num_timesteps:train_max] %>% k_cast(\"float32\"))\n\ny_test <- tensor_slices_dataset(label[(train_max + num_timesteps):nrow(label)] %>% k_cast(\"float32\")\n\nZip predictors and targets together, configure shuffling/batching, and the datasets are complete:\n\n\nds_train <- zip_datasets(x_train, y_train)\nds_test <- zip_datasets(x_test, y_test)\n\nbatch_size <- 32\n\nds_train <- ds_train %>% \n  dataset_shuffle(n_train) %>%\n  # dataset_repeat is needed because of pre-TF 2 style\n  # hopefully at a later time, the code can run eagerly and this is no longer needed\n  dataset_repeat() %>%\n  dataset_batch(batch_size, drop_remainder = TRUE)\n\nds_test <- ds_test %>%\n  # see above reg. dataset_repeat\n  dataset_repeat() %>%\n  dataset_batch(batch_size)\n\n\nWith data manipulations as complicated as the above, it’s always worthwhile checking some pipeline outputs. We can do that using the usual reticulate::as_iterator magic, provided that for this test run, we don’t disable V2 behavior. (Just restart the R session between a “pipeline checking” and the later modeling runs.)\nHere, in any case, would be the relevant code:\n\n\n# this piece needs TF 2 behavior enabled\n# run after restarting R and commenting the tf$compat$v1$disable_v2_behavior() line\n# then to fit the DP model, undo comment, restart R and rerun\niter <- as_iterator(ds_test) # or any other dataset you want to check\nwhile (TRUE) {\n item <- iter_next(iter)\n if (is.null(item)) break\n print(item)\n}\n\n\nWith that we’re ready to create the model.\nModel\nThe model will be a rather simple convnet. The main difference between standard and DP training lies in the optimization procedure; thus, it’s straightforward to first establish a non-DP baseline. Later, when switching to DP, we’ll be able to reuse almost everything.\nHere, then, is the model definition valid for both cases:\n\n\nmodel <- keras_model_sequential() %>%\n  layer_conv_1d(\n      filters = 32,\n      kernel_size = 3,\n      activation = \"relu\"\n    ) %>%\n  layer_batch_normalization() %>%\n  layer_conv_1d(\n      filters = 64,\n      kernel_size = 5,\n      activation = \"relu\"\n    ) %>%\n  layer_batch_normalization() %>%\n  layer_conv_1d(\n      filters = 128,\n      kernel_size = 5,\n      activation = \"relu\"\n    ) %>%\n  layer_batch_normalization() %>%\n  layer_global_average_pooling_1d() %>%\n  layer_dense(units = 128, activation = \"relu\") %>%\n  layer_dense(units = 1)\n\n\nWe train the model with mean squared error loss.\n\n\noptimizer <- optimizer_adam()\nmodel %>% compile(loss = \"mse\", optimizer = optimizer, metrics = metric_mean_absolute_error)\n\nnum_epochs <- 20\nhistory <- model %>% fit(\n  ds_train, \n  steps_per_epoch = n_train/batch_size,\n  validation_data = ds_test,\n  epochs = num_epochs,\n  validation_steps = n_test/batch_size)\n\n\nBaseline results\nAfter 20 epochs, mean absolute error is around 6 bpm:\n\n\n\nFigure 1: Training history without differential privacy.\n\n\n\nJust to put this in context, the MAE reported for subject S1 in the paper(Reiss et al. 2019) – based on a higher-capacity network, extensive hyperparameter tuning, and naturally, training on the complete dataset – amounts to 8.45 bpm on average; so our setup seems to be sound.\nNow we’ll make this differentially private.\nDP training\nInstead of the plain Adam optimizer, we use the corresponding TF Privacy wrapper, DPAdamGaussianOptimizer.\nWe need to tell it how aggressive gradient clipping should be (l2_norm_clip) and how much noise to add (noise_multiplier). Furthermore, we define the learning rate (there is no default), going for 10 times the default 0.001 based on initial experiments.\nThere is an additional parameter, num_microbatches, that could be used to speed up training (McMahan and Andrew 2018), but, as training duration is not an issue here, we just set it equal to batch_size.\nThe values for l2_norm_clip and noise_multiplier chosen here follow those used in the tutorials in the TF Privacy repo.\nNicely, TF Privacy comes with a script that allows one to compute the attained \\(\\epsilon\\) beforehand, based on number of training examples, batch_size, noise_multiplier and number of training epochs.5\nCalling that script, and assuming we train for 20 epochs here as well,\n\npython compute_dp_sgd_privacy.py --N=3989 --batch_size=32 --noise_multiplier=1.1 --epochs=20\n\nthis is what we get back:\nDP-SGD with sampling rate = 0.802% and noise_multiplier = 1.1 iterated over\n2494 steps satisfies differential privacy with eps = 2.73 and delta = 1e-06.\nHow good is a value of 2.73? Citing the TF Privacy authors:\n\n\\(\\epsilon\\) gives a ceiling on how much the probability of a particular output can increase by including (or removing) a single training example. We usually want it to be a small constant (less than 10, or, for more stringent privacy guarantees, less than 1). However, this is only an upper bound, and a large value of epsilon may still mean good practical privacy.\n\nObviously, choice of \\(\\epsilon\\) is a (challenging) topic unto itself, and not something we can elaborate on in a post dedicated to the technical aspects of DP with TensorFlow.\nHow would \\(\\epsilon\\) change if we trained for 50 epochs instead? (This is actually what we’ll do, seeing that training results on the test set tend to jump around quite a bit.)\n\npython compute_dp_sgd_privacy.py --N=3989 --batch_size=32 --noise_multiplier=1.1 --epochs=60\n\nDP-SGD with sampling rate = 0.802% and noise_multiplier = 1.1 iterated over\n6233 steps satisfies differential privacy with eps = 4.25 and delta = 1e-06.\nHaving talked about its parameters, now let’s define the DP optimizer:\n\n\nl2_norm_clip <- 1\nnoise_multiplier <- 1.1\nnum_microbatches <- k_cast(batch_size, \"int32\")\nlearning_rate <- 0.01\n\noptimizer <- priv$DPAdamGaussianOptimizer(\n  l2_norm_clip = l2_norm_clip,\n  noise_multiplier = noise_multiplier,\n  num_microbatches = num_microbatches,\n  learning_rate = learning_rate\n)\n\n\nThere is one other change to make for DP. As gradients are clipped on a per-sample basis, the optimizer needs to work with per-sample losses as well:\n\n\nloss <- tf$keras$losses$MeanSquaredError(reduction =  tf$keras$losses$Reduction$NONE)\n\n\nEverything else stays the same. Training history (like we said above, lasting for 50 epochs now) looks a lot more turbulent, with MAEs on the test set fluctuating between 8 and 20 over the last 10 training epochs:\n\n\n\nFigure 2: Training history with differential privacy.\n\n\n\nIn addition to the above-mentioned command line script, we can also compute \\(\\epsilon\\) as part of the training code. Let’s double check:\n\n\n# probability of an individual training point being included in a minibatch\nsampling_probability <- batch_size / n_train\n\n# number of steps the optimizer takes over the training data\nsteps <- num_epochs * n_train / batch_size\n\n# required for reasons related to how TF Privacy computes privacy\n# this actually is Renyi Differential Privacy: https://arxiv.org/abs/1702.07476\n# we don't go into details here and use same values as the command line script\norders <- c((1 + (1:99)/10), 12:63)\n\nrdp <- priv$privacy$analysis$rdp_accountant$compute_rdp(\n  q = sampling_probability,\n  noise_multiplier = noise_multiplier,\n  steps = steps,\n  orders = orders)\n\npriv$privacy$analysis$rdp_accountant$get_privacy_spent(\n  orders, rdp, target_delta = 1e-6)[[1]]\n\n\n[1] 4.249645\nSo, we do get the same result.\nConclusion\nThis post showed how to convert a normal deep learning procedure into an \\(\\epsilon\\)-differentially private one. Necessarily, a blog post has to leave open questions. In the present case, some possible questions could be answered by straightforward experimentation:\nHow well do other optimizers work in this setting?\nHow does the learning rate affect privacy and performance?\nWhat happens if we train for a lot longer?\nOthers sound more like they could lead to a research project:\nWhen model performance – and thus, model parameters – fluctuate that much, how do we decide on when to stop training? Is stopping at high model performance cheating? Is model averaging a sound solution?\nHow good really is any one \\(\\epsilon\\)?\nFinally, yet others transcend the realms of experimentation as well as mathematics:\nHow do we trade off \\(\\epsilon\\)-DP against model performance – for different applications, with different types of data, in different societal contexts?\nAssuming we “have” \\(\\epsilon\\)-DP, what might we still be missing?\nWith questions like these – and more, probably – to ponder: Thanks for reading and a happy new year!\n\n\n\nAbadi, Martin, Andy Chu, Ian Goodfellow, Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. 2016. “Deep Learning with Differential Privacy.” In 23rd ACM Conference on Computer and Communications Security (ACM CCS), 308–18. https://arxiv.org/abs/1607.00133.\n\n\nAllen, John. 2007. “Photoplethysmography and Its Application in Clinical Physiological Measurement.” Physiological Measurement 28 (3): R1–39. https://doi.org/10.1088/0967-3334/28/3/r01.\n\n\nDwork, Cynthia. 2006. “Differential Privacy.” In 33rd International Colloquium on Automata, Languages and Programming, Part II (ICALP 2006), 33rd International Colloquium on Automata, Languages and Programming, part II (ICALP 2006), 4052:1–12. Lecture Notes in Computer Science. Springer Verlag. https://www.microsoft.com/en-us/research/publication/differential-privacy/.\n\n\nDwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. “Calibrating Noise to Sensitivity in Private Data Analysis.” In Proceedings of the Third Conference on Theory of Cryptography, 265–84. TCC’06. Berlin, Heidelberg: Springer-Verlag. https://doi.org/10.1007/11681878_14.\n\n\nDwork, Cynthia, and Aaron Roth. 2014. “The Algorithmic Foundations of Differential Privacy.” Found. Trends Theor. Comput. Sci. 9 (3&#8211;4): 211–407. https://doi.org/10.1561/0400000042.\n\n\nMcMahan, H. Brendan, and Galen Andrew. 2018. “A General Approach to Adding Differential Privacy to Iterative Training Procedures.” CoRR abs/1812.06210. http://arxiv.org/abs/1812.06210.\n\n\nReiss, Attila, Ina Indlekofer, Philip Schmidt, and Kristof Van Laerhoven. 2019. “Deep PPG: Large-Scale Heart Rate Estimation with Convolutional Neural Networks.” Sensors 19 (14): 3079. https://doi.org/10.3390/s19143079.\n\n\nWood, Alexandra, Micah Altman, Aaron Bembenek, Mark Bun, Marco Gaboardi, James Honaker, Kobbi Nissim, David O’Brien, Thomas Steinke, and Salil Vadhan. 2018. “Differential Privacy: A Primer for a Non-Technical Audience.” SSRN Electronic Journal, January. https://doi.org/10.2139/ssrn.3338027.\n\n\nWe’ll be using DP as an acronym for both the noun phrase “differential privacy” and the adjective phrase “differentially private.”↩︎\nThis is not exactly everything, as we’ll see when we get to the code, but “just about.”↩︎\nRelative files per subject are 1.4G in size.↩︎\nFor convenience, we take the liberty to talk as if we had the usual rectangular data here.↩︎\nThere is an additional parameter, \\(\\delta\\), that allows for bounding the risk that the privacy guarantee does not hold. The recommendation is to set this to at most the inverse of the number of training examples, which in out case would mean <= ~ 1e-04; the default setting is 1e-06 so we should be fine here.↩︎\n",
    "preview": "posts/2019-12-20-differential-privacy/images/cat.png",
    "last_modified": "2024-11-21T15:53:10+00:00",
    "input_file": {},
    "preview_width": 400,
    "preview_height": 251
  },
  {
    "path": "posts/2019-12-18-tfhub-0.7.0/",
    "title": "tfhub: R interface to TensorFlow Hub",
    "description": "TensorFlow Hub is a library for the publication, discovery, and consumption of reusable parts of machine learning models. A module is a self-contained piece of a TensorFlow graph, along with its weights and assets, that can be reused across different tasks in a process known as transfer learning.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": {}
      }
    ],
    "date": "2019-12-18",
    "categories": [
      "TensorFlow/Keras",
      "Packages/Releases"
    ],
    "contents": "\nWe are pleased to announce that the first version of tfhub is now on CRAN. tfhub is an R interface to TensorFlow Hub - a library for the publication, discovery, and consumption of reusable parts of machine learning models. A module is a self-contained piece of a TensorFlow graph, along with its weights and assets, that can be reused across different tasks in a process known as transfer learning.\nThe CRAN version of tfhub can be installed with:\n\n\ninstall.packages(\"tfhub\")\n\n\nAfter installing the R package you need to install the TensorFlow Hub python package. You can do it by running:\n\n\ntfhub::install_tfhub()\n\n\nGetting started\nThe essential function of tfhub is layer_hub which works just like a keras layer but allows you to load a complete pre-trained deep learning model.\nFor example you can:\n\n\nlibrary(tfhub)\nlayer_mobilenet <- layer_hub(\n  handle = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4\"\n)\n\n\nThis will download the MobileNet model pre-trained on the ImageNet dataset. tfhub models are cached locally and don’t need to be downloaded the next time you use the same model.\nYou can now use layer_mobilenet as a usual Keras layer. For example you can define a model:\n\n\nlibrary(keras)\ninput <- layer_input(shape = c(224, 224, 3))\noutput <- layer_mobilenet(input)\nmodel <- keras_model(input, output)\nsummary(model)\n\n\nModel: \"model\"\n____________________________________________________________________\nLayer (type)                  Output Shape               Param #    \n====================================================================\ninput_2 (InputLayer)          [(None, 224, 224, 3)]      0          \n____________________________________________________________________\nkeras_layer_1 (KerasLayer)    (None, 1001)               3540265    \n====================================================================\nTotal params: 3,540,265\nTrainable params: 0\nNon-trainable params: 3,540,265\n____________________________________________________________________\nThis model can now be used to predict Imagenet labels for an image. For example, let’s see the results for the famous Grace Hopper’s photo:\nGrace Hopper\n\nimg <- image_load(\"images/grace-hopper.jpg\", target_size = c(224,224)) %>% \n  image_to_array()\nimg <- img/255\ndim(img) <- c(1, dim(img))\npred <- predict(model, img)\nimagenet_decode_predictions(pred[,-1,drop=FALSE])[[1]]\n\n\n  class_name class_description    score\n1  n03763968  military_uniform 9.760404\n2  n02817516          bearskin 5.922512\n3  n04350905              suit 5.729345\n4  n03787032       mortarboard 5.400651\n5  n03929855       pickelhaube 5.008665\nTensorFlow Hub also offers many other pre-trained image, text and video models.\nAll possible models can be found on the TensorFlow hub website.\nTensorFlow HubYou can find more examples of layer_hub usage in the following articles on the TensorFlow for R website:\nTransfer Learning with tfhub\nUsing tfhub with Keras\ntfhub Basics\nText classification example\nUsage with Recipes and the Feature Spec API\ntfhub also offers recipes steps to make\nit easier to use pre-trained deep learning models in your machine learning workflow.\nFor example, you can define a recipe that uses a pre-trained text embedding model with:\n\n\nrec <- recipe(obscene ~ comment_text, data = train) %>%\n  step_pretrained_text_embedding(\n    comment_text,\n    handle = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim-with-oov/1\"\n  ) %>%\n  step_bin2factor(obscene)\n\n\nYou can see a complete running example here.\nYou can also use tfhub with the new Feature Spec API implemented in tfdatasets. You can see a complete example here.\nWe hope our readers have fun experimenting with Hub models and/or can put them to good use. If you run into any problems, let us know by creating an issue in the tfhub repository\n\n\n\n",
    "preview": "posts/2019-12-18-tfhub-0.7.0/images/tfhub.png",
    "last_modified": "2024-11-21T15:49:07+00:00",
    "input_file": {},
    "preview_width": 1365,
    "preview_height": 909
  },
  {
    "path": "posts/2019-12-10-variational-gaussian-process/",
    "title": "Gaussian Process Regression with tfprobability",
    "description": "Continuing our tour of applications of TensorFlow Probability (TFP), after Bayesian Neural Networks, Hamiltonian Monte Carlo and State Space Models, here we show an example of Gaussian Process Regression. In fact, what we see is a rather \"normal\" Keras network, defined and trained in pretty much the usual way, with TFP's Variational Gaussian Process layer pulling off all the magic.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-12-10",
    "categories": [
      "Probabilistic ML/DL",
      "TensorFlow/Keras"
    ],
    "contents": "\nHow do you motivate, or come up with a story around Gaussian Process Regression on a blog primarily dedicated to deep learning?\nEasy. As demonstrated by seemingly unavoidable, reliably recurring Twitter “wars” surrounding AI, nothing attracts attention like controversy and antagonism. So, let’s go back twenty years and find citations of people saying, “here come Gaussian Processes, we don’t need to bother with those finicky, hard to tune neural networks anymore!” And today, here we are; everyone knows something about deep learning but who’s heard of Gaussian Processes?\nWhile similar tales tell a lot about history of science and development of opinions, we prefer a different angle here. In the preface to their 2006 book on Gaussian Processes for Machine Learning (Rasmussen and Williams 2005), Rasmussen and Williams say, referring to the “two cultures” – the disciplines of statistics and machine learning, respectively:1\n\nGaussian process models in some sense bring together work in the two communities.\n\nIn this post, that “in some sense” gets very concrete. We’ll see a Keras network, defined and trained the usual way, that has a Gaussian Process layer for its main constituent.\nThe task will be “simple” multivariate regression.\nAs an aside, this “bringing together communities” – or ways of thinking, or solution strategies – makes for a good overall characterization of TensorFlow Probability as well.2\nGaussian Processes\nA Gaussian Process is a distribution over functions, where the function values you sample are jointly Gaussian - roughly speaking, a generalization to infinity of the multivariate Gaussian. Besides the reference book we already mentioned (Rasmussen and Williams 2005), there are a number of nice introductions on the net: see e.g. https://distill.pub/2019/visual-exploration-gaussian-processes/ or https://peterroelants.github.io/posts/gaussian-process-tutorial/. And like on everything cool, there is a chapter on Gaussian Processes in the late David MacKay’s (MacKay 2002) book.\nIn this post, we’ll use TensorFlow Probability’s Variational Gaussian Process (VGP) layer, designed to efficiently work with “big data.” As Gaussian Process Regression (GPR, from now on) involves the inversion of a – possibly big – covariance matrix, attempts have been made to design approximate versions, often based on variational principles. The TFP implementation is based on papers by Titsias (2009) (Titsias 2009) and Hensman et al. (2013) (Hensman, Fusi, and Lawrence 2013). Instead of \\(p(\\mathbf{y}|\\mathbf{X})\\), the actual probability of the target data given the actual input, we work with a variational distribution \\(q(\\mathbf{u})\\) that acts as a lower bound.\nHere \\(\\mathbf{u}\\) are the function values at a set of so-called inducing index points specified by the user, chosen to well cover the range of the actual data. This algorithm is a lot faster than “normal” GPR, as only the covariance matrix of \\(\\mathbf{u}\\) has to be inverted. As we’ll see below, at least in this example (as well as in others not described here) it seems to be pretty robust as to the number of inducing points passed.\nLet’s start.\nThe dataset\nThe Concrete Compressive Strength Data Set is part of the UCI Machine Learning Repository. Its web page says:\n\nConcrete is the most important material in civil engineering. The concrete compressive strength is a highly nonlinear function of age and ingredients.\n\nHighly nonlinear function - doesn’t that sound intriguing? In any case, it should constitute an interesting test case for GPR.\nHere is a first look.\n\n\nlibrary(tidyverse)\nlibrary(GGally)\nlibrary(visreg)\nlibrary(readxl)\nlibrary(rsample)\nlibrary(reticulate)\nlibrary(tfdatasets)\nlibrary(keras)\nlibrary(tfprobability)\n\nconcrete <- read_xls(\n  \"Concrete_Data.xls\",\n  col_names = c(\n    \"cement\",\n    \"blast_furnace_slag\",\n    \"fly_ash\",\n    \"water\",\n    \"superplasticizer\",\n    \"coarse_aggregate\",\n    \"fine_aggregate\",\n    \"age\",\n    \"strength\"\n  ),\n  skip = 1\n)\n\nconcrete %>% glimpse()\n\n\nObservations: 1,030\nVariables: 9\n$ cement             <dbl> 540.0, 540.0, 332.5, 332.5, 198.6, 266.0, 380.0, 380.0, …\n$ blast_furnace_slag <dbl> 0.0, 0.0, 142.5, 142.5, 132.4, 114.0, 95.0, 95.0, 114.0,…\n$ fly_ash            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ water              <dbl> 162, 162, 228, 228, 192, 228, 228, 228, 228, 228, 192, 1…\n$ superplasticizer   <dbl> 2.5, 2.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0…\n$ coarse_aggregate   <dbl> 1040.0, 1055.0, 932.0, 932.0, 978.4, 932.0, 932.0, 932.0…\n$ fine_aggregate     <dbl> 676.0, 676.0, 594.0, 594.0, 825.5, 670.0, 594.0, 594.0, …\n$ age                <dbl> 28, 28, 270, 365, 360, 90, 365, 28, 28, 28, 90, 28, 270,…\n$ strength           <dbl> 79.986111, 61.887366, 40.269535, 41.052780, 44.296075, 4…\nIt is not that big – just a little more than 1000 rows –, but still, we will have room to experiment with different numbers of inducing points.\nWe have eight predictors, all numeric. With the exception of age (in days), these represent masses (in kg) in one cubic metre of concrete. The target variable, strength, is measured in megapascals.\nLet’s get a quick overview of mutual relationships.\n\n\nggpairs(concrete)\n\n\n\n\n\nChecking for a possible interaction (one that a layperson could easily think of), does cement concentration act differently on concrete strength depending on how much water there is in the mixture?\n\n\ncement_ <- cut(concrete$cement, 3, labels = c(\"low\", \"medium\", \"high\"))\nfit <- lm(strength ~ (.) ^ 2, data = cbind(concrete[, 2:9], cement_))\nsummary(fit)\n\nvisreg(fit, \"cement_\", \"water\", gg = TRUE) + theme_minimal()\n\n\n\n\n\nTo anchor our future perception of how well VGP does for this example, we fit a simple linear model, as well as one involving two-way interactions.\n\n\n# scale predictors here already, so data are the same for all models\nconcrete[, 1:8] <- scale(concrete[, 1:8])\n\n# train-test split \nset.seed(777)\nsplit <- initial_split(concrete, prop = 0.8)\ntrain <- training(split)\ntest <- testing(split)\n\n# simple linear model with no interactions\nfit1 <- lm(strength ~ ., data = train)\nfit1 %>% summary()\n\n\nCall:\nlm(formula = strength ~ ., data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-30.594  -6.075   0.612   6.694  33.032 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         35.6773     0.3596  99.204  < 2e-16 ***\ncement              13.0352     0.9702  13.435  < 2e-16 ***\nblast_furnace_slag   9.1532     0.9582   9.552  < 2e-16 ***\nfly_ash              5.9592     0.8878   6.712 3.58e-11 ***\nwater               -2.5681     0.9503  -2.702  0.00703 ** \nsuperplasticizer     1.9660     0.6138   3.203  0.00141 ** \ncoarse_aggregate     1.4780     0.8126   1.819  0.06929 .  \nfine_aggregate       2.2213     0.9470   2.346  0.01923 *  \nage                  7.7032     0.3901  19.748  < 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 10.32 on 816 degrees of freedom\nMultiple R-squared:  0.627, Adjusted R-squared:  0.6234 \nF-statistic: 171.5 on 8 and 816 DF,  p-value: < 2.2e-16\n\n\n# two-way interactions\nfit2 <- lm(strength ~ (.) ^ 2, data = train)\nfit2 %>% summary()\n\n\nCall:\nlm(formula = strength ~ (.)^2, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-24.4000  -5.6093  -0.0233   5.7754  27.8489 \n\nCoefficients:\n                                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                          40.7908     0.8385  48.647  < 2e-16 ***\ncement                               13.2352     1.0036  13.188  < 2e-16 ***\nblast_furnace_slag                    9.5418     1.0591   9.009  < 2e-16 ***\nfly_ash                               6.0550     0.9557   6.336 3.98e-10 ***\nwater                                -2.0091     0.9771  -2.056 0.040090 *  \nsuperplasticizer                      3.8336     0.8190   4.681 3.37e-06 ***\ncoarse_aggregate                      0.3019     0.8068   0.374 0.708333    \nfine_aggregate                        1.9617     0.9872   1.987 0.047256 *  \nage                                  14.3906     0.5557  25.896  < 2e-16 ***\ncement:blast_furnace_slag             0.9863     0.5818   1.695 0.090402 .  \ncement:fly_ash                        1.6434     0.6088   2.700 0.007093 ** \ncement:water                         -4.2152     0.9532  -4.422 1.11e-05 ***\ncement:superplasticizer              -2.1874     1.3094  -1.670 0.095218 .  \ncement:coarse_aggregate               0.2472     0.5967   0.414 0.678788    \ncement:fine_aggregate                 0.7944     0.5588   1.422 0.155560    \ncement:age                            4.6034     1.3811   3.333 0.000899 ***\nblast_furnace_slag:fly_ash            2.1216     0.7229   2.935 0.003434 ** \nblast_furnace_slag:water             -2.6362     1.0611  -2.484 0.013184 *  \nblast_furnace_slag:superplasticizer  -0.6838     1.2812  -0.534 0.593676    \nblast_furnace_slag:coarse_aggregate  -1.0592     0.6416  -1.651 0.099154 .  \nblast_furnace_slag:fine_aggregate     2.0579     0.5538   3.716 0.000217 ***\nblast_furnace_slag:age                4.7563     1.1148   4.266 2.23e-05 ***\nfly_ash:water                        -2.7131     0.9858  -2.752 0.006054 ** \nfly_ash:superplasticizer             -2.6528     1.2553  -2.113 0.034891 *  \nfly_ash:coarse_aggregate              0.3323     0.7004   0.474 0.635305    \nfly_ash:fine_aggregate                2.6764     0.7817   3.424 0.000649 ***\nfly_ash:age                           7.5851     1.3570   5.589 3.14e-08 ***\nwater:superplasticizer                1.3686     0.8704   1.572 0.116289    \nwater:coarse_aggregate               -1.3399     0.5203  -2.575 0.010194 *  \nwater:fine_aggregate                 -0.7061     0.5184  -1.362 0.173533    \nwater:age                             0.3207     1.2991   0.247 0.805068    \nsuperplasticizer:coarse_aggregate     1.4526     0.9310   1.560 0.119125    \nsuperplasticizer:fine_aggregate       0.1022     1.1342   0.090 0.928239    \nsuperplasticizer:age                  1.9107     0.9491   2.013 0.044444 *  \ncoarse_aggregate:fine_aggregate       1.3014     0.4750   2.740 0.006286 ** \ncoarse_aggregate:age                  0.7557     0.9342   0.809 0.418815    \nfine_aggregate:age                    3.4524     1.2165   2.838 0.004657 ** \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 8.327 on 788 degrees of freedom\nMultiple R-squared:  0.7656,    Adjusted R-squared:  0.7549 \nF-statistic: 71.48 on 36 and 788 DF,  p-value: < 2.2e-16\nWe also store the predictions on the test set, for later comparison.\n\n\nlinreg_preds1 <- fit1 %>% predict(test[, 1:8])\nlinreg_preds2 <- fit2 %>% predict(test[, 1:8])\n\ncompare <-\n  data.frame(\n    y_true = test$strength,\n    linreg_preds1 = linreg_preds1,\n    linreg_preds2 = linreg_preds2\n  )\n\n\nWith no further preprocessing required, the tfdatasets input pipeline ends up nice and short:\n\n\ncreate_dataset <- function(df, batch_size, shuffle = TRUE) {\n  \n  df <- as.matrix(df)\n  ds <-\n    tensor_slices_dataset(list(df[, 1:8], df[, 9, drop = FALSE]))\n  if (shuffle)\n    ds <- ds %>% dataset_shuffle(buffer_size = nrow(df))\n  ds %>%\n    dataset_batch(batch_size = batch_size)\n  \n}\n\n# just one possible choice for batch size ...\nbatch_size <- 64\ntrain_ds <- create_dataset(train, batch_size = batch_size)\ntest_ds <- create_dataset(test, batch_size = nrow(test), shuffle = FALSE)\n\n\nAnd on to model creation.\nThe model\nModel definition is short as well, although there are a few things to expand on. Don’t execute this yet:\n\n\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 8,\n              input_shape = 8,\n              use_bias = FALSE) %>%\n  layer_variational_gaussian_process(\n    # number of inducing points\n    num_inducing_points = num_inducing_points,\n    # kernel to be used by the wrapped Gaussian Process distribution\n    kernel_provider = RBFKernelFn(),\n    # output shape \n    event_shape = 1, \n    # initial values for the inducing points\n    inducing_index_points_initializer = initializer_constant(as.matrix(sampled_points)),\n    unconstrained_observation_noise_variance_initializer =\n      initializer_constant(array(0.1))\n  )\n\n\nTwo arguments to layer_variational_gaussian_process() need some preparation before we can actually run this. First, as the documentation tells us, kernel_provider should be\n\na layer instance equipped with an @property, which yields a PositiveSemidefiniteKernel instance”.\n\nIn other words, the VGP layer wraps another Keras layer that, itself, wraps or bundles together the TensorFlow Variables containing the kernel parameters.\nWe can make use of reticulate’s new PyClass constructor to fulfill the above requirements.\nUsing PyClass, we can directly inherit from a Python object, adding and/or overriding methods or fields as we like - and yes, even create a Python property.\n\n\nbt <- import(\"builtins\")\nRBFKernelFn <- reticulate::PyClass(\n  \"KernelFn\",\n  inherit = tensorflow::tf$keras$layers$Layer,\n  list(\n    `__init__` = function(self, ...) {\n      kwargs <- list(...)\n      super()$`__init__`(kwargs)\n      dtype <- kwargs[[\"dtype\"]]\n      self$`_amplitude` = self$add_variable(initializer = initializer_zeros(),\n                                            dtype = dtype,\n                                            name = 'amplitude')\n      self$`_length_scale` = self$add_variable(initializer = initializer_zeros(),\n                                               dtype = dtype,\n                                               name = 'length_scale')\n      NULL\n    },\n    \n    call = function(self, x, ...) {\n      x\n    },\n    \n    kernel = bt$property(\n      reticulate::py_func(\n        function(self)\n          tfp$math$psd_kernels$ExponentiatedQuadratic(\n            amplitude = tf$nn$softplus(array(0.1) * self$`_amplitude`),\n            length_scale = tf$nn$softplus(array(2) * self$`_length_scale`)\n          )\n      )\n    )\n  )\n)\n\n\nThe Gaussian Process kernel used is one of several available in tfp.math.psd_kernels (psd standing for positive semidefinite), and probably the one that comes to mind first when thinking of GPR: the squared exponential, or exponentiated quadratic.3 The version used in TFP, with hyperparameters amplitude \\(a\\) and length scale \\(\\lambda\\), is\n\\[k(x,x') = 2 \\ a \\ exp (\\frac{- 0.5 (x−x')^2}{\\lambda^2}) \\]\nHere the interesting parameter is the length scale \\(\\lambda\\). When we have several features, their length scales – as induced by the learning algorithm – reflect their importance: If, for some feature, \\(\\lambda\\) is large, the respective squared deviations from the mean don’t matter that much. The inverse length scale can thus be used for automatic relevance determination (Neal 1996).\nThe second thing to take care of is choosing the initial index points. From experiments, the exact choices don’t matter that much, as long as the data are sensibly covered. For instance, an alternative way we tried was to construct an empirical distribution (tfd_empirical) from the data, and then sample from it. Here instead, we just use an – unnecessary, admittedly, given the availability of sample in R – fancy way to select random observations from the training data:\n\n\nnum_inducing_points <- 50\n\nsample_dist <- tfd_uniform(low = 1, high = nrow(train) + 1)\nsample_ids <- sample_dist %>%\n  tfd_sample(num_inducing_points) %>%\n  tf$cast(tf$int32) %>%\n  as.numeric()\nsampled_points <- train[sample_ids, 1:8]\n\n\nOne interesting point to note before we start training: Computation of the posterior predictive parameters involves a Cholesky decomposition, which could fail if, due to numerical issues, the covariance matrix is no longer positive definite. A sufficient action to take in our case is to do all computations using tf$float64:\n\n\nk_set_floatx(\"float64\")\n\n\nNow we define (for real, this time) and run the model.4\n\n\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 8,\n              input_shape = 8,\n              use_bias = FALSE) %>%\n  layer_variational_gaussian_process(\n    num_inducing_points = num_inducing_points,\n    kernel_provider = RBFKernelFn(),\n    event_shape = 1,\n    inducing_index_points_initializer = initializer_constant(as.matrix(sampled_points)),\n    unconstrained_observation_noise_variance_initializer =\n      initializer_constant(array(0.1))\n  )\n\n# KL weight sums to one for one epoch\nkl_weight <- batch_size / nrow(train)\n\n# loss that implements the VGP algorithm\nloss <- function(y, rv_y)\n  rv_y$variational_loss(y, kl_weight = kl_weight)\n\nmodel %>% compile(optimizer = optimizer_adam(lr = 0.008),\n                  loss = loss,\n                  metrics = \"mse\")\n\nhistory <- model %>% fit(train_ds,\n                         epochs = 100,\n                         validation_data = test_ds)\n\nplot(history)\n\n\n\n\n\nInterestingly, higher numbers of inducing points (we tried 100 and 200) did not have much impact on regression performance.5 Nor does the exact choice of multiplication constants (0.1 and 2) applied to the trained kernel Variables (_amplitude and _length_scale)\n\n\ntfp$math$psd_kernels$ExponentiatedQuadratic(\n  amplitude = tf$nn$softplus(array(0.1) * self$`_amplitude`),\n  length_scale = tf$nn$softplus(array(2) * self$`_length_scale`)\n)\n\n\nmake much of a difference to the end result.6\nPredictions\nWe generate predictions on the test set and add them to the data.frame containing the linear models’ predictions.\nAs with other probabilistic output layers, “the predictions” are in fact distributions; to obtain actual tensors we sample from them. Here, we average over 10 samples:\n\n\nyhats <- model(tf$convert_to_tensor(as.matrix(test[, 1:8])))\n\nyhat_samples <-  yhats %>%\n  tfd_sample(10) %>%\n  tf$squeeze() %>%\n  tf$transpose()\n\nsample_means <- yhat_samples %>% apply(1, mean)\n\ncompare <- compare %>%\n  cbind(vgp_preds = sample_means)\n\n\nWe plot the average VGP predictions against the ground truth, together with the predictions from the simple linear model (cyan) and the model including two-way interactions (violet):\n\n\nggplot(compare, aes(x = y_true)) +\n  geom_abline(slope = 1, intercept = 0) +\n  geom_point(aes(y = vgp_preds, color = \"VGP\")) +\n  geom_point(aes(y = linreg_preds1, color = \"simple lm\"), alpha = 0.4) +\n  geom_point(aes(y = linreg_preds2, color = \"lm w/ interactions\"), alpha = 0.4) +\n  scale_colour_manual(\"\", \n                      values = c(\"VGP\" = \"black\", \"simple lm\" = \"cyan\", \"lm w/ interactions\" = \"violet\")) +\n  coord_cartesian(xlim = c(min(compare$y_true), max(compare$y_true)), ylim = c(min(compare$y_true), max(compare$y_true))) +\n  ylab(\"predictions\") +\n  theme(aspect.ratio = 1) \n\n\n\n\n\nFigure 1: Predictions vs. ground truth for linear regression (no interactions; cyan), linear regression with 2-way interactions (violet), and VGP (black).\n\n\n\nAdditionally, comparing MSEs for the three sets of predictions, we see\n\n\nmse <- function(y_true, y_pred) {\n  sum((y_true - y_pred) ^ 2) / length(y_true)\n}\n\nlm_mse1 <- mse(compare$y_true, compare$linreg_preds1) # 117.3111\nlm_mse2 <- mse(compare$y_true, compare$linreg_preds2) # 80.79726\nvgp_mse <- mse(compare$y_true, compare$vgp_preds)     # 58.49689\n\n\nSo, the VGP does in fact outperform both baselines. Something else we might be interested in: How do its predictions vary? Not as much as we might want, were we to construct uncertainty estimates from them alone. Here we plot the 10 samples we drew before:\n\n\nsamples_df <-\n  data.frame(cbind(compare$y_true, as.matrix(yhat_samples))) %>%\n  gather(key = run, value = prediction, -X1) %>% \n  rename(y_true = \"X1\")\n\nggplot(samples_df, aes(y_true, prediction)) +\n  geom_point(aes(color = run),\n             alpha = 0.2,\n             size = 2) +\n  geom_abline(slope = 1, intercept = 0) +\n  theme(legend.position = \"none\") +\n  ylab(\"repeated predictions\") +\n  theme(aspect.ratio = 1)\n\n\n\n\n\nFigure 2: Predictions from 10 consecutive samples from the VGP distribution.\n\n\n\nDiscussion: Feature Relevance\nAs mentioned above, the inverse length scale can be used as an indicator of feature importance. When using the ExponentiatedQuadratic kernel alone, there will only be a single length scale; in our example, the initial dense layer takes of scaling (and additionally, recombining) the features.\nAlternatively, we could wrap the ExponentiatedQuadratic in a FeatureScaled kernel.\nFeatureScaled has an additional scale_diag parameter related to exactly that: feature scaling. Experiments with FeatureScaled (and initial dense layer removed, to be “fair”) showed slightly worse performance, and the learned scale_diag values varied quite a bit from run to run. For that reason, we chose to present the other approach; however, we include the code for a wrapping FeatureScaled in case readers would like to experiment with this:\n\n\nScaledRBFKernelFn <- reticulate::PyClass(\n  \"KernelFn\",\n  inherit = tensorflow::tf$keras$layers$Layer,\n  list(\n    `__init__` = function(self, ...) {\n      kwargs <- list(...)\n      super()$`__init__`(kwargs)\n      dtype <- kwargs[[\"dtype\"]]\n      self$`_amplitude` = self$add_variable(initializer = initializer_zeros(),\n                                            dtype = dtype,\n                                            name = 'amplitude')\n      self$`_length_scale` = self$add_variable(initializer = initializer_zeros(),\n                                               dtype = dtype,\n                                               name = 'length_scale')\n      self$`_scale_diag` = self$add_variable(\n        initializer = initializer_ones(),\n        dtype = dtype,\n        shape = 8L,\n        name = 'scale_diag'\n      )\n      NULL\n    },\n    \n    call = function(self, x, ...) {\n      x\n    },\n    \n    kernel = bt$property(\n      reticulate::py_func(\n        function(self)\n          tfp$math$psd_kernels$FeatureScaled(\n            kernel = tfp$math$psd_kernels$ExponentiatedQuadratic(\n              amplitude = tf$nn$softplus(array(1) * self$`_amplitude`),\n              length_scale = tf$nn$softplus(array(2) * self$`_length_scale`)\n            ),\n            scale_diag = tf$nn$softplus(array(1) * self$`_scale_diag`)\n          )\n      )\n    )\n  )\n)\n\n\nFinally, if all you cared about was prediction performance, you could use FeatureScaled and keep the initial dense layer all the same. But in that case, you’d probably use a neural network – not a Gaussian Process – anyway …\nThanks for reading!\n\n\n\nBreiman, Leo. 2001. “Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).” Statist. Sci. 16 (3): 199–231. https://doi.org/10.1214/ss/1009213726.\n\n\nHensman, James, Nicolo Fusi, and Neil D. Lawrence. 2013. “Gaussian Processes for Big Data.” CoRR abs/1309.6835. http://arxiv.org/abs/1309.6835.\n\n\nMacKay, David J. C. 2002. Information Theory, Inference & Learning Algorithms. New York, NY, USA: Cambridge University Press.\n\n\nNeal, Radford M. 1996. Bayesian Learning for Neural Networks. Berlin, Heidelberg: Springer-Verlag.\n\n\nRasmussen, Carl Edward, and Christopher K. I. Williams. 2005. Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning). The MIT Press.\n\n\nTitsias, Michalis. 2009. “Variational Learning of Inducing Variables in Sparse Gaussian Processes.” In Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics, edited by David van Dyk and Max Welling, 5:567–74. Proceedings of Machine Learning Research. Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA: PMLR. http://proceedings.mlr.press/v5/titsias09a.html.\n\n\nIn the book, it’s “two communities,” not “two cultures”; we choose L. Breiman’s (Breiman 2001) more punchy expression just … because.↩︎\nSo far, we have seen uses of TensorFlow Probability for such different applications as adding uncertainty estimates to neural networks, Bayesian model estimation using Hamiltonian Monte Carlo, or linear-Gaussian state space models.↩︎\nSee David Duvenaud’s https://www.cs.toronto.edu/~duvenaud/cookbook/ for an excellent synopsis of kernels and kernel composition.↩︎\nIn case you’re wondering about the dense layer up front: This will be discussed in the last section on feature relevance.↩︎\nPerformance being used in the sense of “lower regression error,” not running speed. As to the latter, there definitely is a (negative) relationship between number of inducing points and training speed.↩︎\nThis may sound like a matter of course; it isn’t necessarily, as shown by prior experiments with variational layers e.g. in Adding uncertainty estimates to Keras models with tfprobability.↩︎\n",
    "preview": "posts/2019-12-10-variational-gaussian-process/images/kernel_cookbook.png",
    "last_modified": "2024-11-21T15:50:58+00:00",
    "input_file": {},
    "preview_width": 818,
    "preview_height": 352
  },
  {
    "path": "posts/2019-11-27-gettingstarted-2020/",
    "title": "Getting started with Keras from R - the 2020 edition",
    "description": "Looking for materials to get started with deep learning from R? This post presents useful tutorials, guides, and background documentation on the new TensorFlow for R website.  Advanced users will find pointers to applications of new release 2.0 (or upcoming 2.1!) features alluded to in the recent TensorFlow 2.0 post.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-11-27",
    "categories": [
      "Packages/Releases",
      "TensorFlow/Keras"
    ],
    "contents": "\nIf you’ve been thinking about diving into deep learning for a while – using R, preferentially –, now is a good time. For TensorFlow / Keras, one of the predominant deep learning frameworks on the market, last year was a year of substantial changes; for users, this sometimes would mean ambiguity and confusion about the “right” (or: recommended) way to do things. By now, TensorFlow 2.0 has been the current stable release for about two months; the mists have cleared away, and patterns have emerged, enabling leaner, more modular code that accomplishes a lot in just a few lines.\nTo give the new features the space they deserve, and assemble central contributions from related packages all in one place, we have significantly remodeled the TensorFlow for R website. So this post really has two objectives.\nFirst, it would like to do exactly what is suggested by the title: Point new users to resources that make for an effective start into the subject.\nSecond, it could be read as a “best of new website content”. Thus, as an existing user, you might still be interested in giving it a quick skim, checking for pointers to new features that appear in familiar contexts. To make this easier, we’ll add side notes to highlight new features.\nOverall, the structure of what follows is this. We start from the core question: How do you build a model?, then frame it from both sides; i.e.: What comes before? (data loading / preprocessing) and What comes after? (model saving / deployment).\nAfter that, we quickly go into creating models for different types of data: images, text, tabular.\nThen, we touch on where to find background information, such as: How do I add a custom callback? How do I create a custom layer? How can I define my own training loop?\nFinally, we round up with something that looks like a tiny technical addition but has far greater impact: integrating modules from TensorFlow (TF) Hub.\nGetting started\nHow to build a model?\nIf linear regression is the Hello World of machine learning, non-linear regression has to be the Hello World of neural networks. The Basic Regression tutorial shows how to train a dense network on the Boston Housing dataset. This example uses the Keras Functional API, one of the two “classical” model-building approaches – the one that tends to be used when some sort of flexibility is required. In this case, the desire for flexibility comes from the use of feature columns - a nice new addition to TensorFlow that allows for convenient integration of e.g. feature normalization (more about this in the next section).\n\nThe regression tutorial now uses feature columns for convenient data preprocessing.\nThis introduction to regression is complemented by a tutorial on multi-class classification using “Fashion MNIST”. It is equally suited for a first encounter with Keras.\nA third tutorial in this section is dedicated to text classification. Here too, there is a hidden gem in the current version that makes text preprocessing a lot easier: layer_text_vectorization, one of the brand new Keras preprocessing layers.1 If you’ve used Keras for NLP before: No more messing with text_tokenizer!\n\nCheck out the new text vectorization layer in the text classification tutorial.\nThese tutorials are nice introductions explaining code as well as concepts. What if you’re familiar with the basic procedure and just need a quick reminder (or: something to quickly copy-paste from)? The ideal document to consult for those purposes is the Overview.\nNow – knowledge how to build models is fine, but as in data science overall, there is no modeling without data.\nData ingestion and preprocessing\nTwo detailed, end-to-end tutorials show how to load csv data and\nimages, respectively.\nIn current Keras, two mechanisms are central to data preparation. One is the use of tfdatasets pipelines. tfdatasets lets you load data in a streaming fashion (batch-by-batch), optionally applying transformations as you go. The other handy device here is feature specs andfeature columns. Together with a matching Keras layer, these allow for transforming the input data without having to think about what the new format will mean to Keras.\nWhile there are other types of data not discussed in the docs, the principles – pre-processing pipelines and feature extraction – generalize.\nModel saving\nThe best-performing model is of little use if ephemeral. Straightforward ways of saving Keras models are explained in a dedicated tutorial.\n\nAdvanced users: Additional options exist; see the tutorial on checkpoints.\nAnd unless one’s just tinkering around, the question will often be: How can I deploy my model?\nThere is a complete new section on deployment, featuring options like plumber, Shiny, TensorFlow Serving and RStudio Connect.\n\nCheck out the new section on deployment options.\nAfter this workflow-oriented run-through, let’s see about different types of data you might want to model.\nNeural networks for different kinds of data\nNo introduction to deep learning is complete without image classification. The “Fashion MNIST” classification tutorial mentioned in the beginning is a good introduction, but it uses a fully connected neural network to make it easy to remain focused on the overall approach. Standard models for image recognition, however, are commonly based on a convolutional architecture. Here is a nice introductory tutorial.\nFor text data, the concept of embeddings – distributed representations endowed with a measure of similarity – is central. As in the aforementioned text classification tutorial, embeddings can be learned using the respective Keras layer (layer_embedding); in fact, the more idiosyncratic the dataset, the more recommendable this approach. Often though, it makes a lot of sense to use pre-trained embeddings, obtained from large language models trained on enormous amounts of data. With TensorFlow Hub, discussed in more detail in the last section, pre-trained embeddings can be made use of simply by integrating an adequate hub layer, as shown in one of the Hub tutorials.\n\nModels from TF Hub can now conveniently be integrated into a model as Keras layers.\nAs opposed to images and text, “normal”, a.k.a. tabular, a.k.a. structured data often seems like less of a candidate for deep learning. Historically, the mix of data types – numeric, binary, categorical –, together with different handling in the network (“leave alone” or embed) used to require a fair amount of manual fiddling. In contrast, the Structured data tutorial shows the, quote-unquote, modern way, again using feature columns and feature specs. The consequence: If you’re not sure that in the area of tabular data, deep learning will lead to improved performance – if it’s as easy as that, why not give it a try?\n\nIf you’re working with structured data, definitely check out the feature spec way to do it.\nBefore rounding up with a special on TensorFlow Hub, let’s quickly see where to get more information on immediate and background-level technical questions.\nGuides: topic-related and background information\nThe Guide section has lots of additional information, covering specific questions that will come up when coding Keras models\nHow can I define a custom layer?\nA custom model?\nWhat are training callbacks?\nas well as background knowledge and terminology: What are tensors, Variables, how does automatic differentiation work in TensorFlow?\nLike for the basics, above we pointed out a document called “Quickstart”, for advanced topics here too is a Quickstart that in one end-to-end example, shows how to define and train a custom model. One especially nice aspect is the use of tfautograph, a package developed by T. Kalinowski that – among others – allows for concisely iterating over a dataset in a for loop.\n\nPower users: Check out the custom training Quickstart featuring custom models, GradientTapes and tfautograph.\nFinally, let’s talk about TF Hub.\nA special highlight: Hub layers\nOne of the most interesting aspects of contemporary neural network architectures is the use of transfer learning. Not everyone has the data, or computing facilities, to train big networks on big data from scratch. Through transfer learning, existing pre-trained models can be used for similar (but not identical) applications and in similar (but not identical) domains.\nDepending on one’s requirements, building on an existing model could be more or less cumbersome. Some time ago, TensorFlow Hub was created as a mechanism to publicly share models, or modules, that is, reusable building blocks that could be made use of by others.\nUntil recently, there was no convenient way to incorporate these modules, though.\nStarting from TensorFlow 2.0, Hub modules can now seemlessly be integrated in Keras models, using layer_hub. This is demonstrated in two tutorials, for text and images, respectively. But really, these two documents are just starting points: Starting points into a journey of experimentation, with other modules, combination of modules, areas of applications…\n\nDon’t miss out on the new TensorFlow Hub layer available in Keras… potentially, an extremely powerful way to enhance your models.\nIn sum, we hope you have fun with the “new” (TF 2.0) Keras and find the documentation useful.\nThanks for reading!\n\nIn fact, it is so new that as of this writing, you will have to install the nightly build of TensorFlow – as well as tensorflow from github – to use it.↩︎\n",
    "preview": "posts/2019-11-27-gettingstarted-2020/images/website.png",
    "last_modified": "2024-11-21T15:51:07+00:00",
    "input_file": {},
    "preview_width": 1591,
    "preview_height": 725
  },
  {
    "path": "posts/2019-11-13-variational-convnet/",
    "title": "Variational convnets with tfprobability",
    "description": "In a Bayesian neural network, layer weights are distributions, not tensors. Using tfprobability, the R wrapper to TensorFlow Probability, we can build regular Keras models that have probabilistic layers, and thus get uncertainty estimates \"for free\". In this post, we show how to define, train and obtain predictions from a probabilistic convolutional neural network.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-11-13",
    "categories": [
      "Probabilistic ML/DL",
      "Time Series",
      "TensorFlow/Keras"
    ],
    "contents": "\nA bit more than a year ago, in his beautiful guest post, Nick Strayer showed how to classify a set of everyday activities using smartphone-recorded gyroscope and accelerometer data. Accuracy was very good, but Nick went on to inspect classification results more closely. Were there activities more prone to misclassification than others? And how about those erroneous results: Did the network report them with equal, or less confidence than those that were correct?\nTechnically, when we speak of confidence in that manner, we’re referring to the score obtained for the “winning” class after softmax activation.1 If that winning score is 0.9, we might say “the network is sure that’s a gentoo penguin”; if it’s 0.2, we’d instead conclude “to the network, neither option seemed fitting, but cheetah looked best.”\nThis use of “confidence” is convincing, but it has nothing to do with confidence – or credibility, or prediction, what have you – intervals. What we’d really like to be able to do is put distributions over the network’s weights and make it Bayesian. Using tfprobability’s variational Keras-compatible layers, this is something we actually can do.\nAdding uncertainty estimates to Keras models with tfprobability shows how to use a variational dense layer to obtain estimates of epistemic uncertainty. In this post, we modify the convnet used in Nick’s post to be variational throughout. Before we start, let’s quickly summarize the task.\nThe task\nTo create the Smartphone-Based Recognition of Human Activities and Postural Transitions Data Set (Reyes-Ortiz et al. 2016), the researchers had subjects walk, sit, stand, and transition from one of those activities to another. Meanwhile, two types of smartphone sensors were used to record motion data: Accelerometers measure linear acceleration in three2 dimensions, while gyroscopes are used to track angular velocity around the coordinate axes. Here are the respective raw sensor data for six types of activities from Nick’s original post:\n\n\n\nFigure 1: Source: https://blogs.rstudio.com/tensorflow/posts/2018-07-17-activity-detection/\n\n\n\nJust like Nick, we’re going to zoom in on those six types of activity, and try to infer them from the sensor data. Some data wrangling is needed to get the dataset into a form we can work with; here we’ll build on Nick’s post, and effectively start from the data nicely pre-processed and split up into training and test sets:\n\n\ntrainData %>% glimpse()\n\n\nObservations: 289\nVariables: 6\n$ experiment    <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 17, 18, 19, 2…\n$ userId        <int> 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 7, 7, 9, 9, 10, 10, 11…\n$ activity      <int> 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7…\n$ data          <list> [<data.frame[160 x 6]>, <data.frame[206 x 6]>, <dat…\n$ activityName  <fct> STAND_TO_SIT, STAND_TO_SIT, STAND_TO_SIT, STAND_TO_S…\n$ observationId <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 17, 18, 19, 2…\n\n\ntestData %>% glimpse()\n\n\nObservations: 69\nVariables: 6\n$ experiment    <int> 11, 12, 15, 16, 32, 33, 42, 43, 52, 53, 56, 57, 11, …\n$ userId        <int> 6, 6, 8, 8, 16, 16, 21, 21, 26, 26, 28, 28, 6, 6, 8,…\n$ activity      <int> 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8…\n$ data          <list> [<data.frame[185 x 6]>, <data.frame[151 x 6]>, <dat…\n$ activityName  <fct> STAND_TO_SIT, STAND_TO_SIT, STAND_TO_SIT, STAND_TO_S…\n$ observationId <int> 11, 12, 15, 16, 31, 32, 41, 42, 51, 52, 55, 56, 71, …\nThe code required to arrive at this stage (copied from Nick’s post) may be found in the appendix at the bottom of this page.\nTraining pipeline\nThe dataset in question is small enough to fit in memory – but yours might not be, so it can’t hurt to see some streaming in action. Besides, it’s probably safe to say that with TensorFlow 2.0, tfdatasets pipelines are the way to feed data to a model.\nOnce the code listed in the appendix has run, the sensor data is to be found in trainData$data, a list column containing data.frames where each row corresponds to a point in time and each column holds one of the measurements. However, not all time series (recordings) are of the same length; we thus follow the original post to pad all series to length pad_size (= 338). The expected shape of training batches will then be (batch_size, pad_size, 6).\nWe initially create our training dataset:\n\n\ntrain_x <- train_data$data %>% \n  map(as.matrix) %>%\n  pad_sequences(maxlen = pad_size, dtype = \"float32\") %>%\n  tensor_slices_dataset() \n\ntrain_y <- train_data$activity %>% \n  one_hot_classes() %>% \n  tensor_slices_dataset()\n\ntrain_dataset <- zip_datasets(train_x, train_y)\ntrain_dataset\n\n\n<ZipDataset shapes: ((338, 6), (6,)), types: (tf.float64, tf.float64)>\nThen shuffle and batch it:\n\n\nn_train <- nrow(train_data)\n# the highest possible batch size for this dataset\n# chosen because it yielded the best performance\n# alternatively, experiment with e.g. different learning rates, ...\nbatch_size <- n_train\n\ntrain_dataset <- train_dataset %>% \n  dataset_shuffle(n_train) %>%\n  dataset_batch(batch_size)\ntrain_dataset\n\n\n<BatchDataset shapes: ((None, 338, 6), (None, 6)), types: (tf.float64, tf.float64)>\nSame for the test data.\n\n\ntest_x <- test_data$data %>% \n  map(as.matrix) %>%\n  pad_sequences(maxlen = pad_size, dtype = \"float32\") %>%\n  tensor_slices_dataset() \n\ntest_y <- test_data$activity %>% \n  one_hot_classes() %>% \n  tensor_slices_dataset()\n\nn_test <- nrow(test_data)\ntest_dataset <- zip_datasets(test_x, test_y) %>%\n  dataset_batch(n_test)\n\n\nUsing tfdatasets does not mean we cannot run a quick sanity check on our data:\n\n\nfirst <- test_dataset %>% \n  reticulate::as_iterator() %>% \n  # get first batch (= whole test set, in our case)\n  reticulate::iter_next() %>%\n  # predictors only\n  .[[1]] %>% \n  # first item in batch\n  .[1,,]\nfirst\n\n\ntf.Tensor(\n[[ 0.          0.          0.          0.          0.          0.        ]\n [ 0.          0.          0.          0.          0.          0.        ]\n [ 0.          0.          0.          0.          0.          0.        ]\n ...\n [ 1.00416672  0.2375      0.12916666 -0.40225476 -0.20463985 -0.14782938]\n [ 1.04166663  0.26944447  0.12777779 -0.26755899 -0.02779437 -0.1441642 ]\n [ 1.0250001   0.27083334  0.15277778 -0.19639318  0.35094208 -0.16249016]],\n shape=(338, 6), dtype=float64)\nNow let’s build the network.\nA variational convnet\nWe build on the straightforward convolutional architecture from Nick’s post, just making minor modifications to kernel sizes and numbers of filters. We also throw out all dropout layers; no additional regularization is needed on top of the priors applied to the weights.\nNote the following about the “Bayesified” network.\nEach layer is variational in nature, the convolutional ones (layer_conv_1d_flipout) as well as the dense layers (layer_dense_flipout).\nWith variational layers, we can specify the prior weight distribution as well as the form of the posterior; here the defaults are used, resulting in a standard normal prior and a default mean-field posterior.\nLikewise, the user may influence the divergence function used to assess the mismatch between prior and posterior; in this case, we actually take some action: We scale the (default) KL divergence by the number of samples in the training set.\nOne last thing to note is the output layer. It is a distribution layer, that is, a layer wrapping a distribution – where wrapping means: Training the network is business as usual, but predictions are distributions, one for each data point.\n\n\nlibrary(tfprobability)\n\nnum_classes <- 6\n\n# scale the KL divergence by number of training examples\nn <- n_train %>% tf$cast(tf$float32)\nkl_div <- function(q, p, unused)\n  tfd_kl_divergence(q, p) / n\n\nmodel <- keras_model_sequential()\nmodel %>% \n  layer_conv_1d_flipout(\n    filters = 12,\n    kernel_size = 3, \n    activation = \"relu\",\n    kernel_divergence_fn = kl_div\n  ) %>%\n  layer_conv_1d_flipout(\n    filters = 24,\n    kernel_size = 5, \n    activation = \"relu\",\n    kernel_divergence_fn = kl_div\n  ) %>%\n  layer_conv_1d_flipout(\n    filters = 48,\n    kernel_size = 7, \n    activation = \"relu\",\n    kernel_divergence_fn = kl_div\n  ) %>%\n  layer_global_average_pooling_1d() %>% \n  layer_dense_flipout(\n    units = 48,\n    activation = \"relu\",\n    kernel_divergence_fn = kl_div\n  ) %>% \n  layer_dense_flipout(\n    num_classes, \n    kernel_divergence_fn = kl_div,\n    name = \"dense_output\"\n  ) %>%\n  layer_one_hot_categorical(event_size = num_classes)\n\n\nWe tell the network to minimize the negative log likelihood.\n\n\nnll <- function(y, model) - (model %>% tfd_log_prob(y))\n\n\nThis will become part of the loss. The way we set up this example, this is not its most substantial part though. Here, what dominates the loss is the sum of the KL divergences, added (automatically) to model$losses.\nIn a setup like this, it’s interesting to monitor both parts of the loss separately. We can do this by means of two metrics:\n\n\n# the KL part of the loss\nkl_part <-  function(y_true, y_pred) {\n    kl <- tf$reduce_sum(model$losses)\n    kl\n}\n\n# the NLL part\nnll_part <- function(y_true, y_pred) {\n    cat_dist <- tfd_one_hot_categorical(logits = y_pred)\n    nll <- - (cat_dist %>% tfd_log_prob(y_true) %>% tf$reduce_mean())\n    nll\n}\n\n\nWe train somewhat longer than Nick did in the original post, allowing for early stopping though.\n\n\nmodel %>% compile(\n  optimizer = \"rmsprop\",\n  loss = nll,\n  metrics = c(\"accuracy\", \n              custom_metric(\"kl_part\", kl_part),\n              custom_metric(\"nll_part\", nll_part)),\n  experimental_run_tf_function = FALSE\n)\n\ntrain_history <- model %>% fit(\n  train_dataset,\n  epochs = 1000,\n  validation_data = test_dataset,\n  callbacks = list(\n    callback_early_stopping(patience = 10)\n  )\n)\n\n\nWhile the overall loss declines linearly (and probably would for many more epochs), this is not the case for classification accuracy or the NLL part of the loss:\n\n\n\nFinal accuracy is not as high as in the non-variational setup, though still not bad for a six-class problem. We see that without any additional regularization, there is very little overfitting to the training data.\nNow how do we obtain predictions from this model?\nProbabilistic predictions\nThough we won’t go into this here, it’s good to know that we access more than just the output distributions; through their kernel_posterior attribute, we can access the hidden layers’ posterior weight distributions as well.\nGiven the small size of the test set, we compute all predictions at once. The predictions are now categorical distributions, one for each sample in the batch:\n\n\ntest_data_all <- dataset_collect(test_dataset) %>% { .[[1]][[1]]}\n\none_shot_preds <- model(test_data_all) \n\none_shot_preds\n\n\ntfp.distributions.OneHotCategorical(\n \"sequential_one_hot_categorical_OneHotCategorical_OneHotCategorical\",\n batch_shape=[69], event_shape=[6], dtype=float32)\nWe prefixed those predictions with one_shot to indicate their noisy nature: These are predictions obtained on a single pass through the network, all layer weights being sampled from their respective posteriors.\nFrom the predicted distributions, we calculate mean and standard deviation per (test) sample.\n\n\none_shot_means <- tfd_mean(one_shot_preds) %>% \n  as.matrix() %>%\n  as_tibble() %>% \n  mutate(obs = 1:n()) %>% \n  gather(class, mean, -obs) \n\none_shot_sds <- tfd_stddev(one_shot_preds) %>% \n  as.matrix() %>%\n  as_tibble() %>% \n  mutate(obs = 1:n()) %>% \n  gather(class, sd, -obs) \n\n\nThe standard deviations thus obtained could be said to reflect the overall predictive uncertainty. We can estimate another kind of uncertainty, called epistemic,3 by making a number of passes through the network and then, calculating – again, per test sample – the standard deviations of the predicted means.\n\n\nmc_preds <- purrr::map(1:100, function(x) {\n  preds <- model(test_data_all)\n  tfd_mean(preds) %>% as.matrix()\n})\n\nmc_sds <- abind::abind(mc_preds, along = 3) %>% \n  apply(c(1,2), sd) %>% \n  as_tibble() %>%\n  mutate(obs = 1:n()) %>% \n  gather(class, mc_sd, -obs) \n\n\nPutting it all together, we have\n\n\npred_data <- one_shot_means %>%\n  inner_join(one_shot_sds, by = c(\"obs\", \"class\")) %>% \n  inner_join(mc_sds, by = c(\"obs\", \"class\")) %>% \n  right_join(one_hot_to_label, by = \"class\") %>% \n  arrange(obs)\n\npred_data\n\n\n# A tibble: 414 x 6\n     obs class       mean      sd    mc_sd label       \n   <int> <chr>      <dbl>   <dbl>    <dbl> <fct>       \n 1     1 V1    0.945      0.227   0.0743   STAND_TO_SIT\n 2     1 V2    0.0534     0.225   0.0675   SIT_TO_STAND\n 3     1 V3    0.00114    0.0338  0.0346   SIT_TO_LIE  \n 4     1 V4    0.00000238 0.00154 0.000336 LIE_TO_SIT  \n 5     1 V5    0.0000132  0.00363 0.00164  STAND_TO_LIE\n 6     1 V6    0.0000305  0.00553 0.00398  LIE_TO_STAND\n 7     2 V1    0.993      0.0813  0.149    STAND_TO_SIT\n 8     2 V2    0.00153    0.0390  0.102    SIT_TO_STAND\n 9     2 V3    0.00476    0.0688  0.108    SIT_TO_LIE  \n10     2 V4    0.00000172 0.00131 0.000613 LIE_TO_SIT  \n# … with 404 more rows\nComparing predictions to the ground truth:\n\n\neval_table <- pred_data %>% \n  group_by(obs) %>% \n  summarise(\n    maxprob = max(mean),\n    maxprob_sd = sd[mean == maxprob],\n    maxprob_mc_sd = mc_sd[mean == maxprob],\n    predicted = label[mean == maxprob]\n  ) %>% \n  mutate(\n    truth = test_data$activityName,\n    correct = truth == predicted\n  ) \n\neval_table %>% print(n = 20)\n\n\n# A tibble: 69 x 7\n     obs maxprob maxprob_sd maxprob_mc_sd predicted    truth        correct\n   <int>   <dbl>      <dbl>         <dbl> <fct>        <fct>        <lgl>  \n 1     1   0.945     0.227         0.0743 STAND_TO_SIT STAND_TO_SIT TRUE   \n 2     2   0.993     0.0813        0.149  STAND_TO_SIT STAND_TO_SIT TRUE   \n 3     3   0.733     0.443         0.131  STAND_TO_SIT STAND_TO_SIT TRUE   \n 4     4   0.796     0.403         0.138  STAND_TO_SIT STAND_TO_SIT TRUE   \n 5     5   0.843     0.364         0.358  SIT_TO_STAND STAND_TO_SIT FALSE  \n 6     6   0.816     0.387         0.176  SIT_TO_STAND STAND_TO_SIT FALSE  \n 7     7   0.600     0.490         0.370  STAND_TO_SIT STAND_TO_SIT TRUE   \n 8     8   0.941     0.236         0.0851 STAND_TO_SIT STAND_TO_SIT TRUE   \n 9     9   0.853     0.355         0.274  SIT_TO_STAND STAND_TO_SIT FALSE  \n10    10   0.961     0.195         0.195  STAND_TO_SIT STAND_TO_SIT TRUE   \n11    11   0.918     0.275         0.168  STAND_TO_SIT STAND_TO_SIT TRUE   \n12    12   0.957     0.203         0.150  STAND_TO_SIT STAND_TO_SIT TRUE   \n13    13   0.987     0.114         0.188  SIT_TO_STAND SIT_TO_STAND TRUE   \n14    14   0.974     0.160         0.248  SIT_TO_STAND SIT_TO_STAND TRUE   \n15    15   0.996     0.0657        0.0534 SIT_TO_STAND SIT_TO_STAND TRUE   \n16    16   0.886     0.318         0.0868 SIT_TO_STAND SIT_TO_STAND TRUE   \n17    17   0.773     0.419         0.173  SIT_TO_STAND SIT_TO_STAND TRUE   \n18    18   0.998     0.0444        0.222  SIT_TO_STAND SIT_TO_STAND TRUE   \n19    19   0.885     0.319         0.161  SIT_TO_STAND SIT_TO_STAND TRUE   \n20    20   0.930     0.255         0.271  SIT_TO_STAND SIT_TO_STAND TRUE   \n# … with 49 more rows\nAre standard deviations higher for misclassifications?\n\n\neval_table %>% \n  group_by(truth, predicted) %>% \n  summarise(avg_mean = mean(maxprob),\n            avg_sd = mean(maxprob_sd),\n            avg_mc_sd = mean(maxprob_mc_sd)) %>% \n  mutate(correct = truth == predicted) %>%\n  arrange(avg_mc_sd) \n\n\n# A tibble: 2 x 5\n  correct count avg_mean avg_sd avg_mc_sd\n  <lgl>   <int>    <dbl>  <dbl>     <dbl>\n1 FALSE      19    0.775  0.380     0.237\n2 TRUE       50    0.879  0.264     0.183\nThey are; though perhaps not to the extent we might desire.\nWith just six classes, we can also inspect standard deviations on the individual prediction-target pairings level.\n\n\neval_table %>% \n  group_by(truth, predicted) %>% \n  summarise(cnt = n(),\n            avg_mean = mean(maxprob),\n            avg_sd = mean(maxprob_sd),\n            avg_mc_sd = mean(maxprob_mc_sd)) %>% \n  mutate(correct = truth == predicted) %>%\n  arrange(desc(cnt), avg_mc_sd) \n\n\n# A tibble: 14 x 7\n# Groups:   truth [6]\n   truth        predicted      cnt avg_mean avg_sd avg_mc_sd correct\n   <fct>        <fct>        <int>    <dbl>  <dbl>     <dbl> <lgl>  \n 1 SIT_TO_STAND SIT_TO_STAND    12    0.935  0.205    0.184  TRUE   \n 2 STAND_TO_SIT STAND_TO_SIT     9    0.871  0.284    0.162  TRUE   \n 3 LIE_TO_SIT   LIE_TO_SIT       9    0.765  0.377    0.216  TRUE   \n 4 SIT_TO_LIE   SIT_TO_LIE       8    0.908  0.254    0.187  TRUE   \n 5 STAND_TO_LIE STAND_TO_LIE     7    0.956  0.144    0.132  TRUE   \n 6 LIE_TO_STAND LIE_TO_STAND     5    0.809  0.353    0.227  TRUE   \n 7 SIT_TO_LIE   STAND_TO_LIE     4    0.685  0.436    0.233  FALSE  \n 8 LIE_TO_STAND SIT_TO_STAND     4    0.909  0.271    0.282  FALSE  \n 9 STAND_TO_LIE SIT_TO_LIE       3    0.852  0.337    0.238  FALSE  \n10 STAND_TO_SIT SIT_TO_STAND     3    0.837  0.368    0.269  FALSE  \n11 LIE_TO_STAND LIE_TO_SIT       2    0.689  0.454    0.233  FALSE  \n12 LIE_TO_SIT   STAND_TO_SIT     1    0.548  0.498    0.0805 FALSE  \n13 SIT_TO_STAND LIE_TO_STAND     1    0.530  0.499    0.134  FALSE  \n14 LIE_TO_SIT   LIE_TO_STAND     1    0.824  0.381    0.231  FALSE  \nAgain, we see higher standard deviations for wrong predictions, but not to a high degree.\nConclusion\nWe’ve shown how to build, train, and obtain predictions from a fully variational convnet. Evidently, there is room for experimentation: Alternative layer implementations exist4; a different prior could be specified; the divergence could be calculated differently; and the usual neural network hyperparameter tuning options apply.\nThen, there’s the question of consequences (or: decision making). What is going to happen in high-uncertainty cases, what even is a high-uncertainty case? Naturally, questions like these are out-of-scope for this post, yet of essential importance in real-world applications.\nThanks for reading!\nAppendix\nTo be executed before running this post’s code. Copied from Classifying physical activity from smartphone data.\n\n\nlibrary(keras)     \nlibrary(tidyverse) \n\nactivity_labels <- read.table(\"data/activity_labels.txt\", \n                             col.names = c(\"number\", \"label\")) \n\none_hot_to_label <- activity_labels %>% \n  mutate(number = number - 7) %>% \n  filter(number >= 0) %>% \n  mutate(class = paste0(\"V\",number + 1)) %>% \n  select(-number)\n\nlabels <- read.table(\n  \"data/RawData/labels.txt\",\n  col.names = c(\"experiment\", \"userId\", \"activity\", \"startPos\", \"endPos\")\n)\n\ndataFiles <- list.files(\"data/RawData\")\ndataFiles %>% head()\n\nfileInfo <- data_frame(\n  filePath = dataFiles\n) %>%\n  filter(filePath != \"labels.txt\") %>%\n  separate(filePath, sep = '_',\n           into = c(\"type\", \"experiment\", \"userId\"),\n           remove = FALSE) %>%\n  mutate(\n    experiment = str_remove(experiment, \"exp\"),\n    userId = str_remove_all(userId, \"user|\\\\.txt\")\n  ) %>%\n  spread(type, filePath)\n\n# Read contents of single file to a dataframe with accelerometer and gyro data.\nreadInData <- function(experiment, userId){\n  genFilePath = function(type) {\n    paste0(\"data/RawData/\", type, \"_exp\",experiment, \"_user\", userId, \".txt\")\n  }\n  bind_cols(\n    read.table(genFilePath(\"acc\"), col.names = c(\"a_x\", \"a_y\", \"a_z\")),\n    read.table(genFilePath(\"gyro\"), col.names = c(\"g_x\", \"g_y\", \"g_z\"))\n  )\n}\n\n# Function to read a given file and get the observations contained along\n# with their classes.\nloadFileData <- function(curExperiment, curUserId) {\n\n  # load sensor data from file into dataframe\n  allData <- readInData(curExperiment, curUserId)\n  extractObservation <- function(startPos, endPos){\n    allData[startPos:endPos,]\n  }\n\n  # get observation locations in this file from labels dataframe\n  dataLabels <- labels %>%\n    filter(userId == as.integer(curUserId),\n           experiment == as.integer(curExperiment))\n\n  # extract observations as dataframes and save as a column in dataframe.\n  dataLabels %>%\n    mutate(\n      data = map2(startPos, endPos, extractObservation)\n    ) %>%\n    select(-startPos, -endPos)\n}\n\n# scan through all experiment and userId combos and gather data into a dataframe.\nallObservations <- map2_df(fileInfo$experiment, fileInfo$userId, loadFileData) %>%\n  right_join(activityLabels, by = c(\"activity\" = \"number\")) %>%\n  rename(activityName = label)\n\nwrite_rds(allObservations, \"allObservations.rds\")\n\nallObservations <- readRDS(\"allObservations.rds\")\n\ndesiredActivities <- c(\n  \"STAND_TO_SIT\", \"SIT_TO_STAND\", \"SIT_TO_LIE\", \n  \"LIE_TO_SIT\", \"STAND_TO_LIE\", \"LIE_TO_STAND\"  \n)\n\nfilteredObservations <- allObservations %>% \n  filter(activityName %in% desiredActivities) %>% \n  mutate(observationId = 1:n())\n\n# get all users\nuserIds <- allObservations$userId %>% unique()\n\n# randomly choose 24 (80% of 30 individuals) for training\nset.seed(42) # seed for reproducibility\ntrainIds <- sample(userIds, size = 24)\n\n# set the rest of the users to the testing set\ntestIds <- setdiff(userIds,trainIds)\n\n# filter data. \n# note S.K.: renamed to train_data for consistency with \n# variable naming used in this post\ntrain_data <- filteredObservations %>% \n  filter(userId %in% trainIds)\n\n# note S.K.: renamed to test_data for consistency with \n# variable naming used in this post\ntest_data <- filteredObservations %>% \n  filter(userId %in% testIds)\n\n# note S.K.: renamed to pad_size for consistency with \n# variable naming used in this post\npad_size <- trainData$data %>% \n  map_int(nrow) %>% \n  quantile(p = 0.98) %>% \n  ceiling()\n\n# note S.K.: renamed to one_hot_classes for consistency with \n# variable naming used in this post\none_hot_classes <- . %>% \n  {. - 7} %>%        # bring integers down to 0-6 from 7-12\n  to_categorical()   # One-hot encode\n\n\n\n\n\nReyes-Ortiz, Jorge-L., Luca Oneto, Albert Samà, Xavier Parra, and Davide Anguita. 2016. “Transition-Aware Human Activity Recognition Using Smartphones.” Neurocomput. 171 (C): 754–67. https://doi.org/10.1016/j.neucom.2015.07.085.\n\n\nsee Winner takes all: A look at activations and cost functions↩︎\nor two, depending on the application↩︎\nsee Adding uncertainty estimates to Keras models with tfprobability↩︎\ne.g., layer_conv_1d_reparameterization, layer_dense_local_reparameterization↩︎\n",
    "preview": "posts/2019-11-13-variational-convnet/images/bbb.png",
    "last_modified": "2024-11-21T15:53:20+00:00",
    "input_file": {},
    "preview_width": 796,
    "preview_height": 378
  },
  {
    "path": "posts/2019-11-07-tfp-cran/",
    "title": "tfprobability 0.8 on CRAN: Now how can you use it?",
    "description": "Part of the r-tensorflow ecosystem, tfprobability is an R wrapper to TensorFlow Probability, the Python probabilistic programming framework developed by Google. We take the occasion of tfprobability's acceptance on CRAN to give a high-level introduction, highlighting interesting use cases and applications.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-11-07",
    "categories": [
      "Probabilistic ML/DL",
      "Packages/Releases",
      "TensorFlow/Keras"
    ],
    "contents": "\nAbout a week ago, tfprobability 0.8 was accepted on CRAN. While we’ve been using this package quite frequently already on this blog, in this post we’d like to (re-)introduce it on a high level, especially addressing new users.\ntfprobability, what is it?\ntfprobability is an R wrapper for TensorFlow Probability, a Python library built on top of the TensorFlow framework. So now the question is, what is TensorFlow Probability?\nIf – let’s call it “probabilistic programming” – is not something you do every day, an enumeration of features, or even a hierarchical listing of modules as given on the TensorFlow Probability website might leave you a bit helpless, informative though it may be.\nLet’s start from use cases instead. We’ll look at three high-level example scenarios, before rounding up with a quick tour of more basic building blocks provided by TFP. (Short note aside: We’ll use TFP as an acronym for the Python library as well as the R package, unless we’re referring to the R wrapper specifically, in which case we’ll say tfprobability).\nUse case 1: Extending deep learning\nWe start with the type of use case that might be the most interesting to the majority of our readers: extending deep learning.\nDistribution layers\nIn deep learning, usually output layers are deterministic. True, in classification we are used to talking about “class probabilities”. Take the multi-class case: We may attribute to the network the conclusion, “with 80% probability this is a Bernese mountain dog” – but we can only do this because the last layer’s output has been squished, by a softmax activation, to values between \\(0\\) and \\(1\\). Nonetheless, the actual output is a tensor (a number).\nTFP, however, augments TensorFlow by means of distribution layers: a hybrid species that can be used just like a normal TensorFlow/Keras layer but that, internally, contains the defining characteristics of some probability distribution.\nConcretely, for multi-class classification, we could use a categorical layer (layer_one_hot_categorical), replacing something like\n\n\nlayer_dense(\n  num_classes, \n  activation = \"softmax\"\n)\n\n\nby\n\n\nlayer_one_hot_categorical(event_size = num_classes)\n\n\nThe model, thus modified, now outputs a distribution, not a tensor. However, it can still be trained passing “normal” target tensors. This is what was meant by use like a normal layer, above: TFP will take the distribution, obtain a tensor from it 1, and compare that to the target. The other side of the layer’s personality is seen when generating predictions: Calling the model on fresh data will result in a bunch of distributions, one for every data point. You then call tfd_mean to elicit actual predictions:\n\n\npred_dists <- model(x_test)\npred_means <- pred_dists %>% tfd_mean()\n\n\nYou may be wondering, what good is this? One way or the other, we’ll decide on picking the class with the highest probability, right?\nRight, but there are a number of interesting things you can do with these layers.\nWe’ll quickly introduce three well-known ones here, but wouldn’t be surprised if we saw a lot more emerging in the near future.\nVariational autoencoders, the elegant way\nVariational autoencoders are a prime example of something that got way easier to code when TF-2 style custom models and custom training appeared (TF-2 style, not TF-2, as these techniques actually became available more than a year before TF 2 was finally released). 2\nEvolutionarily, the next step was to use TFP distributions 3, but back in the time some fiddling was required, as distributions could not yet alias as layers.\nDue to those hybrids though, we now can do something like this 4:\n\n# encoder\nencoder_model <- keras_model_sequential() %>%\n  [...] %>%\n  layer_multivariate_normal_tri_l(event_size = encoded_size) %>%\n  layer_kl_divergence_add_loss([...])\n\n# decoder\ndecoder_model <- keras_model_sequential() %>%\n  [...] %>%\n layer_independent_bernoulli([...])\n\n# complete VAE\nvae_model <- keras_model(inputs = encoder_model$inputs,\n                         outputs = decoder_model(encoder_model$outputs[1]))\n\n# loss function\nvae_loss <- function (x, rv_x) - (rv_x %>% tfd_log_prob(x))\n\nBoth the encoder and the decoder are “just” sequential models, joined through the functional API(keras_model). The loss is just the negative log-probability of the data given the model. So where is the other part of the (negative) ELBO, the KL divergence? It is implicit in the encoder’s output layer, layer_kl_divergence_add_loss.\nOur two other examples involve quantifying uncertainty.\nLearning the spread in the data\nIf a model’s last layer wraps a distribution parameterized by location and scale, like the normal distribution, we can train the network to learn not just the mean, but also the spread in the data:\n\n\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 8, activation = \"relu\") %>%\n  layer_dense(units = 2, activation = \"linear\") %>%\n  layer_distribution_lambda(function(x)\n    tfd_normal(# use unit 1 of previous layer\n               loc = x[, 1, drop = FALSE],\n               # use unit 2 of previous layer\n               scale = 1e-3 + tf$math$softplus(x[, 2, drop = FALSE])\n               )\n  )\n\n\nIn essence, this means the network’s predictions will reflect any existing heteroscedasticity in the data. Here is an example: Given simulated training data of shape\n\n\n\nFigure 1: Simulated data (from https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/)\n\n\n\nthe network’s predictions show the same spread:\n\n\n\nFigure 2: Aleatoric uncertainty on simulated data (from https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/)\n\n\n\nPlease see Adding uncertainty estimates to Keras models with tfprobability for a detailed explanation.\nUsing a normal distribution layer as the output, we can capture irreducible variability in the data, also known as aleatoric uncertainty. A different type of probabilistic layer allows to model what is called epistemic uncertainty.\nPutting distributions over network weights\nUsing variational layers, we can make neural networks probabilistic. A simple example could look like so:\n\n\nmodel <- keras_model_sequential() %>%\n  layer_dense_variational(\n    units = 1,\n    make_posterior_fn = posterior_mean_field,\n    make_prior_fn = prior_trainable,\n    kl_weight = 1 / n\n  ) %>%\n  layer_distribution_lambda(function(x)\n    tfd_normal(loc = x, scale = 1))\n\n\nThis defines a network with a single dense layer, containing a single neuron, that has a prior distribution put over its weights. The network will be trained to minimize the KL divergence between that prior and an approximate posterior weight distribution, as well as maximize the probability of the data under the posterior weights. (For details, please again see the aforementioned post.)\nAs a consequence of this setup, each test run will now yield different predictions. For the above simulated data, we might get an ensemble of predictions, like so:\n\n\n\nFigure 3: Epistemic uncertainty on simulated data (from https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/)\n\n\n\nVariational layers for non-dense layers exist, and we’ll see an example next week. Now let’s move on to the next type of use cases, from big data to small data, in a way.\nUse case 2: Fitting Bayesian models with Monte Carlo methods\nIn sciences where data aren’t abound, Markov Chain Monte Carlo (MCMC) methods are common. We’ve shown some examples how to this with TFP (Tadpoles on TensorFlow: Hierarchical partial pooling with tfprobability, Hierarchical partial pooling, continued: Varying slopes models with TensorFlow Probability, Modeling censored data with tfprobability, best read in this order), as well as tried to explain, in an accessible way, some of the background (On leapfrogs, crashing satellites, and going nuts: A very first conceptual introduction to Hamiltonian Monte Carlo).\nMCMC software may roughly be divided into two flavors: “low-level” and “high-level”. Low-level software, like Stan – or TFP, for that matter – requires you to write code in either some programming language, or in a DSL that is pretty close in syntax and semantics to an existing programming language. High-level tools, on the other hand, are DSLs that resemble the way you’d express a model using mathematical notation. (Put differently, the former read like C or Python; the latter read like LaTeX.)\nIn general, low-level software tends to offer more flexibility, while high-level interfaces may be more convenient to use and easier to learn. To start with MCMC in TFP, we recommend checking out the first of the posts listed above, Tadpoles on TensorFlow: Hierarchical partial pooling with tfprobability. If you prefer a higher-level interface, you might be interested in greta, which is built on TFP.\nOur last use case is of a Bayesian nature as well.\nUse case 3: State space models\nState space models are a perhaps lesser used, but highly conceptually attractive way of performing inference and prediction on signals evolving in time. Dynamic linear models with tfprobability is an introduction to dynamic linear models with TFP, showcasing two of their (many) great strengths: ease of performing dynamic regression and additive (de)composition.\nIn dynamic regression, coefficients are allowed to vary over time. Here is an example from the above-mentioned post showing, for both a single predictor and the regression intercept, the filtered (in the sense of Kálmán filtering) estimates over time:\n\n\n\nFigure 4: Filtering estimates from the Kálmán filter (from: https://blogs.rstudio.com/tensorflow/posts/2019-06-25-dynamic_linear_models_tfprobability/).\n\n\n\nAnd here is the ubiquitous AirPassengers dataset, decomposed into a trend and a seasonal component:\n\n\n\nFigure 5: AirPassengers, decomposition into a linear trend and a seasonal component (from: https://blogs.rstudio.com/tensorflow/posts/2019-06-25-dynamic_linear_models_tfprobability/).\n\n\n\nIf this interests you, you might want to take a look at available state space models. Given how rapidly TFP is evolving, plus the model-inherent composability, we expect the number of options in this area to grow quite a bit.\nThat is it for our tour of use cases. To wrap up, let’s talk about the basic building blocks of TFP.\nThe basics: Distributions and bijectors\nNo probabilistic framework without probability distributions – that’s for sure. In release 0.8, TFP has about 80 distributions. But what are bijectors?\nBijectors are invertible, differentiable maps. Getting into the flow: Bijectors in TensorFlow Probability introduces the main ideas and shows how to chain such bijective transformations into a flow. Bijectors are being used by TFP internally all the time, and as a user too you’ll likely encounter situations where you need them.\nOne example is doing MCMC (for example, Hamiltonian Monte Carlo) with TFP. In your model, you might have a prior on a standard deviation. Standard deviations are positive, so you’d like to specify, for example, an exponential distribution for it, resulting in exclusively positive values. But Hamiltonian Monte Carlo has to run in unconstrained space to work. This is where a bijector comes in, mapping between the two spaces used for model specification and sampling.\nFor bijectors too, there are many of them - about 40, ranging from straightforward affine maps to more complex operations on Cholesky factors or the discrete cosine transform.\nTo see both building blocks in action, let’s end with a “Hello World” of TFP, or two, rather.\nHere first is the direct way to obtain samples from a standard normal distribution.\n\n\nlibrary(tfprobability)\n\n# create a normal distribution\nd <- tfd_normal(loc = 0, scale = 1)\n# sample from it\nd %>% tfd_sample(3)\n\n\ntf.Tensor([-1.0863057  -0.61655647  1.8151687 ], shape=(3,), dtype=float32)\nIn case you thought that was too easy, here’s how to do the same using a bijector instead of a distribution.\n\n\n# generate 3 values uniformly distributed between 0 and 1\nu <- runif(3)\n\n# a bijector that in the inverse direction, maps values between 0 and 1\n# to a normal distribution\nb <- tfb_normal_cdf()\n\n# call bijector's inverse transform op\nb %>% tfb_inverse(u) \n\n\ntf.Tensor([ 0.96157753  1.0103974  -1.4986734 ], shape=(3,), dtype=float32)\nWith this we conclude our introduction. If you run into problems using tfprobability, or have questions about it, please open an issue in the github repo.\nThanks for reading!\n\nBy default, just sampling from the distribution – but this is something the user can influence if desired, making use of the convert_to_tensor_fn argument.↩︎\nSee Representation learning with MMD-VAE for an example.↩︎\nDone so, for example, in Getting started with TensorFlow Probability from R.↩︎\nFor a complete running example, see the tfprobability README.↩︎\n",
    "preview": "posts/2019-11-07-tfp-cran/images/tfprobability.png",
    "last_modified": "2024-11-21T15:52:08+00:00",
    "input_file": {},
    "preview_width": 518,
    "preview_height": 600
  },
  {
    "path": "posts/2019-10-23-gpt-2/",
    "title": "Innocent unicorns considered harmful? How to experiment with GPT-2 from R",
    "description": "Is society ready to deal with challenges brought about by artificially-generated  information - fake images, fake videos, fake text? While this post won't answer that question, it should help form an opinion on the threat exerted by fake text as of this writing, autumn 2019.  We introduce gpt2, an R package that wraps OpenAI's public implementation of GPT-2, the language model that early this year surprised the NLP community with the unprecedented quality of its creations.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      },
      {
        "name": "Javier Luraschi",
        "url": {}
      }
    ],
    "date": "2019-10-23",
    "categories": [
      "Natural Language Processing",
      "Packages/Releases"
    ],
    "contents": "\nWhen this year in February, OpenAI presented GPT-2(Radford et al. 2019), a large Transformer-based language model trained on an enormous amount of web-scraped text, their announcement caught great attention, not just in the NLP community. This was primarily due to two facts. First, the samples of generated text were stunning.\nPresented with the following input\n\nIn a shocking finding, scientist [sic] discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\nthis was how the model continued:\n\nThe scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science.\nNow, after almost two centuries, the mystery of what sparked this odd phenomenon is finally solved.\nDr. Jorge Pérez, an evolutionary biologist from the University of La Paz, and several companions, were exploring the Andes Mountains when they found a small valley, with no other animals or humans. Pérez noticed that the valley had what appeared to be a natural fountain, surrounded by two peaks of rock and silver snow. […]\n\nSecond, “due to our concerns about malicious applications” (quote) they didn’t release the full model, but a smaller one that has less than one tenth the number of parameters. Neither did they make public the dataset, nor the training code.\nWhile at first glance, this may look like a marketing move (we created something so powerful that it’s too dangerous to be released to the public!), let’s not make things that easy on ourselves.\nWith great power …\nWhatever your take on the “innate priors in deep learning” discussion – how much knowledge needs to be hardwired into neural networks for them to solve tasks that involve more than pattern matching? – there is no doubt that in many areas, systems driven by “AI”1 will impact\nour lives in an essential, and ever more powerful, way. Although there may be some awareness of the ethical, legal, and political problems this poses, it is probably fair to say that by and large, society is closing its eyes and holding its hands over its ears.\nIf you were a deep learning researcher working in an area susceptible to abuse, generative ML say, what options would you have? As always in the history of science, what can be done will be done; all that remains is the search for antidotes. You may doubt that on a political level, constructive responses could evolve. But you can encourage other researchers to scrutinize the artifacts your algorithm created and develop other algorithms designed to spot the fakes – essentially like in malware detection. Of course this is a feedback system: Like with GANs, impostor algorithms will happily take the feedback and go on working on their shortcomings. But still, deliberately entering this circle might be the only viable action to take.\nAlthough it may be the first thing that comes to mind, the question of veracity here isn’t the only one. With ML systems, it’s always: garbage in - garbage out. What is fed as training data determines the quality of the output, and any biases in its upbringing will carry through to an algorithm’s grown-up behavior. Without interventions, software designed to do translation, autocompletion and the like will be biased.2\nIn this light, all we can sensibly do is – constantly – point out the biases, analyze the artifacts, and conduct adversarial attacks. These are the kinds of responses OpenAI was asking for. In appropriate modesty, they called their approach an experiment. Put plainly, no-one today knows how to deal with the threats emerging from powerful AI appearing in our lives. But there is no way around exploring our options.\nThe story unwinding\nThree months later, OpenAI published an update to the initial post, stating that they had decided on a staged-release strategy. In addition to making public the next-in-size, 355M-parameters version of the model, they also released a dataset of generated outputs from all model sizes, to facilitate research. Last not least, they announced partnerships with academic and non-academic institutions, to increase “societal preparedness” (quote).\nAgain after three months, in a new post OpenAI announced the release of a yet larger – 774M-parameter – version of the model. At the same time, they reported evidence demonstrating insufficiencies in current statistical fake detection, as well as study results suggesting that indeed, text generators exist that can trick humans.\nDue to those results, they said, no decision had yet been taken as to the release of the biggest, the “real” model, of size 1.5 billion parameters.\nGPT-2\nSo what is GPT-2? Among state-of-the-art NLP models, GPT-2 stands out due to the gigantic (40G) dataset it was trained on, as well as its enormous number of weights. The architecture, in contrast, wasn’t new when it appeared. GPT-2, as well as its predecessor GPT (Radford 2018), is based on a transformer architecture.\nThe original Transformer (Vaswani et al. 2017) is an encoder-decoder architecture designed for sequence-to-sequence tasks, like machine translation. The paper introducing it was called “Attention is all you need,” emphasizing – by absence – what you don’t need: RNNs.\nBefore its publication, the prototypical model for e.g. machine translation would use some form of RNN as an encoder, some form of RNN as a decoder, and an attention mechanism that at each time step of output generation, told the decoder where in the encoded input to look. Now the transformer was disposing with RNNs, essentially replacing them by a mechanism called self-attention where already during encoding, the encoder stack would encode each token not independently, but as a weighted sum of tokens encountered before (including itself).3\nMany subsequent NLP models built on the Transformer, but – depending on purpose – either picked up the encoder stack only, or just the decoder stack.\nGPT-2 was trained to predict consecutive words in a sequence. It is thus a language model, a term resounding the conception that an algorithm which can predict future words and sentences somehow has to understand language (and a lot more, we might add).\nAs there is no input to be encoded (apart from an optional one-time prompt), all that is needed is the stack of decoders.\nIn our experiments, we’ll be using the biggest as-yet released pretrained model, but this being a pretrained model our degrees of freedom are limited. We can, of course, condition on different input prompts. In addition, we can influence the sampling algorithm used.\nSampling options with GPT-2\nWhenever a new token is to be predicted, a softmax is taken over the vocabulary.4 Directly taking the softmax output amounts to maximum likelihood estimation. In reality, however, always choosing the maximum likelihood estimate results in highly repetitive output.\nA natural option seems to be using the softmax outputs as probabilities: Instead of just taking the argmax, we sample from the output distribution. Unfortunately, this procedure has negative ramifications of its own. In a big vocabulary, very improbable words together make up a substantial part of the probability mass; at every step of generation, there is thus a non-negligible probability that an improbable word may be chosen. This word will now exert great influence on what is chosen next. In that manner, highly improbable sequences can build up.\nThe task thus is to navigate between the Scylla of determinism and the Charybdis of weirdness. With the GPT-2 model presented below, we have three options:\nvary the temperature (parameter temperature);\nvary top_k, the number of tokens considered; or\nvary top_p, the probability mass considered.\nThe temperature concept is rooted in statistical mechanics. Looking at the Boltzmann distribution used to model state probabilities \\(p_i\\)dependent on energy \\(\\epsilon_i\\):\n\\[p_i \\sim e^{-\\frac{\\epsilon_i}{kT}}\\]\nwe see there is a moderating variable temperature \\(T\\)5 that dependent on whether it’s below or above 1, will exert an either amplifying or attenuating influence on differences between probabilities.\nAnalogously, in the context of predicting the next token, the individual logits are scaled by the temperature, and only then is the softmax taken. Temperatures below zero would make the model even more rigorous in choosing the maximum likelihood candidate; instead, we’d be interested in experimenting with temperatures above 1 to give higher chances to less likely candidates – hopefully, resulting in more human-like text.\nIn top-\\(k\\) sampling, the softmax outputs are sorted, and only the top-\\(k\\) tokens are considered for sampling. The difficulty here is how to choose \\(k\\). Sometimes a few words make up for almost all probability mass, in which case we’d like to choose a low number; in other cases the distribution is flat, and a higher number would be adequate.\nThis sounds like rather than the number of candidates, a target probability mass should be specified. This is the approach suggested by (Holtzman et al. 2019). Their method, called top-\\(p\\), or Nucleus sampling, computes the cumulative distribution of softmax outputs and picks a cut-off point \\(p\\). Only the tokens constituting the top-\\(p\\) portion of probability mass is retained for sampling.\nNow all you need to experiment with GPT-2 is the model.\nSetup\nInstall gpt2 from github:\n\n\nremotes::install_github(\"r-tensorflow/gpt2\")\n\n\nThe R package being a wrapper to the implementation provided by OpenAI, we then need to install the Python runtime.\n\n\ngpt2::install_gpt2(envname = \"r-gpt2\")\n\n\nThis command will also install TensorFlow into the designated environment. All TensorFlow-related installation options (resp. recommendations) apply. Python 3 is required.\nWhile OpenAI indicates a dependency on TensorFlow 1.12, the R package was adapted to work with more current versions. The following versions have been found to be working fine:\nif running on GPU: TF 1.15\nCPU-only: TF 2.0\nUnsurprisingly, with GPT-2, running on GPU vs. CPU makes a huge difference.\nAs a quick test if installation was successful, just run gpt2() with the default parameters:\n\n\n# equivalent to:\n# gpt2(prompt = \"Hello my name is\", model = \"124M\", seed = NULL, batch_size = 1, total_tokens = NULL,\n#      temperature = 1, top_k = 0, top_p = 1)\n# see ?gpt2 for an explanation of the parameters\n#\n# available models as of this writing: 124M, 355M, 774M\n#\n# on first run of a given model, allow time for download\ngpt2()\n\n\nThings to try out\nSo how dangerous exactly is GPT-2? We can’t say, as we don’t have access to the “real” model. But we can compare outputs, given the same prompt, obtained from all available models. The number of parameters has approximately doubled at every release – 124M, 355M, 774M. The biggest, yet unreleased, model, again has twice the number of weights: about 1.5B. In light of the evolution we observe, what do we expect to get from the 1.5B version?\nIn performing these kinds of experiments, don’t forget about the different sampling strategies explained above. Non-default parameters might yield more real-looking results.\nNeedless to say, the prompt we specify will make a difference. The models have been trained on a web-scraped dataset, subject to the quality criterion “3 stars on reddit”. We expect more fluency in certain areas than in others, to put it in a cautious way.\nMost definitely, we expect various biases in the outputs.\nUndoubtedly, by now the reader will have her own ideas about what to test. But there is more.\n“Language Models are Unsupervised Multitask Learners”\nHere we are citing the title of the official GPT-2 paper (Radford et al. 2019). What is that supposed to mean? It means that a model like GPT-2, trained to predict the next token in naturally occurring text, can be used to “solve” standard NLP tasks that, in the majority of cases, are approached via supervised training (translation, for example).\nThe clever idea is to present the model with cues about the task at hand. Some information on how to do this is given in the paper; more (unofficial; conflicting or confirming) hints can be found on the net.\nFrom what we found, here are some things you could try.\nSummarization\nThe clue to induce summarization is “TL;DR:” written on a line by itself. The authors report that this worked best setting top_k = 2 and asking for 100 tokens. Of the generated output, they took the first three sentences as a summary.\nTo try this out, we chose a sequence of content-wise standalone paragraphs from a NASA website dedicated to climate change, the idea being that with a clearly structured text like this, it should be easier to establish relationships between input and output.\n# put this in a variable called text\n\nThe planet's average surface temperature has risen about 1.62 degrees Fahrenheit\n(0.9 degrees Celsius) since the late 19th century, a change driven largely by\nincreased carbon dioxide and other human-made emissions into the atmosphere.4 Most\nof the warming occurred in the past 35 years, with the five warmest years on record\ntaking place since 2010. Not only was 2016 the warmest year on record, but eight of\nthe 12 months that make up the year — from January through September, with the\nexception of June — were the warmest on record for those respective months.\n\nThe oceans have absorbed much of this increased heat, with the top 700 meters\n(about 2,300 feet) of ocean showing warming of more than 0.4 degrees Fahrenheit\nsince 1969.\n\nThe Greenland and Antarctic ice sheets have decreased in mass. Data from NASA's\nGravity Recovery and Climate Experiment show Greenland lost an average of 286\nbillion tons of ice per year between 1993 and 2016, while Antarctica lost about 127\nbillion tons of ice per year during the same time period. The rate of Antarctica\nice mass loss has tripled in the last decade.\n\nGlaciers are retreating almost everywhere around the world — including in the Alps,\nHimalayas, Andes, Rockies, Alaska and Africa.\n\nSatellite observations reveal that the amount of spring snow cover in the Northern\nHemisphere has decreased over the past five decades and that the snow is melting\nearlier.\n\nGlobal sea level rose about 8 inches in the last century. The rate in the last two\ndecades, however, is nearly double that of the last century and is accelerating\nslightly every year.\n\nBoth the extent and thickness of Arctic sea ice has declined rapidly over the last\nseveral decades.\n\nThe number of record high temperature events in the United States has been\nincreasing, while the number of record low temperature events has been decreasing,\nsince 1950. The U.S. has also witnessed increasing numbers of intense rainfall events.\n\nSince the beginning of the Industrial Revolution, the acidity of surface ocean\nwaters has increased by about 30 percent.13,14 This increase is the result of humans\nemitting more carbon dioxide into the atmosphere and hence more being absorbed into\nthe oceans. The amount of carbon dioxide absorbed by the upper layer of the oceans\nis increasing by about 2 billion tons per year.\n\nTL;DR:\n\n\ngpt2(prompt = text,\n     model = \"774M\",\n     total_tokens = 100,\n     top_k = 2)\n\n\nHere is the generated result, whose quality on purpose we don’t comment on. (Of course one can’t help having “gut reactions”; but to actually present an evaluation we’d want to conduct a systematic experiment, varying not only input prompts but also function parameters. All we want to show in this post is how you can set up such experiments yourself.)\n\"\\nGlobal temperatures are rising, but the rate of warming has been accelerating.\n\\n\\nThe oceans have absorbed much of the increased heat, with the top 700 meters of\nocean showing warming of more than 0.4 degrees Fahrenheit since 1969.\n\\n\\nGlaciers are retreating almost everywhere around the world, including in the\nAlps, Himalayas, Andes, Rockies, Alaska and Africa.\n\\n\\nSatellite observations reveal that the amount of spring snow cover in the\nNorthern Hemisphere has decreased over the past\"\nSpeaking of parameters to vary, – they fall into two classes, in a way. It is unproblematic to vary the sampling strategy, let alone the prompt. But for tasks like summarization, or the ones we’ll see below, it doesn’t feel right to have to tell the model how many tokens to generate. Finding the right length of the answer seems to be part of the task.6 Breaking our “we don’t judge” rule just a single time, we can’t help but remark that even in less clear-cut tasks, language generation models that are meant to approach human-level competence would have to fulfill a criterion of relevance (Grice 1975).\nQuestion answering\nTo trick GPT-2 into question answering, the common approach seems to be presenting it with a number of Q: / A: pairs, followed by a final question and a final A: on its own line.\nWe tried like this, asking questions on the above climate change - related text:\n\n\nq <- str_c(str_replace(text, \"\\nTL;DR:\\n\", \"\"), \" \\n\", \"\nQ: What time period has seen the greatest increase in global temperature? \nA: The last 35 years. \nQ: What is happening to the Greenland and Antarctic ice sheets? \nA: They are rapidly decreasing in mass. \nQ: What is happening to glaciers? \nA: \")\n\ngpt2(prompt = q,\n     model = \"774M\",\n     total_tokens = 10,\n     top_p = 0.9)\n\n\nThis did not turn out so well.\n\"\\nQ: What is happening to the Arctic sea\"\nBut maybe, more successful tricks exist.\nTranslation\nFor translation, the strategy presented in the paper is juxtaposing sentences in two languages, joined by ” = “, followed by a single sentence on its own and a” =“.\nThinking that English <-> French might be the combination best represented in the training corpus, we tried the following:\n# save this as eng_fr\n\nThe issue of climate change concerns all of us. = La question du changement\nclimatique nous affecte tous. \\n\nThe problems of climate change and global warming affect all of humanity, as well as\nthe entire ecosystem. = Les problèmes créés par les changements climatiques et le\nréchauffement de la planète touchent toute l'humanité, de même que l'écosystème tout\nentier.\\n\nClimate Change Central is a not-for-profit corporation in Alberta, and its mandate\nis to reduce Alberta's greenhouse gas emissions. = Climate Change Central est une\nsociété sans but lucratif de l'Alberta ayant pour mission de réduire les émissions\nde gaz. \\n\nClimate change will affect all four dimensions of food security: food availability,\nfood accessibility, food utilization and food systems stability. = \"\n\ngpt2(prompt = eng_fr,\n     model = \"774M\",\n     total_tokens = 25,\n     top_p = 0.9)\nResults varied a lot between different runs. Here are three examples:\n\"ét durant les pages relevantes du Centre d'Action des Sciences Humaines et dans sa\nspecies situé,\"\n\n\"études des loi d'affaires, des reasons de demande, des loi d'abord and de\"\n\n\"étiquettes par les changements changements changements et les bois d'escalier,\nainsi que des\"\nConclusion\nWith that, we conclude our tour of “what to explore with GPT-2.” Keep in mind that the yet-unreleased model has double the number of parameters; essentially, what we see is not what we get.\nThis post’s goal was to show how you can experiment with GPT-2 from R. But it also reflects the decision to, from time to time, widen the narrow focus on technology and allow ourselves to think about ethical and societal implications of ML/DL.\nThanks for reading!\n\n\n\nGrice, H. P. 1975. “Logic and Conversation.” In Syntax and Semantics: Vol. 3: Speech Acts, 41–58. Academic Press. http://www.ucl.ac.uk/ls/studypacks/Grice-Logic.pdf.\n\n\nHoltzman, Ari, Jan Buys, Maxwell Forbes, and Yejin Choi. 2019. “The Curious Case of Neural Text Degeneration.” arXiv e-Prints, April, arXiv:1904.09751. https://arxiv.org/abs/1904.09751.\n\n\nRadford, Alec. 2018. “Improving Language Understanding by Generative Pre-Training.” In.\n\n\nRadford, Alec, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. “Language Models Are Unsupervised Multitask Learners.”\n\n\nSun, Tony, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth M. Belding, Kai-Wei Chang, and William Yang Wang. 2019. “Mitigating Gender Bias in Natural Language Processing: Literature Review.” CoRR abs/1906.08976. http://arxiv.org/abs/1906.08976.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In Advances in Neural Information Processing Systems 30, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 5998–6008. Curran Associates, Inc. http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\n\n\nThe acronym here is used for convenience only, not to imply any specific view on what is, or is not, “artificial intelligence.”↩︎\nFor an overview of bias detection and mitigation specific to gender bias, see e.g. (Sun et al. 2019)↩︎\nFor a detailed, and exceptionally visual, explanation of the Transformer, the place to go is Jay Alammar’s post. Also check out The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning, the article that might be held mainly responsible for the pervasive sesame-streetification of NLP.↩︎\nFor an introduction to how softmax activation behaves, see Winner takes all: A look at activations and cost functions.↩︎\n\\(k\\) is the Boltzmann constant↩︎\nFormally, total_tokens isn’t a required parameter. If not passed, a default based on model size will be applied, resulting in lengthy output that definitely will have to be processed by some human-made rule.↩︎\n",
    "preview": "posts/2019-10-23-gpt-2/images/thumb.jpg",
    "last_modified": "2024-11-21T15:50:49+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-10-08-tf2-whatchanges/",
    "title": "TensorFlow 2.0 is here - what changes for R users?",
    "description": "TensorFlow 2.0 was finally released last week. As R users we have two kinds of questions. First, will my keras code still run? And second, what is it that changes? In this post, we answer both and, then, give a tour of exciting new developments in the r-tensorflow ecosystem.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-10-08",
    "categories": [
      "TensorFlow/Keras",
      "Packages/Releases"
    ],
    "contents": "\nThe wait is over – TensorFlow 2.0 (TF 2) is now officially here! What does this mean for us, users of R packages keras and/or tensorflow, which, as we know, rely on the Python TensorFlow backend?\nBefore we go into details and explanations, here is an all-clear, for the concerned user who fears their keras code might become obsolete (it won’t).\nDon’t panic\nIf you are using keras in standard ways, such as those depicted in most code examples and tutorials seen on the web, and things have been working fine for you in recent keras releases (>= 2.2.4.1), don’t worry. Most everything should work without major changes.\nIf you are using an older release of keras (< 2.2.4.1), syntactically things should work fine as well, but you will want to check for changes in behavior/performance.\nAnd now for some news and background. This post aims to do three things:\nExplain the above all-clear statement. Is it really that simple – what exactly is going on?\nCharacterize the changes brought about by TF 2, from the point of view of the R user.\nAnd, perhaps most interestingly: Take a look at what is going on, in the r-tensorflow ecosystem, around new functionality related to the advent of TF 2.\nSome background\nSo if all still works fine (assuming standard usage), why so much ado about TF 2 in Python land?\nThe difference is that on the R side, for the vast majority of users, the framework you used to do deep learning was keras. tensorflow was needed just occasionally, or not at all.\nBetween keras and tensorflow, there was a clear separation of responsibilities: keras was the frontend, depending on TensorFlow as a low-level backend, just like the original Python Keras it was wrapping did. 1. In some cases, this lead to people using the words keras and tensorflow almost synonymously: Maybe they said tensorflow, but the code they wrote was keras.\nThings were different in Python land. There was original Python Keras, but TensorFlow had its own layers API, and there were a number of third-party high-level APIs built on TensorFlow.\nKeras, in contrast, was a separate library that just happened to rely on TensorFlow.\nSo in Python land, now we have a big change: With TF 2, Keras (as incorporated in the TensorFlow codebase) is now the official high-level API for TensorFlow. To bring this across has been a major point of Google’s TF 2 information campaign since the early stages.\nAs R users, who have been focusing on keras all the time, we are essentially less affected. Like we said above, syntactically most everything stays the way it was. So why differentiate between different keras versions?\nWhen keras was written, there was original Python Keras, and that was the library we were binding to. However, Google started to incorporate original Keras code into their TensorFlow codebase as a fork, to continue development independently. For a while there were two “Kerases”: Original Keras and tf.keras. Our R keras offered to switch between implementations 2, the default being original Keras.\nIn keras release 2.2.4.1, anticipating discontinuation of original Keras and wanting to get ready for TF 2, we switched to using tf.keras as the default. While in the beginning, the tf.keras fork and original Keras developed more or less in sync, the latest developments for TF 2 brought with them bigger changes in the tf.keras codebase, especially as regards optimizers.\nThis is why, if you are using a keras version < 2.2.4.1, upgrading to TF 2 you will want to check for changes in behavior and/or performance. 3\nThat’s it for some background. In sum, we’re happy most existing code will run just fine. But for us R users, something must be changing as well, right?\nTF 2 in a nutshell, from an R perspective\nIn fact, the most evident-on-user-level change is something we wrote several posts about, more than a year ago 4. By then, eager execution was a brand-new option that had to be turned on explicitly; TF 2 now makes it the default. Along with it came custom models (a.k.a. subclassed models, in Python land) and custom training, making use of tf$GradientTape. Let’s talk about what those termini refer to, and how they are relevant to R users.\nEager Execution\nIn TF 1, it was all about the graph you built when defining your model. The graph, that was – and is – an Abstract Syntax Tree (AST), with operations as nodes and tensors “flowing” along the edges. Defining a graph and running it (on actual data) were different steps.\nIn contrast, with eager execution, operations are run directly when defined.\nWhile this is a more-than-substantial change that must have required lots of resources to implement, if you use keras you won’t notice. Just as previously, the typical keras workflow of create model -> compile model -> train model never made you think about there being two distinct phases (define and run), now again you don’t have to do anything. Even though the overall execution mode is eager, Keras models are trained in graph mode, to maximize performance. We will talk about how this is done in part 3 when introducing the tfautograph package.\nIf keras runs in graph mode, how can you even see that eager execution is “on”? Well, in TF 1, when you ran a TensorFlow operation on a tensor 5, like so\n\n\nlibrary(tensorflow)\ntf$math$cumprod(1:5)\n\n\nthis is what you saw:\nTensor(\"Cumprod:0\", shape=(5,), dtype=int32)\nTo extract the actual values, you had to create a TensorFlow Session and run the tensor, or alternatively, use keras::k_eval that did this under the hood:\n\n\nlibrary(keras)\ntf$math$cumprod(1:5) %>% k_eval()\n\n\n[1]   1   2   6  24 120\nWith TF 2’s execution mode defaulting to eager, we now automatically see the values contained in the tensor: 6\n\n\ntf$math$cumprod(1:5)\n\n\ntf.Tensor([  1   2   6  24 120], shape=(5,), dtype=int32)\nSo that’s eager execution. In our last year’s Eager-category blog posts, it was always accompanied by custom models, so let’s turn there next.\nCustom models\nAs a keras user, probably you’re familiar with the sequential and functional styles of building a model. Custom models allow for even greater flexibility than functional-style ones. Check out the documentation for how to create one.\nLast year’s series on eager execution has plenty of examples using custom models, featuring not just their flexibility, but another important aspect as well: the way they allow for modular, easily-intelligible code. 7\nEncoder-decoder scenarios are a natural match. If you have seen, or written, “old-style” code for a Generative Adversarial Network (GAN), imagine something like this instead:\n\n# define the generator (simplified)\ngenerator <-\n  function(name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      \n      # define layers for the generator \n      self$fc1 <- layer_dense(units = 7 * 7 * 64, use_bias = FALSE)\n      self$batchnorm1 <- layer_batch_normalization()\n      # more layers ...\n      \n      # define what should happen in the forward pass\n      function(inputs, mask = NULL, training = TRUE) {\n        self$fc1(inputs) %>%\n          self$batchnorm1(training = training) %>%\n          # call remaining layers ...\n      }\n    })\n  }\n\n# define the discriminator\ndiscriminator <-\n  function(name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      \n      self$conv1 <- layer_conv_2d(filters = 64, #...)\n      self$leaky_relu1 <- layer_activation_leaky_relu()\n      # more layers ...\n    \n      function(inputs, mask = NULL, training = TRUE) {\n        inputs %>% self$conv1() %>%\n          self$leaky_relu1() %>%\n          # call remaining layers ...\n      }\n    })\n  }\n\nCoded like this, picture the generator and the discriminator as agents, ready to engage in what is actually the opposite of a zero-sum game.\nThe game, then, can be nicely coded using custom training.\nCustom training\nCustom training, as opposed to using keras fit, allows to interleave the training of several models. Models are called on data, and all calls have to happen inside the context of a GradientTape. In eager mode, GradientTapes are used to keep track of operations such that during backprop, their gradients can be calculated.\nThe following code example shows how using GradientTape-style training, we can see our actors play against each other:\n\n\n# zooming in on a single batch of a single epoch\nwith(tf$GradientTape() %as% gen_tape, { with(tf$GradientTape() %as% disc_tape, {\n  \n  # first, it's the generator's call (yep pun intended)\n  generated_images <- generator(noise)\n  # now the discriminator gives its verdict on the real images \n  disc_real_output <- discriminator(batch, training = TRUE)\n  # as well as the fake ones\n  disc_generated_output <- discriminator(generated_images, training = TRUE)\n  \n  # depending on the discriminator's verdict we just got,\n  # what's the generator's loss?\n  gen_loss <- generator_loss(disc_generated_output)\n  # and what's the loss for the discriminator?\n  disc_loss <- discriminator_loss(disc_real_output, disc_generated_output)\n}) })\n\n# now outside the tape's context compute the respective gradients\ngradients_of_generator <- gen_tape$gradient(gen_loss, generator$variables)\ngradients_of_discriminator <- disc_tape$gradient(disc_loss, discriminator$variables)\n \n# and apply them!\ngenerator_optimizer$apply_gradients(\n  purrr::transpose(list(gradients_of_generator, generator$variables)))\ndiscriminator_optimizer$apply_gradients(\n  purrr::transpose(list(gradients_of_discriminator, discriminator$variables)))\n\n\nAgain, compare this with pre-TF 2 GAN training – it makes for a lot more readable code.\nAs an aside, last year’s post series may have created the impression that with eager execution, you have to use custom (GradientTape) training instead of Keras-style fit. In fact, that was the case at the time those posts were written. Today, Keras-style code works just fine with eager execution.\nSo now with TF 2, we are in an optimal position. We can use custom training when we want to, but we don’t have to if declarative fit is all we need.\nThat’s it for a flashlight on what TF 2 means to R users. We now take a look around in the r-tensorflow ecosystem to see new developments – recent-past, present and future – in areas like data loading, preprocessing, and more.\nNew developments in the r-tensorflow ecosystem\nThese are what we’ll cover:\ntfdatasets: Over the recent past, tfdatasets pipelines have become the preferred way for data loading and preprocessing.\nfeature columns and feature specs: Specify your features recipes-style and have keras generate the adequate layers for them.\nKeras preprocessing layers: Keras preprocessing pipelines integrating functionality such as data augmentation (currently in planning).\ntfhub: Use pretrained models as keras layers, and/or as feature columns in a keras model.\ntf_function and tfautograph: Speed up training by running parts of your code in graph mode.\ntfdatasets input pipelines\nFor 2 years now, the tfdatasets package has been available to load data for training Keras models in a streaming way.\nLogically, there are three steps involved:\nFirst, data has to be loaded from some place. This could be a csv file, a directory containing images, or other sources. In this recent example from Image segmentation with U-Net, information about file names was first stored into an R tibble, and then tensor_slices_dataset was used to create a dataset from it:\n\n\ndata <- tibble(\n  img = list.files(here::here(\"data-raw/train\"), full.names = TRUE),\n  mask = list.files(here::here(\"data-raw/train_masks\"), full.names = TRUE)\n)\n\ndata <- initial_split(data, prop = 0.8)\n\ndataset <- training(data) %>%  \n  tensor_slices_dataset() \n\n\nOnce we have a dataset, we perform any required transformations, mapping over the batch dimension. Continuing with the example from the U-Net post, here we use functions from the tf.image module to (1) load images according to their file type, (2) scale them to values between 0 and 1 (converting to float32 at the same time), and (3) resize them to the desired format:\n\n\ndataset <- dataset %>%\n  dataset_map(~.x %>% list_modify(\n    img = tf$image$decode_jpeg(tf$io$read_file(.x$img)),\n    mask = tf$image$decode_gif(tf$io$read_file(.x$mask))[1,,,][,,1,drop=FALSE]\n  )) %>% \n  dataset_map(~.x %>% list_modify(\n    img = tf$image$convert_image_dtype(.x$img, dtype = tf$float32),\n    mask = tf$image$convert_image_dtype(.x$mask, dtype = tf$float32)\n  )) %>% \n  dataset_map(~.x %>% list_modify(\n    img = tf$image$resize(.x$img, size = shape(128, 128)),\n    mask = tf$image$resize(.x$mask, size = shape(128, 128))\n  ))\n\n\nNote how once you know what these functions do, they free you of a lot of thinking (remember how in the “old” Keras approach to image preprocessing, you were doing things like dividing pixel values by 255 “by hand”?)\nAfter transformation, a third conceptual step relates to item arrangement. You will often want to shuffle, and you certainly will want to batch the data:\n\n\n if (train) {\n    dataset <- dataset %>% \n      dataset_shuffle(buffer_size = batch_size*128)\n  }\n\ndataset <- dataset %>%  dataset_batch(batch_size)\n\n\nSumming up, using tfdatasets you build a pipeline, from loading over transformations to batching, that can then be fed directly to a Keras model. From preprocessing, let’s go a step further and look at a new, extremely convenient way to do feature engineering.\nFeature columns and feature specs\nFeature columns\nas such are a Python-TensorFlow feature, while feature specs are an R-only idiom modeled after the popular recipes package.\nIt all starts off with creating a feature spec object, using formula syntax to indicate what’s predictor and what’s target:\n\n\nlibrary(tfdatasets)\nhearts_dataset <- tensor_slices_dataset(hearts)\nspec <- feature_spec(hearts_dataset, target ~ .)\n\n\nThat specification is then refined by successive information about how we want to make use of the raw predictors. This is where feature columns come into play. Different column types exist, of which you can see a few in the following code snippet:\n\n\nspec <- feature_spec(hearts, target ~ .) %>% \n  step_numeric_column(\n    all_numeric(), -cp, -restecg, -exang, -sex, -fbs,\n    normalizer_fn = scaler_standard()\n  ) %>% \n  step_categorical_column_with_vocabulary_list(thal) %>% \n  step_bucketized_column(age, boundaries = c(18, 25, 30, 35, 40, 45, 50, 55, 60, 65)) %>% \n  step_indicator_column(thal) %>% \n  step_embedding_column(thal, dimension = 2) %>% \n  step_crossed_column(c(thal, bucketized_age), hash_bucket_size = 10) %>%\n  step_indicator_column(crossed_thal_bucketized_age)\n\nspec %>% fit()\n\n\nWhat happened here is that we told TensorFlow, please take all numeric columns (besides a few ones listed exprès) and scale them; take column thal, treat it as categorical and create an embedding for it; discretize age according to the given ranges; and finally, create a crossed column to capture interaction between thal and that discretized age-range column. 8\nThis is nice, but when creating the model, we’ll still have to define all those layers, right? (Which would be pretty cumbersome, having to figure out all the right dimensions…)\nLuckily, we don’t have to. In sync with tfdatasets, keras now provides layer_dense_features to create a layer tailor-made to accommodate the specification.\nAnd we don’t need to create separate input layers either, due to layer_input_from_dataset. Here we see both in action:\n\n\ninput <- layer_input_from_dataset(hearts %>% select(-target))\n\noutput <- input %>% \n  layer_dense_features(feature_columns = dense_features(spec)) %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\n\nFrom then on, it’s just normal keras compile and fit. See the vignette for the complete example. There also is a post on feature columns explaining more of how this works, and illustrating the time-and-nerve-saving effect by comparing with the pre-feature-spec way of working with heterogeneous datasets.\nAs a last item on the topics of preprocessing and feature engineering, let’s look at a promising thing to come in what we hope is the near future.\nKeras preprocessing layers\nReading what we wrote above about using tfdatasets for building a input pipeline, and seeing how we gave an image loading example, you may have been wondering: What about data augmentation functionality available, historically, through keras? Like image_data_generator?\nThis functionality does not seem to fit. But a nice-looking solution is in preparation. In the Keras community, the recent RFC on preprocessing layers for Keras addresses this topic. The RFC is still under discussion, but as soon as it gets implemented in Python we’ll follow up on the R side.\nThe idea is to provide (chainable) preprocessing layers to be used for data transformation and/or augmentation in areas such as image classification, image segmentation, object detection, text processing, and more. 9 The envisioned, in the RFC, pipeline of preprocessing layers should return a dataset, for compatibility with tf.data (our tfdatasets). We’re definitely looking forward to having available this sort of workflow!\nLet’s move on to the next topic, the common denominator being convenience. But now convenience means not having to build billion-parameter models yourself!\nTensorflow Hub and the tfhub package\nTensorflow Hub is a library for publishing and using pretrained models. Existing models can be browsed on tfhub.dev.\nAs of this writing, the original Python library is still under development, so complete stability is not guaranteed. That notwithstanding, the tfhub R package already allows for some instructive experimentation.\nThe traditional Keras idea of using pretrained models typically involved either (1) applying a model like MobileNet as a whole, including its output layer, or (2) chaining a “custom head” to its penultimate layer 10. In contrast, the TF Hub idea is to use a pretrained model as a module in a larger setting.\nThere are two main ways to accomplish this, namely, integrating a module as a keras layer and using it as a feature column. The tfhub README shows the first option:\n\n\nlibrary(tfhub)\nlibrary(keras)\n\ninput <- layer_input(shape = c(32, 32, 3))\n\noutput <- input %>%\n  # we are using a pre-trained MobileNet model!\n  layer_hub(handle = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2\") %>%\n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel <- keras_model(input, output)\n\n\nWhile the tfhub feature columns vignette illustrates the second one:\n\n\nspec <- dataset_train %>%\n  feature_spec(AdoptionSpeed ~ .) %>%\n  step_text_embedding_column(\n    Description,\n    module_spec = \"https://tfhub.dev/google/universal-sentence-encoder/2\"\n    ) %>%\n  step_image_embedding_column(\n    img,\n    module_spec = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/3\"\n  ) %>%\n  step_numeric_column(Age, Fee, Quantity, normalizer_fn = scaler_standard()) %>%\n  step_categorical_column_with_vocabulary_list(\n    has_type(\"string\"), -Description, -RescuerID, -img_path, -PetID, -Name\n  ) %>%\n  step_embedding_column(Breed1:Health, State)\n\n\nBoth usage modes illustrate the high potential of working with Hub modules. Just be cautioned that, as of today, not every model published will work with TF 2.\ntf_function, TF autograph and the R package tfautograph\nAs explained above, the default execution mode in TF 2 is eager. For performance reasons however, in many cases it will be desirable to compile parts of your code into a graph. Calls to Keras layers, for example, are run in graph mode.\nTo compile a function into a graph, wrap it in a call to tf_function, as done e.g. in the post Modeling censored data with tfprobability:\n\n\nrun_mcmc <- function(kernel) {\n  kernel %>% mcmc_sample_chain(\n    num_results = n_steps,\n    num_burnin_steps = n_burnin,\n    current_state = tf$ones_like(initial_betas),\n    trace_fn = trace_fn\n  )\n}\n\n# important for performance: run HMC in graph mode\nrun_mcmc <- tf_function(run_mcmc)\n\n\nOn the Python side, the tf.autograph module automatically translates Python control flow statements into appropriate graph operations.\nIndependently of tf.autograph, the R package tfautograph, developed by Tomasz Kalinowski, implements control flow conversion directly from R to TensorFlow. This lets you use R’s if, while, for, break, and next when writing custom training flows. Check out the package’s extensive documentation for instructive examples!\nConclusion\nWith that, we end our introduction of TF 2 and the new developments that surround it.\nIf you have been using keras in traditional ways, how much changes for you is mainly up to you: Most everything will still work, but new options exist to write more performant, more modular, more elegant code. In particular, check out tfdatasets pipelines for efficient data loading.\nIf you’re an advanced user requiring non-standard setup, have a look into custom training and custom models, and consult the tfautograph documentation to see how the package can help.\nIn any case, stay tuned for upcoming posts showing some of the above-mentioned functionality in action. Thanks for reading!\n\nOriginal Python Keras, and thus, R keras, supported additional backends: Theano and CNTK. But the default backend in R keras always was TensorFlow.↩︎\nNote the terminology: in R keras, implementation referred to the Python library (Keras or TensorFlow, with its module tf.keras) bound to, while backend referred to the framework providing low-level operations, which could be one of Theano, TensorFlow and CNTK.)↩︎\nE.g., parameters like learning_rate may have to be adapted.↩︎\nSee More flexible models with TensorFlow eager execution and Keras for an overview and annotated links.↩︎\nHere the nominal input is an R vector that gets converted to a Python list by reticulate, and to a tensor by TensorFlow.↩︎\nThis is still a tensor though. To continue working with its values in R, we need to convert it to R using as.numeric, as.matrix, as.array etc.↩︎\nFor example, see Generating images with Keras and TensorFlow eager execution on GANs, Neural style transfer with eager execution and Keras on neural style transfer, or Representation learning with MMD-VAE on Variational Autoencoders.↩︎\nstep_indicator_column is there (twice) for technical reasons. Our post on feature columns explains.↩︎\nAs readers working in e.g. image segmentation will know, data augmentation is not as easy as just using image_data_generator on the input images, as analogous distortions have to be applied to the masks.↩︎\nor block of layers↩︎\n",
    "preview": "posts/2019-10-08-tf2-whatchanges/images/thumb.png",
    "last_modified": "2024-11-21T15:49:24+00:00",
    "input_file": {},
    "preview_width": 400,
    "preview_height": 400
  },
  {
    "path": "posts/2019-10-03-intro-to-hmc/",
    "title": "On leapfrogs, crashing satellites, and going nuts: A very first conceptual introduction to Hamiltonian Monte Carlo",
    "description": "TensorFlow Probability, and its R wrapper tfprobability, provide Markov Chain Monte Carlo (MCMC) methods that were used in a number of recent posts on this blog. These posts were directed to users already comfortable with the method, and terminology, per se, which readers mainly interested in deep learning won't necessarily be. Here we try to make up leeway, introducing Hamitonian Monte Carlo (HMC) as well as a few often-heard \"buzzwords\" accompanying it, always striving to keep in mind what it is all \"for\".",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-10-03",
    "categories": [
      "Bayesian Modeling",
      "Concepts"
    ],
    "contents": "\nWhy a very (meaning: VERY!) first conceptual introduction to Hamiltonian Monte Carlo (HMC) on this blog?\nWell, in our endeavor to feature the various capabilities of TensorFlow Probability (TFP) / tfprobability, we started showing examples1 of how to fit hierarchical models, using one of TFP’s joint distribution classes2 and HMC. The technical aspects being complex enough in themselves, we never gave an introduction to the “math side of things.” Here we are trying to make up for this.\nSeeing how it is impossible, in a short blog post, to provide an introduction to Bayesian modeling and Markov Chain Monte Carlo in general, and how there are so many excellent texts doing this already, we will presuppose some prior knowledge. Our specific focus then is on the latest and greatest, the magic buzzwords, the famous incantations: Hamiltonian Monte Carlo, leapfrog steps, NUTS – as always, trying to demystify, to make things as understandable as possible.\nIn that spirit, welcome to a “glossary with a narrative.”\nSo what is it for?\nSampling, or Monte Carlo, techniques in general are used when we want to produce samples from, or statistically describe a distribution we don’t have a closed-form formulation of. Sometimes, we might really be interested in the samples; sometimes we just want them so we can compute, for example, the mean and variance of the distribution.\nWhat distribution? In the type of applications we’re talking about, we have a model, a joint distribution, which is supposed to describe some reality. Starting from the most basic scenario, it might look like this:\n\\[\nx \\sim \\mathcal{Poisson}(\\lambda)\n\\]\nThis “joint distribution” only has a single member, a Poisson distribution, that is supposed to model, say, the number of comments in a code review. We also have data on actual code reviews, like this, say:3\n\n\n\n\n\n\nWe now want to determine the parameter, \\(\\lambda\\), of the Poisson that make these data most likely. So far, we’re not even being Bayesian yet: There is no prior on this parameter. But of course, we want to be Bayesian, so we add one – imagine fixed priors4 on its parameters:\n\\[\nx \\sim \\mathcal{Poisson}(\\lambda)\\\\\n\\lambda \\sim \\gamma(\\alpha, \\beta)\\\\\n\\alpha \\sim [...]\\\\  \n\\beta \\sim [...]\n\\]\nThis being a joint distribution, we have three parameters to determine: \\(\\lambda\\), \\(\\alpha\\) and \\(\\beta\\).\nAnd what we’re interested in is the posterior distribution of the parameters given the data.\nNow, depending on the distributions involved, we usually cannot calculate the posterior distributions in closed form. Instead, we have to use sampling techniques to determine those parameters.5 What we’d like to point out instead is the following: In the upcoming discussions of sampling, HMC & co., it is really easy to forget what is it that we are sampling. Try to always keep in mind that what we’re sampling isn’t the data, it’s parameters: the parameters of the posterior distributions we’re interested in.\nSampling\nSampling methods in general consist of two steps: generating a sample (“proposal”) and deciding whether to keep it or to throw it away (“acceptance”). Intuitively, in our given scenario – where we have measured something and are now looking for a mechanism that explains those measurements – the latter should be easier: We “just” need to determine the likelihood of the data under those hypothetical model parameters. But how do we come up with suggestions to start with?\nIn theory, straightforward(-ish) methods exist that could be used to generate samples from an unknown (in closed form) distribution – as long as their unnormalized probabilities can be evaluated, and the problem is (very) low-dimensional. (For concise portraits of those methods, such as uniform sampling, importance sampling, and rejection sampling, see(MacKay 2002).) Those are not used in MCMC software though, for lack of efficiency and non-suitability in high dimensions. Before HMC became the dominant algorithm in such software, the Metropolis and Gibbs methods were the algorithms of choice. Both are nicely and understandably explained – in the case of Metropolis, often exemplified by nice stories –, and we refer the interested reader to the go-to references, such as (McElreath 2016) and (Kruschke 2010). Both were shown to be less efficient than HMC, the main topic of this post, due to their random-walk behavior: Every proposal is based on the current position in state space, meaning that samples may be highly correlated and state space exploration proceeds slowly.\nHMC\nSo HMC is popular because compared to random-walk-based algorithms, it is a lot more efficient. Unfortunately, it is also a lot more difficult to “get.”6 As discussed in Math, code, concepts: A third road to deep learning, there seem to be (at least) three languages to express an algorithm: Math; code (including pseudo-code, which may or may not be on the verge to math notation); and one I call conceptual which spans the whole range from very abstract to very concrete, even visual. To me personally, HMC is different from most other cases in that even though I find the conceptual explanations fascinating, they result in less “perceived understanding” than either the equations or the code. For people with backgrounds in physics, statistical mechanics and/or differential geometry this will probably be different!\nIn any case, physical analogies make for the best start.\nPhysical analogies\nThe classic physical analogy is given in the reference article, Radford Neal’s “MCMC using Hamiltonian dynamics” (Neal 2012), and nicely explained in a video by Ben Lambert.\nSo there’s this “thing” we want to maximize, the loglikelihood of the data under the model parameters. Alternatively we can say, we want to minimize the negative loglikelihood (like loss in a neural network). This “thing” to be optimized can then be visualized as an object sliding over a landscape with hills and valleys, and like with gradient descent in deep learning, we want it to end up deep down in some valley.\nIn Neal’s own words\n\nIn two dimensions, we can visualize the dynamics as that of a frictionless puck that slides over a surface of varying height. The state of this system consists of the position of the puck, given by a 2D vector q, and the momentum of the puck (its mass times its velocity), given by a 2D vector p.\n\nNow when you hear “momentum” (and given that I’ve primed you to think of deep learning) you may feel that sounds familiar, but even though the respective analogies are related the association does not help that much. In deep learning, momentum is commonly praised for its avoidance of ineffective oscillations in imbalanced optimization landscapes.7\nWith HMC however, the focus is on the concept of energy.\nIn statistical mechanics, the probability of being in some state \\(i\\) is inverse-exponentially related to its energy. (Here \\(T\\) is the temperature; we won’t focus on this so just imagine it being set to 1 in this and subsequent equations.)\n\\[P(E_i) \\sim e^{\\frac{-E_i}{T}} \\]\nAs you might or might not remember from school physics, energy comes in two forms: potential energy and kinetic energy. In the sliding-object scenario, the object’s potential energy corresponds to its height (position), while its kinetic energy is related to its momentum, \\(m\\), by the formula8\n\\[K(m) = \\frac{m^2}{2 * mass} \\]\nNow without kinetic energy, the object would slide downhill always, and as soon as the landscape slopes up again, would come to a halt. Through its momentum though, it is able to continue uphill for a while, just as if, going downhill on your bike, you pick up speed you may make it over the next (short) hill without pedaling.\nSo that’s kinetic energy. The other part, potential energy, corresponds to the thing we really want to know - the negative log posterior of the parameters we’re really after:\n\\[U(\\theta) \\sim - log (P(x | \\theta) P(\\theta))\\]\nSo the “trick” of HMC is augmenting the state space of interest - the vector of posterior parameters - by a momentum vector, to improve optimization efficiency. When we’re finished, the momentum part is just thrown away. (This aspect is especially nicely explained in Ben Lambert’s video.)\nFollowing his exposition and notation, here we have the energy of a state of parameter and momentum vectors, equaling a sum of potential and kinetic energies:\n\\[E(\\theta, m) = U(\\theta) + K(m)\\]\nThe corresponding probability, as per the relationship given above, then is\n\\[P(E) \\sim e^{\\frac{-E}{T}} = e^{\\frac{- U(\\theta)}{T}} e^{\\frac{- K(m)}{T}}\\]\nWe now substitute into this equation, assuming a temperature (T) of 1 and a mass of 1:\n\\[P(E) \\sim P(x | \\theta) P(\\theta) e^{\\frac{- m^2}{2}}\\]\nNow in this formulation, the distribution of momentum is just a standard normal (\\(e^{\\frac{- m^2}{2}}\\))! Thus, we can just integrate out the momentum and take \\(P(\\theta)\\) as samples from the posterior distribution:9\n\\[\n\\begin{aligned}\n& P(\\theta) = \n\\int \\! P(\\theta, m) \\mathrm{d}m = \\frac{1}{Z} \\int \\! P(x | \\theta) P(\\theta) \\mathcal{N}(m|0,1) \\mathrm{d}m\\\\\n& P(\\theta) = \\frac{1}{Z} \\int \\! P(x | \\theta) P(\\theta)\n\\end{aligned}\n\\]\nHow does this work in practice? At every step, we\nsample a new momentum value from its marginal distribution (which is the same as the conditional distribution given \\(U\\), as they are independent), and\nsolve for the path of the particle. This is where Hamilton’s equations come into play.\nHamilton’s equations (equations of motion)\nFor the sake of less confusion, should you decide to read the paper, here we switch to Radford Neal’s notation.\nHamiltonian dynamics operates on a d-dimensional position vector, \\(q\\), and a d-dimensional momentum vector, \\(p\\). The state space is described by the Hamiltonian, a function of \\(p\\) and \\(q\\):\n\\[H(q, p) =U(q) +K(p)\\]\nHere \\(U(q)\\) is the potential energy (called \\(U(\\theta)\\) above), and \\(K(p)\\) is the kinetic energy as a function of momentum (called \\(K(m)\\) above).\nThe partial derivatives of the Hamiltonian determine how \\(p\\) and \\(q\\) change over time, \\(t\\), according to Hamilton’s equations:\n\\[\n\\begin{aligned}\n& \\frac{dq}{dt} = \\frac{\\partial H}{\\partial p}\\\\\n& \\frac{dp}{dt} = - \\frac{\\partial H}{\\partial q}\n\\end{aligned}\n\\]\nHow can we solve this system of partial differential equations? The basic workhorse in numerical integration is Euler’s method, where time (or the independent variable, in general) is advanced by a step of size \\(\\epsilon\\), and a new value of the dependent variable is computed by taking the (partial) derivative and adding it to its current value. For the Hamiltonian system, doing this one equation after the other looks like this:\n\\[\n\\begin{aligned}\n& p(t+\\epsilon) = p(t) + \\epsilon \\frac{dp}{dt}(t) = p(t) − \\epsilon \\frac{\\partial U}{\\partial q}(q(t))\\\\\n& q(t+\\epsilon) = q(t) + \\epsilon \\frac{dq}{dt}(t) = q(t) + \\epsilon \\frac{p(t)}{m})\n\\end{aligned}\n\\]\nHere first a new position is computed for time \\(t + 1\\), making use of the current momentum at time \\(t\\); then a new momentum is computed, also for time \\(t + 1\\), making use of the current position at time \\(t\\).\nThis process can be improved if in step 2, we make use of the new position we just freshly computed in step 1; but let’s directly go to what is actually used in contemporary software, the leapfrog method.\nLeapfrog algorithm\nSo after Hamiltonian, we’ve hit the second magic word: leapfrog. Unlike Hamiltonian however, there is less mystery here. The leapfrog method is “just” a more efficient way to perform the numerical integration.\nIt consists of three steps, basically splitting up the Euler step 1 into two parts, before and after the momentum update:\n\\[\n\\begin{aligned}\n& p(t+\\frac{\\epsilon}{2}) = p(t) − \\frac{\\epsilon}{2} \\frac{\\partial U}{\\partial q}(q(t))\\\\\n& q(t+\\epsilon) = q(t) + \\epsilon \\frac{p(t + \\frac{\\epsilon}{2})}{m}\\\\\n& p(t+ \\epsilon) = p(t+\\frac{\\epsilon}{2}) − \\frac{\\epsilon}{2} \\frac{\\partial U}{\\partial q}(q(t + \\epsilon))\n\\end{aligned}\n\\]\nAs you can see, each step makes use of the corresponding variable-to-differentiate’s value computed in the preceding step. In practice, several leapfrog steps are executed before a proposal is made; so steps 3 and 1 (of the subsequent iteration) are combined.\nProposal – this keyword brings us back to the higher-level “plan.” All this – Hamiltonian equations, leapfrog integration – served to generate a proposal for a new value of the parameters, which can be accepted or not. The way that decision is taken is not particular to HMC and explained in detail in the above-mentioned expositions on the Metropolis algorithm, so we just cover it briefly.\nAcceptance: Metropolis algorithm\nUnder the Metropolis algorithm, proposed new vectors \\(q*\\) and \\(p*\\) are accepted with probability\n\\[\nmin(1, exp(−H(q∗, p∗) +H(q, p)))\n\\]\nThat is, if the proposed parameters yield a higher likelihood, they are accepted; if not, they are accepted only with a certain probability that depends on the ratio between old and new likelihoods.\nIn theory, energy staying constant in a Hamiltonian system, proposals should always be accepted; in practice, loss of precision due to numerical integration may yield an acceptance rate less than 1.\nHMC in a few lines of code\nWe’ve talked about concepts, and we’ve seen the math, but between analogies and equations, it’s easy to lose track of the overall algorithm. Nicely, Radford Neal’s paper (Neal 2012) has some code, too! Here it is reproduced, with just a few additional comments added (many comments were preexisting):\n\n\n# U is a function that returns the potential energy given q\n# grad_U returns the respective partial derivatives\n# epsilon stepsize\n# L number of leapfrog steps\n# current_q current position\n\n# kinetic energy is assumed to be sum(p^2/2) (mass == 1)\nHMC <- function (U, grad_U, epsilon, L, current_q) {\n  q <- current_q\n  # independent standard normal variates\n  p <- rnorm(length(q), 0, 1)  \n  # Make a half step for momentum at the beginning\n  current_p <- p \n  # Alternate full steps for position and momentum\n  p <- p - epsilon * grad_U(q) / 2 \n  for (i in 1:L) {\n    # Make a full step for the position\n    q <- q + epsilon * p\n    # Make a full step for the momentum, except at end of trajectory\n    if (i != L) p <- p - epsilon * grad_U(q)\n    }\n  # Make a half step for momentum at the end\n  p <- p - epsilon * grad_U(q) / 2\n  # Negate momentum at end of trajectory to make the proposal symmetric\n  p <- -p\n  # Evaluate potential and kinetic energies at start and end of trajectory \n  current_U <- U(current_q)\n  current_K <- sum(current_p^2) / 2\n  proposed_U <- U(q)\n  proposed_K <- sum(p^2) / 2\n  # Accept or reject the state at end of trajectory, returning either\n  # the position at the end of the trajectory or the initial position\n  if (runif(1) < exp(current_U-proposed_U+current_K-proposed_K)) {\n    return (q)  # accept\n  } else {\n    return (current_q)  # reject\n  }\n}\n\n\nHopefully, you find this piece of code as helpful as I do. Are we through yet? Well, so far we haven’t encountered the last magic word: NUTS. What, or who, is NUTS?\nNUTS\nNUTS, added to Stan in 201110 and about a month ago, to TensorFlow Probability’s master branch, is an algorithm that aims to circumvent one of the practical difficulties in using HMC: The choice of number of leapfrog steps to perform before making a proposal. The acronym stands for No-U-Turn Sampler, alluding to the avoidance of U-turn-shaped curves in the optimization landscape when the number of leapfrog steps is chosen too high.\nThe reference paper by Hoffman & Gelman (Hoffman and Gelman 2011) also describes a solution to a related difficulty: choosing the step size \\(\\epsilon\\). The respective algorithm, dual averaging, was also recently added to TFP.\nNUTS being more of algorithm in the computer science usage of the word than a thing to explain conceptually, we’ll leave it at that, and ask the interested reader to read the paper – or even, consult the TFP documentation to see how NUTS is implemented there. Instead, we’ll round up with another conceptual analogy, Michael Bétancourts crashing (or not!) satellite (Betancourt 2017).\nHow to avoid crashes\nBétancourt’s article is an awesome read, and a paragraph focusing on a single point made in the paper can be nothing than a “teaser” (which is why we’ll have a picture, too!).\nTo introduce the upcoming analogy, the problem starts with high dimensionality, which is a given in most real-world problems. In high dimensions, as usual, the density function has a mode (the place where it is maximal), but necessarily, there cannot be much volume around it – just like with k-nearest neighbors, the more dimensions you add, the farther your nearest neighbor will be.\nA product of volume and density, the only significant probability mass resides in the so-called typical set,11 which becomes more and more narrow in high dimensions.\nSo, the typical set is what we want to explore, but it gets more and more difficult to find it (and stay there). Now as we saw above, HMC uses gradient information to get near the mode, but if it just followed the gradient of the log probability (the position) it would leave the typical set and stop at the mode.\nThis is where momentum comes in – it counteracts the gradient, and both together ensure that the Markov chain stays on the typical set. Now here’s the satellite analogy, in Bétancourt’s own words:\n\nFor example, instead of trying to reason about a mode, a gradient, and a typical set, we can equivalently reason about a planet, a gravitational field, and an orbit (Figure 14). The probabilistic endeavor of exploring the typical set then becomes a physical endeavor of placing a satellite in a stable orbit around the hypothetical planet. Because these are just two different perspectives of the same mathematical system, they will suffer from the same pathologies. Indeed, if we place a satellite at rest out in space it will fall in the gravitational field and crash into the surface of the planet, just as naive gradient-driven trajectories crash into the mode (Figure 15). From either the probabilistic or physical perspective we are left with a catastrophic outcome.\n\n\nThe physical picture, however, provides an immediate solution: although objects at rest will crash into the planet, we can maintain a stable orbit by endowing our satellite with enough momentum to counteract the gravitational attraction. We have to be careful, however, in how exactly we add momentum to our satellite. If we add too little momentum transverse to the gravitational field, for example, then the gravitational attraction will be too strong and the satellite will still crash into the planet (Figure 16a). On the other hand, if we add too much momentum then the gravitational attraction will be too weak to capture the satellite at all and it will instead fly out into the depths of space (Figure 16b).\n\nAnd here’s the picture I promised (Figure 16 from the paper):\n\n\n\nFigure 1: Figure 16 from (Betancourt 2017)\n\n\n\nAnd with this, we conclude. Hopefully, you’ll have found this helpful – unless you knew it all (or more) beforehand, in which case you probably wouldn’t have read this post :-)\nThanks for reading!\n\n\n\nBetancourt, Michael. 2017. “A Conceptual Introduction to Hamiltonian Monte Carlo.” arXiv e-Prints, January, arXiv:1701.02434. https://arxiv.org/abs/1701.02434.\n\n\nBlei, David M., Alp Kucukelbir, and Jon D. McAuliffe. 2017. “Variational Inference: A Review for Statisticians.” Journal of the American Statistical Association 112 (518): 859–77. https://doi.org/10.1080/01621459.2017.1285773.\n\n\nHoffman, Matthew D., and Andrew Gelman. 2011. “The No-u-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.” https://arxiv.org/abs/1111.4246.\n\n\nKruschke, John K. 2010. Doing Bayesian Data Analysis: A Tutorial with r and BUGS. 1st ed. Orlando, FL, USA: Academic Press, Inc.\n\n\nMacKay, David J. C. 2002. Information Theory, Inference & Learning Algorithms. New York, NY, USA: Cambridge University Press.\n\n\nMcElreath, Richard. 2016. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. CRC Press. http://xcelab.net/rm/statistical-rethinking/.\n\n\nNeal, Radford M. 2012. “MCMC using Hamiltonian dynamics.” arXiv e-Prints, June, arXiv:1206.1901. https://arxiv.org/abs/1206.1901.\n\n\nSee e.g. Tadpoles on TensorFlow: Hierarchical partial pooling with tfprobability and Hierarchical partial pooling, continued: Varying slopes models with TensorFlow Probability↩︎\ntfd_joint_distribution_sequential↩︎\ndata are purely made up↩︎\nelided↩︎\nIn some cases, variational inference is an alternative. See (Blei, Kucukelbir, and McAuliffe 2017) for a nice introduction.↩︎\nBy “get [it]” I mean the subjective feeling of understanding what’s going on; not more and not less.↩︎\nThough see this great distill.pub post for a different view.↩︎\n\\(m\\) here stands for momentum.↩︎\nHere \\(Z\\) is the normalizer required to have the integral sum to 1.↩︎\nAs of today, Stan uses a modified version, see appendix A.5 of (Betancourt 2017), to be cited soon.↩︎\nfor a detailed discussion of typical sets, see the book by McKay (MacKay 2002).↩︎\n",
    "preview": "posts/2019-10-03-intro-to-hmc/images/mb.png",
    "last_modified": "2024-11-21T15:51:00+00:00",
    "input_file": {},
    "preview_width": 548,
    "preview_height": 345
  },
  {
    "path": "posts/2019-09-30-bert-r/",
    "title": "BERT from R",
    "description": "A deep learning model - BERT from Google AI Research - has yielded state-of-the-art results in a wide variety of Natural Language Processing (NLP) tasks. In this tutorial, we will show how to load and train the BERT model from R, using Keras.",
    "author": [
      {
        "name": "Turgut Abdullayev",
        "url": "https://github.com/henry090"
      }
    ],
    "date": "2019-09-30",
    "categories": [
      "Natural Language Processing",
      "TensorFlow/Keras"
    ],
    "contents": "\nToday, we’re happy to feature a guest post written by Turgut Abdullayev, showing how to use BERT from R. Turgut is a data scientist at AccessBank Azerbaijan. Currently, he is pursuing a Ph.D. in economics at Baku State University, Azerbaijan.\nIn the previous post, Sigrid Keydana explained the logic behind the reticulate package and how it enables interoperability between Python and R. So, this time we will build a classification model with BERT, taking into account one of the powerful capabilities of the reticulate package – calling Python from R via importing Python modules.\nBefore we start, make sure that the Python version used is 3, as Python 2 can introduce lots of difficulties while working with BERT, such as Unicode issues related to the input text.\n\nNote: The R implementation presupposes TF Keras while by default, keras-bert does not use it. So, adding that environment variable makes it work.\n\n\n\nSys.setenv(TF_KERAS=1) \n# make sure we use python 3\nreticulate::use_python('C:/Users/turgut.abdullayev/AppData/Local/Continuum/anaconda3/python.exe',\n                       required=T)\n# to see python version\nreticulate::py_config()\n\n\n\npython:         C:/Users/turgut.abdullayev/AppData/Local/Continuum/anaconda3/python.exe\nlibpython:      C:/Users/turgut.abdullayev/AppData/Local/Continuum/anaconda3/python37.dll\npythonhome:     C:\\Users\\TURGUT~1.ABD\\AppData\\Local\\CONTIN~1\\ANACON~1\nversion:        3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)]\nArchitecture:   64bit\nnumpy:          C:\\Users\\TURGUT~1.ABD\\AppData\\Local\\CONTIN~1\\ANACON~1\\lib\\site-packages\\numpy\nnumpy_version:  1.16.4\n\nNOTE: Python version was forced by use_python function\n\nLuckily for us, a convenient way of importing BERT with Keras was created by Zhao HG. It is called Keras-bert. For us, this means that importing that same python library with reticulate will allow us to build a popular state-of-the-art model within R.\nThere are several methods to install keras-bert in Python.\nin Jupyter Notebook, run:\n\n!pip install keras-bert\n\nin Terminal (Linux, Mac OS), run:\n\npython3 -m pip install keras-bert\n\nin Anaconda prompt (Windows), run:\n\nconda install keras-bert\n\nAfter this procedure, you can check whether keras-bert is installed or not.\n\n\nreticulate::py_module_available('keras_bert')\n\n\n\n[1] TRUE\n\nFinally, the TensorFlow version used should be 1.14/1.15. You can check it in the following form:\n\n\ntensorflow::tf_version()\n\n\n\n[1] ‘1.14’\n\nIn a nutshell:\n\npip install keras-bert\ntensorflow::install_tensorflow(version = \"1.15\")\n\nWhat is BERT?\nBERT1 is a pre-trained deep learning model introduced by Google AI Research which has been trained on Wikipedia and BooksCorpus. It has a unique way to understand the structure of a given text. Instead of reading the text from left to right or from right to left, BERT, using an attention mechanism which is called Transformer encoder2, reads the entire word sequences at once. So, it allows to understanding a word based on its surroundings. There are different kind of pre-trained BERT models but the main difference between them is trained parameters. In our case, BERT with 12 encoder layers (Transformer Blocks), 768-hidden hidden units, 12-heads3, and 110M parameters will be used to create a text classification model.\nModel structure\nLoading a pre-trained BERT model is straightforward. The downloaded zip file contains:\nbert_model.ckpt, which is for loading the weights from the TensorFlow checkpoint\nbert_config.json, which is a configuration file\nvocab.txt, which is for text tokenization\n\n\npretrained_path = '/Users/turgutabdullayev/Downloads/uncased_L-12_H-768_A-12'\nconfig_path = file.path(pretrained_path, 'bert_config.json')\ncheckpoint_path = file.path(pretrained_path, 'bert_model.ckpt')\nvocab_path = file.path(pretrained_path, 'vocab.txt')\n\n\nImport Keras-Bert module via reticulate\nLet’s load keras-bert via reticulate and prepare a tokenizer object. The BERT tokenizer will help us to turn words into indices.\n\n\nlibrary(reticulate)\nk_bert = import('keras_bert')\ntoken_dict = k_bert$load_vocabulary(vocab_path)\ntokenizer = k_bert$Tokenizer(token_dict)\n\n\nHow does the tokenizer work?\nBERT uses a WordPiece tokenization strategy. If a word is Out-of-vocabulary (OOV), then BERT will break it down into subwords. (eating => eat, ##ing).\n\n\n\nFigure 1: BERT input representation. The input embeddings are the sum of the token embeddings, the segmentation embeddings, and the position embeddings\n\n\n\nEmbedding Layers in BERT\nThere are 3 types of embedding layers in BERT:\nToken Embeddings help to transform words into vector representations. In our model dimension size is 768.\nSegment Embeddings help to understand the semantic similarity of different pieces of the text.\nPosition Embeddings mean that identical words at different positions will not have the same output representation.\nDefine model parameters and column names\nAs usual with keras, the batch size, number of epochs and the learning rate should be defined for training BERT.\nAdditionally, the sequence length is needed.\n\n\nseq_length = 50L\nbch_size = 70\nepochs = 1\nlearning_rate = 1e-4\n\nDATA_COLUMN = 'comment_text'\nLABEL_COLUMN = 'target'\n\n\n\nNote: the max input length is 512, and the model is extremely compute intensive even on GPU.\n\nLoad BERT model into R\nWe can load the BERT model and automatically pad sequences with seq_len function. Keras-bert4 makes the loading process very easy and comfortable.\n\n\nmodel = k_bert$load_trained_model_from_checkpoint(\n  config_path,\n  checkpoint_path,\n  training=T,\n  trainable=T,\n  seq_len=seq_length)\n\n\nData structure, reading, preparation\nThe dataset for this post is taken from the Kaggle Jigsaw Unintended Bias in Toxicity Classification competition.\nIn order to prepare the dataset, we write a preprocessing function which will read and tokenize data simultaneously. Then, we feed the outputs of the function as input for BERT model.\n\n\n# tokenize text\ntokenize_fun = function(dataset) {\n  c(indices, target, segments) %<-% list(list(),list(),list())\n  for ( i in 1:nrow(dataset)) {\n    c(indices_tok, segments_tok) %<-% tokenizer$encode(dataset[[DATA_COLUMN]][i], \n                                                       max_len=seq_length)\n    indices = indices %>% append(list(as.matrix(indices_tok)))\n    target = target %>% append(dataset[[LABEL_COLUMN]][i])\n    segments = segments %>% append(list(as.matrix(segments_tok)))\n  }\n  return(list(indices,segments, target))\n}\n\n\n\n\n# read data\ndt_data = function(dir, rows_to_read){\n  data = data.table::fread(dir, nrows=rows_to_read)\n  c(x_train, x_segment, y_train) %<-% tokenize_fun(data)\n  return(list(x_train, x_segment, y_train))\n}\n\n\nLoad dataset\nThe way we have written the preprocess function, at first, it will read data, then add zeros and encode words into indices. Hence, we will have 3 output files:\nx_train is input matrix for BERT\nx_segment contains zeros for segment embeddings\ny_train is the output target which we should predict\n\n\nc(x_train,x_segment, y_train) %<-% \ndt_data('~/Downloads/jigsaw-unintended-bias-in-toxicity-classification/train.csv',2000)\n\n\nMatrix format for Keras-Bert\nThe input data are in list format. They need to be extracted and transposed. Then, the train and segment matrices should be placed into the list.\n\n\ntrain = do.call(cbind,x_train) %>% t()\nsegments = do.call(cbind,x_segment) %>% t()\ntargets = do.call(cbind,y_train) %>% t()\n\nconcat = c(list(train ),list(segments))\n\n\nCalculate decay and warmup steps\nUsing the Adam optimizer with warmup helps to lower the learning rate at the beginning of the training process. After certain training steps, the learning rate will gradually be increased, because learning new data without warmup can negatively affect a BERT model.\n\n\nc(decay_steps, warmup_steps) %<-% k_bert$calc_train_steps(\n  targets %>% length(),\n  batch_size=bch_size,\n  epochs=epochs\n)\n\n\nDetermine inputs and outputs, then concatenate them\nIn order to build a binary classification model, the output of the BERT model should contain 1 unit. Therefore, first of all, we should get input and output layers. Then, adding an additional dense layer to the output can perfectly meet our needs.\n\n\nlibrary(keras)\n\ninput_1 = get_layer(model,name = 'Input-Token')$input\ninput_2 = get_layer(model,name = 'Input-Segment')$input\ninputs = list(input_1,input_2)\n\ndense = get_layer(model,name = 'NSP-Dense')$output\n\noutputs = dense %>% layer_dense(units=1L, activation='sigmoid',\n                         kernel_initializer=initializer_truncated_normal(stddev = 0.02),\n                         name = 'output')\n\nmodel = keras_model(inputs = inputs,outputs = outputs)\n\n\nThis is how the model architecture looks like after adding a dense layer and padding input sequences.\n\nModel\n__________________________________________________________________________________________\nLayer (type)                 Output Shape        Param #    Connected to                  \n==========================================================================================\nInput-Token (InputLayer)     (None, 50)          0                                        \n__________________________________________________________________________________________\nInput-Segment (InputLayer)   (None, 50)          0                                        \n__________________________________________________________________________________________\nEmbedding-Token (TokenEmbedd [(None, 50, 768), ( 23440896   Input-Token[0][0]             \n__________________________________________________________________________________________\nEmbedding-Segment (Embedding (None, 50, 768)     1536       Input-Segment[0][0]           \n__________________________________________________________________________________________\nEmbedding-Token-Segment (Add (None, 50, 768)     0          Embedding-Token[0][0]         \n                                                            Embedding-Segment[0][0]       \n__________________________________________________________________________________________\nEmbedding-Position (Position (None, 50, 768)     38400      Embedding-Token-Segment[0][0] \n__________________________________________________________________________________________\nEmbedding-Dropout (Dropout)  (None, 50, 768)     0          Embedding-Position[0][0]      \n__________________________________________________________________________________________\nEmbedding-Norm (LayerNormali (None, 50, 768)     1536       Embedding-Dropout[0][0]       \n__________________________________________________________________________________________\nEncoder-1-MultiHeadSelfAtten (None, 50, 768)     2362368    Embedding-Norm[0][0]          \n__________________________________________________________________________________________\nEncoder-1-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-1-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-1-MultiHeadSelfAtten (None, 50, 768)     0          Embedding-Norm[0][0]          \n                                                            Encoder-1-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-1-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-1-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-1-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-1-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-1-FeedForward-Dropou (None, 50, 768)     0          Encoder-1-FeedForward[0][0]   \n__________________________________________________________________________________________\nEncoder-1-FeedForward-Add (A (None, 50, 768)     0          Encoder-1-MultiHeadSelfAttenti\n                                                            Encoder-1-FeedForward-Dropout[\n__________________________________________________________________________________________\nEncoder-1-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-1-FeedForward-Add[0][0\n__________________________________________________________________________________________\nEncoder-2-MultiHeadSelfAtten (None, 50, 768)     2362368    Encoder-1-FeedForward-Norm[0][\n__________________________________________________________________________________________\nEncoder-2-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-2-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-2-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-1-FeedForward-Norm[0][\n                                                            Encoder-2-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-2-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-2-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-2-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-2-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-2-FeedForward-Dropou (None, 50, 768)     0          Encoder-2-FeedForward[0][0]   \n__________________________________________________________________________________________\nEncoder-2-FeedForward-Add (A (None, 50, 768)     0          Encoder-2-MultiHeadSelfAttenti\n                                                            Encoder-2-FeedForward-Dropout[\n__________________________________________________________________________________________\nEncoder-2-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-2-FeedForward-Add[0][0\n__________________________________________________________________________________________\nEncoder-3-MultiHeadSelfAtten (None, 50, 768)     2362368    Encoder-2-FeedForward-Norm[0][\n__________________________________________________________________________________________\nEncoder-3-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-3-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-3-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-2-FeedForward-Norm[0][\n                                                            Encoder-3-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-3-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-3-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-3-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-3-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-3-FeedForward-Dropou (None, 50, 768)     0          Encoder-3-FeedForward[0][0]   \n__________________________________________________________________________________________\nEncoder-3-FeedForward-Add (A (None, 50, 768)     0          Encoder-3-MultiHeadSelfAttenti\n                                                            Encoder-3-FeedForward-Dropout[\n__________________________________________________________________________________________\nEncoder-3-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-3-FeedForward-Add[0][0\n__________________________________________________________________________________________\nEncoder-4-MultiHeadSelfAtten (None, 50, 768)     2362368    Encoder-3-FeedForward-Norm[0][\n__________________________________________________________________________________________\nEncoder-4-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-4-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-4-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-3-FeedForward-Norm[0][\n                                                            Encoder-4-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-4-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-4-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-4-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-4-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-4-FeedForward-Dropou (None, 50, 768)     0          Encoder-4-FeedForward[0][0]   \n__________________________________________________________________________________________\nEncoder-4-FeedForward-Add (A (None, 50, 768)     0          Encoder-4-MultiHeadSelfAttenti\n                                                            Encoder-4-FeedForward-Dropout[\n__________________________________________________________________________________________\nEncoder-4-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-4-FeedForward-Add[0][0\n__________________________________________________________________________________________\nEncoder-5-MultiHeadSelfAtten (None, 50, 768)     2362368    Encoder-4-FeedForward-Norm[0][\n__________________________________________________________________________________________\nEncoder-5-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-5-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-5-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-4-FeedForward-Norm[0][\n                                                            Encoder-5-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-5-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-5-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-5-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-5-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-5-FeedForward-Dropou (None, 50, 768)     0          Encoder-5-FeedForward[0][0]   \n__________________________________________________________________________________________\nEncoder-5-FeedForward-Add (A (None, 50, 768)     0          Encoder-5-MultiHeadSelfAttenti\n                                                            Encoder-5-FeedForward-Dropout[\n__________________________________________________________________________________________\nEncoder-5-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-5-FeedForward-Add[0][0\n__________________________________________________________________________________________\nEncoder-6-MultiHeadSelfAtten (None, 50, 768)     2362368    Encoder-5-FeedForward-Norm[0][\n__________________________________________________________________________________________\nEncoder-6-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-6-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-6-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-5-FeedForward-Norm[0][\n                                                            Encoder-6-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-6-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-6-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-6-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-6-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-6-FeedForward-Dropou (None, 50, 768)     0          Encoder-6-FeedForward[0][0]   \n__________________________________________________________________________________________\nEncoder-6-FeedForward-Add (A (None, 50, 768)     0          Encoder-6-MultiHeadSelfAttenti\n                                                            Encoder-6-FeedForward-Dropout[\n__________________________________________________________________________________________\nEncoder-6-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-6-FeedForward-Add[0][0\n__________________________________________________________________________________________\nEncoder-7-MultiHeadSelfAtten (None, 50, 768)     2362368    Encoder-6-FeedForward-Norm[0][\n__________________________________________________________________________________________\nEncoder-7-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-7-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-7-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-6-FeedForward-Norm[0][\n                                                            Encoder-7-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-7-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-7-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-7-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-7-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-7-FeedForward-Dropou (None, 50, 768)     0          Encoder-7-FeedForward[0][0]   \n__________________________________________________________________________________________\nEncoder-7-FeedForward-Add (A (None, 50, 768)     0          Encoder-7-MultiHeadSelfAttenti\n                                                            Encoder-7-FeedForward-Dropout[\n__________________________________________________________________________________________\nEncoder-7-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-7-FeedForward-Add[0][0\n__________________________________________________________________________________________\nEncoder-8-MultiHeadSelfAtten (None, 50, 768)     2362368    Encoder-7-FeedForward-Norm[0][\n__________________________________________________________________________________________\nEncoder-8-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-8-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-8-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-7-FeedForward-Norm[0][\n                                                            Encoder-8-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-8-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-8-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-8-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-8-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-8-FeedForward-Dropou (None, 50, 768)     0          Encoder-8-FeedForward[0][0]   \n__________________________________________________________________________________________\nEncoder-8-FeedForward-Add (A (None, 50, 768)     0          Encoder-8-MultiHeadSelfAttenti\n                                                            Encoder-8-FeedForward-Dropout[\n__________________________________________________________________________________________\nEncoder-8-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-8-FeedForward-Add[0][0\n__________________________________________________________________________________________\nEncoder-9-MultiHeadSelfAtten (None, 50, 768)     2362368    Encoder-8-FeedForward-Norm[0][\n__________________________________________________________________________________________\nEncoder-9-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-9-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-9-MultiHeadSelfAtten (None, 50, 768)     0          Encoder-8-FeedForward-Norm[0][\n                                                            Encoder-9-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-9-MultiHeadSelfAtten (None, 50, 768)     1536       Encoder-9-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-9-FeedForward (FeedF (None, 50, 768)     4722432    Encoder-9-MultiHeadSelfAttenti\n__________________________________________________________________________________________\nEncoder-9-FeedForward-Dropou (None, 50, 768)     0          Encoder-9-FeedForward[0][0]   \n__________________________________________________________________________________________\nEncoder-9-FeedForward-Add (A (None, 50, 768)     0          Encoder-9-MultiHeadSelfAttenti\n                                                            Encoder-9-FeedForward-Dropout[\n__________________________________________________________________________________________\nEncoder-9-FeedForward-Norm ( (None, 50, 768)     1536       Encoder-9-FeedForward-Add[0][0\n__________________________________________________________________________________________\nEncoder-10-MultiHeadSelfAtte (None, 50, 768)     2362368    Encoder-9-FeedForward-Norm[0][\n__________________________________________________________________________________________\nEncoder-10-MultiHeadSelfAtte (None, 50, 768)     0          Encoder-10-MultiHeadSelfAttent\n__________________________________________________________________________________________\nEncoder-10-MultiHeadSelfAtte (None, 50, 768)     0          Encoder-9-FeedForward-Norm[0][\n                                                            Encoder-10-MultiHeadSelfAttent\n__________________________________________________________________________________________\nEncoder-10-MultiHeadSelfAtte (None, 50, 768)     1536       Encoder-10-MultiHeadSelfAttent\n__________________________________________________________________________________________\nEncoder-10-FeedForward (Feed (None, 50, 768)     4722432    Encoder-10-MultiHeadSelfAttent\n__________________________________________________________________________________________\nEncoder-10-FeedForward-Dropo (None, 50, 768)     0          Encoder-10-FeedForward[0][0]  \n__________________________________________________________________________________________\nEncoder-10-FeedForward-Add ( (None, 50, 768)     0          Encoder-10-MultiHeadSelfAttent\n                                                            Encoder-10-FeedForward-Dropout\n__________________________________________________________________________________________\nEncoder-10-FeedForward-Norm  (None, 50, 768)     1536       Encoder-10-FeedForward-Add[0][\n__________________________________________________________________________________________\nEncoder-11-MultiHeadSelfAtte (None, 50, 768)     2362368    Encoder-10-FeedForward-Norm[0]\n__________________________________________________________________________________________\nEncoder-11-MultiHeadSelfAtte (None, 50, 768)     0          Encoder-11-MultiHeadSelfAttent\n__________________________________________________________________________________________\nEncoder-11-MultiHeadSelfAtte (None, 50, 768)     0          Encoder-10-FeedForward-Norm[0]\n                                                            Encoder-11-MultiHeadSelfAttent\n__________________________________________________________________________________________\nEncoder-11-MultiHeadSelfAtte (None, 50, 768)     1536       Encoder-11-MultiHeadSelfAttent\n__________________________________________________________________________________________\nEncoder-11-FeedForward (Feed (None, 50, 768)     4722432    Encoder-11-MultiHeadSelfAttent\n__________________________________________________________________________________________\nEncoder-11-FeedForward-Dropo (None, 50, 768)     0          Encoder-11-FeedForward[0][0]  \n__________________________________________________________________________________________\nEncoder-11-FeedForward-Add ( (None, 50, 768)     0          Encoder-11-MultiHeadSelfAttent\n                                                            Encoder-11-FeedForward-Dropout\n__________________________________________________________________________________________\nEncoder-11-FeedForward-Norm  (None, 50, 768)     1536       Encoder-11-FeedForward-Add[0][\n__________________________________________________________________________________________\nEncoder-12-MultiHeadSelfAtte (None, 50, 768)     2362368    Encoder-11-FeedForward-Norm[0]\n__________________________________________________________________________________________\nEncoder-12-MultiHeadSelfAtte (None, 50, 768)     0          Encoder-12-MultiHeadSelfAttent\n__________________________________________________________________________________________\nEncoder-12-MultiHeadSelfAtte (None, 50, 768)     0          Encoder-11-FeedForward-Norm[0]\n                                                            Encoder-12-MultiHeadSelfAttent\n__________________________________________________________________________________________\nEncoder-12-MultiHeadSelfAtte (None, 50, 768)     1536       Encoder-12-MultiHeadSelfAttent\n__________________________________________________________________________________________\nEncoder-12-FeedForward (Feed (None, 50, 768)     4722432    Encoder-12-MultiHeadSelfAttent\n__________________________________________________________________________________________\nEncoder-12-FeedForward-Dropo (None, 50, 768)     0          Encoder-12-FeedForward[0][0]  \n__________________________________________________________________________________________\nEncoder-12-FeedForward-Add ( (None, 50, 768)     0          Encoder-12-MultiHeadSelfAttent\n                                                            Encoder-12-FeedForward-Dropout\n__________________________________________________________________________________________\nEncoder-12-FeedForward-Norm  (None, 50, 768)     1536       Encoder-12-FeedForward-Add[0][\n__________________________________________________________________________________________\nExtract (Extract)            (None, 768)         0          Encoder-12-FeedForward-Norm[0]\n__________________________________________________________________________________________\nNSP-Dense (Dense)            (None, 768)         590592     Extract[0][0]                 \n__________________________________________________________________________________________\noutput (Dense)               (None, 1)           769        NSP-Dense[0][0]               \n==========================================================================================\nTotal params: 109,128,193\nTrainable params: 109,128,193\nNon-trainable params: 0\n__________________________________________________________________________________________\n\nCompile model and begin training\nAus usual with Keras, before training a model, we need to compile the model. And using fit(), we feed it the R arrays.\n\n\nmodel %>% compile(\n  k_bert$AdamWarmup(decay_steps=decay_steps, \n                    warmup_steps=warmup_steps, lr=learning_rate),\n  loss = 'binary_crossentropy',\n  metrics = 'accuracy'\n)\n\nmodel %>% fit(\n  concat,\n  targets,\n  epochs=epochs,\n  batch_size=bch_size, validation_split=0.2)\n\n\nConclusion\nIn this post, we’ve shown how we can use Keras to conveniently load, configure, and train a BERT model.\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding↩︎\nAttention Is All You Need↩︎\nAttention — focuses on salient parts of input by taking a weighted average of them. 768 hidden units divided by 12 chunks and each chunk will have 64 output dimensions, afterward, the result from each chunk will be concatenated and forwarded to the next layer↩︎\nImplementation of the BERT. Official pre-trained models could be loaded for feature extraction and prediction↩︎\n",
    "preview": "posts/2019-09-30-bert-r/images/bert.png",
    "last_modified": "2024-11-21T15:48:48+00:00",
    "input_file": {},
    "preview_width": 437,
    "preview_height": 367
  },
  {
    "path": "posts/2019-08-29-using-tf-from-r/",
    "title": "So, how come we can use TensorFlow from R?",
    "description": "Have you ever wondered why you can call TensorFlow - mostly known as a Python framework - from R? If not - that's how it should be, as the R packages keras and tensorflow aim to make this process as transparent as possible to the user. But for them to be those helpful genies, someone else first has to tame the Python.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-08-29",
    "categories": [
      "TensorFlow/Keras",
      "Meta",
      "Concepts"
    ],
    "contents": "\nWhich computer language is most closely associated with TensorFlow? While on the TensorFlow for R blog, we would of course like the answer to be R, chances are it is Python (though TensorFlow has official 1 bindings for C++, Swift, Javascript, Java, and Go as well).\nSo why is it you can define a Keras model as\n\n\nlibrary(keras)\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 32, activation = \"relu\") %>%\n  layer_dense(units = 1)\n\n\n(nice with %>%s and all!) – then train and evaluate it, get predictions and plot them, all that without ever leaving R?\nThe short answer is, you have keras, tensorflow and reticulate installed.\nreticulate embeds a Python session within the R process. A single process means a single address space: The same objects exist, and can be operated upon, regardless of whether they’re seen by R or by Python. On that basis, tensorflow and keras then wrap the respective Python libraries 2 and let you write R code that, in fact, looks like R.\nThis post first elaborates a bit on the short answer. We then go deeper into what happens in the background.\nOne note on terminology before we jump in: On the R side, we’re making a clear distinction between the packages keras and tensorflow. For Python we are going to use TensorFlow and Keras interchangeably. Historically, these have been different, and TensorFlow was commonly thought of as one possible backend to run Keras on, besides the pioneering, now discontinued Theano, and CNTK. Standalone Keras does still exist, but recent work has been, and is being, done in tf.keras. Of course, this makes Python Keras a subset of Python TensorFlow, but all examples in this post will use that subset so we can use both to refer to the same thing.\nSo keras, tensorflow, reticulate, what are they for?\nFirstly, nothing of this would be possible without reticulate. 3 reticulate is an R package designed to allow seemless interoperability between R and Python. If we absolutely wanted, we could construct a Keras model like this:\n\n\nlibrary(reticulate)\ntf <- import(\"tensorflow\")\nm <- tf$keras$models$Sequential()\nm$`__class__`\n\n\n<class 'tensorflow.python.keras.engine.sequential.Sequential'>\nWe could go on adding layers …\n\n\nm$add(tf$keras$layers$Dense(32, \"relu\"))\nm$add(tf$keras$layers$Dense(1))\nm$layers\n\n\n[[1]]\n<tensorflow.python.keras.layers.core.Dense>\n\n[[2]]\n<tensorflow.python.keras.layers.core.Dense>\n\nBut who would want to? If this were the only way, it’d be less cumbersome to directly write Python instead. Plus, as a user you’d have to know the complete Python-side module structure (now where do optimizers live, currently: tf.keras.optimizers, tf.optimizers …?), and keep up with all path and name changes in the Python API. 4\nThis is where keras comes into play. keras is where the TensorFlow-specific usability, re-usability, and convenience features live. 5\nFunctionality provided by keras spans the whole range between boilerplate-avoidance over enabling elegant, R-like idioms to providing means of advanced feature usage. As an example for the first two, consider layer_dense which, among others, converts its units argument to an integer, and takes arguments in an order that allow it to be “pipe-added” to a model: Instead of\n\n\nmodel <- keras_model_sequential()\nmodel$add(layer_dense(units = 32L))\n\n\nwe can just say\n\n\nmodel <- keras_model_sequential()\nmodel %>% layer_dense(units = 32)\n\n\nWhile these are nice to have, there is more. Advanced functionality in (Python) Keras mostly depends on the ability to subclass objects. One example is custom callbacks. If you were using Python, you’d have to subclass tf.keras.callbacks.Callback. From R, you can create an R6 class inheriting from KerasCallback, like so\n\n\nCustomCallback <- R6::R6Class(\"CustomCallback\",\n    inherit = KerasCallback,\n    public = list(\n      on_train_begin = function(logs) {\n        # do something\n      },\n      on_train_end = function(logs) {\n        # do something\n      }\n    )\n  )\n\n\nThis is because keras defines an actual Python class, RCallback, and maps your R6 class’ methods to it.\nAnother example is custom models, introduced on this blog about a year ago.\nThese models can be trained with custom training loops. In R, you use keras_model_custom to create one, for example, like this:\n\n\nm <- keras_model_custom(name = \"mymodel\", function(self) {\n  self$dense1 <- layer_dense(units = 32, activation = \"relu\")\n  self$dense2 <- layer_dense(units = 10, activation = \"softmax\")\n  \n  function(inputs, mask = NULL) {\n    self$dense1(inputs) %>%\n      self$dense2()\n  }\n})\n\n\nHere, keras will make sure an actual Python object is created which subclasses tf.keras.Model and when called, runs the above anonymous function().\nSo that’s keras. What about the tensorflow package? As a user you only need it when you have to do advanced stuff, like configure TensorFlow device usage or (in TF 1.x) access elements of the Graph or the Session. Internally, it is used by keras heavily. Essential internal functionality includes, e.g., implementations of S3 methods, like print, [ or +, on Tensors, so you can operate on them like on R vectors.\nNow that we know what each of the packages is “for”, let’s dig deeper into what makes this possible.\nShow me the magic: reticulate\nInstead of exposing the topic top-down, we follow a by-example approach, building up complexity as we go. We’ll have three scenarios.\nFirst, we assume we already have a Python object (that has been constructed in whatever way) and need to convert that to R. Then, we’ll investigate how we can create a Python object, calling its constructor. Finally, we go the other way round: We ask how we can pass an R function to Python for later usage.\nScenario 1: R-to-Python conversion\nLet’s assume we have created a Python object in the global namespace, like this:\n\n\npy_run_string(\"x = 1\")\n\n\nSo: There is a variable, called x, with value 1, living in Python world. Now how do we bring this thing into R?\nWe know the main entry point to conversion is py_to_r, defined as a generic in conversion.R:\n\n\npy_to_r <- function(x) {\n  ensure_python_initialized()\n  UseMethod(\"py_to_r\")\n}\n\n\n… with the default implementation calling a function named py_ref_to_r:\n\n#' @export\npy_to_r.default <- function(x) {\n  [...]\n  x <- py_ref_to_r(x)\n  [...]\n}\n\nTo find out more about what is going on, debugging on the R level won’t get us far. We start gdb so we can set breakpoints in C++ functions: 6\n$ R -d gdb\n\nGNU gdb (GDB) Fedora 8.3-6.fc30\n[... some more gdb saying hello ...]\nReading symbols from /usr/lib64/R/bin/exec/R...\nReading symbols from /usr/lib/debug/usr/lib64/R/bin/exec/R-3.6.0-1.fc30.x86_64.debug...\n\nNow start R, load reticulate, and execute the assignment we’re going to presuppose:\n(gdb) run\nStarting program: /usr/lib64/R/bin/exec/R \n[...]\nR version 3.6.0 (2019-04-26) -- \"Planting of a Tree\"\nCopyright (C) 2019 The R Foundation for Statistical Computing\n[...]\n> library(reticulate)\n> py_run_string(\"x = 1\")\nSo that set up our scenario, the Python object (named x) we want to convert to R. Now, use Ctrl-C to “escape” to gdb, set a breakpoint in py_to_r and type c to get back to R:\n(gdb) b py_to_r\nBreakpoint 1 at 0x7fffe48315d0 (2 locations)\n(gdb) c\nNow what are we going to see when we access that x?\n> py$x\n\nThread 1 \"R\" hit Breakpoint 1, 0x00007fffe48315d0 in py_to_r(libpython::_object*, bool)@plt () from /home/key/R/x86_64-redhat-linux-gnu-library/3.6/reticulate/libs/reticulate.so\nHere are the relevant (for our investigation) frames of the backtrace:\nThread 1 \"R\" hit Breakpoint 3, 0x00007fffe48315d0 in py_to_r(libpython::_object*, bool)@plt () from /home/key/R/x86_64-redhat-linux-gnu-library/3.6/reticulate/libs/reticulate.so\n(gdb) bt\n#0  0x00007fffe48315d0 in py_to_r(libpython::_object*, bool)@plt () from /home/key/R/x86_64-redhat-linux-gnu-library/3.6/reticulate/libs/reticulate.so\n#1  0x00007fffe48588a0 in py_ref_to_r_with_convert (x=..., convert=true) at reticulate_types.h:32\n#2  0x00007fffe4858963 in py_ref_to_r (x=...) at /home/key/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include/RcppCommon.h:120\n#3  0x00007fffe483d7a9 in _reticulate_py_ref_to_r (xSEXP=0x55555daa7e50) at /home/key/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include/Rcpp/as.h:151\n...\n...\n#14 0x00007ffff7cc5fc7 in Rf_usemethod (generic=0x55555757ce70 \"py_to_r\", obj=obj@entry=0x55555daa7e50, call=call@entry=0x55555a0fe198, args=args@entry=0x55555557c4e0, \n    rho=rho@entry=0x55555dab2ed0, callrho=0x55555dab48d8, defrho=0x5555575a4068, ans=0x7fffffff69e8) at objects.c:486\nWe’ve removed a few intermediate frames related to (R-level) method dispatch.\nAs we already saw in the source code, py_to_r.default will delegate to a method called py_ref_to_r, which we see appears in #2. But what is _reticulate_py_ref_to_r in #3, the frame just below? Here is where the magic, unseen by the user, begins.\nLet’s look at this from a bird’s eye’s view. To translate an object from one language to another, we need to find a common ground, that is, a third language “spoken” by both of them. In the case of R and Python (as well as in a lot of other cases) this will be C / C++. So assuming we are going to write a C function to talk to Python, how can we use this function in R?\nWhile R users have the ability to call into C directly, using .Call or .External 7, this is made much more convenient by Rcpp 8: You just write your C++ function, and Rcpp takes care of compilation and provides the glue code necessary to call this function from R.\nSo py_ref_to_r really is written in C++:\n\n// [[Rcpp::export]]\nSEXP py_ref_to_r(PyObjectRef x) {\n  return py_ref_to_r_with_convert(x, x.convert());\n}\n\nbut the comment // [[Rcpp::export]] tells Rcpp to generate an R wrapper, py_ref_to_R, that itself calls a C++ wrapper, _reticulate_py_ref_to_r …\n\n\npy_ref_to_r <- function(x) {\n  .Call(`_reticulate_py_ref_to_r`, x)\n}\n\n\nwhich finally wraps the “real” thing, the C++ function py_ref_to_R we saw above.\nVia py_ref_to_r_with_convert in #1, a one-liner that extracts an object’s “convert” feature (see below)\n\n// [[Rcpp::export]]\nSEXP py_ref_to_r_with_convert(PyObjectRef x, bool convert) {\n  return py_to_r(x, convert);\n}\n\nwe finally arrive at py_to_r in #0.\nBefore we look at that, let’s contemplate that C/C++ “bridge” from the other side - Python.\nWhile strictly, Python is a language specification, its reference implementation is CPython, with a core written in C and much more functionality built on top in Python. In CPython, every Python object (including integers or other numeric types) is a PyObject. PyObjects are allocated through and operated on using pointers; most C API functions return a pointer to one, PyObject *.\nSo this is what we expect to work with, from R. What then is PyObjectRef doing in py_ref_to_r?\nPyObjectRef is not part of the C API, it is part of the functionality introduced by reticulate to manage Python objects. Its main purpose is to make sure the Python object is automatically cleaned up when the R object (an Rcpp::Environment) goes out of scope.\nWhy use an R environment to wrap the Python-level pointer? This is because R environments can have finalizers: functions that are called before objects are garbage collected.\nWe use this R-level finalizer to ensure the Python-side object gets finalized as well:\n\nRcpp::RObject xptr = R_MakeExternalPtr((void*) object, R_NilValue, R_NilValue);\nR_RegisterCFinalizer(xptr, python_object_finalize);\n\npython_object_finalize is interesting, as it tells us something crucial about Python – about CPython, to be precise: To find out if an object is still needed, or could be garbage collected, it uses reference counting, thus placing on the user the burden of correctly incrementing and decrementing references according to language semantics.\n\ninline void python_object_finalize(SEXP object) {\n  PyObject* pyObject = (PyObject*)R_ExternalPtrAddr(object);\n  if (pyObject != NULL)\n    Py_DecRef(pyObject);\n}\n\nResuming on PyObjectRef, note that it also stores the “convert” feature of the Python object, used to determine whether that object should be converted to R automatically.\nBack to py_to_r. This one now really gets to work with (a pointer to the) Python object,\n\nSEXP py_to_r(PyObject* x, bool convert) {\n  //...\n}\n\nand – but wait. Didn’t py_ref_to_r_with_convert pass it a PyObjectRef? So how come it receives a PyObject instead? This is because PyObjectRef inherits from Rcpp::Environment, and its implicit conversion operator is used to extract the Python object from the Environment. Concretely, that operator tells the compiler that a PyObjectRef can be used as though it were a PyObject* in some concepts, and the associated code specifies how to convert from PyObjectRef to PyObject*:\n\noperator PyObject*() const {\n  return get();\n}\n\nPyObject* get() const {\n  SEXP pyObject = getFromEnvironment(\"pyobj\");\n  if (pyObject != R_NilValue) {\n    PyObject* obj = (PyObject*)R_ExternalPtrAddr(pyObject);\n    if (obj != NULL)\n      return obj;\n  }\n  Rcpp::stop(\"Unable to access object (object is from previous session and is now invalid)\");\n}\n\nSo py_to_r works with a pointer to a Python object and returns what we want, an R object (a SEXP).\nThe function checks for the type of the object, and then uses Rcpp to construct the adequate R object, in our case, an integer:\n\n\nelse if (scalarType == INTSXP)\n  return IntegerVector::create(PyInt_AsLong(x));\n\nFor other objects, typically there’s more action required; but essentially, the function is “just” a big if-else tree.\nSo this was scenario 1: converting a Python object to R. Now in scenario 2, we assume we still need to create that Python object.\nScenario 2:\nAs this scenario is considerably more complex than the previous one, we will explicitly concentrate on some aspects and leave out others. Importantly, we’ll not go into module loading, which would deserve separate treatment of its own. Instead, we try to shed a light on what’s involved using a concrete example: the ubiquitous, in keras code, keras_model_sequential(). All this R function does is\n\n\nfunction(layers = NULL, name = NULL) {\n  keras$models$Sequential(layers = layers, name = name)\n}\n\n\nHow can keras$models$Sequential() give us an object? When in Python, you run the equivalent\n\ntf.keras.models.Sequential()\n\nthis calls the constructor, that is, the __init__ method of the class:\n\nclass Sequential(training.Model):\n  def __init__(self, layers=None, name=None):\n    # ...\n  # ...\n\nSo this time, before – as always, in the end – getting an R object back from Python, we need to call that constructor, that is, a Python callable. (Python callables subsume functions, constructors, and objects created from a class that has a call method.)\nSo when py_to_r, inspecting its argument’s type, sees it is a Python callable (wrapped in a PyObjectRef, the reticulate-specific subclass of Rcpp::Environment we talked about above), it wraps it (the PyObjectRef) in an R function, using Rcpp:\n\nRcpp::Function f = py_callable_as_function(pyFunc, convert);\n\nThe cpython-side action starts when py_callable_as_function then calls py_call_impl. py_call_impl executes the actual call and returns an R object, a SEXP. Now you may be asking, how does the Python runtime know it shouldn’t deallocate that object, now that its work is done? This is taken of by the same PyObjectRef class used to wrap instances of PyObject *: It can wrap SEXPs as well.\nWhile a lot more could be said about what happens before we finally get to work with that Sequential model from R, let’s stop here and look at our third scenario.\nScenario 3: Calling R from Python\nNot surprisingly, sometimes we need to pass R callbacks to Python. An example are R data generators that can be used with keras models 9.\nIn general, for R objects to be passed to Python, the process is somewhat opposite to what we described in example 1. Say we type:\n\n\npy$a <- 1\n\n\nThis assigns 1 to a variable a in the python main module.\nTo enable assignment, reticulate provides an implementation of the S3 generic $<-, $<-.python.builtin.object, which delegates to py_set_attr, which then calls py_set_attr_impl – yet another C++ function exported via Rcpp.\nLet’s focus on a different aspect here, though. A prerequisite for the assignment to happen is getting that 1 converted to Python. (We’re using the simplest possible example, obviously; but you can imagine this getting a lot more complex if the object isn’t a simple number).\nFor our “minimal example”, we see a stacktrace like the following\n#0 0x00007fffe4832010 in r_to_py_cpp(Rcpp::RObject_Impl<Rcpp::PreserveStorage>, bool)@plt () from /home/key/R/x86_64-redhat-linux-gnu-library/3.6/reticulate/libs/reticulate.so\n#1  0x00007fffe4854f38 in r_to_py_impl (object=..., convert=convert@entry=true) at /home/key/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include/RcppCommon.h:120\n#2  0x00007fffe48418f3 in _reticulate_r_to_py_impl (objectSEXP=0x55555ec88fa8, convertSEXP=<optimized out>) at /home/key/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include/Rcpp/as.h:151\n...\n#12 0x00007ffff7cc5c03 in dispatchMethod (sxp=0x55555d0cf1a0, dotClass=<optimized out>, cptr=cptr@entry=0x7ffffffeaae0, method=method@entry=0x55555bfe06c0, \n    generic=0x555557634458 \"r_to_py\", rho=0x55555d1d98a8, callrho=0x5555555af2d0, defrho=0x555557947430, op=<optimized out>, op=<optimized out>) at objects.c:436\n#13 0x00007ffff7cc5fc7 in Rf_usemethod (generic=0x555557634458 \"r_to_py\", obj=obj@entry=0x55555ec88fa8, call=call@entry=0x55555c0317b8, args=args@entry=0x55555557cc60, \n    rho=rho@entry=0x55555d1d98a8, callrho=0x5555555af2d0, defrho=0x555557947430, ans=0x7ffffffe9928) at objects.c:486\nWhereas r_to_py is a generic (like py_to_r above), r_to_py_impl is wrapped by Rcpp and r_to_py_cpp is a C++ function that branches on the type of the object – basically the counterpart of the C++ r_to_py.\nIn addition to that general process, there is more going on when we call an R function from Python. As Python doesn’t “speak” R, we need to wrap the R function in CPython - basically, we are extending Python here! How to do this is described in the official Extending Python Guide.\nIn official terms, what reticulate does it embed and extend Python.\nEmbed, because it lets you use Python from inside R. Extend, because to enable Python to call back into R it needs to wrap R functions in C, so Python can understand them.\nAs part of the former, the desired Python is loaded (Py_Initialize()); as part of the latter, two functions are defined in a new module named rpycall, that will be loaded when Python itself is loaded.\n\nPyImport_AppendInittab(\"rpycall\", &initializeRPYCall);\n\nThese methods are call_r_function, used by default, and call_python_function_on_main_thread, used in cases where we need to make sure the R function is called on the main thread:\n\nPyMethodDef RPYCallMethods[] = {\n  { \"call_r_function\", (PyCFunction)call_r_function,\n    METH_VARARGS | METH_KEYWORDS, \"Call an R function\" },\n  { \"call_python_function_on_main_thread\", (PyCFunction)call_python_function_on_main_thread,\n    METH_VARARGS | METH_KEYWORDS, \"Call a Python function on the main thread\" },\n  { NULL, NULL, 0, NULL }\n};\n\ncall_python_function_on_main_thread is especially interesting. The R runtime is single-threaded; while the CPython implementation of Python effectively is as well, due to the Global Interpreter Lock, this is not automatically the case when other implementations are used, or C is used directly. So call_python_function_on_main_thread makes sure that unless we can execute on the main thread, we wait.\nThat’s it for our three “spotlights on reticulate”.\nWrapup\nIt goes without saying that there’s a lot about reticulate we didn’t cover in this article, such as memory management, initialization, or specifics of data conversion. Nonetheless, we hope we were able to shed a bit of light on the magic involved in calling TensorFlow from R.\nR is a concise and elegant language, but to a high degree its power comes from its packages, including those that allow you to call into, and interact with, the outside world, such as deep learning frameworks or distributed processing engines. In this post, it was a special pleasure to focus on a central building block that makes much of this possible: reticulate.\nThanks for reading!\n\nor semi-official, dependent on the language; see the TensorFlow website to track status↩︎\nbut see the “note on terminology” below↩︎\nNot without Rcpp either, but we’ll save that for the “Digging deeper” section.↩︎\nof which there are many, currently, accompanying the substantial changes related to the introduction of TF 2.0.↩︎\nIt goes without saying that as a generic mediator between R and Python, reticulate can not provide convenience features for all R packages that use it.↩︎\nFor a very nice introduction to debugging R with a debugger like gdb, see Kevin Ushey’s “Debugging with LLDB” That post uses lldb which is the standard debugger on Macintosh, while here we’re using gdb on linux; but mostly the behaviors are very similar.↩︎\nFor a nice introduction, see version 1 of Advanced R.↩︎\nNot a copy-paste error: For a nice introduction, see version 2 of Advanced R.↩︎\nFor performance reasons, it is often advisable to use tfdatasets instead.↩︎\n",
    "preview": "posts/2019-08-29-using-tf-from-r/images/thumb.png",
    "last_modified": "2024-11-21T15:48:58+00:00",
    "input_file": {},
    "preview_width": 739,
    "preview_height": 516
  },
  {
    "path": "posts/2019-08-23-unet/",
    "title": "Image segmentation with U-Net",
    "description": "In image segmentation, every pixel of an image is assigned a class. Depending on the application, classes could be different cell types; or the task could be binary, as in \"cancer cell yes or no?\". Area of application notwithstanding, the established neural network architecture of choice is U-Net. In this post, we show how to preprocess data and train a U-Net model on the Kaggle Carvana image segmentation data.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": {}
      },
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-08-23",
    "categories": [
      "Image Recognition & Image Processing",
      "TensorFlow/Keras"
    ],
    "contents": "\nSure, it is nice when I have a picture of some object, and a neural network can tell me what kind of object that is. More realistically, there might be several salient objects in that picture, and it tells me what they are, and where they are. The latter task (known as object detection) seems especially prototypical of contemporary AI applications that at the same time are intellectually fascinating and ethically questionable. It’s different with the subject of this post: Successful image segmentation has a lot of undeniably useful applications. For example, it is a sine qua non in medicine, neuroscience, biology and other life sciences.\nSo what, technically, is image segmentation, and how can we train a neural network to do it?\nImage segmentation in a nutshell\nSay we have an image with a bunch of cats in it. In classification, the question is “what’s that?” and the answer we want to hear is: “cat.” In object detection, we again ask “what’s that,” but now that “what” is implicitly plural, and we expect an answer like “there’s a cat, a cat, and a cat, and they’re here, here, and here” (imagine the network pointing, by means of drawing bounding boxes, i.e., rectangles around the detected objects). In segmentation, we want more: We want the whole image covered by “boxes” – which aren’t boxes anymore, but unions of pixel-size “boxlets” – or put differently: We want the network to label every single pixel in the image.\nHere’s an example from the paper we’re going to talk about in a second. On the left is the input image (HeLa cells), next up is the ground truth, and third is the learned segmentation mask.\n\n\n\nFigure 1: Example segmentation from Ronneberger et al. 2015.\n\n\n\nTechnically, a distinction is made between class segmentation and instance segmentation. In class segmentation, referring to the “bunch of cats” example, there are two possible labels: Every pixel is either “cat” or “not cat.” Instance segmentation is more difficult: Here every cat gets their own label. (As an aside, why should that be more difficult? Presupposing human-like cognition, it wouldn’t be – if I have the concept of a cat, instead of just “cattiness,” I “see” there are two cats, not one. But depending on what a specific neural network relies on most – texture, color, isolated parts – those tasks may differ a lot in difficulty.)\nThe network architecture used in this post is adequate for class segmentation tasks and should be applicable to a vast number of practical, scientific as well as non-scientific applications. Speaking of network architecture, how should it look?\nIntroducing U-Net\nGiven their success in image classification, can’t we just use a classic architecture like Inception V[n], ResNet, ResNext … , whatever? The problem is, our task at hand – labeling every pixel – does not fit so well with the classic idea of a CNN. With convnets, the idea is to apply successive layers of convolution and pooling to build up feature maps of decreasing granularity, to finally arrive at an abstract level where we just say: “yep, a cat.” The counterpart being, we lose detail information: To the final classification, it does not matter whether the five pixels in the top-left area are black or white.\nIn practice, the classic architectures use (max) pooling or convolutions with stride > 1 to achieve those successive abstractions – necessarily resulting in decreased spatial resolution.\nSo how can we use a convnet and still preserve detail information? In their 2015 paper U-Net: Convolutional Networks for Biomedical Image Segmentation (Ronneberger, Fischer, and Brox 2015), Olaf Ronneberger et al. came up with what four years later, in 2019, is still the most popular approach. (Which is to say something, four years being a long time, in deep learning.)\nThe idea is stunningly simple. While successive encoding (convolution / max pooling) steps, as usual, reduce resolution, the subsequent decoding – we have to arrive at an output of size same as the input, as we want to label every pixel! – does not simply upsample from the most compressed layer. Instead, during upsampling, at every step we feed in information from the corresponding, in resolution, layer in the downsizing chain.\nFor U-Net, really a picture says more than many words:\n\n\n\nFigure 2: U-Net architecture from Ronneberger et al. 2015.\n\n\n\nAt each upsampling stage we concatenate the output from the previous layer with that from its counterpart in the compression stage. The final output is a mask of size the original image, obtained via 1x1-convolution; no final dense layer is required, instead the output layer is just a convolutional layer with a single filter.\nNow let’s actually train a U-Net. We’re going to use the unet package that lets you create a well-performing model in a single line:\n\n\nremotes::install_github(\"r-tensorflow/unet\")\nlibrary(unet)\n\n# takes additional parameters, including number of downsizing blocks, \n# number of filters to start with, and number of classes to identify\n# see ?unet for more info\nmodel <- unet(input_shape = c(128, 128, 3))\n\n\nSo we have a model, and it looks like we’ll be wanting to feed it 128x128 RGB images. Now how do we get these images?\nThe data\nTo illustrate how applications arise even outside the area of medical research, we’ll use as an example the Kaggle Carvana Image Masking Challenge. The task is to create a segmentation mask separating cars from background. For our current purpose, we only need train.zip and train_mask.zip from the archive provided for download. In the following, we assume those have been extracted to a subdirectory called data-raw.\nLet’s first take a look at some images and their associated segmentation masks.\n\n\n# libraries we're going to need later\nlibrary(keras)\nlibrary(tfdatasets)\nlibrary(tidyverse)\nlibrary(rsample)\nlibrary(reticulate)\n\nimages <- tibble(\n  img = list.files(here::here(\"data-raw/train\"), full.names = TRUE),\n  mask = list.files(here::here(\"data-raw/train_masks\"), full.names = TRUE)\n  ) %>% \n  sample_n(2) %>% \n  map(. %>% magick::image_read() %>% magick::image_resize(\"128x128\"))\n\nout <- magick::image_append(c(\n  magick::image_append(images$img, stack = TRUE), \n  magick::image_append(images$mask, stack = TRUE)\n  )\n)\n\n\n\n\n\nThe photos are RGB-space JPEGs, while the masks are black-and-white GIFs.\nWe split the data into a training and a validation set. We’ll use the latter to monitor generalization performance during training.\n\n\ndata <- tibble(\n  img = list.files(here::here(\"data-raw/train\"), full.names = TRUE),\n  mask = list.files(here::here(\"data-raw/train_masks\"), full.names = TRUE)\n)\n\ndata <- initial_split(data, prop = 0.8)\n\n\nTo feed the data to the network, we’ll use tfdatasets. All preprocessing will end up in a simple pipeline, but we’ll first go over the required actions step-by-step.\nPreprocessing pipeline\nThe first step is to read in the images, making use of the appropriate functions in tf$image.\n\n\ntraining_dataset <- training(data) %>%  \n  tensor_slices_dataset() %>% \n  dataset_map(~.x %>% list_modify(\n    # decode_jpeg yields a 3d tensor of shape (1280, 1918, 3)\n    img = tf$image$decode_jpeg(tf$io$read_file(.x$img)),\n    # decode_gif yields a 4d tensor of shape (1, 1280, 1918, 3),\n    # so we remove the unneeded batch dimension and all but one \n    # of the 3 (identical) channels\n    mask = tf$image$decode_gif(tf$io$read_file(.x$mask))[1,,,][,,1,drop=FALSE]\n  ))\n\n\nWhile constructing a preprocessing pipeline, it’s very useful to check intermediate results.\nIt’s easy to do using reticulate::as_iterator on the dataset:\n\n\nexample <- training_dataset %>% as_iterator() %>% iter_next()\nexample\n\n\n$img\ntf.Tensor(\n[[[243 244 239]\n  [243 244 239]\n  [243 244 239]\n  ...\n ...\n  ...\n  [175 179 178]\n  [175 179 178]\n  [175 179 178]]], shape=(1280, 1918, 3), dtype=uint8)\n\n$mask\ntf.Tensor(\n[[[0]\n  [0]\n  [0]\n  ...\n ...\n  ...\n  [0]\n  [0]\n  [0]]], shape=(1280, 1918, 1), dtype=uint8)\n\nWhile the uint8 datatype makes RGB values easy to read for humans, the network is going to expect floating point numbers. The following code converts its input and additionally, scales values to the interval [0,1):\n\n\ntraining_dataset <- training_dataset %>% \n  dataset_map(~.x %>% list_modify(\n    img = tf$image$convert_image_dtype(.x$img, dtype = tf$float32),\n    mask = tf$image$convert_image_dtype(.x$mask, dtype = tf$float32)\n  ))\n\n\nTo reduce computational cost, we resize the images to size 128x128. This will change the aspect ratio and thus, distort the images, but is not a problem with the given dataset.\n\n\ntraining_dataset <- training_dataset %>% \n  dataset_map(~.x %>% list_modify(\n    img = tf$image$resize(.x$img, size = shape(128, 128)),\n    mask = tf$image$resize(.x$mask, size = shape(128, 128))\n  ))\n\n\nNow, it’s well known that in deep learning, data augmentation is paramount. For segmentation, there’s one thing to consider, which is whether a transformation needs to be applied to the mask as well – this would be the case for e.g. rotations, or flipping. Here, results will be good enough applying just transformations that preserve positions:\n\n\nrandom_bsh <- function(img) {\n  img %>% \n    tf$image$random_brightness(max_delta = 0.3) %>% \n    tf$image$random_contrast(lower = 0.5, upper = 0.7) %>% \n    tf$image$random_saturation(lower = 0.5, upper = 0.7) %>% \n    # make sure we still are between 0 and 1\n    tf$clip_by_value(0, 1) \n}\n\ntraining_dataset <- training_dataset %>% \n  dataset_map(~.x %>% list_modify(\n    img = random_bsh(.x$img)\n  ))\n\n\nAgain, we can use as_iterator to see what these transformations do to our images:\n\n\nexample <- training_dataset %>% as_iterator() %>% iter_next()\nexample$img %>% as.array() %>% as.raster() %>% plot()\n\n\n\n\n\nHere’s the complete preprocessing pipeline.\n\n\ncreate_dataset <- function(data, train, batch_size = 32L) {\n  \n  dataset <- data %>% \n    tensor_slices_dataset() %>% \n    dataset_map(~.x %>% list_modify(\n      img = tf$image$decode_jpeg(tf$io$read_file(.x$img)),\n      mask = tf$image$decode_gif(tf$io$read_file(.x$mask))[1,,,][,,1,drop=FALSE]\n    )) %>% \n    dataset_map(~.x %>% list_modify(\n      img = tf$image$convert_image_dtype(.x$img, dtype = tf$float32),\n      mask = tf$image$convert_image_dtype(.x$mask, dtype = tf$float32)\n    )) %>% \n    dataset_map(~.x %>% list_modify(\n      img = tf$image$resize(.x$img, size = shape(128, 128)),\n      mask = tf$image$resize(.x$mask, size = shape(128, 128))\n    ))\n  \n  # data augmentation performed on training set only\n  if (train) {\n    dataset <- dataset %>% \n      dataset_map(~.x %>% list_modify(\n        img = random_bsh(.x$img)\n      )) \n  }\n  \n  # shuffling on training set only\n  if (train) {\n    dataset <- dataset %>% \n      dataset_shuffle(buffer_size = batch_size*128)\n  }\n  \n  # train in batches; batch size might need to be adapted depending on\n  # available memory\n  dataset <- dataset %>% \n    dataset_batch(batch_size)\n  \n  dataset %>% \n    # output needs to be unnamed\n    dataset_map(unname) \n}\n\n\nTraining and test set creation now is just a matter of two function calls.\n\n\ntraining_dataset <- create_dataset(training(data), train = TRUE)\nvalidation_dataset <- create_dataset(testing(data), train = FALSE)\n\n\nAnd we’re ready to train the model.\nTraining the model\nWe already showed how to create the model, but let’s repeat it here, and check model architecture:\n\n\nmodel <- unet(input_shape = c(128, 128, 3))\nsummary(model)\n\n\nModel: \"model\"\n______________________________________________________________________________________________\nLayer (type)                   Output Shape        Param #    Connected to                    \n==============================================================================================\ninput_1 (InputLayer)           [(None, 128, 128, 3 0                                          \n______________________________________________________________________________________________\nconv2d (Conv2D)                (None, 128, 128, 64 1792       input_1[0][0]                   \n______________________________________________________________________________________________\nconv2d_1 (Conv2D)              (None, 128, 128, 64 36928      conv2d[0][0]                    \n______________________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)   (None, 64, 64, 64)  0          conv2d_1[0][0]                  \n______________________________________________________________________________________________\nconv2d_2 (Conv2D)              (None, 64, 64, 128) 73856      max_pooling2d[0][0]             \n______________________________________________________________________________________________\nconv2d_3 (Conv2D)              (None, 64, 64, 128) 147584     conv2d_2[0][0]                  \n______________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D) (None, 32, 32, 128) 0          conv2d_3[0][0]                  \n______________________________________________________________________________________________\nconv2d_4 (Conv2D)              (None, 32, 32, 256) 295168     max_pooling2d_1[0][0]           \n______________________________________________________________________________________________\nconv2d_5 (Conv2D)              (None, 32, 32, 256) 590080     conv2d_4[0][0]                  \n______________________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D) (None, 16, 16, 256) 0          conv2d_5[0][0]                  \n______________________________________________________________________________________________\nconv2d_6 (Conv2D)              (None, 16, 16, 512) 1180160    max_pooling2d_2[0][0]           \n______________________________________________________________________________________________\nconv2d_7 (Conv2D)              (None, 16, 16, 512) 2359808    conv2d_6[0][0]                  \n______________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D) (None, 8, 8, 512)   0          conv2d_7[0][0]                  \n______________________________________________________________________________________________\ndropout (Dropout)              (None, 8, 8, 512)   0          max_pooling2d_3[0][0]           \n______________________________________________________________________________________________\nconv2d_8 (Conv2D)              (None, 8, 8, 1024)  4719616    dropout[0][0]                   \n______________________________________________________________________________________________\nconv2d_9 (Conv2D)              (None, 8, 8, 1024)  9438208    conv2d_8[0][0]                  \n______________________________________________________________________________________________\nconv2d_transpose (Conv2DTransp (None, 16, 16, 512) 2097664    conv2d_9[0][0]                  \n______________________________________________________________________________________________\nconcatenate (Concatenate)      (None, 16, 16, 1024 0          conv2d_7[0][0]                  \n                                                              conv2d_transpose[0][0]          \n______________________________________________________________________________________________\nconv2d_10 (Conv2D)             (None, 16, 16, 512) 4719104    concatenate[0][0]               \n______________________________________________________________________________________________\nconv2d_11 (Conv2D)             (None, 16, 16, 512) 2359808    conv2d_10[0][0]                 \n______________________________________________________________________________________________\nconv2d_transpose_1 (Conv2DTran (None, 32, 32, 256) 524544     conv2d_11[0][0]                 \n______________________________________________________________________________________________\nconcatenate_1 (Concatenate)    (None, 32, 32, 512) 0          conv2d_5[0][0]                  \n                                                              conv2d_transpose_1[0][0]        \n______________________________________________________________________________________________\nconv2d_12 (Conv2D)             (None, 32, 32, 256) 1179904    concatenate_1[0][0]             \n______________________________________________________________________________________________\nconv2d_13 (Conv2D)             (None, 32, 32, 256) 590080     conv2d_12[0][0]                 \n______________________________________________________________________________________________\nconv2d_transpose_2 (Conv2DTran (None, 64, 64, 128) 131200     conv2d_13[0][0]                 \n______________________________________________________________________________________________\nconcatenate_2 (Concatenate)    (None, 64, 64, 256) 0          conv2d_3[0][0]                  \n                                                              conv2d_transpose_2[0][0]        \n______________________________________________________________________________________________\nconv2d_14 (Conv2D)             (None, 64, 64, 128) 295040     concatenate_2[0][0]             \n______________________________________________________________________________________________\nconv2d_15 (Conv2D)             (None, 64, 64, 128) 147584     conv2d_14[0][0]                 \n______________________________________________________________________________________________\nconv2d_transpose_3 (Conv2DTran (None, 128, 128, 64 32832      conv2d_15[0][0]                 \n______________________________________________________________________________________________\nconcatenate_3 (Concatenate)    (None, 128, 128, 12 0          conv2d_1[0][0]                  \n                                                              conv2d_transpose_3[0][0]        \n______________________________________________________________________________________________\nconv2d_16 (Conv2D)             (None, 128, 128, 64 73792      concatenate_3[0][0]             \n______________________________________________________________________________________________\nconv2d_17 (Conv2D)             (None, 128, 128, 64 36928      conv2d_16[0][0]                 \n______________________________________________________________________________________________\nconv2d_18 (Conv2D)             (None, 128, 128, 1) 65         conv2d_17[0][0]                 \n==============================================================================================\nTotal params: 31,031,745\nTrainable params: 31,031,745\nNon-trainable params: 0\n______________________________________________________________________________________________\nThe “output shape” column shows the expected U-shape numerically: Width and height first go down, until we reach a minimum resolution of 8x8; they then go up again, until we’ve reached the original resolution. At the same time, the number of filters first goes up, then goes down again, until in the output layer we have a single filter. You can also see the concatenate layers appending information that comes from “below” to information that comes “laterally.”\nWhat should be the loss function here? We’re labeling each pixel, so each pixel contributes to the loss. We have a binary problem – each pixel may be “car” or “background” – so we want each output to be close to either 0 or 1. This makes binary_crossentropy the adequate loss function.\nDuring training, we keep track of classification accuracy as well as the dice coefficient, the evaluation metric used in the competition. The dice coefficient is a way to measure the proportion of correct classifications:\n\n\ndice <- custom_metric(\"dice\", function(y_true, y_pred, smooth = 1.0) {\n  y_true_f <- k_flatten(y_true)\n  y_pred_f <- k_flatten(y_pred)\n  intersection <- k_sum(y_true_f * y_pred_f)\n  (2 * intersection + smooth) / (k_sum(y_true_f) + k_sum(y_pred_f) + smooth)\n})\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(lr = 1e-5),\n  loss = \"binary_crossentropy\",\n  metrics = list(dice, metric_binary_accuracy)\n)\n\n\nFitting the model takes some time – how much, of course, will depend on your hardware.1 But the wait pays off: After five epochs, we saw a dice coefficient of ~ 0.87 on the validation set, and an accuracy of ~ 0.95.\nPredictions\nOf course, what we’re ultimately interested in are predictions. Let’s see a few masks generated for items from the validation set:\n\n\nbatch <- validation_dataset %>% as_iterator() %>% iter_next()\npredictions <- predict(model, batch)\n\nimages <- tibble(\n  image = batch[[1]] %>% array_branch(1),\n  predicted_mask = predictions[,,,1] %>% array_branch(1),\n  mask = batch[[2]][,,,1]  %>% array_branch(1)\n) %>% \n  sample_n(2) %>% \n  map_depth(2, function(x) {\n    as.raster(x) %>% magick::image_read()\n  }) %>% \n  map(~do.call(c, .x))\n\n\nout <- magick::image_append(c(\n  magick::image_append(images$mask, stack = TRUE),\n  magick::image_append(images$image, stack = TRUE), \n  magick::image_append(images$predicted_mask, stack = TRUE)\n  )\n)\n\nplot(out)\n\n\n\n\n\nFigure 3: From left to right: ground truth, input image, and predicted mask from U-Net.\n\n\n\nConclusion\nIf there were a competition for the highest sum of usefulness and architectural transparency, U-Net would certainly be a contender. Without much tuning, it’s possible to obtain decent results. If you’re able to put this model to use in your work, or if you have problems using it, let us know! Thanks for reading!\n\n\n\nRonneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” CoRR abs/1505.04597. http://arxiv.org/abs/1505.04597.\n\n\nExpect up to half an hour on a laptop CPU.↩︎\n",
    "preview": "posts/2019-08-23-unet/images/unet.png",
    "last_modified": "2024-11-21T15:51:47+00:00",
    "input_file": {},
    "preview_width": 1400,
    "preview_height": 932
  },
  {
    "path": "posts/2019-07-31-censored-data/",
    "title": "Modeling censored data with tfprobability",
    "description": "In this post we use tfprobability, the R interface to TensorFlow Probability, to model censored data. Again, the exposition is inspired by the treatment of this topic in Richard McElreath's Statistical Rethinking. Instead of cute cats though, we model immaterial entities from the cold world of technology: This post explores durations of CRAN package checks, a dataset that comes with Max Kuhn's parsnip.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-07-31",
    "categories": [
      "Bayesian Modeling",
      "TensorFlow/Keras"
    ],
    "contents": "\nNothing’s ever perfect, and data isn’t either. One type of “imperfection” is missing data, where some features are unobserved for some subjects. (A topic for another post.) Another is censored data, where an event whose characteristics we want to measure does not occur in the observation interval. The example in Richard McElreath’s Statistical Rethinking is time to adoption of cats in an animal shelter. If we fix an interval and observe wait times for those cats that actually did get adopted, our estimate will end up too optimistic: We don’t take into account those cats who weren’t adopted during this interval and thus, would have contributed wait times of length longer than the complete interval.\nIn this post, we use a slightly less emotional example which nonetheless may be of interest, especially to R package developers: time to completion of R CMD check, collected from CRAN and provided by the parsnip package as check_times. Here, the censored portion are those checks that errored out for whatever reason, i.e., for which the check did not complete.\nWhy do we care about the censored portion? In the cat adoption scenario, this is pretty obvious: We want to be able to get a realistic estimate for any unknown cat, not just those cats that will turn out to be “lucky”. How about check_times? Well, if your submission is one of those that errored out, you still care about how long you wait, so even though their percentage is low (< 1%) we don’t want to simply exclude them. Also, there is the possibility that the failing ones would have taken longer, had they run to completion, due to some intrinsic difference between both groups. Conversely, if failures were random, the longer-running checks would have a greater chance to get hit by an error. So here too, exluding the censored data may result in bias.\nHow can we model durations for that censored portion, where the “true duration” is unknown? Taking one step back, how can we model durations in general? Making as few assumptions as possible, the maximum entropy distribution for displacements (in space or time) is the exponential. Thus, for the checks that actually did complete, durations are assumed to be exponentially distributed.\nFor the others, all we know is that in a virtual world where the check completed, it would take at least as long as the given duration. This quantity can be modeled by the exponential complementary cumulative distribution function (CCDF). Why? A cumulative distribution function (CDF) indicates the probability that a value lower or equal to some reference point was reached; e.g., “the probability of durations <= 255 is 0.9”. Its complement, 1 - CDF, then gives the probability that a value will exceed than that reference point.\nLet’s see this in action.\nThe data\nThe following code works with the current stable releases of TensorFlow and TensorFlow Probability, which are 1.14 and 0.7, respectively. If you don’t have tfprobability installed, get it from Github:\n\n\nremotes::install_github(\"rstudio/tfprobability\")\n\n\nThese are the libraries we need. As of TensorFlow 1.14, we call tf$compat$v2$enable_v2_behavior() to run with eager execution.\n\n\nlibrary(tensorflow)\nlibrary(tfprobability)\nlibrary(parsnip)\nlibrary(tidyverse)\nlibrary(zeallot)\nlibrary(gridExtra)\nlibrary(HDInterval)\nlibrary(tidymodels)\nlibrary(survival)\n\ntf$compat$v2$enable_v2_behavior()\n\n\nBesides the check durations we want to model, check_times reports various features of the package in question, such as number of imported packages, number of dependencies, size of code and documentation files, etc. The status variable indicates whether the check completed or errored out.\n\n\ndf <- check_times %>% select(-package)\nglimpse(df)\n\n\nObservations: 13,626\nVariables: 24\n$ authors        <int> 1, 1, 1, 1, 5, 3, 2, 1, 4, 6, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ imports        <dbl> 0, 6, 0, 0, 3, 1, 0, 4, 0, 7, 0, 0, 0, 0, 3, 2, 14, 2, 2, 0…\n$ suggests       <dbl> 2, 4, 0, 0, 2, 0, 2, 2, 0, 0, 2, 8, 0, 0, 2, 0, 1, 3, 0, 0,…\n$ depends        <dbl> 3, 1, 6, 1, 1, 1, 5, 0, 1, 1, 6, 5, 0, 0, 0, 1, 1, 5, 0, 2,…\n$ Roxygen        <dbl> 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,…\n$ gh             <dbl> 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,…\n$ rforge         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ descr          <int> 217, 313, 269, 63, 223, 1031, 135, 344, 204, 335, 104, 163,…\n$ r_count        <int> 2, 20, 8, 0, 10, 10, 16, 3, 6, 14, 16, 4, 1, 1, 11, 5, 7, 1…\n$ r_size         <dbl> 0.029053, 0.046336, 0.078374, 0.000000, 0.019080, 0.032607,…\n$ ns_import      <dbl> 3, 15, 6, 0, 4, 5, 0, 4, 2, 10, 5, 6, 1, 0, 2, 2, 1, 11, 0,…\n$ ns_export      <dbl> 0, 19, 0, 0, 10, 0, 0, 2, 0, 9, 3, 4, 0, 1, 10, 0, 16, 0, 2…\n$ s3_methods     <dbl> 3, 0, 11, 0, 0, 0, 0, 2, 0, 23, 0, 0, 2, 5, 0, 4, 0, 0, 0, …\n$ s4_methods     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ doc_count      <int> 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ doc_size       <dbl> 0.000000, 0.019757, 0.038281, 0.000000, 0.007874, 0.000000,…\n$ src_count      <int> 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 3, 0, 0, 0, 0, 0, 0, 54, 0, 0…\n$ src_size       <dbl> 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000,…\n$ data_count     <int> 2, 0, 0, 3, 3, 1, 10, 0, 4, 2, 2, 146, 0, 0, 0, 0, 0, 10, 0…\n$ data_size      <dbl> 0.025292, 0.000000, 0.000000, 4.885864, 4.595504, 0.006500,…\n$ testthat_count <int> 0, 8, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0,…\n$ testthat_size  <dbl> 0.000000, 0.002496, 0.000000, 0.000000, 0.000000, 0.000000,…\n$ check_time     <dbl> 49, 101, 292, 21, 103, 46, 78, 91, 47, 196, 200, 169, 45, 2…\n$ status         <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\nOf these 13,626 observations, just 103 are censored:\n\n\ntable(df$status)\n\n\n0     1 \n103 13523 \nFor better readability, we’ll work with a subset of the columns. We use surv_reg to help us find a useful and interesting subset of predictors:\n\n\nsurvreg_fit <-\n  surv_reg(dist = \"exponential\") %>% \n  set_engine(\"survreg\") %>% \n  fit(Surv(check_time, status) ~ ., \n      data = df)\ntidy(survreg_fit) \n\n\n# A tibble: 23 x 7\n   term             estimate std.error statistic  p.value conf.low conf.high\n   <chr>               <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n 1 (Intercept)     3.86      0.0219     176.     0.             NA        NA\n 2 authors         0.0139    0.00580      2.40   1.65e- 2       NA        NA\n 3 imports         0.0606    0.00290     20.9    7.49e-97       NA        NA\n 4 suggests        0.0332    0.00358      9.28   1.73e-20       NA        NA\n 5 depends         0.118     0.00617     19.1    5.66e-81       NA        NA\n 6 Roxygen         0.0702    0.0209       3.36   7.87e- 4       NA        NA\n 7 gh              0.00898   0.0217       0.414  6.79e- 1       NA        NA\n 8 rforge          0.0232    0.0662       0.351  7.26e- 1       NA        NA\n 9 descr           0.000138  0.0000337    4.10   4.18e- 5       NA        NA\n10 r_count         0.00209   0.000525     3.98   7.03e- 5       NA        NA\n11 r_size          0.481     0.0819       5.87   4.28e- 9       NA        NA\n12 ns_import       0.00352   0.000896     3.93   8.48e- 5       NA        NA\n13 ns_export      -0.00161   0.000308    -5.24   1.57e- 7       NA        NA\n14 s3_methods      0.000449  0.000421     1.06   2.87e- 1       NA        NA\n15 s4_methods     -0.00154   0.00206     -0.745  4.56e- 1       NA        NA\n16 doc_count       0.0739    0.0117       6.33   2.44e-10       NA        NA\n17 doc_size        2.86      0.517        5.54   3.08e- 8       NA        NA\n18 src_count       0.0122    0.00127      9.58   9.96e-22       NA        NA\n19 src_size       -0.0242    0.0181      -1.34   1.82e- 1       NA        NA\n20 data_count      0.0000415 0.000980     0.0423 9.66e- 1       NA        NA\n21 data_size       0.0217    0.0135       1.61   1.08e- 1       NA        NA\n22 testthat_count -0.000128  0.00127     -0.101  9.20e- 1       NA        NA\n23 testthat_size   0.0108    0.0139       0.774  4.39e- 1       NA        NA\n\nIt seems that if we choose imports, depends, r_size, doc_size, ns_import and ns_export we end up with a mix of (comparatively) powerful predictors from different semantic spaces and of different scales.\nBefore pruning the dataframe, we save away the target variable. In our model and training setup, it is convenient to have censored and uncensored data stored separately, so here we create two target matrices instead of one:\n\n\n# check times for failed checks\n# _c stands for censored\ncheck_time_c <- df %>%\n  filter(status == 0) %>%\n  select(check_time) %>%\n  as.matrix()\n\n# check times for successful checks \ncheck_time_nc <- df %>%\n  filter(status == 1) %>%\n  select(check_time) %>%\n  as.matrix()\n\n\nNow we can zoom in on the variables of interest, setting up one dataframe for the censored data and one for the uncensored data each. All predictors are normalized to avoid overflow during sampling. 1 We add a column of 1s for use as an intercept.\n\n\ndf <- df %>% select(status,\n                    depends,\n                    imports,\n                    doc_size,\n                    r_size,\n                    ns_import,\n                    ns_export) %>%\n  mutate_at(.vars = 2:7, .funs = function(x) (x - min(x))/(max(x)-min(x))) %>%\n  add_column(intercept = rep(1, nrow(df)), .before = 1)\n\n# dataframe of predictors for censored data  \ndf_c <- df %>% filter(status == 0) %>% select(-status)\n# dataframe of predictors for non-censored data \ndf_nc <- df %>% filter(status == 1) %>% select(-status)\n\n\nThat’s it for preparations. But of course we’re curious. Do check times look different? Do predictors – the ones we chose – look different?\nComparing a few meaningful percentiles for both classes, we see that durations for uncompleted checks are higher than those for completed checks throughout, apart from the 100% percentile. It’s not surprising that given the enormous difference in sample size, maximum duration is higher for completed checks. Otherwise though, doesn’t it look like the errored-out package checks “were going to take longer”?\n\npercentiles: check time\n10%\n30%\n50%\n70%\n90%\n100%\ncompleted\n36\n54\n79\n115\n211\n1343\nnot completed\n42\n71\n97\n143\n293\n696\n\nHow about the predictors? We don’t see any differences for depends, the number of package dependencies (apart from, again, the higher maximum reached for packages whose check completed): 2\n\npercentiles: depends\n10%\n30%\n50%\n70%\n90%\n100%\ncompleted\n0\n1\n1\n2\n4\n12\nnot completed\n0\n1\n1\n2\n4\n7\n\nBut for all others, we see the same pattern as reported above for check_time. Number of packages imported is higher for censored data at all percentiles besides the maximum:\n\npercentiles: imports\n10%\n30%\n50%\n70%\n90%\n100%\ncompleted\n0\n0\n2\n4\n9\n43\nnot completed\n0\n1\n5\n8\n12\n22\n\nSame for ns_export, the estimated number of exported functions or methods:\n\npercentiles: ns_export\n10%\n30%\n50%\n70%\n90%\n100%\ncompleted\n0\n1\n2\n8\n26\n2547\nnot completed\n0\n1\n5\n13\n34\n336\n\nAs well as for ns_import, the estimated number of imported functions or methods:\n\npercentiles: ns_import\n10%\n30%\n50%\n70%\n90%\n100%\ncompleted\n0\n1\n3\n6\n19\n312\nnot completed\n0\n2\n5\n11\n23\n297\n\nSame pattern for r_size, the size on disk of files in the R directory:\n\npercentiles: r_size\n10%\n30%\n50%\n70%\n90%\n100%\ncompleted\n0.005\n0.015\n0.031\n0.063\n0.176\n3.746\nnot completed\n0.008\n0.019\n0.041\n0.097\n0.217\n2.148\n\nAnd finally, we see it for doc_size too, where doc_size is the size of .Rmd and .Rnw files:\n\npercentiles: doc_size\n10%\n30%\n50%\n70%\n90%\n100%\ncompleted\n0.000\n0.000\n0.000\n0.000\n0.023\n0.988\nnot completed\n0.000\n0.000\n0.000\n0.011\n0.042\n0.114\n\nGiven our task at hand – model check durations taking into account uncensored as well as censored data – we won’t dwell on differences between both groups any longer; nonetheless we thought it interesting to relate these numbers.\nSo now, back to work. We need to create a model.\nThe model\nAs explained in the introduction, for completed checks duration is modeled using an exponential PDF. This is as straightforward as adding tfd_exponential() to the model function, tfd_joint_distribution_sequential(). 3For the censored portion, we need the exponential CCDF. This one is not, as of today, easily added to the model. What we can do though is calculate its value ourselves and add it to the “main” model likelihood. We’ll see this below when discussing sampling; for now it means the model definition ends up straightforward as it only covers the non-censored data. It is made of just the said exponential PDF and priors for the regression parameters.\nAs for the latter, we use 0-centered, Gaussian priors for all parameters. Standard deviations of 1 turned out to work well. As the priors are all the same, instead of listing a bunch of tfd_normals, we can create them all at once as\n\n\ntfd_sample_distribution(tfd_normal(0, 1), sample_shape = 7)\n\n\nMean check time is modeled as an affine combination of the six predictors and the intercept. Here then is the complete model, instantiated using the uncensored data only:\n\n\nmodel <- function(data) {\n  tfd_joint_distribution_sequential(\n    list(\n      tfd_sample_distribution(tfd_normal(0, 1), sample_shape = 7),\n      function(betas)\n        tfd_independent(\n          tfd_exponential(\n            rate = 1 / tf$math$exp(tf$transpose(\n              tf$matmul(tf$cast(data, betas$dtype), tf$transpose(betas))))),\n          reinterpreted_batch_ndims = 1)))\n}\n\nm <- model(df_nc %>% as.matrix())\n\n\nAlways, we test if samples from that model have the expected shapes:\n\n\nsamples <- m %>% tfd_sample(2)\nsamples\n\n\n[[1]]\ntf.Tensor(\n[[ 1.4184642   0.17583323 -0.06547955 -0.2512014   0.1862184  -1.2662812\n   1.0231884 ]\n [-0.52142304 -1.0036682   2.2664437   1.29737     1.1123234   0.3810004\n   0.1663677 ]], shape=(2, 7), dtype=float32)\n\n[[2]]\ntf.Tensor(\n[[4.4954767  7.865639   1.8388556  ... 7.914391   2.8485563  3.859719  ]\n [1.549662   0.77833986 0.10015647 ... 0.40323067 3.42171    0.69368565]], shape=(2, 13523), dtype=float32)\nThis looks fine: We have a list of length two, one element for each distribution in the model. For both tensors, dimension 1 reflects the batch size (which we arbitrarily set to 2 in this test), while dimension 2 is 7 for the number of normal priors and 13523 for the number of durations predicted.\nHow likely are these samples?\n\n\nm %>% tfd_log_prob(samples)\n\n\ntf.Tensor([-32464.521   -7693.4023], shape=(2,), dtype=float32)\nHere too, the shape is correct, and the values look reasonable.\nThe next thing to do is define the target we want to optimize.\nOptimization target\nAbstractly, the thing to maximize is the log probility of the data – that is, the measured durations – under the model.\nNow here the data comes in two parts, and the target does as well. First, we have the non-censored data, for which\n\n\nm %>% tfd_log_prob(list(betas, tf$cast(target_nc, betas$dtype)))\n\n\nwill calculate the log probability. Second, to obtain log probability for the censored data we write a custom function that calculates the log of the exponential CCDF:\n\n\nget_exponential_lccdf <- function(betas, data, target) {\n  e <-  tfd_independent(tfd_exponential(rate = 1 / tf$math$exp(tf$transpose(tf$matmul(\n    tf$cast(data, betas$dtype), tf$transpose(betas)\n  )))),\n  reinterpreted_batch_ndims = 1)\n  cum_prob <- e %>% tfd_cdf(tf$cast(target, betas$dtype))\n  tf$math$log(1 - cum_prob)\n}\n\n\nBoth parts are combined in a little wrapper function that allows us to compare training including and excluding the censored data. We won’t do that in this post, but you might be interested to do it with your own data, especially if the ratio of censored and uncensored parts is a little less imbalanced.\n\n\nget_log_prob <-\n  function(target_nc,\n           censored_data = NULL,\n           target_c = NULL) {\n    log_prob <- function(betas) {\n      log_prob <-\n        m %>% tfd_log_prob(list(betas, tf$cast(target_nc, betas$dtype)))\n      potential <-\n        if (!is.null(censored_data) && !is.null(target_c))\n          get_exponential_lccdf(betas, censored_data, target_c)\n      else\n        0\n      log_prob + potential\n    }\n    log_prob\n  }\n\nlog_prob <-\n  get_log_prob(\n    check_time_nc %>% tf$transpose(),\n    df_c %>% as.matrix(),\n    check_time_c %>% tf$transpose()\n  )\n\n\nSampling\nWith model and target defined, we’re ready to do sampling.\n\n\nn_chains <- 4\nn_burnin <- 1000\nn_steps <- 1000\n\n# keep track of some diagnostic output, acceptance and step size\ntrace_fn <- function(state, pkr) {\n  list(\n    pkr$inner_results$is_accepted,\n    pkr$inner_results$accepted_results$step_size\n  )\n}\n\n# get shape of initial values \n# to start sampling without producing NaNs, we will feed the algorithm\n# tf$zeros_like(initial_betas)\n# instead \ninitial_betas <- (m %>% tfd_sample(n_chains))[[1]]\n\n\nFor the number of leapfrog steps and the step size, experimentation showed that a combination of 64 / 0.1 yielded reasonable results:\n\n\nhmc <- mcmc_hamiltonian_monte_carlo(\n  target_log_prob_fn = log_prob,\n  num_leapfrog_steps = 64,\n  step_size = 0.1\n) %>%\n  mcmc_simple_step_size_adaptation(target_accept_prob = 0.8,\n                                   num_adaptation_steps = n_burnin)\n\nrun_mcmc <- function(kernel) {\n  kernel %>% mcmc_sample_chain(\n    num_results = n_steps,\n    num_burnin_steps = n_burnin,\n    current_state = tf$ones_like(initial_betas),\n    trace_fn = trace_fn\n  )\n}\n\n# important for performance: run HMC in graph mode\nrun_mcmc <- tf_function(run_mcmc)\n\nres <- hmc %>% run_mcmc()\nsamples <- res$all_states\n\n\nResults\nBefore we inspect the chains, here is a quick look at the proportion of accepted steps and the per-parameter mean step size:\n\n\naccepted <- res$trace[[1]]\nas.numeric(accepted) %>% mean()\n\n\n0.995\n\n\nstep_size <- res$trace[[2]]\nas.numeric(step_size) %>% mean()\n\n\n0.004953894\nWe also store away effective sample sizes and the rhat metrics for later addition to the synopsis.\n\n\neffective_sample_size <- mcmc_effective_sample_size(samples) %>%\n  as.matrix() %>%\n  apply(2, mean)\npotential_scale_reduction <- mcmc_potential_scale_reduction(samples) %>%\n  as.numeric()\n\n\nWe then convert the samples tensor to an R array for use in postprocessing.\n\n\n# 2-item list, where each item has dim (1000, 4)\nsamples <- as.array(samples) %>% array_branch(margin = 3)\n\n\nHow well did the sampling work? The chains mix well, but for some parameters, autocorrelation is still pretty high.\n\n\nprep_tibble <- function(samples) {\n  as_tibble(samples,\n            .name_repair = ~ c(\"chain_1\", \"chain_2\", \"chain_3\", \"chain_4\")) %>%\n    add_column(sample = 1:n_steps) %>%\n    gather(key = \"chain\", value = \"value\",-sample)\n}\n\nplot_trace <- function(samples) {\n  prep_tibble(samples) %>%\n    ggplot(aes(x = sample, y = value, color = chain)) +\n    geom_line() +\n    theme_light() +\n    theme(\n      legend.position = \"none\",\n      axis.title = element_blank(),\n      axis.text = element_blank(),\n      axis.ticks = element_blank()\n    )\n}\n\nplot_traces <- function(samples) {\n  plots <- purrr::map(samples, plot_trace)\n  do.call(grid.arrange, plots)\n}\n\nplot_traces(samples)\n\n\n\n\n\nFigure 1: Trace plots for the 7 parameters.\n\n\n\nNow for a synopsis of posterior parameter statistics, including the usual per-parameter sampling indicators effective sample size and rhat.\n\n\nall_samples <- map(samples, as.vector)\n\nmeans <- map_dbl(all_samples, mean)\n\nsds <- map_dbl(all_samples, sd)\n\nhpdis <- map(all_samples, ~ hdi(.x) %>% t() %>% as_tibble())\n\nsummary <- tibble(\n  mean = means,\n  sd = sds,\n  hpdi = hpdis\n) %>% unnest() %>%\n  add_column(param = colnames(df_c), .after = FALSE) %>%\n  add_column(\n    n_effective = effective_sample_size,\n    rhat = potential_scale_reduction\n  )\n\nsummary\n\n\n# A tibble: 7 x 7\n  param       mean     sd  lower upper n_effective  rhat\n  <chr>      <dbl>  <dbl>  <dbl> <dbl>       <dbl> <dbl>\n1 intercept  4.05  0.0158  4.02   4.08       508.   1.17\n2 depends    1.34  0.0732  1.18   1.47      1000    1.00\n3 imports    2.89  0.121   2.65   3.12      1000    1.00\n4 doc_size   6.18  0.394   5.40   6.94       177.   1.01\n5 r_size     2.93  0.266   2.42   3.46       289.   1.00\n6 ns_import  1.54  0.274   0.987  2.06       387.   1.00\n7 ns_export -0.237 0.675  -1.53   1.10        66.8  1.01\n\n\n\nFigure 2: Posterior means and HPDIs.\n\n\n\nFrom the diagnostics and trace plots, the model seems to work reasonably well, but as there is no straightforward error metric involved, it’s hard to know if actual predictions would even land in an appropriate range.\nTo make sure they do, we inspect predictions from our model as well as from surv_reg.\nThis time, we also split the data into training and test sets. Here first are the predictions from surv_reg:\n\n\ntrain_test_split <- initial_split(check_times, strata = \"status\")\ncheck_time_train <- training(train_test_split)\ncheck_time_test <- testing(train_test_split)\n\nsurvreg_fit <-\n  surv_reg(dist = \"exponential\") %>% \n  set_engine(\"survreg\") %>% \n  fit(Surv(check_time, status) ~ depends + imports + doc_size + r_size + \n        ns_import + ns_export, \n      data = check_time_train)\nsurvreg_fit(sr_fit)\n\n\n# A tibble: 7 x 7\n  term         estimate std.error statistic  p.value conf.low conf.high\n  <chr>           <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)  4.05      0.0174     234.    0.             NA        NA\n2 depends      0.108     0.00701     15.4   3.40e-53       NA        NA\n3 imports      0.0660    0.00327     20.2   1.09e-90       NA        NA\n4 doc_size     7.76      0.543       14.3   2.24e-46       NA        NA\n5 r_size       0.812     0.0889       9.13  6.94e-20       NA        NA\n6 ns_import    0.00501   0.00103      4.85  1.22e- 6       NA        NA\n7 ns_export   -0.000212  0.000375    -0.566 5.71e- 1       NA        NA\n\n\nsurvreg_pred <- \n  predict(survreg_fit, check_time_test) %>% \n  bind_cols(check_time_test %>% select(check_time, status))  \n\nggplot(survreg_pred, aes(x = check_time, y = .pred, color = factor(status))) +\n  geom_point() + \n  coord_cartesian(ylim = c(0, 1400))\n\n\n\n\n\nFigure 3: Test set predictions from surv_reg. One outlier (of value 160421) is excluded via coord_cartesian() to avoid distorting the plot.\n\n\n\nFor the MCMC model, we re-train on just the training set and obtain the parameter summary. The code is analogous to the above and not shown here.\nWe can now predict on the test set, for simplicity just using the posterior means:\n\n\ndf <- check_time_test %>% select(\n                    depends,\n                    imports,\n                    doc_size,\n                    r_size,\n                    ns_import,\n                    ns_export) %>%\n  add_column(intercept = rep(1, nrow(check_time_test)), .before = 1)\n\nmcmc_pred <- df %>% as.matrix() %*% summary$mean %>% exp() %>% as.numeric()\nmcmc_pred <- check_time_test %>% select(check_time, status) %>%\n  add_column(.pred = mcmc_pred)\n\nggplot(mcmc_pred, aes(x = check_time, y = .pred, color = factor(status))) +\n  geom_point() + \n  coord_cartesian(ylim = c(0, 1400)) \n\n\n\n\n\nFigure 4: Test set predictions from the mcmc model. No outliers, just using same scale as above for comparison.\n\n\n\nThis looks good!\nWrapup\nWe’ve shown how to model censored data – or rather, a frequent subtype thereof involving durations – using tfprobability. The check_times data from parsnip were a fun choice, but this modeling technique may be even more useful when censoring is more substantial. Hopefully his post has provided some guidance on how to handle censored data in your own work. Thanks for reading!\n\nBy itself, the predictors being on different scales isn’t a problem. Here we need to normalize due to the log link used in the model above.↩︎\nHere and in the following tables, we report the unnormalized, original values as contained in check_times.↩︎\nFor a first introduction to MCMC sampling with tfprobability, see Tadpoles on TensorFlow: Hierarchical partial pooling with tfprobability↩︎\n",
    "preview": "posts/2019-07-31-censored-data/images/thumb_cropped.png",
    "last_modified": "2024-11-21T15:52:18+00:00",
    "input_file": {},
    "preview_width": 955,
    "preview_height": 396
  },
  {
    "path": "posts/2019-07-09-feature-columns/",
    "title": "TensorFlow feature columns: Transforming your data recipes-style",
    "description": "TensorFlow feature columns provide useful functionality for preprocessing categorical data and chaining transformations, like bucketization or feature crossing. From R, we use them in popular \"recipes\" style, creating and subsequently refining a feature specification. In this post, we show how using feature specs frees cognitive resources and lets you focus on what you really want to accomplish. What's more, because of its elegance, feature-spec code reads nice and is fun to write as well.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": {}
      },
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-07-09",
    "categories": [
      "TensorFlow/Keras",
      "Tabular Data"
    ],
    "contents": "\nIt’s 2019; no one doubts the effectiveness of deep learning in computer vision. Or natural language processing. With “normal,” Excel-style, a.k.a. tabular data however, the situation is different.\nBasically there are two cases: One, you have numeric data only. Then, creating the network is straightforward, and all will be about optimization and hyperparameter search. Two, you have a mix of numeric and categorical data, where categorical could be anything from ordered-numeric to symbolic (e.g., text). In this latter case, with categorical data entering the picture, there is an extremely nice idea you can make use of: embed what are equidistant symbols into a high-dimensional, numeric representation. In that new representation, we can define a distance metric that allows us to make statements like “cycling is closer to running than to baseball,” or “😃 is closer to 😂 than to 😠.” When not dealing with language data, this technique is referred to as entity embeddings.1\nNice as this sounds, why don’t we see entity embeddings used all the time? Well, creating a Keras network that processes a mix of numeric and categorical data used to require a bit of an effort. With TensorFlow’s new feature columns, usable from R through a combination of tfdatasets and keras, there is a much easier way to achieve this. What’s more, tfdatasets follows the popular recipes idiom to initialize, refine, and apply a feature specification %>%-style. And finally, there are ready-made steps for bucketizing a numeric column, or hashing it, or creating crossed columns to capture interactions.\nThis post introduces feature specs starting from a scenario where they don’t exist: basically, the status quo until very recently. Imagine you have a dataset like that from the Porto Seguro car insurance competition where some of the columns are numeric, and some are categorical. You want to train a fully connected network on it, with all categorical columns fed into embedding layers. How can you do that? We then contrast this with the feature spec way, which makes things a lot easier – especially when there’s a lot of categorical columns.\nIn a second applied example, we demonstrate the use of crossed columns on the rugged dataset from Richard McElreath’s rethinking package. Here, we also direct attention to a few technical details that are worth knowing about.\nMixing numeric data and embeddings, the pre-feature-spec way\nOur first example dataset is taken from Kaggle. Two years ago, Brazilian car insurance company Porto Seguro asked participants to predict how likely it is a car owner will file a claim based on a mix of characteristics collected during the previous year. The dataset is comparatively large – there are ~ 600,000 rows in the training set, with 57 predictors. Among others, features are named so as to indicate the type of the data – binary, categorical, or continuous/ordinal.\nWhile it’s common in competitions to try to reverse-engineer column meanings, here we just make use of the type of the data, and see how far that gets us.\nConcretely, this means we want to\nuse binary features just the way they are, as zeroes and ones,\nscale the remaining numeric features to mean 0 and variance 1, and\nembed the categorical variables (each one by itself).\nWe’ll then define a dense network to predict target, the binary outcome. So first, let’s see how we could get our data into shape, as well as build up the network, in a “manual,” pre-feature-columns way.\nWhen loading libraries, we already use the versions we’ll need very soon: Tensorflow 2 (>= beta 1), and the development (= Github) versions of tfdatasets and keras:\n\n\ntensorflow::install_tensorflow(version = \"2.0.0-beta1\")\n\nremotes::install_github(\"rstudio/tfdatasets\")\nremotes::install_github(\"rstudio/keras\")\n\nlibrary(keras)\nlibrary(tfdatasets)\nlibrary(readr)\nlibrary(dplyr)\n\n\nIn this first version of preparing the data, we make our lives easier by assigning different R types, based on what the features represent (categorical, binary, or numeric qualities):2\n\n\n# downloaded from https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data\npath <- \"train.csv\"\n\nporto <- read_csv(path) %>%\n  select(-id) %>%\n  # to obtain number of unique levels, later\n  mutate_at(vars(ends_with(\"cat\")), factor) %>%\n  # to easily keep them apart from the non-binary numeric data\n  mutate_at(vars(ends_with(\"bin\")), as.integer)\n\nporto %>% glimpse()\n\n\nObservations: 595,212\nVariables: 58\n$ target         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,…\n$ ps_ind_01      <dbl> 2, 1, 5, 0, 0, 5, 2, 5, 5, 1, 5, 2, 2, 1, 5, 5,…\n$ ps_ind_02_cat  <fct> 2, 1, 4, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,…\n$ ps_ind_03      <dbl> 5, 7, 9, 2, 0, 4, 3, 4, 3, 2, 2, 3, 1, 3, 11, 3…\n$ ps_ind_04_cat  <fct> 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,…\n$ ps_ind_05_cat  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ps_ind_06_bin  <int> 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ps_ind_07_bin  <int> 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,…\n$ ps_ind_08_bin  <int> 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,…\n$ ps_ind_09_bin  <int> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…\n$ ps_ind_10_bin  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ps_ind_11_bin  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ps_ind_12_bin  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ps_ind_13_bin  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ps_ind_14      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ps_ind_15      <dbl> 11, 3, 12, 8, 9, 6, 8, 13, 6, 4, 3, 9, 10, 12, …\n$ ps_ind_16_bin  <int> 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,…\n$ ps_ind_17_bin  <int> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ps_ind_18_bin  <int> 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,…\n$ ps_reg_01      <dbl> 0.7, 0.8, 0.0, 0.9, 0.7, 0.9, 0.6, 0.7, 0.9, 0.…\n$ ps_reg_02      <dbl> 0.2, 0.4, 0.0, 0.2, 0.6, 1.8, 0.1, 0.4, 0.7, 1.…\n$ ps_reg_03      <dbl> 0.7180703, 0.7660777, -1.0000000, 0.5809475, 0.…\n$ ps_car_01_cat  <fct> 10, 11, 7, 7, 11, 10, 6, 11, 10, 11, 11, 11, 6,…\n$ ps_car_02_cat  <fct> 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,…\n$ ps_car_03_cat  <fct> -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1…\n$ ps_car_04_cat  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 0, 0, 0, 0, 9,…\n$ ps_car_05_cat  <fct> 1, -1, -1, 1, -1, 0, 1, 0, 1, 0, -1, -1, -1, 1,…\n$ ps_car_06_cat  <fct> 4, 11, 14, 11, 14, 14, 11, 11, 14, 14, 13, 11, …\n$ ps_car_07_cat  <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ ps_car_08_cat  <fct> 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,…\n$ ps_car_09_cat  <fct> 0, 2, 2, 3, 2, 0, 0, 2, 0, 2, 2, 0, 2, 2, 2, 0,…\n$ ps_car_10_cat  <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ ps_car_11_cat  <fct> 12, 19, 60, 104, 82, 104, 99, 30, 68, 104, 20, …\n$ ps_car_11      <dbl> 2, 3, 1, 1, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 1, 2,…\n$ ps_car_12      <dbl> 0.4000000, 0.3162278, 0.3162278, 0.3741657, 0.3…\n$ ps_car_13      <dbl> 0.8836789, 0.6188165, 0.6415857, 0.5429488, 0.5…\n$ ps_car_14      <dbl> 0.3708099, 0.3887158, 0.3472751, 0.2949576, 0.3…\n$ ps_car_15      <dbl> 3.605551, 2.449490, 3.316625, 2.000000, 2.00000…\n$ ps_calc_01     <dbl> 0.6, 0.3, 0.5, 0.6, 0.4, 0.7, 0.2, 0.1, 0.9, 0.…\n$ ps_calc_02     <dbl> 0.5, 0.1, 0.7, 0.9, 0.6, 0.8, 0.6, 0.5, 0.8, 0.…\n$ ps_calc_03     <dbl> 0.2, 0.3, 0.1, 0.1, 0.0, 0.4, 0.5, 0.1, 0.6, 0.…\n$ ps_calc_04     <dbl> 3, 2, 2, 2, 2, 3, 2, 1, 3, 2, 2, 2, 4, 2, 3, 2,…\n$ ps_calc_05     <dbl> 1, 1, 2, 4, 2, 1, 2, 2, 1, 2, 3, 2, 1, 1, 1, 1,…\n$ ps_calc_06     <dbl> 10, 9, 9, 7, 6, 8, 8, 7, 7, 8, 8, 8, 8, 10, 8, …\n$ ps_calc_07     <dbl> 1, 5, 1, 1, 3, 2, 1, 1, 3, 2, 2, 2, 4, 1, 2, 5,…\n$ ps_calc_08     <dbl> 10, 8, 8, 8, 10, 11, 8, 6, 9, 9, 9, 10, 11, 8, …\n$ ps_calc_09     <dbl> 1, 1, 2, 4, 2, 3, 3, 1, 4, 1, 4, 1, 1, 3, 3, 2,…\n$ ps_calc_10     <dbl> 5, 7, 7, 2, 12, 8, 10, 13, 11, 11, 7, 8, 9, 8, …\n$ ps_calc_11     <dbl> 9, 3, 4, 2, 3, 4, 3, 7, 4, 3, 6, 9, 6, 2, 4, 5,…\n$ ps_calc_12     <dbl> 1, 1, 2, 2, 1, 2, 0, 1, 2, 5, 3, 2, 3, 0, 1, 2,…\n$ ps_calc_13     <dbl> 5, 1, 7, 4, 1, 0, 0, 3, 1, 0, 3, 1, 3, 4, 3, 6,…\n$ ps_calc_14     <dbl> 8, 9, 7, 9, 3, 9, 10, 6, 5, 6, 6, 10, 8, 3, 9, …\n$ ps_calc_15_bin <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ps_calc_16_bin <int> 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,…\n$ ps_calc_17_bin <int> 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,…\n$ ps_calc_18_bin <int> 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,…\n$ ps_calc_19_bin <int> 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,…\n$ ps_calc_20_bin <int> 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,…\nWe split off 25% for validation.\n\n\n# train-test split\nid_training <- sample.int(nrow(porto), size = 0.75*nrow(porto))\n\nx_train <- porto[id_training,] %>% select(-target)\nx_test <- porto[-id_training,] %>% select(-target)\ny_train <- porto[id_training, \"target\"]\ny_test <- porto[-id_training, \"target\"] \n\n\nThe only thing we want to do to the data before defining the network is scaling the numeric features. Binary and categorical features can stay as is, with the minor correction that for the categorical ones, we’ll actually pass the network the numeric representation of the factor data.\nHere is the scaling.\n\n\ntrain_means <- colMeans(x_train[sapply(x_train, is.double)]) %>% unname()\ntrain_sds <- apply(x_train[sapply(x_train, is.double)], 2, sd)  %>% unname()\ntrain_sds[train_sds == 0] <- 0.000001\n\nx_train[sapply(x_train, is.double)] <- sweep(\n  x_train[sapply(x_train, is.double)],\n  2,\n  train_means\n  ) %>%\n  sweep(2, train_sds, \"/\")\nx_test[sapply(x_test, is.double)] <- sweep(\n  x_test[sapply(x_test, is.double)],\n  2,\n  train_means\n  ) %>%\n  sweep(2, train_sds, \"/\")\n\n\nWhen building the network, we need to specify the input and output dimensionalities for the embedding layers. Input dimensionality refers to the number of different symbols that “come in”; in NLP tasks this would be the vocabulary size while here, it’s simply the number of values a variable can take.3\nOutput dimensionality, the capacity of the internal representation, can then be calculated based on some heuristic. Below, we’ll follow a popular rule of thumb that takes the square root of the dimensionality of the input.\nSo as part one of the network, here we build up the embedding layers in a loop, each wired to the input layer that feeds it:\n\n\n# number of levels per factor, required to specify input dimensionality for\n# the embedding layers\nn_levels_in <- map(x_train %>% select_if(is.factor), compose(length, levels)) %>%\n  unlist() \n\n# output dimensionality for the embedding layers, need +1 because Python is 0-based\nn_levels_out <- n_levels_in %>% sqrt() %>% trunc() %>% `+`(1)\n\n# each embedding layer gets its own input layer\ncat_inputs <- map(n_levels_in, function(l) layer_input(shape = 1)) %>%\n  unname()\n\n# construct the embedding layers, connecting each to its input\nembedding_layers <- vector(mode = \"list\", length = length(cat_inputs))\nfor (i in 1:length(cat_inputs)) {\n  embedding_layer <-  cat_inputs[[i]] %>% \n    layer_embedding(input_dim = n_levels_in[[i]] + 1, output_dim = n_levels_out[[i]]) %>%\n    layer_flatten()\n  embedding_layers[[i]] <- embedding_layer\n}\n\n\nIn case you were wondering about the flatten layer following each embedding: We need to squeeze out the third dimension (introduced by the embedding layers) from the tensors, effectively rendering them rank-2.\nThat is because we want to combine them with the rank-2 tensor coming out of the dense layer processing the numeric features.\nIn order to be able to combine it with anything, we have to actually construct that dense layer first. It will be connected to a single input layer, of shape 43, that takes in the numeric features we scaled as well as the binary features we left untouched:\n\n\n# create a single input and a dense layer for the numeric data\nquant_input <- layer_input(shape = 43)\n  \nquant_dense <- quant_input %>% layer_dense(units = 64)\n\n\nAre parts assembled, we wire them together using layer_concatenate, and we’re good to call keras_model to create the final graph.\n\n\nintermediate_layers <- list(embedding_layers, list(quant_dense)) %>% flatten()\ninputs <- list(cat_inputs, list(quant_input)) %>% flatten()\n\nl <- 0.25\n\noutput <- layer_concatenate(intermediate_layers) %>%\n  layer_dense(units = 30, activation = \"relu\", kernel_regularizer = regularizer_l2(l)) %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_dense(units = 10, activation = \"relu\", kernel_regularizer = regularizer_l2(l)) %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_dense(units = 5, activation = \"relu\", kernel_regularizer = regularizer_l2(l)) %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_dense(units = 1, activation = \"sigmoid\", kernel_regularizer = regularizer_l2(l))\n\nmodel <- keras_model(inputs, output)\n\n\nNow, if you’ve actually read through the whole of this part, you may wish for an easier way to get to this point. So let’s switch to feature specs for the rest of this post.\nFeature specs to the rescue\nIn spirit, the way feature specs are defined follows the example of the recipes package. (It won’t make you hungry, though.) You initialize a feature spec with the prediction target – feature_spec(target ~ .), and then use the %>% to tell it what to do with individual columns. “What to do” here signifies two things:\nFirst, how to “read in” the data. Are they numeric or categorical, and if categorical, what am I supposed to do with them? For example, should I treat all distinct symbols as distinct, resulting in, potentially, an enormous count of categories – or should I constrain myself to a fixed number of entities? Or hash them, even?\nSecond, optional subsequent transformations. Numeric columns may be bucketized; categorical columns may be embedded. Or features could be combined to capture interaction.\nIn this post, we demonstrate the use of a subset of step_ functions. The vignettes on Feature columns and Feature specs illustrate additional functions and their application.\nStarting from the beginning again, here is the complete code for data read-in and train-test split in the feature spec version.\n\n\nlibrary(keras)\nlibrary(tfdatasets)\nlibrary(readr)\nlibrary(dplyr)\n\npath <- \"train.csv\"\n\nporto <- read_csv(path)\n\nporto <- porto %>%\n  mutate_at(vars(ends_with(\"cat\")), as.character)\n\nid_training <- sample.int(nrow(porto), size = 0.75*nrow(porto))\ntraining <- porto[id_training,]\ntesting <- porto[-id_training,]\n\n\nData-prep-wise, recall what our goals are: leave alone if binary; scale if numeric; embed if categorical.\nSpecifying all of this does not need more than a few lines of code:\n\n\nft_spec <- training %>%\n  select(-id) %>%\n  feature_spec(target ~ .) %>%\n  step_numeric_column(ends_with(\"bin\")) %>%\n  step_numeric_column(-ends_with(\"bin\"),\n                      -ends_with(\"cat\"),\n                      normalizer_fn = scaler_standard()\n                      ) %>%\n  step_categorical_column_with_vocabulary_list(ends_with(\"cat\")) %>%\n  step_embedding_column(ends_with(\"cat\"),\n                        dimension = function(vocab_size) as.integer(sqrt(vocab_size) + 1)\n                        ) %>%\n  fit()\n\n\nNote how here we are passing in the training set, and just like with recipes, we won’t need to repeat any of the steps for the validation set. Scaling is taken care of by scaler_standard(), an optional transformation function passed in to step_numeric_column.\nCategorical columns are meant to make use of the complete vocabulary4 and pipe their outputs into embedding layers.\nNow, what actually happened when we called fit()? A lot – for us, as we got rid of a ton of manual preparation. For TensorFlow, nothing really – it just came to know about a few pieces in the graph we’ll ask it to construct.\nBut wait, – don’t we still have to build up that graph ourselves, connecting and concatenating layers?\nConcretely, above, we had to:\ncreate the correct number of input layers, of correct shape; and\nwire them to their matching embedding layers, of correct dimensionality.\nSo here comes the real magic, and it has two steps.\nFirst, we easily create the input layers by calling layer_input_from_dataset:\n`\n\n\ninputs <- layer_input_from_dataset(porto %>% select(-target))\n\n\nAnd second, we can extract the features from the feature spec and have layer_dense_features create the necessary layers based on that information:\n\n\nlayer_dense_features(ft_spec$dense_features())\n\n\nWithout further ado, we add a few dense layers, and there is our model. Magic!\n\n\noutput <- inputs %>%\n  layer_dense_features(ft_spec$dense_features()) %>%\n  layer_dense(units = 30, activation = \"relu\", kernel_regularizer = regularizer_l2(l)) %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_dense(units = 10, activation = \"relu\", kernel_regularizer = regularizer_l2(l)) %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_dense(units = 5, activation = \"relu\", kernel_regularizer = regularizer_l2(l)) %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_dense(units = 1, activation = \"sigmoid\", kernel_regularizer = regularizer_l2(l))\n\nmodel <- keras_model(inputs, output)\n\n\nHow do we feed this model? In the non-feature-columns example, we would have had to feed each input separately, passing a list of tensors. Now we can just pass it the complete training set all at once:\n\n\nmodel %>% fit(x = training, y = training$target)\n\n\nIn the Kaggle competition, submissions are evaluated using the normalized Gini coefficient, which we can calculate with the help of a new metric available in Keras, tf$keras$metrics$AUC(). For training, we can use an approximation to the AUC due to Yan et al. (2003) (Yan et al. 2003). Then training is as straightforward as:\n\n\nauc <- tf$keras$metrics$AUC()\n\ngini <- custom_metric(name = \"gini\", function(y_true, y_pred) {\n  2*auc(y_true, y_pred) - 1\n})\n\n# Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003). \n# Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\nroc_auc_score <- function(y_true, y_pred) {\n\n  pos = tf$boolean_mask(y_pred, tf$cast(y_true, tf$bool))\n  neg = tf$boolean_mask(y_pred, !tf$cast(y_true, tf$bool))\n\n  pos = tf$expand_dims(pos, 0L)\n  neg = tf$expand_dims(neg, 1L)\n\n  # original paper suggests performance is robust to exact parameter choice\n  gamma = 0.2\n  p     = 3\n\n  difference = tf$zeros_like(pos * neg) + pos - neg - gamma\n\n  masked = tf$boolean_mask(difference, difference < 0.0)\n\n  tf$reduce_sum(tf$pow(-masked, p))\n}\n\nmodel %>%\n  compile(\n    loss = roc_auc_score,\n    optimizer = optimizer_adam(),\n    metrics = list(auc, gini)\n  )\n\nmodel %>%\n  fit(\n    x = training,\n    y = training$target,\n    epochs = 50,\n    validation_data = list(testing, testing$target),\n    batch_size = 512\n  )\n\npredictions <- predict(model, testing)\nMetrics::auc(testing$target, predictions)\n\n\nAfter 50 epochs, we achieve an AUC of 0.64 on the validation set, or equivalently, a Gini coefficient of 0.27. Not a bad result for a simple fully connected network!\nWe’ve seen how using feature columns automates away a number of steps in setting up the network, so we can spend more time on actually tuning it. This is most impressively demonstrated on a dataset like this, with more than a handful categorical columns. However, to explain a bit more what to pay attention to when using feature columns, it’s better to choose a smaller example where we can easily do some peeking around.\nLet’s move on to the second application.\nInteractions, and what to look out for\nTo demonstrate the use of step_crossed_column to capture interactions, we make use of the rugged dataset from Richard McElreath’s rethinking package.\nWe want to predict log GDP based on terrain ruggedness, for a number of countries (170, to be precise). However, the effect of ruggedness is different in Africa as opposed to other continents. Citing from Statistical Rethinking\n\nIt makes sense that ruggedness is associated with poorer countries, in most of the world. Rugged terrain means transport is difficult. Which means market access is hampered. Which means reduced gross domestic product. So the reversed relationship within Africa is puzzling. Why should difficult terrain be associated with higher GDP per capita?\n\n\nIf this relationship is at all causal, it may be because rugged regions of Africa were protected against the Atlantic and Indian Ocean slave trades. Slavers preferred to raid easily accessed settlements, with easy routes to the sea. Those regions that suffered under the slave trade understandably continue to suffer economically, long after the decline of slave-trading markets. However, an outcome like GDP has many influences, and is furthermore a strange measure of economic activity. So it is hard to be sure what’s going on here.\n\nWhile the causal situation is difficult, the purely technical one is easily described: We want to learn an interaction. We could rely on the network finding out by itself (in this case it probably will, if we just give it enough parameters). But it’s an excellent occasion to showcase the new step_crossed_column.\nLoading the dataset, zooming in on the variables of interest, and normalizing them the way it is done in Rethinking, we have:\n\n\nlibrary(rethinking)\nlibrary(keras)\nlibrary(tfdatasets)\nlibrary(tibble)\n\ndata(rugged)\n\nd <- rugged\nd <- d[complete.cases(d$rgdppc_2000), ]\n\ndf <- tibble(\n  log_gdp = log(d$rgdppc_2000)/mean(log(d$rgdppc_2000)),\n  rugged = d$rugged/max(d$rugged),\n  africa = d$cont_africa\n)\n\ndf %>% glimpse()\n\n\nObservations: 170\nVariables: 3\n$ log_gdp <dbl> 0.8797119, 0.9647547, 1.1662705, 1.1044854, 0.9149038,…\n$ rugged  <dbl> 0.1383424702, 0.5525636891, 0.1239922606, 0.1249596904…\n$ africa  <int> 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, …\nNow, let’s first forget about the interaction and do the very minimal thing required to work with this data.\nrugged should be a numeric column, while africa is categorical in nature, which means we use one of the step_categorical_[...] functions on it. (In this case we happen to know there are just two categories, Africa and not-Africa, so we could as well treat the column as numeric like in the previous example; but in other applications that won’t be the case, so here we show a method that generalizes to categorical features in general.)\nSo we start out creating a feature spec and adding the two predictor columns. We check the result using feature_spec’s dense_features() method:\n\n\nft_spec <- training %>%\n  feature_spec(log_gdp ~ .) %>%\n  step_numeric_column(rugged) %>%\n  step_categorical_column_with_identity(africa, num_buckets = 2) \n  fit()\n\nft_spec$dense_features()\n\n\n$rugged\nNumericColumn(key='rugged', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)\nHm, that doesn’t look too good. Where’d africa go? In fact, there is one more thing we should have done: convert the categorical column to an indicator column. Why?\nThe rule of thumb is, whenever you have something categorical, including crossed, you need to then transform it into something numeric, which includes indicator and embedding.\nBeing a heuristic, this rule works overall, and it matches our intuition. There’s one exception though, step_bucketized_column, which although it “feels” categorical actually does not need that conversion.\nTherefore, it is best to supplement that intuition with a simple lookup diagram, which is also part of the feature columns vignette.\nWith this diagram, the simple rule is: We always need to end up with something that inherits from DenseColumn. So:\nstep_numeric_column, step_indicator_column, and step_embedding_column are standalone;\nstep_bucketized_column is, too, however categorical it “feels”; and\nall step_categorical_column_[...], as well as step_crossed_column, need to be transformed using one the dense column types.\n\n\n\nFigure 1: For use with Keras, all features need to end up inheriting from DenseColumn somehow.\n\n\n\nThus, we can fix the situation like so:\n\n\nft_spec <- training %>%\n  feature_spec(log_gdp ~ .) %>%\n  step_numeric_column(rugged) %>%\n  step_categorical_column_with_identity(africa, num_buckets = 2) %>%\n  step_indicator_column(africa) %>%\n  fit()\n\n\nand now ft_spec$dense_features() will show us\n$rugged\nNumericColumn(key='rugged', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)\n\n$indicator_africa\nIndicatorColumn(categorical_column=IdentityCategoricalColumn(key='africa', number_buckets=2.0, default_value=None))\n\nWhat we really wanted to do is capture the interaction between ruggedness and continent. To this end, we first bucketize rugged, and then cross it with – already binary – africa. As per the rules, we finally transform into an indicator column:\n\n\nft_spec <- training %>%\n  feature_spec(log_gdp ~ .) %>%\n  step_numeric_column(rugged) %>%\n  step_categorical_column_with_identity(africa, num_buckets = 2) %>%\n  step_indicator_column(africa) %>%\n  step_bucketized_column(rugged,\n                         boundaries = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.8)) %>%\n  step_crossed_column(africa_rugged_interact = c(africa, bucketized_rugged),\n                      hash_bucket_size = 16) %>%\n  step_indicator_column(africa_rugged_interact) %>%\n  fit()\n\n\nLooking at this code you may be asking yourself, now how many features do I have in the model?\nLet’s check.\n\n\nft_spec$dense_features()\n\n\n$rugged\nNumericColumn(key='rugged', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)\n\n$indicator_africa\nIndicatorColumn(categorical_column=IdentityCategoricalColumn(key='africa', number_buckets=2.0, default_value=None))\n\n$bucketized_rugged\nBucketizedColumn(source_column=NumericColumn(key='rugged', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.8))\n\n$indicator_africa_rugged_interact\nIndicatorColumn(categorical_column=CrossedColumn(keys=(IdentityCategoricalColumn(key='africa', number_buckets=2.0, default_value=None), BucketizedColumn(source_column=NumericColumn(key='rugged', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.8))), hash_bucket_size=16.0, hash_key=None))\nWe see that all features, original or transformed, are kept, as long as they inherit from DenseColumn.\nThis means that, for example, the non-bucketized, continuous values of rugged are used as well.\nNow setting up the training goes as expected.\n\n\ninputs <- layer_input_from_dataset(df %>% select(-log_gdp))\n\noutput <- inputs %>%\n  layer_dense_features(ft_spec$dense_features()) %>%\n  layer_dense(units = 8, activation = \"relu\") %>%\n  layer_dense(units = 8, activation = \"relu\") %>%\n  layer_dense(units = 1)\n\nmodel <- keras_model(inputs, output)\n\nmodel %>% compile(loss = \"mse\", optimizer = \"adam\", metrics = \"mse\")\n\nhistory <- model %>% fit(\n  x = training,\n  y = training$log_gdp,\n  validation_data = list(testing, testing$log_gdp),\n  epochs = 100)\n\n\nJust as a sanity check, the final loss on the validation set for this code was ~ 0.014. But really this example did serve different purposes.\nIn a nutshell\nFeature specs are a convenient, elegant way of making categorical data available to Keras, as well as to chain useful transformations like bucketizing and creating crossed columns. The time you save data wrangling may go into tuning and experimentation. Enjoy, and thanks for reading!\n\n\n\nYan, Lian, Robert H Dodier, Michael Mozer, and Richard H Wolniewicz. 2003. “Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.” In Proceedings of the 20th International Conference on Machine Learning (ICML-03), 848–55.\n\n\nsee e.g. Entity embeddings for fun and profit↩︎\nHaving mentioned, above, that these really may be continuous or ordinal, from now on we’ll just call them numeric as we won’t make further use of the distinction. However, fitting ordinal data with neural networks is definitely a topic that would deserve its own post.↩︎\nIn this dataset, every categorical variable is a column of singleton integers.↩︎\nas indicated by leaving out the optional vocabulary_list↩︎\n",
    "preview": "posts/2019-07-09-feature-columns/images/feature_cols_hier.png",
    "last_modified": "2024-11-21T15:49:54+00:00",
    "input_file": {},
    "preview_width": 1172,
    "preview_height": 678
  },
  {
    "path": "posts/2019-06-25-dynamic_linear_models_tfprobability/",
    "title": "Dynamic linear models with tfprobability",
    "description": "Previous posts featuring tfprobability - the R interface to TensorFlow Probability - have focused on enhancements to deep neural networks (e.g., introducing Bayesian uncertainty estimates) and fitting hierarchical models with Hamiltonian Monte Carlo. This time, we show how to fit time series using dynamic linear models (DLMs), yielding posterior predictive forecasts as well as the smoothed and filtered estimates from the Kálmán filter.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-06-24",
    "categories": [
      "Probabilistic ML/DL",
      "Time Series"
    ],
    "contents": "\nWelcome to the world of state space models.1 In this world, there is a latent process, hidden from our eyes; and there are observations we make about the things it produces. The process evolves due to some hidden logic (transition model); and the way it produces the observations follows some hidden logic (observation model). There is noise in process evolution, and there is noise in observation. If the transition and observation models both are linear, and the process as well as observation noise are Gaussian, we have a linear-Gaussian state space model (SSM). The task is to infer the latent state from the observations. The most famous technique is the Kálmán filter.\nIn practical applications, two characteristics of linear-Gaussian SSMs are especially attractive.\nFor one, they let us estimate dynamically changing parameters. In regression, the parameters can be viewed as a hidden state; we may thus have a slope and an intercept that vary over time. When parameters can vary, we speak of dynamic linear models (DLMs). This is the term we’ll use throughout this post when referring to this class of models.\nSecond, linear-Gaussian SSMs are useful in time-series forecasting because Gaussian processes can be added. A time series can thus be framed as, e.g. the sum of a linear trend and a process that varies seasonally.\nUsing tfprobability, the R wrapper to TensorFlow Probability, we illustrate both aspects here. Our first example will be on dynamic linear regression. In a detailed walkthrough, we show on how to fit such a model, how to obtain filtered, as well as smoothed, estimates of the coefficients, and how to obtain forecasts.\nOur second example then illustrates process additivity. This example will build on the first, and may also serve as a quick recap of the overall procedure.\nLet’s jump in.\nDynamic linear regression example: Capital Asset Pricing Model (CAPM)\nOur code builds on the recently released versions of TensorFlow and TensorFlow Probability: 1.14 and 0.7, respectively.\n\n\ndevtools::install_github(\"rstudio/tfprobability\")\n\n# also installs TensorFlow Probability v. 0.7\ntensorflow::install_tensorflow(version = \"1.14\")\n\nlibrary(tensorflow)\nlibrary(tfprobability)\n\nlibrary(tidyverse)\nlibrary(zeallot)\n\n\nNote how there’s one thing we used to do lately that we’re not doing here: We’re not enabling eager execution. We say why in a minute.\nOur example is taken from Petris et al.(2009)(Petris, Petrone, and Campagnoli 2009), chapter 3.2.7.2\nBesides introducing the dlm package, this book provides a nice introduction to the ideas behind DLMs in general.\nTo illustrate dynamic linear regression, the authors feature a dataset, originally from Berndt(1991)(Berndt 1991) that has monthly returns, collected from January 1978 to December 1987, for four different stocks, the 30-day Treasury Bill – standing in for a risk-free asset –, and the value-weighted average returns for all stocks listed at the New York and American Stock Exchanges, representing the overall market returns.\nLet’s take a look.\n\n\n# As the data does not seem to be available at the address given in Petris et al. any more,\n# we put it on the blog for download\n# download from: \n# https://github.com/rstudio/tensorflow-blog/blob/master/docs/posts/2019-06-25-dynamic_linear_models_tfprobability/data/capm.txt\"\ndf <- read_table(\n  \"capm.txt\",\n  col_types = list(X1 = col_date(format = \"%Y.%m\"))) %>%\n  rename(month = X1)\ndf %>% glimpse()\n\n\nObservations: 120\nVariables: 7\n$ month  <date> 1978-01-01, 1978-02-01, 1978-03-01, 1978-04-01, 1978-05-01, 19…\n$ MOBIL  <dbl> -0.046, -0.017, 0.049, 0.077, -0.011, -0.043, 0.028, 0.056, 0.0…\n$ IBM    <dbl> -0.029, -0.043, -0.063, 0.130, -0.018, -0.004, 0.092, 0.049, -0…\n$ WEYER  <dbl> -0.116, -0.135, 0.084, 0.144, -0.031, 0.005, 0.164, 0.039, -0.0…\n$ CITCRP <dbl> -0.115, -0.019, 0.059, 0.127, 0.005, 0.007, 0.032, 0.088, 0.011…\n$ MARKET <dbl> -0.045, 0.010, 0.050, 0.063, 0.067, 0.007, 0.071, 0.079, 0.002,…\n$ RKFREE <dbl> 0.00487, 0.00494, 0.00526, 0.00491, 0.00513, 0.00527, 0.00528, …\n\n\ndf %>% gather(key = \"symbol\", value = \"return\", -month) %>%\n  ggplot(aes(x = month, y = return, color = symbol)) +\n  geom_line() +\n  facet_grid(rows = vars(symbol), scales = \"free\")\n\n\n\n\n\nFigure 1: Monthly returns for selected stocks; data from Berndt (1991).\n\n\n\nThe Capital Asset Pricing Model then assumes a linear relationship between the excess returns of an asset under study and the excess returns of the market. For both, excess returns are obtained by subtracting the returns of the chosen risk-free asset; then, the scaling coefficient between them reveals the asset to either be an “aggressive” investment (slope > 1: changes in the market are amplified), or a conservative one (slope < 1: changes are damped).\nAssuming this relationship does not change over time, we can easily use lm to illustrate this. Following Petris et al. in zooming in on IBM as the asset under study, we have\n\n\n# excess returns of the asset under study\nibm <- df$IBM - df$RKFREE\n# market excess returns\nx <- df$MARKET - df$RKFREE\n\nfit <- lm(ibm ~ x)\nsummary(fit)\n\n\nCall:\nlm(formula = ibm ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.11850 -0.03327 -0.00263  0.03332  0.15042 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.0004896  0.0046400  -0.106    0.916    \nx            0.4568208  0.0675477   6.763 5.49e-10 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.05055 on 118 degrees of freedom\nMultiple R-squared:  0.2793,    Adjusted R-squared:  0.2732 \nF-statistic: 45.74 on 1 and 118 DF,  p-value: 5.489e-10\nSo IBM is found to be a conservative investment, the slope being ~ 0.5. But is this relationship stable over time?\nLet’s turn to tfprobability to investigate.\nWe want to use this example to demonstrate two essential applications of DLMs: obtaining smoothing and/or filtering estimates of the coefficients, as well as forecasting future values. So unlike Petris et al., we divide the dataset into a training and a testing part:3.\n\n\n# zoom in on ibm\nts <- ibm %>% matrix()\n# forecast 12 months\nn_forecast_steps <- 12\nts_train <- ts[1:(length(ts) - n_forecast_steps), 1, drop = FALSE]\n\n# make sure we work with float32 here\nts_train <- tf$cast(ts_train, tf$float32)\nts <- tf$cast(ts, tf$float32)\n\n\nWe now construct the model. sts_dynamic_linear_regression() does what we want:\n\n\n# define the model on the complete series\nlinreg <- ts %>%\n  sts_dynamic_linear_regression(\n    design_matrix = cbind(rep(1, length(x)), x) %>% tf$cast(tf$float32)\n  )\n\n\nWe pass it the column of excess market returns, plus a column of ones, following Petris et al.. Alternatively, we could center the single predictor – this would work just as well.\nHow are we going to train this model? Method-wise, we have a choice between variational inference (VI) and Hamiltonian Monte Carlo (HMC). We will see both. The second question is: Are we going to use graph mode or eager mode? As of today, for both VI and HMC, it is safest – and fastest – to run in graph mode, so this is the only technique we show. In a few weeks, or months, we should be able to prune a lot of sess$run()s from the code!\nNormally in posts, when presenting code we optimize for easy experimentation (meaning: line-by-line executability), not modularity. This time though, with an important number of evaluation statements involved, it’s easiest to pack not just the fitting, but the smoothing and forecasting as well into a function (which you could still step through if you wanted). For VI, we’ll have a fit _with_vi function that does it all. So when we now start explaining what it does, don’t type in the code just yet – it’ll all reappear nicely packed into that function, for you to copy and execute as a whole.\nFitting a time series with variational inference\nFitting with VI pretty much looks like training traditionally used to look in graph-mode TensorFlow. You define a loss – here it’s done using sts_build_factored_variational_loss() –, an optimizer, and an operation for the optimizer to reduce that loss:\n\n\noptimizer <- tf$compat$v1$train$AdamOptimizer(0.1)\n\n# only train on the training set!    \nloss_and_dists <- ts_train %>% sts_build_factored_variational_loss(model = model)\nvariational_loss <- loss_and_dists[[1]]\n\ntrain_op <- optimizer$minimize(variational_loss)\n\n\nNote how the loss is defined on the training set only, not the complete series.\nNow to actually train the model, we create a session and run that operation:\n\n\n with (tf$Session() %as% sess,  {\n      \n      sess$run(tf$compat$v1$global_variables_initializer())\n   \n      for (step in 1:n_iterations) {\n        res <- sess$run(train_op)\n        loss <- sess$run(variational_loss)\n        if (step %% 10 == 0)\n          cat(\"Loss: \", as.numeric(loss), \"\\n\")\n      }\n })\n\n\nGiven we have that session, let’s make use of it and compute all the estimates we desire.\nAgain, – the following snippets will end up in the fit_with_vi function, so don’t run them in isolation just yet.\nObtaining forecasts\nThe first thing we want for the model to give us are forecasts. In order to create them, it needs samples from the posterior. Luckily we already have the posterior distributions, returned from sts_build_factored_variational_loss, so let’s sample from them and pass them to sts_forecast:\n\n\nvariational_distributions <- loss_and_dists[[2]]\nposterior_samples <-\n  Map(\n    function(d) d %>% tfd_sample(n_param_samples),\n    variational_distributions %>% reticulate::py_to_r() %>% unname()\n  )\nforecast_dists <- ts_train %>% sts_forecast(model, posterior_samples, n_forecast_steps)\n\n\nsts_forecast() returns distributions, so we call tfd_mean() to get the posterior predictions and tfd_stddev() for the corresponding standard deviations:\n\n\nfc_means <- forecast_dists %>% tfd_mean()\nfc_sds <- forecast_dists %>% tfd_stddev()\n\n\nBy the way – as we have the full posterior distributions, we are by no means restricted to summary statistics! We could easily use tfd_sample() to obtain individual forecasts.\nSmoothing and filtering (Kálmán filter)\nNow, the second (and last, for this example) thing we will want are the smoothed and filtered regression coefficients. The famous Kálmán Filter is a Bayesian-in-spirit method where at each time step, predictions are corrected by how much they differ from an incoming observation. Filtering estimates are based on observations we’ve seen so far; smoothing estimates are computed “in hindsight,” making use of the complete time series.\nWe first create a state space model from our time series definition:\n\n\n# only do this on the training set\n# returns an instance of tfd_linear_gaussian_state_space_model()\nssm <- model$make_state_space_model(length(ts_train), param_vals = posterior_samples)\n\n\ntfd_linear_gaussian_state_space_model(), technically a distribution, provides the Kálmán filter functionalities of smoothing and filtering.\nTo obtain the smoothed estimates:\n\n\nc(smoothed_means, smoothed_covs) %<-% ssm$posterior_marginals(ts_train)\n\n\nAnd the filtered ones:\n\n\nc(., filtered_means, filtered_covs, ., ., ., .) %<-% ssm$forward_filter(ts_train)\n\n\nFinally, we need to evaluate all those.\n\n\nc(posterior_samples, fc_means, fc_sds, smoothed_means, smoothed_covs, filtered_means, filtered_covs) %<-%\n  sess$run(list(posterior_samples, fc_means, fc_sds, smoothed_means, smoothed_covs, filtered_means, filtered_covs))\n\n\nPutting it all together (the VI edition)\nSo here’s the complete function, fit_with_vi, ready for us to call.\n\n\nfit_with_vi <-\n  function(ts,\n           ts_train,\n           model,\n           n_iterations,\n           n_param_samples,\n           n_forecast_steps,\n           n_forecast_samples) {\n    \n    optimizer <- tf$compat$v1$train$AdamOptimizer(0.1)\n    \n    loss_and_dists <-\n      ts_train %>% sts_build_factored_variational_loss(model = model)\n    variational_loss <- loss_and_dists[[1]]\n    train_op <- optimizer$minimize(variational_loss)\n    \n    with (tf$Session() %as% sess,  {\n      \n      sess$run(tf$compat$v1$global_variables_initializer())\n      for (step in 1:n_iterations) {\n        sess$run(train_op)\n        loss <- sess$run(variational_loss)\n        if (step %% 1 == 0)\n          cat(\"Loss: \", as.numeric(loss), \"\\n\")\n      }\n      variational_distributions <- loss_and_dists[[2]]\n      posterior_samples <-\n        Map(\n          function(d)\n            d %>% tfd_sample(n_param_samples),\n          variational_distributions %>% reticulate::py_to_r() %>% unname()\n        )\n      forecast_dists <-\n        ts_train %>% sts_forecast(model, posterior_samples, n_forecast_steps)\n      fc_means <- forecast_dists %>% tfd_mean()\n      fc_sds <- forecast_dists %>% tfd_stddev()\n      \n      ssm <- model$make_state_space_model(length(ts_train), param_vals = posterior_samples)\n      c(smoothed_means, smoothed_covs) %<-% ssm$posterior_marginals(ts_train)\n      c(., filtered_means, filtered_covs, ., ., ., .) %<-% ssm$forward_filter(ts_train)\n   \n      c(posterior_samples, fc_means, fc_sds, smoothed_means, smoothed_covs, filtered_means, filtered_covs) %<-%\n        sess$run(list(posterior_samples, fc_means, fc_sds, smoothed_means, smoothed_covs, filtered_means, filtered_covs))\n      \n    })\n    \n    list(\n      variational_distributions,\n      posterior_samples,\n      fc_means[, 1],\n      fc_sds[, 1],\n      smoothed_means,\n      smoothed_covs,\n      filtered_means,\n      filtered_covs\n    )\n  }\n\n\nAnd this is how we call it.\n\n\n# number of VI steps\nn_iterations <- 300\n# sample size for posterior samples\nn_param_samples <- 50\n# sample size to draw from the forecast distribution\nn_forecast_samples <- 50\n\n# here's the model again\nmodel <- ts %>%\n  sts_dynamic_linear_regression(design_matrix = cbind(rep(1, length(x)), x) %>% tf$cast(tf$float32))\n\n# call fit_vi defined above\nc(\n  param_distributions,\n  param_samples,\n  fc_means,\n  fc_sds,\n  smoothed_means,\n  smoothed_covs,\n  filtered_means,\n  filtered_covs\n) %<-% fit_vi(\n  ts,\n  ts_train,\n  model,\n  n_iterations,\n  n_param_samples,\n  n_forecast_steps,\n  n_forecast_samples\n)\n\n\nCurious about the results? We’ll see them in a second, but before let’s just quickly glance at the alternative training method: HMC.\nPutting it all together (the HMC edition)\ntfprobability provides sts_fit_with_hmc to fit a DLM using Hamiltonian Monte Carlo. Recent posts (e.g., Hierarchical partial pooling, continued: Varying slopes models with TensorFlow Probability) showed how to set up HMC to fit hierarchical models; here a single function does it all.\nHere is fit_with_hmc, wrapping sts_fit_with_hmc as well as the (unchanged) techniques for obtaining forecasts and smoothed/filtered parameters:\n\n\nnum_results <- 200\nnum_warmup_steps <- 100\n\nfit_hmc <- function(ts,\n                    ts_train,\n                    model,\n                    num_results,\n                    num_warmup_steps,\n                    n_forecast,\n                    n_forecast_samples) {\n  \n  states_and_results <-\n    ts_train %>% sts_fit_with_hmc(\n      model,\n      num_results = num_results,\n      num_warmup_steps = num_warmup_steps,\n      num_variational_steps = num_results + num_warmup_steps\n    )\n  \n  posterior_samples <- states_and_results[[1]]\n  forecast_dists <-\n    ts_train %>% sts_forecast(model, posterior_samples, n_forecast_steps)\n  fc_means <- forecast_dists %>% tfd_mean()\n  fc_sds <- forecast_dists %>% tfd_stddev()\n  \n  ssm <-\n    model$make_state_space_model(length(ts_train), param_vals = posterior_samples)\n  c(smoothed_means, smoothed_covs) %<-% ssm$posterior_marginals(ts_train)\n  c(., filtered_means, filtered_covs, ., ., ., .) %<-% ssm$forward_filter(ts_train)\n  \n  with (tf$Session() %as% sess,  {\n    sess$run(tf$compat$v1$global_variables_initializer())\n    c(\n      posterior_samples,\n      fc_means,\n      fc_sds,\n      smoothed_means,\n      smoothed_covs,\n      filtered_means,\n      filtered_covs\n    ) %<-%\n      sess$run(\n        list(\n          posterior_samples,\n          fc_means,\n          fc_sds,\n          smoothed_means,\n          smoothed_covs,\n          filtered_means,\n          filtered_covs\n        )\n      )\n  })\n  \n  list(\n    posterior_samples,\n    fc_means[, 1],\n    fc_sds[, 1],\n    smoothed_means,\n    smoothed_covs,\n    filtered_means,\n    filtered_covs\n  )\n}\n\nc(\n  param_samples,\n  fc_means,\n  fc_sds,\n  smoothed_means,\n  smoothed_covs,\n  filtered_means,\n  filtered_covs\n) %<-% fit_hmc(ts,\n               ts_train,\n               model,\n               num_results,\n               num_warmup_steps,\n               n_forecast,\n               n_forecast_samples)\n\n\nNow finally, let’s take a look at the forecasts and filtering resp. smoothing estimates.\nForecasts\nPutting all we need into one dataframe, we have\n\n\nsmoothed_means_intercept <- smoothed_means[, , 1] %>% colMeans()\nsmoothed_means_slope <- smoothed_means[, , 2] %>% colMeans()\n\nsmoothed_sds_intercept <- smoothed_covs[, , 1, 1] %>% colMeans() %>% sqrt()\nsmoothed_sds_slope <- smoothed_covs[, , 2, 2] %>% colMeans() %>% sqrt()\n\nfiltered_means_intercept <- filtered_means[, , 1] %>% colMeans()\nfiltered_means_slope <- filtered_means[, , 2] %>% colMeans()\n\nfiltered_sds_intercept <- filtered_covs[, , 1, 1] %>% colMeans() %>% sqrt()\nfiltered_sds_slope <- filtered_covs[, , 2, 2] %>% colMeans() %>% sqrt()\n\nforecast_df <- df %>%\n  select(month, IBM) %>%\n  add_column(pred_mean = c(rep(NA, length(ts_train)), fc_means)) %>%\n  add_column(pred_sd = c(rep(NA, length(ts_train)), fc_sds)) %>%\n  add_column(smoothed_means_intercept = c(smoothed_means_intercept, rep(NA, n_forecast_steps))) %>%\n  add_column(smoothed_means_slope = c(smoothed_means_slope, rep(NA, n_forecast_steps))) %>%\n  add_column(smoothed_sds_intercept = c(smoothed_sds_intercept, rep(NA, n_forecast_steps))) %>%\n  add_column(smoothed_sds_slope = c(smoothed_sds_slope, rep(NA, n_forecast_steps))) %>%\n  add_column(filtered_means_intercept = c(filtered_means_intercept, rep(NA, n_forecast_steps))) %>%\n  add_column(filtered_means_slope = c(filtered_means_slope, rep(NA, n_forecast_steps))) %>%\n  add_column(filtered_sds_intercept = c(filtered_sds_intercept, rep(NA, n_forecast_steps))) %>%\n  add_column(filtered_sds_slope = c(filtered_sds_slope, rep(NA, n_forecast_steps)))\n\n\nSo here first are the forecasts. We’re using the estimates returned from VI, but we could just as well have used those from HMC – they’re nearly indistinguishable. The same goes for the filtering and smoothing estimates displayed below.\n\n\nggplot(forecast_df, aes(x = month, y = IBM)) +\n  geom_line(color = \"grey\") +\n  geom_line(aes(y = pred_mean), color = \"cyan\") +\n  geom_ribbon(\n    aes(ymin = pred_mean - 2 * pred_sd, ymax = pred_mean + 2 * pred_sd),\n    alpha = 0.2,\n    fill = \"cyan\"\n  ) +\n  theme(axis.title = element_blank())\n\n\n\n\n\nFigure 2: 12-point-ahead forecasts for IBM; posterior means +/- 2 standard deviations.\n\n\n\nSmoothing estimates\nHere are the smoothing estimates. The intercept (shown in orange) remains pretty stable over time, but we do see a trend in the slope (displayed in green).4\n\n\nggplot(forecast_df, aes(x = month, y = smoothed_means_intercept)) +\n  geom_line(color = \"orange\") +\n  geom_line(aes(y = smoothed_means_slope),\n            color = \"green\") +\n  geom_ribbon(\n    aes(\n      ymin = smoothed_means_intercept - 2 * smoothed_sds_intercept,\n      ymax = smoothed_means_intercept + 2 * smoothed_sds_intercept\n    ),\n    alpha = 0.3,\n    fill = \"orange\"\n  ) +\n  geom_ribbon(\n    aes(\n      ymin = smoothed_means_slope - 2 * smoothed_sds_slope,\n      ymax = smoothed_means_slope + 2 * smoothed_sds_slope\n    ),\n    alpha = 0.1,\n    fill = \"green\"\n  ) +\n  coord_cartesian(xlim = c(forecast_df$month[1], forecast_df$month[length(ts) - n_forecast_steps]))  +\n  theme(axis.title = element_blank())\n\n\n\n\n\nFigure 3: Smoothing estimates from the Kálmán filter. Green: coefficient for dependence on excess market returns (slope), orange: vector of ones (intercept).\n\n\n\nFiltering estimates\nFor comparison, here are the filtering estimates. Note that the y-axis extends further up and down, so we can capture uncertainty better:5\n\n\nggplot(forecast_df, aes(x = month, y = filtered_means_intercept)) +\n  geom_line(color = \"orange\") +\n  geom_line(aes(y = filtered_means_slope),\n            color = \"green\") +\n  geom_ribbon(\n    aes(\n      ymin = filtered_means_intercept - 2 * filtered_sds_intercept,\n      ymax = filtered_means_intercept + 2 * filtered_sds_intercept\n    ),\n    alpha = 0.3,\n    fill = \"orange\"\n  ) +\n  geom_ribbon(\n    aes(\n      ymin = filtered_means_slope - 2 * filtered_sds_slope,\n      ymax = filtered_means_slope + 2 * filtered_sds_slope\n    ),\n    alpha = 0.1,\n    fill = \"green\"\n  ) +\n  coord_cartesian(ylim = c(-2, 2),\n                  xlim = c(forecast_df$month[1], forecast_df$month[length(ts) - n_forecast_steps])) +\n  theme(axis.title = element_blank())\n\n\n\n\n\nFigure 4: Filtering estimates from the Kálmán filter. Green: coefficient for dependence on excess market returns (slope), orange: vector of ones (intercept).\n\n\n\nSo far, we’ve seen a full example of time-series fitting, forecasting, and smoothing/filtering, in an exciting setting one doesn’t encounter too often: dynamic linear regression. What we haven’t seen as yet is the additivity feature of DLMs, and how it allows us to decompose a time series into its (theorized) constituents.\nLet’s do this next, in our second example, anti-climactically making use of the iris of time series, AirPassengers. Any guesses what components the model might presuppose?\n\n\n\nFigure 5: AirPassengers.\n\n\n\nComposition example: AirPassengers\nLibraries loaded, we prepare the data for tfprobability:\n\n\nlibrary(tensorflow)\nlibrary(tfprobability)\n\nlibrary(tidyverse)\nlibrary(zeallot)\nlibrary(lubridate)\n\nmonth <- seq(ymd(\"1949/1/1\"), ymd(\"1960/12/31\"), by = \"month\")\ndf <- data.frame(month) %>% add_column(n = matrix(AirPassengers))\n\n# train-test split\nts <- df$n %>% matrix()\nn_forecast_steps <- 12\nts_train <- ts[1:(length(ts) - n_forecast_steps), 1, drop = FALSE]\n\n# cast to float32\nts_train <- tf$cast(ts_train, tf$float32)\nts <- tf$cast(ts, tf$float32)\n\n\nThe model is a sum – cf. sts_sum – of a linear trend and a seasonal component:\n\n\nlinear_trend <- ts %>% sts_local_linear_trend()\nmonthly <- ts %>% sts_seasonal(num_seasons = 12)\nmodel <- ts %>% sts_sum(components = list(monthly, linear_trend))\n\n\nAgain, we could use VI as well as MCMC to train the model. Here’s the VI way:6\n\n\nn_iterations <- 100\nn_param_samples <- 50\nn_forecast_samples <- 50\n\noptimizer <- tf$compat$v1$train$AdamOptimizer(0.1)\n\nfit_vi <-\n  function(ts,\n           ts_train,\n           model,\n           n_iterations,\n           n_param_samples,\n           n_forecast_steps,\n           n_forecast_samples) {\n    loss_and_dists <-\n      ts_train %>% sts_build_factored_variational_loss(model = model)\n    variational_loss <- loss_and_dists[[1]]\n    train_op <- optimizer$minimize(variational_loss)\n    \n    with (tf$Session() %as% sess,  {\n      sess$run(tf$compat$v1$global_variables_initializer())\n      for (step in 1:n_iterations) {\n        res <- sess$run(train_op)\n        loss <- sess$run(variational_loss)\n        if (step %% 1 == 0)\n          cat(\"Loss: \", as.numeric(loss), \"\\n\")\n      }\n      variational_distributions <- loss_and_dists[[2]]\n      posterior_samples <-\n        Map(\n          function(d)\n            d %>% tfd_sample(n_param_samples),\n          variational_distributions %>% reticulate::py_to_r() %>% unname()\n        )\n      forecast_dists <-\n        ts_train %>% sts_forecast(model, posterior_samples, n_forecast_steps)\n      fc_means <- forecast_dists %>% tfd_mean()\n      fc_sds <- forecast_dists %>% tfd_stddev()\n      \n      c(posterior_samples,\n        fc_means,\n        fc_sds) %<-%\n        sess$run(list(posterior_samples,\n                      fc_means,\n                      fc_sds))\n    })\n    \n    list(variational_distributions,\n         posterior_samples,\n         fc_means[, 1],\n         fc_sds[, 1])\n  }\n\nc(param_distributions,\n  param_samples,\n  fc_means,\n  fc_sds) %<-% fit_vi(\n    ts,\n    ts_train,\n    model,\n    n_iterations,\n    n_param_samples,\n    n_forecast_steps,\n    n_forecast_samples\n  )\n\n\nFor brevity, we haven’t computed smoothed and/or filtered estimates for the overall model. In this example, this being a sum model, we want to show something else instead: the way it decomposes into components.\nBut first, the forecasts:\n\n\nforecast_df <- df %>%\n  add_column(pred_mean = c(rep(NA, length(ts_train)), fc_means)) %>%\n  add_column(pred_sd = c(rep(NA, length(ts_train)), fc_sds))\n\n\nggplot(forecast_df, aes(x = month, y = n)) +\n  geom_line(color = \"grey\") +\n  geom_line(aes(y = pred_mean), color = \"cyan\") +\n  geom_ribbon(\n    aes(ymin = pred_mean - 2 * pred_sd, ymax = pred_mean + 2 * pred_sd),\n    alpha = 0.2,\n    fill = \"cyan\"\n  ) +\n  theme(axis.title = element_blank())\n\n\n\n\n\nFigure 6: AirPassengers, 12-months-ahead forecast.\n\n\n\nA call to sts_decompose_by_component yields the (centered) components, a linear trend and a seasonal factor:\n\n\ncomponent_dists <-\n  ts_train %>% sts_decompose_by_component(model = model, parameter_samples = param_samples)\n\nseasonal_effect_means <- component_dists[[1]] %>% tfd_mean()\nseasonal_effect_sds <- component_dists[[1]] %>% tfd_stddev()\nlinear_effect_means <- component_dists[[2]] %>% tfd_mean()\nlinear_effect_sds <- component_dists[[2]] %>% tfd_stddev()\n\nwith(tf$Session() %as% sess, {\n  c(\n    seasonal_effect_means,\n    seasonal_effect_sds,\n    linear_effect_means,\n    linear_effect_sds\n  ) %<-% sess$run(\n    list(\n      seasonal_effect_means,\n      seasonal_effect_sds,\n      linear_effect_means,\n      linear_effect_sds\n    )\n  )\n})\n\ncomponents_df <- forecast_df %>%\n  add_column(seasonal_effect_means = c(seasonal_effect_means, rep(NA, n_forecast_steps))) %>%\n  add_column(seasonal_effect_sds = c(seasonal_effect_sds, rep(NA, n_forecast_steps))) %>%\n  add_column(linear_effect_means = c(linear_effect_means, rep(NA, n_forecast_steps))) %>%\n  add_column(linear_effect_sds = c(linear_effect_sds, rep(NA, n_forecast_steps)))\n\nggplot(components_df, aes(x = month, y = n)) +\n  geom_line(aes(y = seasonal_effect_means), color = \"orange\") +\n  geom_ribbon(\n    aes(\n      ymin = seasonal_effect_means - 2 * seasonal_effect_sds,\n      ymax = seasonal_effect_means + 2 * seasonal_effect_sds\n    ),\n    alpha = 0.2,\n    fill = \"orange\"\n  ) +\n  theme(axis.title = element_blank()) +\n  geom_line(aes(y = linear_effect_means), color = \"green\") +\n  geom_ribbon(\n    aes(\n      ymin = linear_effect_means - 2 * linear_effect_sds,\n      ymax = linear_effect_means + 2 * linear_effect_sds\n    ),\n    alpha = 0.2,\n    fill = \"green\"\n  ) +\n  theme(axis.title = element_blank())\n\n\n\n\n\nFigure 7: AirPassengers, decomposition into a linear trend and a seasonal component (both centered).\n\n\n\nWrapping up\nWe’ve seen how with DLMs, there is a bunch of interesting stuff you can do – apart from obtaining forecasts, which probably will be the ultimate goal in most applications – : You can inspect the smoothed and the filtered estimates from the Kálmán filter, and you can decompose a model into its posterior components. A particularly attractive model is dynamic linear regression, featured in our first example, which allows us to obtain regression coefficients that vary over time.\nThis post showed how to accomplish this with tfprobability. As of today, TensorFlow (and thus, TensorFlow Probability) is in a state of substantial internal changes, with eager to become the default execution mode very soon. Concurrently, the awesome TensorFlow Probability development team are adding new and exciting features every day. Consequently, this post is snapshot capturing how to best accomplish those goals now: If you’re reading this a few months from now, chances are that what’s work in progress now will have become a mature method by then, and there may be faster ways to attain the same goals. At the rate TFP is evolving, we’re excited for the things to come!\n\n\n\nBerndt, R. 1991. The Practice of Econometrics. Addison-Wesley.\n\n\nMurphy, Kevin. 2012. Machine Learning: A Probabilistic Perspective. MIT Press.\n\n\nPetris, Giovanni, sonia Petrone, and Patrizia Campagnoli. 2009. Dynamic Linear Models with r. Springer.\n\n\nFor an excellent yet concise introduction to state space models, see chapter 18 of Murphy (2012) (Murphy 2012).↩︎\nA draft version is available online at http://people.bordeaux.inria.fr/pierre.delmoral/dynamics-linear-models.petris_et_al.pdf. However, the draft does not yet fully elaborate on this example.↩︎\nForecasting functionality is not provided by dlm for dynamic linear models, see ?dlmForecast↩︎\nWe do not think it instructive to display the results obtained by Petris et al. here, as this post does not aim to evaluate or compare implementations.↩︎\nIt is still clipped up and below though. We’ve been trying to find a compromise between local visibility of the filtered means on the one hand, and global visibility of uncertainty in the estimates on the other.↩︎\nFor MCMC, you can straightforwardly adapt the above example.↩︎\n",
    "preview": "posts/2019-06-25-dynamic_linear_models_tfprobability/images/thumb.png",
    "last_modified": "2024-11-21T15:50:43+00:00",
    "input_file": {},
    "preview_width": 2012,
    "preview_height": 1065
  },
  {
    "path": "posts/2019-06-05-uncertainty-estimates-tfprobability/",
    "title": "Adding uncertainty estimates to Keras models with tfprobability",
    "description": "As of today, there is no mainstream road to obtaining uncertainty estimates from neural networks. All that can be said is that, normally, approaches tend to be Bayesian in spirit, involving some way of putting a prior over model weights. This holds true as well for the method presented in this post: We show how to use tfprobability, the R interface to TensorFlow Probability, to add uncertainty estimates to a Keras model in an elegant and conceptually plausible way.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-06-05",
    "categories": [
      "Probabilistic ML/DL",
      "TensorFlow/Keras",
      "Concepts"
    ],
    "contents": "\nAbout six months ago, we showed how to create a custom wrapper to obtain uncertainty estimates from a Keras network. Today we present a less laborious, as well faster-running way using tfprobability, the R wrapper to TensorFlow Probability. Like most posts on this blog, this one won’t be short, so let’s quickly state what you can expect in return of reading time.\nWhat to expect from this post\nStarting from what not to expect: There won’t be a recipe that tells you how exactly to set all parameters involved in order to report the “right” uncertainty measures. But then, what are the “right” uncertainty measures? Unless you happen to work with a method that has no (hyper-)parameters to tweak, there will always be questions about how to report uncertainty.\nWhat you can expect, though, is an introduction to obtaining uncertainty estimates for Keras networks, as well as an empirical report of how tweaking (hyper-)parameters may affect the results. As in the aforementioned post, we perform our tests on both a simulated and a real dataset, the Combined Cycle Power Plant Data Set. At the end, in place of strict rules, you should have acquired some intuition that will transfer to other real-world datasets.\nDid you notice our talking about Keras networks above? Indeed this post has an additional goal: So far, we haven’t really discussed yet how tfprobability goes together with keras. Now we finally do (in short: they work together seemlessly).\nFinally, the notions of aleatoric and epistemic uncertainty, which may have stayed a bit abstract in the prior post, should get much more concrete here.\nAleatoric vs. epistemic uncertainty\nReminiscent somehow of the classic decomposition of generalization error into bias and variance, splitting uncertainty into its epistemic and aleatoric constituents separates an irreducible from a reducible part.\nThe reducible part relates to imperfection in the model: In theory, if our model were perfect, epistemic uncertainty would vanish. Put differently, if the training data were unlimited – or if they comprised the whole population – we could just add capacity to the model until we’ve obtained a perfect fit.\nIn contrast, normally there is variation in our measurements. There may be one true process that determines my resting heart rate; nonetheless, actual measurements will vary over time. There is nothing to be done about this: This is the aleatoric part that just remains, to be factored into our expectations.\nNow reading this, you might be thinking: “Wouldn’t a model that actually were perfect capture those pseudo-random fluctuations?”. We’ll leave that phisosophical question be; instead, we’ll try to illustrate the usefulness of this distinction by example, in a practical way. In a nutshell, viewing a model’s aleatoric uncertainty output should caution us to factor in appropriate deviations when making our predictions, while inspecting epistemic uncertainty should help us re-think the appropriateness of the chosen model.\nNow let’s dive in and see how we may accomplish our goal with tfprobability. We start with the simulated dataset.\nUncertainty estimates on simulated data\nDataset\nWe re-use the dataset from the Google TensorFlow Probability team’s blog post on the same subject 1, with one exception: We extend the range of the independent variable a bit on the negative side, to better demonstrate the different methods’ behaviors.\nHere is the data-generating process. We also get library loading out of the way. Like the preceding posts on tfprobability, this one too features recently added functionality, so please use the development versions of tensorflow and tfprobability as well as keras. Call install_tensorflow(version = \"nightly\") to obtain a current nightly build of TensorFlow and TensorFlow Probability:\n\n\n# make sure we use the development versions of tensorflow, tfprobability and keras\ndevtools::install_github(\"rstudio/tensorflow\")\ndevtools::install_github(\"rstudio/tfprobability\")\ndevtools::install_github(\"rstudio/keras\")\n\n# and that we use a nightly build of TensorFlow and TensorFlow Probability\ntensorflow::install_tensorflow(version = \"nightly\")\n\nlibrary(tensorflow)\nlibrary(tfprobability)\nlibrary(keras)\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\n# make sure this code is compatible with TensorFlow 2.0\ntf$compat$v1$enable_v2_behavior()\n\n# generate the data\nx_min <- -40\nx_max <- 60\nn <- 150\nw0 <- 0.125\nb0 <- 5\n\nnormalize <- function(x) (x - x_min) / (x_max - x_min)\n\n# training data; predictor \nx <- x_min + (x_max - x_min) * runif(n) %>% as.matrix()\n\n# training data; target\neps <- rnorm(n) * (3 * (0.25 + (normalize(x)) ^ 2))\ny <- (w0 * x * (1 + sin(x)) + b0) + eps\n\n# test data (predictor)\nx_test <- seq(x_min, x_max, length.out = n) %>% as.matrix()\n\n\nHow does the data look?\n\n\nggplot(data.frame(x = x, y = y), aes(x, y)) + geom_point()\n\n\n\n\n\nFigure 1: Simulated data\n\n\n\nThe task here is single-predictor regression, which in principle we can achieve use Keras dense layers.\nLet’s see how to enhance this by indicating uncertainty, starting from the aleatoric type.\nAleatoric uncertainty\nAleatoric uncertainty, by definition, is not a statement about the model. So why not have the model learn the uncertainty inherent in the data?\nThis is exactly how aleatoric uncertainty is operationalized in this approach. Instead of a single output per input – the predicted mean of the regression – here we have two outputs: one for the mean, and one for the standard deviation.\nHow will we use these? Until shortly, we would have had to roll our own logic. Now with tfprobability, we make the network output not tensors, but distributions – put differently, we make the last layer a distribution layer.\nDistribution layers are Keras layers, but contributed by tfprobability. The awesome thing is that we can train them with just tensors as targets, as usual: No need to compute probabilities ourselves.\nSeveral specialized distribution layers exist, such as layer_kl_divergence_add_loss, layer_independent_bernoulli, or layer_mixture_same_family, but the most general is layer_distribution_lambda. layer_distribution_lambda takes as inputs the preceding layer and outputs a distribution. In order to be able to do this, we need to tell it how to make use of the preceding layer’s activations.\nIn our case, at some point we will want to have a dense layer with two units.\n\n... %>% layer_dense(units = 2, activation = \"linear\") %>%\n\nThen layer_distribution_lambda will use the first unit as the mean of a normal distribution, and the second as its standard deviation.\n\n\nlayer_distribution_lambda(function(x)\n    tfd_normal(loc = x[, 1, drop = FALSE],\n               scale = 1e-3 + tf$math$softplus(x[, 2, drop = FALSE])\n               )\n)\n\n\nHere is the complete model we use. We insert an additional dense layer in front, with a relu activation, to give the model a bit more freedom and capacity. We discuss this, as well as that scale = ... foo, as soon as we’ve finished our walkthrough of model training.\n\n\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 8, activation = \"relu\") %>%\n  layer_dense(units = 2, activation = \"linear\") %>%\n  layer_distribution_lambda(function(x)\n    tfd_normal(loc = x[, 1, drop = FALSE],\n               # ignore on first read, we'll come back to this\n               # scale = 1e-3 + 0.05 * tf$math$softplus(x[, 2, drop = FALSE])\n               scale = 1e-3 + tf$math$softplus(x[, 2, drop = FALSE])\n               )\n  )\n\n\nFor a model that outputs a distribution, the loss is the negative log likelihood given the target data.\n\n\nnegloglik <- function(y, model) - (model %>% tfd_log_prob(y))\n\n\nWe can now compile and fit the model.\n\n\nlearning_rate <- 0.01\nmodel %>% compile(optimizer = optimizer_adam(lr = learning_rate), loss = negloglik)\n\nmodel %>% fit(x, y, epochs = 1000)\n\n\nWe now call the model on the test data to obtain the predictions. The predictions now actually are distributions, and we have 150 of them, one for each datapoint:\n\n\nyhat <- model(tf$constant(x_test))\n\n\ntfp.distributions.Normal(\"sequential/distribution_lambda/Normal/\",\nbatch_shape=[150, 1], event_shape=[], dtype=float32)\nTo obtain the means and standard deviations – the latter being that measure of aleatoric uncertainty we’re interested in – we just call tfd_mean and tfd_stddev on these distributions.\nThat will give us the predicted mean, as well as the predicted variance, per datapoint.\n\n\nmean <- yhat %>% tfd_mean()\nsd <- yhat %>% tfd_stddev()\n\n\nLet’s visualize this. Here are the actual test data points, the predicted means, as well as confidence bands indicating the mean estimate plus/minus two standard deviations.\n\n\nggplot(data.frame(\n  x = x,\n  y = y,\n  mean = as.numeric(mean),\n  sd = as.numeric(sd)\n),\naes(x, y)) +\n  geom_point() +\n  geom_line(aes(x = x_test, y = mean), color = \"violet\", size = 1.5) +\n  geom_ribbon(aes(\n    x = x_test,\n    ymin = mean - 2 * sd,\n    ymax = mean + 2 * sd\n  ),\n  alpha = 0.2,\n  fill = \"grey\")\n\n\n\n\n\nFigure 2: Aleatoric uncertainty on simulated data, using relu activation in the first dense layer.\n\n\n\nThis looks pretty reasonable. What if we had used linear activation in the first layer? Meaning, what if the model had looked like this2:\n\n\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 8, activation = \"linear\") %>%\n  layer_dense(units = 2, activation = \"linear\") %>%\n  layer_distribution_lambda(function(x)\n    tfd_normal(loc = x[, 1, drop = FALSE],\n               scale = 1e-3 + 0.05 * tf$math$softplus(x[, 2, drop = FALSE])\n               )\n  )\n\n\nThis time, the model does not capture the “form” of the data that well, as we’ve disallowed any nonlinearities.\n\n\n\nFigure 3: Aleatoric uncertainty on simulated data, using linear activation in the first dense layer.\n\n\n\nUsing linear activations only, we also need to do more experimenting with the scale = ... line to get the result look “right”. With relu, on the other hand, results are pretty robust to changes in how scale is computed. Which activation do we choose? If our goal is to adequately model variation in the data, we can just choose relu – and leave assessing uncertainty in the model to a different technique (the epistemic uncertainty that is up next).\nOverall, it seems like aleatoric uncertainty is the straightforward part. We want the network to learn the variation inherent in the data, which it does. What do we gain? Instead of obtaining just point estimates, which in this example might turn out pretty bad in the two fan-like areas of the data on the left and right sides, we learn about the spread as well. We’ll thus be appropriately cautious depending on what input range we’re making predictions for.\nEpistemic uncertainty\nNow our focus is on the model. Given a speficic model (e.g., one from the linear family), what kind of data does it say conforms to its expectations?\nTo answer this question, we make use of a variational-dense layer.\nThis is again a Keras layer provided by tfprobability. Internally, it works by minimizing the evidence lower bound (ELBO), thus striving to find an approximative posterior that does two things:\nfit the actual data well (put differently: achieve high log likelihood), and\nstay close to a prior (as measured by KL divergence).\nAs users, we actually specify the form of the posterior as well as that of the prior. Here is how a prior could look.\n\n\nprior_trainable <-\n  function(kernel_size,\n           bias_size = 0,\n           dtype = NULL) {\n    n <- kernel_size + bias_size\n    keras_model_sequential() %>%\n      # we'll comment on this soon\n      # layer_variable(n, dtype = dtype, trainable = FALSE) %>%\n      layer_variable(n, dtype = dtype, trainable = TRUE) %>%\n      layer_distribution_lambda(function(t) {\n        tfd_independent(tfd_normal(loc = t, scale = 1),\n                        reinterpreted_batch_ndims = 1)\n      })\n  }\n\n\nThis prior is itself a Keras model, containing a layer that wraps a variable and a layer_distribution_lambda, that type of distribution-yielding layer we’ve just encountered above. The variable layer could be fixed (non-trainable) or non-trainable, corresponding to a genuine prior or a prior learnt from the data in an empirical Bayes-like way. The distribution layer outputs a normal distribution since we’re in a regression setting.\nThe posterior too is a Keras model – definitely trainable this time. It too outputs a normal distribution:\n\n\nposterior_mean_field <-\n  function(kernel_size,\n           bias_size = 0,\n           dtype = NULL) {\n    n <- kernel_size + bias_size\n    c <- log(expm1(1))\n    keras_model_sequential(list(\n      layer_variable(shape = 2 * n, dtype = dtype),\n      layer_distribution_lambda(\n        make_distribution_fn = function(t) {\n          tfd_independent(tfd_normal(\n            loc = t[1:n],\n            scale = 1e-5 + tf$nn$softplus(c + t[(n + 1):(2 * n)])\n            ), reinterpreted_batch_ndims = 1)\n        }\n      )\n    ))\n  }\n\n\nNow that we’ve defined both, we can set up the model’s layers. The first one, a variational-dense layer, has a single unit. The ensuing distribution layer then takes that unit’s output and uses it for the mean of a normal distribution – while the scale of that Normal is fixed at 1:\n\n\nmodel <- keras_model_sequential() %>%\n  layer_dense_variational(\n    units = 1,\n    make_posterior_fn = posterior_mean_field,\n    make_prior_fn = prior_trainable,\n    kl_weight = 1 / n\n  ) %>%\n  layer_distribution_lambda(function(x)\n    tfd_normal(loc = x, scale = 1))\n\n\nYou may have noticed one argument to layer_dense_variational we haven’t discussed yet, kl_weight.\nThis is used to scale the contribution to the total loss of the KL divergence, and normally should equal one over the number of data points.\nTraining the model is straightforward. As users, we only specify the negative log likelihood part of the loss; the KL divergence part is taken care of transparently by the framework.\n\n\nnegloglik <- function(y, model) - (model %>% tfd_log_prob(y))\nmodel %>% compile(optimizer = optimizer_adam(lr = learning_rate), loss = negloglik)\nmodel %>% fit(x, y, epochs = 1000)\n\n\nBecause of the stochasticity inherent in a variational-dense layer, each time we call this model, we obtain different results: different normal distributions, in this case.\nTo obtain the uncertainty estimates we’re looking for, we therefore call the model a bunch of times – 100, say:\n\n\nyhats <- purrr::map(1:100, function(x) model(tf$constant(x_test)))\n\n\nWe can now plot those 100 predictions – lines, in this case, as there are no nonlinearities:\n\n\nmeans <-\n  purrr::map(yhats, purrr::compose(as.matrix, tfd_mean)) %>% abind::abind()\n\nlines <- data.frame(cbind(x_test, means)) %>%\n  gather(key = run, value = value,-X1)\n\nmean <- apply(means, 1, mean)\n\nggplot(data.frame(x = x, y = y, mean = as.numeric(mean)), aes(x, y)) +\n  geom_point() +\n  geom_line(aes(x = x_test, y = mean), color = \"violet\", size = 1.5) +\n  geom_line(\n    data = lines,\n    aes(x = X1, y = value, color = run),\n    alpha = 0.3,\n    size = 0.5\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\nFigure 4: Epistemic uncertainty on simulated data, using linear activation in the variational-dense layer.\n\n\n\nWhat we see here are essentially different models, consistent with the assumptions built into the architecture. What we’re not accounting for is the spread in the data. Can we do both? We can; but first let’s comment on a few choices that were made and see how they affect the results.\nTo prevent this post from growing to infinite size, we’ve refrained from performing a systematic experiment; please take what follows not as generalizable statements, but as pointers to things you will want to keep in mind in your own ventures. Especially, each (hyper-)parameter is not an island; they could interact in unforeseen ways.\nAfter those words of caution, here are some things we noticed.\nOne question you might ask: Before, in the aleatoric uncertainty setup, we added an additional dense layer to the model, with relu activation. What if we did this here?\nFirstly, we’re not adding any additional, non-variational layers in order to keep the setup “fully Bayesian” – we want priors at every level. As to using relu in layer_dense_variational, we did try that, and the results look pretty similar:\n\n\n\nFigure 5: Epistemic uncertainty on simulated data, using relu activation in the variational-dense layer.\n\n\n\nHowever, things look pretty different if we drastically reduce training time… which brings us to the next observation.\nUnlike in the aleatoric setup, the number of training epochs matter a lot. If we train, quote unquote, too long, the posterior estimates will get closer and closer to the posterior mean: we lose uncertainty. What happens if we train “too short” is even more notable. Here are the results for the linear-activation as well as the relu-activation cases:\n\n\n\nFigure 6: Epistemic uncertainty on simulated data if we train for 100 epochs only. Left: linear activation. Right: relu activation.\n\n\n\nInterestingly, both model families look very different now, and while the linear-activation family looks more reasonable at first, it still considers an overall negative slope consistent with the data.\nSo how many epochs are “long enough”? From observation, we’d say that a working heuristic should probably be based on the rate of loss reduction. But certainly, it’ll make sense to try different numbers of epochs and check the effect on model behavior. As an aside, monitoring estimates over training time may even yield important insights into the assumptions built into a model (e.g., the effect of different activation functions).\nAs important as the number of epochs trained, and similar in effect, is the learning rate. If we replace the learning rate in this setup by 0.001, results will look similar to what we saw above for the epochs = 100 case. Again, we will want to try different learning rates and make sure we train the model “to completion” in some reasonable sense.\nTo conclude this section, let’s quickly look at what happens if we vary two other parameters. What if the prior were non-trainable (see the commented line above)? And what if we scaled the importance of the KL divergence (kl_weight in layer_dense_variational’s argument list) differently, replacing kl_weight = 1/n by kl_weight = 1 (or equivalently, removing it)? Here are the respective results for an otherwise-default setup. They don’t lend themselves to generalization – on different (e.g., bigger!) datasets the outcomes will most certainly look different – but definitely interesting to observe.\n\n\n\nFigure 7: Epistemic uncertainty on simulated data. Left: kl_weight = 1. Right: prior non-trainable.\n\n\n\nNow let’s come back to the question: We’ve modeled spread in the data, we’ve peeked into the heart of the model, – can we do both at the same time?\nWe can, if we combine both approaches. We add an additional unit to the variational-dense layer and use this to learn the variance: once for each “sub-model” contained in the model.\nCombining both aleatoric and epistemic uncertainty\nReusing the prior and posterior from above, this is how the final model looks:\n\n\nmodel <- keras_model_sequential() %>%\n  layer_dense_variational(\n    units = 2,\n    make_posterior_fn = posterior_mean_field,\n    make_prior_fn = prior_trainable,\n    kl_weight = 1 / n\n  ) %>%\n  layer_distribution_lambda(function(x)\n    tfd_normal(loc = x[, 1, drop = FALSE],\n               scale = 1e-3 + tf$math$softplus(0.01 * x[, 2, drop = FALSE])\n               )\n    )\n\n\nWe train this model just like the epistemic-uncertainty only one. We then obtain a measure of uncertainty per predicted line. Or in the words we used above, we now have an ensemble of models each with its own indication of spread in the data. Here is a way we could display this – each colored line is the mean of a distribution, surrounded by a confidence band indicating +/- two standard deviations.\n\n\nyhats <- purrr::map(1:100, function(x) model(tf$constant(x_test)))\nmeans <-\n  purrr::map(yhats, purrr::compose(as.matrix, tfd_mean)) %>% abind::abind()\nsds <-\n  purrr::map(yhats, purrr::compose(as.matrix, tfd_stddev)) %>% abind::abind()\n\nmeans_gathered <- data.frame(cbind(x_test, means)) %>%\n  gather(key = run, value = mean_val,-X1)\nsds_gathered <- data.frame(cbind(x_test, sds)) %>%\n  gather(key = run, value = sd_val,-X1)\n\nlines <-\n  means_gathered %>% inner_join(sds_gathered, by = c(\"X1\", \"run\"))\nmean <- apply(means, 1, mean)\n\nggplot(data.frame(x = x, y = y, mean = as.numeric(mean)), aes(x, y)) +\n  geom_point() +\n  theme(legend.position = \"none\") +\n  geom_line(aes(x = x_test, y = mean), color = \"violet\", size = 1.5) +\n  geom_line(\n    data = lines,\n    aes(x = X1, y = mean_val, color = run),\n    alpha = 0.6,\n    size = 0.5\n  ) +\n  geom_ribbon(\n    data = lines,\n    aes(\n      x = X1,\n      ymin = mean_val - 2 * sd_val,\n      ymax = mean_val + 2 * sd_val,\n      group = run\n    ),\n    alpha = 0.05,\n    fill = \"grey\",\n    inherit.aes = FALSE\n  )\n\n\n\n\n\nFigure 8: Displaying both epistemic and aleatoric uncertainty on the simulated dataset.\n\n\n\nNice! This looks like something we could report.\nAs you might imagine, this model, too, is sensitive to how long (think: number of epochs) or how fast (think: learning rate) we train it. And compared to the epistemic-uncertainty only model, there is an additional choice to be made here: the scaling of the previous layer’s activation – the 0.01 in the scale argument to tfd_normal:\n\n\nscale = 1e-3 + tf$math$softplus(0.01 * x[, 2, drop = FALSE])\n\n\nKeeping everything else constant, here we vary that parameter between 0.01 and 0.05:\n\n\n\nFigure 9: Epistemic plus aleatoric uncertainty on the simulated dataset: Varying the scale argument.\n\n\n\nEvidently, this is another parameter we should be prepared to experiment with.\nNow that we’ve introduced all three types of presenting uncertainty – aleatoric only, epistemic only, or both – let’s see them on the aforementioned Combined Cycle Power Plant Data Set. Please see our previous post on uncertainty for a quick characterization, as well as visualization, of the dataset.\nCombined Cycle Power Plant Data Set\nTo keep this post at a digestible length, we’ll refrain from trying as many alternatives as with the simulated data and mainly stay with what worked well there. This should also give us an idea of how well these “defaults” generalize. We separately inspect two scenarios: The single-predictor setup (using each of the four available predictors alone), and the complete one (using all four predictors at once).\nThe dataset is loaded just as in the previous post.\n\n\nlibrary(tensorflow)\nlibrary(tfprobability)\nlibrary(keras)\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readxl)\n\n# make sure this code is compatible with TensorFlow 2.0\ntf$compat$v1$enable_v2_behavior()\n\ndf <- read_xlsx(\"CCPP/Folds5x2_pp.xlsx\")\n\ndf_scaled <- scale(df)\ncenters <- attr(df_scaled, \"scaled:center\")\nscales <- attr(df_scaled, \"scaled:scale\")\n\nX <- df_scaled[, 1:4]\ntrain_samples <- sample(1:nrow(df_scaled), 0.8 * nrow(X))\nX_train <- X[train_samples, ]\nX_val <- X[-train_samples, ]\n\ny <- df_scaled[, 5] \ny_train <- y[train_samples] %>% as.matrix()\ny_val <- y[-train_samples] %>% as.matrix()\n\n\nFirst we look at the single-predictor case, starting from aleatoric uncertainty.\nSingle predictor: Aleatoric uncertainty\nHere is the “default” aleatoric model again. We also duplicate the plotting code here for the reader’s convenience.\n\n\nn <- nrow(X_train) # 7654\nn_epochs <- 10 # we need fewer epochs because the dataset is so much bigger\n\nbatch_size <- 100\n\nlearning_rate <- 0.01\n\n# variable to fit - change to 2,3,4 to get the other predictors\ni <- 1\n\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 16, activation = \"relu\") %>%\n  layer_dense(units = 2, activation = \"linear\") %>%\n  layer_distribution_lambda(function(x)\n    tfd_normal(loc = x[, 1, drop = FALSE],\n               scale = tf$math$softplus(x[, 2, drop = FALSE])\n               )\n    )\n\nnegloglik <- function(y, model) - (model %>% tfd_log_prob(y))\n\nmodel %>% compile(optimizer = optimizer_adam(lr = learning_rate), loss = negloglik)\n\nhist <-\n  model %>% fit(\n    X_train[, i, drop = FALSE],\n    y_train,\n    validation_data = list(X_val[, i, drop = FALSE], y_val),\n    epochs = n_epochs,\n    batch_size = batch_size\n  )\n\nyhat <- model(tf$constant(X_val[, i, drop = FALSE]))\n\nmean <- yhat %>% tfd_mean()\nsd <- yhat %>% tfd_stddev()\n\nggplot(data.frame(\n  x = X_val[, i],\n  y = y_val,\n  mean = as.numeric(mean),\n  sd = as.numeric(sd)\n),\naes(x, y)) +\n  geom_point() +\n  geom_line(aes(x = x, y = mean), color = \"violet\", size = 1.5) +\n  geom_ribbon(aes(\n    x = x,\n    ymin = mean - 2 * sd,\n    ymax = mean + 2 * sd\n  ),\n  alpha = 0.4,\n  fill = \"grey\")\n\n\nHow well does this work?\n\n\n\nFigure 10: Aleatoric uncertainty on the Combined Cycle Power Plant Data Set; single predictors.\n\n\n\nThis looks pretty good we’d say! How about epistemic uncertainty?\nSingle predictor: Epistemic uncertainty\nHere’s the code:\n\n\nposterior_mean_field <-\n  function(kernel_size,\n           bias_size = 0,\n           dtype = NULL) {\n    n <- kernel_size + bias_size\n    c <- log(expm1(1))\n    keras_model_sequential(list(\n      layer_variable(shape = 2 * n, dtype = dtype),\n      layer_distribution_lambda(\n        make_distribution_fn = function(t) {\n          tfd_independent(tfd_normal(\n            loc = t[1:n],\n            scale = 1e-5 + tf$nn$softplus(c + t[(n + 1):(2 * n)])\n          ), reinterpreted_batch_ndims = 1)\n        }\n      )\n    ))\n  }\n\nprior_trainable <-\n  function(kernel_size,\n           bias_size = 0,\n           dtype = NULL) {\n    n <- kernel_size + bias_size\n    keras_model_sequential() %>%\n      layer_variable(n, dtype = dtype, trainable = TRUE) %>%\n      layer_distribution_lambda(function(t) {\n        tfd_independent(tfd_normal(loc = t, scale = 1),\n                        reinterpreted_batch_ndims = 1)\n      })\n  }\n\nmodel <- keras_model_sequential() %>%\n  layer_dense_variational(\n    units = 1,\n    make_posterior_fn = posterior_mean_field,\n    make_prior_fn = prior_trainable,\n    kl_weight = 1 / n,\n    activation = \"linear\",\n  ) %>%\n  layer_distribution_lambda(function(x)\n    tfd_normal(loc = x, scale = 1))\n\nnegloglik <- function(y, model) - (model %>% tfd_log_prob(y))\nmodel %>% compile(optimizer = optimizer_adam(lr = learning_rate), loss = negloglik)\nhist <-\n  model %>% fit(\n    X_train[, i, drop = FALSE],\n    y_train,\n    validation_data = list(X_val[, i, drop = FALSE], y_val),\n    epochs = n_epochs,\n    batch_size = batch_size\n  )\n\nyhats <- purrr::map(1:100, function(x)\n  yhat <- model(tf$constant(X_val[, i, drop = FALSE])))\n  \nmeans <-\n  purrr::map(yhats, purrr::compose(as.matrix, tfd_mean)) %>% abind::abind()\n\nlines <- data.frame(cbind(X_val[, i], means)) %>%\n  gather(key = run, value = value,-X1)\n\nmean <- apply(means, 1, mean)\nggplot(data.frame(x = X_val[, i], y = y_val, mean = as.numeric(mean)), aes(x, y)) +\n  geom_point() +\n  geom_line(aes(x = X_val[, i], y = mean), color = \"violet\", size = 1.5) +\n  geom_line(\n    data = lines,\n    aes(x = X1, y = value, color = run),\n    alpha = 0.3,\n    size = 0.5\n  ) +\n  theme(legend.position = \"none\")\n\n\nAnd this is the result.\n\n\n\nFigure 11: Epistemic uncertainty on the Combined Cycle Power Plant Data Set; single predictors.\n\n\n\nAs with the simulated data, the linear models seems to “do the right thing”. And here too, we think we will want to augment this with the spread in the data: Thus, on to way three.\nSingle predictor: Combining both types\nHere we go. Again, posterior_mean_field and prior_trainable look just like in the epistemic-only case.\n\n\nmodel <- keras_model_sequential() %>%\n  layer_dense_variational(\n    units = 2,\n    make_posterior_fn = posterior_mean_field,\n    make_prior_fn = prior_trainable,\n    kl_weight = 1 / n,\n    activation = \"linear\"\n  ) %>%\n  layer_distribution_lambda(function(x)\n    tfd_normal(loc = x[, 1, drop = FALSE],\n               scale = 1e-3 + tf$math$softplus(0.01 * x[, 2, drop = FALSE])))\n\n\nnegloglik <- function(y, model)\n  - (model %>% tfd_log_prob(y))\nmodel %>% compile(optimizer = optimizer_adam(lr = learning_rate), loss = negloglik)\nhist <-\n  model %>% fit(\n    X_train[, i, drop = FALSE],\n    y_train,\n    validation_data = list(X_val[, i, drop = FALSE], y_val),\n    epochs = n_epochs,\n    batch_size = batch_size\n  )\n\nyhats <- purrr::map(1:100, function(x)\n  model(tf$constant(X_val[, i, drop = FALSE])))\nmeans <-\n  purrr::map(yhats, purrr::compose(as.matrix, tfd_mean)) %>% abind::abind()\nsds <-\n  purrr::map(yhats, purrr::compose(as.matrix, tfd_stddev)) %>% abind::abind()\n\nmeans_gathered <- data.frame(cbind(X_val[, i], means)) %>%\n  gather(key = run, value = mean_val,-X1)\nsds_gathered <- data.frame(cbind(X_val[, i], sds)) %>%\n  gather(key = run, value = sd_val,-X1)\n\nlines <-\n  means_gathered %>% inner_join(sds_gathered, by = c(\"X1\", \"run\"))\n\nmean <- apply(means, 1, mean)\n\n#lines <- lines %>% filter(run==\"X3\" | run ==\"X4\")\n\nggplot(data.frame(x = X_val[, i], y = y_val, mean = as.numeric(mean)), aes(x, y)) +\n  geom_point() +\n  theme(legend.position = \"none\") +\n  geom_line(aes(x = X_val[, i], y = mean), color = \"violet\", size = 1.5) +\n  geom_line(\n    data = lines,\n    aes(x = X1, y = mean_val, color = run),\n    alpha = 0.2,\n    size = 0.5\n  ) +\ngeom_ribbon(\n  data = lines,\n  aes(\n    x = X1,\n    ymin = mean_val - 2 * sd_val,\n    ymax = mean_val + 2 * sd_val,\n    group = run\n  ),\n  alpha = 0.01,\n  fill = \"grey\",\n  inherit.aes = FALSE\n)\n\n\nAnd the output?\n\n\n\nFigure 12: Combined uncertainty on the Combined Cycle Power Plant Data Set; single predictors.\n\n\n\nThis looks useful! Let’s wrap up with our final test case: Using all four predictors together.\nAll predictors\nThe training code used in this scenario looks just like before, apart from our feeding all predictors to the model. For plotting, we resort to displaying the first principal component on the x-axis – this makes the plots look noisier than before. We also display fewer lines for the epistemic and epistemic-plus-aleatoric cases (20 instead of 100). Here are the results:\n\n\n\nFigure 13: Uncertainty (aleatoric, epistemic, both) on the Combined Cycle Power Plant Data Set; all predictors.\n\n\n\nConclusion\nWhere does this leave us? Compared to the learnable-dropout approach described in the prior post, the way presented here is a lot easier, faster, and more intuitively understandable.\nThe methods per se are that easy to use that in this first introductory post, we could afford to explore alternatives already: something we had no time to do in that previous exposition.\nIn fact, we hope this post leaves you in a position to do your own experiments, on your own data.\nObviously, you will have to make decisions, but isn’t that the way it is in data science? There’s no way around making decisions; we just should be prepared to justify them …\nThanks for reading!\n\nsee also the corresponding notebook↩︎\nyes, we also use that other line for scale that was commented before; more on that in a second↩︎\n",
    "preview": "posts/2019-06-05-uncertainty-estimates-tfprobability/images/uci_both.png",
    "last_modified": "2024-11-21T15:50:00+00:00",
    "input_file": {},
    "preview_width": 2020,
    "preview_height": 1020
  },
  {
    "path": "posts/2019-05-24-varying-slopes/",
    "title": "Hierarchical partial pooling, continued: Varying slopes models with TensorFlow Probability",
    "description": "This post builds on our recent introduction to multi-level modeling with tfprobability, the R wrapper to TensorFlow Probability. We show how to pool not just mean values (\"intercepts\"), but also relationships (\"slopes\"), thus enabling models to learn from data in an even broader way. Again, we use an example from Richard McElreath's \"Statistical Rethinking\"; the terminology as well as the way we present this topic are largely owed to this book.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-05-24",
    "categories": [
      "Bayesian Modeling",
      "TensorFlow/Keras"
    ],
    "contents": "\nIn a previous post, we showed how to use tfprobability – the R interface to TensorFlow Probability – to build a multilevel, or partial pooling model of tadpole survival in differently sized (and thus, differing in inhabitant number) tanks.\nA completely pooled model would have resulted in a global estimate of survival count, irrespective of tank, while an unpooled model would have learned to predict survival count for each tank separately. The former approach does not take into account different circumstances; the latter does not make use of common information. (Also, it clearly has no predictive use unless we want to make predictions for the very same entities we used to train the model.)\nIn contrast, a partially pooled model lets you make predictions for the familiar, as well as new entities: Just use the appropriate prior.\nAssuming we are in fact interested in the same entities – why would we want to apply partial pooling?\nFor the same reasons so much effort in machine learning goes into devising regularization mechanisms. We don’t want to overfit too much to actual measurements, be they related to the same entity or a class of entities. If I want to predict my heart rate as I wake up next morning, based on a single measurement I’m taking now (let’s say it’s evening and I’m frantically typing a blog post), I better take into account some facts about heart rate behavior in general (instead of just projecting into the future the exact value measured right now).\nIn the tadpole example, this means we expect generalization to work better for tanks with many inhabitants, compared to more solitary environments. For the latter ones, we better take a peek at survival rates from other tanks, to supplement the sparse, idiosyncratic information available.\nOr using the technical term, in the latter case we hope for the model to shrink its estimates toward the overall mean more noticeably than in the former.\nThis type of information sharing is already very useful, but it gets better. The tadpole model is a varying intercepts model, as McElreath calls it (or random intercepts, as it is sometimes – confusingly – called 1) – intercepts referring to the way we make predictions for entities (here: tanks), with no predictor variables present. So if we can pool information about intercepts, why not pool information about slopes as well? This will allow us to, in addition, make use of relationships between variables learnt on different entities in the training set.\nSo as you might have guessed by now, varying slopes (or random slopes, if you will) is the topic of today’s post. Again, we take up an example from McElreath’s book, and show how to accomplish the same thing with tfprobability.\nCoffee, please\nUnlike the tadpole case, this time we work with simulated data. This is the data McElreath uses to introduce the varying slopes modeling technique; he then goes on and applies it to one of the book’s most featured datasets, the pro-social (or indifferent, rather!) chimpanzees. For today, we stay with the simulated data for two reasons: First, the subject matter per se is non-trivial enough; and second, we want to keep careful track of what our model does, and whether its output is sufficiently close to the results McElreath obtained from Stan 2.\nSo, the scenario is this. 3 Cafés vary in how popular they are. In a popular café, when you order coffee, you’re likely to wait. In a less popular café, you’ll likely be served much faster. That’s one thing.\nSecond, all cafés tend to be more crowded in the mornings than in the afternoons. Thus in the morning, you’ll wait longer than in the afternoon – this goes for the popular as well as the less popular cafés.\nIn terms of intercepts and slopes, we can picture the morning waits as intercepts, and the resultant afternoon waits as arising due to the slopes of the lines joining each morning and afternoon wait, respectively.\nSo when we partially-pool intercepts, we have one “intercept prior” (itself constrained by a prior, of course), and a set of café-specific intercepts that will vary around it. When we partially-pool slopes, we have a “slope prior” reflecting the overall relationship between morning and afternoon waits, and a set of café-specific slopes reflecting the individual relationships. Cognitively, that means that if you have never been to the Café Gerbeaud in Budapest but have been to cafés before, you might have a less-than-uninformed idea about how long you are going to wait; it also means that if you normally get your coffee in your favorite corner café in the mornings, and now you pass by there in the afternoon, you have an approximate idea how long it’s going to take (namely, fewer minutes than in the mornings).\nSo is that all? Actually, no. In our scenario, intercepts and slopes are related. If, at a less popular café, I always get my coffee before two minutes have passed, there is little room for improvement. At a highly popular café though, if it could easily take ten minutes in the mornings, then there is quite some potential for decrease in waiting time in the afternoon. So in my prediction for this afternoon’s waiting time, I should factor in this interaction effect.\nSo, now that we have an idea of what this is all about, let’s see how we can model these effects with tfprobability. But first, we actually have to generate the data.\nSimulate the data\nWe directly follow McElreath in the way the data are generated.\n\n\n##### Inputs needed to generate the covariance matrix between intercepts and slopes #####\n\n# average morning wait time\na <- 3.5\n# average difference afternoon wait time\n# we wait less in the afternoons\nb <- -1\n# standard deviation in the (café-specific) intercepts\nsigma_a <- 1\n# standard deviation in the (café-specific) slopes\nsigma_b <- 0.5\n# correlation between intercepts and slopes\n# the higher the intercept, the more the wait goes down\nrho <- -0.7\n\n\n##### Generate the covariance matrix #####\n\n# means of intercepts and slopes\nmu <- c(a, b)\n# standard deviations of means and slopes\nsigmas <- c(sigma_a, sigma_b) \n# correlation matrix\n# a correlation matrix has ones on the diagonal and the correlation in the off-diagonals\nrho <- matrix(c(1, rho, rho, 1), nrow = 2) \n# now matrix multiply to get covariance matrix\ncov_matrix <- diag(sigmas) %*% rho %*% diag(sigmas)\n\n\n##### Generate the café-specific intercepts and slopes #####\n\n# 20 cafés overall\nn_cafes <- 20\n\nlibrary(MASS)\nset.seed(5) # used to replicate example\n# multivariate distribution of intercepts and slopes\nvary_effects <- mvrnorm(n_cafes , mu ,cov_matrix)\n# intercepts are in the first column\na_cafe <- vary_effects[ ,1]\n# slopes are in the second\nb_cafe <- vary_effects[ ,2]\n\n\n##### Generate the actual wait times #####\n\nset.seed(22)\n# 10 visits per café\nn_visits <- 10\n\n# alternate values for mornings and afternoons in the data frame\nafternoon <- rep(0:1, n_visits * n_cafes/2)\n# data for each café are consecutive rows in the data frame\ncafe_id <- rep(1:n_cafes, each = n_visits)\n\n# the regression equation for the mean waiting time\nmu <- a_cafe[cafe_id] + b_cafe[cafe_id] * afternoon\n# standard deviation of waiting time within cafés\nsigma <- 0.5 # std dev within cafes\n# generate instances of waiting times\nwait <- rnorm(n_visits * n_cafes, mu, sigma)\n\nd <- data.frame(cafe = cafe_id, afternoon = afternoon, wait = wait)\n\n\nTake a glimpse at the data:\n\n\nd %>% glimpse()\n\n\nObservations: 200\nVariables: 3\n$ cafe      <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3,...\n$ afternoon <int> 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,...\n$ wait      <dbl> 3.9678929, 3.8571978, 4.7278755, 2.7610133, 4.1194827, 3.54365,...\nOn to building the model.\nThe model\nAs in the previous post on multi-level modeling, we use tfd_joint_distribution_sequential to define the model and Hamiltonian Monte Carlo for sampling. Consider taking a look at the first section of that post for a quick reminder of the overall procedure.\nBefore we code the model, let’s quickly get library loading out of the way. Importantly, again just like in the previous post, we need to install a master build of TensorFlow Probability, as we’re making use of very new features not yet available in the current release version. The same goes for the R packages tensorflow and tfprobability: Please install the respective development versions from github.\n\n\ndevtools::install_github(\"rstudio/tensorflow\")\ndevtools::install_github(\"rstudio/tfprobability\")\n\n# this will install the latest nightlies of TensorFlow as well as TensorFlow Probability\ntensorflow::install_tensorflow(version = \"nightly\")\n\nlibrary(tensorflow)\ntf$compat$v1$enable_v2_behavior()\n\nlibrary(tfprobability)\n\nlibrary(tidyverse)\nlibrary(zeallot)\nlibrary(abind)\nlibrary(gridExtra)\nlibrary(HDInterval)\nlibrary(ellipse)\n\n\nNow here is the model definition. We’ll go through it step by step in an instant.\n\n\nmodel <- function(cafe_id) {\n  tfd_joint_distribution_sequential(\n      list(\n        # rho, the prior for the correlation matrix between intercepts and slopes\n        tfd_cholesky_lkj(2, 2), \n        # sigma, prior variance for the waiting time\n        tfd_sample_distribution(tfd_exponential(rate = 1), sample_shape = 1),\n        # sigma_cafe, prior of variances for intercepts and slopes (vector of 2)\n        tfd_sample_distribution(tfd_exponential(rate = 1), sample_shape = 2), \n        # b, the prior mean for the slopes\n        tfd_sample_distribution(tfd_normal(loc = -1, scale = 0.5), sample_shape = 1),\n        # a, the prior mean for the intercepts\n        tfd_sample_distribution(tfd_normal(loc = 5, scale = 2), sample_shape = 1), \n        # mvn, multivariate distribution of intercepts and slopes\n        # shape: batch size, 20, 2\n        function(a,b,sigma_cafe,sigma,chol_rho) \n          tfd_sample_distribution(\n            tfd_multivariate_normal_tri_l(\n              loc = tf$concat(list(a,b), axis = -1L),\n              scale_tril = tf$linalg$LinearOperatorDiag(sigma_cafe)$matmul(chol_rho)),\n            sample_shape = n_cafes),\n        # waiting time\n        # shape should be batch size, 200\n        function(mvn, a, b, sigma_cafe, sigma)\n          tfd_independent(\n            # need to pull out the correct cafe_id in the middle column\n            tfd_normal(\n              loc = (tf$gather(mvn[ , , 1], cafe_id, axis = -1L) +\n                       tf$gather(mvn[ , , 2], cafe_id, axis = -1L) * afternoon), \n              scale=sigma),  # Shape [batch,  1]\n        reinterpreted_batch_ndims=1\n        )\n    )\n  )\n}\n\n\nThe first five distributions are priors. First, we have the prior for the correlation matrix.\nBasically, this would be an LKJ distribution of shape 2x2 and with concentration parameter equal to 2.\nFor performance reasons, we work with a version that inputs and outputs Cholesky factors instead:\n\n\n# rho, the prior correlation matrix between intercepts and slopes\ntfd_cholesky_lkj(2, 2)\n\n\nWhat kind of prior is this? As McElreath keeps reminding us, nothing is more instructive than sampling from the prior. For us to see what’s going on, we use the base LKJ distribution, not the Cholesky one:\n\n\ncorr_prior <- tfd_lkj(2, 2)\ncorrelation <- (corr_prior %>% tfd_sample(100))[ , 1, 2] %>% as.numeric()\nlibrary(ggplot2)\ndata.frame(correlation) %>% ggplot(aes(x = correlation)) + geom_density()\n\n\n\n\n\nSo this prior is moderately skeptical about strong correlations, but pretty open to learning from data.\nThe next distribution in line\n\n\n# sigma, prior variance for the waiting time\ntfd_sample_distribution(tfd_exponential(rate = 1), sample_shape = 1)\n\n\nis the prior for the variance of the waiting time, the very last distribution in the list.\nNext is the prior distribution of variances for the intercepts and slopes. This prior is the same for both cases, but we specify a sample_shape of 2 to get two individual samples.\n\n\n# sigma_cafe, prior of variances for intercepts and slopes (vector of 2)\ntfd_sample_distribution(tfd_exponential(rate = 1), sample_shape = 2)\n\n\nNow that we have the respective prior variances, we move on to the prior means. Both are normal distributions.\n\n\n# b, the prior mean for the slopes\ntfd_sample_distribution(tfd_normal(loc = -1, scale = 0.5), sample_shape = 1)\n\n\n\n\n# a, the prior mean for the intercepts\ntfd_sample_distribution(tfd_normal(loc = 5, scale = 2), sample_shape = 1)\n\n\nOn to the heart of the model, where the partial pooling happens. We are going to construct partially-pooled intercepts and slopes for all of the cafés. Like we said above, intercepts and slopes are not independent; they interact. Thus, we need to use a multivariate normal distribution.\nThe means are given by the prior means defined right above, while the covariance matrix is built from the above prior variances and the prior correlation matrix.\nThe output shape here is determined by the number of cafés: We want an intercept and a slope for every café.\n\n\n# mvn, multivariate distribution of intercepts and slopes\n# shape: batch size, 20, 2\nfunction(a,b,sigma_cafe,sigma,chol_rho) \n  tfd_sample_distribution(\n    tfd_multivariate_normal_tri_l(\n      loc = tf$concat(list(a,b), axis = -1L),\n      scale_tril = tf$linalg$LinearOperatorDiag(sigma_cafe)$matmul(chol_rho)),\n  sample_shape = n_cafes)\n\n\nFinally, we sample the actual waiting times.\nThis code pulls out the correct intercepts and slopes from the multivariate normal and outputs the mean waiting time, dependent on what café we’re in and whether it’s morning or afternoon.\n\n\n        # waiting time\n        # shape: batch size, 200\n        function(mvn, a, b, sigma_cafe, sigma)\n          tfd_independent(\n            # need to pull out the correct cafe_id in the middle column\n            tfd_normal(\n              loc = (tf$gather(mvn[ , , 1], cafe_id, axis = -1L) +\n                       tf$gather(mvn[ , , 2], cafe_id, axis = -1L) * afternoon), \n              scale=sigma), \n        reinterpreted_batch_ndims=1\n        )\n\n\nBefore running the sampling, it’s always a good idea to do a quick check on the model.\n\n\nn_cafes <- 20\ncafe_id <- tf$cast((d$cafe - 1) %% 20, tf$int64)\n\nafternoon <- d$afternoon\nwait <- d$wait\n\n\nWe sample from the model and then, check the log probability.\n\n\nm <- model(cafe_id)\n\ns <- m %>% tfd_sample(3)\nm %>% tfd_log_prob(s)\n\n\nWe want a scalar log probability per member in the batch, which is what we get.\ntf.Tensor([-466.1392  -149.92587 -196.51688], shape=(3,), dtype=float32)\nRunning the chains\nThe actual Monte Carlo sampling works just like in the previous post, with one exception. Sampling happens in unconstrained parameter space, but at the end we need to get valid correlation matrix parameters rho and valid variances sigma and sigma_cafe. Conversion between spaces is done via TFP bijectors. Luckily, this is not something we have to do as users; all we need to specify are appropriate bijectors. For the normal distributions in the model, there is nothing to do.\n\n\nconstraining_bijectors <- list(\n  # make sure the rho[1:4] parameters are valid for a Cholesky factor\n  tfb_correlation_cholesky(),\n  # make sure variance is positive\n  tfb_exp(),\n  # make sure variance is positive\n  tfb_exp(),\n  tfb_identity(),\n  tfb_identity(),\n  tfb_identity()\n)\n\n\nNow we can set up the Hamiltonian Monte Carlo sampler.\n\n\nn_steps <- 500\nn_burnin <- 500\nn_chains <- 4\n\n# set up the optimization objective\nlogprob <- function(rho, sigma, sigma_cafe, b, a, mvn)\n  m %>% tfd_log_prob(list(rho, sigma, sigma_cafe, b, a, mvn, wait))\n\n# initial states for the sampling procedure\nc(initial_rho, initial_sigma, initial_sigma_cafe, initial_b, initial_a, initial_mvn, .) %<-% \n  (m %>% tfd_sample(n_chains))\n\n# HMC sampler, with the above bijectors and step size adaptation\nhmc <- mcmc_hamiltonian_monte_carlo(\n  target_log_prob_fn = logprob,\n  num_leapfrog_steps = 3,\n  step_size = list(0.1, 0.1, 0.1, 0.1, 0.1, 0.1)\n) %>%\n  mcmc_transformed_transition_kernel(bijector = constraining_bijectors) %>%\n  mcmc_simple_step_size_adaptation(target_accept_prob = 0.8,\n                                   num_adaptation_steps = n_burnin)\n\n\nAgain, we can obtain additional diagnostics (here: step sizes and acceptance rates) by registering a trace function:\n\n\ntrace_fn <- function(state, pkr) {\n  list(pkr$inner_results$inner_results$is_accepted,\n       pkr$inner_results$inner_results$accepted_results$step_size)\n}\n\n\nHere, then, is the sampling function. Note how we use tf_function to put it on the graph. At least as of today, this makes a huge difference in sampling performance when using eager execution.\n\n\nrun_mcmc <- function(kernel) {\n  kernel %>% mcmc_sample_chain(\n    num_results = n_steps,\n    num_burnin_steps = n_burnin,\n    current_state = list(initial_rho,\n                         tf$ones_like(initial_sigma),\n                         tf$ones_like(initial_sigma_cafe),\n                         initial_b,\n                         initial_a,\n                         initial_mvn),\n    trace_fn = trace_fn\n  )\n}\n\nrun_mcmc <- tf_function(run_mcmc)\nres <- hmc %>% run_mcmc()\n\nmcmc_trace <- res$all_states\n\n\nSo how do our samples look, and what do we get in terms of posteriors? Let’s see.\nResults\nAt this moment, mcmc_trace is a list of tensors of different shapes, dependent on how we defined the parameters. We need to do a bit of post-processing to be able to summarise and display the results.\n\n\n# the actual mcmc samples\n# for the trace plots, we want to have them in shape (500, 4, 49)\n# that is: (number of steps, number of chains, number of parameters)\nsamples <- abind(\n  # rho 1:4\n  as.array(mcmc_trace[[1]] %>% tf$reshape(list(tf$cast(n_steps, tf$int32), tf$cast(n_chains, tf$int32), 4L))),\n  # sigma\n  as.array(mcmc_trace[[2]]),  \n  # sigma_cafe 1:2\n  as.array(mcmc_trace[[3]][ , , 1]),    \n  as.array(mcmc_trace[[3]][ , , 2]), \n  # b\n  as.array(mcmc_trace[[4]]),  \n  # a\n  as.array(mcmc_trace[[5]]),  \n  # mvn 10:49\n  as.array( mcmc_trace[[6]] %>% tf$reshape(list(tf$cast(n_steps, tf$int32), tf$cast(n_chains, tf$int32), 40L))),\n  along = 3) \n\n# the effective sample sizes\n# we want them in shape (4, 49), which is (number of chains * number of parameters)\ness <- mcmc_effective_sample_size(mcmc_trace) \ness <- cbind(\n  # rho 1:4\n  as.matrix(ess[[1]] %>% tf$reshape(list(tf$cast(n_chains, tf$int32), 4L))),\n  # sigma\n  as.matrix(ess[[2]]),  \n  # sigma_cafe 1:2\n  as.matrix(ess[[3]][ , 1, drop = FALSE]),    \n  as.matrix(ess[[3]][ , 2, drop = FALSE]), \n  # b\n  as.matrix(ess[[4]]),  \n  # a\n  as.matrix(ess[[5]]),  \n  # mvn 10:49\n  as.matrix(ess[[6]] %>% tf$reshape(list(tf$cast(n_chains, tf$int32), 40L)))\n  ) \n\n# the rhat values\n# we want them in shape (49), which is (number of parameters)\nrhat <- mcmc_potential_scale_reduction(mcmc_trace)\nrhat <- c(\n  # rho 1:4\n  as.double(rhat[[1]] %>% tf$reshape(list(4L))),\n  # sigma\n  as.double(rhat[[2]]),  \n  # sigma_cafe 1:2\n  as.double(rhat[[3]][1]),    \n  as.double(rhat[[3]][2]), \n  # b\n  as.double(rhat[[4]]),  \n  # a\n  as.double(rhat[[5]]),  \n  # mvn 10:49\n  as.double(rhat[[6]] %>% tf$reshape(list(40L)))\n  ) \n\n\nTrace plots\nHow well do the chains mix?\n\n\nprep_tibble <- function(samples) {\n  as_tibble(samples, .name_repair = ~ c(\"chain_1\", \"chain_2\", \"chain_3\", \"chain_4\")) %>% \n    add_column(sample = 1:n_steps) %>%\n    gather(key = \"chain\", value = \"value\", -sample)\n}\n\nplot_trace <- function(samples) {\n  prep_tibble(samples) %>% \n    ggplot(aes(x = sample, y = value, color = chain)) +\n    geom_line() + \n    theme_light() +\n    theme(legend.position = \"none\",\n          axis.title = element_blank(),\n          axis.text = element_blank(),\n          axis.ticks = element_blank())\n}\n\nplot_traces <- function(sample_array, num_params) {\n  plots <- purrr::map(1:num_params, ~ plot_trace(sample_array[ , , .x]))\n  do.call(grid.arrange, plots)\n}\n\nplot_traces(samples, 49)\n\n\n\n\n\nAwesome! (The first two parameters of rho, the Cholesky factor of the correlation matrix, need to stay fixed at 1 and 0, respectively.)\nNow, on to some summary statistics on the posteriors of the parameters.\nParameters\nLike last time, we display posterior means and standard deviations, as well as the highest posterior density interval (HPDI). We add effective sample sizes and rhat values.\n\n\ncolumn_names <- c(\n  paste0(\"rho_\", 1:4),\n  \"sigma\",\n  paste0(\"sigma_cafe_\", 1:2),\n  \"b\",\n  \"a\",\n  c(rbind(paste0(\"a_cafe_\", 1:20), paste0(\"b_cafe_\", 1:20)))\n)\n\nall_samples <- matrix(samples, nrow = n_steps * n_chains, ncol = 49)\nall_samples <- all_samples %>%\n  as_tibble(.name_repair = ~ column_names)\n\nall_samples %>% glimpse()\n\nmeans <- all_samples %>% \n  summarise_all(list (mean)) %>% \n  gather(key = \"key\", value = \"mean\")\n\nsds <- all_samples %>% \n  summarise_all(list (sd)) %>% \n  gather(key = \"key\", value = \"sd\")\n\nhpdis <-\n  all_samples %>%\n  summarise_all(list(~ list(hdi(.) %>% t() %>% as_tibble()))) %>% \n   unnest() \n \n hpdis_lower <- hpdis %>% select(-contains(\"upper\")) %>%\n   rename(lower0 = lower) %>%\n   gather(key = \"key\", value = \"lower\") %>% \n   arrange(as.integer(str_sub(key, 6))) %>%\n   mutate(key = column_names)\n \n hpdis_upper <- hpdis %>% select(-contains(\"lower\")) %>%\n   rename(upper0 = upper) %>%\n   gather(key = \"key\", value = \"upper\") %>% \n   arrange(as.integer(str_sub(key, 6))) %>%\n   mutate(key = column_names)\n\nsummary <- means %>% \n  inner_join(sds, by = \"key\") %>% \n  inner_join(hpdis_lower, by = \"key\") %>%\n  inner_join(hpdis_upper, by = \"key\")\n\ness <- apply(ess, 2, mean)\n\nsummary_with_diag <- summary %>% add_column(ess = ess, rhat = rhat)\nprint(summary_with_diag, n = 49)\n\n\n# A tibble: 49 x 7\n   key            mean     sd  lower   upper   ess   rhat\n   <chr>         <dbl>  <dbl>  <dbl>   <dbl> <dbl>  <dbl>\n 1 rho_1         1     0       1      1        NaN    NaN   \n 2 rho_2         0     0       0      0       NaN     NaN   \n 3 rho_3        -0.517 0.176  -0.831 -0.195   42.4   1.01\n 4 rho_4         0.832 0.103   0.644  1.000   46.5   1.02\n 5 sigma         0.473 0.0264  0.420  0.523  424.    1.00\n 6 sigma_cafe_1  0.967 0.163   0.694  1.29    97.9   1.00\n 7 sigma_cafe_2  0.607 0.129   0.386  0.861   42.3   1.03\n 8 b            -1.14  0.141  -1.43  -0.864   95.1   1.00\n 9 a             3.66  0.218   3.22   4.07    75.3   1.01\n10 a_cafe_1      4.20  0.192   3.83   4.57    83.9   1.01\n11 b_cafe_1     -1.13  0.251  -1.63  -0.664   63.6   1.02\n12 a_cafe_2      2.17  0.195   1.79   2.54    59.3   1.01\n13 b_cafe_2     -0.923 0.260  -1.42  -0.388   46.0   1.01\n14 a_cafe_3      4.40  0.195   4.02   4.79    56.7   1.01\n15 b_cafe_3     -1.97  0.258  -2.52  -1.51    43.9   1.01\n16 a_cafe_4      3.22  0.199   2.80   3.57    58.7   1.02\n17 b_cafe_4     -1.20  0.254  -1.70  -0.713   36.3   1.01\n18 a_cafe_5      1.86  0.197   1.45   2.20    52.8   1.03\n19 b_cafe_5     -0.113 0.263  -0.615  0.390   34.6   1.04\n20 a_cafe_6      4.26  0.210   3.87   4.67    43.4   1.02\n21 b_cafe_6     -1.30  0.277  -1.80  -0.713   41.4   1.05\n22 a_cafe_7      3.61  0.198   3.23   3.98    44.9   1.01\n23 b_cafe_7     -1.02  0.263  -1.51  -0.489   37.7   1.03\n24 a_cafe_8      3.95  0.189   3.59   4.31    73.1   1.01\n25 b_cafe_8     -1.64  0.248  -2.10  -1.13    60.7   1.02\n26 a_cafe_9      3.98  0.212   3.57   4.37    76.3   1.03\n27 b_cafe_9     -1.29  0.273  -1.83  -0.776   57.8   1.05\n28 a_cafe_10     3.60  0.187   3.24   3.96   104.    1.01\n29 b_cafe_10    -1.00  0.245  -1.47  -0.512   70.4   1.00\n30 a_cafe_11     1.95  0.200   1.56   2.35    55.9   1.03\n31 b_cafe_11    -0.449 0.266  -1.00   0.0619  42.5   1.04\n32 a_cafe_12     3.84  0.195   3.46   4.22    76.0   1.02\n33 b_cafe_12    -1.17  0.259  -1.65  -0.670   62.5   1.03\n34 a_cafe_13     3.88  0.201   3.50   4.29    62.2   1.02\n35 b_cafe_13    -1.81  0.270  -2.30  -1.29    48.3   1.03\n36 a_cafe_14     3.19  0.212   2.82   3.61    65.9   1.07\n37 b_cafe_14    -0.961 0.278  -1.49  -0.401   49.9   1.06\n38 a_cafe_15     4.46  0.212   4.08   4.91    62.0   1.09\n39 b_cafe_15    -2.20  0.290  -2.72  -1.59    47.8   1.11\n40 a_cafe_16     3.41  0.193   3.02   3.78    62.7   1.02\n41 b_cafe_16    -1.07  0.253  -1.54  -0.567   48.5   1.05\n42 a_cafe_17     4.22  0.201   3.82   4.60    58.7   1.01\n43 b_cafe_17    -1.24  0.273  -1.74  -0.703   43.8   1.01\n44 a_cafe_18     5.77  0.210   5.34   6.18    66.0   1.02\n45 b_cafe_18    -1.05  0.284  -1.61  -0.511   49.8   1.02\n46 a_cafe_19     3.23  0.203   2.88   3.65    52.7   1.02\n47 b_cafe_19    -0.232 0.276  -0.808  0.243   45.2   1.01\n48 a_cafe_20     3.74  0.212   3.35   4.21    48.2   1.04\n49 b_cafe_20    -1.09  0.281  -1.58  -0.506   36.5   1.05\nSo what do we have? If you run this “live”, for the rows a_cafe_n resp. b_cafe_n, you see a nice alternation of white and red coloring: For all cafés, the inferred slopes are negative.\nThe inferred slope prior (b) is around -1.14, which is not too far off from the value we used for sampling: 1.\nThe rho posterior estimates, admittedly, are less useful unless you are accustomed to compose Cholesky factors in your head. We compute the resulting posterior correlations and their mean:\n\n\nrhos <- all_samples[ , 1:4] %>% tibble()\n\nrhos <- rhos %>%\n  apply(1, list) %>%\n  unlist(recursive = FALSE) %>%\n  lapply(function(x) matrix(x, byrow = TRUE, nrow = 2) %>% tcrossprod())\n\nrho <- rhos %>% purrr::map(~ .x[1,2]) %>% unlist()\n\nmean_rho <- mean(rho)\nmean_rho\n\n\n-0.5166775\nThe value we used for sampling was -0.7, so we see the regularization effect. In case you’re wondering, for the same data Stan yields an estimate of -0.5.\nFinally, let’s display equivalents to McElreath’s figures illustrating shrinkage on the parameter (café-specific intercepts and slopes) as well as the outcome (morning resp. afternoon waiting times) scales.\nShrinkage\nAs expected, we see that the individual intercepts and slopes are pulled towards the mean – the more, the further away they are from the center.\n\n\n# just like McElreath, compute unpooled estimates directly from data\na_empirical <- d %>% \n  filter(afternoon == 0) %>%\n  group_by(cafe) %>% \n  summarise(a = mean(wait)) %>%\n  select(a)\n\nb_empirical <- d %>% \n  filter(afternoon == 1) %>%\n  group_by(cafe) %>% \n  summarise(b = mean(wait)) %>%\n  select(b) -\n  a_empirical\n\nempirical_estimates <- bind_cols(\n  a_empirical,\n  b_empirical,\n  type = rep(\"data\", 20))\n\nposterior_estimates <- tibble(\n  a = means %>% filter(\n  str_detect(key, \"^a_cafe\")) %>% select(mean) %>% pull(),\n  b = means %>% filter(\n    str_detect(key, \"^b_cafe\")) %>% select(mean)  %>% pull(),\n  type = rep(\"posterior\", 20))\n  \nall_estimates <- bind_rows(empirical_estimates, posterior_estimates)\n\n# compute posterior mean bivariate Gaussian\n# again following McElreath\nmu_est <- c(means[means$key == \"a\", 2], means[means$key == \"b\", 2]) %>% unlist()\nrho_est <- mean_rho\nsa_est <- means[means$key == \"sigma_cafe_1\", 2] %>% unlist()\nsb_est <- means[means$key == \"sigma_cafe_2\", 2] %>% unlist()\ncov_ab <- sa_est * sb_est * rho_est\nsigma_est <- matrix(c(sa_est^2, cov_ab, cov_ab, sb_est^2), ncol=2) \n\nalpha_levels <- c(0.1, 0.3, 0.5, 0.8, 0.99)\nnames(alpha_levels) <- alpha_levels\n\ncontour_data <- plyr::ldply(\n  alpha_levels,\n  ellipse,\n  x = sigma_est,\n  scale = c(1, 1),\n  centre = mu_est\n)\n\nggplot() +\n  geom_point(data = all_estimates, mapping = aes(x = a, y = b, color = type)) + \n  geom_path(data = contour_data, mapping = aes(x = x, y = y, group = .id))\n\n\n\n\n\nThe same behavior is visible on the outcome scale.\n\n\nwait_times  <- all_estimates %>%\n  mutate(morning = a, afternoon = a + b)\n\n# simulate from posterior means\nv <- MASS::mvrnorm(1e4 , mu_est , sigma_est)\nv[ ,2] <- v[ ,1] + v[ ,2] # calculate afternoon wait\n# construct empirical covariance matrix\nsigma_est2 <- cov(v)\nmu_est2 <- mu_est\nmu_est2[2] <- mu_est[1] + mu_est[2]\n\ncontour_data <- plyr::ldply(\n  alpha_levels,\n  ellipse,\n  x = sigma_est2 %>% unname(),\n  scale = c(1, 1),\n  centre = mu_est2\n)\n\nggplot() +\n  geom_point(data = wait_times, mapping = aes(x = morning, y = afternoon, color = type)) + \n  geom_path(data = contour_data, mapping = aes(x = x, y = y, group = .id))\n\n\n\n\n\nWrapping up\nBy now, we hope we have convinced you of the power inherent in Bayesian modeling, as well as conveyed some ideas on how this is achievable with TensorFlow Probability. As with every DSL though, it takes time to proceed from understanding worked examples to design your own models. And not just time – it helps to have seen a lot of different models, focusing on different tasks and applications.\nOn this blog, we plan to loosely follow up on Bayesian modeling with TFP, picking up some of the tasks and challenges elaborated on in the later chapters of McElreath’s book. Thanks for reading!\n\ncf. the Wikipedia article on multilevel models for a collection of terms encountered when dealing with this subject, and e.g. Gelman’s dissection of various ways random effects are defined↩︎\nWe won’t overload this post by explicitly comparing results here, but we did that when writing the code.↩︎\nDisclaimer: We have not verified whether this is an adequate model of the world, but it really doesn’t matter either.↩︎\n",
    "preview": "posts/2019-05-24-varying-slopes/images/thumb.png",
    "last_modified": "2024-11-21T15:54:09+00:00",
    "input_file": {},
    "preview_width": 509,
    "preview_height": 249
  },
  {
    "path": "posts/2019-05-06-tadpoles-on-tensorflow/",
    "title": "Tadpoles on TensorFlow: Hierarchical partial pooling with tfprobability",
    "description": "This post is a first introduction to MCMC modeling with tfprobability, the R interface to TensorFlow Probability (TFP). Our example is a multi-level model describing tadpole mortality, which may be known to the reader from Richard McElreath's wonderful \"Statistical Rethinking\".",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-05-06",
    "categories": [
      "Bayesian Modeling",
      "TensorFlow/Keras"
    ],
    "contents": "\nBefore we jump into the technicalities: This post is, of course, dedicated to McElreath who wrote one of most intriguing books on Bayesian (or should we just say - scientific?) modeling we’re aware of. If you haven’t read Statistical Rethinking, and are interested in modeling, you might definitely want to check it out. In this post, we’re not going to try to re-tell the story: Our clear focus will, instead, be a demonstration of how to do MCMC with tfprobability.1\nConcretely, this post has two parts. The first is a quick overview of how to use tfd_joint_sequential_distribution to construct a model, and then sample from it using Hamiltonian Monte Carlo. This part can be consulted for quick code look-up, or as a frugal template of the whole process.\nThe second part then walks through a multi-level model in more detail, showing how to extract, post-process and visualize sampling as well as diagnostic outputs.\nReedfrogs\nThe data comes with the rethinking package.\n\n\nlibrary(rethinking)\n\ndata(reedfrogs)\nd <- reedfrogs\nstr(d)\n\n\n'data.frame':   48 obs. of  5 variables:\n $ density : int  10 10 10 10 10 10 10 10 10 10 ...\n $ pred    : Factor w/ 2 levels \"no\",\"pred\": 1 1 1 1 1 1 1 1 2 2 ...\n $ size    : Factor w/ 2 levels \"big\",\"small\": 1 1 1 1 2 2 2 2 1 1 ...\n $ surv    : int  9 10 7 10 9 9 10 9 4 9 ...\n $ propsurv: num  0.9 1 0.7 1 0.9 0.9 1 0.9 0.4 0.9 ...\nThe task is modeling survivor counts among tadpoles, where tadpoles are held in tanks of different sizes (equivalently, different numbers of inhabitants). Each row in the dataset describes one tank, with its initial count of inhabitants (density) and number of survivors (surv).\nIn the technical overview part, we build a simple unpooled model that describes every tank in isolation. Then, in the detailed walk-through, we’ll see how to construct a varying intercepts model that allows for information sharing between tanks.\nConstructing models with tfd_joint_distribution_sequential\ntfd_joint_distribution_sequential represents a model as a list of conditional distributions.\nThis is easiest to see on a real example, so we’ll jump right in, creating an unpooled model of the tadpole data.\nThis is the how the model specification would look in Stan:\nmodel{\n    vector[48] p;\n    a ~ normal( 0 , 1.5 );\n    for ( i in 1:48 ) {\n        p[i] = a[tank[i]];\n        p[i] = inv_logit(p[i]);\n    }\n    S ~ binomial( N , p );\n}\nAnd here is tfd_joint_distribution_sequential:\n\n\nlibrary(tensorflow)\n\n# make sure you have at least version 0.7 of TensorFlow Probability \n# as of this writing, it is required of install the master branch:\n# install_tensorflow(version = \"nightly\")\nlibrary(tfprobability)\n\nn_tadpole_tanks <- nrow(d)\nn_surviving <- d$surv\nn_start <- d$density\n\nm1 <- tfd_joint_distribution_sequential(\n  list(\n    # normal prior of per-tank logits\n    tfd_multivariate_normal_diag(\n      loc = rep(0, n_tadpole_tanks),\n      scale_identity_multiplier = 1.5),\n    # binomial distribution of survival counts\n    function(l)\n      tfd_independent(\n        tfd_binomial(total_count = n_start, logits = l),\n        reinterpreted_batch_ndims = 1\n      )\n  )\n)\n\n\nThe model consists of two distributions: Prior means and variances for the 48 tadpole tanks are specified by tfd_multivariate_normal_diag; then tfd_binomial generates survival counts for each tank.\nNote how the first distribution is unconditional, while the second depends on the first. Note too how the second has to be wrapped in tfd_independent to avoid wrong broadcasting. (This is an aspect of tfd_joint_distribution_sequential usage that deserves to be documented more systematically, which is surely going to happen.2 Just think that this functionality was added to TFP master only three weeks ago!)\nAs an aside, the model specification here ends up shorter than in Stan as tfd_binomial optionally takes logits as parameters.\nAs with every TFP distribution, you can do a quick functionality check by sampling from the model:3\n\n\n# sample a batch of 2 values \n# we get samples for every distribution in the model\ns <- m1 %>% tfd_sample(2)\n\n\n[[1]]\nTensor(\"MultivariateNormalDiag/sample/affine_linear_operator/forward/add:0\",\nshape=(2, 48), dtype=float32)\n\n[[2]]\nTensor(\"IndependentJointDistributionSequential/sample/Beta/sample/Reshape:0\",\nshape=(2, 48), dtype=float32)\nand computing log probabilities:\n\n\n# we should get only the overall log probability of the model\nm1 %>% tfd_log_prob(s)\n\n\nt[[1]]\nTensor(\"MultivariateNormalDiag/sample/affine_linear_operator/forward/add:0\",\nshape=(2, 48), dtype=float32)\n\n[[2]]\nTensor(\"IndependentJointDistributionSequential/sample/Beta/sample/Reshape:0\",\nshape=(2, 48), dtype=float32)\nNow, let’s see how we can sample from this model using Hamiltonian Monte Carlo.\nRunning Hamiltonian Monte Carlo in TFP\nWe define a Hamiltonian Monte Carlo kernel with dynamic step size adaptation based on a desired acceptance probability.\n\n\n# number of steps to run burnin\nn_burnin <- 500\n\n# optimization target is the likelihood of the logits given the data\nlogprob <- function(l)\n  m1 %>% tfd_log_prob(list(l, n_surviving))\n\nhmc <- mcmc_hamiltonian_monte_carlo(\n  target_log_prob_fn = logprob,\n  num_leapfrog_steps = 3,\n  step_size = 0.1,\n) %>%\n  mcmc_simple_step_size_adaptation(\n    target_accept_prob = 0.8,\n    num_adaptation_steps = n_burnin\n  )\n\n\nWe then run the sampler, passing in an initial state. If we want to run \\(n\\) chains, that state has to be of length \\(n\\), for every parameter in the model (here we have just one).\nThe sampling function, mcmc_sample_chain, may optionally be passed a trace_fn that tells TFP which kinds of meta information to save. Here we save acceptance ratios and step sizes.\n\n\n# number of steps after burnin\nn_steps <- 500\n# number of chains\nn_chain <- 4\n\n# get starting values for the parameters\n# their shape implicitly determines the number of chains we will run\n# see current_state parameter passed to mcmc_sample_chain below\nc(initial_logits, .) %<-% (m1 %>% tfd_sample(n_chain))\n\n# tell TFP to keep track of acceptance ratio and step size\ntrace_fn <- function(state, pkr) {\n  list(pkr$inner_results$is_accepted,\n       pkr$inner_results$accepted_results$step_size)\n}\n\nres <- hmc %>% mcmc_sample_chain(\n  num_results = n_steps,\n  num_burnin_steps = n_burnin,\n  current_state = initial_logits,\n  trace_fn = trace_fn\n)\n\n\nWhen sampling is finished, we can access the samples as res$all_states:\n\n\nmcmc_trace <- res$all_states\nmcmc_trace\n\n\nTensor(\"mcmc_sample_chain/trace_scan/TensorArrayStack/TensorArrayGatherV3:0\",\nshape=(500, 4, 48), dtype=float32)\nThis is the shape of the samples for l, the 48 per-tank logits: 500 samples times 4 chains times 48 parameters.\nFrom these samples, we can compute effective sample size and \\(rhat\\) (alias mcmc_potential_scale_reduction):\n\n\n# Tensor(\"Mean:0\", shape=(48,), dtype=float32)\ness <- mcmc_effective_sample_size(mcmc_trace) %>% tf$reduce_mean(axis = 0L)\n\n# Tensor(\"potential_scale_reduction/potential_scale_reduction_single_state/sub_1:0\", shape=(48,), dtype=float32)\nrhat <- mcmc_potential_scale_reduction(mcmc_trace)\n\n\nWhereas diagnostic information is available in res$trace:\n\n\n# Tensor(\"mcmc_sample_chain/trace_scan/TensorArrayStack_1/TensorArrayGatherV3:0\",\n# shape=(500, 4), dtype=bool)\nis_accepted <- res$trace[[1]] \n\n# Tensor(\"mcmc_sample_chain/trace_scan/TensorArrayStack_2/TensorArrayGatherV3:0\",\n# shape=(500,), dtype=float32)\nstep_size <- res$trace[[2]] \n\n\nAfter this quick outline, let’s move on to the topic promised in the title: multi-level modeling, or partial pooling. This time, we’ll also take a closer look at sampling results and diagnostic outputs.\nMulti-level tadpoles 4\nThe multi-level model – or varying intercepts model, in this case: we’ll get to varying slopes in a later post – adds a hyperprior to the model. Instead of deciding on a mean and variance of the normal prior the logits are drawn from, we let the model learn means and variances for individual tanks.\nThese per-tank means, while being priors for the binomial logits, are assumed to be normally distributed, and are themselves regularized by a normal prior for the mean and an exponential prior for the variance.\nFor the Stan-savvy, here is the Stan formulation of this model.\n\nmodel{\n    vector[48] p;\n    sigma ~ exponential( 1 );\n    a_bar ~ normal( 0 , 1.5 );\n    a ~ normal( a_bar , sigma );\n    for ( i in 1:48 ) {\n        p[i] = a[tank[i]];\n        p[i] = inv_logit(p[i]);\n    }\n    S ~ binomial( N , p );\n}\n\nAnd here it is with TFP:\n\n\nm2 <- tfd_joint_distribution_sequential(\n  list(\n    # a_bar, the prior for the mean of the normal distribution of per-tank logits\n    tfd_normal(loc = 0, scale = 1.5),\n    # sigma, the prior for the variance of the normal distribution of per-tank logits\n    tfd_exponential(rate = 1),\n    # normal distribution of per-tank logits\n    # parameters sigma and a_bar refer to the outputs of the above two distributions\n    function(sigma, a_bar) \n      tfd_sample_distribution(\n        tfd_normal(loc = a_bar, scale = sigma),\n        sample_shape = list(n_tadpole_tanks)\n      ), \n    # binomial distribution of survival counts\n    # parameter l refers to the output of the normal distribution immediately above\n    function(l)\n      tfd_independent(\n        tfd_binomial(total_count = n_start, logits = l),\n        reinterpreted_batch_ndims = 1\n      )\n  )\n)\n\n\nTechnically, dependencies in tfd_joint_distribution_sequential are defined via spatial proximity in the list: In the learned prior for the logits\n\n\nfunction(sigma, a_bar) \n      tfd_sample_distribution(\n        tfd_normal(loc = a_bar, scale = sigma),\n        sample_shape = list(n_tadpole_tanks)\n      )\n\n\nsigma refers to the distribution immediately above, and a_bar to the one above that.\nAnalogously, in the distribution of survival counts\n\n\nfunction(l)\n      tfd_independent(\n        tfd_binomial(total_count = n_start, logits = l),\n        reinterpreted_batch_ndims = 1\n      )\n\n\nl refers to the distribution immediately preceding its own definition.\nAgain, let’s sample from this model to see if shapes are correct.\n\n\ns <- m2 %>% tfd_sample(2)\ns \n\n\nThey are.\n[[1]]\nTensor(\"Normal/sample_1/Reshape:0\", shape=(2,), dtype=float32)\n\n[[2]]\nTensor(\"Exponential/sample_1/Reshape:0\", shape=(2,), dtype=float32)\n\n[[3]]\nTensor(\"SampleJointDistributionSequential/sample_1/Normal/sample/Reshape:0\",\nshape=(2, 48), dtype=float32)\n\n[[4]]\nTensor(\"IndependentJointDistributionSequential/sample_1/Beta/sample/Reshape:0\",\nshape=(2, 48), dtype=float32)\nAnd to make sure we get one overall log_prob per batch:\n\n\nm2 %>% tfd_log_prob(s)\n\n\nTensor(\"JointDistributionSequential/log_prob/add_3:0\", shape=(2,), dtype=float32)\nTraining this model works like before, except that now the initial state comprises three parameters, a_bar, sigma and l:\n\n\nc(initial_a, initial_s, initial_logits, .) %<-% (m2 %>% tfd_sample(n_chain))\n\n\nHere is the sampling routine:\n\n\n# the joint log probability now is based on three parameters\nlogprob <- function(a, s, l)\n  m2 %>% tfd_log_prob(list(a, s, l, n_surviving))\n\nhmc <- mcmc_hamiltonian_monte_carlo(\n  target_log_prob_fn = logprob,\n  num_leapfrog_steps = 3,\n  # one step size for each parameter\n  step_size = list(0.1, 0.1, 0.1),\n) %>%\n  mcmc_simple_step_size_adaptation(target_accept_prob = 0.8,\n                                   num_adaptation_steps = n_burnin)\n\nrun_mcmc <- function(kernel) {\n  kernel %>% mcmc_sample_chain(\n    num_results = n_steps,\n    num_burnin_steps = n_burnin,\n    current_state = list(initial_a, tf$ones_like(initial_s), initial_logits),\n    trace_fn = trace_fn\n  )\n}\n\nres <- hmc %>% run_mcmc()\n \nmcmc_trace <- res$all_states\n\n\nThis time, mcmc_trace is a list of three: We have\n[[1]]\nTensor(\"mcmc_sample_chain/trace_scan/TensorArrayStack/TensorArrayGatherV3:0\",\nshape=(500, 4), dtype=float32)\n\n[[2]]\nTensor(\"mcmc_sample_chain/trace_scan/TensorArrayStack_1/TensorArrayGatherV3:0\",\nshape=(500, 4), dtype=float32)\n\n[[3]]\nTensor(\"mcmc_sample_chain/trace_scan/TensorArrayStack_2/TensorArrayGatherV3:0\",\nshape=(500, 4, 48), dtype=float32)\nNow let’s create graph nodes for the results and information we’re interested in.\n\n\n# as above, this is the raw result\nmcmc_trace_ <- res$all_states\n\n# we perform some reshaping operations directly in tensorflow\nall_samples_ <-\n  tf$concat(\n    list(\n      mcmc_trace_[[1]] %>% tf$expand_dims(axis = -1L),\n      mcmc_trace_[[2]]  %>% tf$expand_dims(axis = -1L),\n      mcmc_trace_[[3]]\n    ),\n    axis = -1L\n  ) %>%\n  tf$reshape(list(2000L, 50L))\n\n# diagnostics, also as above\nis_accepted_ <- res$trace[[1]]\nstep_size_ <- res$trace[[2]]\n\n# effective sample size\n# again we use tensorflow to get conveniently shaped outputs\ness_ <- mcmc_effective_sample_size(mcmc_trace) \ness_ <- tf$concat(\n  list(\n    ess_[[1]] %>% tf$expand_dims(axis = -1L),\n    ess_[[2]]  %>% tf$expand_dims(axis = -1L),\n    ess_[[3]]\n  ),\n  axis = -1L\n) \n\n# rhat, conveniently post-processed\nrhat_ <- mcmc_potential_scale_reduction(mcmc_trace)\nrhat_ <- tf$concat(\n  list(\n    rhat_[[1]] %>% tf$expand_dims(axis = -1L),\n    rhat_[[2]]  %>% tf$expand_dims(axis = -1L),\n    rhat_[[3]]\n  ),\n  axis = -1L\n) \n\n\nAnd we’re ready to actually run the chains.\n\n\n# so far, no sampling has been done!\n# the actual sampling happens when we create a Session \n# and run the above-defined nodes\nsess <- tf$Session()\neval <- function(...) sess$run(list(...))\n\nc(mcmc_trace, all_samples, is_accepted, step_size, ess, rhat) %<-%\n  eval(mcmc_trace_, all_samples_, is_accepted_, step_size_, ess_, rhat_)\n\n\nThis time, let’s actually inspect those results.\nMulti-level tadpoles: Results\nFirst, how do the chains behave?\nTrace plots\nExtract the samples for a_bar and sigma, as well as one of the learned priors for the logits:\n\n\na_bar <- mcmc_trace[[1]] %>% as.matrix()\nsigma <- mcmc_trace[[2]] %>% as.matrix()\na_1 <- mcmc_trace[[3]][ , , 1] %>% as.matrix()\n\n\nHere’s a trace plot for a_bar:\n\n\nprep_tibble <- function(samples) {\n  as_tibble(samples, .name_repair = ~ c(\"chain_1\", \"chain_2\", \"chain_3\", \"chain_4\")) %>% \n    add_column(sample = 1:500) %>%\n    gather(key = \"chain\", value = \"value\", -sample)\n}\n\nplot_trace <- function(samples, param_name) {\n  prep_tibble(samples) %>% \n    ggplot(aes(x = sample, y = value, color = chain)) +\n    geom_line() + \n    ggtitle(param_name)\n}\n\nplot_trace(a_bar, \"a_bar\")\n\n\n\n\n\nAnd here for sigma and a_1:\n\n\n\n\n\n\nHow about the posterior distributions of the parameters, first and foremost, the varying intercepts a_1 … a_48?\nPosterior distributions\n\n\nplot_posterior <- function(samples) {\n  prep_tibble(samples) %>% \n    ggplot(aes(x = value, color = chain)) +\n    geom_density() +\n    theme_classic() +\n    theme(legend.position = \"none\",\n          axis.title = element_blank(),\n          axis.text = element_blank(),\n          axis.ticks = element_blank())\n    \n}\n\nplot_posteriors <- function(sample_array, num_params) {\n  plots <- purrr::map(1:num_params, ~ plot_posterior(sample_array[ , , .x] %>% as.matrix()))\n  do.call(grid.arrange, plots)\n}\n\nplot_posteriors(mcmc_trace[[3]], dim(mcmc_trace[[3]])[3])\n\n\n\n\n\nNow let’s see the corresponding posterior means and highest posterior density intervals.\n(The below code includes the hyperpriors in summary as we’ll want to display a complete precis-like output soon.)\nPosterior means and HPDIs\n\n\nall_samples <- all_samples %>%\n  as_tibble(.name_repair = ~ c(\"a_bar\", \"sigma\", paste0(\"a_\", 1:48))) \n\nmeans <- all_samples %>% \n  summarise_all(list (~ mean)) %>% \n  gather(key = \"key\", value = \"mean\")\n\nsds <- all_samples %>% \n  summarise_all(list (~ sd)) %>% \n  gather(key = \"key\", value = \"sd\")\n\nhpdis <-\n  all_samples %>%\n  summarise_all(list(~ list(hdi(.) %>% t() %>% as_tibble()))) %>% \n  unnest() \n\nhpdis_lower <- hpdis %>% select(-contains(\"upper\")) %>%\n  rename(lower0 = lower) %>%\n  gather(key = \"key\", value = \"lower\") %>% \n  arrange(as.integer(str_sub(key, 6))) %>%\n  mutate(key = c(\"a_bar\", \"sigma\", paste0(\"a_\", 1:48)))\n\nhpdis_upper <- hpdis %>% select(-contains(\"lower\")) %>%\n  rename(upper0 = upper) %>%\n  gather(key = \"key\", value = \"upper\") %>% \n  arrange(as.integer(str_sub(key, 6))) %>%\n  mutate(key = c(\"a_bar\", \"sigma\", paste0(\"a_\", 1:48)))\n\nsummary <- means %>% \n  inner_join(sds, by = \"key\") %>% \n  inner_join(hpdis_lower, by = \"key\") %>%\n  inner_join(hpdis_upper, by = \"key\")\n\n\nsummary %>% \n  filter(!key %in% c(\"a_bar\", \"sigma\")) %>%\n  mutate(key_fct = factor(key, levels = unique(key))) %>%\n  ggplot(aes(x = key_fct, y = mean, ymin = lower, ymax = upper)) +\n   geom_pointrange() + \n   coord_flip() +  \n   xlab(\"\") + ylab(\"post. mean and HPDI\") +\n   theme_minimal() \n\n\n\n\n\nNow for an equivalent to precis. We already computed means, standard deviations and the HPDI interval.\nLet’s add n_eff, the effective number of samples, and rhat, the Gelman-Rubin statistic.\nComprehensive summary (a.k.a. “precis”)\n\n\nis_accepted <- is_accepted %>% as.integer() %>% mean()\nstep_size <- purrr::map(step_size, mean)\n\ness <- apply(ess, 2, mean)\n\nsummary_with_diag <- summary %>% add_column(ess = ess, rhat = rhat)\nsummary_with_diag\n\n\n# A tibble: 50 x 7\n   key    mean    sd  lower upper   ess  rhat\n   <chr> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl>\n 1 a_bar  1.35 0.266  0.792  1.87 405.   1.00\n 2 sigma  1.64 0.218  1.23   2.05  83.6  1.00\n 3 a_1    2.14 0.887  0.451  3.92  33.5  1.04\n 4 a_2    3.16 1.13   1.09   5.48  23.7  1.03\n 5 a_3    1.01 0.698 -0.333  2.31  65.2  1.02\n 6 a_4    3.02 1.04   1.06   5.05  31.1  1.03\n 7 a_5    2.11 0.843  0.625  3.88  49.0  1.05\n 8 a_6    2.06 0.904  0.496  3.87  39.8  1.03\n 9 a_7    3.20 1.27   1.11   6.12  14.2  1.02\n10 a_8    2.21 0.894  0.623  4.18  44.7  1.04\n# ... with 40 more rows\nFor the varying intercepts, effective sample sizes are pretty low, indicating we might want to investigate possible reasons.\nLet’s also display posterior survival probabilities, analogously to figure 13.2 in the book.\nPosterior survival probabilities\n\n\nsim_tanks <- rnorm(8000, a_bar, sigma)\ntibble(x = sim_tanks) %>% ggplot(aes(x = x)) + geom_density() + xlab(\"distribution of per-tank logits\")\n\n\n\n\n\n\n\n# our usual sigmoid by another name (undo the logit)\nlogistic <- function(x) 1/(1 + exp(-x))\nprobs <- map_dbl(sim_tanks, logistic)\ntibble(x = probs) %>% ggplot(aes(x = x)) + geom_density() + xlab(\"probability of survival\")\n\n\n\n\n\nFinally, we want to make sure we see the shrinkage behavior displayed in figure 13.1 in the book.\nShrinkage\n\n\nsummary %>% \n  filter(!key %in% c(\"a_bar\", \"sigma\")) %>%\n  select(key, mean) %>%\n  mutate(est_survival = logistic(mean)) %>%\n  add_column(act_survival = d$propsurv) %>%\n  select(-mean) %>%\n  gather(key = \"type\", value = \"value\", -key) %>%\n  ggplot(aes(x = key, y = value, color = type)) +\n  geom_point() +\n  geom_hline(yintercept = mean(d$propsurv), size = 0.5, color = \"cyan\" ) +\n  xlab(\"\") +\n  ylab(\"\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank())\n\n\n\n\n\nWe see results similar in spirit to McElreath’s: estimates are shrunken to the mean (the cyan-colored line). Also, shrinkage seems to be more active in smaller tanks, which are the lower-numbered ones on the left of the plot.\nOutlook\nIn this post, we saw how to construct a varying intercepts model with tfprobability, as well as how to extract sampling results and relevant diagnostics. In an upcoming post, we’ll move on to varying slopes.\nWith non-negligible probability, our example will build on one of Mc Elreath’s again…\nThanks for reading!\n\nFor a supplementary introduction to Bayesian modeling focusing on complete coverage, yet starting from the very beginning, you might want to consult Ben Lambert’s Student’s Guide to Bayesian Statistics.↩︎\nAs of today, lots of useful information is available in Modeling with JointDistribution and Multilevel Modeling Primer, but some experimentation may needed to adapt the – numerous! – examples to your needs.↩︎\nUpdated footnote, as of May 13th: When this post was written, we were still experimenting with the use of tf.function from R, so it seemed safest to code the complete example in graph mode. The next post on MCMC will use eager execution, and show how to achieve good performance by placing the actual sampling procedure on the graph.↩︎\nyep, it’s a quote↩︎\n",
    "preview": "posts/2019-05-06-tadpoles-on-tensorflow/images/thumb.png",
    "last_modified": "2024-11-21T15:50:21+00:00",
    "input_file": {},
    "preview_width": 1612,
    "preview_height": 659
  },
  {
    "path": "posts/2019-04-24-autoregressive-flows/",
    "title": "Experimenting with autoregressive flows in TensorFlow Probability",
    "description": "Continuing from the recent introduction to bijectors in TensorFlow Probability (TFP), this post brings autoregressivity to the table. Using TFP through the new R package tfprobability, we look at the implementation of masked autoregressive flows (MAF) and put them to use on two different datasets.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-04-24",
    "categories": [
      "Probabilistic ML/DL",
      "Generative Models",
      "TensorFlow/Keras"
    ],
    "contents": "\nIn the first part of this mini-series on autoregressive flow models, we looked at bijectors in TensorFlow Probability (TFP), and saw how to use them for sampling and density estimation. We singled out the affine bijector to demonstrate the mechanics of flow construction: We start from a distribution that is easy to sample from, and that allows for straightforward calculation of its density. Then, we attach some number of invertible transformations, optimizing for data-likelihood under the final transformed distribution. The efficiency of that (log)likelihood calculation is where normalizing flows excel: Loglikelihood under the (unknown) target distribution is obtained as a sum of the density under the base distribution of the inverse-transformed data plus the absolute log determinant of the inverse Jacobian.\nNow, an affine flow will seldom be powerful enough to model nonlinear, complex transformations. In constrast, autoregressive models have shown substantive success in density estimation as well as sample generation. Combined with more involved architectures, feature engineering, and extensive compute, the concept of autoregressivity has powered – and is powering – state-of-the-art architectures in areas such as image, speech and video modeling.\nThis post will be concerned with the building blocks of autoregressive flows in TFP. While we won’t exactly be building state-of-the-art models, we’ll try to understand and play with some major ingredients, hopefully enabling the reader to do her own experiments on her own data.\nThis post has three parts: First, we’ll look at autoregressivity and its implementation in TFP. Then, we try to (approximately) reproduce one of the experiments in the “MAF paper” (Masked Autoregressive Flows for Distribution Estimation (Papamakarios, Pavlakou, and Murray 2017)) – essentially a proof of concept. Finally, for the third time on this blog, we come back to the task of analysing audio data, with mixed results.\nAutoregressivity and masking\nIn distribution estimation, autoregressivity enters the scene via the chain rule of probability that decomposes a joint density into a product of conditional densities:\n\\[\np(\\mathbf{x}) = \\prod_{i}p(\\mathbf{x}_i|\\mathbf{x}_{1:i−1})\n\\]\nIn practice, this means that autoregressive models have to impose an order on the variables - an order which might or might not “make sense.” Approaches here include choosing orderings at random and/or using different orderings for each layer.\nWhile in recurrent neural networks, autoregressivity is conserved due to the recurrence relation inherent in state updating, it is not clear a priori how autoregressivity is to be achieved in a densely connected architecture. A computationally efficient solution was proposed in MADE: Masked Autoencoder for Distribution Estimation(Germain et al. 2015): Starting from a densely connected layer, mask out all connections that should not be allowed, i.e., all connections from input feature \\(i\\) to said layer’s activations \\(1 ... i-1\\). Or expressed differently, activation \\(i\\) may be connected to input features \\(1 ... i-1\\) only. Then when adding more layers, care must be taken to ensure that all required connections are masked so that at the end, output \\(i\\) will only ever have seen inputs \\(1 ... i-1\\).\nThus masked autoregressive flows are a fusion of two major approaches - autoregressive models (which need not be flows) and flows (which need not be autoregressive). In TFP these are provided by MaskedAutoregressiveFlow,1 to be used as a bijector in a TransformedDistribution.2\nWhile the documentation shows how to use this bijector, the step from theoretical understanding to coding a “black box” may seem wide. If you’re anything like the author, here you might feel the urge to “look under the hood” and verify that things really are the way you’re assuming. So let’s give in to curiosity and allow ourselves a little escapade into the source code.\nPeeking ahead, this is how we’ll construct a masked autoregressive flow in TFP (again using the still new-ish R bindings provided by tfprobability):\n\n\nlibrary(tfprobability)\n\nmaf <- tfb_masked_autoregressive_flow(\n    shift_and_log_scale_fn = tfb_masked_autoregressive_default_template(\n      hidden_layers = list(num_hidden, num_hidden),\n      activation = tf$nn$tanh)\n)\n\n\nPulling apart the relevant entities here, tfb_masked_autoregressive_flow is a bijector, with the usual methods tfb_forward(), tfb_inverse(), tfb_forward_log_det_jacobian() and tfb_inverse_log_det_jacobian().\nThe default shift_and_log_scale_fn, tfb_masked_autoregressive_default_template, constructs a little neural network of its own, with a configurable number of hidden units per layer, a configurable activation function and optionally, other configurable parameters to be passed to the underlying dense layers. It’s these dense layers that have to respect the autoregressive property. Can we take a look at how this is done? Yes we can, provided we’re not afraid of a little Python.\nmasked_autoregressive_default_template (now leaving out the tfb_ as we’ve entered Python-land) uses masked_dense to do what you’d suppose a thus-named function might be doing: construct a dense layer that has part of the weight matrix masked out. How? We’ll see after a few Python setup statements.\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_probability as tfp\ntfd = tfp.distributions\ntfb = tfp.bijectors\ntf.enable_eager_execution()\n\nThe following code snippets are taken from masked_dense (in its current form on master), and when possible, simplified for better readability, accommodating just the specifics of the chosen example - a toy matrix of shape 2x3:\n\n# construct some toy input data (this line obviously not from the original code)\ninputs = tf.constant(np.arange(1.,7), shape = (2, 3))\n\n# (partly) determine shape of mask from shape of input\ninput_depth = tf.compat.dimension_value(inputs.shape.with_rank_at_least(1)[-1])\nnum_blocks = input_depth\nnum_blocks # 3\n\nOur toy layer should have 4 units:\n\nunits = 4\n\nThe mask is initialized to all zeros. Considering it will be used to elementwise multiply the weight matrix, we’re a bit surprised at its shape (shouldn’t it be the other way round?). No worries; all will turn out correct in the end.\n\nmask = np.zeros([units, input_depth], dtype=tf.float32.as_numpy_dtype())\nmask\n\narray([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]], dtype=float32)\nNow to “whitelist” the allowed connections, we have to fill in ones whenever information flow is allowed by the autoregressive property:\n\ndef _gen_slices(num_blocks, n_in, n_out):\n  slices = []\n  col = 0\n  d_in = n_in // num_blocks\n  d_out = n_out // num_blocks\n  row = d_out \n  for _ in range(num_blocks):\n    row_slice = slice(row, None)\n    col_slice = slice(col, col + d_in)\n    slices.append([row_slice, col_slice])\n    col += d_in\n    row += d_out\n  return slices\n\nslices = _gen_slices(num_blocks, input_depth, units)\nfor [row_slice, col_slice] in slices:\n  mask[row_slice, col_slice] = 1\n\nmask\n\narray([[0., 0., 0.],\n       [1., 0., 0.],\n       [1., 1., 0.],\n       [1., 1., 1.]], dtype=float32)\nAgain, does this look mirror-inverted? A transpose fixes shape and logic both:\n\nmask = mask.t\nmask\n\narray([[0., 1., 1., 1.],\n       [0., 0., 1., 1.],\n       [0., 0., 0., 1.]], dtype=float32)\nNow that we have the mask, we can create the layer (interestingly, as of this writing not (yet?) a tf.keras layer):\n\nlayer = tf.compat.v1.layers.Dense(\n        units,\n        kernel_initializer=masked_initializer, # 1\n        kernel_constraint=lambda x: mask * x   # 2\n        )\n\nHere we see masking going on in two ways. For one, the weight initializer is masked:\n\nkernel_initializer = tf.compat.v1.glorot_normal_initializer()\n\ndef masked_initializer(shape, dtype=None, partition_info=None):\n return mask * kernel_initializer(shape, dtype, partition_info)\n\nAnd secondly, a kernel constraint makes sure that after optimization, the relative units are zeroed out again:\n\nkernel_constraint=lambda x: mask * x \n\nJust for fun, let’s apply the layer to our toy input:\n\nlayer.apply(inputs)\n\n<tf.Tensor: id=30, shape=(2, 4), dtype=float64, numpy=\narray([[ 0.        , -0.7489589 , -0.43329933,  1.42710014],\n       [ 0.        , -2.9958356 , -1.71647246,  1.09258015]])>\nZeroes where expected. And double-checking on the weight matrix…\n\nlayer.kernel\n\n<tf.Variable 'dense/kernel:0' shape=(3, 4) dtype=float64, numpy=\narray([[ 0.        , -0.7489589 , -0.42214942, -0.6473454 ],\n       [-0.        ,  0.        , -0.00557496, -0.46692933],\n       [-0.        , -0.        , -0.        ,  1.00276807]])>\nGood. Now hopefully after this little deep dive, things have become a bit more concrete. Of course in a bigger model, the autoregressive property has to be conserved between layers as well.\nOn to the second topic, application of MAF to a real-world dataset.\nMasked Autoregressive Flow\nThe MAF paper(Papamakarios, Pavlakou, and Murray 2017) applied masked autoregressive flows (as well as single-layer-MADE(Germain et al. 2015) and Real NVP (Dinh, Sohl-Dickstein, and Bengio 2016)) to a number of datasets, including MNIST, CIFAR-10 and several datasets from the UCI Machine Learning Repository.\nWe pick one of the UCI datasets: Gas sensors for home activity monitoring. On this dataset, the MAF authors obtained the best results using a MAF with 10 flows, so this is what we will try.\n\n\n\nFigure 1: Figure from Masked Autoregressive Flow for Density Estimation(Papamakarios, Pavlakou, and Murray 2017)\n\n\n\nCollecting information from the paper, we know that\ndata was included from the file ethylene_CO.txt only;\ndiscrete columns were eliminated, as well as all columns with correlations > .98; and\nthe remaining 8 columns3 were standardised (z-transformed).\nRegarding the neural network architecture, we gather that\neach of the 10 MAF layers was followed by a batchnorm;\nas to feature order, the first MAF layer used the variable order that came with the dataset; then every consecutive layer reversed it;\nspecifically for this dataset and as opposed to all other UCI datasets, tanh was used for activation instead of relu;\nthe Adam optimizer was used, with a learning rate of 1e-4;\nthere were two hidden layers for each MAF, with 100 units each;\ntraining went on until no improvement occurred for 30 consecutive epochs on the validation set; and\nthe base distribution was a multivariate Gaussian.\nThis is all useful information for our attempt to estimate this dataset, but the essential bit is this. In case you knew the dataset already, you might have been wondering how the authors would deal with the dimensionality of the data: It is a time series, and the MADE architecture explored above introduces autoregressivity between features, not time steps. So how is the additional temporal autoregressivity to be handled? The answer is: The time dimension is essentially removed. In the authors’ words,\n\n[…] it is a time series but was treated as if each example were an i.i.d. sample from the marginal distribution.\n\nThis undoubtedly is useful information for our present modeling attempt, but it also tells us something else: We might have to look beyond MADE layers for actual time series modeling.\nNow though let’s look at this example of using MAF for multivariate modeling, with no time or spatial dimension to be taken into account.\nFollowing the hints the authors gave us, this is what we do.\n\n\n# load libraries -------------------------------------------------------------\nlibrary(tensorflow)\nlibrary(tfprobability)\n\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nlibrary(tfdatasets)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(purrr)\nlibrary(caret)\n\n# read data ------------------------------------------------------------------\ndf <- read_table2(\"ethylene_CO.txt\",\n                  skip = 1,\n                  col_names = FALSE)\nglimpse(df)\n\n\nObservations: 4,208,261\nVariables: 19\n$ X1  <dbl> 0.00, 0.01, 0.01, 0.03, 0.04, 0.05, 0.06, 0.07, 0.07, 0.09,...\n$ X2  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n$ X3  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n$ X4  <dbl> -50.85, -49.40, -40.04, -47.14, -33.58, -48.59, -48.27, -47.14,... \n$ X5  <dbl> -1.95, -5.53, -16.09, -10.57, -20.79, -11.54, -9.11, -4.56,...\n$ X6  <dbl> -41.82, -42.78, -27.59, -32.28, -33.25, -36.16, -31.31, -16.57,... \n$ X7  <dbl> 1.30, 0.49, 0.00, 4.40, 6.03, 6.03, 5.37, 4.40, 23.98, 2.77,...\n$ X8  <dbl> -4.07, 3.58, -7.16, -11.22, 3.42, 0.33, -7.97, -2.28, -2.12,...\n$ X9  <dbl> -28.73, -34.55, -42.14, -37.94, -34.22, -29.05, -30.34, -24.35,...\n$ X10 <dbl> -13.49, -9.59, -12.52, -7.16, -14.46, -16.74, -8.62, -13.17,...\n$ X11 <dbl> -3.25, 5.37, -5.86, -1.14, 8.31, -1.14, 7.00, -6.34, -0.81,...\n$ X12 <dbl> 55139.95, 54395.77, 53960.02, 53047.71, 52700.28, 51910.52,...\n$ X13 <dbl> 50669.50, 50046.91, 49299.30, 48907.00, 48330.96, 47609.00,...\n$ X14 <dbl> 9626.26, 9433.20, 9324.40, 9170.64, 9073.64, 8982.88, 8860.51,...\n$ X15 <dbl> 9762.62, 9591.21, 9449.81, 9305.58, 9163.47, 9021.08, 8966.48,...\n$ X16 <dbl> 24544.02, 24137.13, 23628.90, 23101.66, 22689.54, 22159.12,...\n$ X17 <dbl> 21420.68, 20930.33, 20504.94, 20101.42, 19694.07, 19332.57,...\n$ X18 <dbl> 7650.61, 7498.79, 7369.67, 7285.13, 7156.74, 7067.61, 6976.13,...\n$ X19 <dbl> 6928.42, 6800.66, 6697.47, 6578.52, 6468.32, 6385.31, 6300.97,...\n\n\n# we don't know if we'll end up with the same columns as the authors did,\n# but we try (at least we do end up with 8 columns)\ndf <- df[,-(1:3)]\nhc <- findCorrelation(cor(df), cutoff = 0.985)\ndf2 <- df[,-c(hc)]\n\n# scale\ndf2 <- scale(df2)\ndf2\n\n\n# A tibble: 4,208,261 x 8\n      X4     X5     X8    X9    X13    X16    X17   X18\n   <dbl>  <dbl>  <dbl> <dbl>  <dbl>  <dbl>  <dbl> <dbl>\n 1 -50.8  -1.95  -4.07 -28.7 50670. 24544. 21421. 7651.\n 2 -49.4  -5.53   3.58 -34.6 50047. 24137. 20930. 7499.\n 3 -40.0 -16.1   -7.16 -42.1 49299. 23629. 20505. 7370.\n 4 -47.1 -10.6  -11.2  -37.9 48907  23102. 20101. 7285.\n 5 -33.6 -20.8    3.42 -34.2 48331. 22690. 19694. 7157.\n 6 -48.6 -11.5    0.33 -29.0 47609  22159. 19333. 7068.\n 7 -48.3  -9.11  -7.97 -30.3 47047. 21932. 19028. 6976.\n 8 -47.1  -4.56  -2.28 -24.4 46758. 21504. 18780. 6900.\n 9 -42.3  -2.77  -2.12 -27.6 46197. 21125. 18439. 6827.\n10 -44.6   3.58  -0.65 -35.5 45652. 20836. 18209. 6790.\n# … with 4,208,251 more rows\nNow set up the data generation process:\n\n\n# train-test split\nn_rows <- nrow(df2) # 4208261\ntrain_ids <- sample(1:n_rows, 0.5 * n_rows)\nx_train <- df2[train_ids, ]\nx_test <- df2[-train_ids, ]\n\n# create datasets\nbatch_size <- 100\ntrain_dataset <- tf$cast(x_train, tf$float32) %>%\n  tensor_slices_dataset %>%\n  dataset_batch(batch_size)\n\ntest_dataset <- tf$cast(x_test, tf$float32) %>%\n  tensor_slices_dataset %>%\n  dataset_batch(nrow(x_test))\n\n\nTo construct the flow, the first thing needed is the base distribution.\n\n\nbase_dist <- tfd_multivariate_normal_diag(loc = rep(0, ncol(df2)))\n\n\nNow for the flow, by default constructed with batchnorm and permutation of feature order.\n\n\nnum_hidden <- 100\ndim <- ncol(df2)\n\nuse_batchnorm <- TRUE\nuse_permute <- TRUE\nnum_mafs <-10\nnum_layers <- 3 * num_mafs\n\nbijectors <- vector(mode = \"list\", length = num_layers)\n\nfor (i in seq(1, num_layers, by = 3)) {\n  maf <- tfb_masked_autoregressive_flow(\n    shift_and_log_scale_fn = tfb_masked_autoregressive_default_template(\n      hidden_layers = list(num_hidden, num_hidden),\n      activation = tf$nn$tanh))\n  bijectors[[i]] <- maf\n  if (use_batchnorm)\n    bijectors[[i + 1]] <- tfb_batch_normalization()\n  if (use_permute)\n    bijectors[[i + 2]] <- tfb_permute((ncol(df2) - 1):0)\n}\n\nif (use_permute) bijectors <- bijectors[-num_layers]\n\nflow <- bijectors %>%\n  discard(is.null) %>%\n  # tfb_chain expects arguments in reverse order of application\n  rev() %>%\n  tfb_chain()\n\ntarget_dist <- tfd_transformed_distribution(\n  distribution = base_dist,\n  bijector = flow\n)\n\n\nAnd configuring the optimizer:\n\n\noptimizer <- tf$train$AdamOptimizer(1e-4)\n\n\nUnder that isotropic Gaussian we chose as a base distribution, how likely are the data?\n\n\nbase_loglik <- base_dist %>% \n  tfd_log_prob(x_train) %>% \n  tf$reduce_mean()\nbase_loglik %>% as.numeric()        # -11.33871\n\nbase_loglik_test <- base_dist %>% \n  tfd_log_prob(x_test) %>% \n  tf$reduce_mean()\nbase_loglik_test %>% as.numeric()   # -11.36431\n\n\nAnd, just as a quick sanity check: What is the loglikelihood of the data under the transformed distribution before any training?\n\n\ntarget_loglik_pre <-\n  target_dist %>% tfd_log_prob(x_train) %>% tf$reduce_mean()\ntarget_loglik_pre %>% as.numeric()        # -11.22097\n\ntarget_loglik_pre_test <-\n  target_dist %>% tfd_log_prob(x_test) %>% tf$reduce_mean()\ntarget_loglik_pre_test %>% as.numeric()   # -11.36431\n\n\nThe values match - good. Here now is the training loop. Being impatient, we already keep checking the loglikelihood on the (complete) test set to see if we’re making any progress.\n\n\nn_epochs <- 10\n\nfor (i in 1:n_epochs) {\n  \n  agg_loglik <- 0\n  num_batches <- 0\n  iter <- make_iterator_one_shot(train_dataset)\n  \n  until_out_of_range({\n    batch <- iterator_get_next(iter)\n    loss <-\n      function()\n        - tf$reduce_mean(target_dist %>% tfd_log_prob(batch))\n    optimizer$minimize(loss)\n    \n    loglik <- tf$reduce_mean(target_dist %>% tfd_log_prob(batch))\n    agg_loglik <- agg_loglik + loglik\n    num_batches <- num_batches + 1\n    \n    test_iter <- make_iterator_one_shot(test_dataset)\n    test_batch <- iterator_get_next(test_iter)\n    loglik_test_current <- target_dist %>% tfd_log_prob(test_batch) %>% tf$reduce_mean()\n    \n    if (num_batches %% 100 == 1)\n      cat(\n        \"Epoch \",\n        i,\n        \": \",\n        \"Batch \",\n        num_batches,\n        \": \",\n        (agg_loglik %>% as.numeric()) / num_batches,\n        \" --- test: \",\n        loglik_test_current %>% as.numeric(),\n        \"\\n\"\n      )\n  })\n}\n\n\nWith both training and test sets amounting to over 2 million records each, we did not have the patience to run this model until no improvement occurred for 30 consecutive epochs on the validation set (like the authors did). However, the picture we get from one complete epoch’s run is pretty clear: The setup seems to work pretty okay.\nEpoch  1 :  Batch      1:  -8.212026  --- test:  -10.09264 \nEpoch  1 :  Batch   1001:   2.222953  --- test:   1.894102 \nEpoch  1 :  Batch   2001:   2.810996  --- test:   2.147804 \nEpoch  1 :  Batch   3001:   3.136733  --- test:   3.673271 \nEpoch  1 :  Batch   4001:   3.335549  --- test:   4.298822 \nEpoch  1 :  Batch   5001:   3.474280  --- test:   4.502975 \nEpoch  1 :  Batch   6001:   3.606634  --- test:   4.612468 \nEpoch  1 :  Batch   7001:   3.695355  --- test:   4.146113 \nEpoch  1 :  Batch   8001:   3.767195  --- test:   3.770533 \nEpoch  1 :  Batch   9001:   3.837641  --- test:   4.819314 \nEpoch  1 :  Batch  10001:   3.908756  --- test:   4.909763 \nEpoch  1 :  Batch  11001:   3.972645  --- test:   3.234356 \nEpoch  1 :  Batch  12001:   4.020613  --- test:   5.064850 \nEpoch  1 :  Batch  13001:   4.067531  --- test:   4.916662 \nEpoch  1 :  Batch  14001:   4.108388  --- test:   4.857317 \nEpoch  1 :  Batch  15001:   4.147848  --- test:   5.146242 \nEpoch  1 :  Batch  16001:   4.177426  --- test:   4.929565 \nEpoch  1 :  Batch  17001:   4.209732  --- test:   4.840716 \nEpoch  1 :  Batch  18001:   4.239204  --- test:   5.222693 \nEpoch  1 :  Batch  19001:   4.264639  --- test:   5.279918 \nEpoch  1 :  Batch  20001:   4.291542  --- test:   5.29119 \nEpoch  1 :  Batch  21001:   4.314462  --- test:   4.872157 \nEpoch  2 :  Batch      1:   5.212013  --- test:   4.969406 \nWith these training results, we regard the proof of concept as basically successful. However, from our experiments we also have to say that the choice of hyperparameters seems to matter a lot. For example, use of the relu activation function instead of tanh resulted in the network basically learning nothing. (As per the authors, relu worked fine on other datasets that had been z-transformed in just the same way.)\nBatch normalization here was obligatory - and this might go for flows in general. The permutation bijectors, on the other hand, did not make much of a difference on this dataset. Overall the impression is that for flows, we might either need a “bag of tricks” (like is commonly said about GANs), or more involved architectures (see “Outlook” below).\nFinally, we wind up with an experiment, coming back to our favorite audio data, already featured in two posts: Simple Audio Classification with Keras and Audio classification with Keras: Looking closer at the non-deep learning parts.\nAnalysing audio data with MAF\nThe dataset in question consists of recordings of 30 words, pronounced by a number of different speakers. In those previous posts, a convnet was trained to map spectrograms to those 30 classes. Now instead we want to try something different: Train an MAF on one of the classes - the word “zero,” say - and see if we can use the trained network to mark “non-zero” words as less likely: perform anomaly detection, in a way. Spoiler alert: The results were not too encouraging, and if you are interested in a task like this, you might want to consider a different architecture (again, see “Outlook” below).\nNonetheless, we quickly relate what was done, as this task is a nice example of handling data where features vary over more than one axis.\nPreprocessing starts as in the aforementioned previous posts. Here though, we explicitly use eager execution, and may sometimes hard-code known values to keep the code snippets short.\n\nlibrary(tensorflow)\nlibrary(tfprobability)\n\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nlibrary(tfdatasets)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(purrr)\nlibrary(caret)\nlibrary(stringr)\n\n# make decode_wav() run with the current release 1.13.1 as well as with the current master branch\ndecode_wav <- function() if (reticulate::py_has_attr(tf, \"audio\")) tf$audio$decode_wav\n  else tf$contrib$framework$python$ops$audio_ops$decode_wav\n# same for stft()\nstft <- function() if (reticulate::py_has_attr(tf, \"signal\")) tf$signal$stft else tf$spectral$stft\n\nfiles <- fs::dir_ls(path = \"audio/data_1/speech_commands_v0.01/\", # replace by yours\n                    recursive = TRUE,\n                    glob = \"*.wav\")\n\nfiles <- files[!str_detect(files, \"background_noise\")]\n\ndf <- tibble(\n  fname = files,\n  class = fname %>%\n    str_extract(\"v0.01/.*/\") %>%\n    str_replace_all(\"v0.01/\", \"\") %>%\n    str_replace_all(\"/\", \"\")\n)\n\nWe train the MAF on pronunciations of the word “zero.”\n\n\nfor (c in unique(df$class)) {\n  assign(paste0(\"df_\", c), df %>% filter(class == c) %>% select(fname))\n}\n\ndf_ <- df_zero # 2 * 1178 rows\nidx_train <- sample(1:nrow(df_), 0.5 * nrow(df_))\ndf_train <- df_[idx_train, ]\ndf_test <- df_[-idx_train, ]\n\n\nFollowing the approach detailed in Audio classification with Keras: Looking closer at the non-deep learning parts, we’d like to train the network on spectrograms instead of the raw time domain data.\nUsing the same settings for frame_length and frame_step of the Short Term Fourier Transform as in that post, we’d arrive at data shaped number of frames x number of FFT coefficients. To make this work with the masked_dense() employed in tfb_masked_autoregressive_flow(), the data would then have to be flattened, yielding an impressive 25186 features in the joint distribution.\nWith the architecture defined as above in the GAS example, this lead to the network not making much progress. Neither did leaving the data in time domain form, with 16000 features in the joint distribution. Thus, we decided to work with the FFT coefficients computed over the complete window instead, resulting in 257 joint features.4\n\n\nbatch_size <- 100\n\nsampling_rate <- 16000L\ndata_generator <- function(df,\n                           batch_size) {\n  \n  ds <- tensor_slices_dataset(df) \n  \n  ds <- ds %>%\n    dataset_map(function(obs) {\n      wav <-\n        decode_wav()(tf$read_file(tf$reshape(obs$fname, list())))\n      samples <- wav$audio[ ,1]\n      \n      # some wave files have fewer than 16000 samples\n      padding <- list(list(0L, sampling_rate - tf$shape(samples)[1]))\n      padded <- tf$pad(samples, padding)\n      \n      stft_out <- stft()(padded, 16000L, 1L, 512L)\n      magnitude_spectrograms <- tf$abs(stft_out) %>% tf$squeeze()\n    })\n  \n  ds %>% dataset_batch(batch_size)\n  \n}\n\nds_train <- data_generator(df_train, batch_size)\nbatch <- ds_train %>% \n  make_iterator_one_shot() %>%\n  iterator_get_next()\n\ndim(batch) # 100 x 257\n\n\nTraining then proceeded as on the GAS dataset.\n\n\n# define MAF\nbase_dist <-\n  tfd_multivariate_normal_diag(loc = rep(0, dim(batch)[2]))\n\nnum_hidden <- 512 \nuse_batchnorm <- TRUE\nuse_permute <- TRUE\nnum_mafs <- 10 \nnum_layers <- 3 * num_mafs\n\n# store bijectors in a list\nbijectors <- vector(mode = \"list\", length = num_layers)\n\n# fill list, optionally adding batchnorm and permute bijectors\nfor (i in seq(1, num_layers, by = 3)) {\n  maf <- tfb_masked_autoregressive_flow(\n    shift_and_log_scale_fn = tfb_masked_autoregressive_default_template(\n      hidden_layers = list(num_hidden, num_hidden),\n      activation = tf$nn$tanh,\n      ))\n  bijectors[[i]] <- maf\n  if (use_batchnorm)\n    bijectors[[i + 1]] <- tfb_batch_normalization()\n  if (use_permute)\n    bijectors[[i + 2]] <- tfb_permute((dim(batch)[2] - 1):0)\n}\n\nif (use_permute) bijectors <- bijectors[-num_layers]\nflow <- bijectors %>%\n  # possibly clean out empty elements (if no batchnorm or no permute)\n  discard(is.null) %>%\n  rev() %>%\n  tfb_chain()\n\ntarget_dist <- tfd_transformed_distribution(distribution = base_dist,\n                                            bijector = flow)\n\noptimizer <- tf$train$AdamOptimizer(1e-3)\n\n# train MAF\nn_epochs <- 100\nfor (i in 1:n_epochs) {\n  agg_loglik <- 0\n  num_batches <- 0\n  iter <- make_iterator_one_shot(ds_train)\n  until_out_of_range({\n    batch <- iterator_get_next(iter)\n    loss <-\n      function()\n        - tf$reduce_mean(target_dist %>% tfd_log_prob(batch))\n    optimizer$minimize(loss)\n    \n    loglik <- tf$reduce_mean(target_dist %>% tfd_log_prob(batch))\n    agg_loglik <- agg_loglik + loglik\n    num_batches <- num_batches + 1\n    \n    loglik_test_current <- \n      target_dist %>% tfd_log_prob(ds_test) %>% tf$reduce_mean()\n\n    if (num_batches %% 20 == 1)\n      cat(\n        \"Epoch \",\n        i,\n        \": \",\n        \"Batch \",\n        num_batches,\n        \": \",\n        ((agg_loglik %>% as.numeric()) / num_batches) %>% round(1),\n        \" --- test: \",\n        loglik_test_current %>% as.numeric() %>% round(1),\n        \"\\n\"\n      )\n  })\n}\n\n\nDuring training, we also monitored loglikelihoods on three different classes, cat, bird and wow5. Here are the loglikelihoods from the first 10 epochs. “Batch” refers to the current training batch (first batch in the epoch), all other values refer to complete datasets (the complete test set and the three sets selected for comparison).\nepoch   |   batch  |   test   |   \"cat\"  |   \"bird\"  |   \"wow\"  |\n--------|----------|----------|----------|-----------|----------|\n1       |   1443.5 |   1455.2 |   1398.8 |    1434.2 |   1546.0 |\n2       |   1935.0 |   2027.0 |   1941.2 |    1952.3 |   2008.1 | \n3       |   2004.9 |   2073.1 |   2003.5 |    2000.2 |   2072.1 |\n4       |   2063.5 |   2131.7 |   2056.0 |    2061.0 |   2116.4 |        \n5       |   2120.5 |   2172.6 |   2096.2 |    2085.6 |   2150.1 |\n6       |   2151.3 |   2206.4 |   2127.5 |    2110.2 |   2180.6 | \n7       |   2174.4 |   2224.8 |   2142.9 |    2163.2 |   2195.8 |\n8       |   2203.2 |   2250.8 |   2172.0 |    2061.0 |   2221.8 |        \n9       |   2224.6 |   2270.2 |   2186.6 |    2193.7 |   2241.8 |\n10      |   2236.4 |   2274.3 |   2191.4 |    2199.7 |   2243.8 |        \nWhile this does not look too bad, a complete comparison against all twenty-nine non-target classes had “zero” outperformed by seven other classes, with the remaining twenty-two lower in loglikelihood. We don’t have a model for anomaly detection, as yet.\nOutlook\nAs already alluded to several times, for data with temporal and/or spatial orderings more evolved architectures may prove useful. The very successful PixelCNN family is based on masked convolutions, with more recent developments bringing further refinements (e.g. Gated PixelCNN (Oord et al. 2016), PixelCNN++ (Salimans et al. 2017). Attention, too, may be masked and thus rendered autoregressive, as employed in the hybrid PixelSNAIL (Chen et al. 2017) and the - not surprisingly given its name - transformer-based ImageTransformer (Parmar et al. 2018).\nTo conclude, - while this post was interested in the intersection of flows and autoregressivity - and last not least the use therein of TFP bijectors - an upcoming one might dive deeper into autoregressive models specifically… and who knows, perhaps come back to the audio data for a fourth time.\n\n\n\nChen, Xi, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. 2017. “PixelSNAIL: An Improved Autoregressive Generative Model.” CoRR abs/1712.09763. http://arxiv.org/abs/1712.09763.\n\n\nDinh, Laurent, Jascha Sohl-Dickstein, and Samy Bengio. 2016. “Density Estimation Using Real NVP.” CoRR abs/1605.08803. http://arxiv.org/abs/1605.08803.\n\n\nGermain, Mathieu, Karol Gregor, Iain Murray, and Hugo Larochelle. 2015. “MADE: Masked Autoencoder for Distribution Estimation.” CoRR abs/1502.03509. http://arxiv.org/abs/1502.03509.\n\n\nOord, Aaron van den, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. 2016. “Conditional Image Generation with PixelCNN Decoders.” CoRR abs/1606.05328. http://arxiv.org/abs/1606.05328.\n\n\nPapamakarios, George, Theo Pavlakou, and Iain Murray. 2017. “Masked Autoregressive Flow for Density Estimation.” arXiv e-Prints, May, arXiv:1705.07057. https://arxiv.org/abs/1705.07057.\n\n\nParmar, Niki, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, and Alexander Ku. 2018. “Image Transformer.” CoRR abs/1802.05751. http://arxiv.org/abs/1802.05751.\n\n\nSalimans, Tim, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. 2017. “PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications.” CoRR abs/1701.05517. http://arxiv.org/abs/1701.05517.\n\n\ntfb_masked_autoregressive_flow, in R↩︎\nFor a comparison of Masked Autoregressive Flow to its siblings Real NVP (also available as a TFP bijector) and Inverse Autogressive Flow (to be obtained as an inverse of MAF in TFP), see Eric Jang’s excellent tutorial.↩︎\nnot specified individually in the paper↩︎\nWe quickly experimented with a higher number of FFT coefficients, but the approach did not seem that promising.↩︎\nto avoid clutter we don’t show the respective code↩︎\n",
    "preview": "posts/2019-04-24-autoregressive-flows/images/made.png",
    "last_modified": "2024-11-21T15:48:53+00:00",
    "input_file": {},
    "preview_width": 686,
    "preview_height": 398
  },
  {
    "path": "posts/2019-04-16-autokeras/",
    "title": "Auto-Keras: Tuning-free deep learning from R",
    "description": "Sometimes in deep learning, architecture design and hyperparameter tuning pose substantial challenges. Using Auto-Keras, none of these is needed: We start a search procedure and extract the best-performing model. This post presents Auto-Keras in action on the well-known MNIST dataset.",
    "author": [
      {
        "name": "Juan Cruz Rodriguez",
        "url": "https://jcrodriguez.rbind.io"
      }
    ],
    "date": "2019-04-16",
    "categories": [
      "TensorFlow/Keras",
      "Packages/Releases"
    ],
    "contents": "\n\n\nknitr::opts_chunk$set(echo = TRUE, eval = FALSE)\n\n\nToday, we’re happy to feature a guest post written by Juan Cruz, showing how to use Auto-Keras from R. Juan holds a master’s degree in Computer Science. Currently, he is finishing his master’s degree in Applied Statistics, as well as a Ph.D. in Computer Science, at the Universidad Nacional de Córdoba. He started his R journey almost six years ago, applying statistical methods to biology data. He enjoys software projects focused on making machine learning and data science available to everyone.\nIn the past few years, artificial intelligence has been a subject of intense media hype. Machine learning, deep learning, and artificial intelligence come up in countless articles, often outside of technology-minded publications. For most any topic, a brief search on the web yields dozens of texts suggesting the application of one or the other deep learning model.\nHowever, tasks such as feature engineering, hyperparameter tuning, or network design, are by no means easy for people without a rich computer science background. Lately, research started to emerge in the area of what is known as Neural Architecture Search (NAS) (Baker et al. 2016; Pham et al. 2018; Zoph and Le 2016; Luo et al. 2018; Liu et al. 2017; Real et al. 2018; Jin, Song, and Hu 2018). The main goal of NAS algorithms is, given a specific tagged dataset, to search for the most optimal neural network to perform a certain task on that dataset. In this sense, NAS algorithms allow the user to not have to worry about any task related to data science engineering. In other words, given a tagged dataset and a task, e.g., image classification, or text classification among others, the NAS algorithm will train several high-performance deep learning models and return the one that outperforms the rest.\nSeveral NAS algorithms were developed on different platforms (e.g. Google Cloud AutoML), or as libraries of certain programming languages (e.g. Auto-Keras, TPOT, Auto-Sklearn). However, for a language that brings together experts from such diverse disciplines as is the R programming language, to the best of our knowledge, there is no NAS tool to this day. In this post, we present the Auto-Keras R package, an interface from R to the Auto-Keras Python library (Jin, Song, and Hu 2018). Thanks to the use of Auto-Keras, R programmers with few lines of code will be able to train several deep learning models for their data and get the one that outperforms the others.\nLet’s dive into Auto-Keras!\nAuto-Keras\nNote: the Python Auto-Keras library is only compatible with Python 3.6. So make sure this version is currently installed, and correctly set to be used by the reticulate R library.\nInstallation\nTo begin, install the autokeras R package from GitHub as follows:\n\n\nif (!require(\"remotes\")) {\n  install.packages(\"remotes\")\n}\nremotes::install_github(\"jcrodriguez1989/autokeras\")\n\n\nThe Auto-Keras R interface uses the Keras and TensorFlow backend engines by default. To install both the core Auto-Keras library as well as the Keras and TensorFlow backends use the install_autokeras() function:\n\n\nlibrary(\"autokeras\")\ninstall_autokeras()\n\n\nThis will provide you with default CPU-based installations of Keras and TensorFlow. If you want a more customized installation, e.g. if you want to take advantage of NVIDIA GPUs, see the documentation for install_keras() from the keras R library.\nMNIST Example\nWe can learn the basics of Auto-Keras by walking through a simple example: recognizing handwritten digits from the MNIST dataset. MNIST consists of 28 x 28 grayscale images of handwritten digits like this:\n\nThe dataset also includes labels for each image, telling us which digit it is. For example, the label for the above image is 2.\nLoading the Data\nThe MNIST dataset is included with Keras and can be accessed using the dataset_mnist() function from the keras R library. Here we load the dataset, and then create variables for our test and training data:\n\n\nlibrary(\"keras\")\nmnist <- dataset_mnist() # load mnist dataset\nc(x_train, y_train) %<-% mnist$train # get train\nc(x_test, y_test) %<-% mnist$test # and test data\n\n\nThe x data is a 3-d array (images,width,height) of grayscale integer values ranging between 0 to 255.\n\n\nx_train[1, 14:20, 14:20] # show some pixels from the first image\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n[1,]  241  225  160  108    1    0    0\n[2,]   81  240  253  253  119   25    0\n[3,]    0   45  186  253  253  150   27\n[4,]    0    0   16   93  252  253  187\n[5,]    0    0    0    0  249  253  249\n[6,]    0   46  130  183  253  253  207\n[7,]  148  229  253  253  253  250  182\nThe y data is an integer vector with values ranging from 0 to 9.\n\n\nn_imgs <- 8\nhead(y_train, n = n_imgs) # show first 8 labels\n\n\n[1] 5 0 4 1 9 2 1 3\nEach of these images can be plotted in R:\n\n\nlibrary(\"ggplot2\")\nlibrary(\"tidyr\")\n# get each of the first n_imgs from the x_train dataset and\n# convert them to wide format\nmnist_to_plot <-\n  do.call(rbind, lapply(seq_len(n_imgs), function(i) {\n    samp_img <- x_train[i, , ] %>%\n      as.data.frame()\n    colnames(samp_img) <- seq_len(ncol(samp_img))\n    data.frame(\n      img = i,\n      gather(samp_img, \"x\", \"value\", convert = TRUE),\n      y = seq_len(nrow(samp_img))\n    )\n  }))\nggplot(mnist_to_plot, aes(x = x, y = y, fill = value)) + geom_tile() +\n  scale_fill_gradient(low = \"black\", high = \"white\", na.value = NA) +\n  scale_y_reverse() + theme_minimal() + theme(panel.grid = element_blank()) +\n  theme(aspect.ratio = 1) + xlab(\"\") + ylab(\"\") + facet_wrap(~img, nrow = 2)\n\n\n\nData ready, let’s get the model!\nData pre-processing? Model definition? Metrics, epochs definition, anyone? No, none of them are required by Auto-Keras. For image classification tasks, it is enough for Auto-Keras to be passed the x_train and y_train objects as defined above.\nSo, to train several deep learning models for two hours, it is enough to run:\n\n\n# train an Image Classifier for two hours\nclf <- model_image_classifier(verbose = TRUE) %>%\n  fit(x_train, y_train, time_limit = 2 * 60 * 60)\n\n\nSaving Directory: /tmp/autokeras_ZOG76O\nPreprocessing the images.\nPreprocessing finished.\n\nInitializing search.\nInitialization finished.\n\n\n+----------------------------------------------+\n|               Training model 0               |\n+----------------------------------------------+\n\nNo loss decrease after 5 epochs.\n\n\nSaving model.\n+--------------------------------------------------------------------------+\n|        Model ID        |          Loss          |      Metric Value      |\n+--------------------------------------------------------------------------+\n|           0            |  0.19463148526847363   |   0.9843999999999999   |\n+--------------------------------------------------------------------------+\n\n\n+----------------------------------------------+\n|               Training model 1               |\n+----------------------------------------------+\n\nNo loss decrease after 5 epochs.\n\n\nSaving model.\n+--------------------------------------------------------------------------+\n|        Model ID        |          Loss          |      Metric Value      |\n+--------------------------------------------------------------------------+\n|           1            |   0.210642946138978    |         0.984          |\n+--------------------------------------------------------------------------+\nEvaluate it:\n\n\nclf %>% evaluate(x_test, y_test)\n\n\n[1] 0.9866\nAnd then just get the best-trained model with:\n\n\nclf %>% final_fit(x_train, y_train, x_test, y_test, retrain = TRUE)\n\n\nNo loss decrease after 30 epochs.\nEvaluate the final model:\n\n\nclf %>% evaluate(x_test, y_test)\n\n\n[1] 0.9918\nAnd the model can be saved to take it into production with:\n\n\nclf %>% export_autokeras_model(\"./myMnistModel.pkl\")\n\n\nConclusions\nIn this post, the Auto-Keras R package was presented. It was shown that, with almost no deep learning knowledge, it is possible to train models and get the one that returns the best results for the desired task. Here we trained models for two hours. However, we have also tried training for 24 hours, resulting in 15 models being trained, to a final accuracy of 0.9928. Although Auto-Keras will not return a model as efficient as one generated manually by an expert, this new library has its place as an excellent starting point in the world of deep learning. Auto-Keras is an open-source R package, and is freely available in https://github.com/jcrodriguez1989/autokeras/.\nAlthough the Python Auto-Keras library is currently in a pre-release version and comes with not too many types of training tasks, this is likely to change soon, as the project it was recently added to the keras-team set of repositories. This will undoubtedly further its progress a lot.\nSo stay tuned, and thanks for reading!\nReproducibility\nTo correctly reproduce the results of this post, we recommend using the Auto-Keras docker image by typing:\n\ndocker pull jcrodriguez1989/r-autokeras:0.1.0\ndocker run -it jcrodriguez1989/r-autokeras:0.1.0 /bin/bash\n\n\n\n\nBaker, Bowen, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. 2016. “Designing Neural Network Architectures Using Reinforcement Learning.” arXiv Preprint arXiv:1611.02167.\n\n\nJin, Haifeng, Qingquan Song, and Xia Hu. 2018. “Auto-Keras: An Efficient Neural Architecture Search System.” arXiv Preprint arXiv:1806.10282.\n\n\nLiu, Hanxiao, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu. 2017. “Hierarchical Representations for Efficient Architecture Search.” arXiv Preprint arXiv:1711.00436.\n\n\nLuo, Renqian, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. 2018. “Neural Architecture Optimization.” In Advances in Neural Information Processing Systems, 7816–27.\n\n\nPham, Hieu, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeff Dean. 2018. “Efficient Neural Architecture Search via Parameter Sharing.” arXiv Preprint arXiv:1802.03268.\n\n\nReal, Esteban, Alok Aggarwal, Yanping Huang, and Quoc V Le. 2018. “Regularized Evolution for Image Classifier Architecture Search.” arXiv Preprint arXiv:1802.01548.\n\n\nZoph, Barret, and Quoc V Le. 2016. “Neural Architecture Search with Reinforcement Learning.” arXiv Preprint arXiv:1611.01578.\n\n\n\n\n",
    "preview": "posts/2019-04-16-autokeras/images/thumbnail.jpg",
    "last_modified": "2024-11-21T15:53:25+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-04-05-bijectors-flows/",
    "title": "Getting into the flow: Bijectors in TensorFlow Probability",
    "description": "Normalizing flows are one of the lesser known, yet fascinating and successful architectures in unsupervised deep learning. In this post we provide a basic introduction to flows using tfprobability, an R wrapper to TensorFlow Probability. Upcoming posts will build on this, using more complex flows on more complex data.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-04-05",
    "categories": [
      "Probabilistic ML/DL",
      "TensorFlow/Keras",
      "Concepts",
      "Generative Models"
    ],
    "contents": "\nAs of today, deep learning’s greatest successes have taken place in the realm of supervised learning, requiring lots and lots of annotated training data. However, data does not (normally) come with annotations or labels. Also, unsupervised learning is attractive because of the analogy to human cognition.\nOn this blog so far, we have seen two major architectures for unsupervised learning: variational autoencoders and generative adversarial networks. Lesser known, but appealing for conceptual as well as for performance reasons are normalizing flows (Jimenez Rezende and Mohamed 2015). In this and the next post, we’ll introduce flows, focusing on how to implement them using TensorFlow Probability (TFP).\nIn contrast to previous posts involving TFP that accessed its functionality using low-level $-syntax, we now make use of tfprobability, an R wrapper in the style of keras, tensorflow and tfdatasets. A note regarding this package: It is still under heavy development and the API may change. As of this writing, wrappers do not yet exist for all TFP modules, but all TFP functionality is available using $-syntax if need be.\nDensity estimation and sampling\nBack to unsupervised learning, and specifically thinking of variational autoencoders, what are the main things they give us? One thing that’s seldom missing from papers on generative methods are pictures of super-real-looking faces (or bed rooms, or animals …). So evidently sampling (or: generation) is an important part. If we can sample from a model and obtain real-seeming entities, this means the model has learned something about how things are distributed in the world: it has learned a distribution.\nIn the case of variational autoencoders, there is more: The entities are supposed to be determined by a set of distinct, disentangled (hopefully!) latent factors. But this is not the assumption in the case of normalizing flows, so we are not going to elaborate on this here.\nAs a recap, how do we sample from a VAE? We draw from \\(z\\), the latent variable, and run the decoder network on it. The result should - we hope - look like it comes from the empirical data distribution. It should not, however, look exactly like any of the items used to train the VAE, or else we have not learned anything useful.\nThe second thing we may get from a VAE is an assessment of the plausibility of individual data, to be used, for example, in anomaly detection. Here “plausibility” is vague on purpose: With VAE, we don’t have a means to compute an actual density under the posterior.\nWhat if we want, or need, both: generation of samples as well as density estimation? This is where normalizing flows come in.\nNormalizing flows\nA flow is a sequence of differentiable, invertible mappings from data to a “nice” distribution, something we can easily sample from and use to calculate a density. Let’s take as example the canonical way to generate samples from some distribution, the exponential, say.\nWe start by asking our random number generator for some number between 0 and 1:1\n\n\nu <- runif(1)\n\n\nThis number we treat as coming from a cumulative probability distribution (CDF) - from an exponential CDF, to be precise. Now that we have a value from the CDF, all we need to do is map that “back” to a value. That mapping CDF -> value we’re looking for is just the inverse of the CDF of an exponential distribution, the CDF being\n\\[F(x) = 1 - e^{-\\lambda x}\\]\nThe inverse then is\n\\[\nF^{-1}(u) = -\\frac{1}{\\lambda} ln (1 - u)\n\\]\nwhich means we may get our exponential sample doing\n\n\nlambda <- 0.5 # pick some lambda\nx <- -1/lambda * log(1-u)\n\n\nWe see the CDF is actually a flow (or a building block thereof, if we picture most flows as comprising several transformations), since\nIt maps data to a uniform distribution between 0 and 1, allowing to assess data likelihood.\nConversely, it maps a probability to an actual value, thus allowing to generate samples.\nFrom this example, we see why a flow should be invertible, but we don’t yet see why it should be differentiable. This will become clear shortly, but first let’s take a look at how flows are available in tfprobability.\nBijectors\nTFP comes with a treasure trove of transformations, called bijectors, ranging from simple computations like exponentiation to more complex ones like the discrete cosine transform.\nTo get started, let’s use tfprobability to generate samples from the normal distribution.\nThere is a bijector tfb_normal_cdf() that takes input data to the interval \\([0,1]\\). Its inverse transform then yields a random variable with the standard normal distribution:\n\n\nlibrary(tfprobability)\nlibrary(tensorflow)\ntfe_enable_eager_execution()\n\nlibrary(ggplot2)\n\nb <- tfb_normal_cdf()\nu <- runif(1000)\nx <- b %>% tfb_inverse(u) %>% as.numeric()\n\nx %>% data.frame(x = .) %>% ggplot(aes(x = x)) + geom_density()\n\n\n\nConversely, we can use this bijector to determine the (log) probability of a sample from the normal distribution. We’ll check against a straightforward use of tfd_normal in the distributions module:\n\n\nx <- 2.01\nd_n <- tfd_normal(loc = 0, scale = 1) \n\nd_n %>% tfd_log_prob(x) %>% as.numeric() # -2.938989\n\n\nTo obtain that same log probability from the bijector, we add two components:\nFirstly, we run the sample through the forward transformation and compute log probability under the uniform distribution.\nSecondly, as we’re using the uniform distribution to determine probability of a normal sample, we need to track how probability changes under this transformation. This is done by calling tfb_forward_log_det_jacobian (to be further elaborated on below).\n\n\nb <- tfb_normal_cdf()\nd_u <- tfd_uniform()\n\nl <- d_u %>% tfd_log_prob(b %>% tfb_forward(x))\nj <- b %>% tfb_forward_log_det_jacobian(x, event_ndims = 0)\n\n(l + j) %>% as.numeric() # -2.938989\n\n\nWhy does this work? Let’s get some background.\nProbability mass is conserved\nFlows are based on the principle that under transformation, probability mass is conserved. Say we have a flow from \\(x\\) to \\(z\\):\n\\[z = f(x)\\]\nSuppose we sample from \\(z\\) and then, compute the inverse transform to obtain \\(x\\). We know the probability of \\(z\\). What is the probability that \\(x\\), the transformed sample, lies between \\(x_0\\) and \\(x_0 + dx\\)?\nThis probability is \\(p(x) \\ dx\\), the density times the length of the interval. This has to equal the probability that \\(z\\) lies between \\(f(x)\\) and \\(f(x + dx)\\). That new interval has length \\(f'(x) dx\\), so:\n\\[p(x) dx = p(z) f'(x) dx\\]\nOr equivalently\n\\[p(x) = p(z) * dz/dx\\]\nThus, the sample probability \\(p(x)\\) is determined by the base probability \\(p(z)\\) of the transformed distribution, multiplied by how much the flow stretches space.\nThe same goes in higher dimensions: Again, the flow is about the change in probability volume between the \\(z\\) and \\(y\\) spaces:\n\\[p(x) =  p(z) \\frac{vol(dz)}{vol(dx)}\\]\nIn higher dimensions, the Jacobian replaces the derivative. Then, the change in volume is captured by the absolute value of its determinant:\n\\[p(\\mathbf{x}) = p(f(\\mathbf{x})) \\ \\bigg|det\\frac{\\partial f({\\mathbf{x})}}{\\partial{\\mathbf{x}}}\\bigg|\\]\nIn practice, we work with log probabilities, so\n\\[log \\ p(\\mathbf{x}) = log \\ p(f(\\mathbf{x})) + log \\ \\bigg|det\\frac{\\partial f({\\mathbf{x})}}{\\partial{\\mathbf{x}}}\\bigg| \\]\nLet’s see this with another bijector example, tfb_affine_scalar. Below, we construct a mini-flow that maps a few arbitrary chosen \\(x\\) values to double their value (scale = 2):\n\n\nx <- c(0, 0.5, 1)\nb <- tfb_affine_scalar(shift = 0, scale = 2)\n\n\nTo compare densities under the flow, we choose the normal distribution, and look at the log densities:\n\n\nd_n <- tfd_normal(loc = 0, scale = 1)\nd_n %>% tfd_log_prob(x) %>% as.numeric() # -0.9189385 -1.0439385 -1.4189385\n\n\nNow apply the flow and compute the new log densities as a sum of the log densities of the corresponding \\(x\\) values and the log determinant of the Jacobian:\n\n\nz <- b %>% tfb_forward(x)\n\n(d_n  %>% tfd_log_prob(b %>% tfb_inverse(z))) +\n  (b %>% tfb_inverse_log_det_jacobian(z, event_ndims = 0)) %>%\n  as.numeric() # -1.6120857 -1.7370857 -2.1120858\n\n\nWe see that as the values get stretched in space (we multiply by 2), the individual log densities go down.\nWe can verify the cumulative probability stays the same using tfd_transformed_distribution():\n\n\nd_t <- tfd_transformed_distribution(distribution = d_n, bijector = b)\nd_n %>% tfd_cdf(x) %>% as.numeric()  # 0.5000000 0.6914625 0.8413447\n\nd_t %>% tfd_cdf(y) %>% as.numeric()  # 0.5000000 0.6914625 0.8413447\n\n\nSo far, the flows we saw were static - how does this fit into the framework of neural networks?\nTraining a flow\nGiven that flows are bidirectional, there are two ways to think about them. Above, we have mostly stressed the inverse mapping: We want a simple distribution we can sample from, and which we can use to compute a density. In that line, flows are sometimes called “mappings from data to noise” - noise mostly being an isotropic Gaussian. However in practice, we don’t have that “noise” yet, we just have data.\nSo in practice, we have to learn a flow that does such a mapping. We do this by using bijectors with trainable parameters.\nWe’ll see a very simple example here, and leave “real world flows” to the next post.\nThe example is based on part 1 of Eric Jang’s introduction to normalizing flows. The main difference (apart from simplification to show the basic pattern) is that we’re using eager execution.\nWe start from a two-dimensional, isotropic Gaussian, and we want to model data that’s also normal, but with a mean of 1 and a variance of 2 (in both dimensions).\n\n\nlibrary(tensorflow)\nlibrary(tfprobability)\n\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nlibrary(tfdatasets)\n\n# where we start from\nbase_dist <- tfd_multivariate_normal_diag(loc = c(0, 0))\n\n# where we want to go\ntarget_dist <- tfd_multivariate_normal_diag(loc = c(1, 1), scale_identity_multiplier = 2)\n\n# create training data from the target distribution\ntarget_samples <- target_dist %>% tfd_sample(1000) %>% tf$cast(tf$float32)\n\nbatch_size <- 100\ndataset <- tensor_slices_dataset(target_samples) %>%\n  dataset_shuffle(buffer_size = dim(target_samples)[1]) %>%\n  dataset_batch(batch_size)\n\n\nNow we’ll build a tiny neural network, consisting of an affine transformation and a nonlinearity.\nFor the former, we can make use of tfb_affine, the multi-dimensional relative of tfb_affine_scalar.\nAs to nonlinearities, currently TFP comes with tfb_sigmoid and tfb_tanh, but we can build our own parameterized ReLU using tfb_inline:\n\n\n# alpha is a learnable parameter\nbijector_leaky_relu <- function(alpha) {\n  \n  tfb_inline(\n    # forward transform leaves positive values untouched and scales negative ones by alpha\n    forward_fn = function(x)\n      tf$where(tf$greater_equal(x, 0), x, alpha * x),\n    # inverse transform leaves positive values untouched and scales negative ones by 1/alpha\n    inverse_fn = function(y)\n      tf$where(tf$greater_equal(y, 0), y, 1/alpha * y),\n    # volume change is 0 when positive and 1/alpha when negative\n    inverse_log_det_jacobian_fn = function(y) {\n      I <- tf$ones_like(y)\n      J_inv <- tf$where(tf$greater_equal(y, 0), I, 1/alpha * I)\n      log_abs_det_J_inv <- tf$log(tf$abs(J_inv))\n      tf$reduce_sum(log_abs_det_J_inv, axis = 1L)\n    },\n    forward_min_event_ndims = 1\n  )\n}\n\n\nDefine the learnable variables for the affine and the PReLU layers:\n\n\nd <- 2 # dimensionality\nr <- 2 # rank of update\n\n# shift of affine bijector\nshift <- tf$get_variable(\"shift\", d)\n# scale of affine bijector\nL <- tf$get_variable('L', c(d * (d + 1) / 2))\n# rank-r update\nV <- tf$get_variable(\"V\", c(d, r))\n\n# scaling factor of parameterized relu\nalpha <- tf$abs(tf$get_variable('alpha', list())) + 0.01\n\n\nWith eager execution, the variables have to be used inside the loss function, so that is where we define the bijectors. Our little flow now is a tfb_chain of bijectors, and we wrap it in a TransformedDistribution (tfd_transformed_distribution) that links source and target distributions.\n\n\nloss <- function() {\n  \n affine <- tfb_affine(\n        scale_tril = tfb_fill_triangular() %>% tfb_forward(L),\n        scale_perturb_factor = V,\n        shift = shift\n      )\n lrelu <- bijector_leaky_relu(alpha = alpha)  \n \n flow <- list(lrelu, affine) %>% tfb_chain()\n \n dist <- tfd_transformed_distribution(distribution = base_dist,\n                          bijector = flow)\n  \n l <- -tf$reduce_mean(dist$log_prob(batch))\n # keep track of progress\n print(round(as.numeric(l), 2))\n l\n}\n\n\nNow we can actually run the training!\n\n\noptimizer <- tf$train$AdamOptimizer(1e-4)\n\nn_epochs <- 100\nfor (i in 1:n_epochs) {\n  iter <- make_iterator_one_shot(dataset)\n  until_out_of_range({\n    batch <- iterator_get_next(iter)\n    optimizer$minimize(loss)\n  })\n}\n\n\nOutcomes will differ depending on random initialization, but you should see a steady (if slow) progress. Using bijectors, we have actually trained and defined a little neural network.\nOutlook\nUndoubtedly, this flow is too simple to model complex data, but it’s instructive to have seen the basic principles before delving into more complex flows. In the next post, we’ll check out autoregressive flows, again using TFP and tfprobability.\n\n\n\nJimenez Rezende, Danilo, and Shakir Mohamed. 2015. “Variational Inference with Normalizing Flows.” arXiv e-Prints, May, arXiv:1505.05770. https://arxiv.org/abs/1505.05770.\n\n\nYes, using runif(). Just imagine there were no corresponding rexp() in R…↩︎\n",
    "preview": "posts/2019-04-05-bijectors-flows/images/flows.png",
    "last_modified": "2024-11-21T15:51:43+00:00",
    "input_file": {},
    "preview_width": 904,
    "preview_height": 325
  },
  {
    "path": "posts/2019-03-15-concepts-way-to-dl/",
    "title": "Math, code, concepts: A third road to deep learning",
    "description": "Not everybody who wants to get into deep learning has a strong background in math or programming. This post elaborates on a concepts-driven, abstraction-based way to learn what it's all about.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-03-15",
    "categories": [
      "Meta",
      "Concepts"
    ],
    "contents": "\nIn the previous version of their awesome deep learning MOOC, I remember fast.ai’s Jeremy Howard saying something like this:\n\nYou are either a math person or a code person, and […] 1\n\nI may be wrong about the either, and this is not about either versus, say, both. What if in reality, you’re none of the above?\nWhat if you come from a background that is close to neither math and statistics, nor computer science: the humanities, say? You may not have that intuitive, fast, effortless-looking understanding of LaTeX formulae that comes with natural talent and/or years of training, or both - the same goes for computer code.\nUnderstanding always has to start somewhere, so it will have to start with math or code (or both). Also, it’s always iterative, and iterations will often alternate between math and code. But what are things you can do when primarily, you’d say you are a concepts person?\nWhen meaning doesn’t automatically emerge from formulae, it helps to look for materials (blog posts, articles, books) that stress the concepts those formulae are all about. By concepts, I mean abstractions, concise, verbal characterizations of what a formula signifies.2\nLet’s try to make conceptual a bit more concrete. At least three aspects come to mind: useful abstractions, chunking (composing symbols into meaningful blocks), and action (what does that entity actually do?)\nAbstraction\nTo many people, in school, math meant nothing. Calculus was about manufacturing cans: How can we get as much soup as possible into the can while economizing on tin. How about this instead: Calculus3 is about how one thing changes as another changes? Suddenly, you start thinking: What, in my world, can I apply this to?\nA neural network is trained using backprop - just the chain rule of calculus, many texts say. How about life. How would my present be different had I spent more time exercising the ukulele? Then, how much more time would I have spent exercising the ukulele if my mother hadn’t discouraged me so much? And then - how much less discouraging would she have been had she not been forced to give up her own career as a circus artist? And so on.\nAs a more concrete example, take optimizers. With gradient descent as a baseline, what, in a nutshell, is different about momentum, RMSProp, Adam?\nStarting with momentum, this is the formula in one of the go-to posts, Sebastian Ruder’s http://ruder.io/optimizing-gradient-descent/ 4\n\\[v_t = \\gamma v_{t-1} + \\eta \\nabla_{\\theta} J(\\theta) \\\\\n\\theta = \\theta - v_t\\]\nThe formula tells us that the change to the weights5 is made up of two parts: the gradient of the loss6 with respect to the weights, computed at some point in time \\(t\\) (and scaled by the learning rate7), and the previous change computed at time \\(t-1\\) and discounted by some factor \\(\\gamma\\). What does this actually tell us?\nIn his Coursera MOOC, Andrew Ng introduces momentum (and RMSProp, and Adam) after two videos that aren’t even about deep learning. He introduces exponential moving averages, which will be familiar to many R users: We calculate a running average where at each point in time, the running result is weighted by a certain factor (0.9, say), and the current observation by 1 minus that factor (0.1, in this example).\nNow look at how momentum is presented:8\n\\[v = \\beta v + (1-\\beta) dW \\\\ \nW = W - \\alpha v\\]\nWe immediately see how \\(v\\) is the exponential moving average of gradients, and it is this that gets subtracted from the weights (scaled by the learning rate).\nBuilding on that abstraction in the viewers’ minds, Ng goes on to present RMSProp. This time, a moving average is kept of the squared weights 9, and at each time, this average (or rather, its square root) is used to scale the current gradient.\n\\[s = \\beta s + (1-\\beta) dW^2 \\\\ \nW = W - \\alpha \\frac{dW}{\\sqrt s}\\]\nIf you know a bit about Adam, you can guess what comes next: Why not have moving averages in the numerator as well as the denominator?\n\\[v = \\beta_1 v + (1-\\beta_1) dW \\\\ \ns = \\beta_2 s + (1-\\beta_2) dW^2 \\\\\nW = W - \\alpha \\frac{v}{\\sqrt s + \\epsilon}\\]\nOf course, actual implementations may differ in details, and not always expose those features that clearly. But for understanding and memorization, abstractions like this one - exponential moving average - do a lot. Let’s now see about chunking.\nChunking\nLooking again at the above formula from Sebastian Ruder’s post,\n\\[v_t = \\gamma v_{t-1} + \\eta \\nabla_{\\theta} J(\\theta) \\\\\n\\theta = \\theta - v_t\\]\nhow easy is it to parse the first line? Of course that depends on experience, but let’s focus on the formula itself.10\nReading that first line, we mentally build something like an AST (abstract syntax tree). Exploiting programming language vocabulary even further, operator precedence is crucial: To understand the right half of the tree, we want to first parse \\(\\nabla_{\\theta} J(\\theta)\\), and then only take \\(\\eta\\) into consideration.\nMoving on to larger formulae, the problem of operator precedence becomes one of chunking: Take that bunch of symbols and see it as a whole. We could call this abstraction again, just like above. But here, the focus is not on naming things or verbalizing, but on seeing: Seeing at a glance that when you read\n\\[\\frac{e^{z_i}}{\\sum_j{e^{z_j}}}\\]\nit is “just a softmax”. Again, my inspiration for this comes from Jeremy Howard, who I remember demonstrating, in one of the fastai lectures, that this is how you read a paper.\nLet’s turn to a more complex example. Last year’s article on Attention-based Neural Machine Translation with Keras included a short exposition of attention, featuring four steps:\nScoring encoder hidden states as to inasmuch they are a fit to the current decoder hidden state.\nChoosing Luong-style attention now11, we have\n\\[score(\\mathbf{h}_t,\\bar{\\mathbf{h}_s}) = \\mathbf{h}_t^T \\mathbf{W}\\bar{\\mathbf{h}_s}\\]\nOn the right, we see three symbols, which may appear meaningless at first but if we mentally “fade out” the weight matrix in the middle, a dot product appears, indicating that essentially, this is calculating similarity.\nNow comes what’s called attention weights: At the current timestep, which encoder states matter most?\n\\[\\alpha_{ts} = \\frac{exp(score(\\mathbf{h}_t,\\bar{\\mathbf{h}_s}))}{\\sum_{s'=1}^{S}{score(\\mathbf{h}_t,\\bar{\\mathbf{h}_{s'}})}}\\]\nScrolling up a bit, we see that this, in fact, is “just a softmax” (even though the physical appearance is not the same). Here, it is used to normalize the scores, making them sum to 1.\nNext up is the context vector:\n\\[\\mathbf{c}_t= \\sum_s{\\alpha_{ts} \\bar{\\mathbf{h}_s}}\\]\nWithout much thinking - but remembering from right above that the \\(\\alpha\\)s represent attention weights - we see a weighted average.\nFinally, in step\nwe need to actually combine that context vector with the current hidden state (here, done by training a fully connected layer on their concatenation):\n\\[\\mathbf{a}_t = tanh(\\mathbf{W_c} [ \\mathbf{c}_t ; \\mathbf{h}_t])\\]\nThis last step may be a better example of abstraction than of chunking, but anyway those are closely related: We need to chunk adequately to name concepts, and intuition about concepts helps chunk correctly.\nClosely related to abstraction, too, is analyzing what entities do.\nAction\nAlthough not deep learning related (in a narrow sense), my favorite quote comes from one of Gilbert Strang’s lectures on linear algebra:12\n\nMatrices don’t just sit there, they do something.\n\nIf in school calculus was about saving production materials, matrices were about matrix multiplication - the rows-by-columns way. (Or perhaps they existed for us to be trained to compute determinants, seemingly useless numbers that turn out to have a meaning, as we are going to see in a future post.)\nConversely, based on the much more illuminating matrix multiplication as linear combination of columns (resp. rows) view, Gilbert Strang introduces types of matrices as agents, concisely named by initial.\nFor example, when multiplying another matrix \\(A\\) on the right, this permutation matrix \\(P\\)\n\\[\\mathbf{P} = \\left[\\begin{array}\n{rrr}\n0 & 0 & 1 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{array}\\right]\n\\]\nputs \\(A\\)’s third row first, its first row second, and its second row third:\n\\[\\mathbf{PA} = \\left[\\begin{array}\n{rrr}\n0 & 0 & 1 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{array}\\right]\n\\left[\\begin{array}\n{rrr}\n0 & 1 & 1 \\\\\n1 & 3 & 7 \\\\\n2 & 4 & 8\n\\end{array}\\right] =\n\\left[\\begin{array}\n{rrr}\n2 & 4 & 8 \\\\\n0 & 1 & 1 \\\\\n1 & 3 & 7\n\\end{array}\\right]\n\\]\nIn the same way, reflection, rotation, and projection matrices are presented via their actions. The same goes for one of the most interesting topics in linear algebra from the point of view of the data scientist: matrix factorizations. \\(LU\\), \\(QR\\), eigendecomposition, \\(SVD\\) are all characterized by what they do. 13\nWho are the agents in neural networks? Activation functions are agents; this is where we have to mention softmax for the third time: Its strategy was described in Winner takes all: A look at activations and cost functions.\nAlso, optimizers are agents, and this is where we finally include some code. The explicit training loop used in all of the eager execution blog posts so far\nwith(tf$GradientTape() %as% tape, {\n     \n  # run model on current batch\n  preds <- model(x)\n     \n  # compute the loss\n  loss <- mse_loss(y, preds, x)\n})\n    \n# get gradients of loss w.r.t. model weights\ngradients <- tape$gradient(loss, model$variables)\n    \n# update model weights\noptimizer$apply_gradients(\n  purrr::transpose(list(gradients, model$variables)),\n  global_step = tf$train$get_or_create_global_step()\n)\nhas the optimizer do a single thing: apply the gradients it gets passed from the gradient tape.14 Thinking back to the characterization of different optimizers we saw above, this piece of code adds vividness to the thought that optimizers differ in what they actually do once they got those gradients.\nConclusion\nWrapping up, the goal here was to elaborate a bit on a conceptual, abstraction-driven way to get more familiar with the math involved in deep learning (or machine learning, in general). Certainly, the three aspects highlighted interact, overlap, form a whole, and there are other aspects to it. Analogy may be one, but it was left out here because it seems even more subjective, and less general.\nComments describing user experiences are very welcome.\n\nIf I don’t remember correctly: please just allow me to use this as the perfect intro to this post.↩︎\nCertainly visualization may be very useful, too, depending on the topic/algorithm and on “how visual a person” you are. But there is no need to stress the importance of visualization - everybody agrees on it - so this post is dedicated to verbal/conceptual methods.↩︎\nDifferential calculus, to be precise.↩︎\nThis is of course an excellent article that does mention concepts. It is just not intended for beginners, in contrast to the approach highlighted below.↩︎\n\\(\\theta\\)↩︎\n\\(J\\)↩︎\n\\(\\eta\\)↩︎\nFollowing the notation from the video, marginally simplified. Here \\(\\beta\\) is the scale factor applied to the running average, \\(dW\\) is the gradient of the loss with respect to the weights, and \\(\\alpha\\) is the learning rate.↩︎\n\\(W^2\\)↩︎\n As a side note, picking up on Jeremy Howard again: The Greek letters of course don’t make things any easier, but even with a history of studying ancient Greek for five years those formulae aren’t necessarily parsed easily.↩︎\nThe original post showed Bahdanau-style attention.↩︎\nAt least that’s what I remember him saying, approximately. The exact wording does not matter here.↩︎\nAlthough, if this were the paragraph about abstractions, Gilbert Strang’s books would yield perfect examples as well.↩︎\nIn other use cases, the optimizer class may also be used to compute gradients. But that method (optimizer$compute_gradients) is defined in the optimizer superclass and not subclass specific.↩︎\n",
    "preview": "posts/2019-03-15-concepts-way-to-dl/images/prev.jpg",
    "last_modified": "2024-11-21T15:48:16+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-02-07-audio-background/",
    "title": "Audio classification with Keras: Looking closer at the non-deep learning parts",
    "description": "Sometimes, deep learning is seen - and welcomed - as a way to avoid laborious preprocessing of data. However, there are cases where preprocessing of sorts does not only help improve prediction, but constitutes a fascinating topic in itself. One such case is audio classification. In this post, we build on a previous post on this blog, this time focusing on explaining some of the non-deep learning background. We then link the concepts explained to updated for near-future releases TensorFlow code.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-02-07",
    "categories": [
      "TensorFlow/Keras",
      "Concepts",
      "Audio Processing"
    ],
    "contents": "\nAbout half a year ago, this blog featured a post, written by Daniel Falbel, on how to use Keras to classify pieces of spoken language. The article got a lot of attention and not surprisingly, questions arose how to apply that code to different datasets. We’ll take this as a motivation to explore in more depth the preprocessing done in that post: If we know why the input to the network looks the way it looks, we will be able to modify the model specification appropriately if need be.\nIn case you have a background in speech recognition, or even general signal processing, for you the introductory part of this post will probably not contain much news. However, you might still be interested in the code part, which shows how to do things like creating spectrograms with current versions of TensorFlow.1\nIf you don’t have that background, we’re inviting you on a (hopefully) fascinating journey, slightly touching on one of the greater mysteries of this universe.2\nWe’ll use the same dataset as Daniel did in his post, that is, version 1 of the Google speech commands dataset(Warden 2018)\nThe dataset consists of ~ 65,000 WAV files, of length one second or less. Each file is a recording of one of thirty words, uttered by different speakers.\nThe goal then is to train a network to discriminate between spoken words. How should the input to the network look? The WAV files contain amplitudes of sound waves over time. Here are a few examples, corresponding to the words bird, down, sheila, and visual:3\n\nTime domain and frequency domain\nA sound wave is a signal extending in time, analogously to how what enters our visual system extends in space.\nAt each point in time, the current signal is dependent on its past. The obvious architecture to use in modeling it thus seems to be a recurrent neural network.\nHowever, the information contained in the sound wave can be represented in an alternative way: namely, using the frequencies that make up the signal.\nHere we see a sound wave (top) and its frequency representation (bottom).\n\nIn the time representation (referred to as the time domain), the signal is composed of consecutive amplitudes over time. In the frequency domain, it is represented as magnitudes of different frequencies. It may appear as one of the greatest mysteries in this world that you can convert between those two without loss of information, that is: Both representations are essentially equivalent!\nConversion from the time domain to the frequency domain is done using the Fourier transform; to convert back, the Inverse Fourier Transform is used. There exist different types of Fourier transforms depending on whether time is viewed as continuous or discrete, and whether the signal itself is continuous or discrete. In the “real world,” where usually for us, real means virtual as we’re working with digitized signals, the time domain as well as the signal are represented as discrete and so, the Discrete Fourier Transform (DFT) is used. The DFT itself is computed using the FFT (Fast Fourier Transform) algorithm, resulting in significant speedup over a naive implementation.\nLooking back at the above example sound wave, it is a compound of four sine waves, of frequencies 8Hz, 16Hz, 32Hz, and 64Hz, whose amplitudes are added and displayed over time. The compound wave here is assumed to extend infinitely in time. Unlike speech, which changes over time, it can be characterized by a single enumeration of the magnitudes of the frequencies it is composed of. So here the spectrogram, the characterization of a signal by magnitudes of constituent frequencies varying over time, looks essentially one-dimensional.\nHowever, when we ask Praat to create a spectrogram of one of our example sounds (a seven), it could look like this:\n\nHere we see a two-dimensional image of frequency magnitudes over time (higher magnitudes indicated by darker coloring). This two-dimensional representation may be fed to a network, in place of the one-dimensional amplitudes. Accordingly, if we decide to do so we’ll use a convnet instead of an RNN.\nSpectrograms will look different depending on how we create them. We’ll take a look at the essential options in a minute. First though, let’s see what we can’t always do: ask for all frequencies that were contained in the analog signal.4\nSampling\nAbove, we said that both representations, time domain and frequency domain, were essentially equivalent. In our virtual real world, this is only true if the signal we’re working with has been digitized correctly, or as this is commonly phrased, if it has been “properly sampled.”\nTake speech as an example: As an analog signal, speech per se is continuous in time; for us to be able to work with it on a computer, it needs to be converted to happen in discrete time. This conversion of the independent variable (time in our case, space in e.g. image processing) from continuous to discrete is called sampling.\nIn this process of discretization, a crucial decision to be made is the sampling rate to use. The sampling rate has to be at least double the highest frequency in the signal. If it’s not, loss of information will occur. The way this is most often put is the other way round: To preserve all information, the analog signal may not contain frequencies above one-half the sampling rate. This frequency - half the sampling rate - is called the Nyquist rate.\nIf the sampling rate is too low, aliasing takes place: Higher frequencies alias themselves as lower frequencies. This means that not only can’t we get them, they also corrupt the magnitudes of corresponding lower frequencies they are being added to.\nHere’s a schematic example of how a high-frequency signal could alias itself as being lower-frequency. Imagine the high-frequency wave being sampled at integer points (grey circles) only:\n\nIn the case of the speech commands dataset, all sound waves have been sampled at 16 kHz. This means that when we ask Praat for a spectogram, we should not ask for frequencies higher than 8kHz. Here is what happens if we ask for frequencies up to 16kHz instead - we just don’t get them:\n\nNow let’s see what options we do have when creating spectrograms.\nSpectrogram creation options\nIn the above simple sine wave example, the signal stayed constant over time. However in speech utterances, the magnitudes of constituent frequencies change over time. Ideally thus, we’d have an exact frequency representation for every point in time. As an approximation to this ideal, the signal is divided into overlapping windows, and the Fourier transform is computed for each time slice separately. This is called the Short Time Fourier Transform (STFT).\nWhen we compute the spectrogram via the STFT, we need to tell it what size windows to use, and how big to make the overlap. The longer the windows we use, the better the resolution we get in the frequency domain. However, what we gain in resolution there, we lose in the time domain, as we’ll have fewer windows representing the signal. This is a general principle in signal processing: Resolution in the time and frequency domains are inversely related.\nTo make this more concrete, let’s again look at a simple example. Here is the spectrogram of a synthetic sine wave, composed of two components at 1000 Hz and 1200 Hz. The window length was left at its (Praat) default, 5 milliseconds:\n\nWe see that with a short window like that, the two different frequencies are mangled into one in the spectrogram.\nNow enlarge the window to 30 milliseconds, and they are clearly differentiated:\n\nThe above spectrogram of the word “seven” was produced using Praats default of 5 milliseconds. What happens if we use 30 milliseconds instead?\n\nWe get better frequency resolution, but at the price of lower resolution in the time domain. The window length used during preprocessing is a parameter we might want to experiment with later, when training a network.\nAnother input to the STFT to play with is the type of window used to weight the samples in a time slice. Here again are three spectrograms of the above recording of seven, using, respectively, a Hamming, a Hann, and a Gaussian window:\n\nWhile the spectrograms using the Hann and Gaussian windows don’t look much different, the Hamming window seems to have introduced some artifacts.\nBeyond the spectrogram: Mel scale and Mel-Frequency Cepstral Coefficients (MFCCs)\nPreprocessing options don’t end with the spectrogram. A popular transformation applied to the spectrogram is conversion to mel scale, a scale based on how humans actually perceive differences in pitch. We don’t elaborate further on this here,5 but we do briefly comment on the respective TensorFlow code below, in case you’d like to experiment with this.\nIn the past, coefficients transformed to Mel scale have sometimes been further processed to obtain the so-called Mel-Frequency Cepstral Coefficients (MFCCs). Again, we just show the code. For excellent reading on Mel scale conversion and MFCCs (including the reason why MFCCs are less often used nowadays) see this post by Haytham Fayek.\nBack to our original task of speech classification. Now that we’ve gained a bit of insight in what is involved, let’s see how to perform these transformations in TensorFlow.\nPreprocessing for audio classification using TensorFlow\nCode will be represented in snippets according to the functionality it provides, so we may directly map it to what was explained conceptually above.\nA complete example is available here. The complete example builds on Daniel’s original code as much as possible,6 with two exceptions:\nThe code runs in eager as well as in static graph mode. If you decide you only ever need eager mode, there are a few places that can be simplified. This is partly related to the fact that in eager mode, TensorFlow operations in place of tensors return values, which we can directly pass on to TensorFlow functions expecting values, not tensors. In addition, less conversion code is needed when manipulating intermediate values in R.\nWith TensorFlow 1.13 being released any day, and preparations for TF 2.0 running at full speed, we want the code to necessitate as few modifications as possible to run on the next major version of TF. One big difference is that there will no longer be a contrib module. In the original post, contrib was used to read in the .wav files as well as compute the spectrograms. Here, we will use functionality from tf.audio and tf.signal instead.\nAll operations shown below will run inside tf.dataset code, which on the R side is accomplished using the tfdatasets package.\nTo explain the individual operations, we look at a single file, but later we’ll also display the data generator as a whole.\nFor stepping through individual lines, it’s always helpful to have eager mode enabled, independently of whether ultimately we’ll execute in eager or graph mode:\n\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\n\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\n\nWe pick a random .wav file and decode it using tf$audio$decode_wav.7This will give us access to two tensors: the samples themselves, and the sampling rate.\n\n\nfname <- \"data/speech_commands_v0.01/bird/00b01445_nohash_0.wav\"\nwav <- tf$audio$decode_wav(tf$read_file(fname))\n\n\nwav$sample_rate contains the sampling rate. As expected, it is 16000, or 16kHz:\n\n\nsampling_rate <- wav$sample_rate %>% as.numeric()\nsampling_rate\n\n\n16000\nThe samples themselves are accessible as wav$audio, but their shape is (16000, 1), so we have to transpose the tensor to get the usual (batch_size, number of samples) format we need for further processing.\n\n\nsamples <- wav$audio\nsamples <- samples %>% tf$transpose(perm = c(1L, 0L))\nsamples\n\n\ntf.Tensor(\n[[-0.00750732  0.04653931  0.02041626 ... -0.01004028 -0.01300049\n  -0.00250244]], shape=(1, 16000), dtype=float32)\nComputing the spectogram\nTo compute the spectrogram, we use tf$signal$stft (where stft stands for Short Time Fourier Transform). stft expects three non-default arguments: Besides the input signal itself, there are the window size, frame_length, and the stride to use when determining the overlapping windows, frame_step. Both are expressed in units of number of samples. So if we decide on a window length of 30 milliseconds and a stride of 10 milliseconds …\n\n\nwindow_size_ms <- 30\nwindow_stride_ms <- 10\n\n\n… we arrive at the following call:\n\n\nsamples_per_window <- sampling_rate * window_size_ms/1000 \nstride_samples <-  sampling_rate * window_stride_ms/1000 \n\nstft_out <- tf$signal$stft(\n  samples,\n  frame_length = as.integer(samples_per_window),\n  frame_step = as.integer(stride_samples)\n)\n\n\nInspecting the tensor we got back, stft_out, we see, for our single input wave, a matrix of 98 x 257 complex values:\ntf.Tensor(\n[[[ 1.03279948e-04+0.00000000e+00j -1.95371482e-04-6.41121820e-04j\n   -1.60833192e-03+4.97534114e-04j ... -3.61620914e-05-1.07343149e-04j\n   -2.82576875e-05-5.88812982e-05j  2.66879797e-05+0.00000000e+00j] \n   ... \n   ]],\nshape=(1, 98, 257), dtype=complex64)\nHere 98 is the number of periods, which we can compute in advance, based on the number of samples in a window and the size of the stride:8\n\n\nn_periods <- length(seq(samples_per_window/2, sampling_rate - samples_per_window/2, stride_samples))\n\n\n257 is the number of frequencies we obtained magnitudes for. By default, stft will apply a Fast Fourier Transform of size smallest power of 2 greater or equal to the number of samples in a window,9 and then return the fft_length / 2 + 1 unique components of the FFT: the zero-frequency term and the positive-frequency terms.10\nIn our case, the number of samples in a window is 480. The nearest enclosing power of 2 being 512, we end up with 512/2 + 1 = 257 coefficients.\nThis too we can compute in advance:11\n\n\nfft_size <- as.integer(2^trunc(log(samples_per_window, 2)) + 1) \n\n\nBack to the output of the STFT. Taking the elementwise magnitude of the complex values, we obtain an energy spectrogram:\n\n\nmagnitude_spectrograms <- tf$abs(stft_out)\n\n\nIf we stop preprocessing here, we will usually want to log transform the values to better match the sensitivity of the human auditory system:\n\n\nlog_magnitude_spectrograms = tf$log(magnitude_spectrograms + 1e-6)\n\n\nMel spectrograms and Mel-Frequency Cepstral Coefficients (MFCCs)\nIf instead we choose to use Mel spectrograms, we can obtain a transformation matrix that will convert the original spectrograms to Mel scale:\n\n\nlower_edge_hertz <- 0\nupper_edge_hertz <- 2595 * log10(1 + (sampling_rate/2)/700)\nnum_mel_bins <- 64L\nnum_spectrogram_bins <- magnitude_spectrograms$shape[-1]$value\n\nlinear_to_mel_weight_matrix <- tf$signal$linear_to_mel_weight_matrix(\n  num_mel_bins,\n  num_spectrogram_bins,\n  sampling_rate,\n  lower_edge_hertz,\n  upper_edge_hertz\n)\n\n\nApplying that matrix, we obtain a tensor of size (batch_size, number of periods, number of Mel coefficients) which again, we can log-compress if we want:\n\n\nmel_spectrograms <- tf$tensordot(magnitude_spectrograms, linear_to_mel_weight_matrix, 1L)\nlog_mel_spectrograms <- tf$log(mel_spectrograms + 1e-6)\n\n\nJust for completeness’ sake, finally we show the TensorFlow code used to further compute MFCCs. We don’t include this in the complete example as with MFCCs, we would need a different network architecture.\n\n\nnum_mfccs <- 13\nmfccs <- tf$signal$mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[, , 1:num_mfccs]\n\n\nAccommodating different-length inputs\nIn our complete example, we determine the sampling rate from the first file read, thus assuming all recordings have been sampled at the same rate. We do allow for different lengths though. For example in our dataset, had we used this file, just 0.65 seconds long, for demonstration purposes:\n\n\nfname <- \"data/speech_commands_v0.01/bird/1746d7b6_nohash_0.wav\"\n\n\nwe’d have ended up with just 63 periods in the spectrogram. As we have to define a fixed input_size for the first conv layer, we need to pad the corresponding dimension to the maximum possible length, which is n_periods computed above.\nThe padding actually takes place as part of dataset definition. Let’s quickly see dataset definition as a whole, leaving out the possible generation of Mel spectrograms.12\n\n\ndata_generator <- function(df,\n                           window_size_ms,\n                           window_stride_ms) {\n  \n  # assume sampling rate is the same in all samples\n  sampling_rate <-\n    tf$audio$decode_wav(tf$read_file(tf$reshape(df$fname[[1]], list()))) %>% .$sample_rate\n  \n  samples_per_window <- (sampling_rate * window_size_ms) %/% 1000L  \n  stride_samples <-  (sampling_rate * window_stride_ms) %/% 1000L   \n  \n  n_periods <-\n    tf$shape(\n      tf$range(\n        samples_per_window %/% 2L,\n        16000L - samples_per_window %/% 2L,\n        stride_samples\n      )\n    )[1] + 1L\n  \n  n_fft_coefs <-\n    (2 ^ tf$ceil(tf$log(\n      tf$cast(samples_per_window, tf$float32)\n    ) / tf$log(2)) /\n      2 + 1L) %>% tf$cast(tf$int32)\n  \n  ds <- tensor_slices_dataset(df) %>%\n    dataset_shuffle(buffer_size = buffer_size)\n  \n  ds <- ds %>%\n    dataset_map(function(obs) {\n      wav <-\n        tf$audio$decode_wav(tf$read_file(tf$reshape(obs$fname, list())))\n      samples <- wav$audio\n      samples <- samples %>% tf$transpose(perm = c(1L, 0L))\n      \n      stft_out <- tf$signal$stft(samples,\n                                 frame_length = samples_per_window,\n                                 frame_step = stride_samples)\n      \n      magnitude_spectrograms <- tf$abs(stft_out)\n      log_magnitude_spectrograms <- tf$log(magnitude_spectrograms + 1e-6)\n      \n      response <- tf$one_hot(obs$class_id, 30L)\n\n      input <- tf$transpose(log_magnitude_spectrograms, perm = c(1L, 2L, 0L))\n      list(input, response)\n    })\n  \n  ds <- ds %>%\n    dataset_repeat()\n  \n  ds %>%\n    dataset_padded_batch(\n      batch_size = batch_size,\n      padded_shapes = list(tf$stack(list(\n        n_periods, n_fft_coefs,-1L\n      )),\n      tf$constant(-1L, shape = shape(1L))),\n      drop_remainder = TRUE\n    )\n}\n\n\nThe logic is the same as described above, only the code has been generalized to work in eager as well as graph mode. The padding is taken care of by dataset_padded_batch(), which needs to be told the maximum number of periods and the maximum number of coefficients.\nTime for experimentation\nBuilding on the complete example, now is the time for experimentation: How do different window sizes affect classification accuracy? Does transformation to the mel scale yield improved results?13 You might also want to try passing a non-default window_fn to stft (the default being the Hann window) and see how that affects the results. And of course, the straightforward definition of the network leaves a lot of room for improvement.\nWrapping up\nSpeaking of the network: Now that we’ve gained more insight into what is contained in a spectrogram, we might start asking, is a convnet really an adequate solution here? Normally we use convnets on images: two-dimensional data where both dimensions represent the same kind of information. Thus with images, it is natural to have square filter kernels.\nIn a spectrogram though, the time axis and the frequency axis represent fundamentally different types of information, and it is not clear at all that we should treat them equally. Also, whereas in images, the translation invariance of convnets is a desired feature, this is not the case for the frequency axis in a spectrogram.\nClosing the circle, we discover that due to deeper knowledge about the subject domain, we are in a better position to reason about (hopefully) successful network architectures. We leave it to the creativity of our readers to continue the search…\n\n\n\nWarden, P. 2018. “Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.” ArXiv e-Prints, April. https://arxiv.org/abs/1804.03209.\n\n\nAs well as TensorFlow 2.0, more or less.↩︎\nReferring to the Fourier Transform. To cite an authority for this characterization, it is e.g. found in Brad Osgood’s lecture on The Fourier Transform and its Applications.↩︎\nTo display these sound waves, and later on to create spectrograms, we use Praat, a speech analysis and synthesis program that has a lot more functionality than what we’re making use of here.↩︎\nIn practice, working with datasets created for speech analysis, there will be no problems due to low sampling rates (the topic we talk about below). However, the topic is too essential - and interesting! - to skip over in an introductory post like this one.)↩︎\nCf. discussions about the validity of the original experiments.↩︎\nIn particular, we’re leaving the convnet and training code itself nearly unchanged.↩︎\nAs of this writing, tf.audio is only available in the TensorFlow nightly builds. If the decode_wav line fails, simply replace tf$audio by tf$contrib$framework$python$ops$audio_ops.↩︎\nThis code, taken from the original post, is applicable when executing eagerly or when working “on the R side.” The complete example, which dynamically determines the sampling rate and performs all operations so they work inside a static TensorFlow graph, has a more intimidating-looking equivalent that essentially does the same thing.↩︎\nIn the former case, the samples dimension in the time domain will be padded with zeros.↩︎\nFor real signals, the negative-frequency terms are redundant.↩︎\nAgain, the code in the full example looks a bit more involved because it is supposed to be runnable on a static TensorFlow graph.↩︎\nWhich is contained in the complete example code, though.↩︎\nFor us, it didn’t.↩︎\n",
    "preview": "posts/2019-02-07-audio-background/images/seven2.png",
    "last_modified": "2024-11-21T15:48:24+00:00",
    "input_file": {},
    "preview_width": 1714,
    "preview_height": 846
  },
  {
    "path": "posts/2019-01-24-vq-vae/",
    "title": "Discrete Representation Learning with VQ-VAE and TensorFlow Probability",
    "description": "Mostly when thinking of Variational Autoencoders (VAEs), we picture the prior as an isotropic Gaussian. But this is by no means a necessity. The Vector Quantised Variational Autoencoder (VQ-VAE) described in van den Oord et al's \"Neural Discrete Representation Learning\" features a discrete latent space that allows to learn impressively concise latent representations. In this post, we combine elements of Keras, TensorFlow, and TensorFlow Probability to see if we can generate convincing letters resembling those in Kuzushiji-MNIST.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-01-24",
    "categories": [
      "TensorFlow/Keras",
      "Probabilistic ML/DL",
      "Generative Models"
    ],
    "contents": "\nAbout two weeks ago, we introduced TensorFlow Probability (TFP), showing how to create and sample from distributions and put them to use in a Variational Autoencoder (VAE) that learns its prior. Today, we move on to a different specimen in the VAE model zoo: the Vector Quantised Variational Autoencoder (VQ-VAE) described in Neural Discrete Representation Learning (Oord, Vinyals, and Kavukcuoglu 2017). This model differs from most VAEs in that its approximate posterior is not continuous, but discrete - hence the “quantised” in the article’s title. We’ll quickly look at what this means, and then dive directly into the code, combining Keras layers, eager execution, and TFP.\nDiscrete codes\nMany phenomena are best thought of, and modeled, as discrete. This holds for phonemes and lexemes in language, higher-level structures in images (think objects instead of pixels),and tasks that necessitate reasoning and planning.\nThe latent code used in most VAEs, however, is continuous - usually it’s a multivariate Gaussian. Continuous-space VAEs have been found very successful in reconstructing their input, but often they suffer from something called posterior collapse: The decoder is so powerful that it may create realistic output given just any input. This means there is no incentive to learn an expressive latent space.\nIn VQ-VAE, however, each input sample gets mapped deterministically to one of a set of embedding vectors.1 Together, these embedding vectors constitute the prior for the latent space.\nAs such, an embedding vector contains a lot more information than a mean and a variance, and thus, is much harder to ignore by the decoder.\nThe question then is: Where is that magical hat, for us to pull out meaningful embeddings?\nLearning a discrete embedding space\nFrom the above conceptual description, we now have two questions to answer. First, by what mechanism do we assign input samples (that went through the encoder) to appropriate embedding vectors?\nAnd second: How can we learn embedding vectors that actually are useful representations - that when fed to a decoder, will result in entities perceived as belonging to the same species?\nAs regards assignment, a tensor emitted from the encoder is simply mapped to its nearest neighbor in embedding space, using Euclidean distance. The embedding vectors are then updated using exponential moving averages.2 As we’ll see soon, this means that they are actually not being learned using gradient descent - a feature worth pointing out as we don’t come across it every day in deep learning.\nConcretely, how then should the loss function and training process look? This will probably easiest be seen in code.\nCoding the VQ-VAE\nThe complete code for this example, including utilities for model saving and image visualization, is available on github as part of the Keras examples. Order of presentation here may differ from actual execution order for expository purposes, so please to actually run the code consider making use of the example on github.\nSetup and data loading\nAs in all our prior posts on VAEs, we use eager execution, which presupposes the TensorFlow implementation of Keras.\n\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\ntfp <- import(\"tensorflow_probability\")\ntfd <- tfp$distributions\n\nlibrary(tfdatasets)\nlibrary(dplyr)\nlibrary(glue)\n# used for set_defaults; please get the development version:\n# devtools::install_github(\"thomasp85/curry\")\nlibrary(curry) \n\n\nAs in our previous post on doing VAE with TFP, we’ll use Kuzushiji-MNIST(Clanuwat et al. 2018) as input.\nNow is the time to look at what we ended up generating that time and place your bet: How will that compare against the discrete latent space of VQ-VAE?\n\n\nnp <- import(\"numpy\")\n \nkuzushiji <- np$load(\"kmnist-train-imgs.npz\")\nkuzushiji <- kuzushiji$get(\"arr_0\")\n\ntrain_images <- kuzushiji %>%\n  k_expand_dims() %>%\n  k_cast(dtype = \"float32\")\n\ntrain_images <- train_images %>% `/`(255)\n\nbuffer_size <- 60000\nbatch_size <- 64\nnum_examples_to_generate <- batch_size\n\nbatches_per_epoch <- buffer_size / batch_size\n\ntrain_dataset <- tensor_slices_dataset(train_images) %>%\n  dataset_shuffle(buffer_size) %>%\n  dataset_batch(batch_size, drop_remainder = TRUE)\n\n\nHyperparameters\nIn addition to the “usual” hyperparameters we have in deep learning, the VQ-VAE infrastructure introduces a few model-specific ones. First of all, the embedding space is of dimensionality number of embedding vectors times embedding vector size:\n\n\n# number of embedding vectors\nnum_codes <- 64L\n# dimensionality of the embedding vectors\ncode_size <- 16L\n\n\nThe latent space in our example will be of size one, that is, we have a single embedding vector representing the latent code for each input sample. This will be fine for our dataset, but it should be noted that van den Oord et al. used far higher-dimensional latent spaces on e.g. ImageNet and Cifar-10.3\n\n\nlatent_size <- 1\n\n\nEncoder model\nThe encoder uses convolutional layers to extract image features. Its output is a 3-d tensor of shape batchsize * 1 * code_size.\n\n\nactivation <- \"elu\"\n# modularizing the code just a little bit\ndefault_conv <- set_defaults(layer_conv_2d, list(padding = \"same\", activation = activation))\n\n\n\n\nbase_depth <- 32\n\nencoder_model <- function(name = NULL,\n                          code_size) {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$conv1 <- default_conv(filters = base_depth, kernel_size = 5)\n    self$conv2 <- default_conv(filters = base_depth, kernel_size = 5, strides = 2)\n    self$conv3 <- default_conv(filters = 2 * base_depth, kernel_size = 5)\n    self$conv4 <- default_conv(filters = 2 * base_depth, kernel_size = 5, strides = 2)\n    self$conv5 <- default_conv(filters = 4 * latent_size, kernel_size = 7, padding = \"valid\")\n    self$flatten <- layer_flatten()\n    self$dense <- layer_dense(units = latent_size * code_size)\n    self$reshape <- layer_reshape(target_shape = c(latent_size, code_size))\n    \n    function (x, mask = NULL) {\n      x %>% \n        # output shape:  7 28 28 32 \n        self$conv1() %>% \n        # output shape:  7 14 14 32 \n        self$conv2() %>% \n        # output shape:  7 14 14 64 \n        self$conv3() %>% \n        # output shape:  7 7 7 64 \n        self$conv4() %>% \n        # output shape:  7 1 1 4 \n        self$conv5() %>% \n        # output shape:  7 4 \n        self$flatten() %>% \n        # output shape:  7 16 \n        self$dense() %>% \n        # output shape:  7 1 16\n        self$reshape()\n    }\n  })\n}\n\n\nAs always, let’s make use of the fact that we’re using eager execution, and see a few example outputs.\n\n\niter <- make_iterator_one_shot(train_dataset)\nbatch <-  iterator_get_next(iter)\n\nencoder <- encoder_model(code_size = code_size)\nencoded  <- encoder(batch)\nencoded\n\n\ntf.Tensor(\n[[[ 0.00516277 -0.00746826  0.0268365  ... -0.012577   -0.07752544\n   -0.02947626]]\n...\n\n [[-0.04757921 -0.07282603 -0.06814402 ... -0.10861694 -0.01237121\n    0.11455103]]], shape=(64, 1, 16), dtype=float32)\nNow, each of these 16d vectors needs to be mapped to the embedding vector it is closest to. This mapping is taken care of by another model: vector_quantizer.\nVector quantizer model\nThis is how we will instantiate the vector quantizer:\n\n\nvector_quantizer <- vector_quantizer_model(num_codes = num_codes, code_size = code_size)\n\n\nThis model serves two purposes: First, it acts as a store for the embedding vectors. Second, it matches encoder output to available embeddings.\nHere, the current state of embeddings is stored in codebook. ema_means and ema_count are for bookkeeping purposes only (note how they are set to be non-trainable). We’ll see them in use shortly.\n\n\nvector_quantizer_model <- function(name = NULL, num_codes, code_size) {\n  \n    keras_model_custom(name = name, function(self) {\n      \n      self$num_codes <- num_codes\n      self$code_size <- code_size\n      self$codebook <- tf$get_variable(\n        \"codebook\",\n        shape = c(num_codes, code_size), \n        dtype = tf$float32\n        )\n      self$ema_count <- tf$get_variable(\n        name = \"ema_count\", shape = c(num_codes),\n        initializer = tf$constant_initializer(0),\n        trainable = FALSE\n        )\n      self$ema_means = tf$get_variable(\n        name = \"ema_means\",\n        initializer = self$codebook$initialized_value(),\n        trainable = FALSE\n        )\n      \n      function (x, mask = NULL) { \n        \n        # to be filled in shortly ...\n        \n      }\n    })\n}\n\n\nIn addition to the actual embeddings, in its call method vector_quantizer holds the assignment logic.\nFirst, we compute the Euclidean distance of each encoding to the vectors in the codebook (tf$norm).\nWe assign each encoding to the closest as by that distance embedding (tf$argmin) and one-hot-encode the assignments (tf$one_hot). Finally, we isolate the corresponding vector by masking out all others and summing up what’s left over (multiplication followed by tf$reduce_sum).\nRegarding the axis argument used with many TensorFlow functions, please take into consideration that in contrast to their k_* siblings, raw TensorFlow (tf$*) functions expect axis numbering to be 0-based. We also have to add the L’s after the numbers to conform to TensorFlow’s datatype requirements.\n\n\nvector_quantizer_model <- function(name = NULL, num_codes, code_size) {\n  \n    keras_model_custom(name = name, function(self) {\n      \n      # here we have the above instance fields\n      \n      function (x, mask = NULL) {\n    \n        # shape: bs * 1 * num_codes\n         distances <- tf$norm(\n          tf$expand_dims(x, axis = 2L) -\n            tf$reshape(self$codebook, \n                       c(1L, 1L, self$num_codes, self$code_size)),\n                       axis = 3L \n        )\n        \n        # bs * 1\n        assignments <- tf$argmin(distances, axis = 2L)\n        \n        # bs * 1 * num_codes\n        one_hot_assignments <- tf$one_hot(assignments, depth = self$num_codes)\n        \n        # bs * 1 * code_size\n        nearest_codebook_entries <- tf$reduce_sum(\n          tf$expand_dims(\n            one_hot_assignments, -1L) * \n            tf$reshape(self$codebook, c(1L, 1L, self$num_codes, self$code_size)),\n                       axis = 2L \n                       )\n        list(nearest_codebook_entries, one_hot_assignments)\n      }\n    })\n  }\n\n\nNow that we’ve seen how the codes are stored, let’s add functionality for updating them.\nAs we said above, they are not learned via gradient descent. Instead, they are exponential moving averages, continually updated by whatever new “class member” they get assigned.\nSo here is a function update_ema that will take care of this.\nupdate_ema uses TensorFlow moving_averages to\nfirst, keep track of the number of currently assigned samples per code (updated_ema_count), and\nsecond, compute and assign the current exponential moving average (updated_ema_means).\n\n\nmoving_averages <- tf$python$training$moving_averages\n\n# decay to use in computing exponential moving average\ndecay <- 0.99\n\nupdate_ema <- function(\n  vector_quantizer,\n  one_hot_assignments,\n  codes,\n  decay) {\n \n  updated_ema_count <- moving_averages$assign_moving_average(\n    vector_quantizer$ema_count,\n    tf$reduce_sum(one_hot_assignments, axis = c(0L, 1L)),\n    decay,\n    zero_debias = FALSE\n  )\n\n  updated_ema_means <- moving_averages$assign_moving_average(\n    vector_quantizer$ema_means,\n    # selects all assigned values (masking out the others) and sums them up over the batch\n    # (will be divided by count later, so we get an average)\n    tf$reduce_sum(\n      tf$expand_dims(codes, 2L) *\n        tf$expand_dims(one_hot_assignments, 3L), axis = c(0L, 1L)),\n    decay,\n    zero_debias = FALSE\n  )\n\n  updated_ema_count <- updated_ema_count + 1e-5\n  updated_ema_means <-  updated_ema_means / tf$expand_dims(updated_ema_count, axis = -1L)\n  \n  tf$assign(vector_quantizer$codebook, updated_ema_means)\n}\n\n\nBefore we look at the training loop, let’s quickly complete the scene adding in the last actor, the decoder.\nDecoder model\nThe decoder is pretty standard, performing a series of deconvolutions and finally, returning a probability for each image pixel.\n\n\ndefault_deconv <- set_defaults(\n  layer_conv_2d_transpose,\n  list(padding = \"same\", activation = activation)\n)\n\ndecoder_model <- function(name = NULL,\n                          input_size,\n                          output_shape) {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$reshape1 <- layer_reshape(target_shape = c(1, 1, input_size))\n    self$deconv1 <-\n      default_deconv(\n        filters = 2 * base_depth,\n        kernel_size = 7,\n        padding = \"valid\"\n      )\n    self$deconv2 <-\n      default_deconv(filters = 2 * base_depth, kernel_size = 5)\n    self$deconv3 <-\n      default_deconv(\n        filters = 2 * base_depth,\n        kernel_size = 5,\n        strides = 2\n      )\n    self$deconv4 <-\n      default_deconv(filters = base_depth, kernel_size = 5)\n    self$deconv5 <-\n      default_deconv(filters = base_depth,\n                     kernel_size = 5,\n                     strides = 2)\n    self$deconv6 <-\n      default_deconv(filters = base_depth, kernel_size = 5)\n    self$conv1 <-\n      default_conv(filters = output_shape[3],\n                   kernel_size = 5,\n                   activation = \"linear\")\n    \n    function (x, mask = NULL) {\n      \n      x <- x %>%\n        # output shape:  7 1 1 16\n        self$reshape1() %>%\n        # output shape:  7 7 7 64\n        self$deconv1() %>%\n        # output shape:  7 7 7 64\n        self$deconv2() %>%\n        # output shape:  7 14 14 64\n        self$deconv3() %>%\n        # output shape:  7 14 14 32\n        self$deconv4() %>%\n        # output shape:  7 28 28 32\n        self$deconv5() %>%\n        # output shape:  7 28 28 32\n        self$deconv6() %>%\n        # output shape:  7 28 28 1\n        self$conv1()\n      \n      tfd$Independent(tfd$Bernoulli(logits = x),\n                      reinterpreted_batch_ndims = length(output_shape))\n    }\n  })\n}\n\ninput_shape <- c(28, 28, 1)\ndecoder <- decoder_model(input_size = latent_size * code_size,\n                         output_shape = input_shape)\n\n\nNow we’re ready to train. One thing we haven’t really talked about yet is the cost function: Given the differences in architecture (compared to standard VAEs), will the losses still look as expected (the usual add-up of reconstruction loss and KL divergence)?\nWe’ll see that in a second.\nTraining loop\nHere’s the optimizer we’ll use. Losses will be calculated inline.\n\n\noptimizer <- tf$train$AdamOptimizer(learning_rate = learning_rate)\n\n\nThe training loop, as usual, is a loop over epochs, where each iteration is a loop over batches obtained from the dataset.\nFor each batch, we have a forward pass, recorded by a gradientTape, based on which we calculate the loss.\nThe tape will then determine the gradients of all trainable weights throughout the model, and the optimizer will use those gradients to update the weights.\nSo far, all of this conforms to a scheme we’ve oftentimes seen before. One point to note though: In this same loop, we also call update_ema to recalculate the moving averages, as those are not operated on during backprop.\nHere is the essential functionality:4\n\n\nnum_epochs <- 20\n\nfor (epoch in seq_len(num_epochs)) {\n  \n  iter <- make_iterator_one_shot(train_dataset)\n  \n  until_out_of_range({\n    \n    x <-  iterator_get_next(iter)\n    with(tf$GradientTape(persistent = TRUE) %as% tape, {\n      \n      # do forward pass\n      # calculate losses\n      \n    })\n    \n    encoder_gradients <- tape$gradient(loss, encoder$variables)\n    decoder_gradients <- tape$gradient(loss, decoder$variables)\n    \n    optimizer$apply_gradients(purrr::transpose(list(\n      encoder_gradients, encoder$variables\n    )),\n    global_step = tf$train$get_or_create_global_step())\n    \n    optimizer$apply_gradients(purrr::transpose(list(\n      decoder_gradients, decoder$variables\n    )),\n    global_step = tf$train$get_or_create_global_step())\n    \n    update_ema(vector_quantizer,\n               one_hot_assignments,\n               codes,\n               decay)\n\n    # periodically display some generated images\n    # see code on github \n    # visualize_images(\"kuzushiji\", epoch, reconstructed_images, random_images)\n  })\n}\n\n\nNow, for the actual action. Inside the context of the gradient tape, we first determine which encoded input sample gets assigned to which embedding vector.\n\n\ncodes <- encoder(x)\nc(nearest_codebook_entries, one_hot_assignments) %<-% vector_quantizer(codes)\n\n\nNow, for this assignment operation there is no gradient. Instead what we can do is pass the gradients from decoder input straight through to encoder output.\nHere tf$stop_gradient exempts nearest_codebook_entries from the chain of gradients, so encoder and decoder are linked by codes:\n\n\ncodes_straight_through <- codes + tf$stop_gradient(nearest_codebook_entries - codes)\ndecoder_distribution <- decoder(codes_straight_through)\n\n\nIn sum, backprop will take care of the decoder’s as well as the encoder’s weights, whereas the latent embeddings are updated using moving averages, as we’ve seen already.\nNow we’re ready to tackle the losses. There are three components:\nFirst, the reconstruction loss, which is just the log probability of the actual input under the distribution learned by the decoder.\n\n\nreconstruction_loss <- -tf$reduce_mean(decoder_distribution$log_prob(x))\n\n\nSecond, we have the commitment loss, defined as the mean squared deviation of the encoded input samples from the nearest neighbors they’ve been assigned to: We want the network to “commit” to a concise set of latent codes!\n\n\ncommitment_loss <- tf$reduce_mean(tf$square(codes - tf$stop_gradient(nearest_codebook_entries)))\n\n\nFinally, we have the usual KL diverge to a prior. As, a priori, all assignments are equally probable, this component of the loss is constant and can oftentimes be dispensed of. We’re adding it here mainly for illustrative purposes.\n\n\nprior_dist <- tfd$Multinomial(\n  total_count = 1,\n  logits = tf$zeros(c(latent_size, num_codes))\n  )\nprior_loss <- -tf$reduce_mean(\n  tf$reduce_sum(prior_dist$log_prob(one_hot_assignments), 1L)\n  )\n\n\nSumming up all three components, we arrive at the overall loss:5\n\n\nbeta <- 0.25\nloss <- reconstruction_loss + beta * commitment_loss + prior_loss\n\n\nBefore we look at the results, let’s see what happens inside gradientTape at a single glance:\n\n\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\n      \n  codes <- encoder(x)\n  c(nearest_codebook_entries, one_hot_assignments) %<-% vector_quantizer(codes)\n  codes_straight_through <- codes + tf$stop_gradient(nearest_codebook_entries - codes)\n  decoder_distribution <- decoder(codes_straight_through)\n      \n  reconstruction_loss <- -tf$reduce_mean(decoder_distribution$log_prob(x))\n  commitment_loss <- tf$reduce_mean(tf$square(codes - tf$stop_gradient(nearest_codebook_entries)))\n  prior_dist <- tfd$Multinomial(\n    total_count = 1,\n    logits = tf$zeros(c(latent_size, num_codes))\n  )\n  prior_loss <- -tf$reduce_mean(tf$reduce_sum(prior_dist$log_prob(one_hot_assignments), 1L))\n  \n  loss <- reconstruction_loss + beta * commitment_loss + prior_loss\n})\n\n\nResults\nAnd here we go. This time, we can’t have the 2d “morphing view” one generally likes to display with VAEs (there just is no 2d latent space). Instead, the two images below are (1) letters generated from random input and (2) reconstructed actual letters, each saved after training for nine epochs.\nLeft: letters generated from random input. Right: reconstructed input letters.Two things jump to the eye: First, the generated letters are significantly sharper than their continuous-prior counterparts (from the previous post). And second, would you have been able to tell the random image from the reconstruction image?\nConclusion\nAt this point, we’ve hopefully convinced you of the power and effectiveness of this discrete-latents approach.\nHowever, you might secretly have hoped we’d apply this to more complex data, such as the elements of speech we mentioned in the introduction, or higher-resolution images as found in ImageNet.6\nThe truth is that there’s a continuous tradeoff between the number of new and exciting techniques we can show, and the time we can spend on iterations to successfully apply these techniques to complex datasets. In the end it’s you, our readers, who will put these techniques to meaningful use on relevant, real world data.\n\n\n\nClanuwat, Tarin, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. 2018. “Deep Learning for Classical Japanese Literature.” December 3, 2018. https://arxiv.org/abs/cs.CV/1812.01718.\n\n\nOord, Aaron van den, Oriol Vinyals, and Koray Kavukcuoglu. 2017. “Neural Discrete Representation Learning.” CoRR abs/1711.00937. http://arxiv.org/abs/1711.00937.\n\n\nAssuming a 1d latent space, that is. The authors actually used 1d, 2d and 3d spaces in their experiments.↩︎\nIn the paper, the authors actually mention this as one of two ways to learn the prior, the other one being vector quantisation.↩︎\nTo be specific, the authors indicate that they used a field of 32 x 32 latents for ImageNet, and 8 x 8 x 10 for CIFAR10.↩︎\nThe code on github additionally contains functionality to display generated images, output the losses, and save checkpoints.↩︎\nHere beta is a scaling parameter found surprisingly unimportant by the paper authors.↩︎\nAlthough we have to say we find that Kuzushiji-MNIST beats MNIST by far, in complexity and aesthetics!↩︎\n",
    "preview": "posts/2019-01-24-vq-vae/images/thumb1.png",
    "last_modified": "2024-11-21T15:50:10+00:00",
    "input_file": {},
    "preview_width": 510,
    "preview_height": 287
  },
  {
    "path": "posts/2019-01-08-getting-started-with-tf-probability/",
    "title": "Getting started with TensorFlow Probability from R",
    "description": "TensorFlow Probability offers a vast range of functionality ranging from distributions over probabilistic network layers to probabilistic inference. It works seamlessly with core TensorFlow and (TensorFlow) Keras. In this post, we provide a short introduction to the distributions layer and then, use it for sampling and calculating probabilities in a Variational Autoencoder.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-01-08",
    "categories": [
      "TensorFlow/Keras",
      "Probabilistic ML/DL",
      "Generative Models"
    ],
    "contents": "\nWith the abundance of great libraries, in R, for statistical computing, why would you be interested in TensorFlow Probability (TFP, for short)? Well - let’s look at a list of its components:\nDistributions and bijectors (bijectors are reversible, composable maps)\nProbabilistic modeling (Edward2 and probabilistic network layers)\nProbabilistic inference (via MCMC or variational inference)\nNow imagine all these working seamlessly with the TensorFlow framework - core, Keras, contributed modules - and also, running distributed and on GPU. The field of possible applications is vast - and far too diverse to cover as a whole in an introductory blog post.\nInstead, our aim here is to provide a first introduction to TFP, focusing on direct applicability to and interoperability with deep learning.\nWe’ll quickly show how to get started with one of the basic building blocks: distributions. Then, we’ll build a variational autoencoder similar to that in Representation learning with MMD-VAE. This time though, we’ll make use of TFP to sample from the prior and approximate posterior distributions.\nWe’ll regard this post as a “proof on concept” for using TFP with Keras - from R - and plan to follow up with more elaborate examples from the area of semi-supervised representation learning.\nInstalling and using TFP\nTo install TFP together with TensorFlow, simply append tensorflow-probability to the default list of extra packages:1\n\n\nlibrary(tensorflow)\ninstall_tensorflow(\n  extra_packages = c(\"keras\", \"tensorflow-hub\", \"tensorflow-probability\"),\n  version = \"1.12\"\n)\n\n\nNow to use TFP, all we need to do is import it and create some useful handles.\n\n\nlibrary(tensorflow)\ntfp <- import(\"tensorflow_probability\")\ntfd <- tfp$distributions\n\n\nAnd here we go, sampling from a standard normal distribution.\n\n\nn <- tfd$Normal(loc = 0, scale = 1)\nn$sample(6L)\n\n\ntf.Tensor(\n\"Normal_1/sample/Reshape:0\", shape=(6,), dtype=float32\n)\nNow that’s nice, but it’s 2019, we don’t want to have to create a session to evaluate these tensors anymore. In the variational autoencoder example below, we are going to see how TFP and TF eager execution are the perfect match, so why not start using it now.\nTo use eager execution, we have to execute the following lines in a fresh (R) session:\n\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\n\n… and import TFP, same as above.\n\n\ntfp <- import(\"tensorflow_probability\")\ntfd <- tfp$distributions\n\n\nNow let’s quickly look at TFP distributions.\nUsing distributions\nHere’s that standard normal again.\n\n\nn <- tfd$Normal(loc = 0, scale = 1)\n\n\nThings commonly done with a distribution include sampling:\n\n\n# just as in low-level tensorflow, we need to append L to indicate integer arguments\nn$sample(6L) \n\n\ntf.Tensor(\n[-0.34403768 -0.14122334 -1.3832929   1.618252    1.364448   -1.1299014 ],\nshape=(6,),\ndtype=float32\n)\nAs well as getting the log probability. Here we do that simultaneously for three values.\n\n\nn$log_prob(c(-1, 0, 1))\n\n\ntf.Tensor(\n[-1.4189385 -0.9189385 -1.4189385], shape=(3,), dtype=float32\n)\nWe can do the same things with lots of other distributions, e.g., the Bernoulli:\n\n\nb <- tfd$Bernoulli(0.9)\nb$sample(10L)\n\n\ntf.Tensor(\n[1 1 1 0 1 1 0 1 0 1], shape=(10,), dtype=int32\n)\n\n\nb$log_prob(c(0,1,0,0))\n\n\ntf.Tensor(\n[-1.2411538 -0.3411539 -1.2411538 -1.2411538], shape=(4,), dtype=float32\n)\nNote that in the last chunk, we are asking for the log probabilities of four independent draws.\nBatch shapes and event shapes\nIn TFP, we can do the following.\n\n\nns <- tfd$Normal(\n  loc = c(1, 10, -200),\n  scale = c(0.1, 0.1, 1)\n)\nns\n\n\ntfp.distributions.Normal(\n\"Normal/\", batch_shape=(3,), event_shape=(), dtype=float32\n)\nContrary to what it might look like, this is not a multivariate normal. As indicated by batch_shape=(3,), this is a “batch” of independent univariate distributions. The fact that these are univariate is seen in event_shape=(): Each of them lives in one-dimensional event space.\nIf instead we create a single, two-dimensional multivariate normal:\n\n\nn <- tfd$MultivariateNormalDiag(loc = c(0, 10), scale_diag = c(1, 4))\nn\n\n\ntfp.distributions.MultivariateNormalDiag(\n\"MultivariateNormalDiag/\", batch_shape=(), event_shape=(2,), dtype=float32\n)\nwe see batch_shape=(), event_shape=(2,), as expected.\nOf course, we can combine both, creating batches of multivariate distributions:\n\n\nnd_batch <- tfd$MultivariateNormalFullCovariance(\n  loc = list(c(0., 0.), c(1., 1.), c(2., 2.)),\n  covariance_matrix = list(\n    matrix(c(1, .1, .1, 1), ncol = 2),\n    matrix(c(1, .3, .3, 1), ncol = 2),\n    matrix(c(1, .5, .5, 1), ncol = 2))\n)\n\n\nThis example defines a batch of three two-dimensional multivariate normal distributions.\nConverting between batch shapes and event shapes\nStrange as it may sound, situations arise where we want to transform distribution shapes between these types - in fact, we’ll see such a case very soon.\ntfd$Independent is used to convert dimensions in batch_shape to dimensions in event_shape.\nHere is a batch of three independent Bernoulli distributions.\n\n\nbs <- tfd$Bernoulli(probs=c(.3,.5,.7))\nbs\n\n\ntfp.distributions.Bernoulli(\n\"Bernoulli/\", batch_shape=(3,), event_shape=(), dtype=int32\n)\nWe can convert this to a virtual “three-dimensional” Bernoulli like this:\n\n\nb <- tfd$Independent(bs, reinterpreted_batch_ndims = 1L)\nb\n\n\ntfp.distributions.Independent(\n\"IndependentBernoulli/\", batch_shape=(), event_shape=(3,), dtype=int32\n)\nHere reinterpreted_batch_ndims tells TFP how many of the batch dimensions are being used for the event space, starting to count from the right of the shape list.\nWith this basic understanding of TFP distributions, we’re ready to see them used in a VAE.\nVariational autoencoder using TFP\nWe’ll take the (not so) deep convolutional architecture from Representation learning with MMD-VAE and use distributions for sampling and computing probabilities. Optionally, our new VAE will be able to learn the prior distribution.\nConcretely, the following exposition will consist of three parts.\nFirst, we present common code applicable to both a VAE with a static prior, and one that learns the parameters of the prior distribution.\nThen, we have the training loop for the first (static-prior) VAE. Finally, we discuss the training loop and additional model involved in the second (prior-learning) VAE.\nPresenting both versions one after the other leads to code duplications, but avoids scattering confusing if-else branches throughout the code.\nThe second VAE is available as part of the Keras examples so you don’t have to copy out code snippets. The code also contains additional functionality not discussed and replicated here, such as for saving model weights.\nSo, let’s start with the common part.\nAt the risk of repeating ourselves, here again are the preparatory steps (including a few additional library loads).\n\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\ntfp <- import(\"tensorflow_probability\")\ntfd <- tfp$distributions\n\nlibrary(tfdatasets)\nlibrary(dplyr)\nlibrary(glue)\n\n\nDataset\nFor a change from MNIST and Fashion-MNIST, we’ll use the brand new Kuzushiji-MNIST(Clanuwat et al. 2018).\nFrom: Deep Learning for Classical Japanese Literature (Clanuwat et al. 2018)\n\nnp <- import(\"numpy\")\n\nkuzushiji <- np$load(\"kmnist-train-imgs.npz\")\nkuzushiji <- kuzushiji$get(\"arr_0\")\n \ntrain_images <- kuzushiji %>%\n  k_expand_dims() %>%\n  k_cast(dtype = \"float32\")\n\ntrain_images <- train_images %>% `/`(255)\n\n\nAs in that other post, we stream the data via tfdatasets:\nbuffer_size <- 60000\nbatch_size <- 256\nbatches_per_epoch <- buffer_size / batch_size\n\ntrain_dataset <- tensor_slices_dataset(train_images) %>%\n  dataset_shuffle(buffer_size) %>%\n  dataset_batch(batch_size)\nNow let’s see what changes in the encoder and decoder models.\nEncoder\nThe encoder differs from what we had without TFP in that it does not return the approximate posterior means and variances directly as tensors. Instead, it returns a batch of multivariate normal distributions:\n\n\n# you might want to change this depending on the dataset\nlatent_dim <- 2\n\nencoder_model <- function(name = NULL) {\n\n  keras_model_custom(name = name, function(self) {\n  \n    self$conv1 <-\n      layer_conv_2d(\n        filters = 32,\n        kernel_size = 3,\n        strides = 2,\n        activation = \"relu\"\n      )\n    self$conv2 <-\n      layer_conv_2d(\n        filters = 64,\n        kernel_size = 3,\n        strides = 2,\n        activation = \"relu\"\n      )\n    self$flatten <- layer_flatten()\n    self$dense <- layer_dense(units = 2 * latent_dim)\n    \n    function (x, mask = NULL) {\n      x <- x %>%\n        self$conv1() %>%\n        self$conv2() %>%\n        self$flatten() %>%\n        self$dense()\n        \n      tfd$MultivariateNormalDiag(\n        loc = x[, 1:latent_dim],\n        scale_diag = tf$nn$softplus(x[, (latent_dim + 1):(2 * latent_dim)] + 1e-5)\n      )\n    }\n  })\n}\n\n\nLet’s try this out.\n\n\nencoder <- encoder_model()\n\niter <- make_iterator_one_shot(train_dataset)\nx <-  iterator_get_next(iter)\n\napprox_posterior <- encoder(x)\napprox_posterior\n\n\ntfp.distributions.MultivariateNormalDiag(\n\"MultivariateNormalDiag/\", batch_shape=(256,), event_shape=(2,), dtype=float32\n)\n\n\napprox_posterior$sample()\n\n\ntf.Tensor(\n[[ 5.77791929e-01 -1.64988488e-02]\n [ 7.93901443e-01 -1.00042784e+00]\n [-1.56279251e-01 -4.06365871e-01]\n ...\n ...\n [-6.47531569e-01  2.10889503e-02]], shape=(256, 2), dtype=float32)\n\nWe don’t know about you, but we still enjoy the ease of inspecting values with eager execution - a lot.\nNow, on to the decoder, which too returns a distribution instead of a tensor.\nDecoder\nIn the decoder, we see why transformations between batch shape and event shape are useful.\nThe output of self$deconv3 is four-dimensional. What we need is an on-off-probability for every pixel.\nFormerly, this was accomplished by feeding the tensor into a dense layer and applying a sigmoid activation.\nHere, we use tfd$Independent to effectively tranform the tensor into a probability distribution over three-dimensional images (width, height, channel(s)).\n\n\ndecoder_model <- function(name = NULL) {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$dense <- layer_dense(units = 7 * 7 * 32, activation = \"relu\")\n    self$reshape <- layer_reshape(target_shape = c(7, 7, 32))\n    self$deconv1 <-\n      layer_conv_2d_transpose(\n        filters = 64,\n        kernel_size = 3,\n        strides = 2,\n        padding = \"same\",\n        activation = \"relu\"\n      )\n    self$deconv2 <-\n      layer_conv_2d_transpose(\n        filters = 32,\n        kernel_size = 3,\n        strides = 2,\n        padding = \"same\",\n        activation = \"relu\"\n      )\n    self$deconv3 <-\n      layer_conv_2d_transpose(\n        filters = 1,\n        kernel_size = 3,\n        strides = 1,\n        padding = \"same\"\n      )\n    \n    function (x, mask = NULL) {\n      x <- x %>%\n        self$dense() %>%\n        self$reshape() %>%\n        self$deconv1() %>%\n        self$deconv2() %>%\n        self$deconv3()\n      \n      tfd$Independent(tfd$Bernoulli(logits = x),\n                      reinterpreted_batch_ndims = 3L)\n      \n    }\n  })\n}\n\n\nLet’s try this out too.\n\n\ndecoder <- decoder_model()\ndecoder_likelihood <- decoder(approx_posterior_sample)\n\n\ntfp.distributions.Independent(\n\"IndependentBernoulli/\", batch_shape=(256,), event_shape=(28, 28, 1), dtype=int32\n)\nThis distribution will be used to generate the “reconstructions,” as well as determine the loglikelihood of the original samples.\nKL loss and optimizer\nBoth VAEs discussed below will need an optimizer …\n\n\noptimizer <- tf$train$AdamOptimizer(1e-4)\n\n\n… and both will delegate to compute_kl_loss to compute the KL part of the loss.\nThis helper function simply subtracts the log likelihood of the samples2 under the prior from their loglikelihood under the approximate posterior.\n\n\ncompute_kl_loss <- function(\n  latent_prior,\n  approx_posterior,\n  approx_posterior_sample) {\n  \n  kl_div <- approx_posterior$log_prob(approx_posterior_sample) -\n    latent_prior$log_prob(approx_posterior_sample)\n  avg_kl_div <- tf$reduce_mean(kl_div)\n  avg_kl_div\n}\n\n\nNow that we’ve looked at the common parts, we first discuss how to train a VAE with a static prior.\nVAE with static prior\nIn this VAE, we use TFP to create the usual isotropic Gaussian prior.\nWe then directly sample from this distribution in the training loop.\n\n\nlatent_prior <- tfd$MultivariateNormalDiag(\n  loc  = tf$zeros(list(latent_dim)),\n  scale_identity_multiplier = 1\n)\n\n\nAnd here is the complete training loop. We’ll point out the crucial TFP-related steps below.\n\n\nfor (epoch in seq_len(num_epochs)) {\n  iter <- make_iterator_one_shot(train_dataset)\n  \n  total_loss <- 0\n  total_loss_nll <- 0\n  total_loss_kl <- 0\n  \n  until_out_of_range({\n    x <-  iterator_get_next(iter)\n    \n    with(tf$GradientTape(persistent = TRUE) %as% tape, {\n      approx_posterior <- encoder(x)\n      approx_posterior_sample <- approx_posterior$sample()\n      decoder_likelihood <- decoder(approx_posterior_sample)\n      \n      nll <- -decoder_likelihood$log_prob(x)\n      avg_nll <- tf$reduce_mean(nll)\n      \n      kl_loss <- compute_kl_loss(\n        latent_prior,\n        approx_posterior,\n        approx_posterior_sample\n      )\n\n      loss <- kl_loss + avg_nll\n    })\n    \n    total_loss <- total_loss + loss\n    total_loss_nll <- total_loss_nll + avg_nll\n    total_loss_kl <- total_loss_kl + kl_loss\n    \n    encoder_gradients <- tape$gradient(loss, encoder$variables)\n    decoder_gradients <- tape$gradient(loss, decoder$variables)\n    \n    optimizer$apply_gradients(purrr::transpose(list(\n      encoder_gradients, encoder$variables\n    )),\n    global_step = tf$train$get_or_create_global_step())\n    optimizer$apply_gradients(purrr::transpose(list(\n      decoder_gradients, decoder$variables\n    )),\n    global_step = tf$train$get_or_create_global_step())\n \n  })\n  \n  cat(\n    glue(\n      \"Losses (epoch): {epoch}:\",\n      \"  {(as.numeric(total_loss_nll)/batches_per_epoch) %>% round(4)} nll\",\n      \"  {(as.numeric(total_loss_kl)/batches_per_epoch) %>% round(4)} kl\",\n      \"  {(as.numeric(total_loss)/batches_per_epoch) %>% round(4)} total\"\n    ),\n    \"\\n\"\n  )\n}\n\n\nAbove, playing around with the encoder and the decoder, we’ve already seen how\n\n\napprox_posterior <- encoder(x)\n\n\ngives us a distribution we can sample from. We use it to obtain samples from the approximate posterior:\n\n\napprox_posterior_sample <- approx_posterior$sample()\n\n\nThese samples, we take them and feed them to the decoder, who gives us on-off-likelihoods for image pixels.\n\n\ndecoder_likelihood <- decoder(approx_posterior_sample)\n\n\nNow the loss consists of the usual ELBO components: reconstruction loss and KL divergence.\nThe reconstruction loss we directly obtain from TFP, using the learned decoder distribution to assess the likelihood of the original input.\n\n\nnll <- -decoder_likelihood$log_prob(x)\navg_nll <- tf$reduce_mean(nll)\n\n\nThe KL loss we get from compute_kl_loss, the helper function we saw above:\n\n\nkl_loss <- compute_kl_loss(\n        latent_prior,\n        approx_posterior,\n        approx_posterior_sample\n      )\n\n\nWe add both and arrive at the overall VAE loss:\n\n\nloss <- kl_loss + avg_nll\n\n\nApart from these changes due to using TFP, the training process is just normal backprop, the way it looks using eager execution.\nVAE with learnable prior (mixture of Gaussians)\nNow let’s see how instead of using the standard isotropic Gaussian, we could learn a mixture of Gaussians.\nThe choice of number of distributions here is pretty arbitrary. Just as with latent_dim, you might want to experiment and find out what works best on your dataset.\n\n\nmixture_components <- 16\n\nlearnable_prior_model <- function(name = NULL, latent_dim, mixture_components) {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$loc <-\n      tf$get_variable(\n        name = \"loc\",\n        shape = list(mixture_components, latent_dim),\n        dtype = tf$float32\n      )\n    self$raw_scale_diag <- tf$get_variable(\n      name = \"raw_scale_diag\",\n      shape = c(mixture_components, latent_dim),\n      dtype = tf$float32\n    )\n    self$mixture_logits <-\n      tf$get_variable(\n        name = \"mixture_logits\",\n        shape = c(mixture_components),\n        dtype = tf$float32\n      )\n      \n    function (x, mask = NULL) {\n        tfd$MixtureSameFamily(\n          components_distribution = tfd$MultivariateNormalDiag(\n            loc = self$loc,\n            scale_diag = tf$nn$softplus(self$raw_scale_diag)\n          ),\n          mixture_distribution = tfd$Categorical(logits = self$mixture_logits)\n        )\n      }\n    })\n  }\n\n\nIn TFP terminology, components_distribution is the underlying distribution type, and mixture_distribution holds the probabilities that individual components are chosen.\nNote how self$loc, self$raw_scale_diag and self$mixture_logits are TensorFlow Variables and thus, persistent and updatable by backprop.\nNow we create the model.\n\n\nlatent_prior_model <- learnable_prior_model(\n  latent_dim = latent_dim,\n  mixture_components = mixture_components\n)\n\n\nHow do we obtain a latent prior distribution we can sample from? A bit unusually, this model will be called without an input:\n\n\nlatent_prior <- latent_prior_model(NULL)\nlatent_prior\n\n\ntfp.distributions.MixtureSameFamily(\n\"MixtureSameFamily/\", batch_shape=(), event_shape=(2,), dtype=float32\n)\nHere now is the complete training loop. Note how we have a third model to backprop through.\n\n\nfor (epoch in seq_len(num_epochs)) {\n  iter <- make_iterator_one_shot(train_dataset)\n  \n  total_loss <- 0\n  total_loss_nll <- 0\n  total_loss_kl <- 0\n  \n  until_out_of_range({\n    x <-  iterator_get_next(iter)\n    \n    with(tf$GradientTape(persistent = TRUE) %as% tape, {\n      approx_posterior <- encoder(x)\n      \n      approx_posterior_sample <- approx_posterior$sample()\n      decoder_likelihood <- decoder(approx_posterior_sample)\n      \n      nll <- -decoder_likelihood$log_prob(x)\n      avg_nll <- tf$reduce_mean(nll)\n      \n      latent_prior <- latent_prior_model(NULL)\n      \n      kl_loss <- compute_kl_loss(\n        latent_prior,\n        approx_posterior,\n        approx_posterior_sample\n      )\n\n      loss <- kl_loss + avg_nll\n    })\n    \n    total_loss <- total_loss + loss\n    total_loss_nll <- total_loss_nll + avg_nll\n    total_loss_kl <- total_loss_kl + kl_loss\n    \n    encoder_gradients <- tape$gradient(loss, encoder$variables)\n    decoder_gradients <- tape$gradient(loss, decoder$variables)\n    prior_gradients <-\n      tape$gradient(loss, latent_prior_model$variables)\n    \n    optimizer$apply_gradients(purrr::transpose(list(\n      encoder_gradients, encoder$variables\n    )),\n    global_step = tf$train$get_or_create_global_step())\n    optimizer$apply_gradients(purrr::transpose(list(\n      decoder_gradients, decoder$variables\n    )),\n    global_step = tf$train$get_or_create_global_step())\n    optimizer$apply_gradients(purrr::transpose(list(\n      prior_gradients, latent_prior_model$variables\n    )),\n    global_step = tf$train$get_or_create_global_step())\n    \n  })\n  \n  checkpoint$save(file_prefix = checkpoint_prefix)\n  \n  cat(\n    glue(\n      \"Losses (epoch): {epoch}:\",\n      \"  {(as.numeric(total_loss_nll)/batches_per_epoch) %>% round(4)} nll\",\n      \"  {(as.numeric(total_loss_kl)/batches_per_epoch) %>% round(4)} kl\",\n      \"  {(as.numeric(total_loss)/batches_per_epoch) %>% round(4)} total\"\n    ),\n    \"\\n\"\n  )\n}  \n\n\nAnd that’s it! For us, both VAEs yielded similar results, and we did not experience great differences from experimenting with latent dimensionality and the number of mixture distributions. But again, we wouldn’t want to generalize to other datasets, architectures, etc.\nSpeaking of results, how do they look? Here we see letters generated after 40 epochs of training. On the left are random letters, on the right, the usual VAE grid display of latent space.\n\nWrapping up\nHopefully, we’ve succeeded in showing that TensorFlow Probability, eager execution, and Keras make for an attractive combination! If you relate total amount of code required to the complexity of the task, as well as depth of the concepts involved, this should appear as a pretty concise implementation.\nIn the nearer future, we plan to follow up with more involved applications of TensorFlow Probability, mostly from the area of representation learning. Stay tuned!\n\n\n\nClanuwat, Tarin, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. 2018. “Deep Learning for Classical Japanese Literature.” December 3, 2018. https://arxiv.org/abs/cs.CV/1812.01718.\n\n\nAt the time we’re publishing this post, we need to pass the “version” argument to enforce compatibility between TF and TFP. This is due to a quirk in TF’s handling of embedded Python that has us temporarily install TF 1.10 by default, until that quirk is to disappear with the upcoming release of TF 1.13.↩︎\nJust to be clear: by samples here we mean “samples from the approximate posterior”↩︎\n",
    "preview": "posts/2019-01-08-getting-started-with-tf-probability/images/thumb.png",
    "last_modified": "2024-11-21T15:49:31+00:00",
    "input_file": {},
    "preview_width": 884,
    "preview_height": 584
  },
  {
    "path": "posts/2018-12-18-object-detection-concepts/",
    "title": "Concepts in object detection",
    "description": "As shown in a previous post, naming and locating a single object in an image is a task that may be approached in a straightforward way. This is not the same with general object detection, though - naming and locating several objects at once, with no prior information about how many objects are supposed to be detected.\nIn this post, we explain the steps involved in coding a basic single-shot object detector: Not unlike SSD (Single-shot Multibox Detector), but simplified and designed not for best performance, but comprehensibility.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-12-18",
    "categories": [
      "TensorFlow/Keras",
      "Image Recognition & Image Processing"
    ],
    "contents": "\n\n\n\nA few weeks ago, we provided an introduction to the task of naming and locating objects in images.\nCrucially, we confined ourselves to detecting a single object in an image. Reading that article, you might have thought “can’t we just extend this approach to several objects?” The short answer is, not in a straightforward way. We’ll see a longer answer shortly.\nIn this post, we want to detail one viable approach, explaining (and coding) the steps involved. We won’t, however, end up with a production-ready model. So if you read on, you won’t have a model you can export and put on your smartphone, for use in the wild. You should, however, have learned a bit about how this - object detection - is even possible. After all, it might look like magic!\nThe code below is heavily based on fast.ai’s implementation of SSD. While this is not the first time we’re “porting” fast.ai models, in this case we found differences in execution models between PyTorch and TensorFlow to be especially striking, and we will briefly touch on this in our discussion.\nSo why is object detection hard?\nAs we saw, we can classify and detect a single object as follows. We make use of a powerful feature extractor, such as Resnet 50, add a few conv layers for specialization, and then, concatenate two outputs: one that indicates class, and one that has four coordinates specifying a bounding box.\nNow, to detect several objects, can’t we just have several class outputs, and several bounding boxes?\nUnfortunately we can’t. Assume there are two cute cats in the image, and we have just two bounding box detectors.\nHow does each of them know which cat to detect? What happens in practice is that both of them try to designate both cats, so we end up with two bounding boxes in the middle - where there’s no cat. It’s a bit like averaging a bimodal distribution.\nWhat can be done? Overall, there are three approaches to object detection, differing in performance in both common senses of the word: execution time and precision.\nProbably the first option you’d think of (if you haven’t been exposed to the topic before) is running the algorithm over the image piece by piece. This is called the sliding windows approach, and even though in a naive implementation, it would require excessive time, it can be run effectively if making use of fully convolutional models (cf. Overfeat (Sermanet et al. 2013)).\nCurrently the best precision is gained from region proposal approaches (R-CNN(Girshick et al. 2013), Fast R-CNN(Girshick 2015), Faster R-CNN(Ren et al. 2015)). These operate in two steps. A first step points out regions of interest in an image. Then, a convnet classifies and localizes the objects in each region.\nIn the first step, originally non-deep-learning algorithms were used. With Faster R-CNN though, a convnet takes care of region proposal as well, such that the method now is “fully deep learning.”\nLast but not least, there is the class of single shot detectors, like YOLO(Redmon et al. 2015)(Redmon and Farhadi 2016)(Redmon and Farhadi 2018)and SSD(Liu et al. 2015). Just as Overfeat, these do a single pass only, but they add an additional feature that boosts precision: anchor boxes.\nUse of anchor boxes in SSD. Figure from (Liu et al. 2015)Anchor boxes are prototypical object shapes, arranged systematically over the image. In the simplest case, these can just be rectangles (squares) spread out systematically in a grid. A simple grid already solves the basic problem we started with, above: How does each detector know which object to detect? In a single-shot approach like SSD, each detector is mapped to - responsible for - a specific anchor box. We’ll see how this can be achieved below.\nWhat if we have several objects in a grid cell? We can assign more than one anchor box to each cell. Anchor boxes are created with different aspect ratios, to provide a good match to entities of different proportions, such as people or trees on the one hand, and bicycles or balconies on the other. You can see these different anchor boxes in the above figure, in illustrations b and c.\nNow, what if an object spans several grid cells, or even the whole image? It won’t have sufficient overlap with any of the boxes to allow for successful detection. For that reason, SSD puts detectors at several stages in the model - a set of detectors after each successive step of downscaling. We see 8x8 and 4x4 grids in the figure above.\nIn this post, we show how to code a very basic single-shot approach, inspired by SSD but not going to full lengths. We’ll have a basic 16x16 grid of uniform anchors, all applied at the same resolution. In the end, we indicate how to extend this to different aspect ratios and resolutions, focusing on the model architecture.\nA basic single-shot detector\nWe’re using the same dataset as in Naming and locating objects in images - Pascal VOC, the 2007 edition - and we start out with the same preprocessing steps, up and until we have an object imageinfo that contains, in every row, information about a single object in an image.1\nFurther preprocessing\nTo be able to detect multiple objects, we need to aggregate all information on a single image into a single row.\n\n\nimageinfo4ssd <- imageinfo %>%\n  select(category_id,\n         file_name,\n         name,\n         x_left,\n         y_top,\n         x_right,\n         y_bottom,\n         ends_with(\"scaled\"))\n\nimageinfo4ssd <- imageinfo4ssd %>%\n  group_by(file_name) %>%\n  summarise(\n    categories = toString(category_id),\n    name = toString(name),\n    xl = toString(x_left_scaled),\n    yt = toString(y_top_scaled),\n    xr = toString(x_right_scaled),\n    yb = toString(y_bottom_scaled),\n    xl_orig = toString(x_left),\n    yt_orig = toString(y_top),\n    xr_orig = toString(x_right),\n    yb_orig = toString(y_bottom),\n    cnt = n()\n  )\n\n\nLet’s check we got this right.\n\n\nexample <- imageinfo4ssd[5, ]\nimg <- image_read(file.path(img_dir, example$file_name))\nname <- (example$name %>% str_split(pattern = \", \"))[[1]]\nx_left <- (example$xl_orig %>% str_split(pattern = \", \"))[[1]]\nx_right <- (example$xr_orig %>% str_split(pattern = \", \"))[[1]]\ny_top <- (example$yt_orig %>% str_split(pattern = \", \"))[[1]]\ny_bottom <- (example$yb_orig %>% str_split(pattern = \", \"))[[1]]\n\nimg <- image_draw(img)\nfor (i in 1:example$cnt) {\n  rect(x_left[i],\n       y_bottom[i],\n       x_right[i],\n       y_top[i],\n       border = \"white\",\n       lwd = 2)\n  text(\n    x = as.integer(x_right[i]),\n    y = as.integer(y_top[i]),\n    labels = name[i],\n    offset = 1,\n    pos = 2,\n    cex = 1,\n    col = \"white\"\n  )\n}\ndev.off()\nprint(img)\n\n\n\nNow we construct the anchor boxes.\nAnchors\nLike we said above, here we will have one anchor box per cell. Thus, grid cells and anchor boxes, in our case, are the same thing, and we’ll call them by both names, interchangingly, depending on the context.\nJust keep in mind that in more complex models, these will most probably be different entities.\nOur grid will be of size 4x4. We will need the cells’ coordinates, and we’ll start with a center x - center y - height - width representation.\nHere, first, are the center coordinates.\n\n\ncells_per_row <- 4\ngridsize <- 1/cells_per_row\nanchor_offset <- 1 / (cells_per_row * 2) \n\nanchor_xs <- seq(anchor_offset, 1 - anchor_offset, length.out = 4) %>%\n  rep(each = cells_per_row)\nanchor_ys <- seq(anchor_offset, 1 - anchor_offset, length.out = 4) %>%\n  rep(cells_per_row)\n\n\nWe can plot them.\n\n\nggplot(data.frame(x = anchor_xs, y = anchor_ys), aes(x, y)) +\n  geom_point() +\n  coord_cartesian(xlim = c(0,1), ylim = c(0,1)) +\n  theme(aspect.ratio = 1)\n\n\n\nThe center coordinates are supplemented by height and width:\n\n\nanchor_centers <- cbind(anchor_xs, anchor_ys)\nanchor_height_width <- matrix(1 / cells_per_row, nrow = 16, ncol = 2)\n\n\nCombining centers, heights and widths gives us the first representation.\n\n\nanchors <- cbind(anchor_centers, anchor_height_width)\nanchors\n\n\n       [,1]  [,2] [,3] [,4]\n [1,] 0.125 0.125 0.25 0.25\n [2,] 0.125 0.375 0.25 0.25\n [3,] 0.125 0.625 0.25 0.25\n [4,] 0.125 0.875 0.25 0.25\n [5,] 0.375 0.125 0.25 0.25\n [6,] 0.375 0.375 0.25 0.25\n [7,] 0.375 0.625 0.25 0.25\n [8,] 0.375 0.875 0.25 0.25\n [9,] 0.625 0.125 0.25 0.25\n[10,] 0.625 0.375 0.25 0.25\n[11,] 0.625 0.625 0.25 0.25\n[12,] 0.625 0.875 0.25 0.25\n[13,] 0.875 0.125 0.25 0.25\n[14,] 0.875 0.375 0.25 0.25\n[15,] 0.875 0.625 0.25 0.25\n[16,] 0.875 0.875 0.25 0.25\nIn subsequent manipulations, we will sometimes we need a different representation: the corners (top-left, top-right, bottom-right, bottom-left) of the grid cells.\n\n\nhw2corners <- function(centers, height_width) {\n  cbind(centers - height_width / 2, centers + height_width / 2) %>% unname()\n}\n\n# cells are indicated by (xl, yt, xr, yb)\n# successive rows first go down in the image, then to the right\nanchor_corners <- hw2corners(anchor_centers, anchor_height_width)\nanchor_corners\n\n\n      [,1] [,2] [,3] [,4]\n [1,] 0.00 0.00 0.25 0.25\n [2,] 0.00 0.25 0.25 0.50\n [3,] 0.00 0.50 0.25 0.75\n [4,] 0.00 0.75 0.25 1.00\n [5,] 0.25 0.00 0.50 0.25\n [6,] 0.25 0.25 0.50 0.50\n [7,] 0.25 0.50 0.50 0.75\n [8,] 0.25 0.75 0.50 1.00\n [9,] 0.50 0.00 0.75 0.25\n[10,] 0.50 0.25 0.75 0.50\n[11,] 0.50 0.50 0.75 0.75\n[12,] 0.50 0.75 0.75 1.00\n[13,] 0.75 0.00 1.00 0.25\n[14,] 0.75 0.25 1.00 0.50\n[15,] 0.75 0.50 1.00 0.75\n[16,] 0.75 0.75 1.00 1.00\nLet’s take our sample image again and plot it, this time including the grid cells.\nNote that we display the scaled image now - the way the network is going to see it.\n\n\nexample <- imageinfo4ssd[5, ]\nname <- (example$name %>% str_split(pattern = \", \"))[[1]]\nx_left <- (example$xl %>% str_split(pattern = \", \"))[[1]]\nx_right <- (example$xr %>% str_split(pattern = \", \"))[[1]]\ny_top <- (example$yt %>% str_split(pattern = \", \"))[[1]]\ny_bottom <- (example$yb %>% str_split(pattern = \", \"))[[1]]\n\n\nimg <- image_read(file.path(img_dir, example$file_name))\nimg <- image_resize(img, geometry = \"224x224!\")\nimg <- image_draw(img)\n\nfor (i in 1:example$cnt) {\n  rect(x_left[i],\n       y_bottom[i],\n       x_right[i],\n       y_top[i],\n       border = \"white\",\n       lwd = 2)\n  text(\n    x = as.integer(x_right[i]),\n    y = as.integer(y_top[i]),\n    labels = name[i],\n    offset = 0,\n    pos = 2,\n    cex = 1,\n    col = \"white\"\n  )\n}\nfor (i in 1:nrow(anchor_corners)) {\n  rect(\n    anchor_corners[i, 1] * 224,\n    anchor_corners[i, 4] * 224,\n    anchor_corners[i, 3] * 224,\n    anchor_corners[i, 2] * 224,\n    border = \"cyan\",\n    lwd = 1,\n    lty = 3\n  )\n}\n\ndev.off()\nprint(img)\n\n\n\nNow it’s time to address the possibly greatest mystery when you’re new to object detection: How do you actually construct the ground truth input to the network?\nThat is the so-called “matching problem.”\nMatching problem\nTo train the network, we need to assign the ground truth boxes to the grid cells/anchor boxes. We do this based on overlap between bounding boxes on the one hand, and anchor boxes on the other.\nOverlap is computed using Intersection over Union (IoU, =Jaccard Index), as usual.\nAssume we’ve already computed the Jaccard index for all ground truth box - grid cell combinations. We then use the following algorithm:\nFor each ground truth object, find the grid cell it maximally overlaps with.\nFor each grid cell, find the object it overlaps with most.\nIn both cases, identify the entity of greatest overlap as well as the amount of overlap.\nWhen criterium (1) applies, it overrides criterium (2).\nWhen criterium (1) applies, set the amount overlap to a constant, high value: 1.99.\nReturn the combined result, that is, for each grid cell, the object and amount of best (as per the above criteria) overlap.\nHere’s the implementation.\n\n\n# overlaps shape is: number of ground truth objects * number of grid cells\nmap_to_ground_truth <- function(overlaps) {\n  \n  # for each ground truth object, find maximally overlapping cell (crit. 1)\n  # measure of overlap, shape: number of ground truth objects\n  prior_overlap <- apply(overlaps, 1, max)\n  # which cell is this, for each object\n  prior_idx <- apply(overlaps, 1, which.max)\n  \n  # for each grid cell, what object does it overlap with most (crit. 2)\n  # measure of overlap, shape: number of grid cells\n  gt_overlap <-  apply(overlaps, 2, max)\n  # which object is this, for each cell\n  gt_idx <- apply(overlaps, 2, which.max)\n  \n  # set all definitely overlapping cells to respective object (crit. 1)\n  gt_overlap[prior_idx] <- 1.99\n  \n  # now still set all others to best match by crit. 2\n  # actually it's other way round, we start from (2) and overwrite with (1)\n  for (i in 1:length(prior_idx)) {\n    # iterate over all cells \"absolutely assigned\"\n    p <- prior_idx[i] # get respective grid cell\n    gt_idx[p] <- i # assign this cell the object number\n  }\n  \n  # return: for each grid cell, object it overlaps with most + measure of overlap\n  list(gt_overlap, gt_idx)\n  \n}\n\n\nNow here’s the IoU calculation we need for that. We can’t just use the IoU function from the previous post because this time, we want to compute overlaps with all grid cells simultaneously.\nIt’s easiest to do this using tensors, so we temporarily convert the R matrices to tensors:\n\n\n# compute IOU\njaccard <- function(bbox, anchor_corners) {\n  bbox <- k_constant(bbox)\n  anchor_corners <- k_constant(anchor_corners)\n  intersection <- intersect(bbox, anchor_corners)\n  union <-\n    k_expand_dims(box_area(bbox), axis = 2)  + k_expand_dims(box_area(anchor_corners), axis = 1) - intersection\n    res <- intersection / union\n  res %>% k_eval()\n}\n\n# compute intersection for IOU\nintersect <- function(box1, box2) {\n  box1_a <- box1[, 3:4] %>% k_expand_dims(axis = 2)\n  box2_a <- box2[, 3:4] %>% k_expand_dims(axis = 1)\n  max_xy <- k_minimum(box1_a, box2_a)\n  \n  box1_b <- box1[, 1:2] %>% k_expand_dims(axis = 2)\n  box2_b <- box2[, 1:2] %>% k_expand_dims(axis = 1)\n  min_xy <- k_maximum(box1_b, box2_b)\n  \n  intersection <- k_clip(max_xy - min_xy, min = 0, max = Inf)\n  intersection[, , 1] * intersection[, , 2]\n  \n}\n\nbox_area <- function(box) {\n  (box[, 3] - box[, 1]) * (box[, 4] - box[, 2]) \n}\n\n\nBy now you might be wondering - when does all this happen? Interestingly, the example we’re following, fast.ai’s object detection notebook, does all this as part of the loss calculation!\nIn TensorFlow, this is possible in principle (requiring some juggling of tf$cond, tf$while_loop etc., as well as a bit of creativity finding replacements for non-differentiable operations).\nBut, simple facts - like the Keras loss function expecting the same shapes for y_true and y_pred - made it impossible to follow the fast.ai approach. Instead, all matching will take place in the data generator.\nData generator\nThe generator has the familiar structure, known from the predecessor post.\nHere is the complete code - we’ll talk through the details immediately.\n\n\nbatch_size <- 16\nimage_size <- target_width # same as height\n\nthreshold <- 0.4\n\nclass_background <- 21\n\nssd_generator <-\n  function(data,\n           target_height,\n           target_width,\n           shuffle,\n           batch_size) {\n    i <- 1\n    function() {\n      if (shuffle) {\n        indices <- sample(1:nrow(data), size = batch_size)\n      } else {\n        if (i + batch_size >= nrow(data))\n          i <<- 1\n        indices <- c(i:min(i + batch_size - 1, nrow(data)))\n        i <<- i + length(indices)\n      }\n      \n      x <-\n        array(0, dim = c(length(indices), target_height, target_width, 3))\n      y1 <- array(0, dim = c(length(indices), 16))\n      y2 <- array(0, dim = c(length(indices), 16, 4))\n      \n      for (j in 1:length(indices)) {\n        x[j, , , ] <-\n          load_and_preprocess_image(data[[indices[j], \"file_name\"]], target_height, target_width)\n        \n        class_string <- data[indices[j], ]$categories\n        xl_string <- data[indices[j], ]$xl\n        yt_string <- data[indices[j], ]$yt\n        xr_string <- data[indices[j], ]$xr\n        yb_string <- data[indices[j], ]$yb\n        \n        classes <-  str_split(class_string, pattern = \", \")[[1]]\n        xl <-\n          str_split(xl_string, pattern = \", \")[[1]] %>% as.double() %>% `/`(image_size)\n        yt <-\n          str_split(yt_string, pattern = \", \")[[1]] %>% as.double() %>% `/`(image_size)\n        xr <-\n          str_split(xr_string, pattern = \", \")[[1]] %>% as.double() %>% `/`(image_size)\n        yb <-\n          str_split(yb_string, pattern = \", \")[[1]] %>% as.double() %>% `/`(image_size)\n    \n        # rows are objects, columns are coordinates (xl, yt, xr, yb)\n        # anchor_corners are 16 rows with corresponding coordinates\n        bbox <- cbind(xl, yt, xr, yb)\n        overlaps <- jaccard(bbox, anchor_corners)\n        \n        c(gt_overlap, gt_idx) %<-% map_to_ground_truth(overlaps)\n        gt_class <- classes[gt_idx]\n        \n        pos <- gt_overlap > threshold\n        gt_class[gt_overlap < threshold] <- 21\n                \n        # columns correspond to objects\n        boxes <- rbind(xl, yt, xr, yb)\n        # columns correspond to object boxes according to gt_idx\n        gt_bbox <- boxes[, gt_idx]\n        # set those with non-sufficient overlap to 0\n        gt_bbox[, !pos] <- 0\n        gt_bbox <- gt_bbox %>% t()\n        \n        y1[j, ] <- as.integer(gt_class) - 1\n        y2[j, , ] <- gt_bbox\n        \n      }\n\n      x <- x %>% imagenet_preprocess_input()\n      y1 <- y1 %>% to_categorical(num_classes = class_background)\n      list(x, list(y1, y2))\n    }\n  }\n\n\nBefore the generator can trigger any calculations, it needs to first split apart the multiple classes and bounding box coordinates that come in one row of the dataset.\nTo make this more concrete, we show what happens for the “2 people and 2 airplanes” image we just displayed.\nWe copy out code chunk-by-chunk from the generator so results can actually be displayed for inspection.\n\n\ndata <- imageinfo4ssd\nindices <- 1:8\n\nj <- 5 # this is our image\n\nclass_string <- data[indices[j], ]$categories\nxl_string <- data[indices[j], ]$xl\nyt_string <- data[indices[j], ]$yt\nxr_string <- data[indices[j], ]$xr\nyb_string <- data[indices[j], ]$yb\n        \nclasses <-  str_split(class_string, pattern = \", \")[[1]]\nxl <- str_split(xl_string, pattern = \", \")[[1]] %>% as.double() %>% `/`(image_size)\nyt <- str_split(yt_string, pattern = \", \")[[1]] %>% as.double() %>% `/`(image_size)\nxr <- str_split(xr_string, pattern = \", \")[[1]] %>% as.double() %>% `/`(image_size)\nyb <- str_split(yb_string, pattern = \", \")[[1]] %>% as.double() %>% `/`(image_size)\n\n\nSo here are that image’s classes:\n\n\nclasses\n\n\n[1] \"1\"  \"1\"  \"15\" \"15\"\nAnd its left bounding box coordinates:\n\n\nxl\n\n\n[1] 0.20535714 0.26339286 0.38839286 0.04910714\nNow we can cbind those vectors together to obtain a object (bbox) where rows are objects, and coordinates are in the columns:\n\n\n# rows are objects, columns are coordinates (xl, yt, xr, yb)\nbbox <- cbind(xl, yt, xr, yb)\nbbox\n\n\n          xl        yt         xr        yb\n[1,] 0.20535714 0.2723214 0.75000000 0.6473214\n[2,] 0.26339286 0.3080357 0.39285714 0.4330357\n[3,] 0.38839286 0.6383929 0.42410714 0.8125000\n[4,] 0.04910714 0.6696429 0.08482143 0.8437500\nSo we’re ready to compute these boxes’ overlap with all of the 16 grid cells. Recall that anchor_corners stores the grid cells in an analogous way, the cells being in the rows and the coordinates in the columns.\n\n\n# anchor_corners are 16 rows with corresponding coordinates\noverlaps <- jaccard(bbox, anchor_corners)\n\n\nNow that we have the overlaps, we can call the matching logic:\n\n\nc(gt_overlap, gt_idx) %<-% map_to_ground_truth(overlaps)\ngt_overlap\n\n\n [1] 0.00000000 0.03961473 0.04358353 1.99000000 0.00000000 1.99000000 1.99000000 0.03357313 0.00000000\n[10] 0.27127662 0.16019417 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000\nLooking for the value 1.99 in the above - the value indicating maximal, by the above criteria, overlap of an object with a grid cell - we see that box 4 (counting in column-major order here like R does) got matched (to a person, as we’ll see soon), box 6 did (to an airplane), and box 7 did (to a person). How about the other airplane? It got lost in the matching.\nThis is not a problem of the matching algorithm though - it would disappear if we had more than one anchor box per grid cell.\nLooking for the objects just mentioned in the class index, gt_idx, we see that indeed box 4 got matched to object 4 (a person), box 6 got matched to object 2 (an airplane), and box 7 got matched to object 3 (the other person):\n\n\ngt_idx\n\n\n[1] 1 1 4 4 1 2 3 3 1 1 1 1 1 1 1 1\nBy the way, don’t worry about the abundance of 1s here. These are remnants from using which.max to determine maximal overlap, and will disappear soon.\nInstead of thinking in object numbers, we should think in object classes (the respective numerical codes, that is).\n\n\ngt_class <- classes[gt_idx]\ngt_class\n\n\n [1] \"1\"  \"1\"  \"15\" \"15\" \"1\"  \"1\"  \"15\" \"15\" \"1\"  \"1\"  \"1\"  \"1\"  \"1\"  \"1\"  \"1\"  \"1\"\nSo far, we take into account even the very slightest overlap - of 0.1 percent, say.\nOf course, this makes no sense. We set all cells with an overlap < 0.4 to the background class:\n\n\npos <- gt_overlap > threshold\ngt_class[gt_overlap < threshold] <- 21\n\ngt_class\n\n\n[1] \"21\" \"21\" \"21\" \"15\" \"21\" \"1\"  \"15\" \"21\" \"21\" \"21\" \"21\" \"21\" \"21\" \"21\" \"21\" \"21\"\nNow, to construct the targets for learning, we need to put the mapping we found into a data structure.\nThe following gives us a 16x4 matrix of cells and the boxes they are responsible for:\n\n\norig_boxes <- rbind(xl, yt, xr, yb)\n# columns correspond to object boxes according to gt_idx\ngt_bbox <- orig_boxes[, gt_idx]\n# set those with non-sufficient overlap to 0\ngt_bbox[, !pos] <- 0\ngt_bbox <- gt_bbox %>% t()\n\ngt_bbox\n\n\n              xl        yt         xr        yb\n [1,] 0.00000000 0.0000000 0.00000000 0.0000000\n [2,] 0.00000000 0.0000000 0.00000000 0.0000000\n [3,] 0.00000000 0.0000000 0.00000000 0.0000000\n [4,] 0.04910714 0.6696429 0.08482143 0.8437500\n [5,] 0.00000000 0.0000000 0.00000000 0.0000000\n [6,] 0.26339286 0.3080357 0.39285714 0.4330357\n [7,] 0.38839286 0.6383929 0.42410714 0.8125000\n [8,] 0.00000000 0.0000000 0.00000000 0.0000000\n [9,] 0.00000000 0.0000000 0.00000000 0.0000000\n[10,] 0.00000000 0.0000000 0.00000000 0.0000000\n[11,] 0.00000000 0.0000000 0.00000000 0.0000000\n[12,] 0.00000000 0.0000000 0.00000000 0.0000000\n[13,] 0.00000000 0.0000000 0.00000000 0.0000000\n[14,] 0.00000000 0.0000000 0.00000000 0.0000000\n[15,] 0.00000000 0.0000000 0.00000000 0.0000000\n[16,] 0.00000000 0.0000000 0.00000000 0.0000000\nTogether, gt_bbox and gt_class make up the network’s learning targets.\n\n\ny1[j, ] <- as.integer(gt_class) - 1\ny2[j, , ] <- gt_bbox\n\n\nTo summarize, our target is a list of two outputs:\nthe bounding box ground truth of dimensionality number of grid cells times number of box coordinates, and\nthe class ground truth of size number of grid cells times number of classes.\nWe can verify this by asking the generator for a batch of inputs and targets:\n\n\ntrain_gen <- ssd_generator(\n  imageinfo4ssd,\n  target_height = target_height,\n  target_width = target_width,\n  shuffle = TRUE,\n  batch_size = batch_size\n)\n\nbatch <- train_gen()\nc(x, c(y1, y2)) %<-% batch\ndim(y1)\n\n\n[1] 16 16 21\n\n\ndim(y2)\n\n\n[1] 16 16  4\nFinally, we’re ready for the model.\nThe model\nWe start from Resnet 50 as a feature extractor. This gives us tensors of size 7x7x2048.\n\n\nfeature_extractor <- application_resnet50(\n  include_top = FALSE,\n  input_shape = c(224, 224, 3)\n)\n\n\nThen, we append a few conv layers. Three of those layers are “just” there for capacity; the last one though has a additional task: By virtue of strides = 2, it downsamples its input to from 7x7 to 4x4 in the height/width dimensions.\nThis resolution of 4x4 gives us exactly the grid we need!\n\n\ninput <- feature_extractor$input\n\ncommon <- feature_extractor$output %>%\n  layer_conv_2d(\n    filters = 256,\n    kernel_size = 3,\n    padding = \"same\",\n    activation = \"relu\",\n    name = \"head_conv1_1\"\n  ) %>%\n  layer_batch_normalization() %>%\n  layer_conv_2d(\n    filters = 256,\n    kernel_size = 3,\n    padding = \"same\",\n    activation = \"relu\",\n    name = \"head_conv1_2\"\n  ) %>%\n  layer_batch_normalization() %>%\n  layer_conv_2d(\n    filters = 256,\n    kernel_size = 3,\n    padding = \"same\",\n    activation = \"relu\",\n    name = \"head_conv1_3\"\n  ) %>%\n  layer_batch_normalization() %>%\n  layer_conv_2d(\n    filters = 256,\n    kernel_size = 3,\n    strides = 2,\n    padding = \"same\",\n    activation = \"relu\",\n    name = \"head_conv2\"\n  ) %>%\n  layer_batch_normalization() \n\n\nNow we can do as we did in that other post, attach one output for the bounding boxes and one for the classes.\nNote how we don’t aggregate over the spatial grid though. Instead, we reshape it so the 4x4 grid cells appear sequentially.\nHere first is the class output. We have 21 classes (the 20 classes from PASCAL, plus background), and we need to classify each cell. We thus end up with an output of size 16x21.\n\n\nclass_output <-\n  layer_conv_2d(\n    common,\n    filters = 21,\n    kernel_size = 3,\n    padding = \"same\",\n    name = \"class_conv\"\n  ) %>%\n  layer_reshape(target_shape = c(16, 21), name = \"class_output\")\n\n\nFor the bounding box output, we apply a tanh activation so that values lie between -1 and 1. This is because they are used to compute offsets to the grid cell centers.\nThese computations happen in the layer_lambda. We start from the actual anchor box centers, and move them around by a scaled-down version of the activations.\nWe then convert these to anchor corners - same as we did above with the ground truth anchors, just operating on tensors, this time.\n\n\nbbox_output <-\n  layer_conv_2d(\n    common,\n    filters = 4,\n    kernel_size = 3,\n    padding = \"same\",\n    name = \"bbox_conv\"\n  ) %>%\n  layer_reshape(target_shape = c(16, 4), name = \"bbox_flatten\") %>%\n  layer_activation(\"tanh\") %>%\n  layer_lambda(\n    f = function(x) {\n      activation_centers <-\n        (x[, , 1:2] / 2 * gridsize) + k_constant(anchors[, 1:2])\n      activation_height_width <-\n        (x[, , 3:4] / 2 + 1) * k_constant(anchors[, 3:4])\n      activation_corners <-\n        k_concatenate(\n          list(\n            activation_centers - activation_height_width / 2,\n            activation_centers + activation_height_width / 2\n          )\n        )\n     activation_corners\n    },\n    name = \"bbox_output\"\n  )\n\n\nNow that we have all layers, let’s quickly finish up the model definition:\n\n\nmodel <- keras_model(\n  inputs = input,\n  outputs = list(class_output, bbox_output)\n)\n\n\nThe last ingredient missing, then, is the loss function.\nLoss\nTo the model’s two outputs - a classification output and a regression output - correspond two losses, just as in the basic classification + localization model. Only this time, we have 16 grid cells to take care of.\nClass loss uses tf$nn$sigmoid_cross_entropy_with_logits to compute the binary crossentropy between targets and unnormalized network activation, summing over grid cells and dividing by the number of classes.\n\n\n# shapes are batch_size * 16 * 21\nclass_loss <- function(y_true, y_pred) {\n\n  class_loss  <-\n    tf$nn$sigmoid_cross_entropy_with_logits(labels = y_true, logits = y_pred)\n\n  class_loss <-\n    tf$reduce_sum(class_loss) / tf$cast(n_classes + 1, \"float32\")\n  \n  class_loss\n}\n\n\nLocalization loss is calculated for all boxes where in fact there is an object present in the ground truth. All other activations get masked out.\nThe loss itself then is just mean absolute error, scaled by a multiplier designed to bring both loss components to similar magnitudes. In practice, it makes sense to experiment a bit here.\n\n\n# shapes are batch_size * 16 * 4\nbbox_loss <- function(y_true, y_pred) {\n\n  # calculate localization loss for all boxes where ground truth was assigned some overlap\n  # calculate mask\n  pos <- y_true[, , 1] + y_true[, , 3] > 0\n  pos <-\n    pos %>% k_cast(tf$float32) %>% k_reshape(shape = c(batch_size, 16, 1))\n  pos <-\n    tf$tile(pos, multiples = k_constant(c(1L, 1L, 4L), dtype = tf$int32))\n    \n  diff <- y_pred - y_true\n  # mask out irrelevant activations\n  diff <- diff %>% tf$multiply(pos)\n  \n  loc_loss <- diff %>% tf$abs() %>% tf$reduce_mean()\n  loc_loss * 100\n}\n\n\nTraining\nAbove, we’ve already defined the model but we still need to freeze the feature detector’s weights and compile it.\n\n\nmodel %>% freeze_weights()\nmodel %>% unfreeze_weights(from = \"head_conv1_1\")\nmodel\n\n\n\n\nmodel %>% compile(\n  loss = list(class_loss, bbox_loss),\n  optimizer = \"adam\",\n  metrics = list(\n    class_output = custom_metric(\"class_loss\", metric_fn = class_loss),\n    bbox_output = custom_metric(\"bbox_loss\", metric_fn = bbox_loss)\n  )\n)\n\n\nAnd we’re ready to train. Training this model is very time consuming, such that for applications “in the real world,” we might want to do optimize the program for memory consumption and runtime.\nLike we said above, in this post we’re really focusing on understanding the approach.\n\n\nsteps_per_epoch <- nrow(imageinfo4ssd) / batch_size\n\nmodel %>% fit_generator(\n  train_gen,\n  steps_per_epoch = steps_per_epoch,\n  epochs = 5,\n  callbacks = callback_model_checkpoint(\n    \"weights.{epoch:02d}-{loss:.2f}.hdf5\", \n    save_weights_only = TRUE\n  )\n)\n\n\nAfter 5 epochs, this is what we get from the model. It’s on the right way, but it will need many more epochs to reach decent performance.\n\nApart from training for many more epochs, what could we do? We’ll wrap up the post with two directions for improvement, but won’t implement them completely.\nThe first one actually is quick to implement. Here we go.\nFocal loss\nAbove, we were using cross entropy for the classification loss. Let’s look at what that entails.\nBinary cross entropy for predictions when the ground truth equals 1The figure shows loss incurred when the correct answer is 1. We see that even though loss is highest when the network is very wrong, it still incurs significant loss when it’s “right for all practical purposes” - meaning, its output is just above 0.5.\nIn cases of strong class imbalance, this behavior can be problematic. Much training energy is wasted on getting “even more right” on cases where the net is right already - as will happen with instances of the dominant class. Instead, the network should dedicate more effort to the hard cases - exemplars of the rarer classes.\nIn object detection, the prevalent class is background - no class, really. Instead of getting more and more proficient at predicting background, the network had better learn how to tell apart the actual object classes.\nAn alternative was pointed out by the authors of the RetinaNet paper(Lin et al. 2017): They introduced a parameter \\(\\gamma\\)2 that results in decreasing loss for samples that already have been well classified.\nFocal loss downweights contributions from well-classified examples. Figure from (Lin et al. 2017)Different implementations are found on the net, as well as different settings for the hyperparameters. Here’s a direct port of the fast.ai code:\n\n\nalpha <- 0.25\ngamma <- 1\n\nget_weights <- function(y_true, y_pred) {\n  p <- y_pred %>% k_sigmoid()\n  pt <-  y_true*p + (1-p)*(1-y_true)\n  w <- alpha*y_true + (1-alpha)*(1-y_true)\n  w <-  w * (1-pt)^gamma\n  w\n}\n\nclass_loss_focal  <- function(y_true, y_pred) {\n  \n  w <- get_weights(y_true, y_pred)\n  cx <- tf$nn$sigmoid_cross_entropy_with_logits(labels = y_true, logits = y_pred)\n  weighted_cx <- w * cx\n\n  class_loss <-\n   tf$reduce_sum(weighted_cx) / tf$cast(21, \"float32\")\n  \n  class_loss\n}\n\n\nFrom testing this loss, it seems to yield better performance, but does not render obsolete the need for substantive training time.\nFinally, let’s see what we’d have to do if we wanted to use several anchor boxes per grid cells.\nMore anchor boxes\nThe “real SSD” has anchor boxes of different aspect ratios, and it puts detectors at different stages of the network. Let’s implement this.\nAnchor box coordinates\nWe create anchor boxes as combinations of\ndifferent scales, and\n\n\nanchor_zooms <- c(0.7, 1, 1.3)\nanchor_zooms\n\n\n[1] 0.7 1.0 1.3\ndifferent aspect ratios:\n\n\nanchor_ratios <- matrix(c(1, 1, 1, 0.5, 0.5, 1), ncol = 2, byrow = TRUE)\nanchor_ratios\n\n\n     [,1] [,2]\n[1,]  1.0  1.0\n[2,]  1.0  0.5\n[3,]  0.5  1.0\nIn this example, we have nine different combinations:\n\n\nanchor_scales <- rbind(\n  anchor_ratios * anchor_zooms[1],\n  anchor_ratios * anchor_zooms[2],\n  anchor_ratios * anchor_zooms[3]\n)\n\nk <- nrow(anchor_scales)\n\nanchor_scales\n\n\n      [,1] [,2]\n [1,] 0.70 0.70\n [2,] 0.70 0.35\n [3,] 0.35 0.70\n [4,] 1.00 1.00\n [5,] 1.00 0.50\n [6,] 0.50 1.00\n [7,] 1.30 1.30\n [8,] 1.30 0.65\n [9,] 0.65 1.30\nWe place detectors at three stages. Resolutions will be 4x4 (as we had before) and additionally, 2x2 and 1x1:\n\n\nanchor_grids <- c(4,2,1)\n\n\nOnce that’s been determined, we can compute\nx coordinates of the box centers:\n\n\nanchor_offsets <- 1/(anchor_grids * 2)\n\nanchor_x <- map(\n  1:3,\n  function(x) rep(seq(anchor_offsets[x],\n                      1 - anchor_offsets[x],\n                      length.out = anchor_grids[x]),\n                  each = anchor_grids[x])) %>%\n  flatten() %>%\n  unlist()\n\n\ny coordinates of the box centers:\n\n\nanchor_y <- map(\n  1:3,\n  function(y) rep(seq(anchor_offsets[y],\n                      1 - anchor_offsets[y],\n                      length.out = anchor_grids[y]),\n                  times = anchor_grids[y])) %>%\n  flatten() %>%\n  unlist()\n\n\nthe x-y representations of the centers:\n\n\nanchor_centers <- cbind(rep(anchor_x, each = k), rep(anchor_y, each = k))\n\n\nthe sizes of the boxes:\n\n\nanchor_sizes <- map(\n  anchor_grids,\n  function(x)\n   matrix(rep(t(anchor_scales/x), x*x), ncol = 2, byrow = TRUE)\n  ) %>%\n  abind(along = 1)\n\n\nthe sizes of the base grids (0.25, 0.5, and 1):\n\n\ngrid_sizes <- c(rep(0.25, k * anchor_grids[1]^2),\n                rep(0.5, k * anchor_grids[2]^2),\n                rep(1, k * anchor_grids[3]^2)\n                )\n\n\nthe centers-width-height representations of the anchor boxes:\n\n\nanchors <- cbind(anchor_centers, anchor_sizes)\n\n\nand finally, the corners representation of the boxes!\n\n\nhw2corners <- function(centers, height_width) {\n  cbind(centers - height_width / 2, centers + height_width / 2) %>% unname()\n}\n\nanchor_corners <- hw2corners(anchors[ , 1:2], anchors[ , 3:4])\n\n\nSo here, then, is a plot of the (distinct) box centers: One in the middle, for the 9 large boxes, 4 for the 4 * 9 medium-size boxes, and 16 for the 16 * 9 small boxes.\n\nOf course, even if we aren’t going to train this version, we at least need to see these in action!\n\nHow would a model look that could deal with these?\nModel\nAgain, we’d start from a feature detector …\n\n\nfeature_extractor <- application_resnet50(\n  include_top = FALSE,\n  input_shape = c(224, 224, 3)\n)\n\n\n… and attach some custom conv layers.\n\n\ninput <- feature_extractor$input\n\ncommon <- feature_extractor$output %>%\n  layer_conv_2d(\n    filters = 256,\n    kernel_size = 3,\n    padding = \"same\",\n    activation = \"relu\",\n    name = \"head_conv1_1\"\n  ) %>%\n  layer_batch_normalization() %>%\n  layer_conv_2d(\n    filters = 256,\n    kernel_size = 3,\n    padding = \"same\",\n    activation = \"relu\",\n    name = \"head_conv1_2\"\n  ) %>%\n  layer_batch_normalization() %>%\n  layer_conv_2d(\n    filters = 256,\n    kernel_size = 3,\n    padding = \"same\",\n    activation = \"relu\",\n    name = \"head_conv1_3\"\n  ) %>%\n  layer_batch_normalization()\n\n\nThen, things get different. We want to attach detectors (= output layers) to different stages in a pipeline of successive downsamplings.\nIf that doesn’t call for the Keras functional API…\nHere’s the downsizing pipeline.\n\n\n downscale_4x4 <- common %>%\n  layer_conv_2d(\n    filters = 256,\n    kernel_size = 3,\n    strides = 2,\n    padding = \"same\",\n    activation = \"relu\",\n    name = \"downscale_4x4\"\n  ) %>%\n  layer_batch_normalization() \n\n\n\n\ndownscale_2x2 <- downscale_4x4 %>%\n  layer_conv_2d(\n    filters = 256,\n    kernel_size = 3,\n    strides = 2,\n    padding = \"same\",\n    activation = \"relu\",\n    name = \"downscale_2x2\"\n  ) %>%\n  layer_batch_normalization() \n\n\n\n\ndownscale_1x1 <- downscale_2x2 %>%\n  layer_conv_2d(\n    filters = 256,\n    kernel_size = 3,\n    strides = 2,\n    padding = \"same\",\n    activation = \"relu\",\n    name = \"downscale_1x1\"\n  ) %>%\n  layer_batch_normalization() \n\n\nThe bounding box output definitions get a little messier than before, as each output has to take into account its relative anchor box coordinates.\n\n\ncreate_bbox_output <- function(prev_layer, anchor_start, anchor_stop, suffix) {\n  output <- layer_conv_2d(\n    prev_layer,\n    filters = 4 * k,\n    kernel_size = 3,\n    padding = \"same\",\n    name = paste0(\"bbox_conv_\", suffix)\n  ) %>%\n  layer_reshape(target_shape = c(-1, 4), name = paste0(\"bbox_flatten_\", suffix)) %>%\n  layer_activation(\"tanh\") %>%\n  layer_lambda(\n    f = function(x) {\n      activation_centers <-\n        (x[, , 1:2] / 2 * matrix(grid_sizes[anchor_start:anchor_stop], ncol = 1)) +\n        k_constant(anchors[anchor_start:anchor_stop, 1:2])\n      activation_height_width <-\n        (x[, , 3:4] / 2 + 1) * k_constant(anchors[anchor_start:anchor_stop, 3:4])\n      activation_corners <-\n        k_concatenate(\n          list(\n            activation_centers - activation_height_width / 2,\n            activation_centers + activation_height_width / 2\n          )\n        )\n     activation_corners\n    },\n    name = paste0(\"bbox_output_\", suffix)\n  )\n  output\n}\n\n\nHere they are: Each one attached to it’s respective stage of action in the pipeline.\n\n\nbbox_output_4x4 <- create_bbox_output(downscale_4x4, 1, 144, \"4x4\")\n\n\n\n\nbbox_output_2x2 <- create_bbox_output(downscale_2x2, 145, 180, \"2x2\")\n\n\n\n\nbbox_output_1x1 <- create_bbox_output(downscale_1x1, 181, 189, \"1x1\")\n\n\nThe same principle applies to the class outputs.\n\n\ncreate_class_output <- function(prev_layer, suffix) {\n  output <-\n  layer_conv_2d(\n    prev_layer,\n    filters = 21 * k,\n    kernel_size = 3,\n    padding = \"same\",\n    name = paste0(\"class_conv_\", suffix)\n  ) %>%\n  layer_reshape(target_shape = c(-1, 21), name = paste0(\"class_output_\", suffix))\n  output\n}\n\n\n\n\nclass_output_4x4 <- create_class_output(downscale_4x4, \"4x4\")\n\n\n\n\nclass_output_2x2 <- create_class_output(downscale_2x2, \"2x2\")\n\n\n\n\nclass_output_1x1 <- create_class_output(downscale_1x1, \"1x1\")\n\n\nAnd glue it all together, to get the model.\n\n\nmodel <- keras_model(\n  inputs = input,\n  outputs = list(\n    bbox_output_1x1,\n    bbox_output_2x2,\n    bbox_output_4x4,\n    class_output_1x1, \n    class_output_2x2, \n    class_output_4x4)\n)\n\n\nNow, we will stop here. To run this, there is another element that has to be adjusted: the data generator.\nOur focus being on explaining the concepts though, we’ll leave that to the interested reader.\nConclusion\nWhile we haven’t ended up with a good-performing model for object detection, we do hope that we’ve managed to shed some light on the mystery of object detection. What’s a bounding box? What’s an anchor (resp. prior, rep. default) box? How do you match them up in practice?\nIf you’ve “just” read the papers (YOLO, SSD), but never seen any code, it may seem like all actions happen in some wonderland beyond the horizon. They don’t. But coding them, as we’ve seen, can be cumbersome, even in the very basic versions we’ve implemented. To perform object detection in production, then, a lot more time has to be spent on training and tuning models. But sometimes just learning about how something works can be very satisfying.\nFinally, we’d again like to stress how much this post leans on what the fast.ai guys did. Their work most definitely is enriching not just the PyTorch, but also the R-TensorFlow community!\n\n\n\nGirshick, Ross B. 2015. “Fast r-CNN.” CoRR abs/1504.08083. http://arxiv.org/abs/1504.08083.\n\n\nGirshick, Ross B., Jeff Donahue, Trevor Darrell, and Jitendra Malik. 2013. “Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.” CoRR abs/1311.2524. http://arxiv.org/abs/1311.2524.\n\n\nLin, Tsung-Yi, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Dollar. 2017. “Focal Loss for Dense Object Detection.” CoRR abs/1708.02002. http://arxiv.org/abs/1708.02002.\n\n\nLiu, Wei, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott E. Reed, Cheng-Yang Fu, and Alexander C. Berg. 2015. “SSD: Single Shot MultiBox Detector.” CoRR abs/1512.02325. http://arxiv.org/abs/1512.02325.\n\n\nRedmon, Joseph, Santosh Kumar Divvala, Ross B. Girshick, and Ali Farhadi. 2015. “You Only Look Once: Unified, Real-Time Object Detection.” CoRR abs/1506.02640. http://arxiv.org/abs/1506.02640.\n\n\nRedmon, Joseph, and Ali Farhadi. 2016. “Yolo9000: Better, Faster, Stronger.” CoRR abs/1612.08242. http://arxiv.org/abs/1612.08242.\n\n\n———. 2018. “YOLOv3: An Incremental Improvement.” CoRR abs/1804.02767. http://arxiv.org/abs/1804.02767.\n\n\nRen, Shaoqing, Kaiming He, Ross B. Girshick, and Jian Sun. 2015. “Faster r-CNN: Towards Real-Time Object Detection with Region Proposal Networks.” CoRR abs/1506.01497. http://arxiv.org/abs/1506.01497.\n\n\nSermanet, Pierre, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, and Yann LeCun. 2013. “OverFeat: Integrated Recognition, Localization and Detection Using Convolutional Networks.” CoRR abs/1312.6229. http://arxiv.org/abs/1312.6229.\n\n\nWe won’t need imageinfo_maxbb in this post.↩︎\nas well as \\(\\alpha\\), not used in the figure↩︎\n",
    "preview": "posts/2018-12-18-object-detection-concepts/images/results.jpg",
    "last_modified": "2024-11-21T15:49:17+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-26-embeddings-fun-and-profit/",
    "title": "Entity embeddings for fun and profit",
    "description": "Embedding layers are not just useful when working with language data. As \"entity embeddings\", they've recently become famous for applications on tabular, small-scale data. In this post, we exemplify two possible use cases, also drawing attention to what not to expect.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-11-26",
    "categories": [
      "TensorFlow/Keras",
      "Tabular Data"
    ],
    "contents": "\nWhat’s useful about embeddings? Depending on who you ask, answers may vary. For many, the most immediate association may be word vectors and their use in natural language processing (translation, summarization, question answering etc.) There, they are famous for modeling semantic and syntactic relationships, as exemplified by this diagram found in one of the most influential papers on word vectors(Mikolov et al. 2013):\nCountries and their capital cities. Figure from (Mikolov et al. 2013)Others will probably bring up entity embeddings, the magic tool that helped win the Rossmann competition(Guo and Berkhahn 2016) and was greatly popularized by fast.ai’s deep learning course. Here, the idea is to make use of data that is not normally helpful in prediction, like high-dimensional categorical variables.\nAnother (related) idea, also widely spread by fast.ai and explained in this blog, is to apply embeddings to collaborative filtering. This basically builds up entity embeddings of users and items based on the criterion how well these “match” (as indicated by existing ratings).\nSo what are embeddings good for? The way we see it, embeddings are what you make of them. The goal in this post is to provide examples of how to use embeddings to uncover relationships and improve prediction. The examples are just that - examples, chosen to demonstrate a method. The most interesting thing really will be what you make of these methods in your area of work or interest.\nEmbeddings for fun (picturing relationships)\nOur first example will stress the “fun” part, but also show how to technically deal with categorical variables in a dataset.\nWe’ll take this year’s StackOverflow developer survey as a basis and pick a few categorical variables that seem interesting - stuff like “what do people value in a job” and of course, what languages and OSes do people use. Don’t take this too seriously, it’s meant to be fun and demonstrate a method, that’s all.1\nPreparing the data\nEquipped with the libraries we’ll need:\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(keras)\nlibrary(purrr)\nlibrary(forcats)\nlibrary(ggrepel)\n\n\nWe load the data and zoom in on a few categorical variables. Two of them we intend to use as targets: EthicsChoice and JobSatisfaction. EthicsChoice is one of four ethics-related questions and goes\n\n“Imagine that you were asked to write code for a purpose or product that you consider extremely unethical. Do you write the code anyway?”\n\nWith questions like this, it’s never clear what portion of a response should be attributed to social desirability - this question seemed like the least prone to that, which is why we chose it.2\n\n\ndata <- read_csv(\"survey_results_public.csv\")\n\ndata <- data %>% select(\n  FormalEducation,\n  UndergradMajor,\n  starts_with(\"AssessJob\"),\n  EthicsChoice,\n  LanguageWorkedWith,\n  OperatingSystem,\n  EthicsChoice,\n  JobSatisfaction\n)\n\ndata <- data %>% mutate_if(is.character, factor)\n\n\nThe variables we are interested in show a tendency to have been left unanswered by quite a few respondents, so the easiest way to handle missing data here is to exclude the respective participants completely.\n\n\ndata <- na.omit(data)\n\n\nThat leaves us with ~48,000 completed (as far as we’re concerned) questionnaires.\nLooking at the variables’ contents, we see we’ll have to do something with them before we can start training.\n\n\ndata %>% glimpse()\n\n\nObservations: 48,610\nVariables: 16\n$ FormalEducation    <fct> Bachelor’s degree (BA, BS, B.Eng., etc.),...\n$ UndergradMajor     <fct> Mathematics or statistics, A natural scie...\n$ AssessJob1         <int> 10, 1, 8, 8, 5, 6, 6, 6, 9, 7, 3, 1, 6, 7...\n$ AssessJob2         <int> 7, 7, 5, 5, 3, 5, 3, 9, 4, 4, 9, 7, 7, 10...\n$ AssessJob3         <int> 8, 10, 7, 4, 9, 4, 7, 2, 10, 10, 10, 6, 1...\n$ AssessJob4         <int> 1, 8, 1, 9, 4, 2, 4, 4, 3, 2, 6, 10, 4, 1...\n$ AssessJob5         <int> 2, 2, 2, 1, 1, 7, 1, 3, 1, 1, 8, 9, 2, 4,...\n$ AssessJob6         <int> 5, 5, 6, 3, 8, 8, 5, 5, 6, 5, 7, 4, 5, 5,...\n$ AssessJob7         <int> 3, 4, 4, 6, 2, 10, 10, 8, 5, 3, 1, 2, 3, ...\n$ AssessJob8         <int> 4, 3, 3, 2, 7, 1, 8, 7, 2, 6, 2, 3, 1, 3,...\n$ AssessJob9         <int> 9, 6, 10, 10, 10, 9, 9, 10, 7, 9, 4, 8, 9...\n$ AssessJob10        <int> 6, 9, 9, 7, 6, 3, 2, 1, 8, 8, 5, 5, 8, 9,...\n$ EthicsChoice       <fct> No, Depends on what it is, No, Depends on...\n$ LanguageWorkedWith <fct> JavaScript;Python;HTML;CSS, JavaScript;Py...\n$ OperatingSystem    <fct> Linux-based, Linux-based, Windows, Linux-...\n$ JobSatisfaction    <fct> Extremely satisfied, Moderately dissatisf...\n\nTarget variables\nWe want to binarize both target variables. Let’s inspect them, starting with EthicsChoice.\n\n\njslevels <- levels(data$JobSatisfaction)\nelevels <- levels(data$EthicsChoice)\n\ndata <- data %>% mutate(\n  JobSatisfaction = JobSatisfaction %>% fct_relevel(\n    jslevels[1],\n    jslevels[3],\n    jslevels[6],\n    jslevels[5],\n    jslevels[7],\n    jslevels[4],\n    jslevels[2]\n  ),\n  EthicsChoice = EthicsChoice %>% fct_relevel(\n    elevels[2],\n    elevels[1],\n    elevels[3]\n  ) \n)\n\nggplot(data, aes(EthicsChoice)) + geom_bar()\n\n\nDistribution of answers to: “Imagine that you were asked to write code for a purpose or product that you consider extremely unethical. Do you write the code anyway?”You might agree that with a question containing the phrase a purpose or product that you consider extremely unethical, the answer “depends on what it is” feels closer to “yes” than to “no.” If that seems like too skeptical a thought, it’s still the only binarization that achieves a sensible split.\n\n\ndata <- data %>% mutate(\n  EthicsChoice = if_else(as.numeric(EthicsChoice) == 2, 1, 0)\n  )\n\n\nLooking at our second target variable, JobSatisfaction:\nDistribution of answers to: ““How satisfied are you with your current job? If you work more than one job, please answer regarding the one you spend the most hours on.”We think that given the mode at “moderately satisfied,” a sensible way to binarize is a split into “moderately satisfied” and “extremely satisfied” on one side, all remaining options on the other:\n\n\ndata <- data %>% mutate(\n  JobSatisfaction = if_else(as.numeric(JobSatisfaction) > 5, 1, 0)\n  )\n\n\nPredictors\nAmong the predictors, FormalEducation, UndergradMajor and OperatingSystem look pretty harmless - we already turned them into factors so it should be straightforward to one-hot-encode them. For curiosity’s sake, let’s look at how they’re distributed:\n\n\ndata %>% group_by(FormalEducation) %>%\n  summarise(count = n()) %>%\n  arrange(desc(count))\n\n\n  FormalEducation                                        count\n  <fct>                                                  <int>\n1 Bachelor’s degree (BA, BS, B.Eng., etc.)               25558\n2 Master’s degree (MA, MS, M.Eng., MBA, etc.)            12865\n3 Some college/university study without earning a degree  6474\n4 Associate degree                                        1595\n5 Other doctoral degree (Ph.D, Ed.D., etc.)               1395\n6 Professional degree (JD, MD, etc.)                       723\n\n\ndata %>% group_by(UndergradMajor) %>%\n  summarise(count = n()) %>%\n  arrange(desc(count))\n\n\n  UndergradMajor                                                  count\n   <fct>                                                           <int>\n 1 Computer science, computer engineering, or software engineering 30931\n 2 Another engineering discipline (ex. civil, electrical, mechani…  4179\n 3 Information systems, information technology, or system adminis…  3953\n 4 A natural science (ex. biology, chemistry, physics)              2046\n 5 Mathematics or statistics                                        1853\n 6 Web development or web design                                    1171\n 7 A business discipline (ex. accounting, finance, marketing)       1166\n 8 A humanities discipline (ex. literature, history, philosophy)    1104\n 9 A social science (ex. anthropology, psychology, political scie…   888\n10 Fine arts or performing arts (ex. graphic design, music, studi…   791\n11 I never declared a major                                          398\n12 A health science (ex. nursing, pharmacy, radiology)               130\n\n\ndata %>% group_by(OperatingSystem) %>%\n  summarise(count = n()) %>%\n  arrange(desc(count))\n\n\n  OperatingSystem count\n  <fct>           <int>\n1 Windows         23470\n2 MacOS           14216\n3 Linux-based     10837\n4 BSD/Unix           87\nLanguageWorkedWith, on the other hand, contains sequences of programming languages, concatenated by semicolon.\nOne way to unpack these is using Keras’ text_tokenizer.\n\n\nlanguage_tokenizer <- text_tokenizer(split = \";\", filters = \"\")\nlanguage_tokenizer %>% fit_text_tokenizer(data$LanguageWorkedWith)\n\n\nWe have 38 languages overall. Actual usage counts aren’t too surprising:\n\n\ndata.frame(\n  name = language_tokenizer$word_counts %>% names(),\n  count = language_tokenizer$word_counts %>% unlist() %>% unname()\n) %>%\n arrange(desc(count))\n\n\n                   name count\n1            javascript 35224\n2                  html 33287\n3                   css 31744\n4                   sql 29217\n5                  java 21503\n6            bash/shell 20997\n7                python 18623\n8                    c# 17604\n9                   php 13843\n10                  c++ 10846\n11           typescript  9551\n12                    c  9297\n13                 ruby  5352\n14                swift  4014\n15                   go  3784\n16          objective-c  3651\n17               vb.net  3217\n18                    r  3049\n19             assembly  2699\n20               groovy  2541\n21                scala  2475\n22               matlab  2465\n23               kotlin  2305\n24                  vba  2298\n25                 perl  2164\n26       visual basic 6  1729\n27         coffeescript  1711\n28                  lua  1556\n29 delphi/object pascal  1174\n30                 rust  1132\n31              haskell  1058\n32                   f#   764\n33              clojure   696\n34               erlang   560\n35                cobol   317\n36                ocaml   216\n37                julia   215\n38                 hack    94\nNow language_tokenizer will nicely create a one-hot representation of the multiple-choice column.\n\n\nlangs <- language_tokenizer %>%\n  texts_to_matrix(data$LanguageWorkedWith, mode = \"count\")\nlangs[1:3, ]\n\n\n> langs[1:3, ]\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n[1,]    0    1    1    1    0    0    0    1    0     0     0     0     0     0     0     0     0     0     0     0     0\n[2,]    0    1    0    0    0    0    1    1    0     0     0     0     0     0     0     0     0     0     0     0     0\n[3,]    0    0    0    0    1    1    1    0    0     0     1     0     1     0     0     0     0     0     1     0     0\n     [,22] [,23] [,24] [,25] [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38] [,39]\n[1,]     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n[2,]     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n[3,]     0     1     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\nWe can simply append these columns to the dataframe (and do a little cleanup):\n\n\ndata <- data %>% cbind(langs[, 2:39]) # the very first column is not useful\ndata <- data %>% rename_at(vars(`1`:`38`), funs(paste0(language_tokenizer$index_word[as.integer(.)])))\ndata <- data %>% select(-LanguageWorkedWith)\n\n\nWe still have the AssessJob[n] columns to deal with. Here, StackOverflow had people rank what’s important to them about a job. These are the features that were to be ranked:\n\nThe industry that I’d be working in\n\n\nThe financial performance or funding status of the company or organization\n\n\nThe specific department or team I’d be working on\n\n\nThe languages, frameworks, and other technologies I’d be working with\n\n\nThe compensation and benefits offered\n\n\nThe office environment or company culture\n\n\nThe opportunity to work from home/remotely\n\n\nOpportunities for professional development\n\n\nThe diversity of the company or organization\n\n\nHow widely used or impactful the product or service I’d be working on is\n\nColumns AssessJob1 to AssessJob10 contain the respective ranks, that is, values between 1 and 10.\nBased on introspection about the cognitive effort to actually establish an order among 10 items, we decided to pull out the three top-ranked features per person and treat them as equal. Technically, a first step extracts and concatenate these, yielding an intermediary result of e.g.\n$ job_vals<fct> languages_frameworks;compensation;remote, industry;compensation;development, languages_frameworks;compensation;development\n\n\ndata <- data %>% mutate(\n  val_1 = if_else(\n   AssessJob1 == 1, \"industry\", if_else(\n    AssessJob2 == 1, \"company_financial_status\", if_else(\n      AssessJob3 == 1, \"department\", if_else(\n        AssessJob4 == 1, \"languages_frameworks\", if_else(\n          AssessJob5 == 1, \"compensation\", if_else(\n            AssessJob6 == 1, \"company_culture\", if_else(\n              AssessJob7 == 1, \"remote\", if_else(\n                AssessJob8 == 1, \"development\", if_else(\n                  AssessJob10 == 1, \"diversity\", \"impact\"))))))))),\n  val_2 = if_else(\n    AssessJob1 == 2, \"industry\", if_else(\n      AssessJob2 == 2, \"company_financial_status\", if_else(\n        AssessJob3 == 2, \"department\", if_else(\n          AssessJob4 == 2, \"languages_frameworks\", if_else(\n            AssessJob5 == 2, \"compensation\", if_else(\n              AssessJob6 == 2, \"company_culture\", if_else(\n                AssessJob7 == 1, \"remote\", if_else(\n                  AssessJob8 == 1, \"development\", if_else(\n                    AssessJob10 == 1, \"diversity\", \"impact\"))))))))),\n  val_3 = if_else(\n    AssessJob1 == 3, \"industry\", if_else(\n      AssessJob2 == 3, \"company_financial_status\", if_else(\n        AssessJob3 == 3, \"department\", if_else(\n          AssessJob4 == 3, \"languages_frameworks\", if_else(\n            AssessJob5 == 3, \"compensation\", if_else(\n              AssessJob6 == 3, \"company_culture\", if_else(\n                AssessJob7 == 3, \"remote\", if_else(\n                  AssessJob8 == 3, \"development\", if_else(\n                    AssessJob10 == 3, \"diversity\", \"impact\")))))))))\n  )\n\ndata <- data %>% mutate(\n  job_vals = paste(val_1, val_2, val_3, sep = \";\") %>% factor()\n)\n\ndata <- data %>% select(\n  -c(starts_with(\"AssessJob\"), starts_with(\"val_\"))\n)\n\n\nNow that column looks exactly like LanguageWorkedWith looked before, so we can use the same method as above to produce a one-hot-encoded version.\n\n\nvalues_tokenizer <- text_tokenizer(split = \";\", filters = \"\")\nvalues_tokenizer %>% fit_text_tokenizer(data$job_vals)\n\n\nSo what actually do respondents value most?\n                      name count\n1              compensation 27020\n2      languages_frameworks 24216\n3           company_culture 20432\n4               development 15981\n5                    impact 14869\n6                department 10452\n7                    remote 10396\n8                  industry  8294\n9                 diversity  7594\n10 company_financial_status  6576\nUsing the same method as above\n\n\njob_values <- values_tokenizer %>% texts_to_matrix(data$job_vals, mode = \"count\")\ndata <- data %>% cbind(job_values[, 2:11])\ndata <- data %>% rename_at(vars(`1`:`10`), funs(paste0(values_tokenizer$index_word[as.integer(.)])))\ndata <- data %>% select(-job_vals)\ndata %>% glimpse()\n\n\nwe end up with a dataset that looks like this\n> data %>% glimpse()\nObservations: 48,610\nVariables: 53\n$ FormalEducation          <fct> Bachelor’s degree (BA, BS, B.Eng., etc.), Bach...\n$ UndergradMajor           <fct> Mathematics or statistics, A natural science (...\n$ OperatingSystem          <fct> Linux-based, Linux-based, Windows, Linux-based...\n$ JS                       <dbl> 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0...\n$ EC                       <dbl> 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0...\n$ javascript               <dbl> 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1...\n$ html                     <dbl> 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1...\n$ css                      <dbl> 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1...\n$ sql                      <dbl> 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1...\n$ java                     <dbl> 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1...\n$ `bash/shell`             <dbl> 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1...\n$ python                   <dbl> 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0...\n$ `c#`                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0...\n$ php                      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1...\n$ `c++`                    <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0...\n$ typescript               <dbl> 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1...\n$ c                        <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0...\n$ ruby                     <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ swift                    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1...\n$ go                       <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0...\n$ `objective-c`            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ vb.net                   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ r                        <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ assembly                 <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ groovy                   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ scala                    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ matlab                   <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ kotlin                   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ vba                      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ perl                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ `visual basic 6`         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ coffeescript             <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ lua                      <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ `delphi/object pascal`   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ rust                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ haskell                  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ `f#`                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ clojure                  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ erlang                   <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ cobol                    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ ocaml                    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ julia                    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ hack                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ compensation             <dbl> 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0...\n$ languages_frameworks     <dbl> 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0...\n$ company_culture          <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n$ development              <dbl> 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0...\n$ impact                   <dbl> 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1...\n$ department               <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0...\n$ remote                   <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 1, 0...\n$ industry                 <dbl> 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1...\n$ diversity                <dbl> 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0...\n$ company_financial_status <dbl> 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1...\nwhich we further reduce to a design matrix X removing the binarized target variables\n\n\nX <- data %>% select(-c(JobSatisfaction, EthicsChoice))\n\n\nFrom here on, different actions will ensue depending on whether we choose the road of working with a one-hot model or an embeddings model of the predictors.\nThere is one other thing though to be done before: We want to work with the same train-test split in both cases.\n\n\ntrain_indices <- sample(1:nrow(X), 0.8 * nrow(X))\n\n\nOne-hot model\nGiven this is a post about embeddings, why show a one-hot model? First, for instructional reasons - you don’t see many of examples of deep learning on categorical data in the wild. Second, … but we’ll turn to that after having shown both models.\nFor the one-hot model, all that remains to be done is using Keras’ to_categorical on the three remaining variables that are not yet in one-hot form.\n\n\nX_one_hot <- X %>% map_if(is.factor, ~ as.integer(.x) - 1) %>%\n  map_at(\"FormalEducation\", ~ to_categorical(.x) %>% \n           array_reshape(c(length(.x), length(levels(data$FormalEducation))))) %>%\n  map_at(\"UndergradMajor\", ~ to_categorical(.x) %>% \n           array_reshape(c(length(.x), length(levels(data$UndergradMajor))))) %>%\n  map_at(\"OperatingSystem\", ~ to_categorical(.x) %>%\n           array_reshape(c(length(.x), length(levels(data$OperatingSystem))))) %>%\n  abind::abind(along = 2)\n\n\nWe divide up our dataset into train and validation parts\n\n\nx_train <- X_one_hot[train_indices, ] %>% as.matrix()\nx_valid <- X_one_hot[-train_indices, ] %>% as.matrix()\ny_train <- data$EthicsChoice[train_indices] %>% as.matrix()\ny_valid <- data$EthicsChoice[-train_indices] %>% as.matrix()\n\n\nand define a pretty straightforward MLP.\n\n\nmodel <- keras_model_sequential() %>%\n  layer_dense(\n    units = 128,\n    activation = \"selu\"\n  ) %>%\n  layer_dropout(0.5) %>%\n  layer_dense(\n    units = 128,\n    activation = \"selu\"\n  ) %>%\n  layer_dropout(0.5) %>%\n  layer_dense(\n    units = 128,\n    activation = \"selu\"\n  ) %>%\n  layer_dropout(0.5) %>%\n  layer_dense(\n    units = 128,\n    activation = \"selu\"\n  ) %>%\n  layer_dropout(0.5) %>%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %>% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = \"adam\",\n  metrics = \"accuracy\"\n  )\n\n\nTraining this model:\n\n\nhistory <- model %>% fit(\n  x_train,\n  y_train,\n  validation_data = list(x_valid, y_valid),\n  epochs = 20,\n  batch_size = 100\n)\n\nplot(history)\n\n\n…results in an accuracy on the validation set of 0.64 - not an impressive number per se, but interesting given the small amount of predictors and the choice of target variable.3\n\nEmbeddings model\nIn the embeddings model, we don’t need to use to_categorical on the remaining factors, as embedding layers can work with integer input data. We thus just convert the factors to integers:\n\n\nX_embed <- X %>%\n  mutate_if(is.factor, compose(partial(`-`, 1, .first = FALSE), as.integer))\n\n\nNow for the model. Effectively we have five groups of entities here: formal education, undergrad major, operating system, languages worked with, and highest-counting values with respect to jobs. Each of these groups get embedded separately, so we need to use the Keras functional API and declare five different inputs.\n\n\ninput_fe <- layer_input(shape = 1)        # formal education, encoded as integer\ninput_um <- layer_input(shape = 1)        # undergrad major, encoded as integer\ninput_os <- layer_input(shape = 1)        # operating system, encoded as integer\ninput_langs <- layer_input(shape = 38)    # languages worked with, multi-hot-encoded\ninput_vals <- layer_input(shape = 10)     # values, multi-hot-encoded\n\n\nHaving embedded them separately, we concatenate the outputs for further common processing.\n\n\nconcat <- layer_concatenate(\n  list(\n    input_fe %>%\n      layer_embedding(\n        input_dim = length(levels(data$FormalEducation)),\n        output_dim = 64,\n        name = \"fe\"\n      ) %>%\n      layer_flatten(),\n    input_um %>%\n      layer_embedding(\n        input_dim = length(levels(data$UndergradMajor)),\n        output_dim = 64,\n        name = \"um\"\n      ) %>%\n      layer_flatten(),\n    input_os %>%\n      layer_embedding(\n        input_dim = length(levels(data$OperatingSystem)),\n        output_dim = 64,\n        name = \"os\"\n      ) %>%\n      layer_flatten(),\n    input_langs %>%\n       layer_embedding(input_dim = 38, output_dim = 256,\n                       name = \"langs\")%>%\n       layer_flatten(),\n    input_vals %>%\n      layer_embedding(input_dim = 10, output_dim = 128,\n                      name = \"vals\")%>%\n      layer_flatten()\n  )\n)\n\noutput <- concat %>%\n  layer_dense(\n    units = 128,\n    activation = \"relu\"\n  ) %>%\n  layer_dropout(0.5) %>%\n  layer_dense(\n    units = 128,\n    activation = \"relu\"\n  ) %>%\n  layer_dropout(0.5) %>%\n  layer_dense(\n    units = 128,\n    activation = \"relu\"\n  ) %>%\n  layer_dense(\n    units = 128,\n    activation = \"relu\"\n  ) %>%\n  layer_dropout(0.5) %>%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\n\nSo there go model definition and compilation:\n\n\nmodel <- keras_model(list(input_fe, input_um, input_os, input_langs, input_vals), output)\n\nmodel %>% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = \"adam\",\n  metrics = \"accuracy\"\n  )\n\n\nNow to pass the data to the model, we need to chop it up into ranges of columns matching the inputs.\n\n\ny_train <- data$EthicsChoice[train_indices] %>% as.matrix()\ny_valid <- data$EthicsChoice[-train_indices] %>% as.matrix()\n\nx_train <-\n  list(\n    X_embed[train_indices, 1, drop = FALSE] %>% as.matrix() ,\n    X_embed[train_indices , 2, drop = FALSE] %>% as.matrix(),\n    X_embed[train_indices , 3, drop = FALSE] %>% as.matrix(),\n    X_embed[train_indices , 4:41, drop = FALSE] %>% as.matrix(),\n    X_embed[train_indices , 42:51, drop = FALSE] %>% as.matrix()\n  )\nx_valid <- list(\n  X_embed[-train_indices, 1, drop = FALSE] %>% as.matrix() ,\n  X_embed[-train_indices , 2, drop = FALSE] %>% as.matrix(),\n  X_embed[-train_indices , 3, drop = FALSE] %>% as.matrix(),\n  X_embed[-train_indices , 4:41, drop = FALSE] %>% as.matrix(),\n  X_embed[-train_indices , 42:51, drop = FALSE] %>% as.matrix()\n)\n\n\nAnd we’re ready to train.\n\n\nmodel %>% fit(\n  x_train,\n  y_train,\n  validation_data = list(x_valid, y_valid),\n  epochs = 20,\n  batch_size = 100\n)\n\n\nUsing the same train-test split as before, this results in an accuracy of … ~0.64 (just as before). Now we said from the start that using embeddings could serve different purposes, and that in this first use case, we wanted to demonstrate their use for extracting latent relationships. And in any case you could argue that the task is too hard - probably there just is not much of a relationship between the predictors we chose and the target.\nBut this also warrants a more general comment. With all current enthusiasm about using embeddings on tabular data, we are not aware of any systematic comparisons with one-hot-encoded data as regards the actual effect on performance, nor do we know of systematic analyses under what circumstances embeddings will probably be of help. Our working hypothesis is that in the setup we chose, the dimensionality of the original data is so low that the information can simply be encoded “as is” by the network - as long as we create it with sufficient capacity. Our second use case will therefore use data where - hopefully - this won’t be the case.\nBut before, let’s get to the main purpose of this use case: How can we extract those latent relationships from the network?\nExtracting relationships from the learned embeddings\nWe’ll show the code here for the job values embeddings, - it is directly transferable to the other ones.\nThe embeddings, that’s just the weight matrix of the respective layer, of dimension number of different values times embedding size.\n\n\nemb_vals <- (model$get_layer(\"vals\") %>% get_weights())[[1]]\nemb_vals %>% dim() # 10x128\n\n\nWe can then perform dimensionality reduction on the raw values, e.g., PCA\n\n\npca <- prcomp(emb_vals, center = TRUE, scale. = TRUE, rank = 2)$x[, c(\"PC1\", \"PC2\")]\n\n\nand plot the results.\n\n\npca %>%\n  as.data.frame() %>%\n  mutate(class = attr(values_tokenizer$word_index, \"names\")) %>%\n  ggplot(aes(x = PC1, y = PC2)) +\n  geom_point() +\n  geom_label_repel(aes(label = class))\n\n\nThis is what we get (displaying four of the five variables we used embeddings on):\nTwo first principal components of the embeddings for undergrad major (top left), operating system (top right), programming language used (bottom left), and primary values with respect to jobs (bottom right)Now we’ll definitely refrain from taking this too seriously, given the modest accuracy on the prediction task that lead to these embedding matrices.4\nCertainly when assessing the obtained factorization, performance on the main task has to be taken into account.\nBut we’d like to point out something else too: In contrast to unsupervised and semi-supervised techniques like PCA or autoencoders, we made use of an extraneous variable (the ethical behavior to be predicted). So any learned relationships are never “absolute,” but always to be seen in relation to the way they were learned. This is why we chose an additional target variable, JobSatisfaction, so we could compare the embeddings learned on two different tasks. We won’t refer the concrete results here as accuracy turned out to be even lower than with EthicsChoice. We do, however, want to stress this inherent difference to representations learned by, e.g., autoencoders.\nNow let’s address the second use case.\nEmbedding for profit (improving accuracy)\nOur second task here is about fraud detection. The dataset is contained in the DMwR2 package and is called sales:\n\n\ndata(sales, package = \"DMwR2\")\nsales\n\n\n# A tibble: 401,146 x 5\n   ID    Prod  Quant   Val Insp \n   <fct> <fct> <int> <dbl> <fct>\n 1 v1    p1      182  1665 unkn \n 2 v2    p1     3072  8780 unkn \n 3 v3    p1    20393 76990 unkn \n 4 v4    p1      112  1100 unkn \n 5 v3    p1     6164 20260 unkn \n 6 v5    p2      104  1155 unkn \n 7 v6    p2      350  5680 unkn \n 8 v7    p2      200  4010 unkn \n 9 v8    p2      233  2855 unkn \n10 v9    p2      118  1175 unkn \n# ... with 401,136 more rows\nEach row indicates a transaction reported by a salesperson, - ID being the salesperson ID, Prod a product ID, Quant the quantity sold, Val the amount of money it was sold for, and Insp indicating one of three possibilities: (1) the transaction was examined and found fraudulent, (2) it was examined and found okay, and (3) it has not been examined (the vast majority of cases).\nWhile this dataset “cries” for semi-supervised techniques (to make use of the overwhelming amount of unlabeled data), we want to see if using embeddings can help us improve accuracy on a supervised task.\nWe thus recklessly throw away incomplete data as well as all unlabeled entries\n\n\nsales <- filter(sales, !(is.na(Quant)))\nsales <- filter(sales, !(is.na(Val)))\n\nsales <- droplevels(sales %>% filter(Insp != \"unkn\"))\nnrow(sales)\n\n\nwhich leaves us with 15546 transactions.\nOne-hot model\nNow we prepare the data for the one-hot model we want to compare against:\nWith 2821 levels, salesperson ID is far too high-dimensional to work well with one-hot encoding, so we completely drop that column.\nProduct id (Prod) has “just” 797 levels, but with one-hot-encoding, that still results in significant memory demand. We thus zoom in on the 500 top-sellers.\nThe continuous variables Quant and Val are normalized to values between 0 and 1 so they fit with the one-hot-encoded Prod.\n\n\nsales_1hot <- sales\n\nnormalize <- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n\ntop_n <- 500\ntop_prods <- sales_1hot %>% \n  group_by(Prod) %>% \n  summarise(cnt = n()) %>% \n  arrange(desc(cnt)) %>%\n  head(top_n) %>%\n  select(Prod) %>%\n  pull()\nsales_1hot <- droplevels(sales_1hot %>% filter(Prod %in% top_prods))\n\nsales_1hot <- sales_1hot %>%\n  select(-ID) %>%\n  map_if(is.factor, ~ as.integer(.x) - 1) %>%\n  map_at(\"Prod\", ~ to_categorical(.x) %>% array_reshape(c(length(.x), top_n))) %>%\n  map_at(\"Quant\", ~ normalize(.x) %>% array_reshape(c(length(.x), 1))) %>%\n  map_at(\"Val\", ~ normalize(.x) %>% array_reshape(c(length(.x), 1))) %>%\n  abind(along = 2)\n\n\nWe then perform the usual train-test split.\n\n\ntrain_indices <- sample(1:nrow(sales_1hot), 0.7 * nrow(sales_1hot))\n\nX_train <- sales_1hot[train_indices, 1:502] \ny_train <-  sales_1hot[train_indices, 503] %>% as.matrix()\n\nX_valid <- sales_1hot[-train_indices, 1:502] \ny_valid <-  sales_1hot[-train_indices, 503] %>% as.matrix()\n\n\nFor classification on this dataset, which will be the baseline to beat?\n\n\nxtab_train  <- y_train %>% table()\nxtab_valid  <- y_valid %>% table()\nlist(xtab_train[1]/(xtab_train[1] + xtab_train[2]), xtab_valid[1]/(xtab_valid[1] + xtab_valid[2]))\n\n\n[[1]]\n        0 \n0.9393547 \n\n[[2]]\n        0 \n0.9384437 \nSo if we don’t get beyond 94% accuracy on both training and validation sets, we may just as well predict “okay” for every transaction.\nHere then is the model, plus the training routine and evaluation:\n\n\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 256, activation = \"selu\") %>%\n  layer_dropout(dropout_rate) %>% \n  layer_dense(units = 256, activation = \"selu\") %>%\n  layer_dropout(dropout_rate) %>% \n  layer_dense(units = 256, activation = \"selu\") %>%\n  layer_dropout(dropout_rate) %>% \n  layer_dense(units = 256, activation = \"selu\") %>%\n  layer_dropout(dropout_rate) %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %>% compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = c(\"accuracy\"))\n\nmodel %>% fit(\n  X_train,\n  y_train,\n  validation_data = list(X_valid, y_valid),\n  class_weights = list(\"0\" = 0.1, \"1\" = 0.9),\n  batch_size = 128,\n  epochs = 200\n)\n\nmodel %>% evaluate(X_train, y_train, batch_size = 100) \nmodel %>% evaluate(X_valid, y_valid, batch_size = 100) \n\n\nThis model achieved optimal validation accuracy at a dropout rate of 0.2. At that rate, training accuracy was 0.9761, and validation accuracy was 0.9507. At all dropout rates lower than 0.7, validation accuracy did indeed surpass the majority vote baseline.\nCan we further improve performance by embedding the product id?\nEmbeddings model\nFor better comparability, we again discard salesperson information and cap the number of different products at 500.\nOtherwise, data preparation goes as expected for this model:\n\n\nsales_embed <- sales\n\ntop_prods <- sales_embed %>% \n  group_by(Prod) %>% \n  summarise(cnt = n()) %>% \n  arrange(desc(cnt)) %>% \n  head(top_n) %>% \n  select(Prod) %>% \n  pull()\n\nsales_embed <- droplevels(sales_embed %>% filter(Prod %in% top_prods))\n\nsales_embed <- sales_embed %>%\n  select(-ID) %>%\n  mutate_if(is.factor, ~ as.integer(.x) - 1) %>%\n  mutate(Quant = scale(Quant)) %>%\n  mutate(Val = scale(Val))\n\nX_train <- sales_embed[train_indices, 1:3] %>% as.matrix()\ny_train <-  sales_embed[train_indices, 4] %>% as.matrix()\n\nX_valid <- sales_embed[-train_indices, 1:3] %>% as.matrix()\ny_valid <-  sales_embed[-train_indices, 4] %>% as.matrix()\n\n\nThe model we define is as similar as possible to the one-hot alternative:\n\n\nprod_input <- layer_input(shape = 1)\ncont_input <- layer_input(shape = 2)\n\nprod_embed <- prod_input %>% \n  layer_embedding(input_dim = sales_embed$Prod %>% max() + 1,\n                  output_dim = 256\n                  ) %>%\n  layer_flatten()\ncont_dense <- cont_input %>% layer_dense(units = 256, activation = \"selu\")\n\noutput <- layer_concatenate(\n  list(prod_embed, cont_dense)) %>%\n  layer_dropout(dropout_rate) %>% \n  layer_dense(units = 256, activation = \"selu\") %>%\n  layer_dropout(dropout_rate) %>% \n  layer_dense(units = 256, activation = \"selu\") %>%\n  layer_dropout(dropout_rate) %>% \n  layer_dense(units = 256, activation = \"selu\") %>%\n  layer_dropout(dropout_rate) %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n  \nmodel <- keras_model(inputs = list(prod_input, cont_input), outputs = output)\n\nmodel %>% compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = \"accuracy\")\n\nmodel %>% fit(\n  list(X_train[ , 1], X_train[ , 2:3]),\n  y_train,\n  validation_data = list(list(X_valid[ , 1], X_valid[ , 2:3]), y_valid),\n  class_weights = list(\"0\" = 0.1, \"1\" = 0.9),\n  batch_size = 128,\n  epochs = 200\n)\n\nmodel %>% evaluate(list(X_train[ , 1], X_train[ , 2:3]), y_train) \nmodel %>% evaluate(list(X_valid[ , 1], X_valid[ , 2:3]), y_valid)        \n\n\nThis time, accuracies are in fact higher: At the optimal dropout rate (0.3 in this case), training resp. validation accuracy are at 0.9913 and 0.9666, respectively. Quite a difference!\nSo why did we choose this dataset? In contrast to our previous dataset, here the categorical variable is high-dimensional, so well suited for compression and densification. It is interesting that we can make such good use of an ID without knowing what it stands for!\nConclusion\nIn this post, we’ve shown two use cases of embeddings in “simple” tabular data. As stated in the introduction, to us, embeddings are what you make of them. In that vein, if you’ve used embeddings to accomplish things that mattered to your task at hand, please comment and tell us about it!\n\n\n\nGuo, Cheng, and Felix Berkhahn. 2016. “Entity Embeddings of Categorical Variables.” CoRR abs/1604.06737. http://arxiv.org/abs/1604.06737.\n\n\nMikolov, Tomas, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Distributed Representations of Words and Phrases and Their Compositionality.” CoRR abs/1310.4546. http://arxiv.org/abs/1310.4546.\n\n\nWe did think it prudent though to omit variables like country, ethnicity or gender.↩︎\nat least given the way we binarized answers (more on that soon)↩︎\nAs usual when not working with one the “flagship areas” of deep learning, comparisons against other machine learning methods would be interesting. We did, however, not want to further elongate the post, nor distract from its main focus, namely, the use of embeddings with categorical data.↩︎\nNo, no, of course we’re not implying that for programming languages, the second principal component, with R and assembly at its extremes, stands for high-level vs. low-level language here.↩︎\n",
    "preview": "posts/2018-11-26-embeddings-fun-and-profit/images/thumb.png",
    "last_modified": "2024-11-21T15:53:57+00:00",
    "input_file": {},
    "preview_width": 820,
    "preview_height": 410
  },
  {
    "path": "posts/2018-11-12-uncertainty_estimates_dropout/",
    "title": "You sure? A Bayesian approach to obtaining uncertainty estimates from neural networks",
    "description": "In deep learning, there is no obvious way of obtaining uncertainty estimates. In 2016, Gal and Ghahramani proposed a method that is both theoretically grounded and practical: use dropout at test time. In this post, we introduce a refined version of this method (Gal et al. 2017) that has the network itself learn how uncertain it is.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-11-12",
    "categories": [
      "Image Recognition & Image Processing",
      "Probabilistic ML/DL",
      "TensorFlow/Keras"
    ],
    "contents": "\nIf there were a set of survival rules for data scientists, among them would have to be this: Always report uncertainty estimates with your predictions. However, here we are, working with neural networks, and unlike lm, a Keras model does not conveniently output something like a standard error for the weights.\nWe might try to think of rolling your own uncertainty measure - for example, averaging predictions from networks trained from different random weight initializations, for different numbers of epochs, or on different subsets of the data. But we might still be worried that our method is quite a bit, well … ad hoc.\nIn this post, we’ll see a both practical as well as theoretically grounded approach to obtaining uncertainty estimates from neural networks. First, however, let’s quickly talk about why uncertainty is that important - over and above its potential to save a data scientist’s job.\nWhy uncertainty?\nIn a society where automated algorithms are - and will be - entrusted with more and more life-critical tasks, one answer immediately jumps to mind: If the algorithm correctly quantifies its uncertainty, we may have human experts inspect the more uncertain predictions and potentially revise them.\nThis will only work if the network’s self-indicated uncertainty really is indicative of a higher probability of misclassification. Leibig et al.(Leibig et al. 2017) used a predecessor of the method described below to assess neural network uncertainty in detecting diabetic retinopathy. They found that indeed, the distributions of uncertainty were different depending on whether the answer was correct or not:\nFigure from Leibig et al. 2017 (Leibig et al. 2017). Green: uncertainty estimates for wrong predictions. Blue: uncertainty estimates for correct predictions.In addition to quantifying uncertainty, it can make sense to qualify it. In the Bayesian deep learning literature, a distinction is commonly made between epistemic uncertainty and aleatoric uncertainty (Kendall and Gal 2017).\nEpistemic uncertainty refers to imperfections in the model - in the limit of infinite data, this kind of uncertainty should be reducible to 0. Aleatoric uncertainty is due to data sampling and measurement processes and does not depend on the size of the dataset.\nSay we train a model for object detection. With more data, the model should become more sure about what makes a unicycle different from a mountainbike. However, let’s assume all that’s visible of the mountainbike is the front wheel, the fork and the head tube. Then it doesn’t look so different from a unicycle any more!\nWhat would be the consequences if we could distinguish both types of uncertainty? If epistemic uncertainty is high, we can try to get more training data. The remaining aleatoric uncertainty should then keep us cautioned to factor in safety margins in our application.\nProbably no further justifications are required of why we might want to assess model uncertainty - but how can we do this?\nUncertainty estimates through Bayesian deep learning\nIn a Bayesian world, in principle, uncertainty is for free as we don’t just get point estimates (the maximum aposteriori) but the full posterior distribution. Strictly speaking, in Bayesian deep learning, priors should be put over the weights, and the posterior be determined according to Bayes’ rule.\nTo the deep learning practitioner, this sounds pretty arduous - and how do you do it using Keras?\nIn 2016 though, Gal and Ghahramani (Yarin Gal and Ghahramani 2016) showed that when viewing a neural network as an approximation to a Gaussian process, uncertainty estimates can be obtained in a theoretically grounded yet very practical way: by training a network with dropout and then, using dropout at test time too. At test time, dropout lets us extract Monte Carlo samples from the posterior, which can then be used to approximate the true posterior distribution.\n\nYarin Gal has a nice writeup of the why and how on his blog.\nThis is already good news, but it leaves one question open: How do we choose an appropriate dropout rate? The answer is: let the network learn it.\nLearning dropout and uncertainty\nIn several 2017 papers (Y. Gal, Hron, and Kendall 2017),(Kendall and Gal 2017), Gal and his coworkers demonstrated how a network can be trained to dynamically adapt the dropout rate so it is adequate for the amount and characteristics of the data given.\nBesides the predictive mean of the target variable, it can additionally be made to learn the variance.\nThis means we can calculate both types of uncertainty, epistemic and aleatoric, independently, which is useful in the light of their different implications. We then add them up to obtain the overall predictive uncertainty.\nLet’s make this concrete and see how we can implement and test the intended behavior on simulated data.\nIn the implementation, there are three things warranting our special attention:\nThe wrapper class used to add learnable-dropout behavior to a Keras layer;\nThe loss function designed to minimize aleatoric uncertainty; and\nThe ways we can obtain both uncertainties at test time.\nLet’s start with the wrapper.\nA wrapper for learning dropout\nIn this example, we’ll restrict ourselves to learning dropout for dense layers. Technically, we’ll add a weight and a loss to every dense layer we want to use dropout with. This means we’ll create a custom wrapper class that has access to the underlying layer and can modify it.\nThe logic implemented in the wrapper is derived mathematically in the Concrete Dropout paper (Y. Gal, Hron, and Kendall 2017). The below code is a port to R of the Python Keras version found in the paper’s companion github repo.\nSo first, here is the wrapper class - we’ll see how to use it in just a second:\n\n\nlibrary(keras)\n\n# R6 wrapper class, a subclass of KerasWrapper\nConcreteDropout <- R6::R6Class(\"ConcreteDropout\",\n  \n  inherit = KerasWrapper,\n  \n  public = list(\n    weight_regularizer = NULL,\n    dropout_regularizer = NULL,\n    init_min = NULL,\n    init_max = NULL,\n    is_mc_dropout = NULL,\n    supports_masking = TRUE,\n    p_logit = NULL,\n    p = NULL,\n    \n    initialize = function(weight_regularizer,\n                          dropout_regularizer,\n                          init_min,\n                          init_max,\n                          is_mc_dropout) {\n      self$weight_regularizer <- weight_regularizer\n      self$dropout_regularizer <- dropout_regularizer\n      self$is_mc_dropout <- is_mc_dropout\n      self$init_min <- k_log(init_min) - k_log(1 - init_min)\n      self$init_max <- k_log(init_max) - k_log(1 - init_max)\n    },\n    \n    build = function(input_shape) {\n      super$build(input_shape)\n      \n      self$p_logit <- super$add_weight(\n        name = \"p_logit\",\n        shape = shape(1),\n        initializer = initializer_random_uniform(self$init_min, self$init_max),\n        trainable = TRUE\n      )\n\n      self$p <- k_sigmoid(self$p_logit)\n\n      input_dim <- input_shape[[2]]\n\n      weight <- private$py_wrapper$layer$kernel\n      \n      kernel_regularizer <- self$weight_regularizer * \n                            k_sum(k_square(weight)) / \n                            (1 - self$p)\n      \n      dropout_regularizer <- self$p * k_log(self$p)\n      dropout_regularizer <- dropout_regularizer +  \n                             (1 - self$p) * k_log(1 - self$p)\n      dropout_regularizer <- dropout_regularizer * \n                             self$dropout_regularizer * \n                             k_cast(input_dim, k_floatx())\n\n      regularizer <- k_sum(kernel_regularizer + dropout_regularizer)\n      super$add_loss(regularizer)\n    },\n    \n    concrete_dropout = function(x) {\n      eps <- k_cast_to_floatx(k_epsilon())\n      temp <- 0.1\n      \n      unif_noise <- k_random_uniform(shape = k_shape(x))\n      \n      drop_prob <- k_log(self$p + eps) - \n                   k_log(1 - self$p + eps) + \n                   k_log(unif_noise + eps) - \n                   k_log(1 - unif_noise + eps)\n      drop_prob <- k_sigmoid(drop_prob / temp)\n      \n      random_tensor <- 1 - drop_prob\n      \n      retain_prob <- 1 - self$p\n      x <- x * random_tensor\n      x <- x / retain_prob\n      x\n    },\n\n    call = function(x, mask = NULL, training = NULL) {\n      if (self$is_mc_dropout) {\n        super$call(self$concrete_dropout(x))\n      } else {\n        k_in_train_phase(\n          function()\n            super$call(self$concrete_dropout(x)),\n          super$call(x),\n          training = training\n        )\n      }\n    }\n  )\n)\n\n# function for instantiating custom wrapper\nlayer_concrete_dropout <- function(object, \n                                   layer,\n                                   weight_regularizer = 1e-6,\n                                   dropout_regularizer = 1e-5,\n                                   init_min = 0.1,\n                                   init_max = 0.1,\n                                   is_mc_dropout = TRUE,\n                                   name = NULL,\n                                   trainable = TRUE) {\n  create_wrapper(ConcreteDropout, object, list(\n    layer = layer,\n    weight_regularizer = weight_regularizer,\n    dropout_regularizer = dropout_regularizer,\n    init_min = init_min,\n    init_max = init_max,\n    is_mc_dropout = is_mc_dropout,\n    name = name,\n    trainable = trainable\n  ))\n}\n\n\nThe wrapper instantiator has default arguments, but two of them should be adapted to the data: weight_regularizer and dropout_regularizer. Following the authors’ recommendations, they should be set as follows.\nFirst, choose a value for hyperparameter \\(l\\). In this view of a neural network as an approximation to a Gaussian process, \\(l\\) is the prior length-scale, our a priori assumption about the frequency characteristics of the data. Here, we follow Gal’s demo in setting l := 1e-4. Then the initial values for weight_regularizer and dropout_regularizer are derived from the length-scale and the sample size.\n\n\n# sample size (training data)\nn_train <- 1000\n# sample size (validation data)\nn_val <- 1000\n# prior length-scale\nl <- 1e-4\n# initial value for weight regularizer \nwd <- l^2/n_train\n# initial value for dropout regularizer\ndd <- 2/n_train\n\n\nNow let’s see how to use the wrapper in a model.\nDropout model\nIn our demonstration, we’ll have a model with three hidden dense layers, each of which will have its dropout rate calculated by a dedicated wrapper.\n\n\n# we use one-dimensional input data here, but this isn't a necessity\ninput_dim <- 1\n# this too could be > 1 if we wanted\noutput_dim <- 1\nhidden_dim <- 1024\n\ninput <- layer_input(shape = input_dim)\n\noutput <- input %>% layer_concrete_dropout(\n  layer = layer_dense(units = hidden_dim, activation = \"relu\"),\n  weight_regularizer = wd,\n  dropout_regularizer = dd\n  ) %>% layer_concrete_dropout(\n  layer = layer_dense(units = hidden_dim, activation = \"relu\"),\n  weight_regularizer = wd,\n  dropout_regularizer = dd\n  ) %>% layer_concrete_dropout(\n  layer = layer_dense(units = hidden_dim, activation = \"relu\"),\n  weight_regularizer = wd,\n  dropout_regularizer = dd\n)\n\n\nNow, model output is interesting: We have the model yielding not just the predictive (conditional) mean, but also the predictive variance (\\(\\tau^{-1}\\) in Gaussian process parlance):\n\n\nmean <- output %>% layer_concrete_dropout(\n  layer = layer_dense(units = output_dim),\n  weight_regularizer = wd,\n  dropout_regularizer = dd\n)\n\nlog_var <- output %>% layer_concrete_dropout(\n  layer_dense(units = output_dim),\n  weight_regularizer = wd,\n  dropout_regularizer = dd\n)\n\noutput <- layer_concatenate(list(mean, log_var))\n\nmodel <- keras_model(input, output)\n\n\nThe significant thing here is that we learn different variances for different data points. We thus hope to be able to account for heteroscedasticity (different degrees of variability) in the data.\nHeteroscedastic loss\nAccordingly, instead of mean squared error we use a cost function that does not treat all estimates alike(Kendall and Gal 2017):\n\\[\\frac{1}{N} \\sum_i{\\frac{1}{2 \\hat{\\sigma}^2_i} \\ (\\mathbf{y}_i - \\mathbf{\\hat{y}}_i)^2 + \\frac{1}{2} log \\ \\hat{\\sigma}^2_i}\\]\nIn addition to the obligatory target vs. prediction check, this cost function contains two regularization terms:\nFirst, \\(\\frac{1}{2 \\hat{\\sigma}^2_i}\\) downweights the high-uncertainty predictions in the loss function. Put plainly: The model is encouraged to indicate high uncertainty when its predictions are false.\nSecond, \\(\\frac{1}{2} log \\ \\hat{\\sigma}^2_i\\) makes sure the network does not simply indicate high uncertainty everywhere.\nThis logic maps directly to the code (except that as usual, we’re calculating with the log of the variance, for reasons of numerical stability):\n\n\nheteroscedastic_loss <- function(y_true, y_pred) {\n    mean <- y_pred[, 1:output_dim]\n    log_var <- y_pred[, (output_dim + 1):(output_dim * 2)]\n    precision <- k_exp(-log_var)\n    k_sum(precision * (y_true - mean) ^ 2 + log_var, axis = 2)\n  }\n\n\nTraining on simulated data\nNow we generate some test data and train the model.\n\n\ngen_data_1d <- function(n) {\n  sigma <- 1\n  X <- matrix(rnorm(n))\n  w <- 2\n  b <- 8\n  Y <- matrix(X %*% w + b + sigma * rnorm(n))\n  list(X, Y)\n}\n\nc(X, Y) %<-% gen_data_1d(n_train + n_val)\n\nc(X_train, Y_train) %<-% list(X[1:n_train], Y[1:n_train])\nc(X_val, Y_val) %<-% list(X[(n_train + 1):(n_train + n_val)], \n                          Y[(n_train + 1):(n_train + n_val)])\n\nmodel %>% compile(\n  optimizer = \"adam\",\n  loss = heteroscedastic_loss,\n  metrics = c(custom_metric(\"heteroscedastic_loss\", heteroscedastic_loss))\n)\n\nhistory <- model %>% fit(\n  X_train,\n  Y_train,\n  epochs = 30,\n  batch_size = 10\n)\n\n\nWith training finished, we turn to the validation set to obtain estimates on unseen data - including those uncertainty measures this is all about!\nObtain uncertainty estimates via Monte Carlo sampling\nAs often in a Bayesian setup, we construct the posterior (and thus, the posterior predictive) via Monte Carlo sampling.\nUnlike in traditional use of dropout, there is no change in behavior between training and test phases: Dropout stays “on.”\nSo now we get an ensemble of model predictions on the validation set:\n\n\nnum_MC_samples <- 20\n\nMC_samples <- array(0, dim = c(num_MC_samples, n_val, 2 * output_dim))\nfor (k in 1:num_MC_samples) {\n  MC_samples[k, , ] <- (model %>% predict(X_val))\n}\n\n\nRemember, our model predicts the mean as well as the variance. We’ll use the former for calculating epistemic uncertainty, while aleatoric uncertainty is obtained from the latter.\nFirst, we determine the predictive mean as an average of the MC samples’ mean output:\n\n\n# the means are in the first output column\nmeans <- MC_samples[, , 1:output_dim]  \n# average over the MC samples\npredictive_mean <- apply(means, 2, mean) \n\n\nTo calculate epistemic uncertainty, we again use the mean output, but this time we’re interested in the variance of the MC samples:\n\n\nepistemic_uncertainty <- apply(means, 2, var) \n\n\nThen aleatoric uncertainty is the average over the MC samples of the variance output.1.\n\n\nlogvar <- MC_samples[, , (output_dim + 1):(output_dim * 2)]\naleatoric_uncertainty <- exp(colMeans(logvar))\n\n\nNote how this procedure gives us uncertainty estimates individually for every prediction. How do they look?\n\n\ndf <- data.frame(\n  x = X_val,\n  y_pred = predictive_mean,\n  e_u_lower = predictive_mean - sqrt(epistemic_uncertainty),\n  e_u_upper = predictive_mean + sqrt(epistemic_uncertainty),\n  a_u_lower = predictive_mean - sqrt(aleatoric_uncertainty),\n  a_u_upper = predictive_mean + sqrt(aleatoric_uncertainty),\n  u_overall_lower = predictive_mean - \n                    sqrt(epistemic_uncertainty) - \n                    sqrt(aleatoric_uncertainty),\n  u_overall_upper = predictive_mean + \n                    sqrt(epistemic_uncertainty) + \n                    sqrt(aleatoric_uncertainty)\n)\n\n\nHere, first, is epistemic uncertainty, with shaded bands indicating one standard deviation above resp. below the predicted mean:\n\n\nggplot(df, aes(x, y_pred)) + \n  geom_point() + \n  geom_ribbon(aes(ymin = e_u_lower, ymax = e_u_upper), alpha = 0.3)\n\n\nEpistemic uncertainty on the validation set, train size = 1000.This is interesting. The training data (as well as the validation data) were generated from a standard normal distribution, so the model has encountered many more examples close to the mean than outside two, or even three, standard deviations. So it correctly tells us that in those more exotic regions, it feels pretty unsure about its predictions.\nThis is exactly the behavior we want: Risk in automatically applying machine learning methods arises due to unanticipated differences between the training and test (real world) distributions. If the model were to tell us “ehm, not really seen anything like that before, don’t really know what to do” that’d be an enormously valuable outcome.\nSo while epistemic uncertainty has the algorithm reflecting on its model of the world - potentially admitting its shortcomings - aleatoric uncertainty, by definition, is irreducible. Of course, that doesn’t make it any less valuable - we’d know we always have to factor in a safety margin. So how does it look here?\nAleatoric uncertainty on the validation set, train size = 1000.Indeed, the extent of uncertainty does not depend on the amount of data seen at training time.\nFinally, we add up both types to obtain the overall uncertainty when making predictions.\nOverall predictive uncertainty on the validation set, train size = 1000.Now let’s try this method on a real-world dataset.\nCombined cycle power plant electrical energy output estimation\nThis dataset is available from the UCI Machine Learning Repository. We explicitly chose a regression task with continuous variables exclusively, to make for a smooth transition from the simulated data.\nIn the dataset providers’ own words\n\nThe dataset contains 9568 data points collected from a Combined Cycle Power Plant over 6 years (2006-2011), when the power plant was set to work with full load. Features consist of hourly average ambient variables Temperature (T), Ambient Pressure (AP), Relative Humidity (RH) and Exhaust Vacuum (V) to predict the net hourly electrical energy output (EP) of the plant.\n\n\nA combined cycle power plant (CCPP) is composed of gas turbines (GT), steam turbines (ST) and heat recovery steam generators. In a CCPP, the electricity is generated by gas and steam turbines, which are combined in one cycle, and is transferred from one turbine to another. While the Vacuum is collected from and has effect on the Steam Turbine, the other three of the ambient variables effect the GT performance.\n\nWe thus have four predictors and one target variable. We’ll train five models: four single-variable regressions and one making use of all four predictors. It probably goes without saying that our goal here is to inspect uncertainty information, not to fine-tune the model.\nSetup\nLet’s quickly inspect those five variables. Here PE is energy output, the target variable.\n\n\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(GGally)\n\ndf <- read_xlsx(\"CCPP/Folds5x2_pp.xlsx\")\nggscatmat(df)\n\n\n\nWe scale and divide up the data\n\n\ndf_scaled <- scale(df)\n\nX <- df_scaled[, 1:4]\ntrain_samples <- sample(1:nrow(df_scaled), 0.8 * nrow(X))\nX_train <- X[train_samples,]\nX_val <- X[-train_samples,]\n\ny <- df_scaled[, 5] %>% as.matrix()\ny_train <- y[train_samples,]\ny_val <- y[-train_samples,]\n\n\nand get ready for training a few models.\n\n\nn <- nrow(X_train)\nn_epochs <- 100\nbatch_size <- 100\noutput_dim <- 1\nnum_MC_samples <- 20\nl <- 1e-4\nwd <- l^2/n\ndd <- 2/n\n\nget_model <- function(input_dim, hidden_dim) {\n  \n  input <- layer_input(shape = input_dim)\n  output <-\n    input %>% layer_concrete_dropout(\n      layer = layer_dense(units = hidden_dim, activation = \"relu\"),\n      weight_regularizer = wd,\n      dropout_regularizer = dd\n    ) %>% layer_concrete_dropout(\n      layer = layer_dense(units = hidden_dim, activation = \"relu\"),\n      weight_regularizer = wd,\n      dropout_regularizer = dd\n    ) %>% layer_concrete_dropout(\n      layer = layer_dense(units = hidden_dim, activation = \"relu\"),\n      weight_regularizer = wd,\n      dropout_regularizer = dd\n    )\n  \n  mean <-\n    output %>% layer_concrete_dropout(\n      layer = layer_dense(units = output_dim),\n      weight_regularizer = wd,\n      dropout_regularizer = dd\n    )\n  \n  log_var <-\n    output %>% layer_concrete_dropout(\n      layer_dense(units = output_dim),\n      weight_regularizer = wd,\n      dropout_regularizer = dd\n    )\n  \n  output <- layer_concatenate(list(mean, log_var))\n  \n  model <- keras_model(input, output)\n  \n  heteroscedastic_loss <- function(y_true, y_pred) {\n    mean <- y_pred[, 1:output_dim]\n    log_var <- y_pred[, (output_dim + 1):(output_dim * 2)]\n    precision <- k_exp(-log_var)\n    k_sum(precision * (y_true - mean) ^ 2 + log_var, axis = 2)\n  }\n  \n  model %>% compile(optimizer = \"adam\",\n                    loss = heteroscedastic_loss,\n                    metrics = c(\"mse\"))\n  model\n}\n\n\nWe’ll train each of the five models with a hidden_dim of 64.\nWe then obtain 20 Monte Carlo sample from the posterior predictive distribution and calculate the uncertainties as before.\nHere we show the code for the first predictor, “AT.” It is similar for all other cases.\n\n\nmodel <- get_model(1, 64)\nhist <- model %>% fit(\n  X_train[ ,1],\n  y_train,\n  validation_data = list(X_val[ , 1], y_val),\n  epochs = n_epochs,\n  batch_size = batch_size\n)\n\nMC_samples <- array(0, dim = c(num_MC_samples, nrow(X_val), 2 * output_dim))\nfor (k in 1:num_MC_samples) {\n  MC_samples[k, ,] <- (model %>% predict(X_val[ ,1]))\n}\n\nmeans <- MC_samples[, , 1:output_dim]  \npredictive_mean <- apply(means, 2, mean) \nepistemic_uncertainty <- apply(means, 2, var) \nlogvar <- MC_samples[, , (output_dim + 1):(output_dim * 2)]\naleatoric_uncertainty <- exp(colMeans(logvar))\n\npreds <- data.frame(\n  x1 = X_val[, 1],\n  y_true = y_val,\n  y_pred = predictive_mean,\n  e_u_lower = predictive_mean - sqrt(epistemic_uncertainty),\n  e_u_upper = predictive_mean + sqrt(epistemic_uncertainty),\n  a_u_lower = predictive_mean - sqrt(aleatoric_uncertainty),\n  a_u_upper = predictive_mean + sqrt(aleatoric_uncertainty),\n  u_overall_lower = predictive_mean - \n                    sqrt(epistemic_uncertainty) - \n                    sqrt(aleatoric_uncertainty),\n  u_overall_upper = predictive_mean + \n                    sqrt(epistemic_uncertainty) + \n                    sqrt(aleatoric_uncertainty)\n)\n\n\nResult\nNow let’s see the uncertainty estimates for all five models!\nFirst, the single-predictor setup. Ground truth values are displayed in cyan, posterior predictive estimates are black, and the grey bands extend up resp. down by the square root of the calculated uncertainties.\nWe’re starting with ambient temperature, a low-variance predictor.\nWe are surprised how confident the model is that it’s gotten the process logic correct, but high aleatoric uncertainty makes up for this (more or less).\nUncertainties on the validation set using ambient temperature as a single predictor.Now looking at the other predictors, where variance is much higher in the ground truth, it does get a bit difficult to feel comfortable with the model’s confidence. Aleatoric uncertainty is high, but not high enough to capture the true variability in the data. And we certaintly would hope for higher epistemic uncertainty, especially in places where the model introduces arbitrary-looking deviations from linearity.\nUncertainties on the validation set using exhaust vacuum as a single predictor.Uncertainties on the validation set using ambient pressure as a single predictor.Uncertainties on the validation set using relative humidity as a single predictor.Now let’s see uncertainty output when we use all four predictors. We see that now, the Monte Carlo estimates vary a lot more, and accordingly, epistemic uncertainty is a lot higher. Aleatoric uncertainty, on the other hand, got a lot lower. Overall, predictive uncertainty captures the range of ground truth values pretty well.\nUncertainties on the validation set using all 4 predictors.Conclusion\nWe’ve introduced a method to obtain theoretically grounded uncertainty estimates from neural networks.\nWe find the approach intuitively attractive for several reasons: For one, the separation of different types of uncertainty is convincing and practically relevant. Second, uncertainty depends on the amount of data seen in the respective ranges. This is especially relevant when thinking of differences between training and test-time distributions.\nThird, the idea of having the network “become aware of its own uncertainty” is seductive.\nIn practice though, there are open questions as to how to apply the method. From our real-world test above, we immediately ask: Why is the model so confident2 when the ground truth data has high variance? And, thinking experimentally: How would that vary with different data sizes (rows), dimensionality (columns), and hyperparameter settings (including neural network hyperparameters like capacity, number of epochs trained, and activation functions, but also the Gaussian process prior length-scale \\(\\tau\\))?\nFor practical use, more experimentation with different datasets and hyperparameter settings is certainly warranted.\nAnother direction to follow up is application to tasks in image recognition, such as semantic segmentation.\nHere we’d be interested in not just quantifying, but also localizing uncertainty, to see which visual aspects of a scene (occlusion, illumination, uncommon shapes) make objects hard to identify.\n\nThis would require a slightly different wrapper class but again, an R implementation could follow the Python example in Yarin Gal’s repository.\n\n\n\nGal, Yarin, and Zoubin Ghahramani. 2016. “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.” In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, 1050–59. http://jmlr.org/proceedings/papers/v48/gal16.html.\n\n\nGal, Y., J. Hron, and A. Kendall. 2017. “Concrete Dropout.” ArXiv e-Prints, May. https://arxiv.org/abs/1705.07832.\n\n\nKendall, Alex, and Yarin Gal. 2017. “What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?” In Advances in Neural Information Processing Systems 30, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 5574–84. Curran Associates, Inc. http://papers.nips.cc/paper/7141-what-uncertainties-do-we-need-in-bayesian-deep-learning-for-computer-vision.pdf.\n\n\nLeibig, Christian, Vaneeda Allken, Murat Seckin Ayhan, Philipp Berens, and Siegfried Wahl. 2017. “Leveraging Uncertainty Information from Deep Neural Networks for Disease Detection.” bioRxiv. https://doi.org/10.1101/084210.\n\n\nexponentiated because we’ve really been working with the log of the variance↩︎\ntalking epistemic uncertainty↩︎\n",
    "preview": "posts/2018-11-12-uncertainty_estimates_dropout/images/thumb.png",
    "last_modified": "2024-11-21T15:48:21+00:00",
    "input_file": {},
    "preview_width": 2046,
    "preview_height": 872
  },
  {
    "path": "posts/2018-11-05-naming-locating-objects/",
    "title": "Naming and locating objects in images",
    "description": "Object detection (the act of classifying and localizing multiple objects in a scene) is one of the more difficult, but very relevant in practice deep learning tasks. We'll build up to it in several posts. Here we start with the simpler tasks of naming and locating a single object.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-11-05",
    "categories": [
      "TensorFlow/Keras",
      "Image Recognition & Image Processing"
    ],
    "contents": "\nWe’ve all become used to deep learning’s success in image classification. Greater Swiss Mountain dog or Bernese mountain dog? Red panda or giant panda? No problem.\nHowever, in real life it’s not enough to name the single most salient object on a picture. Like it or not, one of the most compelling examples is autonomous driving: We don’t want the algorithm to recognize just that car in front of us, but also the pedestrian about to cross the street. And, just detecting the pedestrian is not sufficient. The exact location of objects matters.\nThe term object detection is commonly used to refer to the task of naming and localizing multiple objects in an image frame. Object detection is difficult; we’ll build up to it in a loose series of posts, focusing on concepts instead of aiming for ultimate performance. Today, we’ll start with a few straightforward building blocks: Classification, both single and multiple; localization; and combining both classification and localization of a single object.\n\nThe structure and approaches of these posts will follow the excellent fast.ai notebook on object detection.\nDataset\nWe’ll be using images and annotations from the Pascal VOC dataset which can be downloaded from this mirror.\nSpecifically, we’ll use data from the 2007 challenge and the same JSON annotation file as used in the fast.ai course.\nQuick download/organization instructions, shamelessly taken from a helpful post on the fast.ai wiki, are as follows:\n# mkdir data && cd data\n# curl -OL http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n# curl -OL https://storage.googleapis.com/coco-dataset/external/PASCAL_VOC.zip\n# tar -xf VOCtrainval_06-Nov-2007.tar\n# unzip PASCAL_VOC.zip\n# mv PASCAL_VOC/*.json .\n# rmdir PASCAL_VOC\n# tar -xvf VOCtrainval_06-Nov-2007.tar\nIn words, we take the images and the annotation file from different places:\nhttp://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar provides us with the images, and after unzipping all we care about is the JPEGImages folder.\nFrom https://storage.googleapis.com/coco-dataset/external/PASCAL_VOC.zip all we will be needing is the annotation file, pascal_train2007.json.\nWhether you’re executing the listed commands or arranging files manually, you should eventually end up with directories/files analogous to these:\n\n\nimg_dir <- \"data/VOCdevkit/VOC2007/JPEGImages\"\nannot_file <- \"data/pascal_train2007.json\"\n\n\nNow we need to extract some information from that json file.\nPreprocessing\nLet’s quickly make sure we have all required libraries loaded.\n\n\nlibrary(keras)\nlibrary(rjson)\nlibrary(magick)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stringr)\n\n\nAnnotations contain information about three types of things we’re interested in.\n\n\nannotations <- fromJSON(file = annot_file)\nstr(annotations, max.level = 1)\n\n\nList of 4\n $ images     :List of 2501\n $ type       : chr \"instances\"\n $ annotations:List of 7844\n $ categories :List of 20\nFirst, characteristics of the image itself (height and width) and where it’s stored. Not surprisingly, here it’s one entry per image.\n\n\nimageinfo <- annotations$images %>% {\n  tibble(\n    id = map_dbl(., \"id\"),\n    file_name = map_chr(., \"file_name\"),\n    image_height = map_dbl(., \"height\"),\n    image_width = map_dbl(., \"width\")\n  )\n}\n\n\nThen, object class ids and bounding box coordinates. There may be multiple of these per image.\nIn Pascal VOC, there are 20 object classes, from ubiquitous vehicles (car, aeroplane) over indispensable animals (cat, sheep) to more rare (in popular datasets) types like potted plant or tv monitor.\n\n\nclasses <- c(\n  \"aeroplane\",\n  \"bicycle\",\n  \"bird\",\n  \"boat\",\n  \"bottle\",\n  \"bus\",\n  \"car\",\n  \"cat\",\n  \"chair\",\n  \"cow\",\n  \"diningtable\",\n  \"dog\",\n  \"horse\",\n  \"motorbike\",\n  \"person\",\n  \"pottedplant\",\n  \"sheep\",\n  \"sofa\",\n  \"train\",\n  \"tvmonitor\"\n)\n\nboxinfo <- annotations$annotations %>% {\n  tibble(\n    image_id = map_dbl(., \"image_id\"),\n    category_id = map_dbl(., \"category_id\"),\n    bbox = map(., \"bbox\")\n  )\n}\n\n\nThe bounding boxes are now stored in a list column and need to be unpacked.\n\n\nboxinfo <- boxinfo %>% \n  mutate(bbox = unlist(map(.$bbox, function(x) paste(x, collapse = \" \"))))\nboxinfo <- boxinfo %>% \n  separate(bbox, into = c(\"x_left\", \"y_top\", \"bbox_width\", \"bbox_height\"))\nboxinfo <- boxinfo %>% mutate_all(as.numeric)\n\n\nFor the bounding boxes, the annotation file provides x_left and y_top coordinates, as well as width and height.\nWe will mostly be working with corner coordinates, so we create the missing x_right and y_bottom.\nAs usual in image processing, the y axis starts from the top.\n\n\nboxinfo <- boxinfo %>% \n  mutate(y_bottom = y_top + bbox_height - 1, x_right = x_left + bbox_width - 1)\n\n\nFinally, we still need to match class ids to class names.\n\n\ncatinfo <- annotations$categories %>%  {\n  tibble(id = map_dbl(., \"id\"), name = map_chr(., \"name\"))\n}\n\n\nSo, putting it all together:\n\n\nimageinfo <- imageinfo %>%\n  inner_join(boxinfo, by = c(\"id\" = \"image_id\")) %>%\n  inner_join(catinfo, by = c(\"category_id\" = \"id\"))\n\n\nNote that here still, we have several entries per image, each annotated object occupying its own row.\nThere’s one step that will bitterly hurt our localization performance if we later forget it, so let’s do it now already: We need to scale all bounding box coordinates according to the actual image size we’ll use when we pass it to our network.\n\n\ntarget_height <- 224\ntarget_width <- 224\n\nimageinfo <- imageinfo %>% mutate(\n  x_left_scaled = (x_left / image_width * target_width) %>% round(),\n  x_right_scaled = (x_right / image_width * target_width) %>% round(),\n  y_top_scaled = (y_top / image_height * target_height) %>% round(),\n  y_bottom_scaled = (y_bottom / image_height * target_height) %>% round(),\n  bbox_width_scaled =  (bbox_width / image_width * target_width) %>% round(),\n  bbox_height_scaled = (bbox_height / image_height * target_height) %>% round()\n)\n\n\nLet’s take a glance at our data. Picking one of the early entries and displaying the original image together with the object annotation yields\n\n\nimg_data <- imageinfo[4,]\nimg <- image_read(file.path(img_dir, img_data$file_name))\nimg <- image_draw(img)\nrect(\n  img_data$x_left,\n  img_data$y_bottom,\n  img_data$x_right,\n  img_data$y_top,\n  border = \"white\",\n  lwd = 2\n)\ntext(\n  img_data$x_left,\n  img_data$y_top,\n  img_data$name,\n  offset = 1,\n  pos = 2,\n  cex = 1.5,\n  col = \"white\"\n)\ndev.off()\n\n\n\nNow as indicated above, in this post we’ll mostly address handling a single object in an image. This means we have to decide, per image, which object to single out.\nA reasonable strategy seems to be choosing the object with the largest ground truth bounding box.\n\n\nimageinfo <- imageinfo %>% mutate(area = bbox_width_scaled * bbox_height_scaled)\n\nimageinfo_maxbb <- imageinfo %>%\n  group_by(id) %>%\n  filter(which.max(area) == row_number())\n\n\nAfter this operation, we only have 2501 images to work with - not many at all! For classification, we could simply use data augmentation as provided by Keras, but to work with localization we’d have to spin our own augmentation algorithm.\nWe’ll leave this to a later occasion and for now, focus on the basics.\nFinally after train-test split\n\n\ntrain_indices <- sample(1:n_samples, 0.8 * n_samples)\ntrain_data <- imageinfo_maxbb[train_indices,]\nvalidation_data <- imageinfo_maxbb[-train_indices,]\n\n\nour training set consists of 2000 images with one annotation each. We’re ready to start training, and we’ll start gently, with single-object classification.\nSingle-object classification\nIn all cases, we will use XCeption as a basic feature extractor. Having been trained on ImageNet, we don’t expect much fine tuning to be necessary to adapt to Pascal VOC, so we leave XCeption’s weights untouched\n\n\nfeature_extractor <-\n  application_xception(\n    include_top = FALSE,\n    input_shape = c(224, 224, 3),\n    pooling = \"avg\"\n)\n\nfeature_extractor %>% freeze_weights()\n\n\nand put just a few custom layers on top.\n\n\nmodel <- keras_model_sequential() %>%\n  feature_extractor %>%\n  layer_batch_normalization() %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_dense(units = 512, activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_dropout(rate = 0.5) %>%\n  layer_dense(units = 20, activation = \"softmax\")\n\nmodel %>% compile(\n  optimizer = \"adam\",\n  loss = \"sparse_categorical_crossentropy\",\n  metrics = list(\"accuracy\")\n)\n\n\nHow should we pass our data to Keras? We could simple use Keras’ image_data_generator, but given we will need custom generators soon, we’ll build a simple one ourselves.\nThis one delivers images as well as the corresponding targets in a stream. Note how the targets are not one-hot-encoded, but integers - using sparse_categorical_crossentropy as a loss function enables this convenience.\n\nSee the Deep learning with R book for an introduction to writing data generators like this one.\n\n\nbatch_size <- 10\n\nload_and_preprocess_image <- function(image_name, target_height, target_width) {\n  img_array <- image_load(\n    file.path(img_dir, image_name),\n    target_size = c(target_height, target_width)\n    ) %>%\n    image_to_array() %>%\n    xception_preprocess_input() \n  dim(img_array) <- c(1, dim(img_array))\n  img_array\n}\n\nclassification_generator <-\n  function(data,\n           target_height,\n           target_width,\n           shuffle,\n           batch_size) {\n    i <- 1\n    function() {\n      if (shuffle) {\n        indices <- sample(1:nrow(data), size = batch_size)\n      } else {\n        if (i + batch_size >= nrow(data))\n          i <<- 1\n        indices <- c(i:min(i + batch_size - 1, nrow(data)))\n        i <<- i + length(indices)\n      }\n      x <-\n        array(0, dim = c(length(indices), target_height, target_width, 3))\n      y <- array(0, dim = c(length(indices), 1))\n      \n      for (j in 1:length(indices)) {\n        x[j, , , ] <-\n          load_and_preprocess_image(data[[indices[j], \"file_name\"]],\n                                    target_height, target_width)\n        y[j, ] <-\n          data[[indices[j], \"category_id\"]] - 1\n      }\n      x <- x / 255\n      list(x, y)\n    }\n  }\n\ntrain_gen <- classification_generator(\n  train_data,\n  target_height = target_height,\n  target_width = target_width,\n  shuffle = TRUE,\n  batch_size = batch_size\n)\n\nvalid_gen <- classification_generator(\n  validation_data,\n  target_height = target_height,\n  target_width = target_width,\n  shuffle = FALSE,\n  batch_size = batch_size\n)\n\n\nNow how does training go?\n\n\nmodel %>% fit_generator(\n  train_gen,\n  epochs = 20,\n  steps_per_epoch = nrow(train_data) / batch_size,\n  validation_data = valid_gen,\n  validation_steps = nrow(validation_data) / batch_size,\n  callbacks = list(\n    callback_model_checkpoint(\n      file.path(\"class_only\", \"weights.{epoch:02d}-{val_loss:.2f}.hdf5\")\n    ),\n    callback_early_stopping(patience = 2)\n  )\n)\n\n\nFor us, after 8 epochs, accuracies on the train resp. validation sets were at 0.68 and 0.74, respectively. Not too bad given given we’re trying to differentiate between 20 classes here.\nNow let’s quickly think what we’d change if we were to classify multiple objects in one image. Changes mostly concern preprocessing steps.\nMultiple object classification\nThis time, we multi-hot-encode our data. For every image (as represented by its filename), here we have a vector of length 20 where 0 indicates absence, 1 means presence of the respective object class:\n\n\nimage_cats <- imageinfo %>% \n  select(category_id) %>%\n  mutate(category_id = category_id - 1) %>%\n  pull() %>%\n  to_categorical(num_classes = 20)\n\nimage_cats <- data.frame(image_cats) %>%\n  add_column(file_name = imageinfo$file_name, .before = TRUE)\n\nimage_cats <- image_cats %>% \n  group_by(file_name) %>% \n  summarise_all(.funs = funs(max))\n\nn_samples <- nrow(image_cats)\ntrain_indices <- sample(1:n_samples, 0.8 * n_samples)\ntrain_data <- image_cats[train_indices,]\nvalidation_data <- image_cats[-train_indices,]\n\n\nCorrespondingly, we modify the generator to return a target of dimensions batch_size * 20, instead of batch_size * 1.\n\n\nclassification_generator <- \n  function(data,\n           target_height,\n           target_width,\n           shuffle,\n           batch_size) {\n    i <- 1\n    function() {\n      if (shuffle) {\n        indices <- sample(1:nrow(data), size = batch_size)\n      } else {\n        if (i + batch_size >= nrow(data))\n          i <<- 1\n        indices <- c(i:min(i + batch_size - 1, nrow(data)))\n        i <<- i + length(indices)\n      }\n      x <-\n        array(0, dim = c(length(indices), target_height, target_width, 3))\n      y <- array(0, dim = c(length(indices), 20))\n      \n      for (j in 1:length(indices)) {\n        x[j, , , ] <-\n          load_and_preprocess_image(data[[indices[j], \"file_name\"]], \n                                    target_height, target_width)\n        y[j, ] <-\n          data[indices[j], 2:21] %>% as.matrix()\n      }\n      x <- x / 255\n      list(x, y)\n    }\n  }\n\ntrain_gen <- classification_generator(\n  train_data,\n  target_height = target_height,\n  target_width = target_width,\n  shuffle = TRUE,\n  batch_size = batch_size\n)\n\nvalid_gen <- classification_generator(\n  validation_data,\n  target_height = target_height,\n  target_width = target_width,\n  shuffle = FALSE,\n  batch_size = batch_size\n)\n\n\nNow, the most interesting change is to the model - even though it’s a change to two lines only.\nWere we to use categorical_crossentropy now (the non-sparse variant of the above), combined with a softmax activation, we would effectively tell the model to pick just one, namely, the most probable object.\n\nSee the introduction to loss functions and activations on this blog for a demonstration.\nInstead, we want to decide: For each object class, is it present in the image or not? Thus, instead of softmax we use sigmoid, paired with binary_crossentropy, to obtain an independent verdict on every class.\n\n\nfeature_extractor <-\n  application_xception(\n    include_top = FALSE,\n    input_shape = c(224, 224, 3),\n    pooling = \"avg\"\n  )\n\nfeature_extractor %>% freeze_weights()\n\nmodel <- keras_model_sequential() %>%\n  feature_extractor %>%\n  layer_batch_normalization() %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_dense(units = 512, activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_dropout(rate = 0.5) %>%\n  layer_dense(units = 20, activation = \"sigmoid\")\n\nmodel %>% compile(optimizer = \"adam\",\n                  loss = \"binary_crossentropy\",\n                  metrics = list(\"accuracy\"))\n\n\nAnd finally, again, we fit the model:\n\n\nmodel %>% fit_generator(\n  train_gen,\n  epochs = 20,\n  steps_per_epoch = nrow(train_data) / batch_size,\n  validation_data = valid_gen,\n  validation_steps = nrow(validation_data) / batch_size,\n  callbacks = list(\n    callback_model_checkpoint(\n      file.path(\"multiclass\", \"weights.{epoch:02d}-{val_loss:.2f}.hdf5\")\n    ),\n    callback_early_stopping(patience = 2)\n  )\n)\n\n\nThis time, (binary) accuracy surpasses 0.95 after one epoch already, on both the train and validation sets. Not surprisingly, accuracy is significantly higher here than when we had to single out one of 20 classes (and that, with other confounding objects present in most cases!).\nNow, chances are that if you’ve done any deep learning before, you’ve done image classification in some form, perhaps even in the multiple-object variant. To build up in the direction of object detection, it is time we add a new ingredient: localization.\nSingle-object localization\nFrom here on, we’re back to dealing with a single object per image. So the question now is, how do we learn bounding boxes?\nIf you’ve never heard of this, the answer will sound unbelievably simple (naive even): We formulate this as a regression problem and aim to predict the actual coordinates. To set realistic expectations - we surely shouldn’t expect ultimate precision here. But in a way it’s amazing it does even work at all.\nWhat does this mean, formulate as a regression problem? Concretely, it means we’ll have a dense output layer with 4 units, each corresponding to a corner coordinate.\nSo let’s start with the model this time. Again, we use Xception, but there’s an important difference here: Whereas before, we said pooling = \"avg\" to obtain an output tensor of dimensions batch_size * number of filters, here we don’t do any averaging or flattening out of the spatial grid. This is because it’s exactly the spatial information we’re interested in!\nFor Xception, the output resolution will be 7x7. So a priori, we shouldn’t expect high precision on objects much smaller than about 32x32 pixels (assuming the standard input size of 224x224).\n\n\nfeature_extractor <- application_xception(\n  include_top = FALSE,\n  input_shape = c(224, 224, 3)\n)\n\nfeature_extractor %>% freeze_weights()\n\n\nNow we append our custom regression module.\n\n\nmodel <- keras_model_sequential() %>%\n  feature_extractor %>%\n  layer_flatten() %>%\n  layer_batch_normalization() %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_dense(units = 512, activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_dropout(rate = 0.5) %>%\n  layer_dense(units = 4)\n\n\nWe will train with one of the loss functions common in regression tasks, mean absolute error. But in tasks like object detection or segmentation, we’re also interested in a more tangible quantity: How much do estimate and ground truth overlap?\nOverlap is usually measured as Intersection over Union, or Jaccard distance. Intersection over Union is exactly what it says, a ratio between space shared by the objects and space occupied when we take them together.\nTo assess the model’s progress, we can easily code this as a custom metric:\n\n\nmetric_iou <- function(y_true, y_pred) {\n  \n  # order is [x_left, y_top, x_right, y_bottom]\n  intersection_xmin <- k_maximum(y_true[ ,1], y_pred[ ,1])\n  intersection_ymin <- k_maximum(y_true[ ,2], y_pred[ ,2])\n  intersection_xmax <- k_minimum(y_true[ ,3], y_pred[ ,3])\n  intersection_ymax <- k_minimum(y_true[ ,4], y_pred[ ,4])\n  \n  area_intersection <- (intersection_xmax - intersection_xmin) * \n                       (intersection_ymax - intersection_ymin)\n  area_y <- (y_true[ ,3] - y_true[ ,1]) * (y_true[ ,4] - y_true[ ,2])\n  area_yhat <- (y_pred[ ,3] - y_pred[ ,1]) * (y_pred[ ,4] - y_pred[ ,2])\n  area_union <- area_y + area_yhat - area_intersection\n  \n  iou <- area_intersection/area_union\n  k_mean(iou)\n  \n}\n\n\nModel compilation then goes like\n\n\nmodel %>% compile(\n  optimizer = \"adam\",\n  loss = \"mae\",\n  metrics = list(custom_metric(\"iou\", metric_iou))\n)\n\n\nNow modify the generator to return bounding box coordinates as targets…\n\n\nlocalization_generator <-\n  function(data,\n           target_height,\n           target_width,\n           shuffle,\n           batch_size) {\n    i <- 1\n    function() {\n      if (shuffle) {\n        indices <- sample(1:nrow(data), size = batch_size)\n      } else {\n        if (i + batch_size >= nrow(data))\n          i <<- 1\n        indices <- c(i:min(i + batch_size - 1, nrow(data)))\n        i <<- i + length(indices)\n      }\n      x <-\n        array(0, dim = c(length(indices), target_height, target_width, 3))\n      y <- array(0, dim = c(length(indices), 4))\n      \n      for (j in 1:length(indices)) {\n        x[j, , , ] <-\n          load_and_preprocess_image(data[[indices[j], \"file_name\"]], \n                                    target_height, target_width)\n        y[j, ] <-\n          data[indices[j], c(\"x_left_scaled\",\n                             \"y_top_scaled\",\n                             \"x_right_scaled\",\n                             \"y_bottom_scaled\")] %>% as.matrix()\n      }\n      x <- x / 255\n      list(x, y)\n    }\n  }\n\ntrain_gen <- localization_generator(\n  train_data,\n  target_height = target_height,\n  target_width = target_width,\n  shuffle = TRUE,\n  batch_size = batch_size\n)\n\nvalid_gen <- localization_generator(\n  validation_data,\n  target_height = target_height,\n  target_width = target_width,\n  shuffle = FALSE,\n  batch_size = batch_size\n)\n\n\n… and we’re ready to go!\n\n\nmodel %>% fit_generator(\n  train_gen,\n  epochs = 20,\n  steps_per_epoch = nrow(train_data) / batch_size,\n  validation_data = valid_gen,\n  validation_steps = nrow(validation_data) / batch_size,\n  callbacks = list(\n    callback_model_checkpoint(\n      file.path(\"loc_only\", \"weights.{epoch:02d}-{val_loss:.2f}.hdf5\")\n    ),\n    callback_early_stopping(patience = 2)\n  )\n)\n\n\nAfter 8 epochs, IOU on both training and test sets is around 0.35. This number doesn’t look too good. To learn more about how training went, we need to see some predictions. Here’s a convenience function that displays an image, the ground truth box of the most salient object (as defined above), and if given, class and bounding box predictions.\n\n\nplot_image_with_boxes <- function(file_name,\n                                  object_class,\n                                  box,\n                                  scaled = FALSE,\n                                  class_pred = NULL,\n                                  box_pred = NULL) {\n  img <- image_read(file.path(img_dir, file_name))\n  if(scaled) img <- image_resize(img, geometry = \"224x224!\")\n  img <- image_draw(img)\n  x_left <- box[1]\n  y_bottom <- box[2]\n  x_right <- box[3]\n  y_top <- box[4]\n  rect(\n    x_left,\n    y_bottom,\n    x_right,\n    y_top,\n    border = \"cyan\",\n    lwd = 2.5\n  )\n  text(\n    x_left,\n    y_top,\n    object_class,\n    offset = 1,\n    pos = 2,\n    cex = 1.5,\n    col = \"cyan\"\n  )\n  if (!is.null(box_pred))\n    rect(box_pred[1],\n         box_pred[2],\n         box_pred[3],\n         box_pred[4],\n         border = \"yellow\",\n         lwd = 2.5)\n  if (!is.null(class_pred))\n    text(\n      box_pred[1],\n      box_pred[2],\n      class_pred,\n      offset = 0,\n      pos = 4,\n      cex = 1.5,\n      col = \"yellow\")\n  dev.off()\n  img %>% image_write(paste0(\"preds_\", file_name))\n  plot(img)\n}\n\n\nFirst, let’s see predictions on sample images from the training set.\n\n\ntrain_1_8 <- train_data[1:8, c(\"file_name\",\n                               \"name\",\n                               \"x_left_scaled\",\n                               \"y_top_scaled\",\n                               \"x_right_scaled\",\n                               \"y_bottom_scaled\")]\n\nfor (i in 1:8) {\n  preds <-\n    model %>% predict(\n      load_and_preprocess_image(train_1_8[i, \"file_name\"], \n                                target_height, target_width),\n      batch_size = 1\n  )\n  plot_image_with_boxes(train_1_8$file_name[i],\n                        train_1_8$name[i],\n                        train_1_8[i, 3:6] %>% as.matrix(),\n                        scaled = TRUE,\n                        box_pred = preds)\n}\n\n\nSample bounding box predictions on the training set.As you’d guess from looking, the cyan-colored boxes are the ground truth ones. Now looking at the predictions explains a lot about the mediocre IOU values! Let’s take the very first sample image - we wanted the model to focus on the sofa, but it picked the table, which is also a category in the dataset (although in the form of dining table). Similar with the image on the right of the first row - we wanted to it to pick just the dog but it included the person, too (by far the most frequently seen category in the dataset).\nSo we actually made the task a lot more difficult than had we stayed with e.g., ImageNet where normally a single object is salient.\nNow check predictions on the validation set.\nSome bounding box predictions on the validation set.Again, we get a similar impression: The model did learn something, but the task is ill defined. Look at the third image in row 2: Isn’t it pretty consequent the model picks all people instead of singling out some special guy?\nIf single-object localization is that straightforward, how technically involved can it be to output a class label at the same time?\nAs long as we stay with a single object, the answer indeed is: not much.\n\nAs a caveat, please note we’re talking about mapping concepts to technical approaches here.\nObtaining ultimate performance is a different thing.\nLet’s finish up today with a constrained combination of classification and localization: detection of a single object.\nSingle-object detection\nCombining regression and classification into one means we’ll want to have two outputs in our model.\nWe’ll thus use the functional API this time.\nOtherwise, there isn’t much new here: We start with an XCeption output of spatial resolution 7x7, append some custom processing and return two outputs, one for bounding box regression and one for classification.\n\n\nfeature_extractor <- application_xception(\n  include_top = FALSE,\n  input_shape = c(224, 224, 3)\n)\n\ninput <- feature_extractor$input\ncommon <- feature_extractor$output %>%\n  layer_flatten(name = \"flatten\") %>%\n  layer_activation_relu() %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_dense(units = 512, activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_dropout(rate = 0.5)\n\nregression_output <-\n  layer_dense(common, units = 4, name = \"regression_output\")\nclass_output <- layer_dense(\n  common,\n  units = 20,\n  activation = \"softmax\",\n  name = \"class_output\"\n)\n\nmodel <- keras_model(\n  inputs = input,\n  outputs = list(regression_output, class_output)\n)\n\n\nWhen defining the losses (mean absolute error and categorical crossentropy, just as in the respective single tasks of regression and classification), we could weight them so they end up on approximately a common scale. In fact that didn’t make much of a difference so we show the respective code in commented form.\n\n\nmodel %>% freeze_weights(to = \"flatten\")\n\nmodel %>% compile(\n  optimizer = \"adam\",\n  loss = list(\"mae\", \"sparse_categorical_crossentropy\"),\n  #loss_weights = list(\n  #  regression_output = 0.05,\n  #  class_output = 0.95),\n  metrics = list(\n    regression_output = custom_metric(\"iou\", metric_iou),\n    class_output = \"accuracy\"\n  )\n)\n\n\nJust like model outputs and losses are both lists, the data generator has to return the ground truth samples in a list.\nFitting the model then goes as usual.\n\nloc_class_generator <-\n  function(data,\n           target_height,\n           target_width,\n           shuffle,\n           batch_size) {\n    i <- 1\n    function() {\n      if (shuffle) {\n        indices <- sample(1:nrow(data), size = batch_size)\n      } else {\n        if (i + batch_size >= nrow(data))\n          i <<- 1\n        indices <- c(i:min(i + batch_size - 1, nrow(data)))\n        i <<- i + length(indices)\n      }\n      x <-\n        array(0, dim = c(length(indices), target_height, target_width, 3))\n      y1 <- array(0, dim = c(length(indices), 4))\n      y2 <- array(0, dim = c(length(indices), 1))\n      \n      for (j in 1:length(indices)) {\n        x[j, , , ] <-\n          load_and_preprocess_image(data[[indices[j], \"file_name\"]], \n                                    target_height, target_width)\n        y1[j, ] <-\n          data[indices[j], c(\"x_left\", \"y_top\", \"x_right\", \"y_bottom\")] \n            %>% as.matrix()\n        y2[j, ] <-\n          data[[indices[j], \"category_id\"]] - 1\n      }\n      x <- x / 255\n      list(x, list(y1, y2))\n    }\n  }\n\ntrain_gen <- loc_class_generator(\n  train_data,\n  target_height = target_height,\n  target_width = target_width,\n  shuffle = TRUE,\n  batch_size = batch_size\n)\n\nvalid_gen <- loc_class_generator(\n  validation_data,\n  target_height = target_height,\n  target_width = target_width,\n  shuffle = FALSE,\n  batch_size = batch_size\n)\n\nmodel %>% fit_generator(\n  train_gen,\n  epochs = 20,\n  steps_per_epoch = nrow(train_data) / batch_size,\n  validation_data = valid_gen,\n  validation_steps = nrow(validation_data) / batch_size,\n  callbacks = list(\n    callback_model_checkpoint(\n      file.path(\"loc_class\", \"weights.{epoch:02d}-{val_loss:.2f}.hdf5\")\n    ),\n    callback_early_stopping(patience = 2)\n  )\n)\n\nWhat about model predictions? A priori we might expect the bounding boxes to look better than in the regression-only model, as a significant part of the model is shared between classification and localization. Intuitively, I should be able to more precisely indicate the boundaries of something if I have an idea what that something is.\nUnfortunately, that didn’t quite happen. The model has become very biased to detecting a person everywhere, which might be advantageous (thinking safety) in an autonomous driving application but isn’t quite what we’d hoped for here.\nExample class and bounding box predictions on the training set.Example class and bounding box predictions on the validation set.Just to double-check this really has to do with class imbalance, here are the actual frequencies:\n\nimageinfo %>% group_by(name)\n  %>% summarise(cnt = n()) \n  %>% arrange(desc(cnt))\n\n# A tibble: 20 x 2\n   name          cnt\n   <chr>       <int>\n 1 person       2705\n 2 car           826\n 3 chair         726\n 4 bottle        338\n 5 pottedplant   305\n 6 bird          294\n 7 dog           271\n 8 sofa          218\n 9 boat          208\n10 horse         207\n11 bicycle       202\n12 motorbike     193\n13 cat           191\n14 sheep         191\n15 tvmonitor     191\n16 cow           185\n17 train         158\n18 aeroplane     156\n19 diningtable   148\n20 bus           131\nTo get better performance, we’d need to find a successful way to deal with this. However, handling class imbalance in deep learning is a topic of its own, and here we want to build up in the direction of objection detection. So we’ll make a cut here and in an upcoming post, think about how we can classify and localize multiple objects in an image.\nConclusion\nWe have seen that single-object classification and localization are conceptually straightforward. The big question now is, are these approaches extensible to multiple objects? Or will new ideas have to come in? We’ll follow up on this giving a short overview of approaches and then, singling in on one of those and implementing it.\n\n\n\n",
    "preview": "posts/2018-11-05-naming-locating-objects/images/preds_train.jpg",
    "last_modified": "2024-11-21T15:49:21+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-10-22-mmd-vae/",
    "title": "Representation learning with MMD-VAE",
    "description": "Like GANs, variational autoencoders (VAEs) are often used to generate images. However, VAEs add an additional promise: namely, to model an underlying latent space. Here, we first look at a typical implementation that maximizes the evidence lower bound. Then, we compare it to one of the more recent competitors, MMD-VAE, from the Info-VAE (information maximizing VAE) family.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-10-22",
    "categories": [
      "TensorFlow/Keras",
      "Unsupervised Learning",
      "Image Recognition & Image Processing"
    ],
    "contents": "\nRecently, we showed how to generate images using generative adversarial networks (GANs). GANs may yield amazing results, but the contract there basically is: what you see is what you get.\nSometimes this may be all we want. In other cases, we may be more interested in actually modelling a domain. We don’t just want to generate realistic-looking samples - we want our samples to be located at specific coordinates in domain space.\nFor example, imagine our domain to be the space of facial expressions. Then our latent space might be conceived as two-dimensional: In accordance with underlying emotional states, expressions vary on a positive-negative scale. At the same time, they vary in intensity. Now if we trained a VAE on a set of facial expressions adequately covering the ranges, and it did in fact “discover” our hypothesized dimensions, we could then use it to generate previously-nonexisting incarnations of points (faces, that is) in latent space.\nVariational autoencoders are similar to probabilistic graphical models in that they assume a latent space that is responsible for the observations, but unobservable. They are similar to plain autoencoders in that they compress, and then decompress again, the input domain. In contrast to plain autoencoders though, the crucial point here is to devise a loss function that allows to obtain informative representations in latent space.\nIn a nutshell\nIn standard VAEs (Kingma and Welling 2013), the objective is to maximize the evidence lower bound (ELBO):\n\\[ELBO\\ = \\ E[log\\ p(x|z)]\\ -\\ KL(q(z)||p(z))\\]\nIn plain words and expressed in terms of how we use it in practice, the first component is the reconstruction loss we also see in plain (non-variational) autoencoders. The second is the Kullback-Leibler divergence between a prior imposed on the latent space (typically, a standard normal distribution) and the representation of latent space as learned from the data.\n\nFor a well-written and intuitive introduction to VAEs, including the why and how of their optimization, see this Tutorial on variational autoencoders (Doersch 2016).\nA major criticism regarding the traditional VAE loss is that it results in uninformative latent space. Alternatives include \\(\\beta\\)-VAE(Burgess et al. 2018), Info-VAE (Zhao, Song, and Ermon 2017), and more. The MMD-VAE(Zhao, Song, and Ermon 2017) implemented below is a subtype of Info-VAE that instead of making each representation in latent space as similar as possible to the prior, coerces the respective distributions to be as close as possible. Here MMD stands for maximum mean discrepancy, a similarity measure for distributions based on matching their respective moments. We explain this in more detail below.\n\nThe main author of the paper(Zhao, Song, and Ermon 2017) has a tutorial on his website explaining the reasons behind this choice of cost function in a very accessible way.\nOur objective today\nIn this post, we are first going to implement a standard VAE that strives to maximize the ELBO. Then, we compare its performance to that of an Info-VAE using the MMD loss.\nOur focus will be on inspecting the latent spaces and see if, and how, they differ as a consequence of the optimization criteria used.\nThe domain we’re going to model will be glamorous (fashion!), but for the sake of manageability, confined to size 28 x 28: We’ll compress and reconstruct images from the Fashion MNIST dataset that has been developed as a drop-in to MNIST.\nA standard variational autoencoder\nSeeing we haven’t used TensorFlow eager execution for some weeks, we’ll do the model in an eager way.\nIf you’re new to eager execution, don’t worry: As every new technique, it needs some getting accustomed to, but you’ll quickly find that many tasks are made easier if you use it. A simple yet complete, template-like example is available as part of the Keras documentation1.\n\nFor interesting applications using eager execution in combination with Keras, ranging from machine translation to neural style transfer, see the recent posts in the Eager category on this blog.\nSetup and data preparation\nAs usual, we start by making sure we’re using the TensorFlow implementation of Keras and enabling eager execution. Besides tensorflow and keras, we also load tfdatasets for use in data streaming.\nBy the way: No need to copy-paste any of the below code snippets. The two approaches are available among our Keras examples, namely, as eager_cvae.R and mmd_cvae.R.\n\nYou might find it interesting to compare non-eager Keras code implementing a variational autoencoder: see variational_autoencoder_deconv.R.\n\n\n# the following 5 lines have to be executed in this order\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nlibrary(tfdatasets)\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(glue)\n\n\nThe data comes conveniently with keras, all we need to do is the usual normalization and reshaping.\n\n\nfashion <- dataset_fashion_mnist()\n\nc(train_images, train_labels) %<-% fashion$train\nc(test_images, test_labels) %<-% fashion$test\n\ntrain_x <- train_images %>%\n  `/`(255) %>%\n  k_reshape(c(60000, 28, 28, 1))\n\ntest_x <- test_images %>% `/`(255) %>%\n  k_reshape(c(10000, 28, 28, 1))\n\n\nWhat do we need the test set for, given we are going to train an unsupervised (a better term being: semi-supervised) model? We’ll use it to see how (previously unknown) data points cluster together in latent space.\nNow prepare for streaming the data to keras:\n\n\nbuffer_size <- 60000\nbatch_size <- 100\nbatches_per_epoch <- buffer_size / batch_size\n\ntrain_dataset <- tensor_slices_dataset(train_x) %>%\n  dataset_shuffle(buffer_size) %>%\n  dataset_batch(batch_size)\n\ntest_dataset <- tensor_slices_dataset(test_x) %>%\n  dataset_batch(10000)\n\n\nNext up is defining the model.\nEncoder-decoder model\nThe model really is two models: the encoder and the decoder. As we’ll see shortly, in the standard version of the VAE there is a third component in between, performing the so-called reparameterization trick.\nThe encoder is a custom model, comprised of two convolutional layers and a dense layer. It returns the output of the dense layer split into two parts, one storing the mean of the latent variables, the other their variance.\n\n\nlatent_dim <- 2\n\nencoder_model <- function(name = NULL) {\n  \n  keras_model_custom(name = name, function(self) {\n    self$conv1 <-\n      layer_conv_2d(\n        filters = 32,\n        kernel_size = 3,\n        strides = 2,\n        activation = \"relu\"\n      )\n    self$conv2 <-\n      layer_conv_2d(\n        filters = 64,\n        kernel_size = 3,\n        strides = 2,\n        activation = \"relu\"\n      )\n    self$flatten <- layer_flatten()\n    self$dense <- layer_dense(units = 2 * latent_dim)\n    \n    function (x, mask = NULL) {\n      x %>%\n        self$conv1() %>%\n        self$conv2() %>%\n        self$flatten() %>%\n        self$dense() %>%\n        tf$split(num_or_size_splits = 2L, axis = 1L) \n    }\n  })\n}\n\n\nWe choose the latent space to be of dimension 2 - just because that makes visualization easy.\nWith more complex data, you will probably benefit from choosing a higher dimensionality here.\nSo the encoder compresses real data into estimates of mean and variance of the latent space.\nWe then “indirectly” sample from this distribution (the so-called reparameterization trick):\n\n\nreparameterize <- function(mean, logvar) {\n  eps <- k_random_normal(shape = mean$shape, dtype = tf$float64)\n  eps * k_exp(logvar * 0.5) + mean\n}\n\n\nThe sampled values will serve as input to the decoder, who will attempt to map them back to the original space.\nThe decoder is basically a sequence of transposed convolutions, upsampling until we reach a resolution of 28x28.\n\n\ndecoder_model <- function(name = NULL) {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$dense <- layer_dense(units = 7 * 7 * 32, activation = \"relu\")\n    self$reshape <- layer_reshape(target_shape = c(7, 7, 32))\n    self$deconv1 <-\n      layer_conv_2d_transpose(\n        filters = 64,\n        kernel_size = 3,\n        strides = 2,\n        padding = \"same\",\n        activation = \"relu\"\n      )\n    self$deconv2 <-\n      layer_conv_2d_transpose(\n        filters = 32,\n        kernel_size = 3,\n        strides = 2,\n        padding = \"same\",\n        activation = \"relu\"\n      )\n    self$deconv3 <-\n      layer_conv_2d_transpose(\n        filters = 1,\n        kernel_size = 3,\n        strides = 1,\n        padding = \"same\"\n      )\n    \n    function (x, mask = NULL) {\n      x %>%\n        self$dense() %>%\n        self$reshape() %>%\n        self$deconv1() %>%\n        self$deconv2() %>%\n        self$deconv3()\n    }\n  })\n}\n\n\nNote how the final deconvolution does not have the sigmoid activation you might have expected. This is because we will be using tf$nn$sigmoid_cross_entropy_with_logits when calculating the loss.\nSpeaking of losses, let’s inspect them now.\nLoss calculations\nOne way to implement the VAE loss is combining reconstruction loss (cross entropy, in the present case) and Kullback-Leibler divergence. In Keras, the latter is available directly as loss_kullback_leibler_divergence.\nHere, we follow a recent Google Colaboratory notebook in batch-estimating the complete ELBO instead (instead of just estimating reconstruction loss and computing the KL-divergence analytically):\n\\[ELBO \\ batch \\ estimate = log\\ p(x_{batch}|z_{sampled})+log\\ p(z)−log\\ q(z_{sampled}|x_{batch})\\]\nCalculation of the normal loglikelihood is packaged into a function so we can reuse it during the training loop.\n\nNote that we’re calculating with the log of the variance, instead of the variance, for reasons of numerical stability.\n\n\nnormal_loglik <- function(sample, mean, logvar, reduce_axis = 2) {\n  loglik <- k_constant(0.5, dtype = tf$float64) *\n    (k_log(2 * k_constant(pi, dtype = tf$float64)) +\n    logvar +\n    k_exp(-logvar) * (sample - mean) ^ 2)\n  - k_sum(loglik, axis = reduce_axis)\n}\n\n\nPeeking ahead some, during training we will compute the above as follows.\nFirst,\n\n\ncrossentropy_loss <- tf$nn$sigmoid_cross_entropy_with_logits(\n  logits = preds,\n  labels = x\n)\nlogpx_z <- - k_sum(crossentropy_loss)\n\n\nyields \\(log \\ p(x|z)\\), the loglikelihood of the reconstructed samples given values sampled from latent space (a.k.a. reconstruction loss).\nThen,\n\n\nlogpz <- normal_loglik(\n  z,\n  k_constant(0, dtype = tf$float64),\n  k_constant(0, dtype = tf$float64)\n)\n\n\ngives \\(log \\ p(z)\\), the prior loglikelihood of \\(z\\). The prior is assumed to be standard normal, as is most often the case with VAEs.\nFinally,\n\n\nlogqz_x <- normal_loglik(z, mean, logvar)\n\n\nvields \\(log \\ q(z|x)\\), the loglikelihood of the samples \\(z\\) given mean and variance computed from the observed samples \\(x\\).\nFrom these three components, we will compute the final loss as\n\n\nloss <- -k_mean(logpx_z + logpz - logqz_x)\n\n\nAfter this peaking ahead, let’s quickly finish the setup so we get ready for training.\nFinal setup\nBesides the loss, we need an optimizer that will strive to diminish it.\n\n\noptimizer <- tf$train$AdamOptimizer(1e-4)\n\n\nWe instantiate our models …\n\n\nencoder <- encoder_model()\ndecoder <- decoder_model()\n\n\nand set up checkpointing, so we can later restore trained weights.\n\n\ncheckpoint_dir <- \"./checkpoints_cvae\"\ncheckpoint_prefix <- file.path(checkpoint_dir, \"ckpt\")\ncheckpoint <- tf$train$Checkpoint(\n  optimizer = optimizer,\n  encoder = encoder,\n  decoder = decoder\n)\n\n\nFrom the training loop, we will, in certain intervals, also call three functions not reproduced here (but available in the code example): generate_random_clothes, used to generate clothes from random samples from the latent space; show_latent_space, that displays the complete test set in latent (2-dimensional, thus easily visualizable) space; and show_grid, that generates clothes according to input values systematically spaced out in a grid.\nLet’s start training! Actually, before we do that, let’s have a look at what these functions display before any training: Instead of clothes, we see random pixels. Latent space has no structure. And different types of clothes do not cluster together in latent space.\n\nTraining loop\nWe’re training for 50 epochs here. For each epoch, we loop over the training set in batches. For each batch, we follow the usual eager execution flow: Inside the context of a GradientTape, apply the model and calculate the current loss; then outside this context calculate the gradients and let the optimizer perform backprop.\nWhat’s special here is that we have two models that both need their gradients calculated and weights adjusted. This can be taken care of by a single gradient tape, provided we create it persistent.\nAfter each epoch, we save current weights and every ten epochs, we also save plots for later inspection.\n\n\nnum_epochs <- 50\n\nfor (epoch in seq_len(num_epochs)) {\n  iter <- make_iterator_one_shot(train_dataset)\n  \n  total_loss <- 0\n  logpx_z_total <- 0\n  logpz_total <- 0\n  logqz_x_total <- 0\n  \n  until_out_of_range({\n    x <-  iterator_get_next(iter)\n    \n    with(tf$GradientTape(persistent = TRUE) %as% tape, {\n      \n      c(mean, logvar) %<-% encoder(x)\n      z <- reparameterize(mean, logvar)\n      preds <- decoder(z)\n      \n      crossentropy_loss <-\n        tf$nn$sigmoid_cross_entropy_with_logits(logits = preds, labels = x)\n      logpx_z <-\n        - k_sum(crossentropy_loss)\n      logpz <-\n        normal_loglik(z,\n                      k_constant(0, dtype = tf$float64),\n                      k_constant(0, dtype = tf$float64)\n        )\n      logqz_x <- normal_loglik(z, mean, logvar)\n      loss <- -k_mean(logpx_z + logpz - logqz_x)\n      \n    })\n\n    total_loss <- total_loss + loss\n    logpx_z_total <- tf$reduce_mean(logpx_z) + logpx_z_total\n    logpz_total <- tf$reduce_mean(logpz) + logpz_total\n    logqz_x_total <- tf$reduce_mean(logqz_x) + logqz_x_total\n    \n    encoder_gradients <- tape$gradient(loss, encoder$variables)\n    decoder_gradients <- tape$gradient(loss, decoder$variables)\n    \n    optimizer$apply_gradients(\n      purrr::transpose(list(encoder_gradients, encoder$variables)),\n      global_step = tf$train$get_or_create_global_step()\n    )\n    optimizer$apply_gradients(\n      purrr::transpose(list(decoder_gradients, decoder$variables)),\n      global_step = tf$train$get_or_create_global_step()\n    )\n    \n  })\n  \n  checkpoint$save(file_prefix = checkpoint_prefix)\n  \n  cat(\n    glue(\n      \"Losses (epoch): {epoch}:\",\n      \"  {(as.numeric(logpx_z_total)/batches_per_epoch) %>% round(2)} logpx_z_total,\",\n      \"  {(as.numeric(logpz_total)/batches_per_epoch) %>% round(2)} logpz_total,\",\n      \"  {(as.numeric(logqz_x_total)/batches_per_epoch) %>% round(2)} logqz_x_total,\",\n      \"  {(as.numeric(total_loss)/batches_per_epoch) %>% round(2)} total\"\n    ),\n    \"\\n\"\n  )\n  \n  if (epoch %% 10 == 0) {\n    generate_random_clothes(epoch)\n    show_latent_space(epoch)\n    show_grid(epoch)\n  }\n}\n\n\nResults\nHow well did that work? Let’s see the kinds of clothes generated after 50 epochs.\n\nAlso, how disentangled (or not) are the different classes in latent space?\n\nAnd now watch different clothes morph into one another.\n\nHow good are these representations? This is hard to say when there is nothing to compare with.\nSo let’s dive into MMD-VAE and see how it does on the same dataset.\nMMD-VAE\nMMD-VAE promises to generate more informative latent features, so we would hope to see different behavior especially in the clustering and morphing plots.\nData setup is the same, and there are only very slight differences in the model. Please check out the complete code for this example, mmd_vae.R, as here we’ll just highlight the differences.\nDifferences in the model(s)\nThere are three differences as regards model architecture.\nOne, the encoder does not have to return the variance, so there is no need for tf$split. The encoder’s call method now just is\n\n\nfunction (x, mask = NULL) {\n  x %>%\n    self$conv1() %>%\n    self$conv2() %>%\n    self$flatten() %>%\n    self$dense() \n}\n\n\nBetween the encoder and the decoder, we don’t need the sampling step anymore, so there is no reparameterization.\nAnd since we won’t use tf$nn$sigmoid_cross_entropy_with_logits to compute the loss, we let the decoder apply the sigmoid in the last deconvolution layer:\n\n\nself$deconv3 <- layer_conv_2d_transpose(\n  filters = 1,\n  kernel_size = 3,\n  strides = 1,\n  padding = \"same\",\n  activation = \"sigmoid\"\n)\n\n\nLoss calculations\nNow, as expected, the big novelty is in the loss function.\nThe loss, maximum mean discrepancy (MMD), is based on the idea that two distributions are identical if and only if all moments are identical.\nConcretely, MMD is estimated using a kernel, such as the Gaussian kernel\n\\[k(z,z')=\\frac{e^{||z-z'||}}{2\\sigma^2}\\]\nto assess similarity between distributions.\nThe idea then is that if two distributions are identical, the average similarity between samples from each distribution should be identical to the average similarity between mixed samples from both distributions:\n\\[MMD(p(z)||q(z))=E_{p(z),p(z')}[k(z,z')]+E_{q(z),q(z')}[k(z,z')]−2E_{p(z),q(z')}[k(z,z')]\\]\nThe following code is a direct port of the author’s original TensorFlow code:\n\n\ncompute_kernel <- function(x, y) {\n  x_size <- k_shape(x)[1]\n  y_size <- k_shape(y)[1]\n  dim <- k_shape(x)[2]\n  tiled_x <- k_tile(\n    k_reshape(x, k_stack(list(x_size, 1, dim))),\n    k_stack(list(1, y_size, 1))\n  )\n  tiled_y <- k_tile(\n    k_reshape(y, k_stack(list(1, y_size, dim))),\n    k_stack(list(x_size, 1, 1))\n  )\n  k_exp(-k_mean(k_square(tiled_x - tiled_y), axis = 3) /\n          k_cast(dim, tf$float64))\n}\n\ncompute_mmd <- function(x, y, sigma_sqr = 1) {\n  x_kernel <- compute_kernel(x, x)\n  y_kernel <- compute_kernel(y, y)\n  xy_kernel <- compute_kernel(x, y)\n  k_mean(x_kernel) + k_mean(y_kernel) - 2 * k_mean(xy_kernel)\n}\n\n\nTraining loop\nThe training loop differs from the standard VAE example only in the loss calculations.\nHere are the respective lines:\n\n\n with(tf$GradientTape(persistent = TRUE) %as% tape, {\n      \n      mean <- encoder(x)\n      preds <- decoder(mean)\n      \n      true_samples <- k_random_normal(\n        shape = c(batch_size, latent_dim),\n        dtype = tf$float64\n      )\n      loss_mmd <- compute_mmd(true_samples, mean)\n      loss_nll <- k_mean(k_square(x - preds))\n      loss <- loss_nll + loss_mmd\n      \n    })\n\n\nSo we simply compute MMD loss as well as reconstruction loss, and add them up. No sampling is involved in this version.\nOf course, we are curious to see how well that worked!\nResults\nAgain, let’s look at some generated clothes first. It seems like edges are much sharper here.\n\nThe clusters too look more nicely spread out in the two dimensions. And, they are centered at (0,0), as we would have hoped for.\n\nFinally, let’s see clothes morph into one another. Here, the smooth, continuous evolutions are impressive!\nAlso, nearly all space is filled with meaningful objects, which hasn’t been the case above.\n\nMNIST\nFor curiosity’s sake, we generated the same kinds of plots after training on original MNIST.\nHere, there are hardly any differences visible in generated random digits after 50 epochs of training.\nLeft: random digits as generated after training with ELBO loss. Right: MMD loss.Also the differences in clustering are not that big.\nLeft: latent space as observed after training with ELBO loss. Right: MMD loss.But here too, the morphing looks much more organic with MMD-VAE.\nLeft: Morphing as observed after training with ELBO loss. Right: MMD loss.Conclusion\nTo us, this demonstrates impressively what big a difference the cost function can make when working with VAEs.\nAnother component open to experimentation may be the prior used for the latent space - see this talk for an overview of alternative priors and the “Variational Mixture of Posteriors” paper (Tomczak and Welling 2017) for a popular recent approach.\nFor both cost functions and priors, we expect effective differences to become way bigger still when we leave the controlled environment of (Fashion) MNIST and work with real-world datasets.\n\n\n\nBurgess, C. P., I. Higgins, A. Pal, L. Matthey, N. Watters, G. Desjardins, and A. Lerchner. 2018. “Understanding Disentangling in Beta-VAE.” ArXiv e-Prints, April. https://arxiv.org/abs/1804.03599.\n\n\nDoersch, C. 2016. “Tutorial on Variational Autoencoders.” ArXiv e-Prints, June. https://arxiv.org/abs/1606.05908.\n\n\nKingma, Diederik P., and Max Welling. 2013. “Auto-Encoding Variational Bayes.” CoRR abs/1312.6114.\n\n\nTomczak, Jakub M., and Max Welling. 2017. “VAE with a VampPrior.” CoRR abs/1705.07120.\n\n\nZhao, Shengjia, Jiaming Song, and Stefano Ermon. 2017. “InfoVAE: Information Maximizing Variational Autoencoders.” CoRR abs/1706.02262. http://arxiv.org/abs/1706.02262.\n\n\nNote: this link was updated as of November 29th, 2019, to point to the most up-to-date version.↩︎\n",
    "preview": "posts/2018-10-22-mmd-vae/images/thumb.png",
    "last_modified": "2024-11-21T15:52:24+00:00",
    "input_file": {},
    "preview_width": 468,
    "preview_height": 178
  },
  {
    "path": "posts/2018-10-11-activations-intro/",
    "title": "Winner takes all: A look at activations and cost functions",
    "description": "Why do we use the activations we use, and how do they relate to the cost functions they tend to co-appear with? In this post we provide a conceptual introduction.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-10-11",
    "categories": [
      "TensorFlow/Keras",
      "Concepts"
    ],
    "contents": "\nYou’re building a Keras model. If you haven’t been doing deep learning for so long, getting the output activations and cost function right might involve some memorization (or lookup). You might be trying to recall the general guidelines like so:\nSo with my cats and dogs, I’m doing 2-class classification, so I have to use sigmoid activation in the output layer, right, and then, it’s binary crossentropy for the cost function…\nOr: I’m doing classification on ImageNet, that’s multi-class, so that was softmax for activation, and then, cost should be categorical crossentropy…\nIt’s fine to memorize stuff like this, but knowing a bit about the reasons behind often makes things easier. So we ask: Why is it that these output activations and cost functions go together? And, do they always have to?\nIn a nutshell\nPut simply, we choose activations that make the network predict what we want it to predict.\nThe cost function is then determined by the model.\nThis is because neural networks are normally optimized using maximum likelihood, and depending on the distribution we assume for the output units, maximum likelihood yields different optimization objectives. All of these objectives then minimize the cross entropy (pragmatically: mismatch) between the true distribution and the predicted distribution.\n\nFor a more mathematical development of these topics, see sections 5.5 and 6.2 of Goodfellow et al., Deep Learning.(Goodfellow, Bengio, and Courville 2016)\nLet’s start with the simplest, the linear case.\nRegression\nFor the botanists among us, here’s a super simple network meant to predict sepal width from sepal length:\n\nIn case you’d like a more comprehensive introduction to doing regression with Keras, see the tutorial on the Keras website.\n\n\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 32) %>%\n  layer_dense(units = 1)\n\nmodel %>% compile(\n  optimizer = \"adam\", \n  loss = \"mean_squared_error\"\n)\n\nmodel %>% fit(\n  x = iris$Sepal.Length %>% as.matrix(),\n  y = iris$Sepal.Width %>% as.matrix(),\n  epochs = 50\n)\n\n\nOur model’s assumption here is that sepal width is normally distributed, given sepal length. Most often, we’re trying to predict the mean of a conditional Gaussian distribution:\n\\[p(y|\\mathbf{x} = N(y; \\mathbf{w}^t\\mathbf{h} + b)\\]\n\nThis formula assumes a single output unit.\nIn that case, the cost function that minimizes cross entropy (equivalently: optimizes maximum likelihood) is mean squared error.\nAnd that’s exactly what we’re using as a cost function above.\nAlternatively, we might wish to predict the median of that conditional distribution. In that case, we’d change the cost function to use mean absolute error:\n\n\nmodel %>% compile(\n  optimizer = \"adam\", \n  loss = \"mean_absolute_error\"\n)\n\n\nNow let’s move on beyond linearity.\nBinary classification\nWe’re enthusiastic bird watchers and want an application to notify us when there’s a bird in our garden - not when the neighbors landed their airplane, though. We’ll thus train a network to distinguish between two classes: birds and airplanes.\n\nFor a more detailed introduction to classification with Keras, see the tutorial on the Keras website.\n\n\n# Using the CIFAR-10 dataset that conveniently comes with Keras.\ncifar10 <- dataset_cifar10()\n\nx_train <- cifar10$train$x / 255\ny_train <- cifar10$train$y\n\nis_bird <- cifar10$train$y == 2\nx_bird <- x_train[is_bird, , ,]\ny_bird <- rep(0, 5000)\n\nis_plane <- cifar10$train$y == 0\nx_plane <- x_train[is_plane, , ,]\ny_plane <- rep(1, 5000)\n\nx <- abind::abind(x_bird, x_plane, along = 1)\ny <- c(y_bird, y_plane)\n\nmodel <- keras_model_sequential() %>%\n  layer_conv_2d(\n    filter = 8,\n    kernel_size = c(3, 3),\n    padding = \"same\",\n    input_shape = c(32, 32, 3),\n    activation = \"relu\"\n  ) %>%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_conv_2d(\n    filter = 8,\n    kernel_size = c(3, 3),\n    padding = \"same\",\n    activation = \"relu\"\n  ) %>%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %>%\nlayer_flatten() %>%\n  layer_dense(units = 32, activation = \"relu\") %>%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %>% compile(\n  optimizer = \"adam\", \n  loss = \"binary_crossentropy\", \n  metrics = \"accuracy\"\n)\n\nmodel %>% fit(\n  x = x,\n  y = y,\n  epochs = 50\n)\n\n\nAlthough we normally talk about “binary classification,” the way the outcome is usually modeled is as a Bernoulli random variable, conditioned on the input data. So:\n\\[P(y = 1|\\mathbf{x}) = p, \\ 0\\leq p\\leq1\\]\nA Bernoulli random variable takes on values between \\(0\\) and \\(1\\). So that’s what our network should produce.\nOne idea might be to just clip all values of \\(\\mathbf{w}^t\\mathbf{h} + b\\) outside that interval. But if we do this, the gradient in these regions will be \\(0\\): The network cannot learn.\nA better way is to squish the complete incoming interval into the range (0,1), using the logistic sigmoid function\n\\[ \\sigma(x) = \\frac{1}{1 + e^{(-x)}} \\]\nThe sigmoid function squishes its input into the interval (0,1).As you can see, the sigmoid function saturates when its input gets very large, or very small. Is this problematic?\nIt depends. In the end, what we care about is if the cost function saturates. Were we to choose mean squared error here, as in the regression task above, that’s indeed what could happen.1\nHowever, if we follow the general principle of maximum likelihood/cross entropy, the loss will be\n\\[- log P (y|\\mathbf{x})\\]\n\nIf you need the details, see section 6.2.2.2 in Goodfellow et al.\nwhere the \\(log\\) undoes the \\(exp\\) in the sigmoid.\nIn Keras, the corresponding loss function is binary_crossentropy. For a single item, the loss will be\n\\(- log(p)\\) when the ground truth is 1\n\\(- log(1-p)\\) when the ground truth is 0\n\nHere p is the predicted probability, i.e., the output activations of the network.\nHere, you can see that when for an individual example, the network predicts the wrong class and is highly confident about it, this example will contributely very strongly to the loss.\nCross entropy penalizes wrong predictions most when they are highly confident.What happens when we distinguish between more than two classes?\nMulti-class classification\nCIFAR-10 has 10 classes; so now we want to decide which of 10 object classes is present in the image.\nHere first is the code: Not many differences to the above, but note the changes in activation and cost function.\n\n\ncifar10 <- dataset_cifar10()\n\nx_train <- cifar10$train$x / 255\ny_train <- cifar10$train$y\n\nmodel <- keras_model_sequential() %>%\n  layer_conv_2d(\n    filter = 8,\n    kernel_size = c(3, 3),\n    padding = \"same\",\n    input_shape = c(32, 32, 3),\n    activation = \"relu\"\n  ) %>%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_conv_2d(\n    filter = 8,\n    kernel_size = c(3, 3),\n    padding = \"same\",\n    activation = \"relu\"\n  ) %>%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_flatten() %>%\n  layer_dense(units = 32, activation = \"relu\") %>%\n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel %>% compile(\n  optimizer = \"adam\",\n  loss = \"sparse_categorical_crossentropy\",\n  metrics = \"accuracy\"\n)\n\nmodel %>% fit(\n  x = x_train,\n  y = y_train,\n  epochs = 50\n)\n\n\nSo now we have softmax combined with categorical crossentropy. Why?\nAgain, we want a valid probability distribution: Probabilities for all disjunct events should sum to 1.\nCIFAR-10 has one object per image; so events are disjunct. Then we have a single-draw multinomial distribution (popularly known as “Multinoulli,” mostly due to Murphy’s Machine learning(Murphy 2012)) that can be modeled by the softmax activation:\n\\[softmax(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_j{e^{z_j}}}\\]\nJust as the sigmoid, the softmax can saturate. In this case, that will happen when differences between outputs become very big.\nAlso like with the sigmoid, a \\(log\\) in the cost function undoes the \\(exp\\) that’s responsible for saturation:\n\\[log\\ softmax(\\mathbf{z})_i = z_i - log\\sum_j{e^{z_j}}\\]\nHere \\(z_i\\) is the class we’re estimating the probability of - we see that its contribution to the loss is linear and thus, can never saturate.\nIn Keras, the loss function that does this for us is called categorical_crossentropy. We use sparse_categorical_crossentropy in the code which is the same as categorical_crossentropy but does not need conversion of integer labels to one-hot vectors.\nLet’s take a closer look at what softmax does. Assume these are the raw outputs of our 10 output units:\nSimulated output before application of softmax.Now this is what the normalized probability distribution looks like after taking the softmax:\nFinal output after softmax.Do you see where the winner takes all in the title comes from? This is an important point to keep in mind: Activation functions are not just there to produce certain desired distributions; they can also change relationships between values.\nConclusion\nWe started this post alluding to common heuristics, such as “for multi-class classification, we use softmax activation, combined with categorical crossentropy as the loss function.” Hopefully, we’ve succeeded in showing why these heuristics make sense.\nHowever, knowing that background, you can also infer when these rules do not apply. For example, say you want to detect several objects in an image. In that case, the winner-takes-all strategy is not the most useful, as we don’t want to exaggerate differences between candidates. So here, we’d use sigmoid on all output units instead, to determine a probability of presence per object.\n\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.\n\n\nMurphy, Kevin. 2012. Machine Learning: A Probabilistic Perspective. MIT Press.\n\n\nThe actual outcome depends on the task. In the above simple classification example, training with mean squared error will attain similar accuracy in similar time.↩︎\n",
    "preview": "posts/2018-10-11-activations-intro/images/output.png",
    "last_modified": "2024-11-21T15:53:05+00:00",
    "input_file": {},
    "preview_width": 800,
    "preview_height": 384
  },
  {
    "path": "posts/2018-10-02-eager-wrapup/",
    "title": "More flexible models with TensorFlow eager execution and Keras",
    "description": "Advanced applications like generative adversarial networks, neural style transfer, and the attention mechanism ubiquitous in natural language processing used to be not-so-simple to implement with the Keras declarative coding paradigm. Now, with the advent of TensorFlow eager execution, things have changed. This post explores using eager execution with R.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-10-02",
    "categories": [
      "TensorFlow/Keras"
    ],
    "contents": "\nIf you have used Keras to create neural networks you are no doubt familiar with the Sequential API, which represents models as a linear stack of layers. The Functional API gives you additional options: Using separate input layers, you can combine text input with tabular data. Using multiple outputs, you can perform regression and classification at the same time. Furthermore, you can reuse layers within and between models.\nWith TensorFlow eager execution, you gain even more flexibility. Using custom models, you define the forward pass through the model completely ad libitum. This means that a lot of architectures get a lot easier to implement, including the applications mentioned above: generative adversarial networks, neural style transfer, various forms of sequence-to-sequence models.\nIn addition, because you have direct access to values, not tensors, model development and debugging are greatly sped up.\nHow does it work?\nIn eager execution, operations are not compiled into a graph, but directly defined in your R code. They return values, not symbolic handles to nodes in a computational graph - meaning, you don’t need access to a TensorFlow session to evaluate them.\n\n\nm1 <- matrix(1:8, nrow = 2, ncol = 4)\nm2 <- matrix(1:8, nrow = 4, ncol = 2)\ntf$matmul(m1, m2)\n\n\ntf.Tensor(\n[[ 50 114]\n [ 60 140]], shape=(2, 2), dtype=int32)\nEager execution, recent though it is, is already supported in the current CRAN releases of keras and tensorflow.\nThe eager execution guide describes the workflow in detail.\nHere’s a quick outline:\nYou define a model, an optimizer, and a loss function.\nData is streamed via tfdatasets, including any preprocessing such as image resizing.\nThen, model training is just a loop over epochs, giving you complete freedom over when (and whether) to execute any actions.\nHow does backpropagation work in this setup? The forward pass is recorded by a GradientTape, and during the backward pass we explicitly calculate gradients of the loss with respect to the model’s weights. These weights are then adjusted by the optimizer.\n\n\nwith(tf$GradientTape() %as% tape, {\n     \n  # run model on current batch\n  preds <- model(x)\n \n  # compute the loss\n  loss <- mse_loss(y, preds, x)\n  \n})\n    \n# get gradients of loss w.r.t. model weights\ngradients <- tape$gradient(loss, model$variables)\n\n# update model weights\noptimizer$apply_gradients(\n  purrr::transpose(list(gradients, model$variables)),\n  global_step = tf$train$get_or_create_global_step()\n)\n\n\nSee the eager execution guide for a complete example. Here, we want to answer the question: Why are we so excited about it? At least three things come to mind:\nThings that used to be complicated become much easier to accomplish.\nModels are easier to develop, and easier to debug.\nThere is a much better match between our mental models and the code we write.\nWe’ll illustrate these points using a set of eager execution case studies that have recently appeared on this blog.\nComplicated stuff made easier\nA good example of architectures that become much easier to define with eager execution are attention models.\nAttention is an important ingredient of sequence-to-sequence models, e.g. (but not only) in machine translation.\nWhen using LSTMs on both the encoding and the decoding sides, the decoder, being a recurrent layer, knows about the sequence it has generated so far. It also (in all but the simplest models) has access to the complete input sequence. But where in the input sequence is the piece of information it needs to generate the next output token?\nIt is this question that attention is meant to address.\nNow consider implementing this in code. Each time it is called to produce a new token, the decoder needs to get current input from the attention mechanism. This means we can’t just squeeze an attention layer between the encoder and the decoder LSTM. Before the advent of eager execution, a solution would have been to implement this in low-level TensorFlow code. With eager execution and custom models, we can just use Keras.\n\nAttention is not just relevant to sequence-to-sequence problems, though. In image captioning, the output is a sequence, while the input is a complete image. When generating a caption, attention is used to focus on parts of the image relevant to different time steps in the text-generating process.\n\nEasy inspection\nIn terms of debuggability, just using custom models (without eager execution) already simplifies things.\nIf we have a custom model like simple_dot from the recent embeddings post and are unsure if we’ve got the shapes correct, we can simply add logging statements, like so:\n\n\n\nfunction(x, mask = NULL) {\n  \n  users <- x[, 1]\n  movies <- x[, 2]\n  \n  user_embedding <- self$user_embedding(users)\n  cat(dim(user_embedding), \"\\n\")\n  \n  movie_embedding <- self$movie_embedding(movies)\n  cat(dim(movie_embedding), \"\\n\")\n  \n  dot <- self$dot(list(user_embedding, movie_embedding))\n  cat(dim(dot), \"\\n\")\n  dot\n}\n\n\nWith eager execution, things get even better: We can print the tensors’ values themselves.1\nBut convenience does not end there. In the training loop we showed above, we can obtain losses, model weights, and gradients just by printing them.\nFor example, add a line after the call to tape$gradient to print the gradients for all layers as a list.\n\n\ngradients <- tape$gradient(loss, model$variables)\nprint(gradients)\n\n\nMatching the mental model\nIf you’ve read Deep Learning with R, you know that it’s possible to program less straightforward workflows, such as those required for training GANs or doing neural style transfer, using the Keras functional API. However, the graph code does not make it easy to keep track of where you are in the workflow.\nNow compare the example from the generating digits with GANs post. Generator and discriminator each get set up as actors in a drama:\n\n\ngenerator <- function(name = NULL) {\n  keras_model_custom(name = name, function(self) {\n    # ...\n  }\n}\n\n\ndiscriminator <- function(name = NULL) {\n  keras_model_custom(name = name, function(self) {\n    # ...\n  }\n}\n\nBoth are informed about their respective loss functions and optimizers.\nThen, the duel starts. The training loop is just a succession of generator actions, discriminator actions, and backpropagation through both models. No need to worry about freezing/unfreezing weights in the appropriate places.\n\nwith(tf$GradientTape() %as% gen_tape, { with(tf$GradientTape() %as% disc_tape, {\n  \n # generator action\n generated_images <- generator(# ...\n   \n # discriminator assessments\n disc_real_output <- discriminator(# ... \n disc_generated_output <- discriminator(# ...\n      \n # generator loss\n gen_loss <- generator_loss(# ...                        \n # discriminator loss\n disc_loss <- discriminator_loss(# ...\n   \n})})\n   \n# calcucate generator gradients   \ngradients_of_generator <- gen_tape$gradient(#...\n  \n# calcucate discriminator gradients   \ngradients_of_discriminator <- disc_tape$gradient(# ...\n \n# apply generator gradients to model weights       \ngenerator_optimizer$apply_gradients(# ...\n\n# apply discriminator gradients to model weights \ndiscriminator_optimizer$apply_gradients(# ...\n\nThe code ends up so close to how we mentally picture the situation that hardly any memorization is needed to keep in mind the overall design.\nRelatedly, this way of programming lends itself to extensive modularization. This is illustrated by the second post on GANs that includes U-Net like downsampling and upsampling steps.\n\nHere, the downsampling and upsampling layers are each factored out into their own models\n\ndownsample <- function(# ...\n  keras_model_custom(name = NULL, function(self) { # ...\n\nsuch that they can be readably composed in the generator’s call method:\n\n# model fields\nself$down1 <- downsample(# ...\nself$down2 <- downsample(# ...\n# ...\n# ...\n\n# call method\nfunction(x, mask = NULL, training = TRUE) {       \n     \n  x1 <- x %>% self$down1(training = training)         \n  x2 <- self$down2(x1, training = training)           \n  # ...\n  # ...\n\nWrapping up\nEager execution is still a very recent feature and under development. We are convinced that many interesting use cases will still turn up as this paradigm gets adopted more widely among deep learning practitioners.\nHowever, now already we have a list of use cases illustrating the vast options, gains in usability, modularization and elegance offered by eager execution code.\nFor quick reference, these cover:\nNeural machine translation with attention. This post provides a detailed introduction to eager execution and its building blocks, as well as an in-depth explanation of the attention mechanism used. Together with the next one, it occupies a very special role in this list: It uses eager execution to solve a problem that otherwise could only be solved with hard-to-read, hard-to-write low-level code.\nImage captioning with attention.\nThis post builds on the first in that it does not re-explain attention in detail; however, it ports the concept to spatial attention applied over image regions.\nGenerating digits with convolutional generative adversarial networks (DCGANs). This post introduces using two custom models, each with their associated loss functions and optimizers, and having them go through forward- and backpropagation in sync. It is perhaps the most impressive example of how eager execution simplifies coding by better alignment to our mental model of the situation.\nImage-to-image translation with pix2pix is another application of generative adversarial networks, but uses a more complex architecture based on U-Net-like downsampling and upsampling. It nicely demonstrates how eager execution allows for modular coding, rendering the final program much more readable.\nNeural style transfer. Finally, this post reformulates the style transfer problem in an eager way, again resulting in readable, concise code.\nWhen diving into these applications, it is a good idea to also refer to the eager execution guide so you don’t lose sight of the forest for the trees.\nWe are excited about the use cases our readers will come up with!\n\nNote that the embeddings example uses standard (graph) execution; refactoring would be needed in order to enable eager execution on it.↩︎\n",
    "preview": "posts/2018-10-02-eager-wrapup/images/m.png",
    "last_modified": "2024-11-21T15:50:07+00:00",
    "input_file": {},
    "preview_width": 384,
    "preview_height": 126
  },
  {
    "path": "posts/2018-09-26-embeddings-recommender/",
    "title": "Collaborative filtering with embeddings",
    "description": "Embeddings are not just for use in natural language processing. Here we apply embeddings to a common task in collaborative filtering - predicting user ratings - and on our way, strive for a better understanding of what an embedding layer really does.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-09-26",
    "categories": [
      "TensorFlow/Keras",
      "Tabular Data"
    ],
    "contents": "\nWhat’s your first association when you read the word embeddings? For most of us, the answer will probably be word embeddings, or word vectors. A quick search for recent papers on arxiv shows what else can be embedded: equations(Krstovski and Blei 2018), vehicle sensor data(Hallac et al. 2018), graphs(Ahmed et al. 2018), code(Alon et al. 2018), spatial data(Jean et al. 2018), biological entities(Zohra Smaili, Gao, and Hoehndorf 2018) … - and what not.\nWhat is so attractive about this concept? Embeddings incorporate the concept of distributed representations, an encoding of information not at specialized locations (dedicated neurons, say), but as a pattern of activations spread out over a network.\nNo better source to cite than Geoffrey Hinton, who played an important role in the development of the concept(Rumelhart, McClelland, and PDP Research Group 1986):\n\nDistributed representation means a many to many relationship between two types of representation (such as concepts and neurons).\nEach concept is represented by many neurons. Each neuron participates in the representation of many concepts.1\n\nThe advantages are manifold. Perhaps the most famous effect of using embeddings is that we can learn and make use of semantic similarity.\nLet’s take a task like sentiment analysis. Initially, what we feed the network are sequences of words, essentially encoded as factors. In this setup, all words are equidistant: Orange is as different from kiwi as it is from thunderstorm. An ensuing embedding layer then maps these representations to dense vectors of floating point numbers, which can be checked for mutual similarity via various similarity measures such as cosine distance.\nWe hope that when we feed these “meaningful” vectors to the next layer(s), better classification will result.\nIn addition, we may be interested in exploring that semantic space for its own sake, or use it in multi-modal transfer learning (Frome et al. 2013).\nIn this post, we’d like to do two things: First, we want to show an interesting application of embeddings beyond natural language processing, namely, their use in collaborative filtering. In this, we follow ideas developed in lesson5-movielens.ipynb which is part of fast.ai’s Deep Learning for Coders class.\nSecond, to gather more intuition, we’d like to take a look “under the hood” at how a simple embedding layer can be implemented.\nSo first, let’s jump into collaborative filtering. Just like the notebook that inspired us, we’ll predict movie ratings. We will use the 2016 ml-latest-small dataset from MovieLens that contains ~100000 ratings of ~9900 movies, rated by ~700 users.\nEmbeddings for collaborative filtering\nIn collaborative filtering, we try to generate recommendations based not on elaborate knowledge about our users and not on detailed profiles of our products, but on how users and products go together. Is product \\(\\mathbf{p}\\) a fit for user \\(\\mathbf{u}\\)? If so, we’ll recommend it.\nOften, this is done via matrix factorization. See, for example, this nice article by the winners of the 2009 Netflix prize, introducing the why and how of matrix factorization techniques as used in collaborative filtering.\nHere’s the general principle. While other techniques like non-negative matrix factorization may be more popular, this diagram of singular value decomposition (SVD) found on Facebook Research is particularly instructive.\nFigure from https://research.fb.com/fast-randomized-svd/The diagram takes its example from the context of text analysis, assuming a co-occurrence matrix of hashtags and users (\\(\\mathbf{A}\\)).\nAs stated above, we’ll instead work with a dataset of movie ratings.\nWere we doing matrix factorization, we would need to somehow address the fact that not every user has rated every movie. As we’ll be using embeddings instead, we won’t have that problem. For the sake of argumentation, though, let’s assume for a moment the ratings were a matrix, not a dataframe in tidy format.\nIn that case, \\(\\mathbf{A}\\) would store the ratings, with each row containing the ratings one user gave to all movies.\nThis matrix then gets decomposed into three matrices:\n\\(\\mathbf{\\Sigma}\\) stores the importance of the latent factors governing the relationship between users and movies.\n\\(\\mathbf{U}\\) contains information on how users score on these latent factors. It’s a representation (embedding) of users by the ratings they gave to the movies.\n\\(\\mathbf{V}\\) stores how movies score on these same latent factors. It’s a representation (embedding) of movies by how they got rated by said users.\nAs soon as we have a representation of movies  as well as users  in the same latent space, we can determine their mutual fit by a simple dot product \\(\\mathbf{m^ t}\\mathbf{u}\\). Assuming the user and movie vectors have been normalized to length 1, this is equivalent to calculating the cosine similarity\n\\[cos(\\theta) = \\frac{\\mathbf{x^ t}\\mathbf{y}}{\\mathbf{||x||}\\space\\mathbf{||y||}}\\]\nWhat does all this have to do with embeddings?\nWell, the same overall principles apply when we work with user resp. movie embeddings, instead of vectors obtained from matrix factorization. We’ll have one layer_embedding for users, one layer_embedding for movies, and a layer_lambda that calculates the dot product.\nHere’s a minimal custom model that does exactly this:2\n\n\nsimple_dot <- function(embedding_dim,\n                       n_users,\n                       n_movies,\n                       name = \"simple_dot\") {\n  \n  keras_model_custom(name = name, function(self) {\n    self$user_embedding <-\n      layer_embedding(\n        input_dim = n_users + 1,\n        output_dim = embedding_dim,\n        embeddings_initializer = initializer_random_uniform(minval = 0, maxval = 0.05),\n        name = \"user_embedding\"\n      )\n    self$movie_embedding <-\n      layer_embedding(\n        input_dim = n_movies + 1,\n        output_dim = embedding_dim,\n        embeddings_initializer = initializer_random_uniform(minval = 0, maxval = 0.05),\n        name = \"movie_embedding\"\n      )\n    self$dot <-\n      layer_lambda(\n        f = function(x) {\n          k_batch_dot(x[[1]], x[[2]], axes = 2)\n        }\n      )\n    \n    function(x, mask = NULL) {\n      users <- x[, 1]\n      movies <- x[, 2]\n      user_embedding <- self$user_embedding(users)\n      movie_embedding <- self$movie_embedding(movies)\n      self$dot(list(user_embedding, movie_embedding))\n    }\n  })\n}\n\n\nWe’re still missing the data though! Let’s load it.\nBesides the ratings themselves, we’ll also get the titles from movies.csv.\n\n\ndata_dir <- \"ml-latest-small\"\nmovies <- read_csv(file.path(data_dir, \"movies.csv\"))\nratings <- read_csv(file.path(data_dir, \"ratings.csv\"))\n\n\nWhile user ids have no gaps in this sample, that’s different for movie ids. We therefore convert them to consecutive numbers, so we can later specify an adequate size for the lookup matrix.\n\n\ndense_movies <- ratings %>% select(movieId) %>% distinct() %>% rowid_to_column()\nratings <- ratings %>% inner_join(dense_movies) %>% rename(movieIdDense = rowid)\nratings <- ratings %>% inner_join(movies) %>% select(userId, movieIdDense, rating, title, genres)\n\n\nLet’s take a note, then, of how many users resp. movies we have.\n\n\nn_movies <- ratings %>% select(movieIdDense) %>% distinct() %>% nrow()\nn_users <- ratings %>% select(userId) %>% distinct() %>% nrow()\n\n\nWe’ll split off 20% of the data for validation.\nAfter training, probably all users will have been seen by the network, while very likely, not all movies will have occurred in the training sample.\n\n\ntrain_indices <- sample(1:nrow(ratings), 0.8 * nrow(ratings))\ntrain_ratings <- ratings[train_indices,]\nvalid_ratings <- ratings[-train_indices,]\n\nx_train <- train_ratings %>% select(c(userId, movieIdDense)) %>% as.matrix()\ny_train <- train_ratings %>% select(rating) %>% as.matrix()\nx_valid <- valid_ratings %>% select(c(userId, movieIdDense)) %>% as.matrix()\ny_valid <- valid_ratings %>% select(rating) %>% as.matrix()\n\n\nTraining a simple dot product model\nWe’re ready to start the training process. Feel free to experiment with different embedding dimensionalities.\n\n\nembedding_dim <- 64\n\nmodel <- simple_dot(embedding_dim, n_users, n_movies)\n\nmodel %>% compile(\n  loss = \"mse\",\n  optimizer = \"adam\"\n)\n\nhistory <- model %>% fit(\n  x_train,\n  y_train,\n  epochs = 10,\n  batch_size = 32,\n  validation_data = list(x_valid, y_valid),\n  callbacks = list(callback_early_stopping(patience = 2))\n)\n\n\nHow well does this work? Final RMSE (the square root of the MSE loss we were using) on the validation set is around 1.08 , while popular benchmarks (e.g., of the LibRec recommender system) lie around 0.91. Also, we’re overfitting early. It looks like we need a slightly more sophisticated system.\nTraining curve for simple dot product modelAccounting for user and movie biases\nA problem with our method is that we attribute the rating as a whole to user-movie interaction.\nHowever, some users are intrinsically more critical, while others tend to be more lenient. Analogously, films differ by average rating.\nWe hope to get better predictions when factoring in these biases.\nConceptually, we then calculate a prediction like this:\n\\[pred =  avg + bias_m + bias_u + \\mathbf{m^ t}\\mathbf{u}\\]\nThe corresponding Keras model gets just slightly more complex. In addition to the user and movie embeddings we’ve already been working with, the below model embeds the average user and the average movie in 1-d space. We then add both biases to the dot product encoding user-movie interaction.\nA sigmoid activation normalizes to a value between 0 and 1, which then gets mapped back to the original space.\nNote how in this model, we also use dropout on the user and movie embeddings (again, the best dropout rate is open to experimentation).\n\n\nmax_rating <- ratings %>% summarise(max_rating = max(rating)) %>% pull()\nmin_rating <- ratings %>% summarise(min_rating = min(rating)) %>% pull()\n\ndot_with_bias <- function(embedding_dim,\n                          n_users,\n                          n_movies,\n                          max_rating,\n                          min_rating,\n                          name = \"dot_with_bias\"\n                          ) {\n  keras_model_custom(name = name, function(self) {\n    \n    self$user_embedding <-\n      layer_embedding(input_dim = n_users + 1,\n                      output_dim = embedding_dim,\n                      name = \"user_embedding\")\n    self$movie_embedding <-\n      layer_embedding(input_dim = n_movies + 1,\n                      output_dim = embedding_dim,\n                      name = \"movie_embedding\")\n    self$user_bias <-\n      layer_embedding(input_dim = n_users + 1,\n                      output_dim = 1,\n                      name = \"user_bias\")\n    self$movie_bias <-\n      layer_embedding(input_dim = n_movies + 1,\n                      output_dim = 1,\n                      name = \"movie_bias\")\n    self$user_dropout <- layer_dropout(rate = 0.3)\n    self$movie_dropout <- layer_dropout(rate = 0.6)\n    self$dot <-\n      layer_lambda(\n        f = function(x)\n          k_batch_dot(x[[1]], x[[2]], axes = 2),\n        name = \"dot\"\n      )\n    self$dot_bias <-\n      layer_lambda(\n        f = function(x)\n          k_sigmoid(x[[1]] + x[[2]] + x[[3]]),\n        name = \"dot_bias\"\n      )\n    self$pred <- layer_lambda(\n      f = function(x)\n        x * (self$max_rating - self$min_rating) + self$min_rating,\n      name = \"pred\"\n    )\n    self$max_rating <- max_rating\n    self$min_rating <- min_rating\n    \n    function(x, mask = NULL) {\n      \n      users <- x[, 1]\n      movies <- x[, 2]\n      user_embedding <-\n        self$user_embedding(users) %>% self$user_dropout()\n      movie_embedding <-\n        self$movie_embedding(movies) %>% self$movie_dropout()\n      dot <- self$dot(list(user_embedding, movie_embedding))\n      dot_bias <-\n        self$dot_bias(list(dot, self$user_bias(users), self$movie_bias(movies)))\n      self$pred(dot_bias)\n    }\n  })\n}\n\n\nHow well does this model perform?\n\n\nmodel <- dot_with_bias(embedding_dim,\n                       n_users,\n                       n_movies,\n                       max_rating,\n                       min_rating)\n\nmodel %>% compile(\n  loss = \"mse\",\n  optimizer = \"adam\"\n)\n\nhistory <- model %>% fit(\n  x_train,\n  y_train,\n  epochs = 10,\n  batch_size = 32,\n  validation_data = list(x_valid, y_valid),\n  callbacks = list(callback_early_stopping(patience = 2))\n)\n\n\nNot only does it overfit later, it actually reaches a way better RMSE of 0.88 on the validation set!\nTraining curve for dot product model with biasesSpending some time on hyperparameter optimization could very well lead to even better results.\nAs this post focuses on the conceptual side though, we want to see what else we can do with those embeddings.\nEmbeddings: a closer look\nWe can easily extract the embedding matrices from the respective layers. Let’s do this for movies now.\n\n\nmovie_embeddings <- (model %>% get_layer(\"movie_embedding\") %>% get_weights())[[1]]\n\n\nHow are they distributed? Here’s a heatmap of the first 20 movies. (Note how we increment the row indices by 1, because the very first row in the embedding matrix belongs to a movie id 0 which does not exist in our dataset.)\nWe see that the embeddings look rather uniformly distributed between -0.5 and 0.5.\n\n\nlevelplot(\n  t(movie_embeddings[2:21, 1:64]),\n  xlab = \"\",\n  ylab = \"\",\n  scale = (list(draw = FALSE)))\n\n\nEmbeddings for first 20 moviesNaturally, we might be interested in dimensionality reduction, and see how specific movies score on the dominant factors.\nA possible way to achieve this is PCA:\n\n\nmovie_pca <- movie_embeddings %>% prcomp(center = FALSE)\ncomponents <- movie_pca$x %>% as.data.frame() %>% rowid_to_column()\n\nplot(movie_pca)\n\n\nPCA: Variance explained by componentLet’s just look at the first principal component as the second one already explains much less variance.\nHere are the 10 movies (out of all that were rated at least 20 times) that scored lowest on the first factor:\n\n\nratings_with_pc12 <-\n  ratings %>% inner_join(components %>% select(rowid, PC1, PC2),\n                         by = c(\"movieIdDense\" = \"rowid\"))\n\nratings_grouped <-\n  ratings_with_pc12 %>%\n  group_by(title) %>%\n  summarize(\n    PC1 = max(PC1),\n    PC2 = max(PC2),\n    rating = mean(rating),\n    genres = max(genres),\n    num_ratings = n()\n  )\n\nratings_grouped %>% filter(num_ratings > 20) %>% arrange(PC1) %>% print(n = 10)\n\n\n# A tibble: 1,247 x 6\n   title                                   PC1      PC2 rating genres                   num_ratings\n   <chr>                                 <dbl>    <dbl>  <dbl> <chr>                          <int>\n 1 Starman (1984)                       -1.15  -0.400     3.45 Adventure|Drama|Romance…          22\n 2 Bulworth (1998)                      -0.820  0.218     3.29 Comedy|Drama|Romance              31\n 3 Cable Guy, The (1996)                -0.801 -0.00333   2.55 Comedy|Thriller                   59\n 4 Species (1995)                       -0.772 -0.126     2.81 Horror|Sci-Fi                     55\n 5 Save the Last Dance (2001)           -0.765  0.0302    3.36 Drama|Romance                     21\n 6 Spanish Prisoner, The (1997)         -0.760  0.435     3.91 Crime|Drama|Mystery|Thr…          23\n 7 Sgt. Bilko (1996)                    -0.757  0.249     2.76 Comedy                            29\n 8 Naked Gun 2 1/2: The Smell of Fear,… -0.749  0.140     3.44 Comedy                            27\n 9 Swordfish (2001)                     -0.694  0.328     2.92 Action|Crime|Drama                33\n10 Addams Family Values (1993)          -0.693  0.251     3.15 Children|Comedy|Fantasy           73\n# ... with 1,237 more rows\nAnd here, inversely, are those that scored highest:\n\n\nratings_grouped %>% filter(num_ratings > 20) %>% arrange(desc(PC1)) %>% print(n = 10)\n\n\n A tibble: 1,247 x 6\n   title                                PC1        PC2 rating genres                    num_ratings\n   <chr>                              <dbl>      <dbl>  <dbl> <chr>                           <int>\n 1 Graduate, The (1967)                1.41  0.0432      4.12 Comedy|Drama|Romance               89\n 2 Vertigo (1958)                      1.38 -0.0000246   4.22 Drama|Mystery|Romance|Th…          69\n 3 Breakfast at Tiffany's (1961)       1.28  0.278       3.59 Drama|Romance                      44\n 4 Treasure of the Sierra Madre, The…  1.28 -0.496       4.3  Action|Adventure|Drama|W…          30\n 5 Boot, Das (Boat, The) (1981)        1.26  0.238       4.17 Action|Drama|War                   51\n 6 Flintstones, The (1994)             1.18  0.762       2.21 Children|Comedy|Fantasy            39\n 7 Rock, The (1996)                    1.17 -0.269       3.74 Action|Adventure|Thriller         135\n 8 In the Heat of the Night (1967)     1.15 -0.110       3.91 Drama|Mystery                      22\n 9 Quiz Show (1994)                    1.14 -0.166       3.75 Drama                              90\n10 Striptease (1996)                   1.14 -0.681       2.46 Comedy|Crime                       39\n# ... with 1,237 more rows\nWe’ll leave it to the knowledgeable reader to name these factors, and proceed to our second topic: How does an embedding layer do what it does?\nDo-it-yourself embeddings\nYou may have heard people say all an embedding layer did was just a lookup. Imagine you had a dataset that, in addition to continuous variables like temperature or barometric pressure, contained a categorical column characterization consisting of tags like “foggy” or “cloudy.” Say characterization had 7 possible values, encoded as a factor with levels 1-7.\nWere we going to feed this variable to a non-embedding layer, layer_dense say, we’d have to take care that those numbers do not get taken for integers, thus falsely implying an interval (or at least ordered) scale. But when we use an embedding as the first layer in a Keras model, we feed in integers all the time! For example, in text classification, a sentence might get encoded as a vector padded with zeroes, like this:\n2  77   4   5 122   55  1  3   0   0  \nThe thing that makes this work is that the embedding layer actually does perform a lookup. Below, you’ll find a very simple3 custom layer that does essentially the same thing as Keras’ layer_embedding:\nIt has a weight matrix self$embeddings that maps from an input space (movies, say) to the output space of latent factors (embeddings).\nWhen we call the layer, as in\nx <- k_gather(self$embeddings, x)\nit looks up the passed-in row number in the weight matrix, thus retrieving an item’s distributed representation from the matrix.\n\n\nSimpleEmbedding <- R6::R6Class(\n  \"SimpleEmbedding\",\n  \n  inherit = KerasLayer,\n  \n  public = list(\n    output_dim = NULL,\n    emb_input_dim = NULL,\n    embeddings = NULL,\n    \n    initialize = function(emb_input_dim, output_dim) {\n      self$emb_input_dim <- emb_input_dim\n      self$output_dim <- output_dim\n    },\n    \n    build = function(input_shape) {\n      self$embeddings <- self$add_weight(\n        name = 'embeddings',\n        shape = list(self$emb_input_dim, self$output_dim),\n        initializer = initializer_random_uniform(),\n        trainable = TRUE\n      )\n    },\n    \n    call = function(x, mask = NULL) {\n      x <- k_cast(x, \"int32\")\n      k_gather(self$embeddings, x)\n    },\n    \n    compute_output_shape = function(input_shape) {\n      list(self$output_dim)\n    }\n  )\n)\n\n\nAs usual with custom layers, we still need a wrapper that takes care of instantiation.\n\n\nlayer_simple_embedding <-\n  function(object,\n           emb_input_dim,\n           output_dim,\n           name = NULL,\n           trainable = TRUE) {\n    create_layer(\n      SimpleEmbedding,\n      object,\n      list(\n        emb_input_dim = as.integer(emb_input_dim),\n        output_dim = as.integer(output_dim),\n        name = name,\n        trainable = trainable\n      )\n    )\n  }\n\n\nDoes this work? Let’s test it on the ratings prediction task! We’ll just substitute the custom layer in the simple dot product model we started out with, and check if we get out a similar RMSE.\nPutting the custom embedding layer to test\nHere’s the simple dot product model again, this time using our custom embedding layer.\n\n\nsimple_dot2 <- function(embedding_dim,\n                       n_users,\n                       n_movies,\n                       name = \"simple_dot2\") {\n  \n  keras_model_custom(name = name, function(self) {\n    self$embedding_dim <- embedding_dim\n    \n    self$user_embedding <-\n      layer_simple_embedding(\n        emb_input_dim = list(n_users + 1),\n        output_dim = embedding_dim,\n        name = \"user_embedding\"\n      )\n    self$movie_embedding <-\n      layer_simple_embedding(\n        emb_input_dim = list(n_movies + 1),\n        output_dim = embedding_dim,\n        name = \"movie_embedding\"\n      )\n    self$dot <-\n      layer_lambda(\n        output_shape = self$embedding_dim,\n        f = function(x) {\n          k_batch_dot(x[[1]], x[[2]], axes = 2)\n        }\n      )\n    \n    function(x, mask = NULL) {\n      users <- x[, 1]\n      movies <- x[, 2]\n      user_embedding <- self$user_embedding(users)\n      movie_embedding <- self$movie_embedding(movies)\n      self$dot(list(user_embedding, movie_embedding))\n    }\n  })\n}\n\nmodel <- simple_dot2(embedding_dim, n_users, n_movies)\n\nmodel %>% compile(\n  loss = \"mse\",\n  optimizer = \"adam\"\n)\n\nhistory <- model %>% fit(\n  x_train,\n  y_train,\n  epochs = 10,\n  batch_size = 32,\n  validation_data = list(x_valid, y_valid),\n  callbacks = list(callback_early_stopping(patience = 2))\n)\n\n\nWe end up with a RMSE of 1.13 on the validation set, which is not far from the 1.08 we obtained when using layer_embedding. At least, this should tell us that we successfully reproduced the approach.\nConclusion\nOur goals in this post were twofold: Shed some light on how an embedding layer can be implemented, and show how embeddings calculated by a neural network can be used as a substitute for component matrices obtained from matrix decomposition. Of course, this is not the only thing that’s fascinating about embeddings!\nFor example, a very practical question is how much actual predictions can be improved by using embeddings instead of one-hot vectors; another is how learned embeddings might differ depending on what task they were trained on.\nLast not least - how do latent factors learned via embeddings differ from those learned by an autoencoder?\nIn that spirit, there is no lack of topics for exploration and poking around …\n\n\n\nAhmed, N. K., R. Rossi, J. Boaz Lee, T. L. Willke, R. Zhou, X. Kong, and H. Eldardiry. 2018. “Learning Role-Based Graph Embeddings.” ArXiv e-Prints, February. https://arxiv.org/abs/1802.02896.\n\n\nAlon, Uri, Meital Zilberstein, Omer Levy, and Eran Yahav. 2018. “Code2vec: Learning Distributed Representations of Code.” CoRR abs/1803.09473. http://arxiv.org/abs/1803.09473.\n\n\nFrome, Andrea, Gregory S. Corrado, Jonathon Shlens, Samy Bengio, Jeffrey Dean, Marc’Aurelio Ranzato, and Tomas Mikolov. 2013. “DeViSE: A Deep Visual-Semantic Embedding Model.” In NIPS, 2121–29.\n\n\nHallac, D., S. Bhooshan, M. Chen, K. Abida, R. Sosic, and J. Leskovec. 2018. “Drive2Vec: Multiscale State-Space Embedding of Vehicular Sensor Data.” ArXiv e-Prints, June. https://arxiv.org/abs/1806.04795.\n\n\nJean, Neal, Sherrie Wang, Anshul Samar, George Azzari, David B. Lobell, and Stefano Ermon. 2018. “Tile2Vec: Unsupervised Representation Learning for Spatially Distributed Data.” CoRR abs/1805.02855. http://arxiv.org/abs/1805.02855.\n\n\nKrstovski, K., and D. M. Blei. 2018. “Equation Embeddings.” ArXiv e-Prints, March. https://arxiv.org/abs/1803.09123.\n\n\nRumelhart, David E., James L. McClelland, and CORPORATE PDP Research Group, eds. 1986. Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 2: Psychological and Biological Models. Cambridge, MA, USA: MIT Press.\n\n\nZohra Smaili, F., X. Gao, and R. Hoehndorf. 2018. “Onto2Vec: Joint Vector-Based Representation of Biological Entities and Their Ontology-Based Annotations.” ArXiv e-Prints, January. https://arxiv.org/abs/1802.00864.\n\n\nFrom: http://www.cs.toronto.edu/~bonner/courses/2014s/csc321/lectures/lec5.pdf↩︎\nCustom models are a recent Keras feature that allow for a flexible definition of the forward pass. While the current use case does not require using a custom model, it nicely illustrates how the network’s logic can quickly be grasped by looking at the call method.↩︎\nIt really is simple; it only works with input length = 1.↩︎\n",
    "preview": "posts/2018-09-26-embeddings-recommender/images/m.png",
    "last_modified": "2024-11-21T15:52:51+00:00",
    "input_file": {},
    "preview_width": 700,
    "preview_height": 402
  },
  {
    "path": "posts/2018-09-20-eager-pix2pix/",
    "title": "Image-to-image translation with pix2pix",
    "description": "Conditional GANs (cGANs) may be used to generate one type of object based on another - e.g., a map based on a photo, or a color video based on black-and-white. Here, we show how to implement the pix2pix approach with Keras and eager execution.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-09-20",
    "categories": [
      "TensorFlow/Keras",
      "Image Recognition & Image Processing",
      "Generative Models"
    ],
    "contents": "\nWhat do we need to train a neural network? A common answer is: a model, a cost function, and an optimization algorithm.\n(I know: I’m leaving out the most important thing here - the data.)\nAs computer programs work with numbers, the cost function has to be pretty specific: We can’t just say predict next month’s demand for lawn mowers please, and do your best, we have to say something like this: Minimize the squared deviation of the estimate from the target value.\nIn some cases it may be straightforward to map a task to a measure of error, in others, it may not. Consider the task of generating non-existing objects of a certain type (like a face, a scene, or a video clip). How do we quantify success?\nThe trick with generative adversarial networks (GANs) is to let the network learn the cost function.\nAs shown in Generating images with Keras and TensorFlow eager execution, in a simple GAN the setup is this: One agent, the generator, keeps on producing fake objects. The other, the discriminator, is tasked to tell apart the real objects from the fake ones. For the generator, loss is augmented when its fraud gets discovered, meaning that the generator’s cost function depends on what the discriminator does. For the discriminator, loss grows when it fails to correctly tell apart generated objects from authentic ones.\nIn a GAN of the type just described, creation starts from white noise. However in the real world, what is required may be a form of transformation, not creation. Take, for example, colorization of black-and-white images, or conversion of aerials to maps. For applications like those, we condition on additional input: Hence the name, conditional adversarial networks.\nPut concretely, this means the generator is passed not (or not only) white noise, but data of a certain input structure, such as edges or shapes. It then has to generate realistic-looking pictures of real objects having those shapes.\nThe discriminator, too, may receive the shapes or edges as input, in addition to the fake and real objects it is tasked to tell apart.\nHere are a few examples of conditioning, taken from the paper we’ll be implementing (see below):\nFigure from Image-to-Image Translation with Conditional Adversarial Networks Isola et al. (2016)In this post, we port to R a Google Colaboratory Notebook using Keras with eager execution. We’re implementing the basic architecture from pix2pix, as described by Isola et al. in their 2016 paper(Isola et al. 2016). It’s an interesting paper to read as it validates the approach on a bunch of different datasets, and shares outcomes of using different loss families, too:\nFigure from Image-to-Image Translation with Conditional Adversarial Networks Isola et al. (2016)Prerequisites\nThe code shown here will work with the current CRAN versions of tensorflow, keras, and tfdatasets. Also, be sure to check that you’re using at least version 1.9 of TensorFlow. If that isn’t the case, as of this writing, this\n\n\nlibrary(tensorflow)\ninstall_tensorflow()\n\n\nwill get you version 1.10.\nWhen loading libraries, please make sure you’re executing the first 4 lines in the exact order shown. We need to make sure we’re using the TensorFlow implementation of Keras (tf.keras in Python land), and we have to enable eager execution before using TensorFlow in any way.\nNo need to copy-paste any code snippets - you’ll find the complete code (in order necessary for execution) here: eager-pix2pix.R.\n\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nlibrary(tfdatasets)\nlibrary(purrr)\n\n\nDataset\nFor this post, we’re working with one of the datasets used in the paper, a preprocessed version of the CMP Facade Dataset.\nImages contain the ground truth - that we’d wish for the generator to generate, and for the discriminator to correctly detect as authentic - and the input we’re conditioning on (a coarse segmention into object classes) next to each other in the same file.\nFigure from https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/Preprocessing\nObviously, our preprocessing will have to split the input images into parts. That’s the first thing that happens in the function below.\nAfter that, action depends on whether we’re in the training or testing phases. If we’re training, we perform random jittering, via upsizing the image to 286x286 and then cropping to the original size of 256x256. In about 50% of the cases, we also flipping the image left-to-right.\nIn both cases, training and testing, we normalize the image to the range between -1 and 1.\nNote the use of the tf$image module for image -related operations. This is required as the images will be streamed via tfdatasets, which works on TensorFlow graphs.\n\n\nimg_width <- 256L\nimg_height <- 256L\n\nload_image <- function(image_file, is_train) {\n\n  image <- tf$read_file(image_file)\n  image <- tf$image$decode_jpeg(image)\n  \n  w <- as.integer(k_shape(image)[2])\n  w2 <- as.integer(w / 2L)\n  real_image <- image[ , 1L:w2, ]\n  input_image <- image[ , (w2 + 1L):w, ]\n  \n  input_image <- k_cast(input_image, tf$float32)\n  real_image <- k_cast(real_image, tf$float32)\n\n  if (is_train) {\n    input_image <-\n      tf$image$resize_images(input_image,\n                             c(286L, 286L),\n                             align_corners = TRUE,\n                             method = 2)\n    real_image <- tf$image$resize_images(real_image,\n                                         c(286L, 286L),\n                                         align_corners = TRUE,\n                                         method = 2)\n    \n    stacked_image <-\n      k_stack(list(input_image, real_image), axis = 1)\n    cropped_image <-\n      tf$random_crop(stacked_image, size = c(2L, img_height, img_width, 3L))\n    c(input_image, real_image) %<-% \n      list(cropped_image[1, , , ], cropped_image[2, , , ])\n    \n    if (runif(1) > 0.5) {\n      input_image <- tf$image$flip_left_right(input_image)\n      real_image <- tf$image$flip_left_right(real_image)\n    }\n    \n  } else {\n    input_image <-\n      tf$image$resize_images(\n        input_image,\n        size = c(img_height, img_width),\n        align_corners = TRUE,\n        method = 2\n      )\n    real_image <-\n      tf$image$resize_images(\n        real_image,\n        size = c(img_height, img_width),\n        align_corners = TRUE,\n        method = 2\n      )\n  }\n  \n  input_image <- (input_image / 127.5) - 1\n  real_image <- (real_image / 127.5) - 1\n  \n  list(input_image, real_image)\n}\n\n\nStreaming the data\nThe images will be streamed via tfdatasets, using a batch size of 1.\nNote how the load_image function we defined above is wrapped in tf$py_func to enable accessing tensor values in the usual eager way (which by default, as of this writing, is not possible with the TensorFlow datasets API).\n\n\n# change to where you unpacked the data\n# there will be train, val and test subdirectories below\ndata_dir <- \"facades\"\n\nbuffer_size <- 400\nbatch_size <- 1\nbatches_per_epoch <- buffer_size / batch_size\n\ntrain_dataset <-\n  tf$data$Dataset$list_files(file.path(data_dir, \"train/*.jpg\")) %>%\n  dataset_shuffle(buffer_size) %>%\n  dataset_map(function(image) {\n    tf$py_func(load_image, list(image, TRUE), list(tf$float32, tf$float32))\n  }) %>%\n  dataset_batch(batch_size)\n\ntest_dataset <-\n  tf$data$Dataset$list_files(file.path(data_dir, \"test/*.jpg\")) %>%\n  dataset_map(function(image) {\n    tf$py_func(load_image, list(image, TRUE), list(tf$float32, tf$float32))\n  }) %>%\n  dataset_batch(batch_size)\n\n\nDefining the actors\nGenerator\nFirst, here’s the generator. Let’s start with a birds-eye view.\nThe generator receives as input a coarse segmentation, of size 256x256, and should produce a nice color image of a facade.\nIt first successively downsamples the input, up to a minimal size of 1x1. Then after maximal condensation, it starts upsampling again, until it has reached the required output resolution of 256x256.\nDuring downsampling, as spatial resolution decreases, the number of filters increases. During upsampling, it goes the opposite way.\n\n\ngenerator <- function(name = \"generator\") {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$down1 <- downsample(64, 4, apply_batchnorm = FALSE)\n    self$down2 <- downsample(128, 4)\n    self$down3 <- downsample(256, 4)\n    self$down4 <- downsample(512, 4)\n    self$down5 <- downsample(512, 4)\n    self$down6 <- downsample(512, 4)\n    self$down7 <- downsample(512, 4)\n    self$down8 <- downsample(512, 4)\n    \n    self$up1 <- upsample(512, 4, apply_dropout = TRUE)\n    self$up2 <- upsample(512, 4, apply_dropout = TRUE)\n    self$up3 <- upsample(512, 4, apply_dropout = TRUE)\n    self$up4 <- upsample(512, 4)\n    self$up5 <- upsample(256, 4)\n    self$up6 <- upsample(128, 4)\n    self$up7 <- upsample(64, 4)\n    \n    self$last <- layer_conv_2d_transpose(\n      filters = 3,\n      kernel_size = 4,\n      strides = 2,\n      padding = \"same\",\n      kernel_initializer = initializer_random_normal(0, 0.2),\n      activation = \"tanh\"\n    )\n    \n    function(x, mask = NULL, training = TRUE) {           # x shape == (bs, 256, 256, 3)\n     \n      x1 <- x %>% self$down1(training = training)         # (bs, 128, 128, 64)\n      x2 <- self$down2(x1, training = training)           # (bs, 64, 64, 128)\n      x3 <- self$down3(x2, training = training)           # (bs, 32, 32, 256)\n      x4 <- self$down4(x3, training = training)           # (bs, 16, 16, 512)\n      x5 <- self$down5(x4, training = training)           # (bs, 8, 8, 512)\n      x6 <- self$down6(x5, training = training)           # (bs, 4, 4, 512)\n      x7 <- self$down7(x6, training = training)           # (bs, 2, 2, 512)\n      x8 <- self$down8(x7, training = training)           # (bs, 1, 1, 512)\n\n      x9 <- self$up1(list(x8, x7), training = training)   # (bs, 2, 2, 1024)\n      x10 <- self$up2(list(x9, x6), training = training)  # (bs, 4, 4, 1024)\n      x11 <- self$up3(list(x10, x5), training = training) # (bs, 8, 8, 1024)\n      x12 <- self$up4(list(x11, x4), training = training) # (bs, 16, 16, 1024)\n      x13 <- self$up5(list(x12, x3), training = training) # (bs, 32, 32, 512)\n      x14 <- self$up6(list(x13, x2), training = training) # (bs, 64, 64, 256)\n      x15 <-self$up7(list(x14, x1), training = training)  # (bs, 128, 128, 128)\n      x16 <- self$last(x15)                               # (bs, 256, 256, 3)\n      x16\n    }\n  })\n}\n\n\nHow can spatial information be preserved if we downsample all the way down to a single pixel? The generator follows the general principle of a U-Net (Ronneberger, Fischer, and Brox 2015), where skip connections exist from layers earlier in the downsampling process to layers later on the way up.\nFigure from (Ronneberger, Fischer, and Brox 2015)Let’s take the line\n\n\nx15 <-self$up7(list(x14, x1), training = training)\n\n\nfrom the call method.\nHere, the inputs to self$up are x14, which went through all of the down- and upsampling, and x1, the output from the very first downsampling step. The former has resolution 64x64, the latter, 128x128. How do they get combined?\nThat’s taken care of by upsample, technically a custom model of its own.\nAs an aside, we remark how custom models let you pack your code into nice, reusable modules.\n\n\nupsample <- function(filters,\n                     size,\n                     apply_dropout = FALSE,\n                     name = \"upsample\") {\n  \n  keras_model_custom(name = NULL, function(self) {\n    \n    self$apply_dropout <- apply_dropout\n    self$up_conv <- layer_conv_2d_transpose(\n      filters = filters,\n      kernel_size = size,\n      strides = 2,\n      padding = \"same\",\n      kernel_initializer = initializer_random_normal(),\n      use_bias = FALSE\n    )\n    self$batchnorm <- layer_batch_normalization()\n    if (self$apply_dropout) {\n      self$dropout <- layer_dropout(rate = 0.5)\n    }\n    \n    function(xs, mask = NULL, training = TRUE) {\n      \n      c(x1, x2) %<-% xs\n      x <- self$up_conv(x1) %>% self$batchnorm(training = training)\n      if (self$apply_dropout) {\n        x %>% self$dropout(training = training)\n      }\n      x %>% layer_activation(\"relu\")\n      concat <- k_concatenate(list(x, x2))\n      concat\n    }\n  })\n}\n\n\nx14 is upsampled to double its size, and x1 is appended as is.\nThe axis of concatenation here is axis 4, the feature map / channels axis. x1 comes with 64 channels, x14 comes out of layer_conv_2d_transpose with 64 channels, too (because self$up7 has been defined that way). So we end up with an image of resolution 128x128 and 128 feature maps for the output of step x15.\nDownsampling, too, is factored out to its own model. Here too, the number of filters is configurable.\n\n\ndownsample <- function(filters,\n                       size,\n                       apply_batchnorm = TRUE,\n                       name = \"downsample\") {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$apply_batchnorm <- apply_batchnorm\n    self$conv1 <- layer_conv_2d(\n      filters = filters,\n      kernel_size = size,\n      strides = 2,\n      padding = 'same',\n      kernel_initializer = initializer_random_normal(0, 0.2),\n      use_bias = FALSE\n    )\n    if (self$apply_batchnorm) {\n      self$batchnorm <- layer_batch_normalization()\n    }\n    \n    function(x, mask = NULL, training = TRUE) {\n      \n      x <- self$conv1(x)\n      if (self$apply_batchnorm) {\n        x %>% self$batchnorm(training = training)\n      }\n      x %>% layer_activation_leaky_relu()\n    }\n  })\n}\n\n\nNow for the discriminator.\nDiscriminator\nAgain, let’s start with a birds-eye view.\nThe discriminator receives as input both the coarse segmentation and the ground truth. Both are concatenated and processed together. Just like the generator, the discriminator is thus conditioned on the segmentation.\nWhat does the discriminator return? The output of self$last has one channel, but a spatial resolution of 30x30: We’re outputting a probability for each of 30x30 image patches (which is why the authors are calling this a PatchGAN).\nThe discriminator thus working on small image patches means it only cares about local structure, and consequently, enforces correctness in the high frequencies only. Correctness in the low frequencies is taken care of by an additional L1 component in the discriminator loss that operates over the whole image (as we’ll see below).\n\n\ndiscriminator <- function(name = \"discriminator\") {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$down1 <- disc_downsample(64, 4, FALSE)\n    self$down2 <- disc_downsample(128, 4)\n    self$down3 <- disc_downsample(256, 4)\n    self$zero_pad1 <- layer_zero_padding_2d()\n    self$conv <- layer_conv_2d(\n      filters = 512,\n      kernel_size = 4,\n      strides = 1,\n      kernel_initializer = initializer_random_normal(),\n      use_bias = FALSE\n    )\n    self$batchnorm <- layer_batch_normalization()\n    self$zero_pad2 <- layer_zero_padding_2d()\n    self$last <- layer_conv_2d(\n      filters = 1,\n      kernel_size = 4,\n      strides = 1,\n      kernel_initializer = initializer_random_normal()\n    )\n    \n    function(x, y, mask = NULL, training = TRUE) {\n      \n      x <- k_concatenate(list(x, y)) %>%            # (bs, 256, 256, channels*2)\n        self$down1(training = training) %>%         # (bs, 128, 128, 64)\n        self$down2(training = training) %>%         # (bs, 64, 64, 128)\n        self$down3(training = training) %>%         # (bs, 32, 32, 256)\n        self$zero_pad1() %>%                        # (bs, 34, 34, 256)\n        self$conv() %>%                             # (bs, 31, 31, 512)\n        self$batchnorm(training = training) %>%\n        layer_activation_leaky_relu() %>%\n        self$zero_pad2() %>%                        # (bs, 33, 33, 512)\n        self$last()                                 # (bs, 30, 30, 1)\n      x\n    }\n  })\n}\n\n\nAnd here’s the factored-out downsampling functionality, again providing the means to configure the number of filters.\n\n\ndisc_downsample <- function(filters,\n                            size,\n                            apply_batchnorm = TRUE,\n                            name = \"disc_downsample\") {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$apply_batchnorm <- apply_batchnorm\n    self$conv1 <- layer_conv_2d(\n      filters = filters,\n      kernel_size = size,\n      strides = 2,\n      padding = 'same',\n      kernel_initializer = initializer_random_normal(0, 0.2),\n      use_bias = FALSE\n    )\n    if (self$apply_batchnorm) {\n      self$batchnorm <- layer_batch_normalization()\n    }\n    \n    function(x, mask = NULL, training = TRUE) {\n      x <- self$conv1(x)\n      if (self$apply_batchnorm) {\n        x %>% self$batchnorm(training = training)\n      }\n      x %>% layer_activation_leaky_relu()\n    }\n  })\n}\n\n\nLosses and optimizer\nAs we said in the introduction, the idea of a GAN is to have the network learn the cost function.\nMore concretely, the thing it should learn is the balance between two losses, the generator loss and the discriminator loss.\nEach of them individually, of course, has to be provided with a loss function, so there are still decisions to be made.\nFor the generator, two things factor into the loss: First, does the discriminator debunk my creations as fake?\nSecond, how big is the absolute deviation of the generated image from the target?\nThe latter factor does not have to be present in a conditional GAN, but was included by the authors to further encourage proximity to the target, and empirically found to deliver better results.\n\n\nlambda <- 100 # value chosen by the authors of the paper\ngenerator_loss <- function(disc_judgment, generated_output, target) {\n    gan_loss <- tf$losses$sigmoid_cross_entropy(\n      tf$ones_like(disc_judgment),\n      disc_judgment\n    )\n    l1_loss <- tf$reduce_mean(tf$abs(target - generated_output))\n    gan_loss + (lambda * l1_loss)\n  }\n\n\nThe discriminator loss looks as in a standard (un-conditional) GAN. Its first component is determined by how accurately it classifies real images as real, while the second depends on its competence in judging fake images as fake.\n\n\ndiscriminator_loss <- function(real_output, generated_output) {\n  real_loss <- tf$losses$sigmoid_cross_entropy(\n    multi_class_labels = tf$ones_like(real_output),\n    logits = real_output\n  )\n  generated_loss <- tf$losses$sigmoid_cross_entropy(\n    multi_class_labels = tf$zeros_like(generated_output),\n    logits = generated_output\n  )\n  real_loss + generated_loss\n}\n\n\nFor optimization, we rely on Adam for both the generator and the discriminator.\n\n\ndiscriminator_optimizer <- tf$train$AdamOptimizer(2e-4, beta1 = 0.5)\ngenerator_optimizer <- tf$train$AdamOptimizer(2e-4, beta1 = 0.5)\n\n\nThe game\nWe’re ready to have the generator and the discriminator play the game!\nBelow, we use defun to compile the respective R functions into TensorFlow graphs, to speed up computations.\n\n\ngenerator <- generator()\ndiscriminator <- discriminator()\n\ngenerator$call = tf$contrib$eager$defun(generator$call)\ndiscriminator$call = tf$contrib$eager$defun(discriminator$call)\n\n\nWe also create a tf$train$Checkpoint object that will allow us to save and restore training weights.\n\n\ncheckpoint_dir <- \"./checkpoints_pix2pix\"\ncheckpoint_prefix <- file.path(checkpoint_dir, \"ckpt\")\ncheckpoint <- tf$train$Checkpoint(\n    generator_optimizer = generator_optimizer,\n    discriminator_optimizer = discriminator_optimizer,\n    generator = generator,\n    discriminator = discriminator\n)\n\n\nTraining is a loop over epochs with an inner loop over batches yielded by the dataset.\nAs usual with eager execution, tf$GradientTape takes care of recording the forward pass and determining the gradients, while the optimizer - there are two of them in this setup - adjusts the networks’ weights.\nEvery tenth epoch, we save the weights, and tell the generator to have a go at the first example of the test set, so we can monitor network progress. See generate_images in the companion code for this functionality.\n\n\ntrain <- function(dataset, num_epochs) {\n  \n  for (epoch in 1:num_epochs) {\n    total_loss_gen <- 0\n    total_loss_disc <- 0\n    iter <- make_iterator_one_shot(train_dataset)\n    \n    until_out_of_range({\n      batch <- iterator_get_next(iter)\n      input_image <- batch[[1]]\n      target <- batch[[2]]\n      \n      with(tf$GradientTape() %as% gen_tape, {\n        with(tf$GradientTape() %as% disc_tape, {\n          \n          gen_output <- generator(input_image, training = TRUE)\n          disc_real_output <-\n            discriminator(input_image, target, training = TRUE)\n          disc_generated_output <-\n            discriminator(input_image, gen_output, training = TRUE)\n          gen_loss <-\n            generator_loss(disc_generated_output, gen_output, target)\n          disc_loss <-\n            discriminator_loss(disc_real_output, disc_generated_output)\n          total_loss_gen <- total_loss_gen + gen_loss\n          total_loss_disc <- total_loss_disc + disc_loss\n        })\n      })\n      \n      generator_gradients <- gen_tape$gradient(gen_loss,\n                                               generator$variables)\n      discriminator_gradients <- disc_tape$gradient(disc_loss,\n                                                    discriminator$variables)\n      \n      generator_optimizer$apply_gradients(transpose(list(\n        generator_gradients,\n        generator$variables\n      )))\n      discriminator_optimizer$apply_gradients(transpose(\n        list(discriminator_gradients,\n             discriminator$variables)\n      ))\n      \n    })\n    \n    cat(\"Epoch \", epoch, \"\\n\")\n    cat(\"Generator loss: \",\n        total_loss_gen$numpy() / batches_per_epoch,\n        \"\\n\")\n    cat(\"Discriminator loss: \",\n        total_loss_disc$numpy() / batches_per_epoch,\n        \"\\n\\n\")\n    \n    if (epoch %% 10 == 0) {\n      test_iter <- make_iterator_one_shot(test_dataset)\n      batch <- iterator_get_next(test_iter)\n      input <- batch[[1]]\n      target <- batch[[2]]\n      generate_images(generator, input, target, paste0(\"epoch_\", i))\n    }\n    \n    if (epoch %% 10 == 0) {\n      checkpoint$save(file_prefix = checkpoint_prefix)\n    }\n  }\n}\n\nif (!restore) {\n  train(train_dataset, 200)\n} \n\n\nThe results\nWhat has the network learned?\nHere’s a pretty typical result from the test set. It doesn’t look so bad.\n\nHere’s another one. Interestingly, the colors used in the fake image match the previous one’s pretty well, even though we used an additional L1 loss to penalize deviations from the original.\n\nThis pick from the test set again shows similar hues, and it might already convey an impression one gets when going through the complete test set: The network has not just learned some balance between creatively turning a coarse mask into a detailed image on the one hand, and reproducing a concrete example on the other hand. It also has internalized the main architectural style present in the dataset.\n\nFor an extreme example, take this. The mask leaves an enormous lot of freedom, while the target image is a pretty untypical (perhaps the most untypical) pick from the test set. The outcome is a structure that could represent a building, or part of a building, of specific texture and color shades.\n\nConclusion\nWhen we say the network has internalized the dominant style of the training set, is this a bad thing? (We’re used to thinking in terms of overfitting on the training set.)\nWith GANs though, one could say it all depends on the purpose. If it doesn’t fit our purpose, one thing we could try is training on several datasets at the same time.\nAgain depending on what we want to achieve, another weakness could be the lack of stochasticity in the model, as stated by the authors of the paper themselves. This will be hard to avoid when working with paired datasets as the ones used in pix2pix. An interesting alternative is CycleGAN(Zhu et al. 2017) that lets you transfer style between complete datasets without using paired instances:\nFigure from Zhu et al. (2017)Finally closing on a more technical note, you may have noticed the prominent checkerboard effects in the above fake examples. This phenomenon (and ways to address it) is superbly explained in a 2016 article on distill.pub (Odena, Dumoulin, and Olah 2016).\nIn our case, it will mostly be due to the use of layer_conv_2d_transpose for upsampling.\nAs per the authors (Odena, Dumoulin, and Olah 2016), a better alternative is upsizing followed by padding and (standard) convolution.\nIf you’re interested, it should be straightforward to modify the example code to use tf$image$resize_images (using ResizeMethod.NEAREST_NEIGHBOR as recommended by the authors), tf$pad and layer_conv2d.\n\n\n\nIsola, Phillip, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. 2016. “Image-to-Image Translation with Conditional Adversarial Networks.” CoRR abs/1611.07004. http://arxiv.org/abs/1611.07004.\n\n\nOdena, Augustus, Vincent Dumoulin, and Chris Olah. 2016. “Deconvolution and Checkerboard Artifacts.” Distill. https://doi.org/10.23915/distill.00003.\n\n\nRonneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” CoRR abs/1505.04597. http://arxiv.org/abs/1505.04597.\n\n\nZhu, Jun-Yan, Taesung Park, Phillip Isola, and Alexei A. Efros. 2017. “Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks.” CoRR abs/1703.10593. http://arxiv.org/abs/1703.10593.\n\n\n\n\n",
    "preview": "posts/2018-09-20-eager-pix2pix/images/pix2pixlosses.png",
    "last_modified": "2024-11-21T15:52:54+00:00",
    "input_file": {},
    "preview_width": 842,
    "preview_height": 536
  },
  {
    "path": "posts/2018-09-17-eager-captioning/",
    "title": "Attention-based Image Captioning with Keras",
    "description": "Image captioning is a challenging task at intersection of vision and language. Here, we demonstrate using Keras and eager execution to incorporate an attention mechanism that allows the network to concentrate on image features relevant to the current state of text generation.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-09-17",
    "categories": [
      "Natural Language Processing",
      "TensorFlow/Keras",
      "Image Recognition & Image Processing"
    ],
    "contents": "\nIn image captioning, an algorithm is given an image and tasked with producing a sensible caption. It is a challenging task for several reasons, not the least being that it involves a notion of saliency or relevance. This is why recent deep learning approaches mostly include some “attention” mechanism (sometimes even more than one) to help focusing on relevant image features.\nIn this post, we demonstrate a formulation of image captioning as an encoder-decoder problem, enhanced by spatial attention over image grid cells. The idea comes from a recent paper on Neural Image Caption Generation with Visual Attention (Xu et al. 2015), and employs the same kind of attention algorithm as detailed in our post on machine translation.\nWe’re porting Python code from a recent Google Colaboratory notebook, using Keras with TensorFlow eager execution to simplify our lives.\nPrerequisites\nThe code shown here will work with the current CRAN versions of tensorflow, keras, and tfdatasets.\nCheck that you’re using at least version 1.9 of TensorFlow. If that isn’t the case, as of this writing, this\n\n\nlibrary(tensorflow)\ninstall_tensorflow()\n\n\nwill get you version 1.10.\nWhen loading libraries, please make sure you’re executing the first 4 lines in this exact order.\nWe need to make sure we’re using the TensorFlow implementation of Keras (tf.keras in Python land), and we have to enable eager execution before using TensorFlow in any way.\nNo need to copy-paste any code snippets - you’ll find the complete code (in order necessary for execution) here: eager-image-captioning.R.\n\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\n\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nnp <- import(\"numpy\")\n\nlibrary(tfdatasets)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(glue)\nlibrary(rjson)\nlibrary(rlang)\nlibrary(dplyr)\nlibrary(magick)\n\n\nThe dataset\nMS-COCO (“Common Objects in Context”) is one of, perhaps the, reference dataset in image captioning (object detection and segmentation, too).\nWe’ll be using the training images and annotations from 2014 - be warned, depending on your location, the download can take a long time.\nAfter unpacking, let’s define where the images and captions are.\n\n\nannotation_file <- \"train2014/annotations/captions_train2014.json\"\nimage_path <- \"train2014/train2014\"\n\n\nThe annotations are in JSON format, and there are 414113 of them! Luckily for us we didn’t have to download that many images - every image comes with 5 different captions, for better generalizability.\n\n\nannotations <- fromJSON(file = annotation_file)\nannot_captions <- annotations[[4]]\n\nnum_captions <- length(annot_captions)\n\n\nWe store both annotations and image paths in lists, for later loading.\n\n\nall_captions <- vector(mode = \"list\", length = num_captions)\nall_img_names <- vector(mode = \"list\", length = num_captions)\n\nfor (i in seq_len(num_captions)) {\n  caption <- paste0(\"<start> \",\n                    annot_captions[[i]][[\"caption\"]],\n                    \" <end>\"\n                    )\n  image_id <- annot_captions[[i]][[\"image_id\"]]\n  full_coco_image_path <- sprintf(\n    \"%s/COCO_train2014_%012d.jpg\",\n    image_path,\n    image_id\n  )\n  all_img_names[[i]] <- full_coco_image_path\n  all_captions[[i]] <- caption\n}\n\n\nDepending on your computing environment, you will for sure want to restrict the number of examples used.\nThis post will use 30000 captioned images, chosen randomly, and set aside 20% for validation.\nBelow, we take random samples, split into training and validation parts. The companion code will also store the indices on disk, so you can pick up on verification and analysis later.\n\n\nnum_examples <- 30000\n\nrandom_sample <- sample(1:num_captions, size = num_examples)\ntrain_indices <- sample(random_sample, size = length(random_sample) * 0.8)\nvalidation_indices <- setdiff(random_sample, train_indices)\n\nsample_captions <- all_captions[random_sample]\nsample_images <- all_img_names[random_sample]\ntrain_captions <- all_captions[train_indices]\ntrain_images <- all_img_names[train_indices]\nvalidation_captions <- all_captions[validation_indices]\nvalidation_images <- all_img_names[validation_indices]\n\n\nInterlude\nBefore really diving into the technical stuff, let’s take a moment to reflect on this task.\nIn typical image-related deep learning walk-throughs, we’re used to seeing well-defined problems - even if in some cases, the solution may be hard. Take, for example, the stereotypical dog vs. cat problem. Some dogs may look like cats and some cats may look like dogs, but that’s about it: All in all, in the usual world we live in, it should be a more or less binary question.\nIf, on the other hand, we ask people to describe what they see in a scene, it’s to be expected from the outset that we’ll get different answers. Still, how much consensus there is will very much depend on the concrete dataset we’re using.\nLet’s take a look at some picks from the very first 20 training items sampled randomly above.\nFigure from MS-COCO 2014Now this image does not leave much room for decision what to focus on, and received a very factual caption indeed: “There is a plate with one slice of bacon a half of orange and bread.” If the dataset were all like this, we’d think a machine learning algorithm should do pretty well here.\nPicking another one from the first 20:\nFigure from MS-COCO 2014What would be salient information to you here? The caption provided goes “A smiling little boy has a checkered shirt.”\nIs the look of the shirt as important as that? You might as well focus on the scenery, - or even something on a completely different level: The age of the photo, or it being an analog one.\nLet’s take a final example.\nFrom MS-COCO 2014What would you say about this scene? The official label we sampled here is “A group of people posing in a funny way for the camera.” Well …\nPlease don’t forget that for each image, the dataset includes five different captions (although our n = 30000 samples probably won’t).\nSo this is not saying the dataset is biased - not at all. Instead, we want to point out the ambiguities and difficulties inherent in the task. Actually, given those difficulties, it’s all the more amazing that the task we’re tackling here - having a network automatically generate image captions - should be possible at all!\nNow let’s see how we can do this.\nExtract image features\nFor the encoding part of our encoder-decoder network, we will make use of InceptionV3 to extract image features. In principle, which features to extract is up to experimentation, - here we just use the last layer before the fully connected top:\n\n\nimage_model <- application_inception_v3(\n  include_top = FALSE,\n  weights = \"imagenet\"\n)\n\n\nFor an image size of 299x299, the output will be of size (batch_size, 8, 8, 2048), that is, we are making use of 2048 feature maps.\nInceptionV3 being a “big model,” where every pass through the model takes time, we want to precompute features in advance and store them on disk.\nWe’ll use tfdatasets to stream images to the model. This means all our preprocessing has to employ tensorflow functions: That’s why we’re not using the more familiar image_load from keras below.\nOur custom load_image will read in, resize and preprocess the images as required for use with InceptionV3:\n\n\nload_image <- function(image_path) {\n  img <-\n    tf$read_file(image_path) %>%\n    tf$image$decode_jpeg(channels = 3) %>%\n    tf$image$resize_images(c(299L, 299L)) %>%\n    tf$keras$applications$inception_v3$preprocess_input()\n  list(img, image_path)\n}\n\n\nNow we’re ready to save the extracted features to disk. The (batch_size, 8, 8, 2048)-sized features will be flattened to (batch_size, 64, 2048). The latter shape is what our encoder, soon to be discussed, will receive as input.\n\n\npreencode <- unique(sample_images) %>% unlist() %>% sort()\nnum_unique <- length(preencode)\n\n# adapt this according to your system's capacities  \nbatch_size_4save <- 1\nimage_dataset <-\n  tensor_slices_dataset(preencode) %>%\n  dataset_map(load_image) %>%\n  dataset_batch(batch_size_4save)\n  \nsave_iter <- make_iterator_one_shot(image_dataset)\n  \nuntil_out_of_range({\n  \n  save_count <- save_count + batch_size_4save\n  batch_4save <- save_iter$get_next()\n  img <- batch_4save[[1]]\n  path <- batch_4save[[2]]\n  batch_features <- image_model(img)\n  batch_features <- tf$reshape(\n    batch_features,\n    list(dim(batch_features)[1], -1L, dim(batch_features)[4]\n  )\n                               )\n  for (i in 1:dim(batch_features)[1]) {\n    np$save(path[i]$numpy()$decode(\"utf-8\"),\n            batch_features[i, , ]$numpy())\n  }\n    \n})\n\n\nBefore we get to the encoder and decoder models though, we need to take care of the captions.\nProcessing the captions\nWe’re using keras text_tokenizer and the text processing functions texts_to_sequences and pad_sequences to transform ascii text into a matrix.\n\n\n# we will use the 5000 most frequent words only\ntop_k <- 5000\ntokenizer <- text_tokenizer(\n  num_words = top_k,\n  oov_token = \"<unk>\",\n  filters = '!\"#$%&()*+.,-/:;=?@[\\\\]^_`{|}~ ')\ntokenizer$fit_on_texts(sample_captions)\n\ntrain_captions_tokenized <-\n  tokenizer %>% texts_to_sequences(train_captions)\nvalidation_captions_tokenized <-\n  tokenizer %>% texts_to_sequences(validation_captions)\n\n# pad_sequences will use 0 to pad all captions to the same length\ntokenizer$word_index[\"<pad>\"] <- 0\n\n# create a lookup dataframe that allows us to go in both directions\nword_index_df <- data.frame(\n  word = tokenizer$word_index %>% names(),\n  index = tokenizer$word_index %>% unlist(use.names = FALSE),\n  stringsAsFactors = FALSE\n)\nword_index_df <- word_index_df %>% arrange(index)\n\ndecode_caption <- function(text) {\n  paste(map(text, function(number)\n    word_index_df %>%\n      filter(index == number) %>%\n      select(word) %>%\n      pull()),\n    collapse = \" \")\n}\n\n# pad all sequences to the same length (the maximum length, in our case)\n# could experiment with shorter padding (truncating the very longest captions)\ncaption_lengths <- map(\n  all_captions[1:num_examples],\n  function(c) str_split(c,\" \")[[1]] %>% length()\n  ) %>% unlist()\nmax_length <- fivenum(caption_lengths)[5]\n\ntrain_captions_padded <-  pad_sequences(\n  train_captions_tokenized,\n  maxlen = max_length,\n  padding = \"post\",\n  truncating = \"post\"\n)\n\nvalidation_captions_padded <- pad_sequences(\n  validation_captions_tokenized,\n  maxlen = max_length,\n  padding = \"post\",\n  truncating = \"post\"\n)\n\n\nLoading the data for training\nNow that we’ve taken care of pre-extracting the features and preprocessing the captions, we need a way to stream them to our captioning model. For that, we’re using tensor_slices_dataset from tfdatasets, passing in the list of paths to the images and the preprocessed captions. Loading the images is then performed as a TensorFlow graph operation (using tf$pyfunc).\nThe original Colab code also shuffles the data on every iteration. Depending on your hardware, this may take a long time, and given the size of the dataset it is not strictly necessary to get reasonable results. (The results reported below were obtained without shuffling.)\n\n\nbatch_size <- 10\nbuffer_size <- num_examples\n\nmap_func <- function(img_name, cap) {\n  p <- paste0(img_name$decode(\"utf-8\"), \".npy\")\n  img_tensor <- np$load(p)\n  img_tensor <- tf$cast(img_tensor, tf$float32)\n  list(img_tensor, cap)\n}\n\ntrain_dataset <-\n  tensor_slices_dataset(list(train_images, train_captions_padded)) %>%\n  dataset_map(\n    function(item1, item2) tf$py_func(map_func, list(item1, item2), list(tf$float32, tf$int32))\n  ) %>%\n  # optionally shuffle the dataset\n  # dataset_shuffle(buffer_size) %>%\n  dataset_batch(batch_size)\n\n\nCaptioning model\nThe model is basically the same as that discussed in the machine translation post. Please refer to that article for an explanation of the concepts, as well as a detailed walk-through of the tensor shapes involved at every step. Here, we provide the tensor shapes as comments in the code snippets, for quick overview/comparison.\nHowever, if you develop your own models, with eager execution you can simply insert debugging/logging statements at arbitrary places in the code - even in model definitions. So you can have a function\n\n\nmaybecat <- function(context, x) {\n  if (debugshapes) {\n    name <- enexpr(x)\n    dims <- paste0(dim(x), collapse = \" \")\n    cat(context, \": shape of \", name, \": \", dims, \"\\n\", sep = \"\")\n  }\n}\n\n\nAnd if you now set\n\n\ndebugshapes <- FALSE\n\n\nyou can trace - not only tensor shapes, but actual tensor values through your models, as shown below for the encoder. (We don’t display any debugging statements after that, but the sample code has many more.)\nEncoder\nNow it’s time to define some some sizing-related hyperparameters and housekeeping variables:\n\n\n# for encoder output\nembedding_dim <- 256\n# decoder (LSTM) capacity\ngru_units <- 512\n# for decoder output\nvocab_size <- top_k\n# number of feature maps gotten from Inception V3\nfeatures_shape <- 2048\n# shape of attention features (flattened from 8x8)\nattention_features_shape <- 64\n\n\nThe encoder in this case is just a fully connected layer, taking in the features extracted from Inception V3 (in flattened form, as they were written to disk), and embedding them in 256-dimensional space.\n\n\ncnn_encoder <- function(embedding_dim, name = NULL) {\n    \n  keras_model_custom(name = name, function(self) {\n      \n    self$fc <- layer_dense(units = embedding_dim, activation = \"relu\")\n      \n    function(x, mask = NULL) {\n      # input shape: (batch_size, 64, features_shape)\n      maybecat(\"encoder input\", x)\n      # shape after fc: (batch_size, 64, embedding_dim)\n      x <- self$fc(x)\n      maybecat(\"encoder output\", x)\n      x\n    }\n  })\n}\n\n\nAttention module\nUnlike in the machine translation post, here the attention module is separated out into its own custom model.\nThe logic is the same though:\n\n\nattention_module <- function(gru_units, name = NULL) {\n  \n  keras_model_custom(name = name, function(self) {\n    \n    self$W1 = layer_dense(units = gru_units)\n    self$W2 = layer_dense(units = gru_units)\n    self$V = layer_dense(units = 1)\n      \n    function(inputs, mask = NULL) {\n      features <- inputs[[1]]\n      hidden <- inputs[[2]]\n      # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n      # hidden shape == (batch_size, gru_units)\n      # hidden_with_time_axis shape == (batch_size, 1, gru_units)\n      hidden_with_time_axis <- k_expand_dims(hidden, axis = 2)\n        \n      # score shape == (batch_size, 64, 1)\n      score <- self$V(k_tanh(self$W1(features) + self$W2(hidden_with_time_axis)))\n      # attention_weights shape == (batch_size, 64, 1)\n      attention_weights <- k_softmax(score, axis = 2)\n      # context_vector shape after sum == (batch_size, embedding_dim)\n      context_vector <- k_sum(attention_weights * features, axis = 2)\n        \n      list(context_vector, attention_weights)\n    }\n  })\n}\n\n\nDecoder\nThe decoder at each time step calls the attention module with the features it got from the encoder and its last hidden state, and receives back an attention vector. The attention vector gets concatenated with the current input and further processed by a GRU and two fully connected layers, the last of which gives us the (unnormalized) probabilities for the next word in the caption.\nThe current input at each time step here is the previous word: the correct one during training (teacher forcing), the last generated one during inference.\n\n\nrnn_decoder <- function(embedding_dim, gru_units, vocab_size, name = NULL) {\n    \n  keras_model_custom(name = name, function(self) {\n      \n    self$gru_units <- gru_units\n    self$embedding <- layer_embedding(input_dim = vocab_size, \n                                      output_dim = embedding_dim)\n    self$gru <- if (tf$test$is_gpu_available()) {\n      layer_cudnn_gru(\n        units = gru_units,\n        return_sequences = TRUE,\n        return_state = TRUE,\n        recurrent_initializer = 'glorot_uniform'\n      )\n    } else {\n      layer_gru(\n        units = gru_units,\n        return_sequences = TRUE,\n        return_state = TRUE,\n        recurrent_initializer = 'glorot_uniform'\n      )\n    }\n      \n    self$fc1 <- layer_dense(units = self$gru_units)\n    self$fc2 <- layer_dense(units = vocab_size)\n      \n    self$attention <- attention_module(self$gru_units)\n      \n    function(inputs, mask = NULL) {\n      x <- inputs[[1]]\n      features <- inputs[[2]]\n      hidden <- inputs[[3]]\n        \n      c(context_vector, attention_weights) %<-% \n        self$attention(list(features, hidden))\n        \n      # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n      x <- self$embedding(x)\n        \n      # x shape after concatenation == (batch_size, 1, 2 * embedding_dim)\n      x <- k_concatenate(list(k_expand_dims(context_vector, 2), x))\n        \n      # passing the concatenated vector to the GRU\n      c(output, state) %<-% self$gru(x)\n        \n      # shape == (batch_size, 1, gru_units)\n      x <- self$fc1(output)\n        \n      # x shape == (batch_size, gru_units)\n      x <- k_reshape(x, c(-1, dim(x)[[3]]))\n        \n      # output shape == (batch_size, vocab_size)\n      x <- self$fc2(x)\n        \n      list(x, state, attention_weights)\n        \n    }\n  })\n}\n\n\nLoss function, and instantiating it all\nNow that we’ve defined our model (built of three custom models), we still need to actually instantiate it (being precise: the two classes we will access from outside, that is, the encoder and the decoder).\nWe also need to instantiate an optimizer (Adam will do), and define our loss function (categorical crossentropy).\nNote that tf$nn$sparse_softmax_cross_entropy_with_logits expects raw logits instead of softmax activations, and that we’re using the sparse variant because our labels are not one-hot-encoded.\n\n\nencoder <- cnn_encoder(embedding_dim)\ndecoder <- rnn_decoder(embedding_dim, gru_units, vocab_size)\n\noptimizer = tf$train$AdamOptimizer()\n\ncx_loss <- function(y_true, y_pred) {\n  mask <- 1 - k_cast(y_true == 0L, dtype = \"float32\")\n  loss <- tf$nn$sparse_softmax_cross_entropy_with_logits(\n    labels = y_true,\n    logits = y_pred\n  ) * mask\n  tf$reduce_mean(loss)\n}\n\n\nTraining\nTraining the captioning model is a time-consuming process, and you will for sure want to save the model’s weights!\nHow does this work with eager execution?\nWe create a tf$train$Checkpoint object, passing it the objects to be saved: In our case, the encoder, the decoder, and the optimizer. Later, at the end of each epoch, we will ask it to write the respective weights to disk.\n\n\nrestore_checkpoint <- FALSE\n\ncheckpoint_dir <- \"./checkpoints_captions\"\ncheckpoint_prefix <- file.path(checkpoint_dir, \"ckpt\")\ncheckpoint <- tf$train$Checkpoint(\n  optimizer = optimizer,\n  encoder = encoder,\n  decoder = decoder\n)\n\n\nAs we’re just starting to train the model, restore_checkpoint is set to false. Later, restoring the weights will be as easy as\n\n\nif (restore_checkpoint) {\n  checkpoint$restore(tf$train$latest_checkpoint(checkpoint_dir))\n}\n\n\nThe training loop is structured just like in the machine translation case: We loop over epochs, batches, and the training targets, feeding in the correct previous word at every timestep.\nAgain, tf$GradientTape takes care of recording the forward pass and calculating the gradients, and the optimizer applies the gradients to the model’s weights.\nAs each epoch ends, we also save the weights.\n\n\nnum_epochs <- 20\n\nif (!restore_checkpoint) {\n  for (epoch in seq_len(num_epochs)) {\n    \n    total_loss <- 0\n    progress <- 0\n    train_iter <- make_iterator_one_shot(train_dataset)\n    \n    until_out_of_range({\n      \n      batch <- iterator_get_next(train_iter)\n      loss <- 0\n      img_tensor <- batch[[1]]\n      target_caption <- batch[[2]]\n      \n      dec_hidden <- k_zeros(c(batch_size, gru_units))\n      \n      dec_input <- k_expand_dims(\n        rep(list(word_index_df[word_index_df$word == \"<start>\", \"index\"]), \n            batch_size)\n      )\n      \n      with(tf$GradientTape() %as% tape, {\n        \n        features <- encoder(img_tensor)\n        \n        for (t in seq_len(dim(target_caption)[2] - 1)) {\n          c(preds, dec_hidden, weights) %<-%\n            decoder(list(dec_input, features, dec_hidden))\n          loss <- loss + cx_loss(target_caption[, t], preds)\n          dec_input <- k_expand_dims(target_caption[, t])\n        }\n        \n      })\n      \n      total_loss <-\n        total_loss + loss / k_cast_to_floatx(dim(target_caption)[2])\n      \n      variables <- c(encoder$variables, decoder$variables)\n      gradients <- tape$gradient(loss, variables)\n      \n      optimizer$apply_gradients(purrr::transpose(list(gradients, variables)),\n                                global_step = tf$train$get_or_create_global_step()\n      )\n    })\n    cat(paste0(\n      \"\\n\\nTotal loss (epoch): \",\n      epoch,\n      \": \",\n      (total_loss / k_cast_to_floatx(buffer_size)) %>% as.double() %>% round(4),\n      \"\\n\"\n    ))\n    \n    checkpoint$save(file_prefix = checkpoint_prefix)\n  }\n}\n\n\nPeeking at results\nJust like in the translation case, it’s interesting to look at model performance during training. The companion code has that functionality integrated, so you can watch model progress for yourself.\nThe basic function here is get_caption: It gets passed the path to an image, loads it, obtains its features from Inception V3, and then asks the encoder-decoder model to generate a caption. If at any point the model produces the end symbol, we stop early. Otherwise, we continue until we hit the predefined maximum length.\n\nget_caption <-\n  function(image) {\n    attention_matrix <-\n      matrix(0, nrow = max_length, ncol = attention_features_shape)\n    temp_input <- k_expand_dims(load_image(image)[[1]], 1)\n    img_tensor_val <- image_model(temp_input)\n    img_tensor_val <- k_reshape(\n      img_tensor_val,\n      list(dim(img_tensor_val)[1], -1, dim(img_tensor_val)[4])\n    )\n    features <- encoder(img_tensor_val)\n    \n    dec_hidden <- k_zeros(c(1, gru_units))\n    dec_input <-\n      k_expand_dims(\n        list(word_index_df[word_index_df$word == \"<start>\", \"index\"])\n      )\n    \n    result <- \"\"\n    \n    for (t in seq_len(max_length - 1)) {\n      \n      c(preds, dec_hidden, attention_weights) %<-%\n        decoder(list(dec_input, features, dec_hidden))\n      attention_weights <- k_reshape(attention_weights, c(-1))\n      attention_matrix[t,] <- attention_weights %>% as.double()\n      \n      pred_idx <- tf$multinomial(exp(preds), num_samples = 1)[1, 1] \n                    %>% as.double()\n      pred_word <-\n        word_index_df[word_index_df$index == pred_idx, \"word\"]\n      \n      if (pred_word == \"<end>\") {\n        result <-\n          paste(result, pred_word)\n        attention_matrix <-\n          attention_matrix[1:length(str_split(result, \" \")[[1]]), , \n                           drop = FALSE]\n        return (list(result, attention_matrix))\n      } else {\n        result <-\n          paste(result, pred_word)\n        dec_input <- k_expand_dims(list(pred_idx))\n      }\n    }\n    \n    list(str_trim(result), attention_matrix)\n  }\n\nWith that functionality, now let’s actually do that: peek at results while the network is learning!\nWe’ve picked 3 examples each from the training and validation sets. Here they are.\nFirst, our picks from the training set:\n\nThree picks from the training set\nLet’s see the target captions:\na herd of giraffe standing on top of a grass covered field\na view of cards driving down a street\nthe skateboarding flips his board off of the sidewalk\nInterestingly, here we also have a demonstration of how labeled datasets (like anything human) may contain errors. (The samples were not picked for that; instead, they were chosen - without too much screening - for being rather unequivocal in their visual content.)\nNow for the validation candidates.\n\nThree picks from the validation set\nand their official captions:\na left handed pitcher throwing the base ball\na woman taking a bite of a slice of pizza in a restaraunt\na woman hitting swinging a tennis racket at a tennis ball on a tennis court\n(Again, any spelling peculiarities have not been introduced by us.)\nEpoch 1\nNow, what does our network produce after the first epoch? Remember that this means, having seen each one of the 24000 training images once.\nFirst then, here are the captions for the train images:\n\na group of sheep standing in the grass\n\n\na group of cars driving down a street\n\n\na man is standing on a street\n\nNot only is the syntax correct in every case, the content isn’t that bad either!\nHow about the validation set?\n\na baseball player is playing baseball uniform is holding a baseball bat\n\n\na man is holding a table with a table with a table with a table with a table with a table with a table with a table with a table with a table with a table with a table with a table with a table\n\n\na tennis player is holding a tennis court\n\nThis certainly tells that the network has been able to generalize over - let’s not call them concepts, but mappings between visual and textual entities, say It’s true that it will have seen some of these images before, because images come with several captions. You could be more strict setting up your training and validation sets - but here, we don’t really care about objective performance scores and so, it does not really matter.\nLet’ skip directly to epoch 20, our last training epoch, and check for further improvements.\nEpoch 20\nThis is what we get for the training images:\n\na group of many tall giraffe standing next to a sheep\n\n\na view of cards and white gloves on a street\n\n\na skateboarding flips his board\n\nAnd this, for the validation images.\n\na baseball catcher and umpire hit a baseball game\n\n\na man is eating a sandwich\n\n\na female tennis player is in the court\n\nI think we might agree that this still leaves room for improvement - but then, we only trained for 20 epochs and on a very small portion of the dataset.\nIn the above code snippets, you may have noticed the decoder returning an attention_matrix - but we weren’t commenting on it.\nNow finally, just as in the translation example, have a look what we can make of that.\nWhere does the network look?\nWe can visualize where the network is “looking” as it generates each word by overlaying the original image and the attention matrix. This example is taken from the 4th epoch.\nHere white-ish squares indicate areas receiving stronger focus. Compared to text-to-text translation though, the mapping is inherently less straightforward - where does one “look” when producing words like “and,” “the,” or “in?”\n\nAttention over image areas\nConclusion\nIt probably goes without saying that much better results are to be expected when training on (much!) more data and for much more time.\nApart from that, there are other options, though. The concept implemented here uses spatial attention over a uniform grid, that is, the attention mechanism guides the decoder where on the grid to look next when generating a caption.\nHowever, this is not the only way, and this is not how it works with humans. A much more plausible approach is a mix of top-down and bottom-up attention. E.g., (Anderson et al. 2017) use object detection techniques to bottom-up isolate interesting objects, and an LSTM stack wherein the first LSTM computes top-down attention guided by the output word generated by the second one.\nAnother interesting approach involving attention is using a multimodal attentive translator (Liu et al. 2017), where the image features are encoded and presented in a sequence, such that we end up with sequence models both on the encoding and the decoding sides.\nAnother alternative is to add a learned topic to the information input (Zhu, Xue, and Yuan 2018), which again is a top-down feature found in human cognition.\nIf you find one of these, or yet another, approach more convincing, an eager execution implementation, in the style of the above, will likely be a sound way of implementing it.\n\n\n\nAnderson, Peter, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. 2017. “Bottom-up and Top-down Attention for Image Captioning and VQA.” CoRR abs/1707.07998. http://arxiv.org/abs/1707.07998.\n\n\nLiu, Chang, Fuchun Sun, Changhu Wang, Feng Wang, and Alan L. Yuille. 2017. “A Multimodal Attentive Translator for Image Captioning.” CoRR abs/1702.05658. http://arxiv.org/abs/1702.05658.\n\n\nXu, Kelvin, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.” CoRR abs/1502.03044. http://arxiv.org/abs/1502.03044.\n\n\nZhu, Zhihao, Zhan Xue, and Zejian Yuan. 2018. “A Topic-Guided Attention for Image Captioning.” CoRR abs/1807.03514v1. https://arxiv.org/abs/1807.03514v1.\n\n\n\n\n",
    "preview": "posts/2018-09-17-eager-captioning/images/showattendandtell.png",
    "last_modified": "2024-11-21T15:50:26+00:00",
    "input_file": {},
    "preview_width": 627,
    "preview_height": 269
  },
  {
    "path": "posts/2018-09-10-eager-style-transfer/",
    "title": "Neural style transfer with eager execution and Keras",
    "description": "Continuing our series on combining Keras with TensorFlow eager execution, we show how to implement neural style transfer in a straightforward way. Based on this easy-to-adapt example, you can easily perform style transfer on your own images.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-09-10",
    "categories": [
      "TensorFlow/Keras",
      "Generative Models",
      "Image Recognition & Image Processing"
    ],
    "contents": "\nHow would your summer holiday’s photos look had Edvard Munch painted them? (Perhaps it’s better not to know).\nLet’s take a more comforting example: How would a nice, summarly river landscape look if painted by Katsushika Hokusai?\nStyle transfer on images is not new, but got a boost when Gatys, Ecker, and Bethge(Gatys, Ecker, and Bethge 2015) showed how to successfully do it with deep learning.\nThe main idea is straightforward: Create a hybrid that is a tradeoff between the content image we want to manipulate, and a style image we want to imitate, by optimizing for maximal resemblance to both at the same time.\nIf you’ve read the chapter on neural style transfer from Deep Learning with R, you may recognize some of the code snippets that follow.\nHowever, there is an important difference: This post uses TensorFlow Eager Execution, allowing for an imperative way of coding that makes it easy to map concepts to code.\nJust like previous posts on eager execution on this blog, this is a port of a Google Colaboratory notebook that performs the same task in Python.\nAs usual, please make sure you have the required package versions installed. And no need to copy the snippets - you’ll find the complete code among the Keras examples.\nPrerequisites\nThe code in this post depends on the most recent versions of several of the TensorFlow R packages. You can install these packages as follows:\ninstall.packages(c(\"tensorflow\", \"keras\", \"tfdatasets\"))\nYou should also be sure that you are running the very latest version of TensorFlow (v1.10), which you can install like so:\nlibrary(tensorflow)\ninstall_tensorflow()\nThere are additional requirements for using TensorFlow eager execution. First, we need to call tfe_enable_eager_execution() right at the beginning of the program. Second, we need to use the implementation of Keras included in TensorFlow, rather than the base Keras implementation.\n\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\n\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nlibrary(purrr)\nlibrary(glue)\n\n\nPrerequisites behind us, let’s get started!\nInput images\nHere is our content image - replace by an image of your own:\n\n\n# If you have enough memory on your GPU, no need to load the images\n# at such small size.\n# This is the size I found working for a 4G GPU.\nimg_shape <- c(128, 128, 3)\n\ncontent_path <- \"isar.jpg\"\n\ncontent_image <-  image_load(content_path, target_size = img_shape[1:2])\ncontent_image %>% \n  image_to_array() %>%\n  `/`(., 255) %>%\n  as.raster() %>%\n  plot()\n\n\n\nAnd here’s the style model, Hokusai’s The Great Wave off Kanagawa, which you can download from Wikimedia Commons:\n\n\nstyle_path <- \"The_Great_Wave_off_Kanagawa.jpg\"\n\nstyle_image <-  image_load(content_path, target_size = img_shape[1:2])\nstyle_image %>% \n  image_to_array() %>%\n  `/`(., 255) %>%\n  as.raster() %>%\n  plot()\n\n\n\nWe create a wrapper that loads and preprocesses the input images for us.\nAs we will be working with VGG19, a network that has been trained on ImageNet, we need to transform our input images in the same way that was used training it. Later, we’ll apply the inverse transformation to our combination image before displaying it.\n\n\nload_and_preprocess_image <- function(path) {\n  img <- image_load(path, target_size = img_shape[1:2]) %>%\n    image_to_array() %>%\n    k_expand_dims(axis = 1) %>%\n    imagenet_preprocess_input()\n}\n\ndeprocess_image <- function(x) {\n  x <- x[1, , ,]\n  # Remove zero-center by mean pixel\n  x[, , 1] <- x[, , 1] + 103.939\n  x[, , 2] <- x[, , 2] + 116.779\n  x[, , 3] <- x[, , 3] + 123.68\n  # 'BGR'->'RGB'\n  x <- x[, , c(3, 2, 1)]\n  x[x > 255] <- 255\n  x[x < 0] <- 0\n  x[] <- as.integer(x) / 255\n  x\n}\n\n\nSetting the scene\nWe are going to use a neural network, but we won’t be training it. Neural style transfer is a bit uncommon in that we don’t optimize the network’s weights, but back propagate the loss to the input layer (the image), in order to move it in the desired direction.\nWe will be interested in two kinds of outputs from the network, corresponding to our two goals.\nFirstly, we want to keep the combination image similar to the content image, on a high level. In a convnet, upper layers map to more holistic concepts, so we are picking a layer high up in the graph to compare outputs from the source and the combination.\nSecondly, the generated image should “look like” the style image. Style corresponds to lower level features like texture, shapes, strokes… So to compare the combination against the style example, we choose a set of lower level conv blocks for comparison and aggregate the results.\n\n\ncontent_layers <- c(\"block5_conv2\")\nstyle_layers <- c(\"block1_conv1\",\n                 \"block2_conv1\",\n                 \"block3_conv1\",\n                 \"block4_conv1\",\n                 \"block5_conv1\")\n\nnum_content_layers <- length(content_layers)\nnum_style_layers <- length(style_layers)\n\nget_model <- function() {\n  vgg <- application_vgg19(include_top = FALSE, weights = \"imagenet\")\n  vgg$trainable <- FALSE\n  style_outputs <- map(style_layers, function(layer) vgg$get_layer(layer)$output)\n  content_outputs <- map(content_layers, function(layer) vgg$get_layer(layer)$output)\n  model_outputs <- c(style_outputs, content_outputs)\n  keras_model(vgg$input, model_outputs)\n}\n\n\nLosses\nWhen optimizing the input image, we will consider three types of losses. Firstly, the content loss: How different is the combination image from the source? Here, we’re using the sum of the squared errors for comparison.\n\n\ncontent_loss <- function(content_image, target) {\n  k_sum(k_square(target - content_image))\n}\n\n\nOur second concern is having the styles match as closely as possible. Style is commonly operationalized as the Gram matrix of flattened feature maps in a layer. We thus assume that style is related to how maps in a layer correlate with other.\nWe therefore compute the Gram matrices of the layers we’re interested in (defined above), for the source image as well as the optimization candidate, and compare them, again using the sum of squared errors.\n\n\ngram_matrix <- function(x) {\n  features <- k_batch_flatten(k_permute_dimensions(x, c(3, 1, 2)))\n  gram <- k_dot(features, k_transpose(features))\n  gram\n}\n\nstyle_loss <- function(gram_target, combination) {\n  gram_comb <- gram_matrix(combination)\n  k_sum(k_square(gram_target - gram_comb)) /\n    (4 * (img_shape[3] ^ 2) * (img_shape[1] * img_shape[2]) ^ 2)\n}\n\n\nThirdly, we don’t want the combination image to look overly pixelated, thus we’re adding in a regularization component, the total variation in the image:\n\n\ntotal_variation_loss <- function(image) {\n  y_ij  <- image[1:(img_shape[1] - 1L), 1:(img_shape[2] - 1L),]\n  y_i1j <- image[2:(img_shape[1]), 1:(img_shape[2] - 1L),]\n  y_ij1 <- image[1:(img_shape[1] - 1L), 2:(img_shape[2]),]\n  a <- k_square(y_ij - y_i1j)\n  b <- k_square(y_ij - y_ij1)\n  k_sum(k_pow(a + b, 1.25))\n}\n\n\nThe tricky thing is how to combine these losses. We’ve reached acceptable results with the following weightings, but feel free to play around as you see fit:\n\n\ncontent_weight <- 100\nstyle_weight <- 0.8\ntotal_variation_weight <- 0.01\n\n\nGet model outputs for the content and style images\nWe need the model’s output for the content and style images, but here it suffices to do this just once.\nWe concatenate both images along the batch dimension, pass that input to the model, and get back a list of outputs, where every element of the list is a 4-d tensor. For the style image, we’re interested in the style outputs at batch position 1, whereas for the content image, we need the content output at batch position 2.\nIn the below comments, please note that the sizes of dimensions 2 and 3 will differ if you’re loading images at a different size.\n\n\nget_feature_representations <-\n  function(model, content_path, style_path) {\n    \n    # dim == (1, 128, 128, 3)\n    style_image <-\n      load_and_process_image(style_path) %>% k_cast(\"float32\")\n    # dim == (1, 128, 128, 3)\n    content_image <-\n      load_and_process_image(content_path) %>% k_cast(\"float32\")\n    # dim == (2, 128, 128, 3)\n    stack_images <- k_concatenate(list(style_image, content_image), axis = 1)\n    \n    # length(model_outputs) == 6\n    # dim(model_outputs[[1]]) = (2, 128, 128, 64)\n    # dim(model_outputs[[6]]) = (2, 8, 8, 512)\n    model_outputs <- model(stack_images)\n    \n    style_features <- \n      model_outputs[1:num_style_layers] %>%\n      map(function(batch) batch[1, , , ])\n    content_features <- \n      model_outputs[(num_style_layers + 1):(num_style_layers + num_content_layers)] %>%\n      map(function(batch) batch[2, , , ])\n    \n    list(style_features, content_features)\n  }\n\n\nComputing the losses\nOn every iteration, we need to pass the combination image through the model, obtain the style and content outputs, and compute the losses. Again, the code is extensively commented with tensor sizes for easy verification, but please keep in mind that the exact numbers presuppose you’re working with 128x128 images.\n\n\ncompute_loss <-\n  function(model, loss_weights, init_image, gram_style_features, content_features) {\n    \n    c(style_weight, content_weight) %<-% loss_weights\n    model_outputs <- model(init_image)\n    style_output_features <- model_outputs[1:num_style_layers]\n    content_output_features <-\n      model_outputs[(num_style_layers + 1):(num_style_layers + num_content_layers)]\n    \n    # style loss\n    weight_per_style_layer <- 1 / num_style_layers\n    style_score <- 0\n    # dim(style_zip[[5]][[1]]) == (512, 512)\n    style_zip <- transpose(list(gram_style_features, style_output_features))\n    for (l in 1:length(style_zip)) {\n      # for l == 1:\n      # dim(target_style) == (64, 64)\n      # dim(comb_style) == (1, 128, 128, 64)\n      c(target_style, comb_style) %<-% style_zip[[l]]\n      style_score <- style_score + weight_per_style_layer * \n        style_loss(target_style, comb_style[1, , , ])\n    }\n    \n    # content loss\n    weight_per_content_layer <- 1 / num_content_layers\n    content_score <- 0\n    content_zip <- transpose(list(content_features, content_output_features))\n    for (l in 1:length(content_zip)) {\n      # dim(comb_content) ==  (1, 8, 8, 512)\n      # dim(target_content) == (8, 8, 512)\n      c(target_content, comb_content) %<-% content_zip[[l]]\n      content_score <- content_score + weight_per_content_layer *\n        content_loss(comb_content[1, , , ], target_content)\n    }\n    \n    # total variation loss\n    variation_loss <- total_variation_loss(init_image[1, , ,])\n    \n    style_score <- style_score * style_weight\n    content_score <- content_score * content_weight\n    variation_score <- variation_loss * total_variation_weight\n    \n    loss <- style_score + content_score + variation_score\n    list(loss, style_score, content_score, variation_score)\n  }\n\n\nComputing the gradients\nAs soon as we have the losses, obtaining the gradients of the overall loss with respect to the input image is just a matter of calling tape$gradient on the GradientTape. Note that the nested call to compute_loss, and thus the call of the model on our combination image, happens inside the GradientTape context.\n\n\ncompute_grads <- \n  function(model, loss_weights, init_image, gram_style_features, content_features) {\n    with(tf$GradientTape() %as% tape, {\n      scores <-\n        compute_loss(model,\n                     loss_weights,\n                     init_image,\n                     gram_style_features,\n                     content_features)\n    })\n    total_loss <- scores[[1]]\n    list(tape$gradient(total_loss, init_image), scores)\n  }\n\n\nTraining phase\nNow it’s time to train! While the natural continuation of this sentence would have been “… the model,” the model we’re training here is not VGG19 (that one we’re just using as a tool), but a minimal setup of just:\na Variable that holds our to-be-optimized image\nthe loss functions we defined above\nan optimizer that will apply the calculated gradients to the image variable (tf$train$AdamOptimizer)\nBelow, we get the style features (of the style image) and the content feature (of the content image) just once, then iterate over the optimization process, saving the output every 100 iterations.\nIn contrast to the original article and the Deep Learning with R book, but following the Google notebook instead, we’re not using L-BFGS for optimization, but Adam, as our goal here is to provide a concise introduction to eager execution.\nHowever, you could plug in another optimization method if you wanted, replacing\noptimizer$apply_gradients(list(tuple(grads, init_image)))\nby an algorithm of your choice (and of course, assigning the result of the optimization to the Variable holding the image).\n\n\nrun_style_transfer <- function(content_path, style_path) {\n  model <- get_model()\n  walk(model$layers, function(layer) layer$trainable = FALSE)\n  \n  c(style_features, content_features) %<-% \n    get_feature_representations(model, content_path, style_path)\n  # dim(gram_style_features[[1]]) == (64, 64)\n  gram_style_features <- map(style_features, function(feature) gram_matrix(feature))\n  \n  init_image <- load_and_process_image(content_path)\n  init_image <- tf$contrib$eager$Variable(init_image, dtype = \"float32\")\n  \n  optimizer <- tf$train$AdamOptimizer(learning_rate = 1,\n                                      beta1 = 0.99,\n                                      epsilon = 1e-1)\n  \n  c(best_loss, best_image) %<-% list(Inf, NULL)\n  loss_weights <- list(style_weight, content_weight)\n  \n  start_time <- Sys.time()\n  global_start <- Sys.time()\n  \n  norm_means <- c(103.939, 116.779, 123.68)\n  min_vals <- -norm_means\n  max_vals <- 255 - norm_means\n  \n  for (i in seq_len(num_iterations)) {\n    # dim(grads) == (1, 128, 128, 3)\n    c(grads, all_losses) %<-% compute_grads(model,\n                                            loss_weights,\n                                            init_image,\n                                            gram_style_features,\n                                            content_features)\n    c(loss, style_score, content_score, variation_score) %<-% all_losses\n    optimizer$apply_gradients(list(tuple(grads, init_image)))\n    clipped <- tf$clip_by_value(init_image, min_vals, max_vals)\n    init_image$assign(clipped)\n    \n    end_time <- Sys.time()\n    \n    if (k_cast_to_floatx(loss) < best_loss) {\n      best_loss <- k_cast_to_floatx(loss)\n      best_image <- init_image\n    }\n    \n    if (i %% 50 == 0) {\n      glue(\"Iteration: {i}\") %>% print()\n      glue(\n        \"Total loss: {k_cast_to_floatx(loss)},\n        style loss: {k_cast_to_floatx(style_score)},\n        content loss: {k_cast_to_floatx(content_score)},\n        total variation loss: {k_cast_to_floatx(variation_score)},\n        time for 1 iteration: {(Sys.time() - start_time) %>% round(2)}\"\n      ) %>% print()\n      \n      if (i %% 100 == 0) {\n        png(paste0(\"style_epoch_\", i, \".png\"))\n        plot_image <- best_image$numpy()\n        plot_image <- deprocess_image(plot_image)\n        plot(as.raster(plot_image), main = glue(\"Iteration {i}\"))\n        dev.off()\n      }\n    }\n  }\n  \n  glue(\"Total time: {Sys.time() - global_start} seconds\") %>% print()\n  list(best_image, best_loss)\n}\n\n\nReady to run\nNow, we’re ready to start the process:\n\n\nc(best_image, best_loss) %<-% run_style_transfer(content_path, style_path)\n\n\nIn our case, results didn’t change much after ~ iteration 1000, and this is how our river landscape was looking:\n\n\n\n… definitely more inviting than had it been painted by Edvard Munch!\nConclusion\nWith neural style transfer, some fiddling around may be needed until you get the result you want. But as our example shows, this doesn’t mean the code has to be complicated. Additionally to being easy to grasp, eager execution also lets you add debugging output, and step through the code line-by-line to check on tensor shapes.\nUntil next time in our eager execution series!\n\n\n\nGatys, Leon A., Alexander S. Ecker, and Matthias Bethge. 2015. “A Neural Algorithm of Artistic Style.” CoRR abs/1508.06576. http://arxiv.org/abs/1508.06576.\n\n\n\n\n",
    "preview": "posts/2018-09-10-eager-style-transfer/images/preview.png",
    "last_modified": "2024-11-21T15:53:43+00:00",
    "input_file": {},
    "preview_width": 344,
    "preview_height": 231
  },
  {
    "path": "posts/2018-09-07-getting-started/",
    "title": "Getting started with deep learning in R",
    "description": "Many fields are benefiting from the use of deep learning, and with the R keras, tensorflow and related packages, you can now easily do state of the art deep learning in R. In this post, we want to give some orientation as to how to best get started.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-09-07",
    "categories": [
      "TensorFlow/Keras"
    ],
    "contents": "\nThere are good reasons to get into deep learning: Deep learning has been outperforming the respective “classical” techniques in areas like image recognition and natural language processing for a while now, and it has the potential to bring interesting insights even to the analysis of tabular data. For many R users interested in deep learning, the hurdle is not so much the mathematical prerequisites (as many have a background in statistics or empirical sciences), but rather how to get started in an efficient way.\nThis post will give an overview of some materials that should prove useful. In the case that you don’t have that background in statistics or similar, we will also present a few helpful resources to catch up with “the math”.\nKeras tutorials\nThe easiest way to get started is using the Keras API. It is a high-level, declarative (in feel) way of specifying a model, training and testing it, originally developed in Python by Francois Chollet and ported to R by JJ Allaire.\nCheck out the tutorials on the Keras website: They introduce basic tasks like classification and regression, as well as basic workflow elements like saving and restoring models, or assessing model performance.\nBasic classification gets you started doing image classification using the Fashion MNIST dataset.\nText classification shows how to do sentiment analysis on movie reviews, and includes the important topic of how to preprocess text for deep learning.\nBasic regression demonstrates the task of predicting a continuous variable by example of the famous Boston housing dataset that ships with Keras.\nOverfitting and underfitting explains how you can assess if your model is under- or over-fitting, and what remedies to take.\nLast but not least, Save and restore models shows how to save checkpoints during and after training, so you don’t lose the fruit of the network’s labor.\nOnce you’ve seen the basics, the website also has more advanced information on implementing custom logic, monitoring and tuning, as well as using and adapting pre-trained models.\nVideos and book\nIf you want a bit more conceptual background, the Deep Learning with R in motion video series provides a nice introduction to basic concepts of machine learning and deep learning, including things often taken for granted, such as derivatives and gradients.\n\nThe first 2 components of the video series (Getting Started and the MNIST Case Study) are free. The remainder of the videos introduce different neural network architectures by way of detailed case studies.\nThe series is a companion to the Deep Learning with R book by Francois Chollet and JJ Allaire. Like the videos, the book has excellent, high-level explanations of deep learning concepts. At the same time, it contains lots of ready-to-use code, presenting examples for all the major architectures and use cases (including fancy stuff like variational autoencoders and GANs).\n\nInspiration\nIf you’re not pursuing a specific goal, but in general curious about what can be done with deep learning, a good place to follow is the TensorFlow for R Blog. There, you’ll find applications of deep learning to business as well as scientific tasks, as well as technical expositions and introductions to new features.\nIn addition, the TensorFlow for R Gallery highlights several case studies that have proven especially useful for getting started in various areas of application.\nReality\nOnce the ideas are there, realization should follow, and for most of us the question will be: Where can I actually train that model? As soon as real-world-size images are involved, or other kinds of higher-dimensional data, you’ll need a modern, high performance GPU so training on your laptop won’t be an option any more.\nThere are a few different ways you can train in the cloud:\nRStudio provides Amazon EC2 AMIs for cloud GPU instances. The AMI has both RStudio Server and the R TensorFlow package suite preinstalled.\nYou can also try out Paperspace cloud GPU desktops (again with the RStudio and the R TensorFlow package suite preinstalled).\nThe cloudml package provides an interface to the Google Cloud Machine Learning engine, which makes it easy to submit batch GPU training jobs to CloudML.\nMore background\nIf you don’t have a very “mathy” background, you might feel that you’d like to supplement the concepts-focused approach from Deep Learning with R with a bit more low-level basics (just as some people feel the need to know at least a bit of C or Assembler when learning a high-level language).\nPersonal recommendations for such cases would include Andrew Ng’s deep learning specialization on Coursera (videos are free to watch), and the book(s) and recorded lectures on linear algebra by Gilbert Strang.\nOf course, the ultimate reference on deep learning, as of today, is the Deep Learning textbook by Ian Goodfellow, Yoshua Bengio and Aaron Courville. The book covers everything from background in linear algebra, probability theory and optimization via basic architectures such as CNNs or RNNs, on to unsupervised models on the frontier of the very latest research.\nGetting help\nLast not least, should you encounter problems with the software (or with mapping your task to runnable code), a good idea is to create a GitHub issue in the respective repository, e.g., rstudio/keras.\nBest of luck for your deep learning journey with R!\n\n\n\n",
    "preview": "posts/2018-09-07-getting-started/images/digits.png",
    "last_modified": "2024-11-21T15:54:19+00:00",
    "input_file": {},
    "preview_width": 557,
    "preview_height": 317
  },
  {
    "path": "posts/2018-08-26-eager-dcgan/",
    "title": "Generating images with Keras and TensorFlow eager execution",
    "description": "Generative adversarial networks (GANs) are a popular deep learning approach to generating new entities (often but not always images). We show how to code them using Keras and TensorFlow eager execution.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-08-26",
    "categories": [
      "TensorFlow/Keras",
      "Generative Models",
      "Image Recognition & Image Processing"
    ],
    "contents": "\nThe recent announcement of TensorFlow 2.0 names eager execution as the number one central feature of the new major version. What does this mean for R users?\nAs demonstrated in our recent post on neural machine translation, you can use eager execution from R now already, in combination with Keras custom models and the datasets API. It’s good to know you can use it - but why should you? And in which cases?\nIn this and a few upcoming posts, we want to show how eager execution can make developing models a lot easier. The degree of simplication will depend on the task - and just how much easier you’ll find the new way might also depend on your experience using the functional API to model more complex relationships.\nEven if you think that GANs, encoder-decoder architectures, or neural style transfer didn’t pose any problems before the advent of eager execution, you might find that the alternative is a better fit to how we humans mentally picture problems.\nFor this post, we are porting code from a recent Google Colaboratory notebook implementing the DCGAN architecture.(Radford, Metz, and Chintala 2015)\nNo prior knowledge of GANs is required - we’ll keep this post practical (no maths) and focus on how to achieve your goal, mapping a simple and vivid concept into an astonishingly small number of lines of code.\nAs in the post on machine translation with attention, we first have to cover some prerequisites.\nBy the way, no need to copy out the code snippets - you’ll find the complete code in eager_dcgan.R).\nPrerequisites\nThe code in this post depends on the newest CRAN versions of several of the TensorFlow R packages. You can install these packages as follows:\ninstall.packages(c(\"tensorflow\", \"keras\", \"tfdatasets\"))\nYou should also be sure that you are running the very latest version of TensorFlow (v1.10), which you can install like so:\nlibrary(tensorflow)\ninstall_tensorflow()\nThere are additional requirements for using TensorFlow eager execution. First, we need to call tfe_enable_eager_execution() right at the beginning of the program. Second, we need to use the implementation of Keras included in TensorFlow, rather than the base Keras implementation.\nWe’ll also use the tfdatasets package for our input pipeline. So we end up with the following preamble to set things up:\n\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\n\nlibrary(tensorflow)\ntfe_enable_eager_execution(device_policy = \"silent\")\n\nlibrary(tfdatasets)\n\n\nThat’s it. Let’s get started.\nSo what’s a GAN?\nGAN stands for Generative Adversarial Network(Goodfellow et al. 2014). It is a setup of two agents, the generator and the discriminator, that act against each other (thus, adversarial). It is generative because the goal is to generate output (as opposed to, say, classification or regression).\nIn human learning, feedback - direct or indirect - plays a central role. Say we wanted to forge a banknote (as long as those still exist). Assuming we can get away with unsuccessful trials, we would get better and better at forgery over time. Optimizing our technique, we would end up rich.\nThis concept of optimizing from feedback is embodied in the first of the two agents, the generator. It gets its feedback from the discriminator, in an upside-down way: If it can fool the discriminator, making it believe that the banknote was real, all is fine; if the discriminator notices the fake, it has to do things differently. For a neural network, that means it has to update its weights.\nHow does the discriminator know what is real and what is fake? It too has to be trained, on real banknotes (or whatever the kind of objects involved) and the fake ones produced by the generator. So the complete setup is two agents competing, one striving to generate realistic-looking fake objects, and the other, to disavow the deception. The purpose of training is to have both evolve and get better, in turn causing the other to get better, too.\nIn this system, there is no objective minimum to the loss function: We want both components to learn and getter better “in lockstep,” instead of one winning out over the other. This makes optimization difficult.\nIn practice therefore, tuning a GAN can seem more like alchemy than like science, and it often makes sense to lean on practices and “tricks” reported by others.\nIn this example, just like in the Google notebook we’re porting, the goal is to generate MNIST digits. While that may not sound like the most exciting task one could imagine, it lets us focus on the mechanics, and allows us to keep computation and memory requirements (comparatively) low.\nLet’s load the data (training set needed only) and then, look at the first actor in our drama, the generator.\nTraining data\n\n\nmnist <- dataset_mnist()\nc(train_images, train_labels) %<-% mnist$train\n\ntrain_images <- train_images %>% \n  k_expand_dims() %>%\n  k_cast(dtype = \"float32\")\n\n# normalize images to [-1, 1] because the generator uses tanh activation\ntrain_images <- (train_images - 127.5) / 127.5\n\n\nOur complete training set will be streamed once per epoch:\n\n\nbuffer_size <- 60000\nbatch_size <- 256\nbatches_per_epoch <- (buffer_size / batch_size) %>% round()\n\ntrain_dataset <- tensor_slices_dataset(train_images) %>%\n  dataset_shuffle(buffer_size) %>%\n  dataset_batch(batch_size)\n\n\nThis input will be fed to the discriminator only.\nGenerator\nBoth generator and discriminator are Keras custom models.\nIn contrast to custom layers, custom models allow you to construct models as independent units, complete with custom forward pass logic, backprop and optimization. The model-generating function defines the layers the model (self) wants assigned, and returns the function that implements the forward pass.\nAs we will soon see, the generator gets passed vectors of random noise for input. This vector is transformed to 3d (height, width, channels) and then, successively upsampled to the required output size of (28,28,3).\n\n\ngenerator <-\n  function(name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      \n      self$fc1 <- layer_dense(units = 7 * 7 * 64, use_bias = FALSE)\n      self$batchnorm1 <- layer_batch_normalization()\n      self$leaky_relu1 <- layer_activation_leaky_relu()\n      self$conv1 <-\n        layer_conv_2d_transpose(\n          filters = 64,\n          kernel_size = c(5, 5),\n          strides = c(1, 1),\n          padding = \"same\",\n          use_bias = FALSE\n        )\n      self$batchnorm2 <- layer_batch_normalization()\n      self$leaky_relu2 <- layer_activation_leaky_relu()\n      self$conv2 <-\n        layer_conv_2d_transpose(\n          filters = 32,\n          kernel_size = c(5, 5),\n          strides = c(2, 2),\n          padding = \"same\",\n          use_bias = FALSE\n        )\n      self$batchnorm3 <- layer_batch_normalization()\n      self$leaky_relu3 <- layer_activation_leaky_relu()\n      self$conv3 <-\n        layer_conv_2d_transpose(\n          filters = 1,\n          kernel_size = c(5, 5),\n          strides = c(2, 2),\n          padding = \"same\",\n          use_bias = FALSE,\n          activation = \"tanh\"\n        )\n      \n      function(inputs, mask = NULL, training = TRUE) {\n        self$fc1(inputs) %>%\n          self$batchnorm1(training = training) %>%\n          self$leaky_relu1() %>%\n          k_reshape(shape = c(-1, 7, 7, 64)) %>%\n          self$conv1() %>%\n          self$batchnorm2(training = training) %>%\n          self$leaky_relu2() %>%\n          self$conv2() %>%\n          self$batchnorm3(training = training) %>%\n          self$leaky_relu3() %>%\n          self$conv3()\n      }\n    })\n  }\n\n\nDiscriminator\nThe discriminator is just a pretty normal convolutional network outputting a score. Here, usage of “score” instead of “probability” is on purpose: If you look at the last layer, it is fully connected, of size 1 but lacking the usual sigmoid activation. This is because unlike Keras’ loss_binary_crossentropy, the loss function we’ll be using here - tf$losses$sigmoid_cross_entropy - works with the raw logits, not the outputs of the sigmoid.\n\n\ndiscriminator <-\n  function(name = NULL) {\n    keras_model_custom(name = name, function(self) {\n      \n      self$conv1 <- layer_conv_2d(\n        filters = 64,\n        kernel_size = c(5, 5),\n        strides = c(2, 2),\n        padding = \"same\"\n      )\n      self$leaky_relu1 <- layer_activation_leaky_relu()\n      self$dropout <- layer_dropout(rate = 0.3)\n      self$conv2 <-\n        layer_conv_2d(\n          filters = 128,\n          kernel_size = c(5, 5),\n          strides = c(2, 2),\n          padding = \"same\"\n        )\n      self$leaky_relu2 <- layer_activation_leaky_relu()\n      self$flatten <- layer_flatten()\n      self$fc1 <- layer_dense(units = 1)\n      \n      function(inputs, mask = NULL, training = TRUE) {\n        inputs %>% self$conv1() %>%\n          self$leaky_relu1() %>%\n          self$dropout(training = training) %>%\n          self$conv2() %>%\n          self$leaky_relu2() %>%\n          self$flatten() %>%\n          self$fc1()\n      }\n    })\n  }\n\n\nSetting the scene\nBefore we can start training, we need to create the usual components of a deep learning setup: the model (or models, in this case), the loss function(s), and the optimizer(s).\nModel creation is just a function call, with a little extra on top:\n\n\ngenerator <- generator()\ndiscriminator <- discriminator()\n\n# https://www.tensorflow.org/api_docs/python/tf/contrib/eager/defun\ngenerator$call = tf$contrib$eager$defun(generator$call)\ndiscriminator$call = tf$contrib$eager$defun(discriminator$call)\n\n\ndefun compiles an R function (once per different combination of argument shapes and non-tensor objects values)) into a TensorFlow graph, and is used to speed up computations. This comes with side effects and possibly unexpected behavior - please consult the documentation for the details. Here, we were mainly curious in how much of a speedup we might notice when using this from R - in our example, it resulted in a speedup of 130%.\nOn to the losses. Discriminator loss consists of two parts: Does it correctly identify real images as real, and does it correctly spot fake images as fake.\nHere real_output and generated_output contain the logits returned from the discriminator - that is, its judgment of whether the respective images are fake or real.\n\n\ndiscriminator_loss <- function(real_output, generated_output) {\n  real_loss <- tf$losses$sigmoid_cross_entropy(\n    multi_class_labels = k_ones_like(real_output),\n    logits = real_output)\n  generated_loss <- tf$losses$sigmoid_cross_entropy(\n    multi_class_labels = k_zeros_like(generated_output),\n    logits = generated_output)\n  real_loss + generated_loss\n}\n\n\nGenerator loss depends on how the discriminator judged its creations: It would hope for them all to be seen as real.\n\n\ngenerator_loss <- function(generated_output) {\n  tf$losses$sigmoid_cross_entropy(\n    tf$ones_like(generated_output),\n    generated_output)\n}\n\n\nNow we still need to define optimizers, one for each model.\n\n\ndiscriminator_optimizer <- tf$train$AdamOptimizer(1e-4)\ngenerator_optimizer <- tf$train$AdamOptimizer(1e-4)\n\n\nTraining loop\nThere are two models, two loss functions and two optimizers, but there is just one training loop, as both models depend on each other.\nThe training loop will be over MNIST images streamed in batches, but we still need input to the generator - a random vector of size 100, in this case.\n\n\nnoise_dim <- 100\n\n\nLet’s take the training loop step by step.\nThere will be an outer and an inner loop, one over epochs and one over batches.\nAt the start of each epoch, we create a fresh iterator over the dataset:\n\nfor (epoch in seq_len(num_epochs)) {\n  start <- Sys.time()\n  total_loss_gen <- 0\n  total_loss_disc <- 0\n  iter <- make_iterator_one_shot(train_dataset)\n\nNow for every batch we obtain from the iterator, we are calling the generator and having it generate images from random noise. Then, we’re calling the dicriminator on real images as well as the fake images just generated. For the discriminator, its relative outputs are directly fed into the loss function. For the generator, its loss will depend on how the discriminator judged its creations:\n\nuntil_out_of_range({\n  batch <- iterator_get_next(iter)\n  noise <- k_random_normal(c(batch_size, noise_dim))\n  with(tf$GradientTape() %as% gen_tape, { with(tf$GradientTape() %as% disc_tape, {\n    generated_images <- generator(noise)\n    disc_real_output <- discriminator(batch, training = TRUE)\n    disc_generated_output <-\n       discriminator(generated_images, training = TRUE)\n    gen_loss <- generator_loss(disc_generated_output)\n    disc_loss <- discriminator_loss(disc_real_output, disc_generated_output)\n  }) })\n\nNote that all model calls happen inside tf$GradientTape contexts. This is so the forward passes can be recorded and “played back” to back propagate the losses through the network.\nObtain the gradients of the losses to the respective models’ variables (tape$gradient) and have the optimizers apply them to the models’ weights (optimizer$apply_gradients):\n\n\ngradients_of_generator <-\n  gen_tape$gradient(gen_loss, generator$variables)\ngradients_of_discriminator <-\n  disc_tape$gradient(disc_loss, discriminator$variables)\n      \ngenerator_optimizer$apply_gradients(purrr::transpose(\n  list(gradients_of_generator, generator$variables)\n))\ndiscriminator_optimizer$apply_gradients(purrr::transpose(\n  list(gradients_of_discriminator, discriminator$variables)\n))\n      \ntotal_loss_gen <- total_loss_gen + gen_loss\ntotal_loss_disc <- total_loss_disc + disc_loss\n\n\nThis ends the loop over batches. Finish off the loop over epochs displaying current losses and saving a few of the generator’s artwork:\n\n\ncat(\"Time for epoch \", epoch, \": \", Sys.time() - start, \"\\n\")\ncat(\"Generator loss: \", total_loss_gen$numpy() / batches_per_epoch, \"\\n\")\ncat(\"Discriminator loss: \", total_loss_disc$numpy() / batches_per_epoch, \"\\n\\n\")\nif (epoch %% 10 == 0)\n  generate_and_save_images(generator,\n                           epoch,\n                           random_vector_for_generation)\n\n\nHere’s the training loop again, shown as a whole - even including the lines for reporting on progress, it is remarkably concise, and allows for a quick grasp of what is going on:\n\n\ntrain <- function(dataset, epochs, noise_dim) {\n  for (epoch in seq_len(num_epochs)) {\n    start <- Sys.time()\n    total_loss_gen <- 0\n    total_loss_disc <- 0\n    iter <- make_iterator_one_shot(train_dataset)\n    \n    until_out_of_range({\n      batch <- iterator_get_next(iter)\n      noise <- k_random_normal(c(batch_size, noise_dim))\n      with(tf$GradientTape() %as% gen_tape, { with(tf$GradientTape() %as% disc_tape, {\n        generated_images <- generator(noise)\n        disc_real_output <- discriminator(batch, training = TRUE)\n        disc_generated_output <-\n          discriminator(generated_images, training = TRUE)\n        gen_loss <- generator_loss(disc_generated_output)\n        disc_loss <-\n          discriminator_loss(disc_real_output, disc_generated_output)\n      }) })\n      \n      gradients_of_generator <-\n        gen_tape$gradient(gen_loss, generator$variables)\n      gradients_of_discriminator <-\n        disc_tape$gradient(disc_loss, discriminator$variables)\n      \n      generator_optimizer$apply_gradients(purrr::transpose(\n        list(gradients_of_generator, generator$variables)\n      ))\n      discriminator_optimizer$apply_gradients(purrr::transpose(\n        list(gradients_of_discriminator, discriminator$variables)\n      ))\n      \n      total_loss_gen <- total_loss_gen + gen_loss\n      total_loss_disc <- total_loss_disc + disc_loss\n      \n    })\n    \n    cat(\"Time for epoch \", epoch, \": \", Sys.time() - start, \"\\n\")\n    cat(\"Generator loss: \", total_loss_gen$numpy() / batches_per_epoch, \"\\n\")\n    cat(\"Discriminator loss: \", total_loss_disc$numpy() / batches_per_epoch, \"\\n\\n\")\n    if (epoch %% 10 == 0)\n      generate_and_save_images(generator,\n                               epoch,\n                               random_vector_for_generation)\n    \n  }\n}\n\n\nHere’s the function for saving generated images…\n\n\ngenerate_and_save_images <- function(model, epoch, test_input) {\n  predictions <- model(test_input, training = FALSE)\n  png(paste0(\"images_epoch_\", epoch, \".png\"))\n  par(mfcol = c(5, 5))\n  par(mar = c(0.5, 0.5, 0.5, 0.5),\n      xaxs = 'i',\n      yaxs = 'i')\n  for (i in 1:25) {\n    img <- predictions[i, , , 1]\n    img <- t(apply(img, 2, rev))\n    image(\n      1:28,\n      1:28,\n      img * 127.5 + 127.5,\n      col = gray((0:255) / 255),\n      xaxt = 'n',\n      yaxt = 'n'\n    )\n  }\n  dev.off()\n}\n\n\n… and we’re ready to go!\n\n\nnum_epochs <- 150\ntrain(train_dataset, num_epochs, noise_dim)\n\n\nResults\nHere are some generated images after training for 150 epochs:\n\nAs they say, your results will most certainly vary!\nConclusion\nWhile certainly tuning GANs will remain a challenge, we hope we were able to show that mapping concepts to code is not difficult when using eager execution. In case you’ve played around with GANs before, you may have found you needed to pay careful attention to set up the losses the right way, freeze the discriminator’s weights when needed, etc. This need goes away with eager execution.\nIn upcoming posts, we will show further examples where using it makes model development easier.\n\n\n\nGoodfellow, Ian J., Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. 2014. “Generative Adversarial Nets.” In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, 2672–80. http://papers.nips.cc/paper/5423-generative-adversarial-nets.\n\n\nRadford, Alec, Luke Metz, and Soumith Chintala. 2015. “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.” CoRR abs/1511.06434. http://arxiv.org/abs/1511.06434.\n\n\n\n\n",
    "preview": "posts/2018-08-26-eager-dcgan/images/thumb.png",
    "last_modified": "2024-11-21T15:51:26+00:00",
    "input_file": {},
    "preview_width": 240,
    "preview_height": 144
  },
  {
    "path": "posts/2018-07-30-attention-layer/",
    "title": "Attention-based Neural Machine Translation with Keras",
    "description": "As sequence to sequence prediction tasks get more involved, attention mechanisms have proven helpful. A prominent example is neural machine translation. Following a recent Google Colaboratory notebook, we show how to implement attention in R.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-07-30",
    "categories": [
      "Natural Language Processing",
      "TensorFlow/Keras"
    ],
    "contents": "\nThese days it is not difficult to find sample code that demonstrates sequence to sequence translation using Keras. However, within the past few years it has been established that depending on the task, incorporating an attention mechanism significantly improves performance.\nFirst and foremost, this was the case for neural machine translation (see (Bahdanau, Cho, and Bengio 2014) and (Luong, Pham, and Manning 2015) for prominent work).\nBut other areas performing sequence to sequence translation were profiting from incorporating an attention mechanism, too: E.g., (Xu et al. 2015) applied attention to image captioning, and (Vinyals et al. 2014), to parsing.\nIdeally, using Keras, we’d just have an attention layer managing this for us. Unfortunately, as can be seen googling for code snippets and blog posts, implementing attention in pure Keras is not that straightforward.\nConsequently, until a short time ago, the best thing to do seemed to be translating the TensorFlow Neural Machine Translation Tutorial to R TensorFlow. Then, TensorFlow eager execution happened, and turned out a game changer for a number of things that used to be difficult (not the least of which is debugging). With eager execution, tensor operations are executed immediately, as opposed to of building a graph to be evaluated later. This means we can immediately inspect the values in our tensors - and it also means we can imperatively code loops to perform interleavings of sorts that earlier were more challenging to accomplish.\nUnder these circumstances, it is not surprising that the interactive notebook on neural machine translation, published on Colaboratory, got a lot of attention for its straightforward implementation and highly intellegible explanations.\nOur goal here is to do the same thing from R. We will not end up with Keras code exactly the way we used to write it, but a hybrid of Keras layers and imperative code enabled by TensorFlow eager execution.\nPrerequisites\nThe code in this post depends on the development versions of several of the TensorFlow R packages. You can install these packages as follows:\ndevtools::install_github(c(\n  \"rstudio/reticulate\",\n  \"rstudio/tensorflow\",\n  \"rstudio/keras\",\n  \"rstudio/tfdatasets\"\n))\nYou should also be sure that you are running the very latest version of TensorFlow (v1.9), which you can install like so:\nlibrary(tensorflow)\ninstall_tensorflow()\nThere are additional requirements for using TensorFlow eager execution. First, we need to call tfe_enable_eager_execution() right at the beginning of the program. Second, we need to use the implementation of Keras included in TensorFlow, rather than the base Keras implementation. This is because at a later point, we are going to access model$variables which at this point does not exist in base Keras.\nWe’ll also use the tfdatasets package for our input pipeline. So we end up with the below libraries needed for this example.\nOne more aside: Please don’t copy-paste the code from the snippets for execution - you’ll find the complete code for this post here. In the post, we may deviate from required execution order for purposes of narrative.\n\n\nlibrary(keras)\nuse_implementation(\"tensorflow\")\n\nlibrary(tensorflow)\ntfe_enable_eager_execution()\n\nlibrary(tfdatasets)\n\nlibrary(purrr)\nlibrary(stringr)\nlibrary(reshape2)\nlibrary(viridis)\nlibrary(ggplot2)\nlibrary(tibble)\n\n\nPreparing the data\nAs our focus is on implementing the attention mechanism, we’re going to do a quick pass through pre-preprocessing.\nAll operations are contained in short functions that are independently testable (which also makes it easy should you want to experiment with different preprocessing actions).\nThe site https://www.manythings.org/anki/ is a great source for multilingual datasets. For variation, we’ll choose a different dataset from the colab notebook, and try to translate English to Dutch. I’m going to assume you have the unzipped file nld.txt in a subdirectory called data in your current directory.\nThe file contains 28224 sentence pairs, of which we are going to use the first 10000. Under this restriction, sentences range from one-word exclamations\nRun!    Ren!\nWow!    Da's niet gek!\nFire!   Vuur!\nover short phrases\nAre you crazy?  Ben je gek?\nDo cats dream?  Dromen katten?\nFeed the bird!  Geef de vogel voer!\nto simple sentences such as\nMy brother will kill me.    Mijn broer zal me vermoorden.\nNo one knows the future.    Niemand kent de toekomst.\nPlease ask someone else.    Vraag alsjeblieft iemand anders.\n\n\nfilepath <- file.path(\"data\", \"nld.txt\")\n\nlines <- readLines(filepath, n = 10000)\nsentences <- str_split(lines, \"\\t\")\n\n\nBasic preprocessing includes adding space before punctuation, replacing special characters, reducing multiple spaces to one, and adding <start> and <stop> tokens at the beginnings resp. ends of the sentences.\n\n\nspace_before_punct <- function(sentence) {\n  str_replace_all(sentence, \"([?.!])\", \" \\\\1\")\n}\n\nreplace_special_chars <- function(sentence) {\n  str_replace_all(sentence, \"[^a-zA-Z?.!,¿]+\", \" \")\n}\n\nadd_tokens <- function(sentence) {\n  paste0(\"<start> \", sentence, \" <stop>\")\n}\nadd_tokens <- Vectorize(add_tokens, USE.NAMES = FALSE)\n\npreprocess_sentence <- compose(add_tokens,\n                               str_squish,\n                               replace_special_chars,\n                               space_before_punct)\n\nword_pairs <- map(sentences, preprocess_sentence)\n\n\nAs usual with text data, we need to create lookup indices to get from words to integers and vice versa: one index each for the source and target languages.\n\n\ncreate_index <- function(sentences) {\n  unique_words <- sentences %>% unlist() %>% paste(collapse = \" \") %>%\n    str_split(pattern = \" \") %>% .[[1]] %>% unique() %>% sort()\n  index <- data.frame(\n    word = unique_words,\n    index = 1:length(unique_words),\n    stringsAsFactors = FALSE\n  ) %>%\n    add_row(word = \"<pad>\",\n                    index = 0,\n                    .before = 1)\n  index\n}\n\nword2index <- function(word, index_df) {\n  index_df[index_df$word == word, \"index\"]\n}\nindex2word <- function(index, index_df) {\n  index_df[index_df$index == index, \"word\"]\n}\n\nsrc_index <- create_index(map(word_pairs, ~ .[[1]]))\ntarget_index <- create_index(map(word_pairs, ~ .[[2]]))\n\n\nConversion of text to integers uses the above indices as well as Keras’ convenient pad_sequences function, which leaves us with matrices of integers, padded up to maximum sentence length found in the source and target corpora, respectively.\n\n\nsentence2digits <- function(sentence, index_df) {\n  map((sentence %>% str_split(pattern = \" \"))[[1]], function(word)\n    word2index(word, index_df))\n}\n\nsentlist2diglist <- function(sentence_list, index_df) {\n  map(sentence_list, function(sentence)\n    sentence2digits(sentence, index_df))\n}\n\nsrc_diglist <-\n  sentlist2diglist(map(word_pairs, ~ .[[1]]), src_index)\nsrc_maxlen <- map(src_diglist, length) %>% unlist() %>% max()\nsrc_matrix <-\n  pad_sequences(src_diglist, maxlen = src_maxlen,  padding = \"post\")\n\ntarget_diglist <-\n  sentlist2diglist(map(word_pairs, ~ .[[2]]), target_index)\ntarget_maxlen <- map(target_diglist, length) %>% unlist() %>% max()\ntarget_matrix <-\n  pad_sequences(target_diglist, maxlen = target_maxlen, padding = \"post\")\n\n\nAll that remains to be done is the train-test split.\n\n\ntrain_indices <-\n  sample(nrow(src_matrix), size = nrow(src_matrix) * 0.8)\n\nvalidation_indices <- setdiff(1:nrow(src_matrix), train_indices)\n\nx_train <- src_matrix[train_indices, ]\ny_train <- target_matrix[train_indices, ]\n\nx_valid <- src_matrix[validation_indices, ]\ny_valid <- target_matrix[validation_indices, ]\n\nbuffer_size <- nrow(x_train)\n\n# just for convenience, so we may get a glimpse at translation \n# performance during training\ntrain_sentences <- sentences[train_indices]\nvalidation_sentences <- sentences[validation_indices]\nvalidation_sample <- sample(validation_sentences, 5)\n\n\nCreating datasets to iterate over\nThis section does not contain much code, but it shows an important technique: the use of datasets.\nRemember the olden times when we used to pass in hand-crafted generators to Keras models? With tfdatasets, we can scalably feed data directly to the Keras fit function, having various preparatory actions being performed directly in native code. In our case, we will not be using fit, instead iterate directly over the tensors contained in the dataset.\n\n\ntrain_dataset <- \n  tensor_slices_dataset(keras_array(list(x_train, y_train)))  %>%\n  dataset_shuffle(buffer_size = buffer_size) %>%\n  dataset_batch(batch_size, drop_remainder = TRUE)\n\nvalidation_dataset <-\n  tensor_slices_dataset(keras_array(list(x_valid, y_valid))) %>%\n  dataset_shuffle(buffer_size = buffer_size) %>%\n  dataset_batch(batch_size, drop_remainder = TRUE)\n\n\nNow we are ready to roll! In fact, before talking about that training loop we need to dive into the implementation of the core logic: the custom layers responsible for performing the attention operation.\nAttention encoder\nWe will create two custom layers, only the second of which is going to incorporate attention logic.\nHowever, it’s worth introducing the encoder in detail too, because technically this is not a custom layer but a custom model, as described here.\nCustom models allow you to create member layers and then, specify custom functionality defining the operations to be performed on these layers.\nLet’s look at the complete code for the encoder.\n\n\nattention_encoder <-\n  \n  function(gru_units,\n           embedding_dim,\n           src_vocab_size,\n           name = NULL) {\n    \n    keras_model_custom(name = name, function(self) {\n      \n      self$embedding <-\n        layer_embedding(\n          input_dim = src_vocab_size,\n          output_dim = embedding_dim\n        )\n      \n      self$gru <-\n        layer_gru(\n          units = gru_units,\n          return_sequences = TRUE,\n          return_state = TRUE\n        )\n      \n      function(inputs, mask = NULL) {\n        \n        x <- inputs[[1]]\n        hidden <- inputs[[2]]\n        \n        x <- self$embedding(x)\n        c(output, state) %<-% self$gru(x, initial_state = hidden)\n    \n        list(output, state)\n      }\n    })\n  }\n\n\nThe encoder has two layers, an embedding and a GRU layer. The ensuing anonymous function specifies what should happen when the layer is called.\nOne thing that might look unexpected is the argument passed to that function: It is a list of tensors, where the first element are the inputs, and the second is the hidden state at the point the layer is called (in traditional Keras RNN usage, we are accustomed to seeing state manipulations being done transparently for us.)\nAs the input to the call flows through the operations, let’s keep track of the shapes involved:\nx, the input, is of size (batch_size, max_length_input), where max_length_input is the number of digits constituting a source sentence. (Remember we’ve padded them to be of uniform length.) In familiar RNN parlance, we could also speak of timesteps here (we soon will).\nAfter the embedding step, the tensors will have an additional axis, as each timestep (token) will have been embedded as an embedding_dim-dimensional vector. So our shapes are now (batch_size, max_length_input, embedding_dim).\nNote how when calling the GRU, we’re passing in the hidden state we received as initial_state. We get back a list: the GRU output and last hidden state.\nAt this point, it helps to look up RNN output shapes in the documentation.\nWe have specified our GRU to return sequences as well as the state. Our asking for the state means we’ll get back a list of tensors: the output, and the last state(s) - a single last state in this case as we’re using GRU. That state itself will be of shape (batch_size, gru_units).\nOur asking for sequences means the output will be of shape (batch_size, max_length_input, gru_units). So that’s that. We bundle output and last state in a list and pass it to the calling code.\nBefore we show the decoder, we need to say a few things about attention.\nAttention in a nutshell\nAs T. Luong nicely puts it in his thesis, the idea of the attention mechanism is\n\nto provide a ‘random access memory’ of source hidden states which one can constantly refer to as translation progresses.\n\nThis means that at every timestep, the decoder receives not just the previous decoder hidden state, but also the complete output from the encoder. It then “makes up its mind” as to what part of the encoded input matters at the current point in time.\nAlthough various attention mechanisms exist, the basic procedure often goes like this.\n\nIn our description, we’re closely following (Luong, Pham, and Manning 2015), in accordance with the colaboratory notebook on NMT.\nFirst, we create a score that relates the decoder hidden state at a given timestep to the encoder hidden states at every timestep.\nThe score function can take different shapes; the following is commonly referred to as Bahdanau style (additive) attention.\nNote that when referring to this as Bahdanau style attention, we - like others - do not imply exact agreement with the formulae in (Bahdanau, Cho, and Bengio 2014). It is about the general way encoder and decoder hidden states are combined - additively or multiplicatively.\n\\[score(\\mathbf{h}_t,\\bar{\\mathbf{h}_s}) = \\mathbf{v}_a^T tanh(\\mathbf{W_1}\\mathbf{h}_t + \\mathbf{W_2}\\bar{\\mathbf{h}_s})\\]\nFrom these scores, we want to find the encoder states that matter most to the current decoder timestep.\nBasically, we just normalize the scores doing a softmax, which leaves us with a set of attention weights (also called alignment vectors):\n\\[\\alpha_{ts} = \\frac{exp(score(\\mathbf{h}_t,\\bar{\\mathbf{h}_s}))}{\\sum_{s'=1}^{S}{score(\\mathbf{h}_t,\\bar{\\mathbf{h}_{s'}})}}\\]\nFrom these attention weights, we create the context vector. This is basically an average of the source hidden states, weighted by the attention weights:\n\\[\\mathbf{c}_t= \\sum_s{\\alpha_{ts} \\bar{\\mathbf{h}_s}}\\]\nNow we need to relate this to the state the decoder is in. We calculate the attention vector from a concatenation of context vector and current decoder hidden state:\n\\[\\mathbf{a}_t = tanh(\\mathbf{W_c} [ \\mathbf{c}_t ; \\mathbf{h}_t])\\]\nIn sum, we see how at each timestep, the attention mechanism combines information from the sequence of encoder states, and the current decoder hidden state. We’ll soon see a third source of information entering the calculation, which will be dependent on whether we’re in the training or the prediction phase.\nAttention decoder\nNow let’s look at how the attention decoder implements the above logic. We will be following the colab notebook in presenting a slight simplification of the score function, which will not prevent the decoder from successfully translating our example sentences.\n\n\nattention_decoder <-\n  function(object,\n           gru_units,\n           embedding_dim,\n           target_vocab_size,\n           name = NULL) {\n    \n    keras_model_custom(name = name, function(self) {\n      \n      self$gru <-\n        layer_gru(\n          units = gru_units,\n          return_sequences = TRUE,\n          return_state = TRUE\n        )\n      \n      self$embedding <-\n        layer_embedding(input_dim = target_vocab_size, \n                        output_dim = embedding_dim)\n      \n      gru_units <- gru_units\n      self$fc <- layer_dense(units = target_vocab_size)\n      self$W1 <- layer_dense(units = gru_units)\n      self$W2 <- layer_dense(units = gru_units)\n      self$V <- layer_dense(units = 1L)\n \n      function(inputs, mask = NULL) {\n        \n        x <- inputs[[1]]\n        hidden <- inputs[[2]]\n        encoder_output <- inputs[[3]]\n        \n        hidden_with_time_axis <- k_expand_dims(hidden, 2)\n        \n        score <- self$V(k_tanh(self$W1(encoder_output) + \n                                self$W2(hidden_with_time_axis)))\n        \n        attention_weights <- k_softmax(score, axis = 2)\n        \n        context_vector <- attention_weights * encoder_output\n        context_vector <- k_sum(context_vector, axis = 2)\n    \n        x <- self$embedding(x)\n       \n        x <- k_concatenate(list(k_expand_dims(context_vector, 2), x), axis = 3)\n        \n        c(output, state) %<-% self$gru(x)\n   \n        output <- k_reshape(output, c(-1, gru_units))\n    \n        x <- self$fc(output)\n \n        list(x, state, attention_weights)\n        \n      }\n      \n    })\n  }\n\n\nFirstly, we notice that in addition to the usual embedding and GRU layers we’d expect in a decoder, there are a few additional dense layers. We’ll comment on those as we go.\nThis time, the first argument to what is effectively the call function consists of three parts: input, hidden state, and the output from the encoder.\nFirst we need to calculate the score, which basically means addition of two matrix multiplications.\nFor that addition, the shapes have to match. Now encoder_output is of shape (batch_size, max_length_input, gru_units), while hidden has shape (batch_size, gru_units). We thus add an axis “in the middle,” obtaining hidden_with_time_axis, of shape (batch_size, 1, gru_units).\nAfter applying the tanh and the fully connected layer to the result of the addition, score will be of shape (batch_size, max_length_input, 1). The next step calculates the softmax, to get the attention weights.\nNow softmax by default is applied on the last axis - but here we’re applying it on the second axis, since it is with respect to the input timesteps we want to normalize the scores.\nAfter normalization, the shape is still (batch_size, max_length_input, 1).\nNext up we compute the context vector, as a weighted average of encoder hidden states. Its shape is (batch_size, gru_units). Note that like with the softmax operation above, we sum over the second axis, which corresponds to the number of timesteps in the input received from the encoder.\nWe still have to take care of the third source of information: the input. Having been passed through the embedding layer, its shape is (batch_size, 1, embedding_dim). Here, the second axis is of dimension 1 as we’re forecasting a single token at a time.\nNow, let’s concatenate the context vector and the embedded input, to arrive at the attention vector.\nIf you compare the code with the formula above, you’ll see that here we’re skipping the tanh and the additional fully connected layer, and just leave it at the concatenation.\nAfter concatenation, the shape now is (batch_size, 1, embedding_dim + gru_units).\nThe ensuing GRU operation, as usual, gives us back output and shape tensors. The output tensor is flattened to shape (batch_size, gru_units) and passed through the final densely connected layer, after which the output has shape (batch_size, target_vocab_size). With that, we’re going to be able to forecast the next token for every input in the batch.\nRemains to return everything we’re interested in: the output (to be used for forecasting), the last GRU hidden state (to be passed back in to the decoder), and the attention weights for this batch (for plotting). And that’s that!\nCreating the “model”\nWe’re almost ready to train the model. The model? We don’t have a model yet. The next steps will feel a bit unusual if you’re accustomed to the traditional Keras create model -> compile model -> fit model  workflow.\nLet’s have a look.\nFirst, we need a few bookkeeping variables.\n\n\nbatch_size <- 32\nembedding_dim <- 64\ngru_units <- 256\n\nsrc_vocab_size <- nrow(src_index)\ntarget_vocab_size <- nrow(target_index)\n\n\nNow, we create the encoder and decoder objects - it’s tempting to call them layers, but technically both are custom Keras models.\n\n\nencoder <- attention_encoder(\n  gru_units = gru_units,\n  embedding_dim = embedding_dim,\n  src_vocab_size = src_vocab_size\n)\n\ndecoder <- attention_decoder(\n  gru_units = gru_units,\n  embedding_dim = embedding_dim,\n  target_vocab_size = target_vocab_size\n)\n\n\nSo as we’re going along, assembling a model “from pieces,” we still need a loss function, and an optimizer.\n\n\noptimizer <- tf$train$AdamOptimizer()\n\ncx_loss <- function(y_true, y_pred) {\n  mask <- ifelse(y_true == 0L, 0, 1)\n  loss <-\n    tf$nn$sparse_softmax_cross_entropy_with_logits(labels = y_true,\n                                                   logits = y_pred) * mask\n  tf$reduce_mean(loss)\n}\n\n\nNow we’re ready to train.\nTraining phase\nIn the training phase, we’re using teacher forcing, which is the established name for feeding the model the (correct) target at time \\(t\\) as input for the next calculation step at time \\(t + 1\\).\nThis is in contrast to the inference phase, when the decoder output is fed back as input to the next time step.\nThe training phase consists of three loops: firstly, we’re looping over epochs, secondly, over the dataset, and thirdly, over the target sequence we’re predicting.\nFor each batch, we’re encoding the source sequence, getting back the output sequence as well as the last hidden state. The hidden state we then use to initialize the decoder.\nNow, we enter the target sequence prediction loop. For each timestep to be predicted, we call the decoder with the input (which due to teacher forcing is the ground truth from the previous step), its previous hidden state, and the complete encoder output. At each step, the decoder returns predictions, its hidden state and the attention weights.\n\n\nn_epochs <- 50\n\nencoder_init_hidden <- k_zeros(c(batch_size, gru_units))\n\nfor (epoch in seq_len(n_epochs)) {\n  \n  total_loss <- 0\n  iteration <- 0\n    \n  iter <- make_iterator_one_shot(train_dataset)\n    \n  until_out_of_range({\n    \n    batch <- iterator_get_next(iter)\n    loss <- 0\n    x <- batch[[1]]\n    y <- batch[[2]]\n    iteration <- iteration + 1\n      \n    with(tf$GradientTape() %as% tape, {\n      c(enc_output, enc_hidden) %<-% encoder(list(x, encoder_init_hidden))\n \n      dec_hidden <- enc_hidden\n      dec_input <-\n        k_expand_dims(rep(list(\n          word2index(\"<start>\", target_index)\n        ), batch_size))\n        \n\n      for (t in seq_len(target_maxlen - 1)) {\n        c(preds, dec_hidden, weights) %<-%\n          decoder(list(dec_input, dec_hidden, enc_output))\n        loss <- loss + cx_loss(y[, t], preds)\n     \n        dec_input <- k_expand_dims(y[, t])\n      }\n      \n    })\n      \n    total_loss <-\n      total_loss + loss / k_cast_to_floatx(dim(y)[2])\n      \n      paste0(\n        \"Batch loss (epoch/batch): \",\n        epoch,\n        \"/\",\n        iter,\n        \": \",\n        (loss / k_cast_to_floatx(dim(y)[2])) %>% \n          as.double() %>% round(4),\n        \"\\n\"\n      )\n      \n    variables <- c(encoder$variables, decoder$variables)\n    gradients <- tape$gradient(loss, variables)\n      \n    optimizer$apply_gradients(\n      purrr::transpose(list(gradients, variables)),\n      global_step = tf$train$get_or_create_global_step()\n    )\n      \n  })\n    \n    paste0(\n      \"Total loss (epoch): \",\n      epoch,\n      \": \",\n      (total_loss / k_cast_to_floatx(buffer_size)) %>% \n        as.double() %>% round(4),\n      \"\\n\"\n    )\n}\n\n\nHow does backpropagation work with this new flow? With eager execution, a GradientTape records operations performed on the forward pass. This recording is then “played back” to perform backpropagation.\nConcretely put, during the forward pass, we have the tape recording the model’s actions, and we keep incrementally updating the loss.\nThen, outside the tape’s context, we ask the tape for the gradients of the accumulated loss with respect to the model’s variables. Once we know the gradients, we can have the optimizer apply them to those variables.\nThis variables slot, by the way, does not (as of this writing) exist in the base implementation of Keras, which is why we have to resort to the TensorFlow implementation.\nInference\nAs soon as we have a trained model, we can get translating! Actually, we don’t have to wait. We can integrate a few sample translations directly into the training loop, and watch the network progressing (hopefully!).\nThe complete code for this post does it like this, however here we’re arranging the steps in a more didactical order.\nThe inference loop differs from the training procedure mainly it that it does not use teacher forcing.\nInstead, we feed back the current prediction as input to the next decoding timestep.\nThe actual predicted word is chosen from the exponentiated raw scores returned by the decoder using a multinomial distribution.\nWe also include a function to plot a heatmap that shows where in the source attention is being directed as the translation is produced.\n\n\nevaluate <-\n  function(sentence) {\n    attention_matrix <-\n      matrix(0, nrow = target_maxlen, ncol = src_maxlen)\n    \n    sentence <- preprocess_sentence(sentence)\n    input <- sentence2digits(sentence, src_index)\n    input <-\n      pad_sequences(list(input), maxlen = src_maxlen,  padding = \"post\")\n    input <- k_constant(input)\n    \n    result <- \"\"\n    \n    hidden <- k_zeros(c(1, gru_units))\n    c(enc_output, enc_hidden) %<-% encoder(list(input, hidden))\n    \n    dec_hidden <- enc_hidden\n    dec_input <-\n      k_expand_dims(list(word2index(\"<start>\", target_index)))\n    \n    for (t in seq_len(target_maxlen - 1)) {\n      c(preds, dec_hidden, attention_weights) %<-%\n        decoder(list(dec_input, dec_hidden, enc_output))\n      attention_weights <- k_reshape(attention_weights, c(-1))\n      attention_matrix[t, ] <- attention_weights %>% as.double()\n      \n      pred_idx <-\n        tf$multinomial(k_exp(preds), num_samples = 1)[1, 1] %>% as.double()\n      pred_word <- index2word(pred_idx, target_index)\n      \n      if (pred_word == '<stop>') {\n        result <-\n          paste0(result, pred_word)\n        return (list(result, sentence, attention_matrix))\n      } else {\n        result <-\n          paste0(result, pred_word, \" \")\n        dec_input <- k_expand_dims(list(pred_idx))\n      }\n    }\n    list(str_trim(result), sentence, attention_matrix)\n  }\n\nplot_attention <-\n  function(attention_matrix,\n           words_sentence,\n           words_result) {\n    melted <- melt(attention_matrix)\n    ggplot(data = melted, aes(\n      x = factor(Var2),\n      y = factor(Var1),\n      fill = value\n    )) +\n      geom_tile() + scale_fill_viridis() + guides(fill = FALSE) +\n      theme(axis.ticks = element_blank()) +\n      xlab(\"\") +\n      ylab(\"\") +\n      scale_x_discrete(labels = words_sentence, position = \"top\") +\n      scale_y_discrete(labels = words_result) + \n      theme(aspect.ratio = 1)\n  }\n\n\ntranslate <- function(sentence) {\n  c(result, sentence, attention_matrix) %<-% evaluate(sentence)\n  print(paste0(\"Input: \",  sentence))\n  print(paste0(\"Predicted translation: \", result))\n  attention_matrix <-\n    attention_matrix[1:length(str_split(result, \" \")[[1]]),\n                     1:length(str_split(sentence, \" \")[[1]])]\n  plot_attention(attention_matrix,\n                 str_split(sentence, \" \")[[1]],\n                 str_split(result, \" \")[[1]])\n}\n\n\nLearning to translate\nUsing the sample code, you can see yourself how learning progresses. This is how it worked in our case.\n(We are always looking at the same sentences - sampled from the training and test sets, respectively - so we can more easily see the evolution.)\nOn completion of the very first epoch, our network starts every Dutch sentence with Ik. No doubt, there must be many sentences starting in the first person in our corpus!\n(Note: these five sentences are all from the training set.)\nInput: <start> I did that easily . <stop>\nPredicted translation: <start> Ik . <stop>\n\nInput: <start> Look in the mirror . <stop>\nPredicted translation: <start> Ik . <stop>\n\nInput: <start> Tom wanted revenge . <stop>\nPredicted translation: <start> Ik . <stop>\n\nInput: <start> It s very kind of you . <stop>\nPredicted translation: <start> Ik . <stop>\n\nInput: <start> I refuse to answer . <stop>\nPredicted translation: <start> Ik . <stop>\nOne epoch later it seems to have picked up common phrases, although their use does not look related to the input.\nAnd definitely, it has problems to recognize when it’s over…\nInput: <start> I did that easily . <stop>\nPredicted translation: <start> Ik ben een een een een een een een een een een\n\nInput: <start> Look in the mirror . <stop>\nPredicted translation: <start> Tom is een een een een een een een een een een\n\nInput: <start> Tom wanted revenge . <stop>\nPredicted translation: <start> Tom is een een een een een een een een een een\n\nInput: <start> It s very kind of you . <stop>\nPredicted translation: <start> Ik ben een een een een een een een een een een\n\nInput: <start> I refuse to answer . <stop>\nPredicted translation: <start> Ik ben een een een een een een een een een een\nJumping ahead to epoch 7, the translations still are completely wrong, but somehow start capturing overall sentence structure (like the imperative in sentence 2).\nInput: <start> I did that easily . <stop>\nPredicted translation: <start> Ik heb je niet . <stop>\n\nInput: <start> Look in the mirror . <stop>\nPredicted translation: <start> Ga naar de buurt . <stop>\n\nInput: <start> Tom wanted revenge . <stop>\nPredicted translation: <start> Tom heeft Tom . <stop>\n\nInput: <start> It s very kind of you . <stop>\nPredicted translation: <start> Het is een auto . <stop>\n\nInput: <start> I refuse to answer . <stop>\nPredicted translation: <start> Ik heb de buurt . <stop>\nFast forward to epoch 17. Samples from the training set are starting to look better:\nInput: <start> I did that easily . <stop>\nPredicted translation: <start> Ik heb dat hij gedaan . <stop>\n\nInput: <start> Look in the mirror . <stop>\nPredicted translation: <start> Kijk in de spiegel . <stop>\n\nInput: <start> Tom wanted revenge . <stop>\nPredicted translation: <start> Tom wilde dood . <stop>\n\nInput: <start> It s very kind of you . <stop>\nPredicted translation: <start> Het is erg goed voor je . <stop>\n\nInput: <start> I refuse to answer . <stop>\nPredicted translation: <start> Ik speel te antwoorden . <stop>\nWhereas samples from the test set still look pretty random. Although interestingly, not random in the sense of not having syntactic or semantic structure! Breng de televisie op is a perfectly reasonable sentence, if not the most lucky translation of Think happy thoughts.\nInput: <start> It s entirely my fault . <stop>\nPredicted translation: <start> Het is het mijn woord . <stop>\n\nInput: <start> You re trustworthy . <stop>\nPredicted translation: <start> Je bent net . <stop>\n\nInput: <start> I want to live in Italy . <stop>\nPredicted translation: <start> Ik wil in een leugen . <stop>\n\nInput: <start> He has seven sons . <stop>\nPredicted translation: <start> Hij heeft Frans uit . <stop>\n\nInput: <start> Think happy thoughts . <stop>\nPredicted translation: <start> Breng de televisie op . <stop>\nWhere are we at after 30 epochs? By now, the training samples have been pretty much memorized (the third sentence is suffering from political correctness though, matching Tom wanted revenge to Tom wilde vrienden):\nInput: <start> I did that easily . <stop>\nPredicted translation: <start> Ik heb dat zonder moeite gedaan . <stop>\n\nInput: <start> Look in the mirror . <stop>\nPredicted translation: <start> Kijk in de spiegel . <stop>\n\nInput: <start> Tom wanted revenge . <stop>\nPredicted translation: <start> Tom wilde vrienden . <stop>\n\nInput: <start> It s very kind of you . <stop>\nPredicted translation: <start> Het is erg aardig van je . <stop>\n\nInput: <start> I refuse to answer . <stop>\nPredicted translation: <start> Ik weiger te antwoorden . <stop>\nHow about the test sentences? They’ve started to look much better. One sentence (Ik wil in Itali leven) has even been translated entirely correctly. And we see something like the concept of numerals appearing (seven translated by acht)…\nInput: <start> It s entirely my fault . <stop>\nPredicted translation: <start> Het is bijna mijn beurt . <stop>\n\nInput: <start> You re trustworthy . <stop>\nPredicted translation: <start> Je bent zo zijn . <stop>\n\nInput: <start> I want to live in Italy . <stop>\nPredicted translation: <start> Ik wil in Itali leven . <stop>\n\nInput: <start> He has seven sons . <stop>\nPredicted translation: <start> Hij heeft acht geleden . <stop>\n\nInput: <start> Think happy thoughts . <stop>\nPredicted translation: <start> Zorg alstublieft goed uit . <stop>\nAs you see it can be quite interesting watching the network’s “language capability” evolve.\nNow, how about subjecting our network to a little MRI scan? Since we’re collecting the attention weights, we can visualize what part of the source text the decoder is attending to at every timestep.\nWhat is the decoder looking at?\nFirst, let’s take an example where word orders in both languages are the same.\nInput: <start> It s very kind of you . <stop>\nPredicted translation: <start> Het is erg aardig van je . <stop>\n\n\n\nWe see that overall, given a sample where respective sentences align very well, the decoder pretty much looks where it is supposed to.\nLet’s pick something a little more complicated.\nInput: <start> I did that easily . <stop>\"\nPredicted translation: <start> Ik heb dat zonder moeite gedaan . <stop>\nThe translation is correct, but word order in both languages isn’t the same here: did corresponds to the analytic perfect heb … gedaan. Will we be able to see that in the attention plot?\n\n\n\nThe answer is no. It would be interesting to check again after training for a couple more epochs.\nFinally, let’s investigate this translation from the test set (which is entirely correct):\nInput: <start> I want to live in Italy . <stop>\nPredicted translation: <start> Ik wil in Itali leven . <stop>\n\n\n\nThese two sentences don’t align well. We see that Dutch in correctly picks English in (skipping over to live), then Itali attends to Italy. Finally leven is produced without us witnessing the decoder looking back to live. Here again, it would be interesting to watch what happens a few epochs later!\nNext up\nThere are many ways to go from here. For one, we didn’t do any hyperparameter optimization.\n(See e.g. (Luong, Pham, and Manning 2015) for an extensive experiment on architectures and hyperparameters for NMT.)\nSecond, provided you have access to the required hardware, you might be curious how good an algorithm like this can get when trained on a real big dataset, using a real big network.\nThird, alternative attention mechanisms have been suggested (see e.g. T. Luong’s thesis which we followed rather closely in the description of attention above).\nLast not least, no one said attention need be useful only in the context of machine translation. Out there, a plenty of sequence prediction (time series) problems are waiting to be explored with respect to its potential usefulness…\n\n\n\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural Machine Translation by Jointly Learning to Align and Translate.” CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473.\n\n\nLuong, Minh-Thang, Hieu Pham, and Christopher D. Manning. 2015. “Effective Approaches to Attention-Based Neural Machine Translation.” CoRR abs/1508.04025. http://arxiv.org/abs/1508.04025.\n\n\nVinyals, Oriol, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey E. Hinton. 2014. “Grammar as a Foreign Language.” CoRR abs/1412.7449. http://arxiv.org/abs/1412.7449.\n\n\nXu, Kelvin, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.” CoRR abs/1502.03044. http://arxiv.org/abs/1502.03044.\n\n\n\n\n",
    "preview": "posts/2018-07-30-attention-layer/images/attention.png",
    "last_modified": "2024-11-21T15:52:41+00:00",
    "input_file": {},
    "preview_width": 606,
    "preview_height": 448
  },
  {
    "path": "posts/2018-07-17-activity-detection/",
    "title": "Classifying physical activity from smartphone data",
    "description": "Using Keras to train a convolutional neural network to classify physical activity. The dataset was built from the recordings of 30 subjects performing basic activities and postural transitions while carrying a waist-mounted smartphone with embedded inertial sensors.",
    "author": [
      {
        "name": "Nick Strayer",
        "url": "http://nickstrayer.me"
      }
    ],
    "date": "2018-07-17",
    "categories": [],
    "contents": "\nIntroduction\nIn this post we’ll describe how to use smartphone accelerometer and gyroscope data to predict the physical activities of the individuals carrying the phones. The data used in this post comes from the Smartphone-Based Recognition of Human Activities and Postural Transitions Data Set distributed by the University of California, Irvine. Thirty individuals were tasked with performing various basic activities with an attached smartphone recording movement using an accelerometer and gyroscope.\nBefore we begin, let’s load the various libraries that we’ll use in the analysis:\n\n\nlibrary(keras)     # Neural Networks\nlibrary(tidyverse) # Data cleaning / Visualization\nlibrary(knitr)     # Table printing\nlibrary(rmarkdown) # Misc. output utilities \nlibrary(ggridges)  # Visualization\n\nActivities dataset\nThe data used in this post come from the Smartphone-Based Recognition of Human Activities and Postural Transitions Data Set(Reyes-Ortiz et al. 2016) distributed by the University of California, Irvine.\n\nThroughout this post, data/ is the directory created by downloading and unzipping this dataset.\nWhen downloaded from the link above, the data contains two different ‘parts.’ One that has been pre-processed using various feature extraction techniques such as fast-fourier transform, and another RawData section that simply supplies the raw X,Y,Z directions of an accelerometer and gyroscope. None of the standard noise filtering or feature extraction used in accelerometer data has been applied. This is the data set we will use.\nThe motivation for working with the raw data in this post is to aid the transition of the code/concepts to time series data in less well-characterized domains. While a more accurate model could be made by utilizing the filtered/cleaned data provided, the filtering and transformation can vary greatly from task to task; requiring lots of manual effort and domain knowledge. One of the beautiful things about deep learning is the feature extraction is learned from the data, not outside knowledge.\nActivity labels\nThe data has integer encodings for the activities which, while not important to the model itself, are helpful for use to see. Let’s load them first.\n\n\nactivityLabels <- read.table(\"data/activity_labels.txt\", \n                             col.names = c(\"number\", \"label\")) \n\nactivityLabels %>% kable(align = c(\"c\", \"l\"))\nnumber\nlabel\n1\nWALKING\n2\nWALKING_UPSTAIRS\n3\nWALKING_DOWNSTAIRS\n4\nSITTING\n5\nSTANDING\n6\nLAYING\n7\nSTAND_TO_SIT\n8\nSIT_TO_STAND\n9\nSIT_TO_LIE\n10\nLIE_TO_SIT\n11\nSTAND_TO_LIE\n12\nLIE_TO_STAND\n\nNext, we load in the labels key for the RawData. This file is a list of all of the observations, or individual activity recordings, contained in the data set. The key for the columns is taken from the data README.txt.\n\nColumn 1: experiment number ID, \nColumn 2: user number ID, \nColumn 3: activity number ID \nColumn 4: Label start point \nColumn 5: Label end point \nThe start and end points are in number of signal log samples (recorded at 50hz).\nLet’s take a look at the first 50 rows:\n\n\nlabels <- read.table(\n  \"data/RawData/labels.txt\",\n  col.names = c(\"experiment\", \"userId\", \"activity\", \"startPos\", \"endPos\")\n)\n\nlabels %>% \n  head(50) %>% \n  paged_table()\n\n\n\nFile names\nNext, let’s look at the actual files of the user data provided to us in RawData/\n\n\ndataFiles <- list.files(\"data/RawData\")\ndataFiles %>% head()\n\n[1] \"acc_exp01_user01.txt\" \"acc_exp02_user01.txt\"\n[3] \"acc_exp03_user02.txt\" \"acc_exp04_user02.txt\"\n[5] \"acc_exp05_user03.txt\" \"acc_exp06_user03.txt\"\n\nThere is a three-part file naming scheme. The first part is the type of data the file contains: either acc for accelerometer or gyro for gyroscope. Next is the experiment number, and last is the user Id for the recording. Let’s load these into a dataframe for ease of use later.\n\n\nfileInfo <- data_frame(\n  filePath = dataFiles\n) %>%\n  filter(filePath != \"labels.txt\") %>% \n  separate(filePath, sep = '_', \n           into = c(\"type\", \"experiment\", \"userId\"), \n           remove = FALSE) %>% \n  mutate(\n    experiment = str_remove(experiment, \"exp\"),\n    userId = str_remove_all(userId, \"user|\\\\.txt\")\n  ) %>% \n  spread(type, filePath)\n\nfileInfo %>% head() %>% kable()\nexperiment\nuserId\nacc\ngyro\n01\n01\nacc_exp01_user01.txt\ngyro_exp01_user01.txt\n02\n01\nacc_exp02_user01.txt\ngyro_exp02_user01.txt\n03\n02\nacc_exp03_user02.txt\ngyro_exp03_user02.txt\n04\n02\nacc_exp04_user02.txt\ngyro_exp04_user02.txt\n05\n03\nacc_exp05_user03.txt\ngyro_exp05_user03.txt\n06\n03\nacc_exp06_user03.txt\ngyro_exp06_user03.txt\n\nReading and gathering data\nBefore we can do anything with the data provided we need to get it into a model-friendly format. This means we want to have a list of observations, their class (or activity label), and the data corresponding to the recording.\nTo obtain this we will scan through each of the recording files present in dataFiles, look up what observations are contained in the recording, extract those recordings and return everything to an easy to model with dataframe.\n\n\n# Read contents of single file to a dataframe with accelerometer and gyro data.\nreadInData <- function(experiment, userId){\n  genFilePath = function(type) {\n    paste0(\"data/RawData/\", type, \"_exp\",experiment, \"_user\", userId, \".txt\")\n  }  \n  \n  bind_cols(\n    read.table(genFilePath(\"acc\"), col.names = c(\"a_x\", \"a_y\", \"a_z\")),\n    read.table(genFilePath(\"gyro\"), col.names = c(\"g_x\", \"g_y\", \"g_z\"))\n  )\n}\n\n# Function to read a given file and get the observations contained along\n# with their classes.\n\nloadFileData <- function(curExperiment, curUserId) {\n  \n  # load sensor data from file into dataframe\n  allData <- readInData(curExperiment, curUserId)\n\n  extractObservation <- function(startPos, endPos){\n    allData[startPos:endPos,]\n  }\n  \n  # get observation locations in this file from labels dataframe\n  dataLabels <- labels %>% \n    filter(userId == as.integer(curUserId), \n           experiment == as.integer(curExperiment))\n  \n\n  # extract observations as dataframes and save as a column in dataframe.\n  dataLabels %>% \n    mutate(\n      data = map2(startPos, endPos, extractObservation)\n    ) %>% \n    select(-startPos, -endPos)\n}\n\n# scan through all experiment and userId combos and gather data into a dataframe. \nallObservations <- map2_df(fileInfo$experiment, fileInfo$userId, loadFileData) %>% \n  right_join(activityLabels, by = c(\"activity\" = \"number\")) %>% \n  rename(activityName = label)\n\n# cache work. \nwrite_rds(allObservations, \"allObservations.rds\")\nallObservations %>% dim()\n\n\n\n[1] 1214    5\n\nExploring the data\nNow that we have all the data loaded along with the experiment, userId, and activity labels, we can explore the data set.\nLength of recordings\nLet’s first look at the length of the recordings by activity.\n\n\nallObservations %>% \n  mutate(recording_length = map_int(data,nrow)) %>% \n  ggplot(aes(x = recording_length, y = activityName)) +\n  geom_density_ridges(alpha = 0.8)\n\n\nThe fact there is such a difference in length of recording between the different activity types requires us to be a bit careful with how we proceed. If we train the model on every class at once we are going to have to pad all the observations to the length of the longest, which would leave a large majority of the observations with a huge proportion of their data being just padding-zeros. Because of this, we will fit our model to just the largest ‘group’ of observations length activities, these include STAND_TO_SIT, STAND_TO_LIE, SIT_TO_STAND, SIT_TO_LIE, LIE_TO_STAND, and LIE_TO_SIT.\n\nIt is notable that the activities that we have selected here are all ‘transitions.’ So in a way we are creating a change-point detection algorithm.\nAn interesting future direction would be attempting to use another architecture such as an RNN that can handle variable length inputs and training it on all the data. However, you would run the risk of the model learning simply that if the observation is long it is most likely one of the four longest classes which would not generalize to a scenario where you were running this model on a real-time-stream of data.\nFiltering activities\nBased on our work from above, let’s subset the data to just be of the activities of interest.\n\n\ndesiredActivities <- c(\n  \"STAND_TO_SIT\", \"SIT_TO_STAND\", \"SIT_TO_LIE\", \n  \"LIE_TO_SIT\", \"STAND_TO_LIE\", \"LIE_TO_STAND\"  \n)\n\nfilteredObservations <- allObservations %>% \n  filter(activityName %in% desiredActivities) %>% \n  mutate(observationId = 1:n())\n\nfilteredObservations %>% paged_table()\n\n\n\nSo after our aggressive pruning of the data we will have a respectable amount of data left upon which our model can learn.\nTraining/testing split\nBefore we go any further into exploring the data for our model, in an attempt to be as fair as possible with our performance measures, we need to split the data into a train and test set. Since each user performed all activities just once (with the exception of one who only did 10 of the 12 activities) by splitting on userId we will ensure that our model sees new people exclusively when we test it.\n\n\n# get all users\nuserIds <- allObservations$userId %>% unique()\n\n# randomly choose 24 (80% of 30 individuals) for training\nset.seed(42) # seed for reproducibility\ntrainIds <- sample(userIds, size = 24)\n\n# set the rest of the users to the testing set\ntestIds <- setdiff(userIds,trainIds)\n\n# filter data. \ntrainData <- filteredObservations %>% \n  filter(userId %in% trainIds)\n\ntestData <- filteredObservations %>% \n  filter(userId %in% testIds)\n\nVisualizing activities\nNow that we have trimmed our data by removing activities and splitting off a test set, we can actually visualize the data for each class to see if there’s any immediately discernible shape that our model may be able to pick up on.\nFirst let’s unpack our data from its dataframe of one-row-per-observation to a tidy version of all the observations.\n\n\nunpackedObs <- 1:nrow(trainData) %>% \n  map_df(function(rowNum){\n    dataRow <- trainData[rowNum, ]\n    dataRow$data[[1]] %>% \n      mutate(\n        activityName = dataRow$activityName, \n        observationId = dataRow$observationId,\n        time = 1:n() )\n  }) %>% \n  gather(reading, value, -time, -activityName, -observationId) %>% \n  separate(reading, into = c(\"type\", \"direction\"), sep = \"_\") %>% \n  mutate(type = ifelse(type == \"a\", \"acceleration\", \"gyro\"))\n\nNow we have an unpacked set of our observations, let’s visualize them!\n\n\nunpackedObs %>% \n  ggplot(aes(x = time, y = value, color = direction)) +\n  geom_line(alpha = 0.2) +\n  geom_smooth(se = FALSE, alpha = 0.7, size = 0.5) +\n  facet_grid(type ~ activityName, scales = \"free_y\") +\n  theme_minimal() +\n  theme( axis.text.x = element_blank() )\n\n\nSo at least in the accelerometer data patterns definitely emerge. One would imagine that the model may have trouble with the differences between LIE_TO_SIT and LIE_TO_STAND as they have a similar profile on average. The same goes for SIT_TO_STAND and STAND_TO_SIT.\nPreprocessing\nBefore we can train the neural network, we need to take a couple of steps to preprocess the data.\nPadding observations\nFirst we will decide what length to pad (and truncate) our sequences to by finding what the 98th percentile length is. By not using the very longest observation length this will help us avoid extra-long outlier recordings messing up the padding.\n\n\npadSize <- trainData$data %>% \n  map_int(nrow) %>% \n  quantile(p = 0.98) %>% \n  ceiling()\npadSize\n\n98% \n334 \n\nNow we simply need to convert our list of observations to matrices, then use the super handy pad_sequences() function in Keras to pad all observations and turn them into a 3D tensor for us.\n\n\nconvertToTensor <- . %>% \n  map(as.matrix) %>% \n  pad_sequences(maxlen = padSize)\n\ntrainObs <- trainData$data %>% convertToTensor()\ntestObs <- testData$data %>% convertToTensor()\n  \ndim(trainObs)\n\n[1] 286 334   6\n\nWonderful, we now have our data in a nice neural-network-friendly format of a 3D tensor with dimensions (<num obs>, <sequence length>, <channels>).\n\nIf we were working with a video instead of sensor data, this would be a 4D Tensor. If we were using FMRI data, this could be a 5D tensor!\nOne-hot encoding\nThere’s one last thing we need to do before we can train our model, and that is turn our observation classes from integers into one-hot, or dummy encoded, vectors. Luckily, again Keras has supplied us with a very helpful function to do just this.\n\n\noneHotClasses <- . %>% \n  {. - 7} %>%        # bring integers down to 0-6 from 7-12\n  to_categorical() # One-hot encode\n\ntrainY <- trainData$activity %>% oneHotClasses()\ntestY <- testData$activity %>% oneHotClasses()\n\nModeling\nArchitecture\nSince we have temporally dense time-series data we will make use of 1D convolutional layers. With temporally-dense data, an RNN has to learn very long dependencies in order to pick up on patterns, CNNs can simply stack a few convolutional layers to build pattern representations of substantial length. Since we are also simply looking for a single classification of activity for each observation, we can just use pooling to ‘summarize’ the CNNs view of the data into a dense layer.\n\nFor more information on the differences between the two architectures for sequence data see (Graves 2012) and (LeCun, Bengio, and Hinton 2015).\nIn addition to stacking two layer_conv_1d() layers, we will use batch norm and dropout (the spatial variant(Tompson et al. 2014) on the convolutional layers and standard on the dense) to regularize the network.\n\n\ninput_shape <- dim(trainObs)[-1]\nnum_classes <- dim(trainY)[2]\n\nfilters <- 24     # number of convolutional filters to learn\nkernel_size <- 8  # how many time-steps each conv layer sees.\ndense_size <- 48  # size of our penultimate dense layer. \n\n# Initialize model\nmodel <- keras_model_sequential()\nmodel %>% \n  layer_conv_1d(\n    filters = filters,\n    kernel_size = kernel_size, \n    input_shape = input_shape,\n    padding = \"valid\", \n    activation = \"relu\"\n  ) %>%\n  layer_batch_normalization() %>%\n  layer_spatial_dropout_1d(0.15) %>% \n  layer_conv_1d(\n    filters = filters/2,\n    kernel_size = kernel_size,\n    activation = \"relu\",\n  ) %>%\n  # Apply average pooling:\n  layer_global_average_pooling_1d() %>% \n  layer_batch_normalization() %>%\n  layer_dropout(0.2) %>% \n  layer_dense(\n    dense_size,\n    activation = \"relu\"\n  ) %>% \n  layer_batch_normalization() %>%\n  layer_dropout(0.25) %>% \n  layer_dense(\n    num_classes, \n    activation = \"softmax\",\n    name = \"dense_output\"\n  ) \n\nsummary(model)\n\n______________________________________________________________________\nLayer (type)                   Output Shape                Param #    \n======================================================================\nconv1d_1 (Conv1D)              (None, 327, 24)             1176       \n______________________________________________________________________\nbatch_normalization_1 (BatchNo (None, 327, 24)             96         \n______________________________________________________________________\nspatial_dropout1d_1 (SpatialDr (None, 327, 24)             0          \n______________________________________________________________________\nconv1d_2 (Conv1D)              (None, 320, 12)             2316       \n______________________________________________________________________\nglobal_average_pooling1d_1 (Gl (None, 12)                  0          \n______________________________________________________________________\nbatch_normalization_2 (BatchNo (None, 12)                  48         \n______________________________________________________________________\ndropout_1 (Dropout)            (None, 12)                  0          \n______________________________________________________________________\ndense_1 (Dense)                (None, 48)                  624        \n______________________________________________________________________\nbatch_normalization_3 (BatchNo (None, 48)                  192        \n______________________________________________________________________\ndropout_2 (Dropout)            (None, 48)                  0          \n______________________________________________________________________\ndense_output (Dense)           (None, 6)                   294        \n======================================================================\nTotal params: 4,746\nTrainable params: 4,578\nNon-trainable params: 168\n______________________________________________________________________\n\nTraining\nNow we can train the model using our test and training data. Note that we use callback_model_checkpoint() to ensure that we save only the best variation of the model (desirable since at some point in training the model may begin to overfit or otherwise stop improving).\n\n\n# Compile model\nmodel %>% compile(\n  loss = \"categorical_crossentropy\",\n  optimizer = \"rmsprop\",\n  metrics = \"accuracy\"\n)\n\ntrainHistory <- model %>%\n  fit(\n    x = trainObs, y = trainY,\n    epochs = 350,\n    validation_data = list(testObs, testY),\n    callbacks = list(\n      callback_model_checkpoint(\"best_model.h5\", \n                                save_best_only = TRUE)\n    )\n  )\n\n\n\n\n\n\n\n\n\n\nThe model is learning something! We get a respectable 94.4% accuracy on the validation data, not bad with six possible classes to choose from. Let’s look into the validation performance a little deeper to see where the model is messing up.\nEvaluation\nNow that we have a trained model let’s investigate the errors that it made on our testing data. We can load the best model from training based upon validation accuracy and then look at each observation, what the model predicted, how high a probability it assigned, and the true activity label.\n\n\n# dataframe to get labels onto one-hot encoded prediction columns\noneHotToLabel <- activityLabels %>% \n  mutate(number = number - 7) %>% \n  filter(number >= 0) %>% \n  mutate(class = paste0(\"V\",number + 1)) %>% \n  select(-number)\n\n# Load our best model checkpoint\nbestModel <- load_model_hdf5(\"best_model.h5\")\n\ntidyPredictionProbs <- bestModel %>% \n  predict(testObs) %>% \n  as_data_frame() %>% \n  mutate(obs = 1:n()) %>% \n  gather(class, prob, -obs) %>% \n  right_join(oneHotToLabel, by = \"class\")\n\npredictionPerformance <- tidyPredictionProbs %>% \n  group_by(obs) %>% \n  summarise(\n    highestProb = max(prob),\n    predicted = label[prob == highestProb]\n  ) %>% \n  mutate(\n    truth = testData$activityName,\n    correct = truth == predicted\n  ) \n\npredictionPerformance %>% paged_table()\n\n\n\nFirst, let’s look at how ‘confident’ the model was by if the prediction was correct or not.\n\n\npredictionPerformance %>% \n  mutate(result = ifelse(correct, 'Correct', 'Incorrect')) %>% \n  ggplot(aes(highestProb)) +\n  geom_histogram(binwidth = 0.01) +\n  geom_rug(alpha = 0.5) +\n  facet_grid(result~.) +\n  ggtitle(\"Probabilities associated with prediction by correctness\")\n\n\nReassuringly it seems the model was, on average, less confident about its classifications for the incorrect results than the correct ones. (Although, the sample size is too small to say anything definitively.)\n\nIf you desire a model that can truly tell you how ‘confident’ it is in a prediction (rather than just a probability), look into bayesian neural networks(Kononenko 1989) and the more recent use of dropout-as-bayes(Gal and Ghahramani 2016).\nLet’s see what activities the model had the hardest time with using a confusion matrix.\n\n\npredictionPerformance %>% \n  group_by(truth, predicted) %>% \n  summarise(count = n()) %>% \n  mutate(good = truth == predicted) %>% \n  ggplot(aes(x = truth,  y = predicted)) +\n  geom_point(aes(size = count, color = good)) +\n  geom_text(aes(label = count), \n            hjust = 0, vjust = 0, \n            nudge_x = 0.1, nudge_y = 0.1) + \n  guides(color = FALSE, size = FALSE) +\n  theme_minimal()\n\n\nWe see that, as the preliminary visualization suggested, the model had a bit of trouble with distinguishing between LIE_TO_SIT and LIE_TO_STAND classes, along with the SIT_TO_LIE and STAND_TO_LIE, which also have similar visual profiles.\nFuture directions\nThe most obvious future direction to take this analysis would be to attempt to make the model more general by working with more of the supplied activity types. Another interesting direction would be to not separate the recordings into distinct ‘observations’ but instead keep them as one streaming set of data, much like a real world deployment of a model would work, and see how well a model could classify streaming data and detect changes in activity.\n\n\nGal, Yarin, and Zoubin Ghahramani. 2016. “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.” In International Conference on Machine Learning, 1050–9.\n\n\nGraves, Alex. 2012. “Supervised Sequence Labelling.” In Supervised Sequence Labelling with Recurrent Neural Networks, 5–13. Springer.\n\n\nKononenko, Igor. 1989. “Bayesian Neural Networks.” Biological Cybernetics 61 (5). Springer: 361–70.\n\n\nLeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. 2015. “Deep Learning.” Nature 521 (7553). Nature Publishing Group: 436.\n\n\nReyes-Ortiz, Jorge-L, Luca Oneto, Albert Samà, Xavier Parra, and Davide Anguita. 2016. “Transition-Aware Human Activity Recognition Using Smartphones.” Neurocomputing 171. Elsevier: 754–67.\n\n\nTompson, Jonathan, Ross Goroshin, Arjun Jain, Yann LeCun, and Christoph Bregler. 2014. “Efficient Object Localization Using Convolutional Networks.” CoRR abs/1411.4280. http://arxiv.org/abs/1411.4280.\n\n\n\n\n",
    "preview": "posts/2018-07-17-activity-detection/index_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2024-11-21T15:38:35+00:00",
    "input_file": {},
    "preview_width": 1152,
    "preview_height": 768
  },
  {
    "path": "posts/2018-06-25-sunspots-lstm/",
    "title": "Predicting Sunspot Frequency with Keras",
    "description": "In this post we will examine making time series predictions using the sunspots dataset that ships with base R. Sunspots are dark spots on the sun, associated with lower temperature. Our post will focus on both how to apply deep learning to time series forecasting, and how to properly apply cross validation in this domain.",
    "author": [
      {
        "name": "Matt Dancho",
        "url": "https://github.com/mdancho84"
      },
      {
        "name": "Sigrid Keydana",
        "url": "https://github.com/skeydan"
      }
    ],
    "date": "2018-06-25",
    "categories": [
      "TensorFlow/Keras",
      "Time Series"
    ],
    "contents": "\nForecasting sunspots with deep learning\nIn this post we will examine making time series predictions using the sunspots dataset that ships with base R. Sunspots are dark spots on the sun, associated with lower temperature. Here’s an image from NASA showing the solar phenomenon.\nFigure from https://www.nasa.gov/content/goddard/largest-sunspot-of-solar-cycleWe’re using the monthly version of the dataset, sunspots.month (there is a yearly version, too).\nIt contains 265 years worth of data (from 1749 through 2013) on the number of sunspots per month.\n\n\n\nForecasting this dataset is challenging because of high short term variability as well as long-term irregularities evident in the cycles. For example, maximum amplitudes reached by the low frequency cycle differ a lot, as does the number of high frequency cycle steps needed to reach that maximum low frequency cycle height.\nOur post will focus on two dominant aspects: how to apply deep learning to time series forecasting, and how to properly apply cross validation in this domain.\nFor the latter, we will use the rsample package that allows to do resampling on time series data.\nAs to the former, our goal is not to reach utmost performance but to show the general course of action when using recurrent neural networks to model this kind of data.\nRecurrent neural networks\nWhen our data has a sequential structure, it is recurrent neural networks (RNNs) we use to model it.\nAs of today, among RNNs, the best established architectures are the GRU (Gated Recurrent Unit) and the LSTM (Long Short Term Memory). For today, let’s not zoom in on what makes them special, but on what they have in common with the most stripped-down RNN: the basic recurrence structure.\nIn contrast to the prototype of a neural network, often called Multilayer Perceptron (MLP), the RNN has a state that is carried on over time. This is nicely seen in this diagram from Goodfellow et al., a.k.a. the “bible of deep learning”:\nFigure from: http://www.deeplearningbook.orgAt each time, the state is a combination of the current input and the previous hidden state. This is reminiscent of autoregressive models, but with neural networks, there has to be some point where we halt the dependence.\nThat’s because in order to determine the weights, we keep calculating how our loss changes as the input changes.\nNow if the input we have to consider, at an arbitrary timestep, ranges back indefinitely - then we will not be able to calculate all those gradients.\nIn practice, then, our hidden state will, at every iteration, be carried forward through a fixed number of steps.\nWe’ll come back to that as soon as we’ve loaded and pre-processed the data.\nSetup, pre-processing, and exploration\nLibraries\nHere, first, are the libraries needed for this tutorial.\n\n\n# Core Tidyverse\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(forcats)\n\n# Time Series\nlibrary(timetk)\nlibrary(tidyquant)\nlibrary(tibbletime)\n\n# Visualization\nlibrary(cowplot)\n\n# Preprocessing\nlibrary(recipes)\n\n# Sampling / Accuracy\nlibrary(rsample)\nlibrary(yardstick) \n\n# Modeling\nlibrary(keras)\nlibrary(tfruns)\n\n\nIf you have not previously run Keras in R, you will need to install Keras using the install_keras() function.\n\n\n# Install Keras if you have not installed before\ninstall_keras()\n\n\nData\nsunspot.month is a ts class (not tidy), so we’ll convert to a tidy data set using the tk_tbl() function from timetk. We use this instead of as.tibble() from tibble to automatically preserve the time series index as a zoo yearmon index. Last, we’ll convert the zoo index to date using lubridate::as_date() (loaded with tidyquant) and then change to a tbl_time object to make time series operations easier.\n\n\nsun_spots <- datasets::sunspot.month %>%\n    tk_tbl() %>%\n    mutate(index = as_date(index)) %>%\n    as_tbl_time(index = index)\n\nsun_spots\n\n\n# A time tibble: 3,177 x 2\n# Index: index\n   index      value\n   <date>     <dbl>\n 1 1749-01-01  58  \n 2 1749-02-01  62.6\n 3 1749-03-01  70  \n 4 1749-04-01  55.7\n 5 1749-05-01  85  \n 6 1749-06-01  83.5\n 7 1749-07-01  94.8\n 8 1749-08-01  66.3\n 9 1749-09-01  75.9\n10 1749-10-01  75.5\n# ... with 3,167 more rows\nExploratory data analysis\nThe time series is long (265 years!). We can visualize the time series both in full, and zoomed in on the first 10 years to get a feel for the series.\nVisualizing sunspot data with cowplot\nWe’ll make two ggplots and combine them using cowplot::plot_grid(). Note that for the zoomed in plot, we make use of tibbletime::time_filter(), which is an easy way to perform time-based filtering.\n\n\np1 <- sun_spots %>%\n    ggplot(aes(index, value)) +\n    geom_point(color = palette_light()[[1]], alpha = 0.5) +\n    theme_tq() +\n    labs(\n        title = \"From 1749 to 2013 (Full Data Set)\"\n    )\n\np2 <- sun_spots %>%\n    filter_time(\"start\" ~ \"1800\") %>%\n    ggplot(aes(index, value)) +\n    geom_line(color = palette_light()[[1]], alpha = 0.5) +\n    geom_point(color = palette_light()[[1]]) +\n    geom_smooth(method = \"loess\", span = 0.2, se = FALSE) +\n    theme_tq() +\n    labs(\n        title = \"1749 to 1759 (Zoomed In To Show Changes over the Year)\",\n        caption = \"datasets::sunspot.month\"\n    )\n\np_title <- ggdraw() + \n  draw_label(\"Sunspots\", size = 18, fontface = \"bold\", \n             colour = palette_light()[[1]])\n\nplot_grid(p_title, p1, p2, ncol = 1, rel_heights = c(0.1, 1, 1))\n\n\n\n\n\nBacktesting: time series cross validation\nWhen doing cross validation on sequential data, the time dependencies on preceding samples must be preserved. We can create a cross validation sampling plan by offsetting the window used to select sequential sub-samples. In essence, we’re creatively dealing with the fact that there’s no future test data available by creating multiple synthetic “futures” - a process often, esp. in finance, called “backtesting”.\nAs mentioned in the introduction, the rsample package includes facitlities for backtesting on time series. The vignette, “Time Series Analysis Example”, describes a procedure that uses the rolling_origin() function to create samples designed for time series cross validation. We’ll use this approach.\nDeveloping a backtesting strategy\nThe sampling plan we create uses 100 years (initial = 12 x 100 samples) for the training set and 50 years (assess = 12 x 50) for the testing (validation) set. We select a skip span of about 22 years (skip = 12 x 22 - 1) to approximately evenly distribute the samples into 6 sets that span the entire 265 years of sunspots history. Last, we select cumulative = FALSE to allow the origin to shift which ensures that models on more recent data are not given an unfair advantage (more observations) over those operating on less recent data. The tibble return contains the rolling_origin_resamples.\n\n\nperiods_train <- 12 * 100\nperiods_test  <- 12 * 50\nskip_span     <- 12 * 22 - 1\n\nrolling_origin_resamples <- rolling_origin(\n  sun_spots,\n  initial    = periods_train,\n  assess     = periods_test,\n  cumulative = FALSE,\n  skip       = skip_span\n)\n\nrolling_origin_resamples\n\n\n# Rolling origin forecast resampling \n# A tibble: 6 x 2\n  splits       id    \n  <list>       <chr> \n1 <S3: rsplit> Slice1\n2 <S3: rsplit> Slice2\n3 <S3: rsplit> Slice3\n4 <S3: rsplit> Slice4\n5 <S3: rsplit> Slice5\n6 <S3: rsplit> Slice6\nVisualizing the backtesting strategy\nWe can visualize the resamples with two custom functions. The first, plot_split(), plots one of the resampling splits using ggplot2. Note that an expand_y_axis argument is added to expand the date range to the full sun_spots dataset date range. This will become useful when we visualize all plots together.\n\n\n# Plotting function for a single split\nplot_split <- function(split, expand_y_axis = TRUE, \n                       alpha = 1, size = 1, base_size = 14) {\n    \n    # Manipulate data\n    train_tbl <- training(split) %>%\n        add_column(key = \"training\") \n    \n    test_tbl  <- testing(split) %>%\n        add_column(key = \"testing\") \n    \n    data_manipulated <- bind_rows(train_tbl, test_tbl) %>%\n        as_tbl_time(index = index) %>%\n        mutate(key = fct_relevel(key, \"training\", \"testing\"))\n        \n    # Collect attributes\n    train_time_summary <- train_tbl %>%\n        tk_index() %>%\n        tk_get_timeseries_summary()\n    \n    test_time_summary <- test_tbl %>%\n        tk_index() %>%\n        tk_get_timeseries_summary()\n    \n    # Visualize\n    g <- data_manipulated %>%\n        ggplot(aes(x = index, y = value, color = key)) +\n        geom_line(size = size, alpha = alpha) +\n        theme_tq(base_size = base_size) +\n        scale_color_tq() +\n        labs(\n          title    = glue(\"Split: {split$id}\"),\n          subtitle = glue(\"{train_time_summary$start} to \", \n                          \"{test_time_summary$end}\"),\n            y = \"\", x = \"\"\n        ) +\n        theme(legend.position = \"none\") \n    \n    if (expand_y_axis) {\n        \n        sun_spots_time_summary <- sun_spots %>% \n            tk_index() %>% \n            tk_get_timeseries_summary()\n        \n        g <- g +\n            scale_x_date(limits = c(sun_spots_time_summary$start, \n                                    sun_spots_time_summary$end))\n    }\n    \n    g\n}\n\n\nThe plot_split() function takes one split (in this case Slice01), and returns a visual of the sampling strategy. We expand the axis to the range for the full dataset using expand_y_axis = TRUE.\n\n\nrolling_origin_resamples$splits[[1]] %>%\n    plot_split(expand_y_axis = TRUE) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\nThe second function, plot_sampling_plan(), scales the plot_split() function to all of the samples using purrr and cowplot.\n\n\n# Plotting function that scales to all splits \nplot_sampling_plan <- function(sampling_tbl, expand_y_axis = TRUE, \n                               ncol = 3, alpha = 1, size = 1, base_size = 14, \n                               title = \"Sampling Plan\") {\n    \n    # Map plot_split() to sampling_tbl\n    sampling_tbl_with_plots <- sampling_tbl %>%\n        mutate(gg_plots = map(splits, plot_split, \n                              expand_y_axis = expand_y_axis,\n                              alpha = alpha, base_size = base_size))\n    \n    # Make plots with cowplot\n    plot_list <- sampling_tbl_with_plots$gg_plots \n    \n    p_temp <- plot_list[[1]] + theme(legend.position = \"bottom\")\n    legend <- get_legend(p_temp)\n    \n    p_body  <- plot_grid(plotlist = plot_list, ncol = ncol)\n    \n    p_title <- ggdraw() + \n        draw_label(title, size = 14, fontface = \"bold\", \n                   colour = palette_light()[[1]])\n    \n    g <- plot_grid(p_title, p_body, legend, ncol = 1, \n                   rel_heights = c(0.05, 1, 0.05))\n    \n    g\n    \n}\n\n\nWe can now visualize the entire backtesting strategy with plot_sampling_plan(). We can see how the sampling plan shifts the sampling window with each progressive slice of the train/test splits.\n\n\nrolling_origin_resamples %>%\n    plot_sampling_plan(expand_y_axis = T, ncol = 3, alpha = 1, size = 1, base_size = 10, \n                       title = \"Backtesting Strategy: Rolling Origin Sampling Plan\")\n\n\n\n\n\n\nAnd, we can set expand_y_axis = FALSE to zoom in on the samples.\n\n\nrolling_origin_resamples %>%\n    plot_sampling_plan(expand_y_axis = F, ncol = 3, alpha = 1, size = 1, base_size = 10, \n                       title = \"Backtesting Strategy: Zoomed In\")\n\n\n\n\n\n\nWe’ll use this backtesting strategy (6 samples from one time series each with 50/10 split in years and a ~20 year offset) when testing the veracity of the LSTM model on the sunspots dataset.\nThe LSTM model\nTo begin, we’ll develop an LSTM model on a single sample from the backtesting strategy, namely, the most recent slice. We’ll then apply the model to all samples to investigate modeling performance.\n\n\nexample_split    <- rolling_origin_resamples$splits[[6]]\nexample_split_id <- rolling_origin_resamples$id[[6]]\n\n\nWe can reuse the plot_split() function to visualize the split. Set expand_y_axis = FALSE to zoom in on the subsample.\n\n\nplot_split(example_split, expand_y_axis = FALSE, size = 0.5) +\n    theme(legend.position = \"bottom\") +\n    ggtitle(glue(\"Split: {example_split_id}\"))\n\n\n\nData setup\nTo aid hyperparameter tuning, besides the training set we also need a validation set.\nFor example, we will use a callback, callback_early_stopping, that stops training when no significant performance is seen on the validation set (what’s considered significant is up to you).\nWe will dedicate 2 thirds of the analysis set to training, and 1 third to validation.\n\n\ndf_trn <- analysis(example_split)[1:800, , drop = FALSE]\ndf_val <- analysis(example_split)[801:1200, , drop = FALSE]\ndf_tst <- assessment(example_split)\n\n\nFirst, let’s combine the training and testing data sets into a single data set with a column key that specifies where they came from (either “training” or “testing)”. Note that the tbl_time object will need to have the index respecified during the bind_rows() step, but this issue should be corrected in dplyr soon.\n\n\ndf <- bind_rows(\n  df_trn %>% add_column(key = \"training\"),\n  df_val %>% add_column(key = \"validation\"),\n  df_tst %>% add_column(key = \"testing\")\n) %>%\n  as_tbl_time(index = index)\n\ndf\n\n\n# A time tibble: 1,800 x 3\n# Index: index\n   index      value key     \n   <date>     <dbl> <chr>   \n 1 1849-06-01  81.1 training\n 2 1849-07-01  78   training\n 3 1849-08-01  67.7 training\n 4 1849-09-01  93.7 training\n 5 1849-10-01  71.5 training\n 6 1849-11-01  99   training\n 7 1849-12-01  97   training\n 8 1850-01-01  78   training\n 9 1850-02-01  89.4 training\n10 1850-03-01  82.6 training\n# ... with 1,790 more rows\nPreprocessing with recipes\nThe LSTM algorithm will usually work better if the input data has been centered and scaled. We can conveniently accomplish this using the recipes package. In addition to step_center and step_scale, we’re using step_sqrt to reduce variance and remov outliers. The actual transformations are executed when we bake the data according to the recipe:\n\n\nrec_obj <- recipe(value ~ ., df) %>%\n    step_sqrt(value) %>%\n    step_center(value) %>%\n    step_scale(value) %>%\n    prep()\n\ndf_processed_tbl <- bake(rec_obj, df)\n\ndf_processed_tbl\n\n\n# A tibble: 1,800 x 3\n   index      value key     \n   <date>     <dbl> <fct>   \n 1 1849-06-01 0.714 training\n 2 1849-07-01 0.660 training\n 3 1849-08-01 0.473 training\n 4 1849-09-01 0.922 training\n 5 1849-10-01 0.544 training\n 6 1849-11-01 1.01  training\n 7 1849-12-01 0.974 training\n 8 1850-01-01 0.660 training\n 9 1850-02-01 0.852 training\n10 1850-03-01 0.739 training\n# ... with 1,790 more rows\nNext, let’s capture the original center and scale so we can invert the steps after modeling. The square root step can then simply be undone by squaring the back-transformed data.\n\n\ncenter_history <- rec_obj$steps[[2]]$means[\"value\"]\nscale_history  <- rec_obj$steps[[3]]$sds[\"value\"]\n\nc(\"center\" = center_history, \"scale\" = scale_history)\n\n\ncenter.value  scale.value \n    6.694468     3.238935 \nReshaping the data\nKeras LSTM expects the input as well as the target data to be in a specific shape.\nThe input has to be a 3-d array of size num_samples, num_timesteps, num_features.\nHere, num_samples is the number of observations in the set. This will get fed to the model in portions of batch_size. The second dimension, num_timesteps, is the length of the hidden state we were talking about above. Finally, the third dimension is the number of predictors we’re using. For univariate time series, this is 1.\nHow long should we choose the hidden state to be? This generally depends on the dataset and our goal.\nIf we did one-step-ahead forecasts - thus, forecasting the following month only - our main concern would be choosing a state length that allows to learn any patterns present in the data.\nNow say we wanted to forecast 12 months instead, as does SILSO, the World Data Center for the production, preservation and dissemination of the international sunspot number.\nThe way we can do this, with Keras, is by wiring the LSTM hidden states to sets of consecutive outputs of the same length. Thus, if we want to produce predictions for 12 months, our LSTM should have a hidden state length of 12.\nThese 12 time steps will then get wired to 12 linear predictor units using a time_distributed() wrapper.\nThat wrapper’s task is to apply the same calculation (i.e., the same weight matrix) to every state input it receives.\nNow, what’s the target array’s format supposed to be? As we’re forecasting several timesteps here, the target data again needs to be 3-dimensional. Dimension 1 again is the batch dimension, dimension 2 again corresponds to the number of timesteps (the forecasted ones), and dimension 3 is the size of the wrapped layer.\nIn our case, the wrapped layer is a layer_dense() of a single unit, as we want exactly one prediction per point in time.\nSo, let’s reshape the data. The main action here is creating the sliding windows of 12 steps of input, followed by 12 steps of output each. This is easiest to understand with a shorter and simpler example. Say our input were the numbers from 1 to 10, and our chosen sequence length (state size) were 4. Tthis is how we would want our training input to look:\n1,2,3,4\n2,3,4,5\n3,4,5,6\nAnd our target data, correspondingly:\n5,6,7,8\n6,7,8,9\n7,8,9,10\nWe’ll define a short function that does this reshaping on a given dataset.\nThen finally, we add the third axis that is formally needed (even though that axis is of size 1 in our case).\n\n\n# these variables are being defined just because of the order in which\n# we present things in this post (first the data, then the model)\n# they will be superseded by FLAGS$n_timesteps, FLAGS$batch_size and n_predictions\n# in the following snippet\nn_timesteps <- 12\nn_predictions <- n_timesteps\nbatch_size <- 10\n\n# functions used\nbuild_matrix <- function(tseries, overall_timesteps) {\n  t(sapply(1:(length(tseries) - overall_timesteps + 1), function(x) \n    tseries[x:(x + overall_timesteps - 1)]))\n}\n\nreshape_X_3d <- function(X) {\n  dim(X) <- c(dim(X)[1], dim(X)[2], 1)\n  X\n}\n\n# extract values from data frame\ntrain_vals <- df_processed_tbl %>%\n  filter(key == \"training\") %>%\n  select(value) %>%\n  pull()\nvalid_vals <- df_processed_tbl %>%\n  filter(key == \"validation\") %>%\n  select(value) %>%\n  pull()\ntest_vals <- df_processed_tbl %>%\n  filter(key == \"testing\") %>%\n  select(value) %>%\n  pull()\n\n\n# build the windowed matrices\ntrain_matrix <-\n  build_matrix(train_vals, n_timesteps + n_predictions)\nvalid_matrix <-\n  build_matrix(valid_vals, n_timesteps + n_predictions)\ntest_matrix <- build_matrix(test_vals, n_timesteps + n_predictions)\n\n# separate matrices into training and testing parts\n# also, discard last batch if there are fewer than batch_size samples\n# (a purely technical requirement)\nX_train <- train_matrix[, 1:n_timesteps]\ny_train <- train_matrix[, (n_timesteps + 1):(n_timesteps * 2)]\nX_train <- X_train[1:(nrow(X_train) %/% batch_size * batch_size), ]\ny_train <- y_train[1:(nrow(y_train) %/% batch_size * batch_size), ]\n\nX_valid <- valid_matrix[, 1:n_timesteps]\ny_valid <- valid_matrix[, (n_timesteps + 1):(n_timesteps * 2)]\nX_valid <- X_valid[1:(nrow(X_valid) %/% batch_size * batch_size), ]\ny_valid <- y_valid[1:(nrow(y_valid) %/% batch_size * batch_size), ]\n\nX_test <- test_matrix[, 1:n_timesteps]\ny_test <- test_matrix[, (n_timesteps + 1):(n_timesteps * 2)]\nX_test <- X_test[1:(nrow(X_test) %/% batch_size * batch_size), ]\ny_test <- y_test[1:(nrow(y_test) %/% batch_size * batch_size), ]\n# add on the required third axis\nX_train <- reshape_X_3d(X_train)\nX_valid <- reshape_X_3d(X_valid)\nX_test <- reshape_X_3d(X_test)\n\ny_train <- reshape_X_3d(y_train)\ny_valid <- reshape_X_3d(y_valid)\ny_test <- reshape_X_3d(y_test)\n\n\nBuilding the LSTM model\nNow that we have our data in the required form, let’s finally build the model.\nAs always in deep learning, an important, and often time-consuming, part of the job is tuning hyperparameters. To keep this post self-contained, and considering this is primarily a tutorial on how to use LSTM in R, let’s assume the following settings were found after extensive experimentation (in reality experimentation did take place, but not to a degree that performance couldn’t possibly be improved).\nInstead of hard coding the hyperparameters, we’ll use tfruns to set up an environment where we could easily perform grid search.\nWe’ll quickly comment on what these parameters do but mainly leave those topics to further posts.\n\n\nFLAGS <- flags(\n  # There is a so-called \"stateful LSTM\" in Keras. While LSTM is stateful\n  # per se, this adds a further tweak where the hidden states get \n  # initialized with values from the item at same position in the previous\n  # batch. This is helpful just under specific circumstances, or if you want\n  # to create an \"infinite stream\" of states, in which case you'd use 1 as \n  # the batch size. Below, we show how the code would have to be changed to\n  # use this, but it won't be further discussed here.\n  flag_boolean(\"stateful\", FALSE),\n  # Should we use several layers of LSTM?\n  # Again, just included for completeness, it did not yield any superior \n  # performance on this task.\n  # This will actually stack exactly one additional layer of LSTM units.\n  flag_boolean(\"stack_layers\", FALSE),\n  # number of samples fed to the model in one go\n  flag_integer(\"batch_size\", 10),\n  # size of the hidden state, equals size of predictions\n  flag_integer(\"n_timesteps\", 12),\n  # how many epochs to train for\n  flag_integer(\"n_epochs\", 100),\n  # fraction of the units to drop for the linear transformation of the inputs\n  flag_numeric(\"dropout\", 0.2),\n  # fraction of the units to drop for the linear transformation of the \n  # recurrent state\n  flag_numeric(\"recurrent_dropout\", 0.2),\n  # loss function. Found to work better for this specific case than mean\n  # squared error\n  flag_string(\"loss\", \"logcosh\"),\n  # optimizer = stochastic gradient descent. Seemed to work better than adam \n  # or rmsprop here (as indicated by limited testing)\n  flag_string(\"optimizer_type\", \"sgd\"),\n  # size of the LSTM layer\n  flag_integer(\"n_units\", 128),\n  # learning rate\n  flag_numeric(\"lr\", 0.003),\n  # momentum, an additional parameter to the SGD optimizer\n  flag_numeric(\"momentum\", 0.9),\n  # parameter to the early stopping callback\n  flag_integer(\"patience\", 10)\n)\n\n# the number of predictions we'll make equals the length of the hidden state\nn_predictions <- FLAGS$n_timesteps\n# how many features = predictors we have\nn_features <- 1\n# just in case we wanted to try different optimizers, we could add here\noptimizer <- switch(FLAGS$optimizer_type,\n                    sgd = optimizer_sgd(lr = FLAGS$lr, \n                                        momentum = FLAGS$momentum)\n                    )\n\n# callbacks to be passed to the fit() function\n# We just use one here: we may stop before n_epochs if the loss on the\n# validation set does not decrease (by a configurable amount, over a \n# configurable time)\ncallbacks <- list(\n  callback_early_stopping(patience = FLAGS$patience)\n)\n\n\nAfter all these preparations, the code for constructing and training the model is rather short!\nLet’s first quickly view the “long version”, that would allow you to test stacking several LSTMs or use a stateful LSTM, then go through the final short version (that does neither) and comment on it.\nThis, just for reference, is the complete code.\n\n\nmodel <- keras_model_sequential()\n\nmodel %>%\n  layer_lstm(\n    units = FLAGS$n_units,\n    batch_input_shape = c(FLAGS$batch_size, FLAGS$n_timesteps, n_features),\n    dropout = FLAGS$dropout,\n    recurrent_dropout = FLAGS$recurrent_dropout,\n    return_sequences = TRUE,\n    stateful = FLAGS$stateful\n  )\n\nif (FLAGS$stack_layers) {\n  model %>%\n    layer_lstm(\n      units = FLAGS$n_units,\n      dropout = FLAGS$dropout,\n      recurrent_dropout = FLAGS$recurrent_dropout,\n      return_sequences = TRUE,\n      stateful = FLAGS$stateful\n    )\n}\nmodel %>% time_distributed(layer_dense(units = 1))\n\nmodel %>%\n  compile(\n    loss = FLAGS$loss,\n    optimizer = optimizer,\n    metrics = list(\"mean_squared_error\")\n  )\n\nif (!FLAGS$stateful) {\n  model %>% fit(\n    x          = X_train,\n    y          = y_train,\n    validation_data = list(X_valid, y_valid),\n    batch_size = FLAGS$batch_size,\n    epochs     = FLAGS$n_epochs,\n    callbacks = callbacks\n  )\n  \n} else {\n  for (i in 1:FLAGS$n_epochs) {\n    model %>% fit(\n      x          = X_train,\n      y          = y_train,\n      validation_data = list(X_valid, y_valid),\n      callbacks = callbacks,\n      batch_size = FLAGS$batch_size,\n      epochs     = 1,\n      shuffle    = FALSE\n    )\n    model %>% reset_states()\n  }\n}\n\nif (FLAGS$stateful)\n  model %>% reset_states()\n\n\nNow let’s step through the simpler, yet better (or equally) performing configuration below.\n\n\n# create the model\nmodel <- keras_model_sequential()\n\n# add layers\n# we have just two, the LSTM and the time_distributed \nmodel %>%\n  layer_lstm(\n    units = FLAGS$n_units, \n    # the first layer in a model needs to know the shape of the input data\n    batch_input_shape  = c(FLAGS$batch_size, FLAGS$n_timesteps, n_features),\n    dropout = FLAGS$dropout,\n    recurrent_dropout = FLAGS$recurrent_dropout,\n    # by default, an LSTM just returns the final state\n    return_sequences = TRUE\n  ) %>% time_distributed(layer_dense(units = 1))\n\nmodel %>%\n  compile(\n    loss = FLAGS$loss,\n    optimizer = optimizer,\n    # in addition to the loss, Keras will inform us about current \n    # MSE while training\n    metrics = list(\"mean_squared_error\")\n  )\n\nhistory <- model %>% fit(\n  x          = X_train,\n  y          = y_train,\n  validation_data = list(X_valid, y_valid),\n  batch_size = FLAGS$batch_size,\n  epochs     = FLAGS$n_epochs,\n  callbacks = callbacks\n)\n\n\nAs we see, training was stopped after ~55 epochs as validation loss did not decrease any more.\nWe also see that performance on the validation set is way worse than performance on the training set - normally indicating overfitting.\nThis topic too, we’ll leave to a separate discussion another time, but interestingly regularization using higher values of dropout and recurrent_dropout (combined with increasing model capacity) did not yield better generalization performance. This is probably related to the characteristics of this specific time series we mentioned in the introduction.\n\n\nplot(history, metrics = \"loss\")\n\n\n\nNow let’s see how well the model was able to capture the characteristics of the training set.\n\n\npred_train <- model %>%\n  predict(X_train, batch_size = FLAGS$batch_size) %>%\n  .[, , 1]\n\n# Retransform values to original scale\npred_train <- (pred_train * scale_history + center_history) ^2\ncompare_train <- df %>% filter(key == \"training\")\n\n# build a dataframe that has both actual and predicted values\nfor (i in 1:nrow(pred_train)) {\n  varname <- paste0(\"pred_train\", i)\n  compare_train <-\n    mutate(compare_train,!!varname := c(\n      rep(NA, FLAGS$n_timesteps + i - 1),\n      pred_train[i,],\n      rep(NA, nrow(compare_train) - FLAGS$n_timesteps * 2 - i + 1)\n    ))\n}\n\n\nWe compute the average RSME over all sequences of predictions.\n\n\ncoln <- colnames(compare_train)[4:ncol(compare_train)]\ncols <- map(coln, quo(sym(.)))\nrsme_train <-\n  map_dbl(cols, function(col)\n    rmse(\n      compare_train,\n      truth = value,\n      estimate = !!col,\n      na.rm = TRUE\n    )) %>% mean()\n\nrsme_train\n\n\n21.01495\nHow do these predictions really look? As a visualization of all predicted sequences would look pretty crowded, we arbitrarily pick start points at regular intervals.\n\n\nggplot(compare_train, aes(x = index, y = value)) + geom_line() +\n  geom_line(aes(y = pred_train1), color = \"cyan\") +\n  geom_line(aes(y = pred_train50), color = \"red\") +\n  geom_line(aes(y = pred_train100), color = \"green\") +\n  geom_line(aes(y = pred_train150), color = \"violet\") +\n  geom_line(aes(y = pred_train200), color = \"cyan\") +\n  geom_line(aes(y = pred_train250), color = \"red\") +\n  geom_line(aes(y = pred_train300), color = \"red\") +\n  geom_line(aes(y = pred_train350), color = \"green\") +\n  geom_line(aes(y = pred_train400), color = \"cyan\") +\n  geom_line(aes(y = pred_train450), color = \"red\") +\n  geom_line(aes(y = pred_train500), color = \"green\") +\n  geom_line(aes(y = pred_train550), color = \"violet\") +\n  geom_line(aes(y = pred_train600), color = \"cyan\") +\n  geom_line(aes(y = pred_train650), color = \"red\") +\n  geom_line(aes(y = pred_train700), color = \"red\") +\n  geom_line(aes(y = pred_train750), color = \"green\") +\n  ggtitle(\"Predictions on the training set\")\n\n\n\n\n\nThis looks pretty good. From the validation loss, we don’t quite expect the same from the test set, though.\nLet’s see.\n\n\npred_test <- model %>%\n  predict(X_test, batch_size = FLAGS$batch_size) %>%\n  .[, , 1]\n\n# Retransform values to original scale\npred_test <- (pred_test * scale_history + center_history) ^2\npred_test[1:10, 1:5] %>% print()\ncompare_test <- df %>% filter(key == \"testing\")\n\n# build a dataframe that has both actual and predicted values\nfor (i in 1:nrow(pred_test)) {\n  varname <- paste0(\"pred_test\", i)\n  compare_test <-\n    mutate(compare_test,!!varname := c(\n      rep(NA, FLAGS$n_timesteps + i - 1),\n      pred_test[i,],\n      rep(NA, nrow(compare_test) - FLAGS$n_timesteps * 2 - i + 1)\n    ))\n}\n\ncompare_test %>% write_csv(str_replace(model_path, \".hdf5\", \".test.csv\"))\ncompare_test[FLAGS$n_timesteps:(FLAGS$n_timesteps + 10), c(2, 4:8)] %>% print()\n\ncoln <- colnames(compare_test)[4:ncol(compare_test)]\ncols <- map(coln, quo(sym(.)))\nrsme_test <-\n  map_dbl(cols, function(col)\n    rmse(\n      compare_test,\n      truth = value,\n      estimate = !!col,\n      na.rm = TRUE\n    )) %>% mean()\n\nrsme_test\n\n\n31.31616\n\n\nggplot(compare_test, aes(x = index, y = value)) + geom_line() +\n  geom_line(aes(y = pred_test1), color = \"cyan\") +\n  geom_line(aes(y = pred_test50), color = \"red\") +\n  geom_line(aes(y = pred_test100), color = \"green\") +\n  geom_line(aes(y = pred_test150), color = \"violet\") +\n  geom_line(aes(y = pred_test200), color = \"cyan\") +\n  geom_line(aes(y = pred_test250), color = \"red\") +\n  geom_line(aes(y = pred_test300), color = \"green\") +\n  geom_line(aes(y = pred_test350), color = \"cyan\") +\n  geom_line(aes(y = pred_test400), color = \"red\") +\n  geom_line(aes(y = pred_test450), color = \"green\") +  \n  geom_line(aes(y = pred_test500), color = \"cyan\") +\n  geom_line(aes(y = pred_test550), color = \"violet\") +\n  ggtitle(\"Predictions on test set\")\n\n\n\n\n\nThat’s not as good as on the training set, but not bad either, given this time series is quite challenging.\nHaving defined and run our model on a manually chosen example split, let’s now revert to our overall re-sampling frame.\nBacktesting the model on all splits\nTo obtain predictions on all splits, we move the above code into a function and apply it to all splits.\nFirst, here’s the function. It returns a list of two dataframes, one for the training and test sets each, that contain the model’s predictions together with the actual values.\n\n\nobtain_predictions <- function(split) {\n  df_trn <- analysis(split)[1:800, , drop = FALSE]\n  df_val <- analysis(split)[801:1200, , drop = FALSE]\n  df_tst <- assessment(split)\n  \n  df <- bind_rows(\n    df_trn %>% add_column(key = \"training\"),\n    df_val %>% add_column(key = \"validation\"),\n    df_tst %>% add_column(key = \"testing\")\n  ) %>%\n    as_tbl_time(index = index)\n  \n  rec_obj <- recipe(value ~ ., df) %>%\n    step_sqrt(value) %>%\n    step_center(value) %>%\n    step_scale(value) %>%\n    prep()\n  \n  df_processed_tbl <- bake(rec_obj, df)\n  \n  center_history <- rec_obj$steps[[2]]$means[\"value\"]\n  scale_history  <- rec_obj$steps[[3]]$sds[\"value\"]\n  \n  FLAGS <- flags(\n    flag_boolean(\"stateful\", FALSE),\n    flag_boolean(\"stack_layers\", FALSE),\n    flag_integer(\"batch_size\", 10),\n    flag_integer(\"n_timesteps\", 12),\n    flag_integer(\"n_epochs\", 100),\n    flag_numeric(\"dropout\", 0.2),\n    flag_numeric(\"recurrent_dropout\", 0.2),\n    flag_string(\"loss\", \"logcosh\"),\n    flag_string(\"optimizer_type\", \"sgd\"),\n    flag_integer(\"n_units\", 128),\n    flag_numeric(\"lr\", 0.003),\n    flag_numeric(\"momentum\", 0.9),\n    flag_integer(\"patience\", 10)\n  )\n  \n  n_predictions <- FLAGS$n_timesteps\n  n_features <- 1\n  \n  optimizer <- switch(FLAGS$optimizer_type,\n                      sgd = optimizer_sgd(lr = FLAGS$lr, momentum = FLAGS$momentum))\n  callbacks <- list(\n    callback_early_stopping(patience = FLAGS$patience)\n  )\n  \n  train_vals <- df_processed_tbl %>%\n    filter(key == \"training\") %>%\n    select(value) %>%\n    pull()\n  valid_vals <- df_processed_tbl %>%\n    filter(key == \"validation\") %>%\n    select(value) %>%\n    pull()\n  test_vals <- df_processed_tbl %>%\n    filter(key == \"testing\") %>%\n    select(value) %>%\n    pull()\n  \n  train_matrix <-\n    build_matrix(train_vals, FLAGS$n_timesteps + n_predictions)\n  valid_matrix <-\n    build_matrix(valid_vals, FLAGS$n_timesteps + n_predictions)\n  test_matrix <-\n    build_matrix(test_vals, FLAGS$n_timesteps + n_predictions)\n  \n  X_train <- train_matrix[, 1:FLAGS$n_timesteps]\n  y_train <-\n    train_matrix[, (FLAGS$n_timesteps + 1):(FLAGS$n_timesteps * 2)]\n  X_train <-\n    X_train[1:(nrow(X_train) %/% FLAGS$batch_size * FLAGS$batch_size),]\n  y_train <-\n    y_train[1:(nrow(y_train) %/% FLAGS$batch_size * FLAGS$batch_size),]\n  \n  X_valid <- valid_matrix[, 1:FLAGS$n_timesteps]\n  y_valid <-\n    valid_matrix[, (FLAGS$n_timesteps + 1):(FLAGS$n_timesteps * 2)]\n  X_valid <-\n    X_valid[1:(nrow(X_valid) %/% FLAGS$batch_size * FLAGS$batch_size),]\n  y_valid <-\n    y_valid[1:(nrow(y_valid) %/% FLAGS$batch_size * FLAGS$batch_size),]\n  \n  X_test <- test_matrix[, 1:FLAGS$n_timesteps]\n  y_test <-\n    test_matrix[, (FLAGS$n_timesteps + 1):(FLAGS$n_timesteps * 2)]\n  X_test <-\n    X_test[1:(nrow(X_test) %/% FLAGS$batch_size * FLAGS$batch_size),]\n  y_test <-\n    y_test[1:(nrow(y_test) %/% FLAGS$batch_size * FLAGS$batch_size),]\n  \n  X_train <- reshape_X_3d(X_train)\n  X_valid <- reshape_X_3d(X_valid)\n  X_test <- reshape_X_3d(X_test)\n  \n  y_train <- reshape_X_3d(y_train)\n  y_valid <- reshape_X_3d(y_valid)\n  y_test <- reshape_X_3d(y_test)\n  \n  model <- keras_model_sequential()\n  \n  model %>%\n    layer_lstm(\n      units            = FLAGS$n_units,\n      batch_input_shape  = c(FLAGS$batch_size, FLAGS$n_timesteps, n_features),\n      dropout = FLAGS$dropout,\n      recurrent_dropout = FLAGS$recurrent_dropout,\n      return_sequences = TRUE\n    )     %>% time_distributed(layer_dense(units = 1))\n  \n  model %>%\n    compile(\n      loss = FLAGS$loss,\n      optimizer = optimizer,\n      metrics = list(\"mean_squared_error\")\n    )\n  \n  model %>% fit(\n    x          = X_train,\n    y          = y_train,\n    validation_data = list(X_valid, y_valid),\n    batch_size = FLAGS$batch_size,\n    epochs     = FLAGS$n_epochs,\n    callbacks = callbacks\n  )\n  \n  \n  pred_train <- model %>%\n    predict(X_train, batch_size = FLAGS$batch_size) %>%\n    .[, , 1]\n  \n  # Retransform values\n  pred_train <- (pred_train * scale_history + center_history) ^ 2\n  compare_train <- df %>% filter(key == \"training\")\n  \n  for (i in 1:nrow(pred_train)) {\n    varname <- paste0(\"pred_train\", i)\n    compare_train <-\n      mutate(compare_train, !!varname := c(\n        rep(NA, FLAGS$n_timesteps + i - 1),\n        pred_train[i, ],\n        rep(NA, nrow(compare_train) - FLAGS$n_timesteps * 2 - i + 1)\n      ))\n  }\n  \n  pred_test <- model %>%\n    predict(X_test, batch_size = FLAGS$batch_size) %>%\n    .[, , 1]\n  \n  # Retransform values\n  pred_test <- (pred_test * scale_history + center_history) ^ 2\n  compare_test <- df %>% filter(key == \"testing\")\n  \n  for (i in 1:nrow(pred_test)) {\n    varname <- paste0(\"pred_test\", i)\n    compare_test <-\n      mutate(compare_test, !!varname := c(\n        rep(NA, FLAGS$n_timesteps + i - 1),\n        pred_test[i, ],\n        rep(NA, nrow(compare_test) - FLAGS$n_timesteps * 2 - i + 1)\n      ))\n  }\n  list(train = compare_train, test = compare_test)\n  \n}\n\n\nMapping the function over all splits yields a list of predictions.\n\n\nall_split_preds <- rolling_origin_resamples %>%\n     mutate(predict = map(splits, obtain_predictions))\n\n\nCalculate RMSE on all splits:\n\n\ncalc_rmse <- function(df) {\n  coln <- colnames(df)[4:ncol(df)]\n  cols <- map(coln, quo(sym(.)))\n  map_dbl(cols, function(col)\n    rmse(\n      df,\n      truth = value,\n      estimate = !!col,\n      na.rm = TRUE\n    )) %>% mean()\n}\n\nall_split_preds <- all_split_preds %>% unnest(predict)\nall_split_preds_train <- all_split_preds[seq(1, 11, by = 2), ]\nall_split_preds_test <- all_split_preds[seq(2, 12, by = 2), ]\n\nall_split_rmses_train <- all_split_preds_train %>%\n  mutate(rmse = map_dbl(predict, calc_rmse)) %>%\n  select(id, rmse)\n\nall_split_rmses_test <- all_split_preds_test %>%\n  mutate(rmse = map_dbl(predict, calc_rmse)) %>%\n  select(id, rmse)\n\n\nHow does it look? Here’s RMSE on the training set for the 6 splits.\n\n\nall_split_rmses_train\n\n\n# A tibble: 6 x 2\n  id      rmse\n  <chr>  <dbl>\n1 Slice1  22.2\n2 Slice2  20.9\n3 Slice3  18.8\n4 Slice4  23.5\n5 Slice5  22.1\n6 Slice6  21.1\n\n\nall_split_rmses_test\n\n\n# A tibble: 6 x 2\n  id      rmse\n  <chr>  <dbl>\n1 Slice1  21.6\n2 Slice2  20.6\n3 Slice3  21.3\n4 Slice4  31.4\n5 Slice5  35.2\n6 Slice6  31.4\nLooking at these numbers, we see something interesting: Generalization performance is much better for the first three slices of the time series than for the latter ones. This confirms our impression, stated above, that there seems to be some hidden development going on, rendering forecasting more difficult.\nAnd here are visualizations of the predictions on the respective training and test sets.\nFirst, the training sets:\n\n\nplot_train <- function(slice, name) {\n  ggplot(slice, aes(x = index, y = value)) + geom_line() +\n    geom_line(aes(y = pred_train1), color = \"cyan\") +\n    geom_line(aes(y = pred_train50), color = \"red\") +\n    geom_line(aes(y = pred_train100), color = \"green\") +\n    geom_line(aes(y = pred_train150), color = \"violet\") +\n    geom_line(aes(y = pred_train200), color = \"cyan\") +\n    geom_line(aes(y = pred_train250), color = \"red\") +\n    geom_line(aes(y = pred_train300), color = \"red\") +\n    geom_line(aes(y = pred_train350), color = \"green\") +\n    geom_line(aes(y = pred_train400), color = \"cyan\") +\n    geom_line(aes(y = pred_train450), color = \"red\") +\n    geom_line(aes(y = pred_train500), color = \"green\") +\n    geom_line(aes(y = pred_train550), color = \"violet\") +\n    geom_line(aes(y = pred_train600), color = \"cyan\") +\n    geom_line(aes(y = pred_train650), color = \"red\") +\n    geom_line(aes(y = pred_train700), color = \"red\") +\n    geom_line(aes(y = pred_train750), color = \"green\") +\n    ggtitle(name)\n}\n\ntrain_plots <- map2(all_split_preds_train$predict, all_split_preds_train$id, plot_train)\np_body_train  <- plot_grid(plotlist = train_plots, ncol = 3)\np_title_train <- ggdraw() + \n  draw_label(\"Backtested Predictions: Training Sets\", size = 18, fontface = \"bold\")\n\nplot_grid(p_title_train, p_body_train, ncol = 1, rel_heights = c(0.05, 1, 0.05))\n\n\n\n\n\n\nAnd the test sets:\n\n\nplot_test <- function(slice, name) {\n  ggplot(slice, aes(x = index, y = value)) + geom_line() +\n    geom_line(aes(y = pred_test1), color = \"cyan\") +\n    geom_line(aes(y = pred_test50), color = \"red\") +\n    geom_line(aes(y = pred_test100), color = \"green\") +\n    geom_line(aes(y = pred_test150), color = \"violet\") +\n    geom_line(aes(y = pred_test200), color = \"cyan\") +\n    geom_line(aes(y = pred_test250), color = \"red\") +\n    geom_line(aes(y = pred_test300), color = \"green\") +\n    geom_line(aes(y = pred_test350), color = \"cyan\") +\n    geom_line(aes(y = pred_test400), color = \"red\") +\n    geom_line(aes(y = pred_test450), color = \"green\") +  \n    geom_line(aes(y = pred_test500), color = \"cyan\") +\n    geom_line(aes(y = pred_test550), color = \"violet\") +\n    ggtitle(name)\n}\n\ntest_plots <- map2(all_split_preds_test$predict, all_split_preds_test$id, plot_test)\n\np_body_test  <- plot_grid(plotlist = test_plots, ncol = 3)\np_title_test <- ggdraw() + \n  draw_label(\"Backtested Predictions: Test Sets\", size = 18, fontface = \"bold\")\n\nplot_grid(p_title_test, p_body_test, ncol = 1, rel_heights = c(0.05, 1, 0.05))\n\n\n\n\n\n\nThis has been a long post, and necessarily will have left a lot of questions open, first and foremost: How do we obtain good settings for the hyperparameters (learning rate, number of epochs, dropout)?\nHow do we choose the length of the hidden state? Or even, can we have an intuition how well LSTM will perform on a given dataset (with its specific characteristics)?\nWe will tackle questions like the above in upcoming posts.\n\n\n\n",
    "preview": "posts/2018-06-25-sunspots-lstm/images/backtested_test.png",
    "last_modified": "2024-11-21T15:51:06+00:00",
    "input_file": {},
    "preview_width": 800,
    "preview_height": 416
  },
  {
    "path": "posts/2018-06-06-simple-audio-classification-keras/",
    "title": "Simple Audio Classification with Keras",
    "description": "In this tutorial we will build a deep learning model to classify words. We will use the Speech Commands dataset which consists of 65,000 one-second audio files of people saying 30 different words.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": "https://github.com/dfalbel"
      }
    ],
    "date": "2018-06-06",
    "categories": [
      "TensorFlow/Keras",
      "Audio Processing"
    ],
    "contents": "\nIntroduction\nIn this tutorial we will build a deep learning model to classify words. We will use tfdatasets to handle data IO and pre-processing, and Keras to build and train the model.\nWe will use the Speech Commands dataset which consists of 65,000 one-second audio files of people saying 30 different words. Each file contains a single spoken English word. The dataset was released by Google under CC License.\nOur model is a Keras port of the TensorFlow tutorial on Simple Audio Recognition which in turn was inspired by Convolutional Neural Networks for Small-footprint Keyword Spotting. There are other approaches to the speech recognition task, like recurrent neural networks, dilated (atrous) convolutions or Learning from Between-class Examples for Deep Sound Recognition.\nThe model we will implement here is not the state of the art for audio recognition systems, which are way more complex, but is relatively simple and fast to train. Plus, we show how to efficiently use tfdatasets to preprocess and serve data.\nAudio representation\nMany deep learning models are end-to-end, i.e. we let the model learn useful representations directly from the raw data. However, audio data grows very fast - 16,000 samples per second with a very rich structure at many time-scales. In order to avoid having to deal with raw wave sound data, researchers usually use some kind of feature engineering.\nEvery sound wave can be represented by its spectrum, and digitally it can be computed using the Fast Fourier Transform (FFT).\nBy Phonical - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=64473578A common way to represent audio data is to break it into small chunks, which usually overlap. For each chunk we use the FFT to calculate the magnitude of the frequency spectrum. The spectra are then combined, side by side, to form what we call a spectrogram.\nIt’s also common for speech recognition systems to further transform the spectrum and compute the Mel-Frequency Cepstral Coefficients. This transformation takes into account that the human ear can’t discern the difference between two closely spaced frequencies and smartly creates bins on the frequency axis. A great tutorial on MFCCs can be found here.\nBy Aquegg - Own work, Public Domain, https://commons.wikimedia.org/w/index.php?curid=5544473After this procedure, we have an image for each audio sample and we can use convolutional neural networks, the standard architecture type in image recognition models.\nDownloading\nFirst, let’s download data to a directory in our project. You can either download from this link (~1GB) or from R with:\n\n\ndir.create(\"data\")\n\ndownload.file(\n  url = \"http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\", \n  destfile = \"data/speech_commands_v0.01.tar.gz\"\n)\n\nuntar(\"data/speech_commands_v0.01.tar.gz\", exdir = \"data/speech_commands_v0.01\")\n\n\nInside the data directory we will have a folder called speech_commands_v0.01. The WAV audio files inside this directory are organised in sub-folders with the label names. For example, all one-second audio files of people speaking the word “bed” are inside the bed directory. There are 30 of them and a special one called _background_noise_ which contains various patterns that could be mixed in to simulate background noise.\nImporting\nIn this step we will list all audio .wav files into a tibble with 3 columns:\nfname: the file name;\nclass: the label for each audio file;\nclass_id: a unique integer number starting from zero for each class - used to one-hot encode the classes.\nThis will be useful to the next step when we will create a generator using the tfdatasets package.\n\n\nlibrary(stringr)\nlibrary(dplyr)\n\nfiles <- fs::dir_ls(\n  path = \"data/speech_commands_v0.01/\", \n  recursive = TRUE, \n  glob = \"*.wav\"\n)\n\nfiles <- files[!str_detect(files, \"background_noise\")]\n\ndf <- data_frame(\n  fname = files, \n  class = fname %>% str_extract(\"1/.*/\") %>% \n    str_replace_all(\"1/\", \"\") %>%\n    str_replace_all(\"/\", \"\"),\n  class_id = class %>% as.factor() %>% as.integer() - 1L\n)\n\n\nGenerator\nWe will now create our Dataset, which in the context of tfdatasets, adds operations to the TensorFlow graph in order to read and pre-process data. Since they are TensorFlow ops, they are executed in C++ and in parallel with model training.\nThe generator we will create will be responsible for reading the audio files from disk, creating the spectrogram for each one and batching the outputs.\nLet’s start by creating the dataset from slices of the data.frame with audio file names and classes we just created.\n\n\nlibrary(tfdatasets)\nds <- tensor_slices_dataset(df) \n\n\nNow, let’s define the parameters for spectrogram creation. We need to define window_size_ms which is the size in milliseconds of each chunk we will break the audio wave into, and window_stride_ms, the distance between the centers of adjacent chunks:\n\n\nwindow_size_ms <- 30\nwindow_stride_ms <- 10\n\n\nNow we will convert the window size and stride from milliseconds to samples. We are considering that our audio files have 16,000 samples per second (1000 ms).\n\n\nwindow_size <- as.integer(16000*window_size_ms/1000)\nstride <- as.integer(16000*window_stride_ms/1000)\n\n\nWe will obtain other quantities that will be useful for spectrogram creation, like the number of chunks and the FFT size, i.e., the number of bins on the frequency axis. The function we are going to use to compute the spectrogram doesn’t allow us to change the FFT size and instead by default uses the first power of 2 greater than the window size.\n\n\nfft_size <- as.integer(2^trunc(log(window_size, 2)) + 1)\nn_chunks <- length(seq(window_size/2, 16000 - window_size/2, stride))\n\n\nWe will now use dataset_map which allows us to specify a pre-processing function for each observation (line) of our dataset. It’s in this step that we read the raw audio file from disk and create its spectrogram and the one-hot encoded response vector.\n\n\n# shortcuts to used TensorFlow modules.\naudio_ops <- tf$contrib$framework$python$ops$audio_ops\n\nds <- ds %>%\n  dataset_map(function(obs) {\n    \n    # a good way to debug when building tfdatsets pipelines is to use a print\n    # statement like this:\n    # print(str(obs))\n    \n    # decoding wav files\n    audio_binary <- tf$read_file(tf$reshape(obs$fname, shape = list()))\n    wav <- audio_ops$decode_wav(audio_binary, desired_channels = 1)\n    \n    # create the spectrogram\n    spectrogram <- audio_ops$audio_spectrogram(\n      wav$audio, \n      window_size = window_size, \n      stride = stride,\n      magnitude_squared = TRUE\n    )\n    \n    # normalization\n    spectrogram <- tf$log(tf$abs(spectrogram) + 0.01)\n    \n    # moving channels to last dim\n    spectrogram <- tf$transpose(spectrogram, perm = c(1L, 2L, 0L))\n    \n    # transform the class_id into a one-hot encoded vector\n    response <- tf$one_hot(obs$class_id, 30L)\n    \n    list(spectrogram, response)\n  }) \n\n\nNow, we will specify how we want batch observations from the dataset. We’re using dataset_shuffle since we want to shuffle observations from the dataset, otherwise it would follow the order of the df object. Then we use dataset_repeat in order to tell TensorFlow that we want to keep taking observations from the dataset even if all observations have already been used. And most importantly here, we use dataset_padded_batch to specify that we want batches of size 32, but they should be padded, ie. if some observation has a different size we pad it with zeroes. The padded shape is passed to dataset_padded_batch via the padded_shapes argument and we use NULL to state that this dimension doesn’t need to be padded.\n\n\nds <- ds %>% \n  dataset_shuffle(buffer_size = 100) %>%\n  dataset_repeat() %>%\n  dataset_padded_batch(\n    batch_size = 32, \n    padded_shapes = list(\n      shape(n_chunks, fft_size, NULL), \n      shape(NULL)\n    )\n  )\n\n\nThis is our dataset specification, but we would need to rewrite all the code for the validation data, so it’s good practice to wrap this into a function of the data and other important parameters like window_size_ms and window_stride_ms. Below, we will define a function called data_generator that will create the generator depending on those inputs.\n\n\ndata_generator <- function(df, batch_size, shuffle = TRUE, \n                           window_size_ms = 30, window_stride_ms = 10) {\n  \n  window_size <- as.integer(16000*window_size_ms/1000)\n  stride <- as.integer(16000*window_stride_ms/1000)\n  fft_size <- as.integer(2^trunc(log(window_size, 2)) + 1)\n  n_chunks <- length(seq(window_size/2, 16000 - window_size/2, stride))\n  \n  ds <- tensor_slices_dataset(df)\n  \n  if (shuffle) \n    ds <- ds %>% dataset_shuffle(buffer_size = 100)  \n  \n  ds <- ds %>%\n    dataset_map(function(obs) {\n      \n      # decoding wav files\n      audio_binary <- tf$read_file(tf$reshape(obs$fname, shape = list()))\n      wav <- audio_ops$decode_wav(audio_binary, desired_channels = 1)\n      \n      # create the spectrogram\n      spectrogram <- audio_ops$audio_spectrogram(\n        wav$audio, \n        window_size = window_size, \n        stride = stride,\n        magnitude_squared = TRUE\n      )\n      \n      spectrogram <- tf$log(tf$abs(spectrogram) + 0.01)\n      spectrogram <- tf$transpose(spectrogram, perm = c(1L, 2L, 0L))\n      \n      # transform the class_id into a one-hot encoded vector\n      response <- tf$one_hot(obs$class_id, 30L)\n      \n      list(spectrogram, response)\n    }) %>%\n    dataset_repeat()\n  \n  ds <- ds %>% \n    dataset_padded_batch(batch_size, list(shape(n_chunks, fft_size, NULL), shape(NULL)))\n  \n  ds\n}\n\n\nNow, we can define training and validation data generators. It’s worth noting that executing this won’t actually compute any spectrogram or read any file. It will only define in the TensorFlow graph how it should read and pre-process data.\n\n\nset.seed(6)\nid_train <- sample(nrow(df), size = 0.7*nrow(df))\n\nds_train <- data_generator(\n  df[id_train,], \n  batch_size = 32, \n  window_size_ms = 30, \n  window_stride_ms = 10\n)\nds_validation <- data_generator(\n  df[-id_train,], \n  batch_size = 32, \n  shuffle = FALSE, \n  window_size_ms = 30, \n  window_stride_ms = 10\n)\n\n\nTo actually get a batch from the generator we could create a TensorFlow session and ask it to run the generator. For example:\n\n\nsess <- tf$Session()\nbatch <- next_batch(ds_train)\nstr(sess$run(batch))\n\n\nList of 2\n $ : num [1:32, 1:98, 1:257, 1] -4.6 -4.6 -4.61 -4.6 -4.6 ...\n $ : num [1:32, 1:30] 0 0 0 0 0 0 0 0 0 0 ...\nEach time you run sess$run(batch) you should see a different batch of observations.\nModel definition\nNow that we know how we will feed our data we can focus on the model definition. The spectrogram can be treated like an image, so architectures that are commonly used in image recognition tasks should work well with the spectrograms too.\nWe will build a convolutional neural network similar to what we have built here for the MNIST dataset.\nThe input size is defined by the number of chunks and the FFT size. Like we explained earlier, they can be obtained from the window_size_ms and window_stride_ms used to generate the spectrogram.\n\n\nwindow_size <- as.integer(16000*window_size_ms/1000)\nstride <- as.integer(16000*window_stride_ms/1000)\nfft_size <- as.integer(2^trunc(log(window_size, 2)) + 1)\nn_chunks <- length(seq(window_size/2, 16000 - window_size/2, stride))\n\n\nWe will now define our model using the Keras sequential API:\n\n\nmodel <- keras_model_sequential()\nmodel %>%  \n  layer_conv_2d(input_shape = c(n_chunks, fft_size, 1), \n                filters = 32, kernel_size = c(3,3), activation = 'relu') %>% \n  layer_max_pooling_2d(pool_size = c(2, 2)) %>% \n  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>% \n  layer_max_pooling_2d(pool_size = c(2, 2)) %>% \n  layer_conv_2d(filters = 128, kernel_size = c(3,3), activation = 'relu') %>% \n  layer_max_pooling_2d(pool_size = c(2, 2)) %>% \n  layer_conv_2d(filters = 256, kernel_size = c(3,3), activation = 'relu') %>% \n  layer_max_pooling_2d(pool_size = c(2, 2)) %>% \n  layer_dropout(rate = 0.25) %>% \n  layer_flatten() %>% \n  layer_dense(units = 128, activation = 'relu') %>% \n  layer_dropout(rate = 0.5) %>% \n  layer_dense(units = 30, activation = 'softmax')\n\n\nWe used 4 layers of convolutions combined with max pooling layers to extract features from the spectrogram images and 2 dense layers at the top. Our network is comparatively simple when compared to more advanced architectures like ResNet or DenseNet that perform very well on image recognition tasks.\nNow let’s compile our model. We will use categorical cross entropy as the loss function and use the Adadelta optimizer. It’s also here that we define that we will look at the accuracy metric during training.\n\n\nmodel %>% compile(\n  loss = loss_categorical_crossentropy,\n  optimizer = optimizer_adadelta(),\n  metrics = c('accuracy')\n)\n\n\nModel fitting\nNow, we will fit our model. In Keras we can use TensorFlow Datasets as inputs to the fit_generator function and we will do it here.\n\n\nmodel %>% fit_generator(\n  generator = ds_train,\n  steps_per_epoch = 0.7*nrow(df)/32,\n  epochs = 10, \n  validation_data = ds_validation, \n  validation_steps = 0.3*nrow(df)/32\n)\n\n\nEpoch 1/10\n1415/1415 [==============================] - 87s 62ms/step - loss: 2.0225 - acc: 0.4184 - val_loss: 0.7855 - val_acc: 0.7907\nEpoch 2/10\n1415/1415 [==============================] - 75s 53ms/step - loss: 0.8781 - acc: 0.7432 - val_loss: 0.4522 - val_acc: 0.8704\nEpoch 3/10\n1415/1415 [==============================] - 75s 53ms/step - loss: 0.6196 - acc: 0.8190 - val_loss: 0.3513 - val_acc: 0.9006\nEpoch 4/10\n1415/1415 [==============================] - 75s 53ms/step - loss: 0.4958 - acc: 0.8543 - val_loss: 0.3130 - val_acc: 0.9117\nEpoch 5/10\n1415/1415 [==============================] - 75s 53ms/step - loss: 0.4282 - acc: 0.8754 - val_loss: 0.2866 - val_acc: 0.9213\nEpoch 6/10\n1415/1415 [==============================] - 76s 53ms/step - loss: 0.3852 - acc: 0.8885 - val_loss: 0.2732 - val_acc: 0.9252\nEpoch 7/10\n1415/1415 [==============================] - 75s 53ms/step - loss: 0.3566 - acc: 0.8991 - val_loss: 0.2700 - val_acc: 0.9269\nEpoch 8/10\n1415/1415 [==============================] - 76s 54ms/step - loss: 0.3364 - acc: 0.9045 - val_loss: 0.2573 - val_acc: 0.9284\nEpoch 9/10\n1415/1415 [==============================] - 76s 53ms/step - loss: 0.3220 - acc: 0.9087 - val_loss: 0.2537 - val_acc: 0.9323\nEpoch 10/10\n1415/1415 [==============================] - 76s 54ms/step - loss: 0.2997 - acc: 0.9150 - val_loss: 0.2582 - val_acc: 0.9323\nThe model’s accuracy is 93.23%. Let’s learn how to make predictions and take a look at the confusion matrix.\nMaking predictions\nWe can use thepredict_generator function to make predictions on a new dataset. Let’s make predictions for our validation dataset.\nThe predict_generator function needs a step argument which is the number of times the generator will be called.\nWe can calculate the number of steps by knowing the batch size, and the size of the validation dataset.\n\n\ndf_validation <- df[-id_train,]\nn_steps <- nrow(df_validation)/32 + 1\n\n\nWe can then use the predict_generator function:\n\n\npredictions <- predict_generator(\n  model, \n  ds_validation, \n  steps = n_steps\n  )\nstr(predictions)\n\n\nnum [1:19424, 1:30] 1.22e-13 7.30e-19 5.29e-10 6.66e-22 1.12e-17 ...\nThis will output a matrix with 30 columns - one for each word and n_steps*batch_size number of rows. Note that it starts repeating the dataset at the end to create a full batch.\nWe can compute the predicted class by taking the column with the highest probability, for example.\n\n\nclasses <- apply(predictions, 1, which.max) - 1\n\n\nA nice visualization of the confusion matrix is to create an alluvial diagram:\n\n\nlibrary(dplyr)\nlibrary(alluvial)\nx <- df_validation %>%\n  mutate(pred_class_id = head(classes, nrow(df_validation))) %>%\n  left_join(\n    df_validation %>% distinct(class_id, class) %>% rename(pred_class = class),\n    by = c(\"pred_class_id\" = \"class_id\")\n  ) %>%\n  mutate(correct = pred_class == class) %>%\n  count(pred_class, class, correct)\n\nalluvial(\n  x %>% select(class, pred_class),\n  freq = x$n,\n  col = ifelse(x$correct, \"lightblue\", \"red\"),\n  border = ifelse(x$correct, \"lightblue\", \"red\"),\n  alpha = 0.6,\n  hide = x$n < 20\n)\n\n\nAlluvial PlotWe can see from the diagram that the most relevant mistake our model makes is to classify “tree” as “three”. There are other common errors like classifying “go” as “no”, “up” as “off”. At 93% accuracy for 30 classes, and considering the errors we can say that this model is pretty reasonable.\nThe saved model occupies 25Mb of disk space, which is reasonable for a desktop but may not be on small devices. We could train a smaller model, with fewer layers, and see how much the performance decreases.\nIn speech recognition tasks its also common to do some kind of data augmentation by mixing a background noise to the spoken audio, making it more useful for real applications where it’s common to have other irrelevant sounds happening in the environment.\nThe full code to reproduce this tutorial is available here.\n\n\n\n",
    "preview": "https://upload.wikimedia.org/wikipedia/commons/6/61/FFT-Time-Frequency-View.png",
    "last_modified": "2024-11-21T15:52:21+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-04-02-rstudio-gpu-paperspace/",
    "title": "GPU Workstations in the Cloud with Paperspace",
    "description": "If you don't have local access to a modern NVIDIA GPU, your best bet is typically to run GPU intensive training jobs in the cloud. Paperspace is a cloud service that provides access to a fully preconfigured Ubuntu 16.04 desktop environment equipped with a GPU.",
    "author": [
      {
        "name": "J.J. Allaire",
        "url": "https://github.com/jjallaire"
      }
    ],
    "date": "2018-04-02",
    "categories": [
      "Cloud"
    ],
    "contents": "\nWe are very pleased to announce the availability of an RStudio TensorFlow template for the Paperspace cloud desktop service.\nIf you don’t have local access to a modern NVIDIA GPU, your best bet is typically to run GPU intensive training jobs in the cloud. Paperspace is a cloud service that provides access to a fully preconfigured Ubuntu 16.04 desktop environment equipped with a GPU. With the addition of the RStudio TensorFlow template you can now provision a ready to use RStudio TensorFlow w/ GPU workstation in just a few clicks. Preconfigured software includes:\nRStudio Desktop and RStudio Server\nNVIDIA GPU libraries (CUDA 8.0 and cuDNN 6.0)\nTensorFlow v1.4 w/ GPU\nThe R keras, tfestimators, and tensorflow packages.\nThe tidyverse suite of packages (ggplot2, dplyr, tidyr, readr, etc.)\nGetting Started\nTo get started, first signup for a Paperspace account (you can use the RSTUDIO promo code when you sign up to receive a $5 account credit).\nThen, create a new Paperspace instance using the RStudio template:\n\n\n\nThen, choose one of the Paperspace GPU instances (as opposed to the CPU instances). For example, here we select the P4000 machine type which includes an NVIDIA Quadro P4000 GPU:\n\n\n\nSee the Cloud Desktop GPUs with Paperspace article on the TensorFlow for R website for full details on getting started.\nTraining a Convolutional MNIST Model\nThe performance gains for training convoluational and recurrent models on GPUs can be substantial. Let’s try training the Keras MNIST CNN example on our new Paperspace instance:\n\n\n\nTraining the model for 12 epochs takes about 1 minute (~ 5 seconds per epoch). On the other hand, training the same model on CPU on a high end Macbook Pro takes 15 minutes! (~ 75 seconds per epoch). Using a Paperspace GPU yields a 15x performance gain in model training.\nThis model was trained on an NVIDIA Quadro P4000, which costs $0.40 per hour. Paperspace instances can be configured to automatically shut down after a period of inactivity to prevent accruing cloud charges when you aren’t actually using the machine.\nIf you are training convolutional or recurrent models and don’t currently have access to a local NVIDIA GPU, using RStudio on Paperspace is a great way to accelerate training performance. You can use the RSTUDIO promo code when you sign up for Paperspace to receive a $5 account credit.\n\n\n\n",
    "preview": "posts/2018-04-02-rstudio-gpu-paperspace/images/paperspace-mnist-cnn.png",
    "last_modified": "2024-11-21T15:52:31+00:00",
    "input_file": {},
    "preview_width": 2030,
    "preview_height": 1338
  },
  {
    "path": "posts/2018-03-09-lime-v04-the-kitten-picture-edition/",
    "title": "lime v0.4: The Kitten Picture Edition",
    "description": "A new major release of lime has landed on CRAN. lime is an R port of the Python library of the same name by Marco Ribeiro that allows the user to pry open black box machine learning models and explain their outcomes on a per-observation basis",
    "author": [
      {
        "name": "Thomas Lin Pedersen",
        "url": "https://github.com/thomasp85"
      }
    ],
    "date": "2018-03-09",
    "categories": [
      "Packages/Releases",
      "TensorFlow/Keras",
      "Explainability",
      "Image Recognition & Image Processing"
    ],
    "contents": "\nIntroduction\nI’m happy to report a new major release of lime has landed on CRAN. lime is\nan R port of the Python library of the same name by Marco Ribeiro that allows\nthe user to pry open black box machine learning models and explain their\noutcomes on a per-observation basis. It works by modelling the outcome of the\nblack box in the local neighborhood around the observation to explain and using\nthis local model to explain why (not how) the black box did what it did. For\nmore information about the theory of lime I will direct you to the article\nintroducing the methodology.\nNew features\nThe meat of this release centers around two new features that are somewhat\nlinked: Native support for keras models and support for explaining image models.\nkeras and images\nJ.J. Allaire was kind enough to namedrop lime during his keynote introduction\nof the tensorflow and keras packages and I felt compelled to support them\nnatively. As keras is by far the most popular way to interface with tensorflow\nit is first in line for build-in support. The addition of keras means that\nlime now directly supports models from the following packages:\ncaret\nmlr\nxgboost\nh2o\nkeras\nIf you’re working on something too obscure or cutting edge to not be able to use\nthese packages it is still possible to make your model lime compliant by\nproviding predict_model() and model_type() methods for it.\nkeras models are used just like any other model, by passing it into the lime()\nfunction along with the training data in order to create an explainer object.\nBecause we’re soon going to talk about image models, we’ll be using one of the\npre-trained ImageNet models that is available from keras itself:\n\n\nlibrary(keras)\nlibrary(lime)\nlibrary(magick)\n\nmodel <- application_vgg16(\n  weights = \"imagenet\",\n  include_top = TRUE\n)\nmodel\n\n\nModel\n______________________________________________________________________________________________\nLayer (type)                              Output Shape                         Param #        \n==============================================================================================\ninput_1 (InputLayer)                      (None, 224, 224, 3)                  0              \n______________________________________________________________________________________________\nblock1_conv1 (Conv2D)                     (None, 224, 224, 64)                 1792           \n______________________________________________________________________________________________\nblock1_conv2 (Conv2D)                     (None, 224, 224, 64)                 36928          \n______________________________________________________________________________________________\nblock1_pool (MaxPooling2D)                (None, 112, 112, 64)                 0              \n______________________________________________________________________________________________\nblock2_conv1 (Conv2D)                     (None, 112, 112, 128)                73856          \n______________________________________________________________________________________________\nblock2_conv2 (Conv2D)                     (None, 112, 112, 128)                147584         \n______________________________________________________________________________________________\nblock2_pool (MaxPooling2D)                (None, 56, 56, 128)                  0              \n______________________________________________________________________________________________\nblock3_conv1 (Conv2D)                     (None, 56, 56, 256)                  295168         \n______________________________________________________________________________________________\nblock3_conv2 (Conv2D)                     (None, 56, 56, 256)                  590080         \n______________________________________________________________________________________________\nblock3_conv3 (Conv2D)                     (None, 56, 56, 256)                  590080         \n______________________________________________________________________________________________\nblock3_pool (MaxPooling2D)                (None, 28, 28, 256)                  0              \n______________________________________________________________________________________________\nblock4_conv1 (Conv2D)                     (None, 28, 28, 512)                  1180160        \n______________________________________________________________________________________________\nblock4_conv2 (Conv2D)                     (None, 28, 28, 512)                  2359808        \n______________________________________________________________________________________________\nblock4_conv3 (Conv2D)                     (None, 28, 28, 512)                  2359808        \n______________________________________________________________________________________________\nblock4_pool (MaxPooling2D)                (None, 14, 14, 512)                  0              \n______________________________________________________________________________________________\nblock5_conv1 (Conv2D)                     (None, 14, 14, 512)                  2359808        \n______________________________________________________________________________________________\nblock5_conv2 (Conv2D)                     (None, 14, 14, 512)                  2359808        \n______________________________________________________________________________________________\nblock5_conv3 (Conv2D)                     (None, 14, 14, 512)                  2359808        \n______________________________________________________________________________________________\nblock5_pool (MaxPooling2D)                (None, 7, 7, 512)                    0              \n______________________________________________________________________________________________\nflatten (Flatten)                         (None, 25088)                        0              \n______________________________________________________________________________________________\nfc1 (Dense)                               (None, 4096)                         102764544      \n______________________________________________________________________________________________\nfc2 (Dense)                               (None, 4096)                         16781312       \n______________________________________________________________________________________________\npredictions (Dense)                       (None, 1000)                         4097000        \n==============================================================================================\nTotal params: 138,357,544\nTrainable params: 138,357,544\nNon-trainable params: 0\n______________________________________________________________________________________________\n\nThe vgg16 model is an image classification model that has been build as part of\nthe ImageNet competition where the goal is to classify pictures into 1000\ncategories with the highest accuracy. As we can see it is fairly complicated.\nIn order to create an explainer we will need to pass in the training data as\nwell. For image data the training data is really only used to tell lime that we\nare dealing with an image model, so any image will suffice. The format for the\ntraining data is simply the path to the images, and because the internet runs on\nkitten pictures we’ll use one of these:\n\n\nimg <- image_read('https://www.data-imaginist.com/assets/images/kitten.jpg')\nimg_path <- file.path(tempdir(), 'kitten.jpg')\nimage_write(img, img_path)\nplot(as.raster(img))\n\n\n\nAs with text models the explainer will need to know how to prepare the input\ndata for the model. For keras models this means formatting the image data as\ntensors. Thankfully keras comes with a lot of tools for reshaping image data:\n\n\nimage_prep <- function(x) {\n  arrays <- lapply(x, function(path) {\n    img <- image_load(path, target_size = c(224,224))\n    x <- image_to_array(img)\n    x <- array_reshape(x, c(1, dim(x)))\n    x <- imagenet_preprocess_input(x)\n  })\n  do.call(abind::abind, c(arrays, list(along = 1)))\n}\nexplainer <- lime(img_path, model, image_prep)\n\n\nWe now have an explainer model for understanding how the vgg16 neural network\nmakes its predictions. Before we go along, lets see what the model think of our\nkitten:\n\n\nres <- predict(model, image_prep(img_path))\nimagenet_decode_predictions(res)\n\n\n[[1]]\n  class_name class_description      score\n1  n02124075      Egyptian_cat 0.48913878\n2  n02123045             tabby 0.15177219\n3  n02123159         tiger_cat 0.10270492\n4  n02127052              lynx 0.02638111\n5  n03793489             mouse 0.00852214\nSo, it is pretty sure about the whole cat thing. The reason we need to use\nimagenet_decode_predictions() is that the output of a keras model is always\njust a nameless tensor:\n\n\ndim(res)\n\n\n[1]    1 1000\n\n\ndimnames(res)\n\n\nNULL\nWe are used to classifiers knowing the class labels, but this is not the case\nfor keras. Motivated by this, lime now have a way to define/overwrite the\nclass labels of a model, using the as_classifier() function. Let’s redo our\nexplainer:\n\n\nmodel_labels <- readRDS(system.file('extdata', 'imagenet_labels.rds', package = 'lime'))\nexplainer <- lime(img_path, as_classifier(model, model_labels), image_prep)\n\n\n\nThere is also an as_regressor() function which tells lime, without a doubt,\nthat the model is a regression model. Most models can be introspected to see\nwhich type of model they are, but neural networks doesn’t really care. lime\nguesses the model type from the activation used in the last layer (linear\nactivation == regression), but if that heuristic fails then\nas_regressor()/as_classifier() can be used.\n\nWe are now ready to poke into the model and find out what makes it think our\nimage is of an Egyptian cat. But… first I’ll have to talk about yet another\nconcept: superpixels (I promise I’ll get to the explanation part in a bit).\nIn order to create meaningful permutations of our image (remember, this is the\ncentral idea in lime), we have to define how to do so. The permutations needs\nto be substantial enough to have an impact on the image, but not so much that\nthe model completely fails to recognise the content in every case - further,\nthey should lead to an interpretable result. The concept of superpixels lends\nitself well to these constraints. In short, a superpixel is a patch of an area\nwith high homogeneity, and superpixel segmentation is a clustering of image\npixels into a number of superpixels. By segmenting the image to explain into\nsuperpixels we can turn area of contextual similarity on and off during the\npermutations and find out if that area is important. It is still necessary to\nexperiment a bit as the optimal number of superpixels depend on the content of\nthe image. Remember, we need them to be large enough to have an impact but not\nso large that the class probability becomes effectively binary. lime comes\nwith a function to assess the superpixel segmentation before beginning the\nexplanation and it is recommended to play with it a bit — with time you’ll\nlikely get a feel for the right values:\n\n\n# default\nplot_superpixels(img_path)\n\n\n\n\n\n# Changing some settings\nplot_superpixels(img_path, n_superpixels = 200, weight = 40)\n\n\n\nThe default is set to a pretty low number of superpixels — if the subject of\ninterest is relatively small it may be necessary to increase the number of\nsuperpixels so that the full subject does not end up in one, or a few\nsuperpixels. The weight parameter will allow you to make the segments more\ncompact by weighting spatial distance higher than colour distance. For this\nexample we’ll stick with the defaults.\nBe aware that explaining image\nmodels is much heavier than tabular or text data. In effect it will create 1000\nnew images per explanation (default permutation size for images) and run these\nthrough the model. As image classification models are often quite heavy, this\nwill result in computation time measured in minutes. The permutation is batched\n(default to 10 permutations per batch), so you should not be afraid of running\nout of RAM or hard-drive space.\n\n\nexplanation <- explain(img_path, explainer, n_labels = 2, n_features = 20)\n\n\nThe output of an image explanation is a data frame of the same format as that\nfrom tabular and text data. Each feature will be a superpixel and the pixel\nrange of the superpixel will be used as its description. Usually the explanation\nwill only make sense in the context of the image itself, so the new version of\nlime also comes with a plot_image_explanation() function to do just that.\nLet’s see what our explanation have to tell us:\n\n\nplot_image_explanation(explanation)\n\n\n\nWe can see that the model, for both the major predicted classes, focuses on the\ncat, which is nice since they are both different cat breeds. The plot function\ngot a few different functions to help you tweak the visual, and it filters low\nscoring superpixels away by default. An alternative view that puts more focus\non the relevant superpixels, but removes the context can be seen by using\ndisplay = 'block':\n\n\nplot_image_explanation(explanation, display = 'block', threshold = 0.01)\n\n\n\nWhile not as common with image explanations it is also possible to look at the\nareas of an image that contradicts the class:\n\n\nplot_image_explanation(explanation, threshold = 0, show_negative = TRUE, fill_alpha = 0.6)\n\n\n\nAs each explanation takes longer time to create and needs to be tweaked on a\nper-image basis, image explanations are not something that you’ll create in\nlarge batches as you might do with tabular and text data. Still, a few\nexplanations might allow you to understand your model better and be used for\ncommunicating the workings of your model. Further, as the time-limiting factor\nin image explanations are the image classifier and not lime itself, it is bound\nto improve as image classifiers becomes more performant.\nGrab back\nApart from keras and image support, a slew of other features and improvements\nhave been added. Here’s a quick overview:\nAll explanation plots now include the fit of the ridge regression used to make\nthe explanation. This makes it easy to assess how good the assumptions about\nlocal linearity are kept.\nWhen explaining tabular data the default distance measure is now 'gower'\nfrom the gower package. gower makes it possible to measure distances\nbetween heterogeneous data without converting all features to numeric and\nexperimenting with different exponential kernels.\nWhen explaining tabular data numerical features will no longer be sampled from\na normal distribution during permutations, but from a kernel density defined\nby the training data. This should ensure that the permutations are more\nrepresentative of the expected input.\nWrapping up\nThis release represents an important milestone for lime in R. With the\naddition of image explanations the lime package is now on par or above its\nPython relative, feature-wise. Further development will focus on improving the\nperformance of the model, e.g. by adding parallelisation or improving the local\nmodel definition, as well as exploring alternative explanation types such as\nanchor.\nHappy Explaining!\n\n\n\n",
    "preview": "posts/2018-03-09-lime-v04-the-kitten-picture-edition/images/unnamed-chunk-8-1.png",
    "last_modified": "2024-11-21T15:50:34+00:00",
    "input_file": {},
    "preview_width": 1344,
    "preview_height": 672
  },
  {
    "path": "posts/2018-01-29-dl-for-cancer-immunotherapy/",
    "title": "Deep Learning for Cancer Immunotherapy",
    "description": "The aim of this post is to illustrate how deep learning is being applied in cancer immunotherapy (Immuno-oncology or Immunooncology) - a cancer treatment strategy, where the aim is to utilize the cancer patient's own immune system to fight the cancer.",
    "author": [
      {
        "name": "Leon Eyrich Jessen",
        "url": "https://twitter.com/jessenleon"
      }
    ],
    "date": "2018-01-29",
    "categories": [
      "TensorFlow/Keras",
      "Tabular Data"
    ],
    "contents": "\nIntroduction\nIn my research, I apply deep learning to unravel molecular interactions in the human immune system. One application of my research is within cancer immunotherapy (Immuno-oncology or Immunooncology) - a cancer treatment strategy, where the aim is to utilize the cancer patient’s own immune system to fight the cancer.\nThe aim of this post is to illustrates how deep learning is successfully being applied to model key molecular interactions in the human immune system. Molecular interactions are highly context dependent and therefore non-linear. Deep learning is a powerful tool to capture non-linearity and has therefore proven invaluable and highly successful. In particular in modelling the molecular interaction between the Major Histocompability Complex type I (MHCI) and peptides (The state-of-the-art model netMHCpan identifies 96.5% of natural peptides at a very high specificity of 98.5%).\nAdoptive T-cell therapy\nSome brief background before diving in. Special immune cells (T-cells) patrol our body, scanning the cells to check if they are healthy. On the surface of our cells is the MHCI - a highly specialized molecular system, which reflects the health status inside our cells. This is done by displaying small fragments of proteins called peptides, thus reflecting the inside of the cell. T-cells probe these molecular displays to check if the peptides are from our own body (self) or foreign (non-self), e.g. from a virus infection or cancer. If a displayed peptide is non-self, the T-cells has the power to terminate the cell.\nSimon Caulton, Adoptive T-cell therapy, CC BY-SA 3.0\nAdoptive T-cell therapy is a form of cancer immunotherapy that aims to isolate tumor infiltrating T-cells from the tumor in the patient, possibly genetically engineer them to be cancer-specific, grow them in great numbers and reintroduce them into the body to fight the cancer. In order to terminate cancer cells, the T-cell needs to be activated by being exposed to tumor peptides bound to MHCI (pMHCI). By analyzing the tumor genetics, relevant peptides can be identified and depending on the patients particular type of MHCI, we can predict which pMHCI are likely to be present in the tumor in the patient and thus which pMHCIs should be used to activate the T-cells.\nPeptide Classification Model\nFor this use case, we applied three models to classify whether a given peptide is a ‘strong binder’ SB, ‘weak binder’ WB or ‘non-binder’ NB. to MHCI (Specific type: HLA-A*02:01). Thereby, the classification uncovers which peptides, will be presented to the T-cells. The models we tested were:\nA deep feed forward fully connected ANN\nA convolutional ANN (connected to a FFN)\nA random forest (for comparison)\nNext, we’ll dive into building the artificial neural network. If you want to a more detailed explanation of cancer immunotherapy and how it interacts with the human immune system before going further, see the primer on cancer immunotherapy at the end of the post.\nPrerequisites\nThis example utilizes the keras package, several tidyverse packages, as well as the ggseqlogo and PepTools packages. You can install these packages as follows:\n\n\n# Keras + TensorFlow and it's dependencies\ninstall.packages(\"keras\")\nlibrary(keras)\ninstall_keras()\n\n# Tidyverse (readr, ggplot2, etc.)\ninstall.packages(\"tidyverse\")\n\n# Packages for sequence logos and peptides\ndevtools::install_github(\"omarwagih/ggseqlogo\")\ndevtools::install_github(\"leonjessen/PepTools\")\n\n\nWe can now load all of the packages we need for this example:\n\n\nlibrary(keras)\nlibrary(tidyverse)\nlibrary(PepTools)\n\n\nPeptide Data\nThe input data for this use case was created by generating 1,000,000 random 9-mer peptides by sampling the one-letter code for the 20 amino acids, i.e. ARNDCQEGHILKMFPSTWYV, and then submitting the peptides to MHCI binding prediction using the current state-of-the-art model netMHCpan. Different variants of MHCI exists, so for this case we chose HLA-A*02:01. This method assigns ‘strong binder’ SB, ‘weak binder’ WB or ‘non-binder’ NB to each peptide.\nSince n(SB) < n(WB) << n(NB), the data was subsequently balanced by down sampling, such that n(SB) = n(WB) = n(NB) = 7,920. Thus, a data set with a total of 23,760 data points was created. 10% of the data points were randomly assigned as test data and the remainder as train data. It should be noted that since the data set originates from a model, the outcome of this particular use case will be a model of a model. However, netMHCpan is very accurate (96.5% of natural ligands are identified at a very high specificity 98.5%).\nIn the following each peptide will be encoded by assigning a vector of 20 values, where each value is the probability of the amino acid mutating into 1 of the 20 others as defined by the BLOSUM62 matrix using the pep_encode() function from the PepTools package. This way each peptide is converted to an ‘image’ matrix with 9 rows and 20 columns.\nLet’s load the data:\n\n\npep_file <- get_file(\n  \"ran_peps_netMHCpan40_predicted_A0201_reduced_cleaned_balanced.tsv\", \n  origin = \"https://git.io/vb3Xa\"\n) \npep_dat <- read_tsv(file = pep_file)\n\n\nThe example peptide data looks like this:\n\n\npep_dat %>% head(5)\n\n\n# A tibble: 5 x 4\n  peptide   label_chr label_num data_type\n  <chr>     <chr>         <int> <chr>    \n1 LLTDAQRIV WB                1 train    \n2 LMAFYLYEV SB                2 train    \n3 VMSPITLPT WB                1 test     \n4 SLHLTNCFV WB                1 train    \n5 RQFTCMIAV WB                1 train   \nWhere peptide is the 9-mer peptides, label_chr defines whether the peptide was predicted by netMHCpan to be a strong-binder SB, weak-binder WB or NB non-binder to HLA-A*02:01.\nlabel_num is equivalent to label_chr, such that NB = 0, WB = 1 and SB = 2. Finally data_type defines whether the particular data point is part of the train set used to build the model or the ~10% data left out test set, which will be used for final performance evaluation.\nThe data has been balanced, as shown in this summary:\n\n\npep_dat %>% group_by(label_chr, data_type) %>% summarise(n = n())\n\n\n# A tibble: 6 x 3\n# Groups:   label_chr [?]\n  label_chr data_type     n\n  <chr>     <chr>     <int>\n1 NB        test        782\n2 NB        train      7138\n3 SB        test        802\n4 SB        train      7118\n5 WB        test        792\n6 WB        train      7128\nWe can use the ggseqlogo package to visualize the sequence motif for the strong binders using a sequence logo. This allows us to see which positions in the peptide and which amino acids are critical for the binding to MHC (Higher letters indicate more importance):\n\n\npep_dat %>% filter(label_chr=='SB') %>% pull(peptide) %>% ggseqlogo()\n\n\n\n\n\nFrom the sequence logo, it is evident, that L,M,I,V are found often at p2 and p9 amongst the strong binders. In fact these position are referred to as the anchor positions, which interact with the MHCI. The T-cell on the other hand, will recognize p3-p8.\nData Preparation\nWe are creating a model f, where x is the peptide and y is one of three classes SB, WB and NB, such that f(x) = y. Each x is encoded into a 2-dimensional ‘image’, which we can visualize using the pep_plot_images() function:\n\n\npep_dat %>% filter(label_chr=='SB') %>% head(1) %>% pull(peptide) %>% pep_plot_images\n\n\n\n\n\nTo feed data into a neural network we need to encode it as a multi-dimensional array (or “tensor”). For this dataset we can do this with the PepTools::pep_encode() function, which takes a character vector of peptides and transforms them into a 3D array of ‘total number of peptides’ x ‘length of each peptide (9)’ x ‘number of unique amino acids (20)’. For example:\n\n\nstr(pep_encode(c(\"LLTDAQRIV\", \"LLTDAQRIV\")))\n\n\n num [1:2, 1:9, 1:20] 0.0445 0.0445 0.0445 0.0445 0.073 ...\nHere’s how we transform the data frame into 3-D arrays of training and test data:\n\n\nx_train <- pep_dat %>% filter(data_type == 'train') %>% pull(peptide)   %>% pep_encode\ny_train <- pep_dat %>% filter(data_type == 'train') %>% pull(label_num) %>% array\nx_test  <- pep_dat %>% filter(data_type == 'test')  %>% pull(peptide)   %>% pep_encode\ny_test  <- pep_dat %>% filter(data_type == 'test')  %>% pull(label_num) %>% array\n\n\nTo prepare the data for training we convert the 3-d arrays into matrices by reshaping width and height into a single dimension (9x20 peptide ‘images’ are flattened into vectors of lengths 180):\n\n\nx_train <- array_reshape(x_train, c(nrow(x_train), 9, 20, 1))\nx_test  <- array_reshape(x_test, c(nrow(x_test), 9, 20, 1))\n\n\nThe y data is an integer vector with values ranging from 0 to 2. To prepare this data for training we one-hot encode the vectors into binary class matrices using the Keras to_categorical function:\n\n\ny_train <- to_categorical(y_train, num_classes = 3)\ny_test  <- to_categorical(y_test,  num_classes = 3)\n\n\nDefining the Model\nThe core data structure of Keras is a model, a way to organize layers. The simplest type of model is the sequential model, a linear stack of layers. We begin by creating a sequential model and then adding layers using the pipe (%>%) operator:\n\n\nmodel <- keras_model_sequential() %>% \n  layer_dense(units  = 180, activation = 'relu', input_shape = 180) %>% \n  layer_dropout(rate = 0.4) %>% \n  layer_dense(units  = 90, activation  = 'relu') %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units  = 3, activation   = 'softmax')\n\n\nA dense layer is a standard neural network layer with each input node is connected to an output node. A dropout layer sets a random proportion of activations from the previous layer to 0, which helps to prevent overfitting.\nThe input_shape argument to the first layer specifies the shape of the input data (a length 180 numeric vector representing a peptide ‘image’). The final layer outputs a length 3 numeric vector (probabilities for each class SB, WB and NB) using a softmax activation function.\nWe can use the summary() function to print the details of the model:\n\n\nsummary(model)\n\n\nLayer (type)                        Output Shape                    Param #     \n================================================================================\ndense_1 (Dense)                     (None, 180)                     32580       \n________________________________________________________________________________\ndropout_1 (Dropout)                 (None, 180)                     0           \n________________________________________________________________________________\ndense_2 (Dense)                     (None, 90)                      16290       \n________________________________________________________________________________\ndropout_2 (Dropout)                 (None, 90)                      0           \n________________________________________________________________________________\ndense_3 (Dense)                     (None, 3)                       273         \n================================================================================\nTotal params: 49,143\nTrainable params: 49,143\nNon-trainable params: 0\n________________________________________________________________________________\nNext, we compile the model with appropriate loss function, optimizer, and metrics:\n\n\nmodel %>% compile(\n  loss      = 'categorical_crossentropy',\n  optimizer = optimizer_rmsprop(),\n  metrics   = c('accuracy')\n)\n\n\nTraining and Evaluation\nWe use the fit() function to train the model for 150 epochs using batches of 50 peptide ‘images’:\n\n\nhistory = model %>% fit(\n  x_train, y_train, \n  epochs = 150, \n  batch_size = 50, \n  validation_split = 0.2\n)\n\n\nWe can visualize the training progress by plotting the history object returned from fit():\n\n\nplot(history)\n\n\n\n\n\nWe can now evaluate the model’s performance on the original ~10% left out test data:\n\n\nperf = model %>% evaluate(x_test, y_test)\nperf\n\n\n$loss\n[1] 0.2449334\n\n$acc\n[1] 0.9461279\nWe can also visualize the predictions on the test data:\n\n\nacc     = perf$acc %>% round(3)*100\ny_pred  = model %>% predict_classes(x_test)\ny_real  = y_test %>% apply(1,function(x){ return( which(x==1) - 1) })\nresults = tibble(y_real = y_real %>% factor, y_pred = y_pred %>% factor,\n                 Correct = ifelse(y_real == y_pred,\"yes\",\"no\") %>% factor)\ntitle = 'Performance on 10% unseen data - Feed Forward Neural Network'\nxlab  = 'Measured (Real class, as predicted by netMHCpan-4.0)'\nylab  = 'Predicted (Class assigned by Keras/TensorFlow deep FFN)'\nresults %>%\n  ggplot(aes(x = y_pred, y = y_real, colour = Correct)) +\n  geom_point() +\n  ggtitle(label = title, subtitle = paste0(\"Accuracy = \", acc,\"%\")) +\n  xlab(xlab) +\n  ylab(ylab) +\n  scale_color_manual(labels = c('No', 'Yes'),\n                     values = c('tomato','cornflowerblue')) +\n  geom_jitter() +\n  theme_bw()\n\n\n\n\n\nThe final result was a performance on the 10% unseen data of just short of 95% accuracy.\nConvolutional Neural Network\nIn order to test a more complex architecture, we also implemented a Convolutional Neural Network. To make the comparison, we repeated the data preparation as described above and only changed the architecture by including a single 2d convolutional layer and then feeding that into the same architecture as the FFN above:\n\n\nmodel <- keras_model_sequential() %>%\n  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',\n                input_shape = c(9, 20, 1)) %>%\n  layer_dropout(rate = 0.25) %>% \n  layer_flatten() %>% \n  layer_dense(units  = 180, activation = 'relu') %>% \n  layer_dropout(rate = 0.4) %>% \n  layer_dense(units  = 90, activation  = 'relu') %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units  = 3, activation   = 'softmax')\n\n\n\n\n\nThis resulted in a performance on the 10% unseen data of 92% accuracy.\nOne might have expected the CNN to be able to better capture the information in the peptide ‘images’. There is however a crucial difference between the peptide ‘images’ and the e.g. MNIST dataset. The peptide ‘images’ do not contain edges and spatially arranged continuous structures, rather they are a set of pixels with p2 always at p2 and likewise for p9, which are determinants for binding.\nRandom Forest\nKnowing that deep ;earning is not necessarily the right tool for all prediction tasks, we also created a random forest model on the exact same data using the randomForest package.\nThe x and y training data was prepared slightly different using PepTools::pep_encode_mat\n\n\n# Setup training data\ntarget  <- 'train'\nx_train <- pep_dat %>% filter(data_type==target) %>% pull(peptide) %>%\n  pep_encode_mat %>% select(-peptide)\ny_train <- pep_dat %>% filter(data_type==target) %>% pull(label_num) %>% factor\n\n# Setup test data\ntarget <- 'test'\nx_test <- pep_dat %>% filter(data_type==target) %>% pull(peptide) %>%\n  pep_encode_mat %>% select(-peptide)\ny_test <- pep_dat %>% filter(data_type==target) %>% pull(label_num) %>% factor\n\n\nThe random forest model was then run using 100 trees like so:\n\n\nrf_classifier <- randomForest(x = x_train, y = y_train, ntree = 100)\n\n\nThe results of the model were collected as follows:\n\n\ny_pred    <- predict(rf_classifier, x_test)\nn_correct <- table(observed = y_test, predicted = y_pred) %>% diag %>% sum\nacc       <- (n_correct / length(y_test)) %>% round(3) * 100\nresults   <- tibble(y_real  = y_test,\n                   y_pred  = y_pred,\n                   Correct = ifelse(y_real == y_pred,\"yes\",\"no\") %>% factor)\n\n\nWe can then visualize the performance as we did with the FFN and the CNN:\n\n\ntitle = \"Performance on 10% unseen data - Random Forest\"\nxlab  = \"Measured (Real class, as predicted by netMHCpan-4.0)\"\nylab  = \"Predicted (Class assigned by random forest)\"\nf_out = \"plots/03_rf_01_results_3_by_3_confusion_matrix.png\"\nresults %>%\n  ggplot(aes(x = y_pred, y = y_real, colour = Correct)) +\n  geom_point() +\n  xlab(xlab) +\n  ylab(ylab) +\n  ggtitle(label = title, subtitle = paste0(\"Accuracy = \", acc,\"%\")) +\n  scale_color_manual(labels = c('No', 'Yes'),\n                     values = c('tomato','cornflowerblue')) +\n  geom_jitter() +\n  theme_bw()\n\n\n\n\n\nConclusion\nIn this post you have been shown how we build 3 models: A Feed Forward Neural Network (FFN), a Convolutional Neural Network (CNN) and a Random Forest (RF). Using the same data, we obtained performances of ~95%, ~92% and ~82% for the FFN, CNN and RF respectively. The R-code for these models are available here:\nFeed Forward Neural Network\nConvolutional Neural Network\nRandom Forest\nIt is evident that the deep learning models capture the information in the system much better than the random forest model. However, the CNN model didn’t not perform as well as the straightforward FFN. This illustrates one of the pitfalls of deep learning - blind alleys. There are a huge number of architectures available, and when combined with hyperparameter tuning the potential model space is breathtakingly large.\nTo increase the likelihood of finding a good architecture and the right hyper-parameters it is important to know and understand the data you are modeling. Also, if possible include several sources of data. For the case of peptide-MHC interaction, we include not only information of the strength of the binding as measured in the laboratory, but also information from actual human cells, where peptide-MHC complexes are extracted and analysed.\nIt should be noted that when we build models in the research group, a lot of work goes into creating balanced training and test sets. Models are also trained and evaluated using cross-validation, usually 5-fold. We then save each of the five models and create an ensemble prediction - wisdom-of-the-crowd. We are very careful to avoiding overfitting as this of course decreases the models extrapolation performance.\nThere is no doubt that deep learning already plays a major role in unraveling the complexities of the human immune system and associated diseases. With the release of TensorFlow by Google along with the keras and tensorflow R packages we now have the tools available in R to explore this frontier.\nPrimer on Cancer Immunotherapy\nHere is an elaborated background on DNA, proteins and cancer 1. However, brief and simplified as this is naturally a hugely complex subject.\nDNA\nThe cell is the basic unit of life. Each cell in our body harbors ~2 meters (6 feet) of DNA, which is identical across all cells. DNA makes up the blue print for our body - our genetic code - using only four nucleic acids (hence the name DNA = DeoxyriboNucleic Acid). We can represent the genetic code, using: a,c,g and t. Each cell carries ~3,200,000,000 of these letters, which constitute the blue print for our entire body. The letters are organised into ~20,000 genes and from the genes we get proteins. In Bioinformatics, we represent DNA sequences as repeats of the four nucleotides, e.g. ctccgacgaatttcatgttcagggatagct....\nProteins\nComparing with a building - if DNA is the blue print of how to construct a building, then the proteins are the bricks, windows, chimney, plumbing etc. Some proteins are structural (like a brick), whereas others are functional (like a window you can open and close). All ~100,000 proteins in our body are made by of only 20 small molecules called amino acids. Like with DNA, we can represent these 20 amino acids using: A,R,N,D,C,Q,E,G,H,I,L,K,M,F,P,S,T,W,Y and V (note lowercase for DNA and uppercase for amino acids). The average size of a protein in the human body ~300 amino acids and the sequence is the combination of the 20 amino acids making up the protein written consecutively, e.g.: MRYEMGYWTAFRRDCRCTKSVPSQWEAADN.... The attentive reader will notice, that I mentioned ~20,000 genes, from which we get ~100,000 proteins. This is due to the DNA in one gene being able to join in different ways and thus produce more than one protein.\nPeptides\nA peptide is a small fragment of a protein of length ~5-15 amino acids. MHCI predominantly binds peptides containing 9 amino acids - A so called 9-mer. Peptides play a crucial role in the monitoring of cells in our body by the human immune system. The data used in this use case consist solely of 9-mers.\nThe Human Immune System\nInside each cell, proteins are constantly being produced from DNA. In order not to clutter the cell, proteins are also constantly broken down into peptides which are then recycled to produce new proteins. Some of these peptides are caught by a system and bound to MHCI (Major Histocompatibility Complex type 1, MHCI) and transported from inside of the cell to the outside, where the peptide is displayed. The viewer of this display is the human immune system. Special immune cells (T-cells) patrol the body, looking for cells displaying unexpected peptides. If a displayed peptide is unexpected, the T-cells will terminate the cell. The T-cells have been educated to recognize foreign peptides (non-self) and ignore peptides which originate from our own body (self). This is the hallmark of the immune system - Protecting us by distinguishing self from non-self. I the immune system is not active enough and thus fails to recognize non-self arising from an infection it is potentially fatal. On the other hand if the immune system is too active and starts recognizing not only non-self, but also self, you get autoimmune disease, which likewise is potentially fatal.\nCancer\nCancer arises when errors (mutations) occur inside the cell, resulting in changed proteins. This means that if the original protein was e.g. MRYEMGYWTAFRRDCRCTKSVPSQWEAADN..., then the new erroneous protein could be e.g. MRYEMGYWTAFRRDCRCTKSVPSQWEAADR.... The result of this is that the peptide displayed on the cell surface is altered. The T-cells will now recognize the peptide as unexpected and terminate the cell. However, the environment around a cancer tumor is very hostile to the T-cells, which are supposed to recognize and terminate the cell.\nCancer Immunotherapy aims at taking a sample of the tumor and isolate the T-cells, grow them in great numbers and then reintroduce them into the body. Now, despite the hostile environment around the tumor, sheer numbers result in the T-cells out competing the tumor. A special branch of cancer immunotherapy aims at introducing T-cells, which have been specially engineered to recognize a tumor. However, in this case it is of utmost importance to ensure that the T-cell does indeed recognize the tumor and nothing else than the tumor. If introduced T-cells recognize healthy tissue, the outcome can be fatal. It is therefore extremely important to understand the molecular interaction between the sick cell, i.e. the peptide and the MHCI, and the T-cell.\nOur peptide classification model illustrates how deep learning is being applied to increase our understanding of the molecular interactions governing the activation of the T-cells.\n\nInside Life Science, Genetics by the Numbers: https://publications.nigms.nih.gov/insidelifescience/genetics-numbers.html↩︎\n",
    "preview": "posts/2018-01-29-dl-for-cancer-immunotherapy/images/01_ffn_02_results_3_by_3_confusion_matrix.png",
    "last_modified": "2024-11-21T15:50:15+00:00",
    "input_file": {},
    "preview_width": 3000,
    "preview_height": 1800
  },
  {
    "path": "posts/2018-01-24-keras-fraud-autoencoder/",
    "title": "Predicting Fraud with Autoencoders and Keras",
    "description": "In this post we will train an autoencoder to detect credit card fraud. We will also demonstrate how to train Keras models in the cloud using CloudML. The basis of our model will be the Kaggle Credit Card Fraud Detection dataset.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": "https://github.com/dfalbel"
      }
    ],
    "date": "2018-01-25",
    "categories": [
      "TensorFlow/Keras",
      "Unsupervised Learning",
      "Cloud"
    ],
    "contents": "\nOverview\nIn this post we will train an autoencoder to detect credit card fraud. We will also demonstrate how to train Keras models in the cloud using CloudML.\nThe basis of our model will be the Kaggle Credit Card Fraud Detection dataset, which was collected during a research collaboration of Worldline and the Machine Learning Group of ULB (Université Libre de Bruxelles) on big data mining and fraud detection.\nThe dataset contains credit card transactions by European cardholders made over a two day period in September 2013. There are 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for only 0.172% of all transactions.\nReading the data\nAfter downloading the data from Kaggle, you can read it in to R with read_csv():\n\n\nlibrary(readr)\ndf <- read_csv(\"data-raw/creditcard.csv\", col_types = list(Time = col_number()))\n\n\nThe input variables consist of only numerical values which are the result of a PCA transformation. In order to preserve confidentiality, no more information about the original features was provided. The features V1, …, V28 were obtained with PCA. There are however 2 features (Time and Amount) that were not transformed.\nTime is the seconds elapsed between each transaction and the first transaction in the dataset. Amount is the transaction amount and could be used for cost-sensitive learning. The Class variable takes value 1 in case of fraud and 0 otherwise.\nAutoencoders\nSince only 0.172% of the observations are frauds, we have a highly unbalanced classification problem. With this kind of problem, traditional classification approaches usually don’t work very well because we have only a very small sample of the rarer class.\nAn autoencoder is a neural network that is used to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. For this problem we will train an autoencoder to encode non-fraud observations from our training set. Since frauds are supposed to have a different distribution then normal transactions, we expect that our autoencoder will have higher reconstruction errors on frauds then on normal transactions. This means that we can use the reconstruction error as a quantity that indicates if a transaction is fraudulent or not.\nIf you want to learn more about autoencoders, a good starting point is this video from Larochelle on YouTube and Chapter 14 from the Deep Learning book by Goodfellow et al.\nVisualization\nFor an autoencoder to work well we have a strong initial assumption: that the distribution of variables for normal transactions is different from the distribution for fraudulent ones. Let’s make some plots to verify this. Variables were transformed to a [0,1] interval for plotting.\n\n\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ggridges)\ndf %>%\n  gather(variable, value, -Class) %>%\n  ggplot(aes(y = as.factor(variable), \n             fill = as.factor(Class), \n             x = percent_rank(value))) +\n  geom_density_ridges()\n\n\n\nWe can see that distributions of variables for fraudulent transactions are very different then from normal ones, except for the Time variable, which seems to have the exact same distribution.\nPreprocessing\nBefore the modeling steps we need to do some preprocessing. We will split the dataset into train and test sets and then we will Min-max normalize our data (this is done because neural networks work much better with small input values). We will also remove the Time variable as it has the exact same distribution for normal and fraudulent transactions.\nBased on the Time variable we will use the first 200,000 observations for training and the rest for testing. This is good practice because when using the model we want to predict future frauds based on transactions that happened before.\n\n\ndf_train <- df %>% filter(row_number(Time) <= 200000) %>% select(-Time)\ndf_test <- df %>% filter(row_number(Time) > 200000) %>% select(-Time)\n\n\nNow let’s work on normalization of inputs. We created 2 functions to help us. The first one gets descriptive statistics about the dataset that are used for scaling. Then we have a function to perform the min-max scaling. It’s important to note that we applied the same normalization constants for training and test sets.\n\n\nlibrary(purrr)\n\n#' Gets descriptive statistics for every variable in the dataset.\nget_desc <- function(x) {\n  map(x, ~list(\n    min = min(.x),\n    max = max(.x),\n    mean = mean(.x),\n    sd = sd(.x)\n  ))\n} \n\n#' Given a dataset and normalization constants it will create a min-max normalized\n#' version of the dataset.\nnormalization_minmax <- function(x, desc) {\n  map2_dfc(x, desc, ~(.x - .y$min)/(.y$max - .y$min))\n}\n\n\nNow let’s create normalized versions of our datasets. We also transformed our data frames to matrices since this is the format expected by Keras.\n\n\ndesc <- df_train %>% \n  select(-Class) %>% \n  get_desc()\n\nx_train <- df_train %>%\n  select(-Class) %>%\n  normalization_minmax(desc) %>%\n  as.matrix()\n\nx_test <- df_test %>%\n  select(-Class) %>%\n  normalization_minmax(desc) %>%\n  as.matrix()\n\ny_train <- df_train$Class\ny_test <- df_test$Class\n\n\nModel definition\nWe will now define our model in Keras, a symmetric autoencoder with 4 dense layers.\n\n\nlibrary(keras)\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_dense(units = 15, activation = \"tanh\", input_shape = ncol(x_train)) %>%\n  layer_dense(units = 10, activation = \"tanh\") %>%\n  layer_dense(units = 15, activation = \"tanh\") %>%\n  layer_dense(units = ncol(x_train))\n\nsummary(model)\n\n\n___________________________________________________________________________________\nLayer (type)                         Output Shape                     Param #      \n===================================================================================\ndense_1 (Dense)                      (None, 15)                       450          \n___________________________________________________________________________________\ndense_2 (Dense)                      (None, 10)                       160          \n___________________________________________________________________________________\ndense_3 (Dense)                      (None, 15)                       165          \n___________________________________________________________________________________\ndense_4 (Dense)                      (None, 29)                       464          \n===================================================================================\nTotal params: 1,239\nTrainable params: 1,239\nNon-trainable params: 0\n___________________________________________________________________________________\nWe will then compile our model, using the mean squared error loss and the Adam optimizer for training.\n\n\nmodel %>% compile(\n  loss = \"mean_squared_error\", \n  optimizer = \"adam\"\n)\n\n\nTraining the model\nWe can now train our model using the fit() function. Training the model is reasonably fast (~ 14s per epoch on my laptop). We will only feed to our model the observations of normal (non-fraudulent) transactions.\nWe will use callback_model_checkpoint() in order to save our model after each epoch. By passing the argument save_best_only = TRUE we will keep on disk only the epoch with smallest loss value on the test set.\nWe will also use callback_early_stopping() to stop training if the validation loss stops decreasing for 5 epochs.\n\n\ncheckpoint <- callback_model_checkpoint(\n  filepath = \"model.hdf5\", \n  save_best_only = TRUE, \n  period = 1,\n  verbose = 1\n)\n\nearly_stopping <- callback_early_stopping(patience = 5)\n\nmodel %>% fit(\n  x = x_train[y_train == 0,], \n  y = x_train[y_train == 0,], \n  epochs = 100, \n  batch_size = 32,\n  validation_data = list(x_test[y_test == 0,], x_test[y_test == 0,]), \n  callbacks = list(checkpoint, early_stopping)\n)\n\n\nTrain on 199615 samples, validate on 84700 samples\nEpoch 1/100\n199615/199615 [==============================] - 17s 83us/step - loss: 0.0036 - val_loss: 6.8522e-04d from inf to 0.00069, saving model to model.hdf5\nEpoch 2/100\n199615/199615 [==============================] - 17s 86us/step - loss: 4.7817e-04 - val_loss: 4.7266e-04d from 0.00069 to 0.00047, saving model to model.hdf5\nEpoch 3/100\n199615/199615 [==============================] - 19s 94us/step - loss: 3.7753e-04 - val_loss: 4.2430e-04d from 0.00047 to 0.00042, saving model to model.hdf5\nEpoch 4/100\n199615/199615 [==============================] - 19s 94us/step - loss: 3.3937e-04 - val_loss: 4.0299e-04d from 0.00042 to 0.00040, saving model to model.hdf5\nEpoch 5/100\n199615/199615 [==============================] - 19s 94us/step - loss: 3.2259e-04 - val_loss: 4.0852e-04 improve\nEpoch 6/100\n199615/199615 [==============================] - 18s 91us/step - loss: 3.1668e-04 - val_loss: 4.0746e-04 improve\n...\nAfter training we can get the final loss for the test set by using the evaluate() fucntion.\n\n\nloss <- evaluate(model, x = x_test[y_test == 0,], y = x_test[y_test == 0,])\nloss\n\n\n        loss \n0.0003534254 \nTuning with CloudML\nWe may be able to get better results by tuning our model hyperparameters. We can tune, for example, the normalization function, the learning rate, the activation functions and the size of hidden layers. CloudML uses Bayesian optimization to tune hyperparameters of models as described in this blog post.\nWe can use the cloudml package to tune our model, but first we need to prepare our project by creating a training flag for each hyperparameter and a tuning.yml file that will tell CloudML what parameters we want to tune and how.\nThe full script used for training on CloudML can be found at https://github.com/dfalbel/fraud-autoencoder-example. The most important modifications to the code were adding the training flags:\n\n\nFLAGS <- flags(\n  flag_string(\"normalization\", \"minmax\", \"One of minmax, zscore\"),\n  flag_string(\"activation\", \"relu\", \"One of relu, selu, tanh, sigmoid\"),\n  flag_numeric(\"learning_rate\", 0.001, \"Optimizer Learning Rate\"),\n  flag_integer(\"hidden_size\", 15, \"The hidden layer size\")\n)\n\n\nWe then used the FLAGS variable inside the script to drive the hyperparameters of the model, for example:\n\n\nmodel %>% compile(\n  optimizer = optimizer_adam(lr = FLAGS$learning_rate), \n  loss = 'mean_squared_error',\n)\n\n\nWe also created a tuning.yml file describing how hyperparameters should be varied during training, as well as what metric we wanted to optimize (in this case it was the validation loss: val_loss).\ntuning.yml\ntrainingInput:\n  scaleTier: CUSTOM\n  masterType: standard_gpu\n  hyperparameters:\n    goal: MINIMIZE\n    hyperparameterMetricTag: val_loss\n    maxTrials: 10\n    maxParallelTrials: 5\n    params:\n      - parameterName: normalization\n        type: CATEGORICAL\n        categoricalValues: [zscore, minmax]\n      - parameterName: activation\n        type: CATEGORICAL\n        categoricalValues: [relu, selu, tanh, sigmoid]\n      - parameterName: learning_rate\n        type: DOUBLE\n        minValue: 0.000001\n        maxValue: 0.1\n        scaleType: UNIT_LOG_SCALE\n      - parameterName: hidden_size\n        type: INTEGER\n        minValue: 5\n        maxValue: 50\n        scaleType: UNIT_LINEAR_SCALE\nWe describe the type of machine we want to use (in this case a standard_gpu instance), the metric we want to minimize while tuning, and the the maximum number of trials (i.e. number of combinations of hyperparameters we want to test). We then specify how we want to vary each hyperparameter during tuning.\nYou can learn more about the tuning.yml file at the Tensorflow for R documentation and at Google’s official documentation on CloudML.\nNow we are ready to send the job to Google CloudML. We can do this by running:\n\n\nlibrary(cloudml)\ncloudml_train(\"train.R\", config = \"tuning.yml\")\n\n\nThe cloudml package takes care of uploading the dataset and installing any R package dependencies required to run the script on CloudML. If you are using RStudio v1.1 or higher, it will also allow you to monitor your job in a background terminal. You can also monitor your job using the Google Cloud Console.\nAfter the job is finished we can collect the job results with:\n\n\njob_collect()\n\n\nThis will copy the files from the job with the best val_loss performance on CloudML to your local system and open a report summarizing the training run.\n\nSince we used a callback to save model checkpoints during training, the model file was also copied from Google CloudML. Files created during training are copied to the “runs” subdirectory of the working directory from which cloudml_train() is called. You can determine this directory for the most recent run with:\n\n\nlatest_run()$run_dir\n\n\n[1] runs/cloudml_2018_01_23_221244595-03\nYou can also list all previous runs and their validation losses with:\n\n\nls_runs(order = metric_val_loss, decreasing = FALSE)\n\n\n                    run_dir metric_loss metric_val_loss\n1 runs/2017-12-09T21-01-11Z      0.2577          0.1482\n2 runs/2017-12-09T21-00-11Z      0.2655          0.1505\n3 runs/2017-12-09T19-59-44Z      0.2597          0.1402\n4 runs/2017-12-09T19-56-48Z      0.2610          0.1459\n\nUse View(ls_runs()) to view all columns\nIn our case the job downloaded from CloudML was saved to runs/cloudml_2018_01_23_221244595-03/, so the saved model file is available at runs/cloudml_2018_01_23_221244595-03/model.hdf5. We can now use our tuned model to make predictions.\nMaking predictions\nNow that we trained and tuned our model we are ready to generate predictions with our autoencoder. We are interested in the MSE for each observation and we expect that observations of fraudulent transactions will have higher MSE’s.\nFirst, let’s load our model.\n\n\nmodel <- load_model_hdf5(\"runs/cloudml_2018_01_23_221244595-03/model.hdf5\", \n                         compile = FALSE)\n\n\nNow let’s calculate the MSE for the training and test set observations.\n\n\npred_train <- predict(model, x_train)\nmse_train <- apply((x_train - pred_train)^2, 1, sum)\n\npred_test <- predict(model, x_test)\nmse_test <- apply((x_test - pred_test)^2, 1, sum)\n\n\nA good measure of model performance in highly unbalanced datasets is the Area Under the ROC Curve (AUC). AUC has a nice interpretation for this problem, it’s the probability that a fraudulent transaction will have higher MSE then a normal one. We can calculate this using the Metrics package, which implements a wide variety of common machine learning model performance metrics.\n\n\nlibrary(Metrics)\nauc(y_train, mse_train)\nauc(y_test, mse_test)\n\n\n[1] 0.9546814\n[1] 0.9403554\nTo use the model in practice for making predictions we need to find a threshold \\(k\\) for the MSE, then if if \\(MSE > k\\) we consider that transaction a fraud (otherwise we consider it normal). To define this value it’s useful to look at precision and recall while varying the threshold \\(k\\).\n\n\npossible_k <- seq(0, 0.5, length.out = 100)\nprecision <- sapply(possible_k, function(k) {\n  predicted_class <- as.numeric(mse_test > k)\n  sum(predicted_class == 1 & y_test == 1)/sum(predicted_class)\n})\n\nqplot(possible_k, precision, geom = \"line\") \n  + labs(x = \"Threshold\", y = \"Precision\")\n\n\n\n\n\nrecall <- sapply(possible_k, function(k) {\n  predicted_class <- as.numeric(mse_test > k)\n  sum(predicted_class == 1 & y_test == 1)/sum(y_test)\n})\nqplot(possible_k, recall, geom = \"line\") \n  + labs(x = \"Threshold\", y = \"Recall\")\n\n\n\nA good starting point would be to choose the threshold with maximum precision but we could also base our decision on how much money we might lose from fraudulent transactions.\nSuppose each manual verification of fraud costs us $1 but if we don’t verify a transaction and it’s a fraud we will lose this transaction amount. Let’s find for each threshold value how much money we would lose.\n\n\ncost_per_verification <- 1\n\nlost_money <- sapply(possible_k, function(k) {\n  predicted_class <- as.numeric(mse_test > k)\n  sum(cost_per_verification * predicted_class + (predicted_class == 0) * y_test * df_test$Amount) \n})\n\nqplot(possible_k, lost_money, geom = \"line\") + labs(x = \"Threshold\", y = \"Lost Money\")\n\n\n\nWe can find the best threshold in this case with:\n\n\npossible_k[which.min(lost_money)]\n\n\n[1] 0.005050505\nIf we needed to manually verify all frauds, it would cost us ~$13,000. Using our model we can reduce this to ~$2,500.\n\n\n\n",
    "preview": "posts/2018-01-24-keras-fraud-autoencoder/images/preview.png",
    "last_modified": "2024-11-21T15:49:12+00:00",
    "input_file": {},
    "preview_width": 790,
    "preview_height": 537
  },
  {
    "path": "posts/2018-01-24-analyzing-rtweet-data-with-kerasformula/",
    "title": "Analyzing rtweet Data with kerasformula",
    "description": "The kerasformula package offers a high-level interface for the R interface to Keras. It’s main interface is the kms function, a regression-style interface to keras_model_sequential that uses formulas and sparse matrices. We use kerasformula to predict how popular tweets will be based on how often the tweet was retweeted and favorited.",
    "author": [
      {
        "name": "Pete Mohanty",
        "url": "https://sites.google.com/site/petemohanty/"
      }
    ],
    "date": "2018-01-24",
    "categories": [
      "TensorFlow/Keras",
      "Natural Language Processing"
    ],
    "contents": "\nOverview\nThe kerasformula package offers a high-level interface for the R interface to Keras. It’s main interface is the kms function, a regression-style interface to keras_model_sequential that uses formulas and sparse matrices.\nThe kerasformula package is available on CRAN, and can be installed with:\n\n\n# install the kerasformula package\ninstall.packages(\"kerasformula\")    \n# or devtools::install_github(\"rdrr1990/kerasformula\")\n\nlibrary(kerasformula)\n\n# install the core keras library (if you haven't already done so)\n# see ?install_keras() for options e.g. install_keras(tensorflow = \"gpu\")\ninstall_keras()\n\n\nThe kms() function\nMany classic machine learning tutorials assume that data come in a relatively homogenous form (e.g., pixels for digit recognition or word counts or ranks) which can make coding somewhat cumbersome when data is contained in a heterogenous data frame. kms() takes advantage of the flexibility of R formulas to smooth this process.\nkms builds dense neural nets and, after fitting them, returns a single object with predictions, measures of fit, and details about the function call. kms accepts a number of parameters including the loss and activation functions found in keras. kms also accepts compiled keras_model_sequential objects allowing for even further customization. This little demo shows how kms can aid is model building and hyperparameter selection (e.g., batch size) starting with raw data gathered using library(rtweet).\n\n\n\nLet’s look at #rstats tweets (excluding retweets) for a six-day period ending January 24, 2018 at 10:40. This happens to give us a nice reasonable number of observations to work with in terms of runtime (and the purpose of this document is to show syntax, not build particularly predictive models).\n\n\nrstats <- search_tweets(\"#rstats\", n = 10000, include_rts = FALSE)\ndim(rstats)\n\n\n  [1] 2840   42\nSuppose our goal is to predict how popular tweets will be based on how often the tweet was retweeted and favorited (which correlate strongly).\n\n\ncor(rstats$favorite_count, rstats$retweet_count, method=\"spearman\")\n\n\n    [1] 0.7051952\nSince few tweeets go viral, the data are quite skewed towards zero.\n\n\n\n\nGetting the most out of formulas\nLet’s suppose we are interested in putting tweets into categories based on popularity but we’re not sure how finely-grained we want to make distinctions. Some of the data, like rstats$mentions_screen_name comes in a list of varying lengths, so let’s write a helper function to count non-NA entries.\n\n\nn <- function(x) {\n  unlist(lapply(x, function(y){length(y) - is.na(y[1])}))\n}\n\n\nLet’s start with a dense neural net, the default of kms. We can use base R functions to help clean the data–in this case, cut to discretize the outcome, grepl to look for key words, and weekdays and format to capture different aspects of the time the tweet was posted.\n\n\nbreaks <- c(-1, 0, 1, 10, 100, 1000, 10000)\npopularity <- kms(cut(retweet_count + favorite_count, breaks) ~ screen_name + \n                  source + n(hashtags) + n(mentions_screen_name) + \n                  n(urls_url) + nchar(text) +\n                  grepl('photo', media_type) +\n                  weekdays(created_at) + \n                  format(created_at, '%H'), rstats)\nplot(popularity$history) \n  + ggtitle(paste(\"#rstat popularity:\", \n            paste0(round(100*popularity$evaluations$acc, 1), \"%\"),\n            \"out-of-sample accuracy\")) \n  + theme_minimal()\n\npopularity$confusion\n\n\n\npopularity$confusion\n\n                    (-1,0] (0,1] (1,10] (10,100] (100,1e+03] (1e+03,1e+04]\n      (-1,0]            37    12     28        2           0             0\n      (0,1]             14    19     72        1           0             0\n      (1,10]             6    11    187       30           0             0\n      (10,100]           1     3     54       68           0             0\n      (100,1e+03]        0     0      4       10           0             0\n      (1e+03,1e+04]      0     0      0        1           0             0\nThe model only classifies about 55% of the out-of-sample data correctly and that predictive accuracy doesn’t improve after the first ten epochs. The confusion matrix suggests that model does best with tweets that are retweeted a handful of times but overpredicts the 1-10 level. The history plot also suggests that out-of-sample accuracy is not very stable. We can easily change the breakpoints and number of epochs.\n\n\nbreaks <- c(-1, 0, 1, 25, 50, 75, 100, 500, 1000, 10000)\npopularity <- kms(cut(retweet_count + favorite_count, breaks) ~  \n                  n(hashtags) + n(mentions_screen_name) + n(urls_url) +\n                  nchar(text) +\n                  screen_name + source +\n                  grepl('photo', media_type) +\n                  weekdays(created_at) + \n                  format(created_at, '%H'), rstats, Nepochs = 10)\n\nplot(popularity$history) \n  + ggtitle(paste(\"#rstat popularity (new breakpoints):\",\n            paste0(round(100*popularity$evaluations$acc, 1), \"%\"),\n            \"out-of-sample accuracy\")) \n  + theme_minimal()\n\n\n\nThat helped some (about 5% additional predictive accuracy). Suppose we want to add a little more data. Let’s first store the input formula.\n\n\npop_input <- \"cut(retweet_count + favorite_count, breaks) ~  \n                          n(hashtags) + n(mentions_screen_name) + n(urls_url) +\n                          nchar(text) +\n                          screen_name + source +\n                          grepl('photo', media_type) +\n                          weekdays(created_at) + \n                          format(created_at, '%H')\"\n\n\nHere we use paste0 to add to the formula by looping over user IDs adding something like:\ngrepl(\"12233344455556\", mentions_user_id)\n\n\nmentions <- unlist(rstats$mentions_user_id)\nmentions <- unique(mentions[which(table(mentions) > 5)]) # remove infrequent\nmentions <- mentions[!is.na(mentions)] # drop NA\n\nfor(i in mentions)\n  pop_input <- paste0(pop_input, \" + \", \"grepl(\", i, \", mentions_user_id)\")\n\npopularity <- kms(pop_input, rstats)\n\n\n\n\n\n\nThat helped a touch but the predictive accuracy is still fairly unstable across epochs…\nCustomizing layers with kms()\nWe could add more data, perhaps add individual words from the text or some other summary stat (mean(text %in% LETTERS) to see if all caps explains popularity). But let’s alter the neural net.\nThe input.formula is used to create a sparse model matrix. For example, rstats$source (Twitter or Twitter-client application type) and rstats$screen_name are character vectors that will be dummied out. How many columns does it have?\n\n\npopularity$P\n\n\n    [1] 1277\nSay we wanted to reshape the layers to transition more gradually from the input shape to the output.\n\n\npopularity <- kms(pop_input, rstats,\n                  layers = list(\n                    units = c(1024, 512, 256, 128, NA),\n                    activation = c(\"relu\", \"relu\", \"relu\", \"relu\", \"softmax\"), \n                    dropout = c(0.5, 0.45, 0.4, 0.35, NA)\n                  ))\n\n\n\n\n\n\nkms builds a keras_sequential_model(), which is a stack of linear layers. The input shape is determined by the dimensionality of the model matrix (popularity$P) but after that users are free to determine the number of layers and so on. The kms argument layers expects a list, the first entry of which is a vector units with which to call keras::layer_dense(). The first element the number of units in the first layer, the second element for the second layer, and so on (NA as the final element connotes to auto-detect the final number of units based on the observed number of outcomes). activation is also passed to layer_dense() and may take values such as softmax, relu, elu, and linear. (kms also has a separate parameter to control the optimizer; by default kms(... optimizer = 'rms_prop').) The dropout that follows each dense layer rate prevents overfitting (but of course isn’t applicable to the final layer).\nChoosing a Batch Size\nBy default, kms uses batches of 32. Suppose we were happy with our model but didn’t have any particular intuition about what the size should be.\n\n\nNbatch <- c(16, 32, 64)\nNruns <- 4\naccuracy <- matrix(nrow = Nruns, ncol = length(Nbatch))\ncolnames(accuracy) <- paste0(\"Nbatch_\", Nbatch)\n\nest <- list()\nfor(i in 1:Nruns){\n  for(j in 1:length(Nbatch)){\n   est[[i]] <- kms(pop_input, rstats, Nepochs = 2, batch_size = Nbatch[j])\n   accuracy[i,j] <- est[[i]][[\"evaluations\"]][[\"acc\"]]\n  }\n}\n  \ncolMeans(accuracy)\n\n\n    Nbatch_16 Nbatch_32 Nbatch_64 \n    0.5088407 0.3820850 0.5556952 \nFor the sake of curtailing runtime, the number of epochs was set arbitrarily short but, from those results, 64 is the best batch size.\nMaking predictions for new data\nThus far, we have been using the default settings for kms which first splits data into 80% training and 20% testing. Of the 80% training, a certain portion is set aside for validation and that’s what produces the epoch-by-epoch graphs of loss and accuracy. The 20% is only used at the end to assess predictive accuracy.\nBut suppose you wanted to make predictions on a new data set…\n\n\npopularity <- kms(pop_input, rstats[1:1000,])\npredictions <- predict(popularity, rstats[1001:2000,])\npredictions$accuracy\n\n\n    [1] 0.579\n\n\n# predictions$confusion\n\n\nBecause the formula creates a dummy variable for each screen name and mention, any given set of tweets is all but guaranteed to have different columns. predict.kms_fit is an S3 method that takes the new data and constructs a (sparse) model matrix that preserves the original structure of the training matrix. predict then returns the predictions along with a confusion matrix and accuracy score.\nIf your newdata has the same observed levels of y and columns of x_train (the model matrix), you can also use keras::predict_classes on object$model.\nUsing a compiled Keras model\nThis section shows how to input a model compiled in the fashion typical to library(keras), which is useful for more advanced models. Here is an example for lstm analogous to the imbd with Keras example.\n\n\nk <- keras_model_sequential()\nk %>%\n  layer_embedding(input_dim = popularity$P, output_dim = popularity$P) %>% \n  layer_lstm(units = 512, dropout = 0.4, recurrent_dropout = 0.2) %>% \n  layer_dense(units = 256, activation = \"relu\") %>%\n  layer_dropout(0.3) %>%\n  layer_dense(units = 8, # number of levels observed on y (outcome)  \n              activation = 'sigmoid')\n\nk %>% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = 'rmsprop',\n  metrics = c('accuracy')\n)\n\npopularity_lstm <- kms(pop_input, rstats, k)\n\n\nQuestions? Comments?\nDrop me a line via the project’s Github repo. Special thanks to @dfalbel and @jjallaire for helpful suggestions!!\n\n\n\n",
    "preview": "posts/2018-01-24-analyzing-rtweet-data-with-kerasformula/images/densities-1.png",
    "last_modified": "2024-11-21T15:53:13+00:00",
    "input_file": {},
    "preview_width": 672,
    "preview_height": 480
  },
  {
    "path": "posts/2018-01-11-keras-customer-churn/",
    "title": "Deep Learning With Keras To Predict Customer Churn",
    "description": "Using Keras to predict customer churn based on the IBM Watson Telco Customer Churn dataset. We also demonstrate using the lime package to help explain which features drive individual model predictions. In addition, we use three new packages to assist with Machine Learning: recipes for preprocessing, rsample for sampling data and yardstick for model metrics.",
    "author": [
      {
        "name": "Matt Dancho",
        "url": "https://github.com/mdancho84"
      }
    ],
    "date": "2018-01-11",
    "categories": [
      "TensorFlow/Keras",
      "Tabular Data",
      "Explainability"
    ],
    "contents": "\n\n\nIntroduction\nCustomer churn is a problem that all companies need to monitor, especially those that depend on subscription-based revenue streams. The simple fact is that most organizations have data that can be used to target these individuals and to understand the key drivers of churn, and we now have Keras for Deep Learning available in R (Yes, in R!!), which predicted customer churn with 82% accuracy.\nWe’re super excited for this article because we are using the new keras package to produce an Artificial Neural Network (ANN) model on the IBM Watson Telco Customer Churn Data Set! As with most business problems, it’s equally important to explain what features drive the model, which is why we’ll use the lime package for explainability. We cross-checked the LIME results with a Correlation Analysis using the corrr package.\nIn addition, we use three new packages to assist with Machine Learning (ML): recipes for preprocessing, rsample for sampling data and yardstick for model metrics. These are relatively new additions to CRAN developed by Max Kuhn at RStudio (creator of the caret package). It seems that R is quickly developing ML tools that rival Python. Good news if you’re interested in applying Deep Learning in R! We are so let’s get going!!\nCustomer Churn: Hurts Sales, Hurts Company\nCustomer churn refers to the situation when a customer ends their relationship with a company, and it’s a costly problem. Customers are the fuel that powers a business. Loss of customers impacts sales. Further, it’s much more difficult and costly to gain new customers than it is to retain existing customers. As a result, organizations need to focus on reducing customer churn.\nThe good news is that machine learning can help. For many businesses that offer subscription based services, it’s critical to both predict customer churn and explain what features relate to customer churn. Older techniques such as logistic regression can be less accurate than newer techniques such as deep learning, which is why we are going to show you how to model an ANN in R with the keras package.\nChurn Modeling With Artificial Neural Networks (Keras)\nArtificial Neural Networks (ANN) are now a staple within the sub-field of Machine Learning called Deep Learning. Deep learning algorithms can be vastly superior to traditional regression and classification methods (e.g. linear and logistic regression) because of the ability to model interactions between features that would otherwise go undetected. The challenge becomes explainability, which is often needed to support the business case. The good news is we get the best of both worlds with keras and lime.\nIBM Watson Dataset (Where We Got The Data)\nThe dataset used for this tutorial is IBM Watson Telco Dataset. According to IBM, the business challenge is…\n\nA telecommunications company [Telco] is concerned about the number of customers leaving their landline business for cable competitors. They need to understand who is leaving. Imagine that you’re an analyst at this company and you have to find out who is leaving and why.\n\nThe dataset includes information about:\nCustomers who left within the last month: The column is called Churn\nServices that each customer has signed up for: phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\nCustomer account information: how long they’ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges\nDemographic info about customers: gender, age range, and if they have partners and dependents\nDeep Learning With Keras (What We Did With The Data)\nIn this example we show you how to use keras to develop a sophisticated and highly accurate deep learning model in R. We walk you through the preprocessing steps, investing time into how to format the data for Keras. We inspect the various classification metrics, and show that an un-tuned ANN model can easily get 82% accuracy on the unseen data. Here’s the deep learning training history visualization.\n\nWe have some fun with preprocessing the data (yes, preprocessing can actually be fun and easy!). We use the new recipes package to simplify the preprocessing workflow.\nWe end by showing you how to explain the ANN with the lime package. Neural networks used to be frowned upon because of the “black box” nature meaning these sophisticated models (ANNs are highly accurate) are difficult to explain using traditional methods. Not any more with LIME! Here’s the feature importance visualization.\n\nWe also cross-checked the LIME results with a Correlation Analysis using the corrr package. Here’s the correlation visualization.\n\n\n\nWe even built a Shiny Application with a Customer Scorecard to monitor customer churn risk and to make recommendations on how to improve customer health! Feel free to take it for a spin.\n\n\n\nCredits\nWe saw that just last week the same Telco customer churn dataset was used in the article, Predict Customer Churn – Logistic Regression, Decision Tree and Random Forest. We thought the article was excellent.\nThis article takes a different approach with Keras, LIME, Correlation Analysis, and a few other cutting edge packages. We encourage the readers to check out both articles because, although the problem is the same, both solutions are beneficial to those learning data science and advanced modeling.\nPrerequisites\nWe use the following libraries in this tutorial:\nkeras: Library that ports Keras from Python enabling deep learning in R. Visit the documentation for more information.\nlime: Used to explain the predictions of black box classifiers. Deep Learning falls into this category.\ntidyquant: Loads the tidyverse (dplyr, ggplot2, etc) and has nice visualization functions with theme_tq(). Visit the tidyquant documentation and the tidyverse documentation for more information on the individual packages.\nrsample: New package for generating resamples. Visit the documentation for more information.\nrecipes: New package for preprocessing machine learning data sets. Visit the documentation for more information.\nyardstick: Tidy methods for measuring model performance. Visit the GitHub Page for more information.\ncorrr: Tidy methods for correlation. Visit the GitHub Page for more information.\nInstall the following packages with install.packages().\n\n\npkgs <- c(\"keras\", \"lime\", \"tidyquant\", \"rsample\", \"recipes\", \"yardstick\", \"corrr\")\ninstall.packages(pkgs)\n\n\nLoad Libraries\nLoad the libraries.\n\n\n# Load libraries\nlibrary(keras)\nlibrary(lime)\nlibrary(tidyquant)\nlibrary(rsample)\nlibrary(recipes)\nlibrary(yardstick)\nlibrary(corrr)\n\n\nIf you have not previously run Keras in R, you will need to install Keras using the install_keras() function.\n\n\n# Install Keras if you have not installed before\ninstall_keras()\n\n\nImport Data\nDownload the IBM Watson Telco Data Set here. Next, use read_csv() to import the data into a nice tidy data frame. We use the glimpse() function to quickly inspect the data. We have the target “Churn” and all other variables are potential predictors. The raw data set needs to be cleaned and preprocessed for ML.\n\n\nchurn_data_raw <- read_csv(\"WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n\nglimpse(churn_data_raw)\n\n\nObservations: 7,043\nVariables: 21\n$ customerID       <chr> \"7590-VHVEG\", \"5575-GNVDE\", \"3668-QPYBK\", \"77...\n$ gender           <chr> \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"...\n$ SeniorCitizen    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n$ Partner          <chr> \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"N...\n$ Dependents       <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"N...\n$ tenure           <int> 1, 34, 2, 45, 2, 8, 22, 10, 28, 62, 13, 16, 5...\n$ PhoneService     <chr> \"No\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"Yes\"...\n$ MultipleLines    <chr> \"No phone service\", \"No\", \"No\", \"No phone ser...\n$ InternetService  <chr> \"DSL\", \"DSL\", \"DSL\", \"DSL\", \"Fiber optic\", \"F...\n$ OnlineSecurity   <chr> \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"No\", \"No\", ...\n$ OnlineBackup     <chr> \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"Yes\", ...\n$ DeviceProtection <chr> \"No\", \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"No\", ...\n$ TechSupport      <chr> \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"N...\n$ StreamingTV      <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"...\n$ StreamingMovies  <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"N...\n$ Contract         <chr> \"Month-to-month\", \"One year\", \"Month-to-month...\n$ PaperlessBilling <chr> \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"Yes\"...\n$ PaymentMethod    <chr> \"Electronic check\", \"Mailed check\", \"Mailed c...\n$ MonthlyCharges   <dbl> 29.85, 56.95, 53.85, 42.30, 70.70, 99.65, 89....\n$ TotalCharges     <dbl> 29.85, 1889.50, 108.15, 1840.75, 151.65, 820....\n$ Churn            <chr> \"No\", \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"No\", ...\nPreprocess Data\nWe’ll go through a few steps to preprocess the data for ML. First, we “prune” the data, which is nothing more than removing unnecessary columns and rows. Then we split into training and testing sets. After that we explore the training set to uncover transformations that will be needed for deep learning. We save the best for last. We end by preprocessing the data with the new recipes package.\nPrune The Data\nThe data has a few columns and rows we’d like to remove:\nThe “customerID” column is a unique identifier for each observation that isn’t needed for modeling. We can de-select this column.\nThe data has 11 NA values all in the “TotalCharges” column. Because it’s such a small percentage of the total population (99.8% complete cases), we can drop these observations with the drop_na() function from tidyr. Note that these may be customers that have not yet been charged, and therefore an alternative is to replace with zero or -99 to segregate this population from the rest.\nMy preference is to have the target in the first column so we’ll include a final select() ooperation to do so.\nWe’ll perform the cleaning operation with one tidyverse pipe (%>%) chain.\n\n\n# Remove unnecessary data\nchurn_data_tbl <- churn_data_raw %>%\n  select(-customerID) %>%\n  drop_na() %>%\n  select(Churn, everything())\n    \nglimpse(churn_data_tbl)\n\n\nObservations: 7,032\nVariables: 20\n$ Churn            <chr> \"No\", \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"No\", ...\n$ gender           <chr> \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"...\n$ SeniorCitizen    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n$ Partner          <chr> \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"N...\n$ Dependents       <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"N...\n$ tenure           <int> 1, 34, 2, 45, 2, 8, 22, 10, 28, 62, 13, 16, 5...\n$ PhoneService     <chr> \"No\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"Yes\"...\n$ MultipleLines    <chr> \"No phone service\", \"No\", \"No\", \"No phone ser...\n$ InternetService  <chr> \"DSL\", \"DSL\", \"DSL\", \"DSL\", \"Fiber optic\", \"F...\n$ OnlineSecurity   <chr> \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"No\", \"No\", ...\n$ OnlineBackup     <chr> \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"Yes\", ...\n$ DeviceProtection <chr> \"No\", \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"No\", ...\n$ TechSupport      <chr> \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"N...\n$ StreamingTV      <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"...\n$ StreamingMovies  <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"N...\n$ Contract         <chr> \"Month-to-month\", \"One year\", \"Month-to-month...\n$ PaperlessBilling <chr> \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"Yes\"...\n$ PaymentMethod    <chr> \"Electronic check\", \"Mailed check\", \"Mailed c...\n$ MonthlyCharges   <dbl> 29.85, 56.95, 53.85, 42.30, 70.70, 99.65, 89....\n$ TotalCharges     <dbl> 29.85, 1889.50, 108.15, 1840.75, 151.65, 820..\nSplit Into Train/Test Sets\nWe have a new package, rsample, which is very useful for sampling methods. It has the initial_split() function for splitting data sets into training and testing sets. The return is a special rsplit object.\n\n\n# Split test/training sets\nset.seed(100)\ntrain_test_split <- initial_split(churn_data_tbl, prop = 0.8)\ntrain_test_split\n\n\n<5626/1406/7032>\nWe can retrieve our training and testing sets using training() and testing() functions.\n\n\n# Retrieve train and test sets\ntrain_tbl <- training(train_test_split)\ntest_tbl  <- testing(train_test_split) \n\n\nExploration: What Transformation Steps Are Needed For ML?\nThis phase of the analysis is often called exploratory analysis, but basically we are trying to answer the question, “What steps are needed to prepare for ML?” The key concept is knowing what transformations are needed to run the algorithm most effectively. Artificial Neural Networks are best when the data is one-hot encoded, scaled and centered. In addition, other transformations may be beneficial as well to make relationships easier for the algorithm to identify. A full exploratory analysis is not practical in this article. With that said we’ll cover a few tips on transformations that can help as they relate to this dataset. In the next section, we will implement the preprocessing techniques.\nDiscretize The “tenure” Feature\nNumeric features like age, years worked, length of time in a position can generalize a group (or cohort). We see this in marketing a lot (think “millennials”, which identifies a group born in a certain timeframe). The “tenure” feature falls into this category of numeric features that can be discretized into groups.\n\n\n\n\nWe can split into six cohorts that divide up the user base by tenure in roughly one year (12 month) increments. This should help the ML algorithm detect if a group is more/less susceptible to customer churn.\n\n\n\n\nTransform The “TotalCharges” Feature\nWhat we don’t like to see is when a lot of observations are bunched within a small part of the range.\n\n\n\n\nWe can use a log transformation to even out the data into more of a normal distribution. It’s not perfect, but it’s quick and easy to get our data spread out a bit more.\n\n\n\n\nPro Tip: A quick test is to see if the log transformation increases the magnitude of the correlation between “TotalCharges” and “Churn”. We’ll use a few dplyr operations along with the corrr package to perform a quick correlation.\ncorrelate(): Performs tidy correlations on numeric data\nfocus(): Similar to select(). Takes columns and focuses on only the rows/columns of importance.\nfashion(): Makes the formatting aesthetically easier to read.\n\n\n# Determine if log transformation improves correlation \n# between TotalCharges and Churn\ntrain_tbl %>%\n  select(Churn, TotalCharges) %>%\n  mutate(\n      Churn = Churn %>% as.factor() %>% as.numeric(),\n      LogTotalCharges = log(TotalCharges)\n      ) %>%\n  correlate() %>%\n  focus(Churn) %>%\n  fashion()\n\n\n          rowname Churn\n1    TotalCharges  -.20\n2 LogTotalCharges  -.25\nThe correlation between “Churn” and “LogTotalCharges” is greatest in magnitude indicating the log transformation should improve the accuracy of the ANN model we build. Therefore, we should perform the log transformation.\nOne-Hot Encoding\nOne-hot encoding is the process of converting categorical data to sparse data, which has columns of only zeros and ones (this is also called creating “dummy variables” or a “design matrix”). All non-numeric data will need to be converted to dummy variables. This is simple for binary Yes/No data because we can simply convert to 1’s and 0’s. It becomes slightly more complicated with multiple categories, which requires creating new columns of 1’s and 0`s for each category (actually one less). We have four features that are multi-category: Contract, Internet Service, Multiple Lines, and Payment Method.\n\n\n\n\nFeature Scaling\nANN’s typically perform faster and often times with higher accuracy when the features are scaled and/or normalized (aka centered and scaled, also known as standardizing). Because ANNs use gradient descent, weights tend to update faster. According to Sebastian Raschka, an expert in the field of Deep Learning, several examples when feature scaling is important are:\n\nk-nearest neighbors with an Euclidean distance measure if want all features to contribute equally\nk-means (see k-nearest neighbors)\nlogistic regression, SVMs, perceptrons, neural networks etc. if you are using gradient descent/ascent-based optimization, otherwise some weights will update much faster than others\nlinear discriminant analysis, principal component analysis, kernel principal component analysis since you want to find directions of maximizing the variance (under the constraints that those directions/eigenvectors/principal components are orthogonal); you want to have features on the same scale since you’d emphasize variables on “larger measurement scales” more. There are many more cases than I can possibly list here … I always recommend you to think about the algorithm and what it’s doing, and then it typically becomes obvious whether we want to scale your features or not.\n\nThe interested reader can read Sebastian Raschka’s article for a full discussion on the scaling/normalization topic. Pro Tip: When in doubt, standardize the data.\nPreprocessing With Recipes\nLet’s implement the preprocessing steps/transformations uncovered during our exploration. Max Kuhn (creator of caret) has been putting some work into Rlang ML tools lately, and the payoff is beginning to take shape. A new package, recipes, makes creating ML data preprocessing workflows a breeze! It takes a little getting used to, but I’ve found that it really helps manage the preprocessing steps. We’ll go over the nitty gritty as it applies to this problem.\nStep 1: Create A Recipe\nA “recipe” is nothing more than a series of steps you would like to perform on the training, testing and/or validation sets. Think of preprocessing data like baking a cake (I’m not a baker but stay with me). The recipe is our steps to make the cake. It doesn’t do anything other than create the playbook for baking.\nWe use the recipe() function to implement our preprocessing steps. The function takes a familiar object argument, which is a modeling function such as object = Churn ~ . meaning “Churn” is the outcome (aka response, predictor, target) and all other features are predictors. The function also takes the data argument, which gives the “recipe steps” perspective on how to apply during baking (next).\nA recipe is not very useful until we add “steps”, which are used to transform the data during baking. The package contains a number of useful “step functions” that can be applied. The entire list of Step Functions can be viewed here. For our model, we use:\nstep_discretize() with the option = list(cuts = 6) to cut the continuous variable for “tenure” (number of years as a customer) to group customers into cohorts.\nstep_log() to log transform “TotalCharges”.\nstep_dummy() to one-hot encode the categorical data. Note that this adds columns of one/zero for categorical data with three or more categories.\nstep_center() to mean-center the data.\nstep_scale() to scale the data.\nThe last step is to prepare the recipe with the prep() function. This step is used to “estimate the required parameters from a training set that can later be applied to other data sets”. This is important for centering and scaling and other functions that use parameters defined from the training set.\nHere’s how simple it is to implement the preprocessing steps that we went over!\n\n\n# Create recipe\nrec_obj <- recipe(Churn ~ ., data = train_tbl) %>%\n  step_discretize(tenure, options = list(cuts = 6)) %>%\n  step_log(TotalCharges) %>%\n  step_dummy(all_nominal(), -all_outcomes()) %>%\n  step_center(all_predictors(), -all_outcomes()) %>%\n  step_scale(all_predictors(), -all_outcomes()) %>%\n  prep(data = train_tbl)\n\n\nWe can print the recipe object if we ever forget what steps were used to prepare the data. Pro Tip: We can save the recipe object as an RDS file using saveRDS(), and then use it to bake() (discussed next) future raw data into ML-ready data in production!\n\n\n# Print the recipe object\nrec_obj\n\n\nData Recipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         19\n\nTraining data contained 5626 data points and no missing data.\n\nSteps:\n\nDummy variables from tenure [trained]\nLog transformation on TotalCharges [trained]\nDummy variables from ~gender, ~Partner, ... [trained]\nCentering for SeniorCitizen, ... [trained]\nScaling for SeniorCitizen, ... [trained]\nStep 2: Baking With Your Recipe\nNow for the fun part! We can apply the “recipe” to any data set with the bake() function, and it processes the data following our recipe steps. We’ll apply to our training and testing data to convert from raw data to a machine learning dataset. Check our training set out with glimpse(). Now that’s an ML-ready dataset prepared for ANN modeling!!\n\n\n# Predictors\nx_train_tbl <- bake(rec_obj, newdata = train_tbl) %>% select(-Churn)\nx_test_tbl  <- bake(rec_obj, newdata = test_tbl) %>% select(-Churn)\n\nglimpse(x_train_tbl)\n\n\nObservations: 5,626\nVariables: 35\n$ SeniorCitizen                         <dbl> -0.4351959, -0.4351...\n$ MonthlyCharges                        <dbl> -1.1575972, -0.2601...\n$ TotalCharges                          <dbl> -2.275819130, 0.389...\n$ gender_Male                           <dbl> -1.0016900, 0.99813...\n$ Partner_Yes                           <dbl> 1.0262054, -0.97429...\n$ Dependents_Yes                        <dbl> -0.6507747, -0.6507...\n$ tenure_bin1                           <dbl> 2.1677790, -0.46121...\n$ tenure_bin2                           <dbl> -0.4389453, -0.4389...\n$ tenure_bin3                           <dbl> -0.4481273, -0.4481...\n$ tenure_bin4                           <dbl> -0.4509837, 2.21698...\n$ tenure_bin5                           <dbl> -0.4498419, -0.4498...\n$ tenure_bin6                           <dbl> -0.4337508, -0.4337...\n$ PhoneService_Yes                      <dbl> -3.0407367, 0.32880...\n$ MultipleLines_No.phone.service        <dbl> 3.0407367, -0.32880...\n$ MultipleLines_Yes                     <dbl> -0.8571364, -0.8571...\n$ InternetService_Fiber.optic           <dbl> -0.8884255, -0.8884...\n$ InternetService_No                    <dbl> -0.5272627, -0.5272...\n$ OnlineSecurity_No.internet.service    <dbl> -0.5272627, -0.5272...\n$ OnlineSecurity_Yes                    <dbl> -0.6369654, 1.56966...\n$ OnlineBackup_No.internet.service      <dbl> -0.5272627, -0.5272...\n$ OnlineBackup_Yes                      <dbl> 1.3771987, -0.72598...\n$ DeviceProtection_No.internet.service  <dbl> -0.5272627, -0.5272...\n$ DeviceProtection_Yes                  <dbl> -0.7259826, 1.37719...\n$ TechSupport_No.internet.service       <dbl> -0.5272627, -0.5272...\n$ TechSupport_Yes                       <dbl> -0.6358628, -0.6358...\n$ StreamingTV_No.internet.service       <dbl> -0.5272627, -0.5272...\n$ StreamingTV_Yes                       <dbl> -0.7917326, -0.7917...\n$ StreamingMovies_No.internet.service   <dbl> -0.5272627, -0.5272...\n$ StreamingMovies_Yes                   <dbl> -0.797388, -0.79738...\n$ Contract_One.year                     <dbl> -0.5156834, 1.93882...\n$ Contract_Two.year                     <dbl> -0.5618358, -0.5618...\n$ PaperlessBilling_Yes                  <dbl> 0.8330334, -1.20021...\n$ PaymentMethod_Credit.card..automatic. <dbl> -0.5231315, -0.5231...\n$ PaymentMethod_Electronic.check        <dbl> 1.4154085, -0.70638...\n$ PaymentMethod_Mailed.check            <dbl> -0.5517013, 1.81225...\nStep 3: Don’t Forget The Target\nOne last step, we need to store the actual values (truth) as y_train_vec and y_test_vec, which are needed for modeling our ANN. We convert to a series of numeric ones and zeros which can be accepted by the Keras ANN modeling functions. We add “vec” to the name so we can easily remember the class of the object (it’s easy to get confused when working with tibbles, vectors, and matrix data types).\n\n\n# Response variables for training and testing sets\ny_train_vec <- ifelse(pull(train_tbl, Churn) == \"Yes\", 1, 0)\ny_test_vec  <- ifelse(pull(test_tbl, Churn) == \"Yes\", 1, 0)\n\n\nModel Customer Churn With Keras (Deep Learning)\nThis is super exciting!! Finally, Deep Learning with Keras in R! The team at RStudio has done fantastic work recently to create the keras package, which implements Keras in R. Very cool!\nBackground On Artifical Neural Networks\nFor those unfamiliar with Neural Networks (and those that need a refresher), read this article. It’s very comprehensive, and you’ll leave with a general understanding of the types of deep learning and how they work.\n\n\nSource: Xenon Stack\n\nDeep Learning has been available in R for some time, but the primary packages used in the wild have not (this includes Keras, Tensor Flow, Theano, etc, which are all Python libraries). It’s worth mentioning that a number of other Deep Learning packages exist in R including h2o, mxnet, and others. The interested reader can check out this blog post for a comparison of deep learning packages in R.\nBuilding A Deep Learning Model\nWe’re going to build a special class of ANN called a Multi-Layer Perceptron (MLP). MLPs are one of the simplest forms of deep learning, but they are both highly accurate and serve as a jumping-off point for more complex algorithms. MLPs are quite versatile as they can be used for regression, binary and multi classification (and are typically quite good at classification problems).\nWe’ll build a three layer MLP with Keras. Let’s walk-through the steps before we implement in R.\nInitialize a sequential model: The first step is to initialize a sequential model with keras_model_sequential(), which is the beginning of our Keras model. The sequential model is composed of a linear stack of layers.\nApply layers to the sequential model: Layers consist of the input layer, hidden layers and an output layer. The input layer is the data and provided it’s formatted correctly there’s nothing more to discuss. The hidden layers and output layers are what controls the ANN inner workings.\nHidden Layers: Hidden layers form the neural network nodes that enable non-linear activation using weights. The hidden layers are created using layer_dense(). We’ll add two hidden layers. We’ll apply units = 16, which is the number of nodes. We’ll select kernel_initializer = \"uniform\" and activation = \"relu\" for both layers. The first layer needs to have the input_shape = 35, which is the number of columns in the training set. Key Point: While we are arbitrarily selecting the number of hidden layers, units, kernel initializers and activation functions, these parameters can be optimized through a process called hyperparameter tuning that is discussed in Next Steps.\nDropout Layers: Dropout layers are used to control overfitting. This eliminates weights below a cutoff threshold to prevent low weights from overfitting the layers. We use the layer_dropout() function add two drop out layers with rate = 0.10 to remove weights below 10%.\nOutput Layer: The output layer specifies the shape of the output and the method of assimilating the learned information. The output layer is applied using the layer_dense(). For binary values, the shape should be units = 1. For multi-classification, the units should correspond to the number of classes. We set the kernel_initializer = \"uniform\" and the activation = \"sigmoid\" (common for binary classification).\n\nCompile the model: The last step is to compile the model with compile(). We’ll use optimizer = \"adam\", which is one of the most popular optimization algorithms. We select loss = \"binary_crossentropy\" since this is a binary classification problem. We’ll select metrics = c(\"accuracy\") to be evaluated during training and testing. Key Point: The optimizer is often included in the tuning process.\nLet’s codify the discussion above to build our Keras MLP-flavored ANN model.\n\n\n# Building our Artificial Neural Network\nmodel_keras <- keras_model_sequential()\n\nmodel_keras %>% \n  \n  # First hidden layer\n  layer_dense(\n    units              = 16, \n    kernel_initializer = \"uniform\", \n    activation         = \"relu\", \n    input_shape        = ncol(x_train_tbl)) %>% \n  \n  # Dropout to prevent overfitting\n  layer_dropout(rate = 0.1) %>%\n  \n  # Second hidden layer\n  layer_dense(\n    units              = 16, \n    kernel_initializer = \"uniform\", \n    activation         = \"relu\") %>% \n  \n  # Dropout to prevent overfitting\n  layer_dropout(rate = 0.1) %>%\n  \n  # Output layer\n  layer_dense(\n    units              = 1, \n    kernel_initializer = \"uniform\", \n    activation         = \"sigmoid\") %>% \n  \n  # Compile ANN\n  compile(\n    optimizer = 'adam',\n    loss      = 'binary_crossentropy',\n    metrics   = c('accuracy')\n  )\n\nkeras_model\n\n\nModel\n___________________________________________________________________________________________________\nLayer (type)                                Output Shape                            Param #        \n===================================================================================================\ndense_1 (Dense)                             (None, 16)                              576            \n___________________________________________________________________________________________________\ndropout_1 (Dropout)                         (None, 16)                              0              \n___________________________________________________________________________________________________\ndense_2 (Dense)                             (None, 16)                              272            \n___________________________________________________________________________________________________\ndropout_2 (Dropout)                         (None, 16)                              0              \n___________________________________________________________________________________________________\ndense_3 (Dense)                             (None, 1)                               17             \n===================================================================================================\nTotal params: 865\nTrainable params: 865\nNon-trainable params: 0\n___________________________________________________________________________________________________\nWe use the fit() function to run the ANN on our training data. The object is our model, and x and y are our training data in matrix and numeric vector forms, respectively. The batch_size = 50 sets the number samples per gradient update within each epoch. We set epochs = 35 to control the number training cycles. Typically we want to keep the batch size high since this decreases the error within each training cycle (epoch). We also want epochs to be large, which is important in visualizing the training history (discussed below). We set validation_split = 0.30 to include 30% of the data for model validation, which prevents overfitting. The training process should complete in 15 seconds or so.\n\n\n# Fit the keras model to the training data\nhistory <- fit(\n  object           = model_keras, \n  x                = as.matrix(x_train_tbl), \n  y                = y_train_vec,\n  batch_size       = 50, \n  epochs           = 35,\n  validation_split = 0.30\n)\n\n\nWe can inspect the training history. We want to make sure there is minimal difference between the validation accuracy and the training accuracy.\n\n\n# Print a summary of the training history\nprint(history)\n\n\nTrained on 3,938 samples, validated on 1,688 samples (batch_size=50, epochs=35)\nFinal epoch (plot to see history):\nval_loss: 0.4215\n val_acc: 0.8057\n    loss: 0.399\n     acc: 0.8101\nWe can visualize the Keras training history using the plot() function. What we want to see is the validation accuracy and loss leveling off, which means the model has completed training. We see that there is some divergence between training loss/accuracy and validation loss/accuracy. This model indicates we can possibly stop training at an earlier epoch. Pro Tip: Only use enough epochs to get a high validation accuracy. Once validation accuracy curve begins to flatten or decrease, it’s time to stop training.\n\n\n# Plot the training/validation history of our Keras model\nplot(history) \n\n\n\nMaking Predictions\nWe’ve got a good model based on the validation accuracy. Now let’s make some predictions from our keras model on the test data set, which was unseen during modeling (we use this for the true performance assessment). We have two functions to generate predictions:\npredict_classes(): Generates class values as a matrix of ones and zeros. Since we are dealing with binary classification, we’ll convert the output to a vector.\npredict_proba(): Generates the class probabilities as a numeric matrix indicating the probability of being a class. Again, we convert to a numeric vector because there is only one column output.\n\n\n# Predicted Class\nyhat_keras_class_vec <- predict_classes(object = model_keras, x = as.matrix(x_test_tbl)) %>%\n    as.vector()\n\n# Predicted Class Probability\nyhat_keras_prob_vec  <- predict_proba(object = model_keras, x = as.matrix(x_test_tbl)) %>%\n    as.vector()\n\n\nInspect Performance With Yardstick\nThe yardstick package has a collection of handy functions for measuring performance of machine learning models. We’ll overview some metrics we can use to understand the performance of our model.\nFirst, let’s get the data formatted for yardstick. We create a data frame with the truth (actual values as factors), estimate (predicted values as factors), and the class probability (probability of yes as numeric). We use the fct_recode() function from the forcats package to assist with recoding as Yes/No values.\n\n\n# Format test data and predictions for yardstick metrics\nestimates_keras_tbl <- tibble(\n  truth      = as.factor(y_test_vec) %>% fct_recode(yes = \"1\", no = \"0\"),\n  estimate   = as.factor(yhat_keras_class_vec) %>% fct_recode(yes = \"1\", no = \"0\"),\n  class_prob = yhat_keras_prob_vec\n)\n\nestimates_keras_tbl\n\n\n# A tibble: 1,406 x 3\n    truth estimate  class_prob\n   <fctr>   <fctr>       <dbl>\n 1    yes       no 0.328355074\n 2    yes      yes 0.633630514\n 3     no       no 0.004589651\n 4     no       no 0.007402068\n 5     no       no 0.049968336\n 6     no       no 0.116824441\n 7     no      yes 0.775479317\n 8     no       no 0.492996633\n 9     no       no 0.011550998\n10     no       no 0.004276015\n# ... with 1,396 more rows\nNow that we have the data formatted, we can take advantage of the yardstick package. The only other thing we need to do is to set options(yardstick.event_first = FALSE). As pointed out by ad1729 in GitHub Issue 13, the default is to classify 0 as the positive class instead of 1.\n\n\noptions(yardstick.event_first = FALSE)\n\n\nConfusion Table\nWe can use the conf_mat() function to get the confusion table. We see that the model was by no means perfect, but it did a decent job of identifying customers likely to churn.\n\n\n# Confusion Table\nestimates_keras_tbl %>% conf_mat(truth, estimate)\n\n\n          Truth\nPrediction  no yes\n       no  950 161\n       yes  99 196\nAccuracy\nWe can use the metrics() function to get an accuracy measurement from the test set. We are getting roughly 82% accuracy.\n\n\n# Accuracy\nestimates_keras_tbl %>% metrics(truth, estimate)\n\n\n# A tibble: 1 x 1\n   accuracy\n      <dbl>\n1 0.8150782\nAUC\nWe can also get the ROC Area Under the Curve (AUC) measurement. AUC is often a good metric used to compare different classifiers and to compare to randomly guessing (AUC_random = 0.50). Our model has AUC = 0.85, which is much better than randomly guessing. Tuning and testing different classification algorithms may yield even better results.\n\n\n# AUC\nestimates_keras_tbl %>% roc_auc(truth, class_prob)\n\n\n[1] 0.8523951\nPrecision And Recall\nPrecision is when the model predicts “yes”, how often is it actually “yes”. Recall (also true positive rate or specificity) is when the actual value is “yes” how often is the model correct. We can get precision() and recall() measurements using yardstick.\n\n\n# Precision\ntibble(\n  precision = estimates_keras_tbl %>% precision(truth, estimate),\n  recall    = estimates_keras_tbl %>% recall(truth, estimate)\n)\n\n\n# A tibble: 1 x 2\n  precision    recall\n      <dbl>     <dbl>\n1 0.6644068 0.5490196\nPrecision and recall are very important to the business case: The organization is concerned with balancing the cost of targeting and retaining customers at risk of leaving with the cost of inadvertently targeting customers that are not planning to leave (and potentially decreasing revenue from this group). The threshold above which to predict Churn = “Yes” can be adjusted to optimize for the business problem. This becomes an Customer Lifetime Value optimization problem that is discussed further in Next Steps.\nF1 Score\nWe can also get the F1-score, which is a weighted average between the precision and recall. Machine learning classifier thresholds are often adjusted to maximize the F1-score. However, this is often not the optimal solution to the business problem.\n\n\n# F1-Statistic\nestimates_keras_tbl %>% f_meas(truth, estimate, beta = 1)\n\n\n[1] 0.601227\nExplain The Model With LIME\nLIME stands for Local Interpretable Model-agnostic Explanations, and is a method for explaining black-box machine learning model classifiers. For those new to LIME, this YouTube video does a really nice job explaining how LIME helps to identify feature importance with black box machine learning models (e.g. deep learning, stacked ensembles, random forest).\n\n\n\n\nSetup\nThe lime package implements LIME in R. One thing to note is that it’s not setup out-of-the-box to work with keras. The good news is with a few functions we can get everything working properly. We’ll need to make two custom functions:\nmodel_type: Used to tell lime what type of model we are dealing with. It could be classification, regression, survival, etc.\npredict_model: Used to allow lime to perform predictions that its algorithm can interpret.\nThe first thing we need to do is identify the class of our model object. We do this with the class() function.\n\n\nclass(model_keras)\n\n\n[1] \"keras.models.Sequential\"        \n[2] \"keras.engine.training.Model\"    \n[3] \"keras.engine.topology.Container\"\n[4] \"keras.engine.topology.Layer\"    \n[5] \"python.builtin.object\"\nNext we create our model_type() function. It’s only input is x the keras model. The function simply returns “classification”, which tells LIME we are classifying.\n\n\n# Setup lime::model_type() function for keras\nmodel_type.keras.models.Sequential <- function(x, ...) {\n  \"classification\"\n}\n\n\nNow we can create our predict_model() function, which wraps keras::predict_proba(). The trick here is to realize that it’s inputs must be x a model, newdata a dataframe object (this is important), and type which is not used but can be use to switch the output type. The output is also a little tricky because it must be in the format of probabilities by classification (this is important; shown next).\n\n\n# Setup lime::predict_model() function for keras\npredict_model.keras.models.Sequential <- function(x, newdata, type, ...) {\n  pred <- predict_proba(object = x, x = as.matrix(newdata))\n  data.frame(Yes = pred, No = 1 - pred)\n}\n\n\nRun this next script to show you what the output looks like and to test our predict_model() function. See how it’s the probabilities by classification. It must be in this form for model_type = \"classification\".\n\n\n# Test our predict_model() function\npredict_model(x = model_keras, newdata = x_test_tbl, type = 'raw') %>%\n  tibble::as_tibble()\n\n\n# A tibble: 1,406 x 2\n           Yes        No\n         <dbl>     <dbl>\n 1 0.328355074 0.6716449\n 2 0.633630514 0.3663695\n 3 0.004589651 0.9954103\n 4 0.007402068 0.9925979\n 5 0.049968336 0.9500317\n 6 0.116824441 0.8831756\n 7 0.775479317 0.2245207\n 8 0.492996633 0.5070034\n 9 0.011550998 0.9884490\n10 0.004276015 0.9957240\n# ... with 1,396 more rows\nNow the fun part, we create an explainer using the lime() function. Just pass the training data set without the “Attribution column”. The form must be a data frame, which is OK since our predict_model function will switch it to an keras object. Set model = automl_leader our leader model, and bin_continuous = FALSE. We could tell the algorithm to bin continuous variables, but this may not make sense for categorical numeric data that we didn’t change to factors.\n\n\n# Run lime() on training set\nexplainer <- lime::lime(\n  x              = x_train_tbl, \n  model          = model_keras, \n  bin_continuous = FALSE\n)\n\n\nNow we run the explain() function, which returns our explanation. This can take a minute to run so we limit it to just the first ten rows of the test data set. We set n_labels = 1 because we care about explaining a single class. Setting n_features = 4 returns the top four features that are critical to each case. Finally, setting kernel_width = 0.5 allows us to increase the “model_r2” value by shrinking the localized evaluation.\n\n\n# Run explain() on explainer\nexplanation <- lime::explain(\n  x_test_tbl[1:10, ], \n  explainer    = explainer, \n  n_labels     = 1, \n  n_features   = 4,\n  kernel_width = 0.5\n)\n\n\nFeature Importance Visualization\nThe payoff for the work we put in using LIME is this feature importance plot. This allows us to visualize each of the first ten cases (observations) from the test data. The top four features for each case are shown. Note that they are not the same for each case. The green bars mean that the feature supports the model conclusion, and the red bars contradict. A few important features based on frequency in first ten cases:\nTenure (7 cases)\nSenior Citizen (5 cases)\nOnline Security (4 cases)\n\n\nplot_features(explanation) +\n  labs(title = \"LIME Feature Importance Visualization\",\n       subtitle = \"Hold Out (Test) Set, First 10 Cases Shown\")\n\n\n\nAnother excellent visualization can be performed using plot_explanations(), which produces a facetted heatmap of all case/label/feature combinations. It’s a more condensed version of plot_features(), but we need to be careful because it does not provide exact statistics and it makes it less easy to investigate binned features (Notice that “tenure” would not be identified as a contributor even though it shows up as a top feature in 7 of 10 cases).\n\n\nplot_explanations(explanation) +\n    labs(title = \"LIME Feature Importance Heatmap\",\n         subtitle = \"Hold Out (Test) Set, First 10 Cases Shown\")\n\n\n\n\n\nCheck Explanations With Correlation Analysis\nOne thing we need to be careful with the LIME visualization is that we are only doing a sample of the data, in our case the first 10 test observations. Therefore, we are gaining a very localized understanding of how the ANN works. However, we also want to know on from a global perspective what drives feature importance.\nWe can perform a correlation analysis on the training set as well to help glean what features correlate globally to “Churn”. We’ll use the corrr package, which performs tidy correlations with the function correlate(). We can get the correlations as follows.\n\n\n# Feature correlations to Churn\ncorrr_analysis <- x_train_tbl %>%\n  mutate(Churn = y_train_vec) %>%\n  correlate() %>%\n  focus(Churn) %>%\n  rename(feature = rowname) %>%\n  arrange(abs(Churn)) %>%\n  mutate(feature = as_factor(feature)) \ncorrr_analysis\n\n\n# A tibble: 35 x 2\n                          feature        Churn\n                           <fctr>        <dbl>\n 1                    gender_Male -0.006690899\n 2                    tenure_bin3 -0.009557165\n 3 MultipleLines_No.phone.service -0.016950072\n 4               PhoneService_Yes  0.016950072\n 5              MultipleLines_Yes  0.032103354\n 6                StreamingTV_Yes  0.066192594\n 7            StreamingMovies_Yes  0.067643871\n 8           DeviceProtection_Yes -0.073301197\n 9                    tenure_bin4 -0.073371838\n10     PaymentMethod_Mailed.check -0.080451164\n# ... with 25 more rows\nThe correlation visualization helps in distinguishing which features are relavant to Churn.\n\n# Correlation visualization\ncorrr_analysis %>%\n  ggplot(aes(x = Churn, y = fct_reorder(feature, desc(Churn)))) +\n  geom_point() +\n  # Positive Correlations - Contribute to churn\n  geom_segment(aes(xend = 0, yend = feature), \n               color = palette_light()[[2]], \n               data = corrr_analysis %>% filter(Churn > 0)) +\n  geom_point(color = palette_light()[[2]], \n             data = corrr_analysis %>% filter(Churn > 0)) +\n  # Negative Correlations - Prevent churn\n  geom_segment(aes(xend = 0, yend = feature), \n               color = palette_light()[[1]], \n               data = corrr_analysis %>% filter(Churn < 0)) +\n  geom_point(color = palette_light()[[1]], \n             data = corrr_analysis %>% filter(Churn < 0)) +\n  # Vertical lines\n  geom_vline(xintercept = 0, color = palette_light()[[5]], size = 1, linetype = 2) +\n  geom_vline(xintercept = -0.25, color = palette_light()[[5]], size = 1, linetype = 2) +\n  geom_vline(xintercept = 0.25, color = palette_light()[[5]], size = 1, linetype = 2) +\n  # Aesthetics\n  theme_tq() +\n  labs(title = \"Churn Correlation Analysis\",\n       subtitle = paste(\"Positive Correlations (contribute to churn),\",\n                        \"Negative Correlations (prevent churn)\")\n       y = \"Feature Importance\")\n\n\n\n\nThe correlation analysis helps us quickly disseminate which features that the LIME analysis may be excluding. We can see that the following features are highly correlated (magnitude > 0.25):\nIncreases Likelihood of Churn (Red):\n- Tenure = Bin 1 (<12 Months)\n- Internet Service = “Fiber Optic”\n- Payment Method = “Electronic Check”\nDecreases Likelihood of Churn (Blue):\n- Contract = “Two Year”\n- Total Charges (Note that this may be a biproduct of additional services such as Online Security)\nFeature Investigation\nWe can investigate features that are most frequent in the LIME feature importance visualization along with those that the correlation analysis shows an above normal magnitude. We’ll investigate:\nTenure (7/10 LIME Cases, Highly Correlated)\nContract (Highly Correlated)\nInternet Service (Highly Correlated)\nPayment Method (Highly Correlated)\nSenior Citizen (5/10 LIME Cases)\nOnline Security (4/10 LIME Cases)\nTenure (7/10 LIME Cases, Highly Correlated)\nLIME cases indicate that the ANN model is using this feature frequently and high correlation agrees that this is important. Investigating the feature distribution, it appears that customers with lower tenure (bin 1) are more likely to leave. Opportunity: Target customers with less than 12 month tenure.\n\n\n\n\n\n\nContract (Highly Correlated)\nWhile LIME did not indicate this as a primary feature in the first 10 cases, the feature is clearly correlated with those electing to stay. Customers with one and two year contracts are much less likely to churn. Opportunity: Offer promotion to switch to long term contracts.\n\n\n\n\n\n\nInternet Service (Highly Correlated)\nWhile LIME did not indicate this as a primary feature in the first 10 cases, the feature is clearly correlated with those electing to stay. Customers with fiber optic service are more likely to churn while those with no internet service are less likely to churn. Improvement Area: Customers may be dissatisfied with fiber optic service.\n\n\n\n\n\n\nPayment Method (Highly Correlated)\nWhile LIME did not indicate this as a primary feature in the first 10 cases, the feature is clearly correlated with those electing to stay. Customers with electronic check are more likely to leave. Opportunity: Offer customers a promotion to switch to automatic payments.\n\n\n\n\n\n\nSenior Citizen (5/10 LIME Cases)\nSenior citizen appeared in several of the LIME cases indicating it was important to the ANN for the 10 samples. However, it was not highly correlated to Churn, which may indicate that the ANN is using in an more sophisticated manner (e.g. as an interaction). It’s difficult to say that senior citizens are more likely to leave, but non-senior citizens appear less at risk of churning. Opportunity: Target users in the lower age demographic.\n\n\n\n\n\n\nOnline Security (4/10 LIME Cases)\nCustomers that did not sign up for online security were more likely to leave while customers with no internet service or online security were less likely to leave. Opportunity: Promote online security and other packages that increase retention rates.\n\n\n\n\n\n\nNext Steps: Business Science University\nWe’ve just scratched the surface with the solution to this problem, but unfortunately there’s only so much ground we can cover in an article. Here are a few next steps that I’m pleased to announce will be covered in a Business Science University course coming in 2018!\nCustomer Lifetime Value\nYour organization needs to see the financial benefit so always tie your analysis to sales, profitability or ROI. Customer Lifetime Value (CLV) is a methodology that ties the business profitability to the retention rate. While we did not implement the CLV methodology herein, a full customer churn analysis would tie the churn to an classification cutoff (threshold) optimization to maximize the CLV with the predictive ANN model.\nThe simplified CLV model is:\n\\[ \nCLV=GC*\\frac{1}{1+d-r} \n\\]\nWhere,\nGC is the gross contribution per customer\nd is the annual discount rate\nr is the retention rate\nANN Performance Evaluation and Improvement\nThe ANN model we built is good, but it could be better. How we understand our model accuracy and improve on it is through the combination of two techniques:\nK-Fold Cross-Fold Validation: Used to obtain bounds for accuracy estimates.\nHyper Parameter Tuning: Used to improve model performance by searching for the best parameters possible.\nWe need to implement K-Fold Cross Validation and Hyper Parameter Tuning if we want a best-in-class model.\nDistributing Analytics\nIt’s critical to communicate data science insights to decision makers in the organization. Most decision makers in organizations are not data scientists, but these individuals make important decisions on a day-to-day basis. The Shiny application below includes a Customer Scorecard to monitor customer health (risk of churn).\n\n\n\nBusiness Science University\nYou’re probably wondering why we are going into so much detail on next steps. We are happy to announce a new project for 2018: Business Science University, an online school dedicated to helping data science learners.\nBenefits to learners:\nBuild your own online GitHub portfolio of data science projects to market your skills to future employers!\nLearn real-world applications in People Analytics (HR), Customer Analytics, Marketing Analytics, Social Media Analytics, Text Mining and Natural Language Processing (NLP), Financial and Time Series Analytics, and more!\nUse advanced machine learning techniques for both high accuracy modeling and explaining features that have an effect on the outcome!\nCreate ML-powered web-applications that can be distributed throughout an organization, enabling non-data scientists to benefit from algorithms in a user-friendly way!\nEnrollment is open so please signup for special perks. Just go to Business Science University and select enroll.\nConclusions\nCustomer churn is a costly problem. The good news is that machine learning can solve churn problems, making the organization more profitable in the process. In this article, we saw how Deep Learning can be used to predict customer churn. We built an ANN model using the new keras package that achieved 82% predictive accuracy (without tuning)! We used three new machine learning packages to help with preprocessing and measuring performance: recipes, rsample and yardstick. Finally we used lime to explain the Deep Learning model, which traditionally was impossible! We checked the LIME results with a Correlation Analysis, which brought to light other features to investigate. For the IBM Telco dataset, tenure, contract type, internet service type, payment menthod, senior citizen status, and online security status were useful in diagnosing customer churn. We hope you enjoyed this article!\n\n\n\n",
    "preview": "posts/2018-01-11-keras-customer-churn/images/customer_churn_analysis_corrr.png",
    "last_modified": "2024-11-21T15:52:01+00:00",
    "input_file": {},
    "preview_width": 2696,
    "preview_height": 1696
  },
  {
    "path": "posts/2018-01-10-r-interface-to-cloudml/",
    "title": "R Interface to Google CloudML",
    "description": "We are excited to announce the availability of the cloudml package, which provides an R interface to Google Cloud Machine Learning Engine. CloudML provides a number of services including on-demand access to training on GPUs and hyperparameter tuning to optimize key attributes of model architectures.",
    "author": [
      {
        "name": "J.J. Allaire",
        "url": "https://github.com/jjallaire"
      }
    ],
    "date": "2018-01-10",
    "categories": [
      "Cloud",
      "Packages/Releases"
    ],
    "contents": "\nOverview\nWe are excited to announce the availability of the cloudml package, which provides an R interface to Google Cloud Machine Learning Engine. CloudML provides a number of services including:\nScalable training of models built with the keras, tfestimators, and tensorflow R packages.\nOn-demand access to training on GPUs, including the new Tesla P100 GPUs from NVIDIA®.\nHyperparameter tuning to optmize key attributes of model architectures in order to maximize predictive accuracy.\nDeployment of trained models to the Google global prediction platform that can support thousands of users and TBs of data.\nTraining with CloudML\nOnce you’ve configured your system to publish to CloudML, training a model is as straightforward as calling the cloudml_train() function:\n\n\nlibrary(cloudml)\ncloudml_train(\"train.R\")\n\n\nCloudML provides a variety of GPU configurations, which can be easily selected when calling cloudml_train(). For example, the following would train the same model as above but with a Tesla K80 GPU:\n\n\ncloudml_train(\"train.R\", master_type = \"standard_gpu\")\n\n\nTo train using a Tesla P100 GPU you would specify \"standard_p100\":\n\n\ncloudml_train(\"train.R\", master_type = \"standard_p100\")\n\n\nWhen training completes the job is collected and a training run report is displayed:\n\n\nLearning More\nCheck out the cloudml package documentation to get started with training and deploying models on CloudML.\nYou can also find out more about the various capabilities of CloudML in these articles:\nTraining with CloudML goes into additional depth on managing training jobs and their output.\nHyperparameter Tuning explores how you can improve the performance of your models by running many trials with distinct hyperparameters (e.g. number and size of layers) to determine their optimal values.\nGoogle Cloud Storage provides information on copying data between your local machine and Google Storage and also describes how to use data within Google Storage during training.\nDeploying Models describes how to deploy trained models and generate predictions from them.\n\n\n\n",
    "preview": "posts/2018-01-10-r-interface-to-cloudml/images/cloudml.png",
    "last_modified": "2024-11-21T15:53:00+00:00",
    "input_file": {},
    "preview_width": 394,
    "preview_height": 211
  },
  {
    "path": "posts/2018-01-09-keras-duplicate-questions-quora/",
    "title": "Classifying Duplicate Questions from Quora with Keras",
    "description": "In this post we will use Keras to classify duplicated questions from Quora. Our implementation is inspired by the Siamese Recurrent Architecture, with modifications to the similarity measure and the embedding layers (the original paper uses pre-trained word vectors)",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": "https://github.com/dfalbel"
      }
    ],
    "date": "2018-01-09",
    "categories": [
      "TensorFlow/Keras",
      "Natural Language Processing"
    ],
    "contents": "\nIntroduction\nIn this post we will use Keras to classify duplicated questions from Quora.\nThe dataset first appeared in the Kaggle competition Quora Question Pairs and consists of approximately 400,000 pairs of questions along with a column indicating if the question pair is considered a duplicate.\nOur implementation is inspired by the Siamese Recurrent Architecture, with modifications to the similarity\nmeasure and the embedding layers (the original paper uses pre-trained word vectors). Using this kind\nof architecture dates back to 2005 with Le Cun et al and is useful for\nverification tasks. The idea is to learn a function that maps input patterns into a\ntarget space such that a similarity measure in the target space approximates\nthe “semantic” distance in the input space.\nAfter the competition, Quora also described their approach to this problem in this blog post.\nDowloading data\nData can be downloaded from the Kaggle dataset webpage\nor from Quora’s release of the dataset:\n\n\nlibrary(keras)\nquora_data <- get_file(\n  \"quora_duplicate_questions.tsv\",\n  \"https://qim.ec.quoracdn.net/quora_duplicate_questions.tsv\"\n)\n\n\nWe are using the Keras get_file() function so that the file download is cached.\nReading and preprocessing\nWe will first load data into R and do some preprocessing to make it easier to\ninclude in the model. After downloading the data, you can read it\nusing the readr read_tsv() function.\n\n\nlibrary(readr)\ndf <- read_tsv(quora_data)\n\n\nWe will create a Keras tokenizer to transform each word into an integer\ntoken. We will also specify a hyperparameter of our model: the vocabulary size.\nFor now let’s use the 50,000 most common words (we’ll tune this parameter later).\nThe tokenizer will be fit using all unique questions from the dataset.\n\n\ntokenizer <- text_tokenizer(num_words = 50000)\ntokenizer %>% fit_text_tokenizer(unique(c(df$question1, df$question2)))\n\n\nLet’s save the tokenizer to disk in order to use it for inference later.\n\n\nsave_text_tokenizer(tokenizer, \"tokenizer-question-pairs\")\n\n\nWe will now use the text tokenizer to transform each question into a list\nof integers.\n\n\nquestion1 <- texts_to_sequences(tokenizer, df$question1)\nquestion2 <- texts_to_sequences(tokenizer, df$question2)\n\n\nLet’s take a look at the number of words in each question. This will helps us to\ndecide the padding length, another hyperparameter of our model. Padding the sequences normalizes them to the same size so that we can feed them to the Keras model.\n\n\nlibrary(purrr)\nquestions_length <- c(\n  map_int(question1, length),\n  map_int(question2, length)\n)\n\nquantile(questions_length, c(0.8, 0.9, 0.95, 0.99))\n\n\n80% 90% 95% 99% \n 14  18  23  31 \nWe can see that 99% of questions have at most length 31 so we’ll choose a padding\nlength between 15 and 30. Let’s start with 20 (we’ll also tune this parameter later).\nThe default padding value is 0, but we are already using this value for words that\ndon’t appear within the 50,000 most frequent, so we’ll use 50,001 instead.\n\n\nquestion1_padded <- pad_sequences(question1, maxlen = 20, value = 50000 + 1)\nquestion2_padded <- pad_sequences(question2, maxlen = 20, value = 50000 + 1)\n\n\nWe have now finished the preprocessing steps. We will now run a simple benchmark\nmodel before moving on to the Keras model.\nSimple benchmark\nBefore creating a complicated model let’s take a simple approach.\nLet’s create two predictors: percentage of words from question1 that\nappear in the question2 and vice-versa. Then we will use a logistic\nregression to predict if the questions are duplicate.\n\n\nperc_words_question1 <- map2_dbl(question1, question2, ~mean(.x %in% .y))\nperc_words_question2 <- map2_dbl(question2, question1, ~mean(.x %in% .y))\n\ndf_model <- data.frame(\n  perc_words_question1 = perc_words_question1,\n  perc_words_question2 = perc_words_question2,\n  is_duplicate = df$is_duplicate\n) %>%\n  na.omit()\n\n\nNow that we have our predictors, let’s create the logistic model.\nWe will take a small sample for validation.\n\n\nval_sample <- sample.int(nrow(df_model), 0.1*nrow(df_model))\nlogistic_regression <- glm(\n  is_duplicate ~ perc_words_question1 + perc_words_question2, \n  family = \"binomial\",\n  data = df_model[-val_sample,]\n)\nsummary(logistic_regression)\n\n\nCall:\nglm(formula = is_duplicate ~ perc_words_question1 + perc_words_question2, \n    family = \"binomial\", data = df_model[-val_sample, ])\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.5938  -0.9097  -0.6106   1.1452   2.0292  \n\nCoefficients:\n                      Estimate Std. Error z value Pr(>|z|)    \n(Intercept)          -2.259007   0.009668 -233.66   <2e-16 ***\nperc_words_question1  1.517990   0.023038   65.89   <2e-16 ***\nperc_words_question2  1.681410   0.022795   73.76   <2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 479158  on 363843  degrees of freedom\nResidual deviance: 431627  on 363841  degrees of freedom\n  (17 observations deleted due to missingness)\nAIC: 431633\n\nNumber of Fisher Scoring iterations: 3\nLet’s calculate the accuracy on our validation set.\n\n\npred <- predict(logistic_regression, df_model[val_sample,], type = \"response\")\npred <- pred > mean(df_model$is_duplicate[-val_sample])\naccuracy <- table(pred, df_model$is_duplicate[val_sample]) %>% \n  prop.table() %>% \n  diag() %>% \n  sum()\naccuracy\n\n\n[1] 0.6573577\nWe got an accuracy of 65.7%. Not all that much better than random guessing.\nNow let’s create our model in Keras.\nModel definition\nWe will use a Siamese network to predict whether the pairs are duplicated or not.\nThe idea is to create a model that can embed the questions (sequence of words)\ninto a vector. Then we can compare the vectors for each question using a similarity\nmeasure and tell if the questions are duplicated or not.\nFirst let’s define the inputs for the model.\n\n\ninput1 <- layer_input(shape = c(20), name = \"input_question1\")\ninput2 <- layer_input(shape = c(20), name = \"input_question2\")\n\n\nThen let’s the define the part of the model that will embed the questions in a\nvector.\n\n\nword_embedder <- layer_embedding( \n  input_dim = 50000 + 2, # vocab size + UNK token + padding value\n  output_dim = 128,      # hyperparameter - embedding size\n  input_length = 20,     # padding size,\n  embeddings_regularizer = regularizer_l2(0.0001) # hyperparameter - regularization \n)\n\nseq_embedder <- layer_lstm(\n  units = 128, # hyperparameter -- sequence embedding size\n  kernel_regularizer = regularizer_l2(0.0001) # hyperparameter - regularization \n)\n\n\nNow we will define the relationship between the input vectors and the embeddings\nlayers. Note that we use the same layers and weights on both inputs. That’s why\nthis is called a Siamese network. It makes sense, because we don’t want to have different\noutputs if question1 is switched with question2.\n\n\nvector1 <- input1 %>% word_embedder() %>% seq_embedder()\nvector2 <- input2 %>% word_embedder() %>% seq_embedder()\n\n\nWe then define the similarity measure we want to optimize. We want duplicated questions\nto have higher values of similarity. In this example we’ll use the cosine similarity,\nbut any similarity measure could be used. Remember that the cosine similarity is the\nnormalized dot product of the vectors, but for training it’s not necessary to\nnormalize the results.\n\n\ncosine_similarity <- layer_dot(list(vector1, vector2), axes = 1)\n\n\nNext, we define a final sigmoid layer to output the probability of both questions\nbeing duplicated.\n\n\noutput <- cosine_similarity %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\n\nNow that let’s define the Keras model in terms of it’s inputs and outputs and\ncompile it. In the compilation phase we define our loss function and optimizer.\nLike in the Kaggle challenge, we will minimize the logloss (equivalent\nto minimizing the binary crossentropy). We will use the Adam optimizer.\n\n\nmodel <- keras_model(list(input1, input2), output)\nmodel %>% compile(\n  optimizer = \"adam\", \n  metrics = list(acc = metric_binary_accuracy), \n  loss = \"binary_crossentropy\"\n)\n\n\nWe can then take a look at out model with the summary function.\n\n\nsummary(model)\n\n\n_______________________________________________________________________________________\nLayer (type)                Output Shape       Param #    Connected to                 \n=======================================================================================\ninput_question1 (InputLayer (None, 20)         0                                       \n_______________________________________________________________________________________\ninput_question2 (InputLayer (None, 20)         0                                       \n_______________________________________________________________________________________\nembedding_1 (Embedding)     (None, 20, 128)    6400256    input_question1[0][0]        \n                                                          input_question2[0][0]        \n_______________________________________________________________________________________\nlstm_1 (LSTM)               (None, 128)        131584     embedding_1[0][0]            \n                                                          embedding_1[1][0]            \n_______________________________________________________________________________________\ndot_1 (Dot)                 (None, 1)          0          lstm_1[0][0]                 \n                                                          lstm_1[1][0]                 \n_______________________________________________________________________________________\ndense_1 (Dense)             (None, 1)          2          dot_1[0][0]                  \n=======================================================================================\nTotal params: 6,531,842\nTrainable params: 6,531,842\nNon-trainable params: 0\n_______________________________________________________________________________________\nModel fitting\nNow we will fit and tune our model. However before proceeding let’s take a sample for validation.\n\n\nset.seed(1817328)\nval_sample <- sample.int(nrow(question1_padded), size = 0.1*nrow(question1_padded))\n\ntrain_question1_padded <- question1_padded[-val_sample,]\ntrain_question2_padded <- question2_padded[-val_sample,]\ntrain_is_duplicate <- df$is_duplicate[-val_sample]\n\nval_question1_padded <- question1_padded[val_sample,]\nval_question2_padded <- question2_padded[val_sample,]\nval_is_duplicate <- df$is_duplicate[val_sample]\n\n\nNow we use the fit() function to train the model:\n\n\nmodel %>% fit(\n  list(train_question1_padded, train_question2_padded),\n  train_is_duplicate, \n  batch_size = 64, \n  epochs = 10, \n  validation_data = list(\n    list(val_question1_padded, val_question2_padded), \n    val_is_duplicate\n  )\n)\n\n\nTrain on 363861 samples, validate on 40429 samples\nEpoch 1/10\n363861/363861 [==============================] - 89s 245us/step - loss: 0.5860 - acc: 0.7248 - val_loss: 0.5590 - val_acc: 0.7449\nEpoch 2/10\n363861/363861 [==============================] - 88s 243us/step - loss: 0.5528 - acc: 0.7461 - val_loss: 0.5472 - val_acc: 0.7510\nEpoch 3/10\n363861/363861 [==============================] - 88s 242us/step - loss: 0.5428 - acc: 0.7536 - val_loss: 0.5439 - val_acc: 0.7515\nEpoch 4/10\n363861/363861 [==============================] - 88s 242us/step - loss: 0.5353 - acc: 0.7595 - val_loss: 0.5358 - val_acc: 0.7590\nEpoch 5/10\n363861/363861 [==============================] - 88s 242us/step - loss: 0.5299 - acc: 0.7633 - val_loss: 0.5358 - val_acc: 0.7592\nEpoch 6/10\n363861/363861 [==============================] - 88s 242us/step - loss: 0.5256 - acc: 0.7662 - val_loss: 0.5309 - val_acc: 0.7631\nEpoch 7/10\n363861/363861 [==============================] - 88s 242us/step - loss: 0.5211 - acc: 0.7701 - val_loss: 0.5349 - val_acc: 0.7586\nEpoch 8/10\n363861/363861 [==============================] - 88s 242us/step - loss: 0.5173 - acc: 0.7733 - val_loss: 0.5278 - val_acc: 0.7667\nEpoch 9/10\n363861/363861 [==============================] - 88s 242us/step - loss: 0.5138 - acc: 0.7762 - val_loss: 0.5292 - val_acc: 0.7667\nEpoch 10/10\n363861/363861 [==============================] - 88s 242us/step - loss: 0.5092 - acc: 0.7794 - val_loss: 0.5313 - val_acc: 0.7654\nAfter training completes, we can save our model for inference with the save_model_hdf5()\nfunction.\n\n\nsave_model_hdf5(model, \"model-question-pairs.hdf5\")\n\n\nModel tuning\nNow that we have a reasonable model, let’s tune the hyperparameters using the\ntfruns package. We’ll begin by adding FLAGS declarations to our script for all hyperparameters we want to tune (FLAGS allow us to vary hyperparmaeters without changing our source code):\n\n\nFLAGS <- flags(\n  flag_integer(\"vocab_size\", 50000),\n  flag_integer(\"max_len_padding\", 20),\n  flag_integer(\"embedding_size\", 256),\n  flag_numeric(\"regularization\", 0.0001),\n  flag_integer(\"seq_embedding_size\", 512)\n)\n\n\nWith this FLAGS definition we can now write our code in terms of the flags. For example:\n\n\ninput1 <- layer_input(shape = c(FLAGS$max_len_padding))\ninput2 <- layer_input(shape = c(FLAGS$max_len_padding))\n\nembedding <- layer_embedding(\n  input_dim = FLAGS$vocab_size + 2, \n  output_dim = FLAGS$embedding_size, \n  input_length = FLAGS$max_len_padding, \n  embeddings_regularizer = regularizer_l2(l = FLAGS$regularization)\n)\n\n\nThe full source code of the script with FLAGS can be found here.\nWe additionally added an early stopping callback in the training step in order to stop training\nif validation loss doesn’t decrease for 5 epochs in a row. This will hopefully reduce training time for bad models. We also added a learning rate reducer to reduce the learning rate by a factor of 10 when the loss doesn’t decrease for 3 epochs (this technique typically increases model accuracy).\n\n\nmodel %>% fit(\n  ...,\n  callbacks = list(\n    callback_early_stopping(patience = 5),\n    callback_reduce_lr_on_plateau(patience = 3)\n  )\n)\n\n\nWe can now execute a tuning run to probe for the optimal combination of hyperparameters. We call the tuning_run() function, passing a list with\nthe possible values for each flag. The tuning_run() function will be responsible for executing the script for all combinations of hyperparameters. We also specify\nthe sample parameter to train the model for only a random sample from all combinations (reducing training time significantly).\n\n\nlibrary(tfruns)\n\nruns <- tuning_run(\n  \"question-pairs.R\", \n  flags = list(\n    vocab_size = c(30000, 40000, 50000, 60000),\n    max_len_padding = c(15, 20, 25),\n    embedding_size = c(64, 128, 256),\n    regularization = c(0.00001, 0.0001, 0.001),\n    seq_embedding_size = c(128, 256, 512)\n  ), \n  runs_dir = \"tuning\", \n  sample = 0.2\n)\n\n\nThe tuning run will return a data.frame with results for all runs.\nWe found that the best run attained 84.9% accuracy using the combination of hyperparameters shown below, so we modify our training script to use these values as the defaults:\n\n\nFLAGS <- flags(\n  flag_integer(\"vocab_size\", 50000),\n  flag_integer(\"max_len_padding\", 20),\n  flag_integer(\"embedding_size\", 256),\n  flag_numeric(\"regularization\", 1e-4),\n  flag_integer(\"seq_embedding_size\", 512)\n)\n\n\nMaking predictions\nNow that we have trained and tuned our model we can start making predictions.\nAt prediction time we will load both the text tokenizer and the model we saved\nto disk earlier.\n\n\nlibrary(keras)\nmodel <- load_model_hdf5(\"model-question-pairs.hdf5\", compile = FALSE)\ntokenizer <- load_text_tokenizer(\"tokenizer-question-pairs\")\n\n\nSince we won’t continue training the model, we specified the compile = FALSE argument.\nNow let`s define a function to create predictions. In this function we we preprocess the input data in the same way we preprocessed the training data:\n\n\npredict_question_pairs <- function(model, tokenizer, q1, q2) {\n  q1 <- texts_to_sequences(tokenizer, list(q1))\n  q2 <- texts_to_sequences(tokenizer, list(q2))\n  \n  q1 <- pad_sequences(q1, 20)\n  q2 <- pad_sequences(q2, 20)\n  \n  as.numeric(predict(model, list(q1, q2)))\n}\n\n\nWe can now call it with new pairs of questions, for example:\n\n\npredict_question_pairs(\n  model,\n  tokenizer,\n  \"What's R programming?\",\n  \"What's R in programming?\"\n)\n\n\n[1] 0.9784008\nPrediction is quite fast (~40 milliseconds).\nDeploying the model\nTo demonstrate deployment of the trained model, we created a simple Shiny application, where\nyou can paste 2 questions from Quora and find the probability of them being duplicated. Try changing the questions below or entering two entirely different questions.\n\n\nThe shiny application can be found at https://jjallaire.shinyapps.io/shiny-quora/ and it’s source code at https://github.com/dfalbel/shiny-quora-question-pairs.\nNote that when deploying a Keras model you only need to load the previously saved model file and tokenizer (no training data or model training steps are required).\nWrapping up\nWe trained a Siamese LSTM that gives us reasonable accuracy (84%). Quora’s state of the art is 87%.\nWe can improve our model by using pre-trained word embeddings on larger datasets. For example, try using what’s described in this example. Quora uses their own complete corpus to train the word embeddings.\nAfter training we deployed our model as a Shiny application which given two Quora questions calculates the probability of their being duplicates.\n\n\n\n",
    "preview": "posts/2018-01-09-keras-duplicate-questions-quora/keras-duplicate-questions-quora.png",
    "last_modified": "2024-11-21T15:53:16+00:00",
    "input_file": {},
    "preview_width": 1302,
    "preview_height": 788
  },
  {
    "path": "posts/2017-12-22-word-embeddings-with-keras/",
    "title": "Word Embeddings with Keras",
    "description": "Word embedding is a method used to map words of a vocabulary to dense vectors of real numbers where semantically similar words are mapped to nearby points. In this example we'll use Keras to generate word embeddings for the Amazon Fine Foods Reviews dataset.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": "https://github.com/dfalbel"
      }
    ],
    "date": "2017-12-22",
    "categories": [
      "TensorFlow/Keras",
      "Natural Language Processing"
    ],
    "contents": "\nIntroduction\nWord embedding is a method used to map words of a vocabulary to\ndense vectors of real numbers where semantically similar words are mapped to\nnearby points. Representing words in this vector space help\nalgorithms achieve better performance in natural language\nprocessing tasks like syntactic parsing and sentiment analysis by grouping\nsimilar words. For example, we expect that in the embedding space\n“cats” and “dogs” are mapped to nearby points since they are\nboth animals, mammals, pets, etc.\nIn this tutorial we will implement the skip-gram model created by Mikolov et al in R using the keras package.\nThe skip-gram model is a flavor of word2vec, a class of\ncomputationally-efficient predictive models for learning word\nembeddings from raw text. We won’t address theoretical details about embeddings and\nthe skip-gram model. If you want to get more details you can read the paper\nlinked above. The TensorFlow Vector Representation of Words tutorial includes additional details as does the Deep Learning With R notebook about embeddings.\nThere are other ways to create vector representations of words. For example,\nGloVe Embeddings are implemented in the text2vec package by Dmitriy Selivanov.\nThere’s also a tidy approach described in Julia Silge’s blog post Word Vectors with Tidy Data Principles.\nGetting the Data\nWe will use the Amazon Fine Foods Reviews dataset.\nThis dataset consists of reviews of fine foods from Amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and narrative text.\nData can be downloaded (~116MB) by running:\n\n\ndownload.file(\"https://snap.stanford.edu/data/finefoods.txt.gz\", \"finefoods.txt.gz\")\n\n\nWe will now load the plain text reviews into R.\n\n\nlibrary(readr)\nlibrary(stringr)\nreviews <- read_lines(\"finefoods.txt.gz\") \nreviews <- reviews[str_sub(reviews, 1, 12) == \"review/text:\"]\nreviews <- str_sub(reviews, start = 14)\nreviews <- iconv(reviews, to = \"UTF-8\")\n\n\nLet’s take a look at some reviews we have in the dataset.\n\n\nhead(reviews, 2)\n\n\n[1] \"I have bought several of the Vitality canned dog food products ...\n[2] \"Product arrived labeled as Jumbo Salted Peanuts...the peanuts ... \nPreprocessing\nWe’ll begin with some text pre-processing using a keras text_tokenizer(). The tokenizer will be\nresponsible for transforming each review into a sequence of integer tokens (which will subsequently be used as\ninput into the skip-gram model).\n\n\nlibrary(keras)\ntokenizer <- text_tokenizer(num_words = 20000)\ntokenizer %>% fit_text_tokenizer(reviews)\n\n\nNote that the tokenizer object is modified in place by the call to fit_text_tokenizer().\nAn integer token will be assigned for each of the 20,000 most common words (the other words will\nbe assigned to token 0).\nSkip-Gram Model\nIn the skip-gram model we will use each word as input to a log-linear classifier\nwith a projection layer, then predict words within a certain range before and after\nthis word. It would be very computationally expensive to output a probability\ndistribution over all the vocabulary for each target word we input into the model. Instead,\nwe are going to use negative sampling, meaning we will sample some words that don’t\nappear in the context and train a binary classifier to predict if the context word we\npassed is truly from the context or not.\nIn more practical terms, for the skip-gram model we will input a 1d integer vector of\nthe target word tokens and a 1d integer vector of sampled context word tokens. We will\ngenerate a prediction of 1 if the sampled word really appeared in the context and 0 if it didn’t.\nWe will now define a generator function to yield batches for model training.\n\n\nlibrary(reticulate)\nlibrary(purrr)\nskipgrams_generator <- function(text, tokenizer, window_size, negative_samples) {\n  gen <- texts_to_sequences_generator(tokenizer, sample(text))\n  function() {\n    skip <- generator_next(gen) %>%\n      skipgrams(\n        vocabulary_size = tokenizer$num_words, \n        window_size = window_size, \n        negative_samples = 1\n      )\n    x <- transpose(skip$couples) %>% map(. %>% unlist %>% as.matrix(ncol = 1))\n    y <- skip$labels %>% as.matrix(ncol = 1)\n    list(x, y)\n  }\n}\n\n\nA generator function\nis a function that returns a different value each time it is called (generator functions are often used to provide streaming or dynamic data for training models). Our generator function will receive a vector of texts,\na tokenizer and the arguments for the skip-gram (the size of the window around each\ntarget word we examine and how many negative samples we want\nto sample for each target word).\nNow let’s start defining the keras model. We will use the Keras functional API.\n\n\nembedding_size <- 128  # Dimension of the embedding vector.\nskip_window <- 5       # How many words to consider left and right.\nnum_sampled <- 1       # Number of negative examples to sample for each word.\n\n\nWe will first write placeholders for the inputs using the layer_input function.\n\n\ninput_target <- layer_input(shape = 1)\ninput_context <- layer_input(shape = 1)\n\n\nNow let’s define the embedding matrix. The embedding is a matrix with dimensions\n(vocabulary, embedding_size) that acts as lookup table for the word vectors.\n\n\nembedding <- layer_embedding(\n  input_dim = tokenizer$num_words + 1, \n  output_dim = embedding_size, \n  input_length = 1, \n  name = \"embedding\"\n)\n\ntarget_vector <- input_target %>% \n  embedding() %>% \n  layer_flatten()\n\ncontext_vector <- input_context %>%\n  embedding() %>%\n  layer_flatten()\n\n\nThe next step is to define how the target_vector will be related to the context_vector\nin order to make our network output 1 when the context word really appeared in the\ncontext and 0 otherwise. We want target_vector to be similar to the context_vector\nif they appeared in the same context. A typical measure of similarity is the cosine\nsimilarity. Give two vectors \\(A\\) and \\(B\\)\nthe cosine similarity is defined by the Euclidean Dot product of \\(A\\) and \\(B\\) normalized by their\nmagnitude. As we don’t need the similarity to be normalized inside the network, we will only calculate\nthe dot product and then output a dense layer with sigmoid activation.\n\n\ndot_product <- layer_dot(list(target_vector, context_vector), axes = 1)\noutput <- layer_dense(dot_product, units = 1, activation = \"sigmoid\")\n\n\nNow we will create the model and compile it.\n\n\nmodel <- keras_model(list(input_target, input_context), output)\nmodel %>% compile(loss = \"binary_crossentropy\", optimizer = \"adam\")\n\n\nWe can see the full definition of the model by calling summary:\n\n\nsummary(model)\n\n\n_________________________________________________________________________________________\nLayer (type)                 Output Shape       Param #    Connected to                  \n=========================================================================================\ninput_1 (InputLayer)         (None, 1)          0                                        \n_________________________________________________________________________________________\ninput_2 (InputLayer)         (None, 1)          0                                        \n_________________________________________________________________________________________\nembedding (Embedding)        (None, 1, 128)     2560128    input_1[0][0]                 \n                                                           input_2[0][0]                 \n_________________________________________________________________________________________\nflatten_1 (Flatten)          (None, 128)        0          embedding[0][0]               \n_________________________________________________________________________________________\nflatten_2 (Flatten)          (None, 128)        0          embedding[1][0]               \n_________________________________________________________________________________________\ndot_1 (Dot)                  (None, 1)          0          flatten_1[0][0]               \n                                                           flatten_2[0][0]               \n_________________________________________________________________________________________\ndense_1 (Dense)              (None, 1)          2          dot_1[0][0]                   \n=========================================================================================\nTotal params: 2,560,130\nTrainable params: 2,560,130\nNon-trainable params: 0\n_________________________________________________________________________________________\nModel Training\nWe will fit the model using the fit_generator() function We need to specify the number of\ntraining steps as well as number of epochs we want to train. We will train for\n100,000 steps for 5 epochs. This is quite slow (~1000 seconds per epoch on a modern GPU). Note that you\nmay also get reasonable results with just one epoch of training.\n\n\nmodel %>%\n  fit_generator(\n    skipgrams_generator(reviews, tokenizer, skip_window, negative_samples), \n    steps_per_epoch = 100000, epochs = 5\n    )\n\n\nEpoch 1/1\n100000/100000 [==============================] - 1092s - loss: 0.3749      \nEpoch 2/5\n100000/100000 [==============================] - 1094s - loss: 0.3548     \nEpoch 3/5\n100000/100000 [==============================] - 1053s - loss: 0.3630     \nEpoch 4/5\n100000/100000 [==============================] - 1020s - loss: 0.3737     \nEpoch 5/5\n100000/100000 [==============================] - 1017s - loss: 0.3823 \nWe can now extract the embeddings matrix from the model by using the get_weights()\nfunction. We also added row.names to our embedding matrix so we can easily find\nwhere each word is.\n\n\nlibrary(dplyr)\n\nembedding_matrix <- get_weights(model)[[1]]\n\nwords <- data_frame(\n  word = names(tokenizer$word_index), \n  id = as.integer(unlist(tokenizer$word_index))\n)\n\nwords <- words %>%\n  filter(id <= tokenizer$num_words) %>%\n  arrange(id)\n\nrow.names(embedding_matrix) <- c(\"UNK\", words$word)\n\n\nUnderstanding the Embeddings\nWe can now find words that are close to each other in the embedding. We will\nuse the cosine similarity, since this is what we trained the model to\nminimize.\n\n\nlibrary(text2vec)\n\nfind_similar_words <- function(word, embedding_matrix, n = 5) {\n  similarities <- embedding_matrix[word, , drop = FALSE] %>%\n    sim2(embedding_matrix, y = ., method = \"cosine\")\n  \n  similarities[,1] %>% sort(decreasing = TRUE) %>% head(n)\n}\n\n\n\n\nfind_similar_words(\"2\", embedding_matrix)\n\n\n        2         4         3       two         6 \n1.0000000 0.9830254 0.9777042 0.9765668 0.9722549 \n\n\nfind_similar_words(\"little\", embedding_matrix)\n\n\n   little       bit       few     small     treat \n1.0000000 0.9501037 0.9478287 0.9309829 0.9286966 \n\n\nfind_similar_words(\"delicious\", embedding_matrix)\n\n\ndelicious     tasty wonderful   amazing     yummy \n1.0000000 0.9632145 0.9619508 0.9617954 0.9529505 \n\n\nfind_similar_words(\"cats\", embedding_matrix)\n\n\n     cats      dogs      kids       cat       dog \n1.0000000 0.9844937 0.9743756 0.9676026 0.9624494 \nThe t-SNE algorithm can be used to visualize the embeddings. Because of time constraints we\nwill only use it with the first 500 words. To understand more about the t-SNE method see the article How to Use t-SNE Effectively.\nThis plot may look like a mess, but if you zoom into the small groups you end up seeing some nice patterns.\nTry, for example, to find a group of web related words like http, href, etc. Another group\nthat may be easy to pick out is the pronouns group: she, he, her, etc.\n\n\nlibrary(Rtsne)\nlibrary(ggplot2)\nlibrary(plotly)\n\ntsne <- Rtsne(embedding_matrix[2:500,], perplexity = 50, pca = FALSE)\n\ntsne_plot <- tsne$Y %>%\n  as.data.frame() %>%\n  mutate(word = row.names(embedding_matrix)[2:500]) %>%\n  ggplot(aes(x = V1, y = V2, label = word)) + \n  geom_text(size = 3)\ntsne_plot\n\n\n\n\n\n\n\n",
    "preview": "posts/2017-12-22-word-embeddings-with-keras/word-embeddings-with-keras.png",
    "last_modified": "2024-11-21T15:50:05+00:00",
    "input_file": {},
    "preview_width": 700,
    "preview_height": 450
  },
  {
    "path": "posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks/",
    "title": "Time Series Forecasting with Recurrent Neural Networks",
    "description": "In this post, we'll review three advanced techniques for improving the performance and generalization power of recurrent neural networks.  We'll demonstrate all three concepts on a temperature-forecasting problem, where you have access to a time series of data points coming from sensors installed on the roof of a building.",
    "author": [
      {
        "name": "François Chollet",
        "url": "https://github.com/fchollet"
      },
      {
        "name": "J.J. Allaire",
        "url": "https://github.com/jjallaire"
      }
    ],
    "date": "2017-12-20",
    "categories": [
      "TensorFlow/Keras",
      "Time Series"
    ],
    "contents": "\nOverview\nIn this post, we’ll review three advanced techniques for improving the performance and generalization power of recurrent neural networks. By the end of the section, you’ll know most of what there is to know about using recurrent networks with Keras. We’ll demonstrate all three concepts on a temperature-forecasting problem, where you have access to a time series of data points coming from sensors installed on the roof of a building, such as temperature, air pressure, and humidity, which you use to predict what the temperature will be 24 hours after the last data point. This is a fairly challenging problem that exemplifies many common difficulties encountered when working with time series.\nWe’ll cover the following techniques:\nRecurrent dropout — This is a specific, built-in way to use dropout to fight overfitting in recurrent layers.\nStacking recurrent layers — This increases the representational power of the network (at the cost of higher computational loads).\nBidirectional recurrent layers — These present the same information to a recurrent network in different ways, increasing accuracy and mitigating forgetting issues.\nA temperature-forecasting problem\nUntil now, the only sequence data we’ve covered has been text data, such as the IMDB dataset and the Reuters dataset. But sequence data is found in many more problems than just language processing. In all the examples in this section, you’ll play with a weather timeseries dataset recorded at the Weather Station at the Max Planck Institute for Biogeochemistry in Jena, Germany.\nIn this dataset, 14 different quantities (such air temperature, atmospheric pressure, humidity, wind direction, and so on) were recorded every 10 minutes, over several years. The original data goes back to 2003, but this example is limited to data from 2009–2016. This dataset is perfect for learning to work with numerical time series. You’ll use it to build a model that takes as input some data from the recent past (a few days’ worth of data points) and predicts the air temperature 24 hours in the future.\nDownload and uncompress the data as follows:\n\n\ndir.create(\"~/Downloads/jena_climate\", recursive = TRUE)\ndownload.file(\n  \"https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\",\n  \"~/Downloads/jena_climate/jena_climate_2009_2016.csv.zip\"\n)\nunzip(\n  \"~/Downloads/jena_climate/jena_climate_2009_2016.csv.zip\",\n  exdir = \"~/Downloads/jena_climate\"\n)\n\n\nLet’s look at the data.\n\n\nlibrary(tibble)\nlibrary(readr)\n\ndata_dir <- \"~/Downloads/jena_climate\"\nfname <- file.path(data_dir, \"jena_climate_2009_2016.csv\")\ndata <- read_csv(fname)\n\nglimpse(data)\n\n\nObservations: 420,551\nVariables: 15\n$ `Date Time`       <chr> \"01.01.2009 00:10:00\", \"01.01.2009 00:20:00\", \"...\n$ `p (mbar)`        <dbl> 996.52, 996.57, 996.53, 996.51, 996.51, 996.50,...\n$ `T (degC)`        <dbl> -8.02, -8.41, -8.51, -8.31, -8.27, -8.05, -7.62...\n$ `Tpot (K)`        <dbl> 265.40, 265.01, 264.91, 265.12, 265.15, 265.38,...\n$ `Tdew (degC)`     <dbl> -8.90, -9.28, -9.31, -9.07, -9.04, -8.78, -8.30...\n$ `rh (%)`          <dbl> 93.3, 93.4, 93.9, 94.2, 94.1, 94.4, 94.8, 94.4,...\n$ `VPmax (mbar)`    <dbl> 3.33, 3.23, 3.21, 3.26, 3.27, 3.33, 3.44, 3.44,...\n$ `VPact (mbar)`    <dbl> 3.11, 3.02, 3.01, 3.07, 3.08, 3.14, 3.26, 3.25,...\n$ `VPdef (mbar)`    <dbl> 0.22, 0.21, 0.20, 0.19, 0.19, 0.19, 0.18, 0.19,...\n$ `sh (g/kg)`       <dbl> 1.94, 1.89, 1.88, 1.92, 1.92, 1.96, 2.04, 2.03,...\n$ `H2OC (mmol/mol)` <dbl> 3.12, 3.03, 3.02, 3.08, 3.09, 3.15, 3.27, 3.26,...\n$ `rho (g/m**3)`    <dbl> 1307.75, 1309.80, 1310.24, 1309.19, 1309.00, 13...\n$ `wv (m/s)`        <dbl> 1.03, 0.72, 0.19, 0.34, 0.32, 0.21, 0.18, 0.19,...\n$ `max. wv (m/s)`   <dbl> 1.75, 1.50, 0.63, 0.50, 0.63, 0.63, 0.63, 0.50,...\n$ `wd (deg)`        <dbl> 152.3, 136.1, 171.6, 198.0, 214.3, 192.7, 166.5...\nHere is the plot of temperature (in degrees Celsius) over time. On this plot, you can clearly see the yearly periodicity of temperature.\n\n\nlibrary(ggplot2)\nggplot(data, aes(x = 1:nrow(data), y = `T (degC)`)) + geom_line()\n\n\n\nHere is a more narrow plot of the first 10 days of temperature data (see figure 6.15). Because the data is recorded every 10 minutes, you get 144 data points\nper day.\n\n\nggplot(data[1:1440,], aes(x = 1:1440, y = `T (degC)`)) + geom_line()\n\n\n\nOn this plot, you can see daily periodicity, especially evident for the last 4 days. Also note that this 10-day period must be coming from a fairly cold winter month.\nIf you were trying to predict average temperature for the next month given a few months of past data, the problem would be easy, due to the reliable year-scale periodicity of the data. But looking at the data over a scale of days, the temperature looks a lot more chaotic. Is this time series predictable at a daily scale? Let’s find out.\nPreparing the data\nThe exact formulation of the problem will be as follows: given data going as far back as lookback timesteps (a timestep is 10 minutes) and sampled every steps timesteps, can you predict the temperature in delay timesteps? You’ll use the following parameter values:\nlookback = 1440 — Observations will go back 10 days.\nsteps = 6 — Observations will be sampled at one data point per hour.\ndelay = 144 — Targets will be 24 hours in the future.\nTo get started, you need to do two things:\nPreprocess the data to a format a neural network can ingest. This is easy: the data is already numerical, so you don’t need to do any vectorization. But each time series in the data is on a different scale (for example, temperature is typically between -20 and +30, but atmospheric pressure, measured in mbar, is around 1,000). You’ll normalize each time series independently so that they all take small values on a similar scale.\nWrite a generator function that takes the current array of float data and yields batches of data from the recent past, along with a target temperature in the future. Because the samples in the dataset are highly redundant (sample N and sample N + 1 will have most of their timesteps in common), it would be wasteful to explicitly allocate every sample. Instead, you’ll generate the samples on the fly using the original data.\n\n\n\nNOTE: Understanding generator functions\nA generator function is a special type of function that you call repeatedly to obtain a sequence of values from. Often generators need to maintain internal state, so they are typically constructed by calling another yet another function which returns the generator function (the environment of the function which returns the generator is then used to track state).\nFor example, the sequence_generator() function below returns a generator function that yields an infinite sequence of numbers:\n\n\nsequence_generator <- function(start) {\n  value <- start - 1\n  function() {\n    value <<- value + 1\n    value\n  }\n}\n\ngen <- sequence_generator(10)\ngen()\n\n\n[1] 10\n\n\ngen()\n\n\n[1] 11\nThe current state of the generator is the value variable that is defined outside of the function. Note that superassignment (<<-) is used to update this state from within the function.\nGenerator functions can signal completion by returning the value NULL. However, generator functions passed to Keras training methods (e.g. fit_generator()) should always return values infinitely (the number of calls to the generator function is controlled by the epochs and steps_per_epoch parameters).\n\nFirst, you’ll convert the R data frame which we read earlier into a matrix of floating point values (we’ll discard the first column which included a text timestamp):\n\n\ndata <- data.matrix(data[,-1])\n\n\nYou’ll then preprocess the data by subtracting the mean of each time series and dividing by the standard deviation. You’re going to use the first 200,000 timesteps as training data, so compute the mean and standard deviation for normalization only on this fraction of the data.\n\n\ntrain_data <- data[1:200000,]\nmean <- apply(train_data, 2, mean)\nstd <- apply(train_data, 2, sd)\ndata <- scale(data, center = mean, scale = std)\n\n\nThe code for the data generator you’ll use is below. It yields a list (samples, targets), where samples is one batch of input data and targets is the corresponding array of target temperatures. It takes the following arguments:\ndata — The original array of floating-point data, which you normalized in listing 6.32.\nlookback — How many timesteps back the input data should go.\ndelay — How many timesteps in the future the target should be.\nmin_index and max_index — Indices in the data array that delimit which timesteps to draw from. This is useful for keeping a segment of the data for validation and another for testing.\nshuffle — Whether to shuffle the samples or draw them in chronological order.\nbatch_size — The number of samples per batch.\nstep — The period, in timesteps, at which you sample data. You’ll set it 6 in order to draw one data point every hour.\n\n\ngenerator <- function(data, lookback, delay, min_index, max_index,\n                      shuffle = FALSE, batch_size = 128, step = 6) {\n  if (is.null(max_index))\n    max_index <- nrow(data) - delay - 1\n  i <- min_index + lookback\n  function() {\n    if (shuffle) {\n      rows <- sample(c((min_index+lookback):max_index), size = batch_size)\n    } else {\n      if (i + batch_size >= max_index)\n        i <<- min_index + lookback\n      rows <- c(i:min(i+batch_size-1, max_index))\n      i <<- i + length(rows)\n    }\n\n    samples <- array(0, dim = c(length(rows),\n                                lookback / step,\n                                dim(data)[[-1]]))\n    targets <- array(0, dim = c(length(rows)))\n                      \n    for (j in 1:length(rows)) {\n      indices <- seq(rows[[j]] - lookback, rows[[j]]-1,\n                     length.out = dim(samples)[[2]])\n      samples[j,,] <- data[indices,]\n      targets[[j]] <- data[rows[[j]] + delay,2]\n    }           \n    list(samples, targets)\n  }\n}\n\n\nThe i variable contains the state that tracks next window of data to return, so it is updated using superassignment (e.g. i <<- i + length(rows)).\nNow, let’s use the abstract generator function to instantiate three generators: one for training, one for validation, and one for testing. Each will look at different temporal segments of the original data: the training generator looks at the first 200,000 timesteps, the validation generator looks at the following 100,000, and the test generator looks at the remainder.\n\n\nlookback <- 1440\nstep <- 6\ndelay <- 144\nbatch_size <- 128\n\ntrain_gen <- generator(\n  data,\n  lookback = lookback,\n  delay = delay,\n  min_index = 1,\n  max_index = 200000,\n  shuffle = TRUE,\n  step = step, \n  batch_size = batch_size\n)\n\nval_gen = generator(\n  data,\n  lookback = lookback,\n  delay = delay,\n  min_index = 200001,\n  max_index = 300000,\n  step = step,\n  batch_size = batch_size\n)\n\ntest_gen <- generator(\n  data,\n  lookback = lookback,\n  delay = delay,\n  min_index = 300001,\n  max_index = NULL,\n  step = step,\n  batch_size = batch_size\n)\n\n# How many steps to draw from val_gen in order to see the entire validation set\nval_steps <- (300000 - 200001 - lookback) / batch_size\n\n# How many steps to draw from test_gen in order to see the entire test set\ntest_steps <- (nrow(data) - 300001 - lookback) / batch_size\n\n\nA common-sense, non-machine-learning baseline\nBefore you start using black-box deep-learning models to solve the temperature-prediction problem, let’s try a simple, common-sense approach. It will serve as a sanity check, and it will establish a baseline that you’ll have to beat in order to demonstrate the usefulness of more-advanced machine-learning models. Such common-sense baselines can be useful when you’re approaching a new problem for which there is no known solution (yet). A classic example is that of unbalanced classification tasks, where some classes are much more common than others. If your dataset contains 90% instances of class A and 10% instances of class B, then a common-sense approach to the classification task is to always predict “A” when presented with a new sample. Such a classifier is 90% accurate overall, and any learning-based approach should therefore beat this 90% score in order to demonstrate usefulness. Sometimes, such elementary baselines can prove surprisingly hard to beat.\nIn this case, the temperature time series can safely be assumed to be continuous (the temperatures tomorrow are likely to be close to the temperatures today) as well as periodical with a daily period. Thus a common-sense approach is to always predict that the temperature 24 hours from now will be equal to the temperature right now. Let’s evaluate this approach, using the mean absolute error (MAE) metric:\n\n\nmean(abs(preds - targets))\n\n\nHere’s the evaluation loop.\n\n\nlibrary(keras)\nevaluate_naive_method <- function() {\n  batch_maes <- c()\n  for (step in 1:val_steps) {\n    c(samples, targets) %<-% val_gen()\n    preds <- samples[,dim(samples)[[2]],2]\n    mae <- mean(abs(preds - targets))\n    batch_maes <- c(batch_maes, mae)\n  }\n  print(mean(batch_maes))\n}\n\nevaluate_naive_method()\n\n\nThis yields an MAE of 0.29. Because the temperature data has been normalized to be centered on 0 and have a standard deviation of 1, this number isn’t immediately interpretable. It translates to an average absolute error of 0.29 x temperature_std degrees Celsius: 2.57˚C.\n\n\ncelsius_mae <- 0.29 * std[[2]]\n\n\nThat’s a fairly large average absolute error. Now the game is to use your knowledge of deep learning to do better.\nA basic machine-learning approach\nIn the same way that it’s useful to establish a common-sense baseline before trying machine-learning approaches, it’s useful to try simple, cheap machine-learning models (such as small, densely connected networks) before looking into complicated and computationally expensive models such as RNNs. This is the best way to make sure any further complexity you throw at the problem is legitimate and delivers real benefits.\nThe following listing shows a fully connected model that starts by flattening the data and then runs it through two dense layers. Note the lack of activation function on the last dense layer, which is typical for a regression problem. You use MAE as the loss. Because you evaluate on the exact same data and with the exact same metric you did with the common-sense approach, the results will be directly comparable.\n\n\nlibrary(keras)\n\nmodel <- keras_model_sequential() %>% \n  layer_flatten(input_shape = c(lookback / step, dim(data)[-1])) %>% \n  layer_dense(units = 32, activation = \"relu\") %>% \n  layer_dense(units = 1)\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(),\n  loss = \"mae\"\n)\n\nhistory <- model %>% fit_generator(\n  train_gen,\n  steps_per_epoch = 500,\n  epochs = 20,\n  validation_data = val_gen,\n  validation_steps = val_steps\n)\n\n\nLet’s display the loss curves for validation and training.\n\nSome of the validation losses are close to the no-learning baseline, but not reliably. This goes to show the merit of having this baseline in the first place: it turns out to be not easy to outperform. Your common sense contains a lot of valuable information that a machine-learning model doesn’t have access to.\nYou may wonder, if a simple, well-performing model exists to go from the data to the targets (the common-sense baseline), why doesn’t the model you’re training find it and improve on it? Because this simple solution isn’t what your training setup is looking for. The space of models in which you’re searching for a solution – that is, your hypothesis space – is the space of all possible two-layer networks with the configuration you defined. These networks are already fairly complicated. When you’re looking for a solution with a space of complicated models, the simple, well-performing baseline may be unlearnable, even if it’s technically part of the hypothesis space. That is a pretty significant limitation of machine learning in general: unless the learning algorithm is hardcoded to look for a specific kind of simple model, parameter learning can sometimes fail to find a simple solution to a simple problem.\nA first recurrent baseline\nThe first fully connected approach didn’t do well, but that doesn’t mean machine learning isn’t applicable to this problem. The previous approach first flattened the time series, which removed the notion of time from the input data. Let’s instead look at the data as what it is: a sequence, where causality and order matter. You’ll try a recurrent-sequence processing model – it should be the perfect fit for such sequence data, precisely because it exploits the temporal ordering of data points, unlike the first approach.\nInstead of the LSTM layer introduced in the previous section, you’ll use the GRU layer, developed by Chung et al. in 2014. Gated recurrent unit (GRU) layers work using the same principle as LSTM, but they’re somewhat streamlined and thus cheaper to run (although they may not have as much representational power as LSTM). This trade-off between computational expensiveness and representational power is seen everywhere in machine learning.\n\n\nmodel <- keras_model_sequential() %>% \n  layer_gru(units = 32, input_shape = list(NULL, dim(data)[[-1]])) %>% \n  layer_dense(units = 1)\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(),\n  loss = \"mae\"\n)\n\nhistory <- model %>% fit_generator(\n  train_gen,\n  steps_per_epoch = 500,\n  epochs = 20,\n  validation_data = val_gen,\n  validation_steps = val_steps\n)\n\n\nThe results are plotted below. Much better! You can significantly beat the common-sense baseline, demonstrating the value of machine learning as well as the superiority of recurrent networks compared to sequence-flattening dense networks on this type of task.\n\nThe new validation MAE of ~0.265 (before you start significantly overfitting) translates to a mean absolute error of 2.35˚C after denormalization. That’s a solid gain on the initial error of 2.57˚C, but you probably still have a bit of a margin for improvement.\nUsing recurrent dropout to fight overfitting\nIt’s evident from the training and validation curves that the model is overfitting: the training and validation losses start to diverge considerably after a few epochs. You’re already familiar with a classic technique for fighting this phenomenon: dropout, which randomly zeros out input units of a layer in order to break happenstance correlations in the training data that the layer is exposed to. But how to correctly apply dropout in recurrent networks isn’t a trivial question. It has long been known that applying dropout before a recurrent layer hinders learning rather than helping with regularization. In 2015, Yarin Gal, as part of his PhD thesis on Bayesian deep learning, determined the proper way to use dropout with a recurrent network: the same dropout mask (the same pattern of dropped units) should be applied at every timestep, instead of a dropout mask that varies randomly from timestep to timestep. What’s more, in order to regularize the representations formed by the recurrent gates of layers such as layer_gru and layer_lstm, a temporally constant dropout mask should be applied to the inner recurrent activations of the layer (a recurrent dropout mask). Using the same dropout mask at every timestep allows the network to properly propagate its learning error through time; a temporally random dropout mask would disrupt this error signal and be harmful to the learning process.\nYarin Gal did his research using Keras and helped build this mechanism directly into Keras recurrent layers. Every recurrent layer in Keras has two dropout-related arguments: dropout, a float specifying the dropout rate for input units of the layer, and recurrent_dropout, specifying the dropout rate of the recurrent units. Let’s add dropout and recurrent dropout to the layer_gru and see how doing so impacts overfitting. Because networks being regularized with dropout always take longer to fully converge, you’ll train the network for twice as many epochs.\n\n\nmodel <- keras_model_sequential() %>% \n  layer_gru(units = 32, dropout = 0.2, recurrent_dropout = 0.2,\n            input_shape = list(NULL, dim(data)[[-1]])) %>% \n  layer_dense(units = 1)\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(),\n  loss = \"mae\"\n)\n\nhistory <- model %>% fit_generator(\n  train_gen,\n  steps_per_epoch = 500,\n  epochs = 40,\n  validation_data = val_gen,\n  validation_steps = val_steps\n)\n\n\nThe plot below shows the results. Success! You’re no longer overfitting during the first 20 epochs. But although you have more stable evaluation scores, your best scores aren’t much lower than they were previously.\n\nStacking recurrent layers\nBecause you’re no longer overfitting but seem to have hit a performance bottleneck, you should consider increasing the capacity of the network. Recall the description of the universal machine-learning workflow: it’s generally a good idea to increase the capacity of your network until overfitting becomes the primary obstacle (assuming you’re already taking basic steps to mitigate overfitting, such as using dropout). As long as you aren’t overfitting too badly, you’re likely under capacity.\nIncreasing network capacity is typically done by increasing the number of units in the layers or adding more layers. Recurrent layer stacking is a classic way to build more-powerful recurrent networks: for instance, what currently powers the Google Translate algorithm is a stack of seven large LSTM layers – that’s huge.\nTo stack recurrent layers on top of each other in Keras, all intermediate layers should return their full sequence of outputs (a 3D tensor) rather than their output at the last timestep. This is done by specifying return_sequences = TRUE.\n\n\nmodel <- keras_model_sequential() %>% \n  layer_gru(units = 32, \n            dropout = 0.1, \n            recurrent_dropout = 0.5,\n            return_sequences = TRUE,\n            input_shape = list(NULL, dim(data)[[-1]])) %>% \n  layer_gru(units = 64, activation = \"relu\",\n            dropout = 0.1,\n            recurrent_dropout = 0.5) %>% \n  layer_dense(units = 1)\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(),\n  loss = \"mae\"\n)\n\nhistory <- model %>% fit_generator(\n  train_gen,\n  steps_per_epoch = 500,\n  epochs = 40,\n  validation_data = val_gen,\n  validation_steps = val_steps\n)\n\n\nThe figure below shows the results. You can see that the added layer does improve the results a bit, though not significantly. You can draw two conclusions:\nBecause you’re still not overfitting too badly, you could safely increase the size of your layers in a quest for validation-loss improvement. This has a non-negligible computational cost, though.\nAdding a layer didn’t help by a significant factor, so you may be seeing diminishing returns from increasing network capacity at this point.\n\nUsing bidirectional RNNs\nThe last technique introduced in this section is called bidirectional RNNs. A bidirectional RNN is a common RNN variant that can offer greater performance than a regular RNN on certain tasks. It’s frequently used in natural-language processing – you could call it the Swiss Army knife of deep learning for natural-language processing.\nRNNs are notably order dependent, or time dependent: they process the timesteps of their input sequences in order, and shuffling or reversing the timesteps can completely change the representations the RNN extracts from the sequence. This is precisely the reason they perform well on problems where order is meaningful, such as the temperature-forecasting problem. A bidirectional RNN exploits the order sensitivity of RNNs: it consists of using two regular RNNs, such as the layer_gru and layer_lstm you’re already familiar with, each of which processes the input sequence in one direction (chronologically and antichronologically), and then merging their representations. By processing a sequence both ways, a bidirectional RNN can catch patterns that may be overlooked by a unidirectional RNN.\nRemarkably, the fact that the RNN layers in this section have processed sequences in chronological order (older timesteps first) may have been an arbitrary decision. At least, it’s a decision we made no attempt to question so far. Could the RNNs have performed well enough if they processed input sequences in antichronological order, for instance (newer timesteps first)? Let’s try this in practice and see what happens. All you need to do is write a variant of the data generator where the input sequences are reverted along the time dimension (replace the last line with list(samples[,ncol(samples):1,], targets)). Training the same one-GRU-layer network that you used in the first experiment in this section, you get the results shown below.\n\nThe reversed-order GRU underperforms even the common-sense baseline, indicating that in this case, chronological processing is important to the success of your approach. This makes perfect sense: the underlying GRU layer will typically be better at remembering the recent past than the distant past, and naturally the more recent weather data points are more predictive than older data points for the problem (that’s what makes the common-sense baseline fairly strong). Thus the chronological version of the layer is bound to outperform the reversed-order version. Importantly, this isn’t true for many other problems, including natural language: intuitively, the importance of a word in understanding a sentence isn’t usually dependent on its position in the sentence. Let’s try the same trick on the LSTM IMDB example from section 6.2.\n\nlibrary(keras)\n\n# Number of words to consider as features\nmax_features <- 10000  \n\n# Cuts off texts after this number of words\nmaxlen <- 500\n\nimdb <- dataset_imdb(num_words = max_features)\nc(c(x_train, y_train), c(x_test, y_test)) %<-% imdb\n\n# Reverses sequences\nx_train <- lapply(x_train, rev)\nx_test <- lapply(x_test, rev) \n\n# Pads sequences\nx_train <- pad_sequences(x_train, maxlen = maxlen)  <4>\nx_test <- pad_sequences(x_test, maxlen = maxlen)\n\nmodel <- keras_model_sequential() %>% \n  layer_embedding(input_dim = max_features, output_dim = 128) %>% \n  layer_lstm(units = 32) %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %>% compile(\n  optimizer = \"rmsprop\",\n  loss = \"binary_crossentropy\",\n  metrics = c(\"acc\")\n)\n  \nhistory <- model %>% fit(\n  x_train, y_train,\n  epochs = 10,\n  batch_size = 128,\n  validation_split = 0.2\n)\n\nYou get performance nearly identical to that of the chronological-order LSTM. Remarkably, on such a text dataset, reversed-order processing works just as well as chronological processing, confirming the\nhypothesis that, although word order does matter in understanding language, which order you use isn’t crucial. Importantly, an RNN trained on reversed sequences will learn different representations than one trained on the original sequences, much as you would have different mental models if time flowed backward in the real world – if you lived a life where you died on your first day and were born on your last day. In machine learning, representations that are different yet useful are always worth exploiting, and the more they differ, the better: they offer a new angle from which to look at your data, capturing aspects of the data that were missed by other approaches, and thus they can help boost performance on a task. This is the intuition behind ensembling, a concept we’ll explore in chapter 7.\nA bidirectional RNN exploits this idea to improve on the performance of chronological-order RNNs. It looks at its input sequence both ways, obtaining potentially richer representations and capturing patterns that may have been missed by the chronological-order version alone.\n\nTo instantiate a bidirectional RNN in Keras, you use the bidirectional() function, which takes a recurrent layer instance as an argument. The bidirectional() function creates a second, separate instance of this recurrent layer and uses one instance for processing the input sequences in chronological order and the other instance for processing the input sequences in reversed order. Let’s try it on the IMDB sentiment-analysis task.\n\n\nmodel <- keras_model_sequential() %>% \n  layer_embedding(input_dim = max_features, output_dim = 32) %>% \n  bidirectional(\n    layer_lstm(units = 32)\n  ) %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %>% compile(\n  optimizer = \"rmsprop\",\n  loss = \"binary_crossentropy\",\n  metrics = c(\"acc\")\n)\n\nhistory <- model %>% fit(\n  x_train, y_train,\n  epochs = 10,\n  batch_size = 128,\n  validation_split = 0.2\n)\n\n\nIt performs slightly better than the regular LSTM you tried in the previous section, achieving over 89% validation accuracy. It also seems to overfit more quickly, which is unsurprising because a bidirectional layer has twice as many parameters as a chronological LSTM. With some regularization, the bidirectional approach would likely be a strong performer on this task.\nNow let’s try the same approach on the temperature prediction task.\n\n\nmodel <- keras_model_sequential() %>% \n  bidirectional(\n    layer_gru(units = 32), input_shape = list(NULL, dim(data)[[-1]])\n  ) %>% \n  layer_dense(units = 1)\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(),\n  loss = \"mae\"\n)\n\nhistory <- model %>% fit_generator(\n  train_gen,\n  steps_per_epoch = 500,\n  epochs = 40,\n  validation_data = val_gen,\n  validation_steps = val_steps\n)\n\n\nThis performs about as well as the regular layer_gru. It’s easy to understand why: all the predictive capacity must come from the chronological half of the network, because the antichronological half is known to be severely underperforming on this task (again, because the recent past matters much more than the distant past in this case).\nGoing even further\nThere are many other things you could try, in order to improve performance on the temperature-forecasting problem:\nAdjust the number of units in each recurrent layer in the stacked setup. The current choices are largely arbitrary and thus probably suboptimal.\nAdjust the learning rate used by the RMSprop optimizer.\nTry using layer_lstm instead of layer_gru.\nTry using a bigger densely connected regressor on top of the recurrent layers: that is, a bigger dense layer or even a stack of dense layers.\nDon’t forget to eventually run the best-performing models (in terms of validation MAE) on the test set! Otherwise, you’ll develop architectures that are overfitting to the validation set.\nAs always, deep learning is more an art than a science. We can provide guidelines that suggest what is likely to work or not work on a given problem, but, ultimately, every problem is unique; you’ll have to evaluate different strategies empirically. There is currently no theory that will tell you in advance precisely what you should do to optimally solve a problem. You must iterate.\nWrapping up\nHere’s what you should take away from this section:\nAs you first learned in chapter 4, when approaching a new problem, it’s good to first establish common-sense baselines for your metric of choice. If you don’t have a baseline to beat, you can’t tell whether you’re making real progress.\nTry simple models before expensive ones, to justify the additional expense. Sometimes a simple model will turn out to be your best option.\nWhen you have data where temporal ordering matters, recurrent networks are a great fit and easily outperform models that first flatten the temporal data.\nTo use dropout with recurrent networks, you should use a time-constant dropout mask and recurrent dropout mask. These are built into Keras recurrent layers, so all you have to do is use the dropout and recurrent_dropout arguments of recurrent layers.\nStacked RNNs provide more representational power than a single RNN layer. They’re also much more expensive and thus not always worth it. Although they offer clear gains on complex problems (such as machine translation), they may not always be relevant to smaller, simpler problems.\nBidirectional RNNs, which look at a sequence both ways, are useful on natural-language processing problems. But they aren’t strong performers on sequence data where the recent past is much more informative than the beginning of the sequence.\n\nNOTE: Markets and machine learning\nSome readers are bound to want to take the techniques we’ve introduced here and try them on the problem of forecasting the future price of securities on the stock market (or currency exchange rates, and so on). Markets have very different statistical characteristics than natural phenomena such as weather patterns. Trying to use machine learning to beat markets, when you only have access to publicly available data, is a difficult endeavor, and you’re likely to waste your time and resources with nothing to show for it.\nAlways remember that when it comes to markets, past performance is not a good predictor of future returns – looking in the rear-view mirror is a bad way to drive. Machine learning, on the other hand, is applicable to datasets where the past is a good predictor of the future.\n\n\n\n\n",
    "preview": "posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks/images/jena_temp-r.png",
    "last_modified": "2024-11-21T15:51:11+00:00",
    "input_file": {},
    "preview_width": 6000,
    "preview_height": 4000
  },
  {
    "path": "posts/2017-12-14-image-classification-on-small-datasets/",
    "title": "Image Classification on Small Datasets with Keras",
    "description": "Having to train an image-classification model using very little data is a common situation, in this article we review three techniques for tackling this problem including feature extraction and fine tuning from a pretrained network.",
    "author": [
      {
        "name": "François Chollet",
        "url": "https://github.com/fchollet"
      },
      {
        "name": "J.J. Allaire",
        "url": "https://github.com/jjallaire"
      }
    ],
    "date": "2017-12-14",
    "categories": [
      "TensorFlow/Keras",
      "Image Recognition & Image Processing"
    ],
    "contents": "\nTraining a convnet with a small dataset\nHaving to train an image-classification model using very little data is a common situation, which you’ll likely encounter in practice if you ever do computer vision in a professional context. A “few” samples can mean anywhere from a few hundred to a few tens of thousands of images. As a practical example, we’ll focus on classifying images as dogs or cats, in a dataset containing 4,000 pictures of cats and dogs (2,000 cats, 2,000 dogs). We’ll use 2,000 pictures for training – 1,000 for validation, and 1,000 for testing.\nIn Chapter 5 of the Deep Learning with R book we review three techniques for tackling this problem. The first of these is training a small model from scratch on what little data you have (which achieves an accuracy of 82%). Subsequently we use feature extraction with a pretrained network (resulting in an accuracy of 90%) and fine-tuning a pretrained network (with a final accuracy of 97%). In this post we’ll cover only the second and third techniques.\nThe relevance of deep learning for small-data problems\nYou’ll sometimes hear that deep learning only works when lots of data is available. This is valid in part: one fundamental characteristic of deep learning is that it can find interesting features in the training data on its own, without any need for manual feature engineering, and this can only be achieved when lots of training examples are available. This is especially true for problems where the input samples are very high-dimensional, like images.\nBut what constitutes lots of samples is relative – relative to the size and depth of the network you’re trying to train, for starters. It isn’t possible to train a convnet to solve a complex problem with just a few tens of samples, but a few hundred can potentially suffice if the model is small and well regularized and the task is simple. Because convnets learn local, translation-invariant features, they’re highly data efficient on perceptual problems. Training a convnet from scratch on a very small image dataset will still yield reasonable results despite a relative lack of data, without the need for any custom feature engineering. You’ll see this in action in this section.\nWhat’s more, deep-learning models are by nature highly repurposable: you can take, say, an image-classification or speech-to-text model trained on a large-scale dataset and reuse it on a significantly different problem with only minor changes. Specifically, in the case of computer vision, many pretrained models (usually trained on the ImageNet dataset) are now publicly available for download and can be used to bootstrap powerful vision models out of very little data. That’s what you’ll do in the next section. Let’s start by getting your hands on the data.\nDownloading the data\nThe Dogs vs. Cats dataset that you’ll use isn’t packaged with Keras. It was made available by Kaggle as part of a computer-vision competition in late 2013, back when convnets weren’t mainstream. You can download the original dataset from https://www.kaggle.com/c/dogs-vs-cats/data (you’ll need to create a Kaggle account if you don’t already have one – don’t worry, the process is painless).\nThe pictures are medium-resolution color JPEGs. Here are some examples:\n\nUnsurprisingly, the dogs-versus-cats Kaggle competition in 2013 was won by entrants who used convnets. The best entries achieved up to 95% accuracy. Below you’ll end up with a 97% accuracy, even though you’ll train your models on less than 10% of the data that was available to the competitors.\nThis dataset contains 25,000 images of dogs and cats (12,500 from each class) and is 543 MB (compressed). After downloading and uncompressing it, you’ll create a new dataset containing three subsets: a training set with 1,000 samples of each class, a validation set with 500 samples of each class, and a test set with 500 samples of each class.\nFollowing is the code to do this:\n\n\noriginal_dataset_dir <- \"~/Downloads/kaggle_original_data\"\n\nbase_dir <- \"~/Downloads/cats_and_dogs_small\"\ndir.create(base_dir)\n\ntrain_dir <- file.path(base_dir, \"train\")\ndir.create(train_dir)\nvalidation_dir <- file.path(base_dir, \"validation\")\ndir.create(validation_dir)\ntest_dir <- file.path(base_dir, \"test\")\ndir.create(test_dir)\n\ntrain_cats_dir <- file.path(train_dir, \"cats\")\ndir.create(train_cats_dir)\n\ntrain_dogs_dir <- file.path(train_dir, \"dogs\")\ndir.create(train_dogs_dir)\n\nvalidation_cats_dir <- file.path(validation_dir, \"cats\")\ndir.create(validation_cats_dir)\n\nvalidation_dogs_dir <- file.path(validation_dir, \"dogs\")\ndir.create(validation_dogs_dir)\n\ntest_cats_dir <- file.path(test_dir, \"cats\")\ndir.create(test_cats_dir)\n\ntest_dogs_dir <- file.path(test_dir, \"dogs\")\ndir.create(test_dogs_dir)\n\nfnames <- paste0(\"cat.\", 1:1000, \".jpg\")\nfile.copy(file.path(original_dataset_dir, fnames), \n          file.path(train_cats_dir)) \n\nfnames <- paste0(\"cat.\", 1001:1500, \".jpg\")\nfile.copy(file.path(original_dataset_dir, fnames), \n          file.path(validation_cats_dir))\n\nfnames <- paste0(\"cat.\", 1501:2000, \".jpg\")\nfile.copy(file.path(original_dataset_dir, fnames),\n          file.path(test_cats_dir))\n\nfnames <- paste0(\"dog.\", 1:1000, \".jpg\")\nfile.copy(file.path(original_dataset_dir, fnames),\n          file.path(train_dogs_dir))\n\nfnames <- paste0(\"dog.\", 1001:1500, \".jpg\")\nfile.copy(file.path(original_dataset_dir, fnames),\n          file.path(validation_dogs_dir)) \n\nfnames <- paste0(\"dog.\", 1501:2000, \".jpg\")\nfile.copy(file.path(original_dataset_dir, fnames),\n          file.path(test_dogs_dir))\n\n\nUsing a pretrained convnet\nA common and highly effective approach to deep learning on small image datasets is to use a pretrained network. A pretrained network is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task. If this original dataset is large enough and general enough, then the spatial hierarchy of features learned by the pretrained network can effectively act as a generic model of the visual world, and hence its features can prove useful for many different computer-vision problems, even though these new problems may involve completely different classes than those of the original task. For instance, you might train a network on ImageNet (where classes are mostly animals and everyday objects) and then repurpose this trained network for something as remote as identifying furniture items in images. Such portability of learned features across different problems is a key advantage of deep learning compared to many older, shallow-learning approaches, and it makes deep learning very effective for small-data problems.\nIn this case, let’s consider a large convnet trained on the ImageNet dataset (1.4 million labeled images and 1,000 different classes). ImageNet contains many animal classes, including different species of cats and dogs, and you can thus expect to perform well on the dogs-versus-cats classification problem.\nYou’ll use the VGG16 architecture, developed by Karen Simonyan and Andrew Zisserman in 2014; it’s a simple and widely used convnet architecture for ImageNet. Although it’s an older model, far from the current state of the art and somewhat heavier than many other recent models, I chose it because its architecture is similar to what you’re already familiar with and is easy to understand without introducing any new concepts. This may be your first encounter with one of these cutesy model names – VGG, ResNet, Inception, Inception-ResNet, Xception, and so on; you’ll get used to them, because they will come up frequently if you keep doing deep learning for computer vision.\nThere are two ways to use a pretrained network: feature extraction and fine-tuning. We’ll cover both of them. Let’s start with feature extraction.\nFeature extraction\nFeature extraction consists of using the representations learned by a previous network to extract interesting features from new samples. These features are then run through a new classifier, which is trained from scratch.\nAs you saw previously, convnets used for image classification comprise two parts: they start with a series of pooling and convolution layers, and they end with a densely connected classifier. The first part is called the convolutional base of the model. In the case of convnets, feature extraction consists of taking the convolutional base of a previously trained network, running the new data through it, and training a new classifier on top of the output.\n\nWhy only reuse the convolutional base? Could you reuse the densely connected classifier as well? In general, doing so should be avoided. The reason is that the representations learned by the convolutional base are likely to be more generic and therefore more reusable: the feature maps of a convnet are presence maps of generic concepts over a picture, which is likely to be useful regardless of the computer-vision problem at hand. But the representations learned by the classifier will necessarily be specific to the set of classes on which the model was trained – they will only contain information about the presence probability of this or that class in the entire picture. Additionally, representations found in densely connected layers no longer contain any information about where objects are located in the input image: these layers get rid of the notion of space, whereas the object location is still described by convolutional feature maps. For problems where object location matters, densely connected features are largely useless.\nNote that the level of generality (and therefore reusability) of the representations extracted by specific convolution layers depends on the depth of the layer in the model. Layers that come earlier in the model extract local, highly generic feature maps (such as visual edges, colors, and textures), whereas layers that are higher up extract more-abstract concepts (such as “cat ear” or “dog eye”). So if your new dataset differs a lot from the dataset on which the original model was trained, you may be better off using only the first few layers of the model to do feature extraction, rather than using the entire convolutional base.\nIn this case, because the ImageNet class set contains multiple dog and cat classes, it’s likely to be beneficial to reuse the information contained in the densely connected layers of the original model. But we’ll choose not to, in order to cover the more general case where the class set of the new problem doesn’t overlap the class set of the original model.\nLet’s put this in practice by using the convolutional base of the VGG16 network, trained on ImageNet, to extract interesting features from cat and dog images, and then train a dogs-versus-cats classifier on top of these features.\nThe VGG16 model, among others, comes prepackaged with Keras. Here’s the list of image-classification models (all pretrained on the ImageNet dataset) that are available as part of Keras:\nXception\nInception V3\nResNet50\nVGG16\nVGG19\nMobileNet\nLet’s instantiate the VGG16 model.\n\n\nlibrary(keras)\n\nconv_base <- application_vgg16(\n  weights = \"imagenet\",\n  include_top = FALSE,\n  input_shape = c(150, 150, 3)\n)\n\n\nYou pass three arguments to the function:\nweights specifies the weight checkpoint from which to initialize the model.\ninclude_top refers to including (or not) the densely connected classifier on top of the network. By default, this densely connected classifier corresponds to the 1,000 classes from ImageNet. Because you intend to use your own densely connected classifier (with only two classes: cat and dog), you don’t need to include it.\ninput_shape is the shape of the image tensors that you’ll feed to the network. This argument is purely optional: if you don’t pass it, the network will be able to process inputs of any size.\nHere’s the detail of the architecture of the VGG16 convolutional base. It’s similar to the simple convnets you’re already familiar with:\n\n\nsummary(conv_base)\n\n\nLayer (type)                     Output Shape          Param #  \n================================================================\ninput_1 (InputLayer)             (None, 150, 150, 3)   0       \n________________________________________________________________\nblock1_conv1 (Convolution2D)     (None, 150, 150, 64)  1792     \n________________________________________________________________\nblock1_conv2 (Convolution2D)     (None, 150, 150, 64)  36928    \n________________________________________________________________\nblock1_pool (MaxPooling2D)       (None, 75, 75, 64)    0        \n________________________________________________________________\nblock2_conv1 (Convolution2D)     (None, 75, 75, 128)   73856    \n________________________________________________________________\nblock2_conv2 (Convolution2D)     (None, 75, 75, 128)   147584   \n________________________________________________________________\nblock2_pool (MaxPooling2D)       (None, 37, 37, 128)   0        \n________________________________________________________________\nblock3_conv1 (Convolution2D)     (None, 37, 37, 256)   295168   \n________________________________________________________________\nblock3_conv2 (Convolution2D)     (None, 37, 37, 256)   590080   \n________________________________________________________________\nblock3_conv3 (Convolution2D)     (None, 37, 37, 256)   590080   \n________________________________________________________________\nblock3_pool (MaxPooling2D)       (None, 18, 18, 256)   0        \n________________________________________________________________\nblock4_conv1 (Convolution2D)     (None, 18, 18, 512)   1180160  \n________________________________________________________________\nblock4_conv2 (Convolution2D)     (None, 18, 18, 512)   2359808  \n________________________________________________________________\nblock4_conv3 (Convolution2D)     (None, 18, 18, 512)   2359808  \n________________________________________________________________\nblock4_pool (MaxPooling2D)       (None, 9, 9, 512)     0        \n________________________________________________________________\nblock5_conv1 (Convolution2D)     (None, 9, 9, 512)     2359808  \n________________________________________________________________\nblock5_conv2 (Convolution2D)     (None, 9, 9, 512)     2359808  \n________________________________________________________________\nblock5_conv3 (Convolution2D)     (None, 9, 9, 512)     2359808  \n________________________________________________________________\nblock5_pool (MaxPooling2D)       (None, 4, 4, 512)     0        \n================================================================\nTotal params: 14,714,688\nTrainable params: 14,714,688\nNon-trainable params: 0\nThe final feature map has shape (4, 4, 512). That’s the feature on top of which you’ll stick a densely connected classifier.\nAt this point, there are two ways you could proceed:\nRunning the convolutional base over your dataset, recording its output to an array on disk, and then using this data as input to a standalone, densely connected classifier similar to those you saw in part 1 of this book. This solution is fast and cheap to run, because it only requires running the convolutional base once for every input image, and the convolutional base is by far the most expensive part of the pipeline. But for the same reason, this technique won’t allow you to use data augmentation.\nExtending the model you have (conv_base) by adding dense layers on top, and running the whole thing end to end on the input data. This will allow you to use data augmentation, because every input image goes through the convolutional base every time it’s seen by the model. But for the same reason, this technique is far more expensive than the first.\nIn this post we’ll cover the second technique in detail (in the book we cover both). Note that this technique is so expensive that you should only attempt it if you have access to a GPU – it’s absolutely intractable on a CPU.\nFeature extraction with data augmentation\nBecause models behave just like layers, you can add a model (like conv_base) to a sequential model just like you would add a layer.\n\n\nmodel <- keras_model_sequential() %>% \n  conv_base %>% \n  layer_flatten() %>% \n  layer_dense(units = 256, activation = \"relu\") %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\n\nThis is what the model looks like now:\n\n\nsummary(model)\n\n\nLayer (type)                     Output Shape          Param #  \n================================================================\nvgg16 (Model)                    (None, 4, 4, 512)     14714688                                     \n________________________________________________________________\nflatten_1 (Flatten)              (None, 8192)          0        \n________________________________________________________________\ndense_1 (Dense)                  (None, 256)           2097408  \n________________________________________________________________\ndense_2 (Dense)                  (None, 1)             257      \n================================================================\nTotal params: 16,812,353\nTrainable params: 16,812,353\nNon-trainable params: 0\nAs you can see, the convolutional base of VGG16 has 14,714,688 parameters, which is very large. The classifier you’re adding on top has 2 million parameters.\nBefore you compile and train the model, it’s very important to freeze the convolutional base. Freezing a layer or set of layers means preventing their weights from being updated during training. If you don’t do this, then the representations that were previously learned by the convolutional base will be modified during training. Because the dense layers on top are randomly initialized, very large weight updates would be propagated through the network, effectively destroying the representations previously learned.\nIn Keras, you freeze a network using the freeze_weights() function:\n\n\nlength(model$trainable_weights)\n\n\n[1] 30\n\n\nfreeze_weights(conv_base)\nlength(model$trainable_weights)\n\n\n[1] 4\nWith this setup, only the weights from the two dense layers that you added will be trained. That’s a total of four weight tensors: two per layer (the main weight matrix and the bias vector). Note that in order for these changes to take effect, you must first compile the model. If you ever modify weight trainability after compilation, you should then recompile the model, or these changes will be ignored.\nUsing data augmentation\nOverfitting is caused by having too few samples to learn from, rendering you unable to train a model that can generalize to new data. Given infinite data, your model would be exposed to every possible aspect of the data distribution at hand: you would never overfit. Data augmentation takes the approach of generating more training data from existing training samples, by augmenting the samples via a number of random transformations that yield believable-looking images. The goal is that at training time, your model will never see the exact same picture twice. This helps expose the model to more aspects of the data and generalize better.\nIn Keras, this can be done by configuring a number of random transformations to be performed on the images read by an image_data_generator(). For example:\n\n\ntrain_datagen = image_data_generator(\n  rescale = 1/255,\n  rotation_range = 40,\n  width_shift_range = 0.2,\n  height_shift_range = 0.2,\n  shear_range = 0.2,\n  zoom_range = 0.2,\n  horizontal_flip = TRUE,\n  fill_mode = \"nearest\"\n)\n\n\nThese are just a few of the options available (for more, see the Keras documentation). Let’s quickly go over this code:\nrotation_range is a value in degrees (0–180), a range within which to randomly rotate pictures.\nwidth_shift and height_shift are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally.\nshear_range is for randomly applying shearing transformations.\nzoom_range is for randomly zooming inside pictures.\nhorizontal_flip is for randomly flipping half the images horizontally – relevant when there are no assumptions of horizontal asymmetry (for example, real-world pictures).\nfill_mode is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.\nNow we can train our model using the image data generator:\n\n\n# Note that the validation data shouldn't be augmented!\ntest_datagen <- image_data_generator(rescale = 1/255)  \n\ntrain_generator <- flow_images_from_directory(\n  train_dir,                  # Target directory  \n  train_datagen,              # Data generator\n  target_size = c(150, 150),  # Resizes all images to 150 × 150\n  batch_size = 20,\n  class_mode = \"binary\"       # binary_crossentropy loss for binary labels\n)\n\nvalidation_generator <- flow_images_from_directory(\n  validation_dir,\n  test_datagen,\n  target_size = c(150, 150),\n  batch_size = 20,\n  class_mode = \"binary\"\n)\n\nmodel %>% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = optimizer_rmsprop(lr = 2e-5),\n  metrics = c(\"accuracy\")\n)\n\nhistory <- model %>% fit_generator(\n  train_generator,\n  steps_per_epoch = 100,\n  epochs = 30,\n  validation_data = validation_generator,\n  validation_steps = 50\n)\n\n\nLet’s plot the results. As you can see, you reach a validation accuracy of about 90%.\n\nFine-tuning\nAnother widely used technique for model reuse, complementary to feature extraction, is fine-tuning\nFine-tuning consists of unfreezing a few of the top layers of a frozen model base used for feature extraction, and jointly training both the newly added part of the model (in this case, the fully connected classifier) and these top layers. This is called fine-tuning because it slightly adjusts the more abstract\nrepresentations of the model being reused, in order to make them more relevant for the problem at hand.\n\nI stated earlier that it’s necessary to freeze the convolution base of VGG16 in order to be able to train a randomly initialized classifier on top. For the same reason, it’s only possible to fine-tune the top layers of the convolutional base once the classifier on top has already been trained. If the classifier isn’t already trained, then the error signal propagating through the network during training will be too large, and the representations previously learned by the layers being fine-tuned will be destroyed. Thus the steps for fine-tuning a network are as follows:\nAdd your custom network on top of an already-trained base network.\nFreeze the base network.\nTrain the part you added.\nUnfreeze some layers in the base network.\nJointly train both these layers and the part you added.\nYou already completed the first three steps when doing feature extraction. Let’s proceed with step 4: you’ll unfreeze your conv_base and then freeze individual layers inside it.\nAs a reminder, this is what your convolutional base looks like:\n\n\nsummary(conv_base)\n\n\nLayer (type)                     Output Shape          Param #  \n================================================================\ninput_1 (InputLayer)             (None, 150, 150, 3)   0        \n________________________________________________________________\nblock1_conv1 (Convolution2D)     (None, 150, 150, 64)  1792     \n________________________________________________________________\nblock1_conv2 (Convolution2D)     (None, 150, 150, 64)  36928    \n________________________________________________________________\nblock1_pool (MaxPooling2D)       (None, 75, 75, 64)    0        \n________________________________________________________________\nblock2_conv1 (Convolution2D)     (None, 75, 75, 128)   73856    \n________________________________________________________________\nblock2_conv2 (Convolution2D)     (None, 75, 75, 128)   147584   \n________________________________________________________________\nblock2_pool (MaxPooling2D)       (None, 37, 37, 128)   0        \n________________________________________________________________\nblock3_conv1 (Convolution2D)     (None, 37, 37, 256)   295168   \n________________________________________________________________\nblock3_conv2 (Convolution2D)     (None, 37, 37, 256)   590080   \n________________________________________________________________\nblock3_conv3 (Convolution2D)     (None, 37, 37, 256)   590080   \n________________________________________________________________\nblock3_pool (MaxPooling2D)       (None, 18, 18, 256)   0        \n________________________________________________________________\nblock4_conv1 (Convolution2D)     (None, 18, 18, 512)   1180160  \n________________________________________________________________\nblock4_conv2 (Convolution2D)     (None, 18, 18, 512)   2359808  \n________________________________________________________________\nblock4_conv3 (Convolution2D)     (None, 18, 18, 512)   2359808  \n________________________________________________________________\nblock4_pool (MaxPooling2D)       (None, 9, 9, 512)     0        \n________________________________________________________________\nblock5_conv1 (Convolution2D)     (None, 9, 9, 512)     2359808  \n________________________________________________________________\nblock5_conv2 (Convolution2D)     (None, 9, 9, 512)     2359808  \n________________________________________________________________\nblock5_conv3 (Convolution2D)     (None, 9, 9, 512)     2359808  \n________________________________________________________________\nblock5_pool (MaxPooling2D)       (None, 4, 4, 512)     0        \n================================================================\nTotal params: 14714688\nYou’ll fine-tune all of the layers from block3_conv1 and on. Why not fine-tune the entire convolutional base? You could. But you need to consider the following:\nEarlier layers in the convolutional base encode more-generic, reusable features, whereas layers higher up encode more-specialized features. It’s more useful to fine-tune the more specialized features, because these are the ones that need to be repurposed on your new problem. There would be fast-decreasing returns in fine-tuning lower layers.\nThe more parameters you’re training, the more you’re at risk of overfitting. The convolutional base has 15 million parameters, so it would be risky to attempt to train it on your small dataset.\nThus, in this situation, it’s a good strategy to fine-tune only some of the layers in the convolutional base. Let’s set this up, starting from where you left off in the previous example.\n\n\nunfreeze_weights(conv_base, from = \"block3_conv1\")\n\n\nNow you can begin fine-tuning the network. You’ll do this with the RMSProp optimizer, using a very low learning rate. The reason for using a low learning rate is that you want to limit the magnitude of the modifications you make to the representations of the three layers you’re fine-tuning. Updates that are too large may harm these representations.\n\n\nmodel %>% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = optimizer_rmsprop(lr = 1e-5),\n  metrics = c(\"accuracy\")\n)\n\nhistory <- model %>% fit_generator(\n  train_generator,\n  steps_per_epoch = 100,\n  epochs = 100,\n  validation_data = validation_generator,\n  validation_steps = 50\n)\n\n\nLet’s plot our results:\n\nYou’re seeing a nice 6% absolute improvement in accuracy, from about 90% to above 96%.\nNote that the loss curve doesn’t show any real improvement (in fact, it’s deteriorating). You may wonder, how could accuracy stay stable or improve if the loss isn’t decreasing? The answer is simple: what you display is an average of pointwise loss values; but what matters for accuracy is the distribution of the loss values, not their average, because accuracy is the result of a binary thresholding of the class probability predicted by the model. The model may still be improving even if this isn’t reflected in the average loss.\nYou can now finally evaluate this model on the test data:\n\n\ntest_generator <- flow_images_from_directory(\n  test_dir,\n  test_datagen,\n  target_size = c(150, 150),\n  batch_size = 20,\n  class_mode = \"binary\"\n)\n\n\n\n\nmodel %>% evaluate_generator(test_generator, steps = 50)\n\n\n$loss\n[1] 0.2158171\n\n$acc\n[1] 0.965\nHere you get a test accuracy of 96.5%. In the original Kaggle competition around this dataset, this would have been one of the top results. But using modern deep-learning techniques, you managed to reach this result using only a small fraction of the training data available (about 10%). There is a huge difference between being able to train on 20,000 samples compared to 2,000 samples!\nTake-aways: using convnets with small datasets\nHere’s what you should take away from the exercises in the past two sections:\nConvnets are the best type of machine-learning models for computer-vision tasks. It’s possible to train one from scratch even on a very small dataset, with decent results.\nOn a small dataset, overfitting will be the main issue. Data augmentation is a powerful way to fight overfitting when you’re working with image data.\nIt’s easy to reuse an existing convnet on a new dataset via feature extraction. This is a valuable technique for working with small image datasets.\nAs a complement to feature extraction, you can use fine-tuning, which adapts to a new problem some of the representations previously learned by an existing model. This pushes performance a bit further.\nNow you have a solid set of tools for dealing with image-classification problems – in particular with small datasets.\n\n\n\n",
    "preview": "posts/2017-12-14-image-classification-on-small-datasets/images/swapping_fc_classifier.png",
    "last_modified": "2024-11-21T15:50:54+00:00",
    "input_file": {},
    "preview_width": 678,
    "preview_height": 453
  },
  {
    "path": "posts/2017-12-07-text-classification-with-keras/",
    "title": "Deep Learning for Text Classification with Keras",
    "description": "Two-class classification, or binary classification, may be the most widely applied kind of machine-learning problem. In this excerpt from the book Deep Learning with R, you'll learn to classify movie reviews as positive or negative, based on the text content of the reviews.",
    "author": [
      {
        "name": "François Chollet",
        "url": "https://github.com/fchollet"
      },
      {
        "name": "J.J. Allaire",
        "url": "https://github.com/jjallaire"
      }
    ],
    "date": "2017-12-07",
    "categories": [
      "TensorFlow/Keras",
      "Natural Language Processing"
    ],
    "contents": "\nThe IMDB dataset\nIn this example, we’ll work with the IMDB dataset: a set of 50,000 highly polarized reviews from the Internet Movie Database. They’re split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting of 50% negative and 50% positive reviews.\nWhy use separate training and test sets? Because you should never test a machine-learning model on the same data that you used to train it! Just because a model performs well on its training data doesn’t mean it will perform well on data it has never seen; and what you care about is your model’s performance on new data (because you already know the labels of your training data – obviously\nyou don’t need your model to predict those). For instance, it’s possible that your model could end up merely memorizing a mapping between your training samples and their targets, which would be useless for the task of predicting targets for data the model has never seen before. We’ll go over this point in much more detail in the next chapter.\nJust like the MNIST dataset, the IMDB dataset comes packaged with Keras. It has already been preprocessed: the reviews (sequences of words) have been turned into sequences of integers, where each integer stands for a specific word in a dictionary.\nThe following code will load the dataset (when you run it the first time, about 80 MB of data will be downloaded to your machine).\n\n\nlibrary(keras)\nimdb <- dataset_imdb(num_words = 10000)\ntrain_data <- imdb$train$x\ntrain_labels <- imdb$train$y\ntest_data <- imdb$test$x\ntest_labels <- imdb$test$y\n\n\nThe argument num_words = 10000 means you’ll only keep the top 10,000 most frequently occurring words in the training data. Rare words will be discarded. This allows you to work with vector data of manageable size.\nThe variables train_data and test_data are lists of reviews; each review is a list of word indices (encoding a sequence of words). train_labels and test_labels are lists of 0s and 1s, where 0 stands for negative and 1 stands for positive:\n\n\nstr(train_data[[1]])\n\n\nint [1:218] 1 14 22 16 43 530 973 1622 1385 65 ...\n\n\ntrain_labels[[1]]\n\n\n[1] 1\nBecause you’re restricting yourself to the top 10,000 most frequent words, no word index will exceed 10,000:\n\n\nmax(sapply(train_data, max))\n\n\n[1] 9999\nFor kicks, here’s how you can quickly decode one of these reviews back to English words:\n\n\n# Named list mapping words to an integer index.\nword_index <- dataset_imdb_word_index()  \nreverse_word_index <- names(word_index)\nnames(reverse_word_index) <- word_index\n\n# Decodes the review. Note that the indices are offset by 3 because 0, 1, and \n# 2 are reserved indices for \"padding,\" \"start of sequence,\" and \"unknown.\"\ndecoded_review <- sapply(train_data[[1]], function(index) {\n  word <- if (index >= 3) reverse_word_index[[as.character(index - 3)]]\n  if (!is.null(word)) word else \"?\"\n})\ncat(decoded_review)\n\n\n? this film was just brilliant casting location scenery story direction\neveryone's really suited the part they played and you could just imagine\nbeing there robert ? is an amazing actor and now the same being director\n? father came from the same scottish island as myself so i loved the fact\nthere was a real connection with this film the witty remarks throughout\nthe film were great it was just brilliant so much that i bought the film\nas soon as it was released for ? and would recommend it to everyone to \nwatch and the fly fishing was amazing really cried at the end it was so\nsad and you know what they say if you cry at a film it must have been \ngood and this definitely was also ? to the two little boy's that played'\nthe ? of norman and paul they were just brilliant children are often left\nout of the ? list i think because the stars that play them all grown up\nare such a big profile for the whole film but these children are amazing\nand should be praised for what they have done don't you think the whole\nstory was so lovely because it was true and was someone's life after all\nthat was shared with us all\nPreparing the data\nYou can’t feed lists of integers into a neural network. You have to turn your lists into tensors. There are two ways to do that:\nPad your lists so that they all have the same length, turn them into an integer tensor of shape (samples, word_indices), and then use as the first layer in your network a layer capable of handling such integer tensors (the “embedding” layer, which we’ll cover in detail later in the book).\nOne-hot encode your lists to turn them into vectors of 0s and 1s. This would mean, for instance, turning the sequence [3, 5] into a 10,000-dimensional vector that would be all 0s except for indices 3 and 5, which would be 1s. Then you could use as the first layer in your network a dense layer, capable of handling floating-point vector data.\nLet’s go with the latter solution to vectorize the data, which you’ll do manually for maximum clarity.\n\n\nvectorize_sequences <- function(sequences, dimension = 10000) {\n  # Creates an all-zero matrix of shape (length(sequences), dimension)\n  results <- matrix(0, nrow = length(sequences), ncol = dimension) \n  for (i in 1:length(sequences))\n    # Sets specific indices of results[i] to 1s\n    results[i, sequences[[i]]] <- 1 \n  results\n}\n\nx_train <- vectorize_sequences(train_data)\nx_test <- vectorize_sequences(test_data)\n\n\nHere’s what the samples look like now:\n\n\nstr(x_train[1,])\n\n\n num [1:10000] 1 1 0 1 1 1 1 1 1 0 ...\nYou should also convert your labels from integer to numeric, which is straightforward:\n\n\ny_train <- as.numeric(train_labels)\ny_test <- as.numeric(test_labels)\n\n\nNow the data is ready to be fed into a neural network.\nBuilding your network\nThe input data is vectors, and the labels are scalars (1s and 0s): this is the easiest setup you’ll ever encounter. A type of network that performs well on such a problem is a simple stack of fully connected (“dense”) layers with relu activations: layer_dense(units = 16, activation = \"relu\").\nThe argument being passed to each dense layer (16) is the number of hidden units of the layer. A hidden unit is a dimension in the representation space of the layer. You may remember from chapter 2 that each such dense layer with a relu activation implements the following chain of tensor operations:\noutput = relu(dot(W, input) + b)\nHaving 16 hidden units means the weight matrix W will have shape (input_dimension, 16): the dot product with W will project the input data onto a 16-dimensional representation space (and then you’ll add the bias vector b and apply the relu operation). You can intuitively understand the dimensionality of your representation space as “how much freedom you’re allowing the network to have when learning internal representations.” Having more hidden units (a higher-dimensional representation space) allows your network to learn more-complex representations, but it makes the network more computationally expensive and may lead to learning unwanted patterns (patterns that\nwill improve performance on the training data but not on the test data).\nThere are two key architecture decisions to be made about such stack of dense layers:\nHow many layers to use\nHow many hidden units to choose for each layer\nIn chapter 4, you’ll learn formal principles to guide you in making these choices. For the time being, you’ll have to trust me with the following architecture choice:\nTwo intermediate layers with 16 hidden units each\nA third layer that will output the scalar prediction regarding the sentiment of the current review\nThe intermediate layers will use relu as their activation function, and the final layer will use a sigmoid activation so as to output a probability (a score between 0 and 1, indicating how likely the sample is to have the target “1”: how likely the review is to be positive). A relu (rectified linear unit) is a function meant to zero out negative values.\n\nA sigmoid “squashes” arbitrary values into the [0, 1] interval, outputting something that can be interpreted as a probability.\n\nHere’s what the network looks like.\n\nHere’s the Keras implementation, similar to the MNIST example you saw previously.\n\n\nlibrary(keras)\n\nmodel <- keras_model_sequential() %>% \n  layer_dense(units = 16, activation = \"relu\", input_shape = c(10000)) %>% \n  layer_dense(units = 16, activation = \"relu\") %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\n\nActivation Functions\nNote that without an activation function like relu (also called a non-linearity), the dense layer would consist of two linear operations – a dot product and an addition:\noutput = dot(W, input) + b\nSo the layer could only learn linear transformations (affine transformations) of the input data: the hypothesis space of the layer would be the set of all possible linear transformations of the input data into a 16-dimensional space. Such a hypothesis space is too restricted and wouldn’t benefit from multiple layers of representations, because a deep stack of linear layers would still implement a linear operation: adding more layers wouldn’t extend the hypothesis space.\nIn order to get access to a much richer hypothesis space that would benefit from deep representations, you need a non-linearity, or activation function. relu is the most popular activation function in deep learning, but there are many other candidates, which all come with similarly strange names: prelu, elu, and so on.\nLoss Function and Optimizer\nFinally, you need to choose a loss function and an optimizer. Because you’re facing a binary classification problem and the output of your network is a probability (you end your network with a single-unit layer with a sigmoid activation), it’s best to use the binary_crossentropy loss. It isn’t the only viable choice: you could use, for instance, mean_squared_error. But crossentropy is usually the best choice when you’re dealing with models that output probabilities. Crossentropy is a quantity from the field of Information Theory that measures the distance between probability distributions or, in this case, between the ground-truth distribution and your predictions.\nHere’s the step where you configure the model with the rmsprop optimizer and the binary_crossentropy loss function. Note that you’ll also monitor accuracy during training.\n\n\nmodel %>% compile(\n  optimizer = \"rmsprop\",\n  loss = \"binary_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\n\nYou’re passing your optimizer, loss function, and metrics as strings, which is possible because rmsprop, binary_crossentropy, and accuracy are packaged as part of Keras. Sometimes you may want to configure the parameters of your optimizer or pass a custom loss function or metric function. The former can be done by passing an optimizer instance as the optimizer argument:\n\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(lr=0.001),\n  loss = \"binary_crossentropy\",\n  metrics = c(\"accuracy\")\n) \n\n\nCustom loss and metrics functions can be provided by passing function objects as the loss and/or metrics arguments\n\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(lr = 0.001),\n  loss = loss_binary_crossentropy,\n  metrics = metric_binary_accuracy\n) \n\n\nValidating your approach\nIn order to monitor during training the accuracy of the model on data it has never seen before, you’ll create a validation set by setting apart 10,000 samples from the original training data.\n\n\nval_indices <- 1:10000\n\nx_val <- x_train[val_indices,]\npartial_x_train <- x_train[-val_indices,]\n\ny_val <- y_train[val_indices]\npartial_y_train <- y_train[-val_indices]\n\n\nYou’ll now train the model for 20 epochs (20 iterations over all samples in the x_train and y_train tensors), in mini-batches of 512 samples. At the same time, you’ll monitor loss and accuracy on the 10,000 samples that you set apart. You do so by passing the validation data as the validation_data argument.\n\n\nmodel %>% compile(\n  optimizer = \"rmsprop\",\n  loss = \"binary_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\nhistory <- model %>% fit(\n  partial_x_train,\n  partial_y_train,\n  epochs = 20,\n  batch_size = 512,\n  validation_data = list(x_val, y_val)\n)\n\n\nOn CPU, this will take less than 2 seconds per epoch – training is over in 20 seconds. At the end of every epoch, there is a slight pause as the model computes its loss and accuracy on the 10,000 samples of the validation data.\nNote that the call to fit() returns a history object. The history object has a plot() method that enables us to visualize the training and validation metrics by epoch:\n\n\nplot(history)\n\n\n\nThe accuracy is plotted on the top panel and the loss on the bottom panel. Note that your own results may vary slightly due to a different random initialization of your network.\nAs you can see, the training loss decreases with every epoch, and the training accuracy increases with every epoch. That’s what you would expect when running a gradient-descent optimization – the quantity you’re trying to minimize should be less with every iteration. But that isn’t the case for the validation loss and accuracy: they seem to peak at the fourth epoch. This is an example of what we warned against earlier: a model that performs better on the training data isn’t necessarily a model that will do better on data it has never seen before. In precise terms, what you’re seeing is overfitting: after the second epoch, you’re overoptimizing on the training data, and you end up learning representations that are specific to the training data and don’t generalize to data outside of the training set.\nIn this case, to prevent overfitting, you could stop training after three epochs. In general, you can use a range of techniques to mitigate overfitting,which we’ll cover in chapter 4.\nLet’s train a new network from scratch for four epochs and then evaluate it on the test data.\n\n\nmodel <- keras_model_sequential() %>% \n  layer_dense(units = 16, activation = \"relu\", input_shape = c(10000)) %>% \n  layer_dense(units = 16, activation = \"relu\") %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %>% compile(\n  optimizer = \"rmsprop\",\n  loss = \"binary_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\nmodel %>% fit(x_train, y_train, epochs = 4, batch_size = 512)\nresults <- model %>% evaluate(x_test, y_test)\n\n\n\n\nresults\n\n\n$loss\n[1] 0.2900235\n\n$acc\n[1] 0.88512\nThis fairly naive approach achieves an accuracy of 88%. With state-of-the-art approaches, you should be able to get close to 95%.\nGenerating predictions\nAfter having trained a network, you’ll want to use it in a practical setting. You can generate the likelihood of reviews being positive by using the predict method:\n\n\nmodel %>% predict(x_test[1:10,])\n\n\n [1,] 0.92306918\n [2,] 0.84061098\n [3,] 0.99952853\n [4,] 0.67913240\n [5,] 0.73874789\n [6,] 0.23108074\n [7,] 0.01230567\n [8,] 0.04898361\n [9,] 0.99017477\n[10,] 0.72034937\nAs you can see, the network is confident for some samples (0.99 or more, or 0.01 or less) but less confident for others (0.7, 0.2).\nFurther experiments\nThe following experiments will help convince you that the architecture choices you’ve made are all fairly reasonable, although there’s still room for improvement.\nYou used two hidden layers. Try using one or three hidden layers, and see how doing so affects validation and test accuracy.\nTry using layers with more hidden units or fewer hidden units: 32 units, 64 units, and so on.\nTry using the mse loss function instead of binary_crossentropy.\nTry using the tanh activation (an activation that was popular in the early days of neural networks) instead of relu.\nWrapping up\nHere’s what you should take away from this example:\nYou usually need to do quite a bit of preprocessing on your raw data in order to be able to feed it – as tensors – into a neural network. Sequences of words can be encoded as binary vectors, but there are other encoding options, too.\nStacks of dense layers with relu activations can solve a wide range of problems (including sentiment classification), and you’ll likely use them frequently.\nIn a binary classification problem (two output classes), your network should end with a dense layer with one unit and a sigmoid activation: the output of your network should be a scalar between 0 and 1, encoding a probability.\nWith such a scalar sigmoid output on a binary classification problem, the loss function you should use is binary_crossentropy.\nThe rmsprop optimizer is generally a good enough choice, whatever your problem. That’s one less thing for you to worry about.\nAs they get better on their training data, neural networks eventually start overfitting and end up obtaining increasingly worse results on data they’ve\nnever seen before. Be sure to always monitor performance on data that is outside of the training set.\n\n\n\n",
    "preview": "posts/2017-12-07-text-classification-with-keras/images/training-history.png",
    "last_modified": "2024-11-21T15:50:45+00:00",
    "input_file": {},
    "preview_width": 1400,
    "preview_height": 865
  },
  {
    "path": "posts/2017-10-04-tfruns/",
    "title": "tfruns: Tools for TensorFlow Training Runs",
    "description": "The tfruns package provides a suite of tools for tracking, visualizing, and managing TensorFlow training runs and experiments from R.",
    "author": [
      {
        "name": "J.J. Allaire",
        "url": "https://github.com/jjallaire"
      }
    ],
    "date": "2017-10-04",
    "categories": [
      "Packages/Releases"
    ],
    "contents": "\nThe tfruns package provides a suite of tools for tracking, visualizing, and managing TensorFlow training runs and experiments from R. Use the tfruns package to:\nTrack the hyperparameters, metrics, output, and source code of every training run.\nCompare hyperparmaeters and metrics across runs to find the best performing model.\nAutomatically generate reports to visualize individual training runs or comparisons between runs.\n\nYou can install the tfruns package from GitHub as follows:\n\n\ndevtools::install_github(\"rstudio/tfruns\")\n\n\nComplete documentation for tfruns is available on the TensorFlow for R website.\ntfruns is intended to be used with the keras and/or the tfestimators packages, both of which provide higher level interfaces to TensorFlow from R. These packages can be installed with:\n\n\n# keras\ninstall.packages(\"keras\")\n\n# tfestimators\ndevtools::install_github(\"rstudio/tfestimators\")\n\n\nTraining\nIn the following sections we’ll describe the various capabilities of tfruns. Our example training script (mnist_mlp.R) trains a Keras model to recognize MNIST digits.\nTo train a model with tfruns, just use the training_run() function in place of the source() function to execute your R script. For example:\n\n\nlibrary(tfruns)\ntraining_run(\"mnist_mlp.R\")\n\n\nWhen training is completed, a summary of the run will automatically be displayed if you are within an interactive R session:\n\nThe metrics and output of each run are automatically captured within a run directory which is unique for each run that you initiate. Note that for Keras and TF Estimator models this data is captured automatically (no changes to your source code are required).\nYou can call the latest_run() function to view the results of the last run (including the path to the run directory which stores all of the run’s output):\n\n\nlatest_run()\n\n\n$ run_dir           : chr \"runs/2017-10-02T14-23-38Z\"\n$ eval_loss         : num 0.0956\n$ eval_acc          : num 0.98\n$ metric_loss       : num 0.0624\n$ metric_acc        : num 0.984\n$ metric_val_loss   : num 0.0962\n$ metric_val_acc    : num 0.98\n$ flag_dropout1     : num 0.4\n$ flag_dropout2     : num 0.3\n$ samples           : int 48000\n$ validation_samples: int 12000\n$ batch_size        : int 128\n$ epochs            : int 20\n$ epochs_completed  : int 20\n$ metrics           : chr \"(metrics data frame)\"\n$ model             : chr \"(model summary)\"\n$ loss_function     : chr \"categorical_crossentropy\"\n$ optimizer         : chr \"RMSprop\"\n$ learning_rate     : num 0.001\n$ script            : chr \"mnist_mlp.R\"\n$ start             : POSIXct[1:1], format: \"2017-10-02 14:23:38\"\n$ end               : POSIXct[1:1], format: \"2017-10-02 14:24:24\"\n$ completed         : logi TRUE\n$ output            : chr \"(script ouptut)\"\n$ source_code       : chr \"(source archive)\"\n$ context           : chr \"local\"\n$ type              : chr \"training\"\nThe run directory used in the example above is “runs/2017-10-02T14-23-38Z”. Run directories are by default generated within the “runs” subdirectory of the current working directory, and use a timestamp as the name of the run directory. You can view the report for any given run using the view_run() function:\n\n\nview_run(\"runs/2017-10-02T14-23-38Z\")\n\n\nComparing Runs\nLet’s make a couple of changes to our training script to see if we can improve model performance. We’ll change the number of units in our first dense layer to 128, change the learning_rate from 0.001 to 0.003 and run 30 rather than 20 epochs. After making these changes to the source code we re-run the script using training_run() as before:\n\n\ntraining_run(\"mnist_mlp.R\")\n\n\nThis will also show us a report summarizing the results of the run, but what we are really interested in is a comparison between this run and the previous one. We can view a comparison via the compare_runs() function:\n\n\ncompare_runs()\n\n\n\nThe comparison report shows the model attributes and metrics side-by-side, as well as differences in the source code and output of the training script.\nNote that compare_runs() will by default compare the last two runs, however you can pass any two run directories you like to be compared.\nUsing Flags\nTuning a model often requires exploring the impact of changes to many hyperparameters. The best way to approach this is generally not by changing the source code of the training script as we did above, but instead by defining flags for key parameters you may want to vary. In the example script you can see that we have done this for the dropout layers:\n\n\nFLAGS <- flags(\n  flag_numeric(\"dropout1\", 0.4),\n  flag_numeric(\"dropout2\", 0.3)\n)\n\n\nThese flags are then used in the definition of our model here:\n\n\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_dense(units = 128, activation = 'relu', input_shape = c(784)) %>%\n  layer_dropout(rate = FLAGS$dropout1) %>%\n  layer_dense(units = 128, activation = 'relu') %>%\n  layer_dropout(rate = FLAGS$dropout2) %>%\n  layer_dense(units = 10, activation = 'softmax')\n\n\nOnce we’ve defined flags, we can pass alternate flag values to training_run() as follows:\n\n\ntraining_run('mnist_mlp.R', flags = c(dropout1 = 0.2, dropout2 = 0.2))\n\n\nYou aren’t required to specify all of the flags (any flags excluded will simply use their default value).\nFlags make it very straightforward to systematically explore the impact of changes to hyperparameters on model performance, for example:\n\n\nfor (dropout1 in c(0.1, 0.2, 0.3))\n  training_run('mnist_mlp.R', flags = c(dropout1 = dropout1))\n\n\nFlag values are automatically included in run data with a “flag_” prefix (e.g. flag_dropout1, flag_dropout2).\nSee the article on training flags for additional documentation on using flags.\nAnalyzing Runs\nWe’ve demonstrated visualizing and comparing one or two runs, however as you accumulate more runs you’ll generally want to analyze and compare runs many runs. You can use the ls_runs() function to yield a data frame with summary information on all of the runs you’ve conducted within a given directory:\n\n\nls_runs()\n\n\n# A tibble: 6 x 27\n                    run_dir eval_loss eval_acc metric_loss metric_acc metric_val_loss\n                      <chr>     <dbl>    <dbl>       <dbl>      <dbl>           <dbl>\n1 runs/2017-10-02T14-56-57Z    0.1263   0.9784      0.0773     0.9807          0.1283\n2 runs/2017-10-02T14-56-04Z    0.1323   0.9783      0.0545     0.9860          0.1414\n3 runs/2017-10-02T14-55-11Z    0.1407   0.9804      0.0348     0.9914          0.1542\n4 runs/2017-10-02T14-51-44Z    0.1164   0.9801      0.0448     0.9882          0.1396\n5 runs/2017-10-02T14-37-00Z    0.1338   0.9750      0.1097     0.9732          0.1328\n6 runs/2017-10-02T14-23-38Z    0.0956   0.9796      0.0624     0.9835          0.0962\n# ... with 21 more variables: metric_val_acc <dbl>, flag_dropout1 <dbl>,\n#   flag_dropout2 <dbl>, samples <int>, validation_samples <int>, batch_size <int>,\n#   epochs <int>, epochs_completed <int>, metrics <chr>, model <chr>, loss_function <chr>,\n#   optimizer <chr>, learning_rate <dbl>, script <chr>, start <dttm>, end <dttm>,\n#   completed <lgl>, output <chr>, source_code <chr>, context <chr>, type <chr>\nSince ls_runs() returns a data frame you can also render a sortable, filterable version of it within RStudio using the View() function:\n\n\nView(ls_runs())\n\n\n\nThe ls_runs() function also supports subset and order arguments. For example, the following will yield all runs with an eval accuracy better than 0.98:\n\n\nls_runs(eval_acc > 0.98, order = eval_acc)\n\n\nYou can pass the results of ls_runs() to compare runs (which will always compare the first two runs passed). For example, this will compare the two runs that performed best in terms of evaluation accuracy:\n\n\ncompare_runs(ls_runs(eval_acc > 0.98, order = eval_acc))\n\n\n\nRStudio IDE\nIf you use RStudio with tfruns, it’s strongly recommended that you update to the current Preview Release of RStudio v1.1, as there are are a number of points of integration with the IDE that require this newer release.\nAddin\nThe tfruns package installs an RStudio IDE addin which provides quick access to frequently used functions from the Addins menu:\n\nNote that you can use Tools -> Modify Keyboard Shortcuts within RStudio to assign a keyboard shortcut to one or more of the addin commands.\nBackground Training\nRStudio v1.1 includes a Terminal pane alongside the Console pane. Since training runs can become quite lengthy, it’s often useful to run them in the background in order to keep the R console free for other work. You can do this from a Terminal as follows:\n\nIf you are not running within RStudio then you can of course use a system terminal window for background training.\nPublishing Reports\nTraining run views and comparisons are HTML documents which can be saved and shared with others. When viewing a report within RStudio v1.1 you can save a copy of the report or publish it to RPubs or RStudio Connect:\n\nIf you are not running within RStudio then you can use the save_run_view() and save_run_comparison() functions to create standalone HTML versions of run reports.\nManaging Runs\nThere are a variety of tools available for managing training run output, including:\nExporting run artifacts (e.g. saved models).\nCopying and purging run directories.\nUsing a custom run directory for an experiment or other set of related runs.\nThe Managing Runs article provides additional details on using these features.\n\n\n\n",
    "preview": "posts/2017-10-04-tfruns/preview.png",
    "last_modified": "2024-11-21T15:51:40+00:00",
    "input_file": {},
    "preview_width": 2006,
    "preview_height": 1116
  },
  {
    "path": "posts/2017-09-06-keras-for-r/",
    "title": "Keras for R",
    "description": "We are excited to announce that the keras package is now available on CRAN. The package provides an R interface to Keras, a high-level neural networks API developed with a focus on enabling fast experimentation.",
    "author": [
      {
        "name": "J.J. Allaire",
        "url": "https://github.com/jjallaire"
      }
    ],
    "date": "2017-09-05",
    "categories": [
      "TensorFlow/Keras",
      "Packages/Releases"
    ],
    "contents": "\nWe are excited to announce that the keras package is now available on CRAN. The package provides an R interface to Keras, a high-level neural networks API developed with a focus on enabling fast experimentation. Keras has the following key features:\nAllows the same code to run on CPU or on GPU, seamlessly.\nUser-friendly API which makes it easy to quickly prototype deep learning models.\nBuilt-in support for convolutional networks (for computer vision), recurrent networks (for sequence processing), and any combination of both.\nSupports arbitrary network architectures: multi-input or multi-output models, layer sharing, model sharing, etc. This means that Keras is appropriate for building essentially any deep learning model, from a memory network to a neural Turing machine.\nIs capable of running on top of multiple back-ends including TensorFlow, CNTK, or Theano.\nIf you are already familiar with Keras and want to jump right in, check out https://tensorflow.rstudio.com/keras which has everything you need to get started including over 20 complete examples to learn from.\nTo learn a bit more about Keras and why we’re so excited to announce the Keras interface for R, read on!\nKeras and Deep Learning\nInterest in deep learning has been accelerating rapidly over the past few years, and several deep learning frameworks have emerged over the same time frame. Of all the available frameworks, Keras has stood out for its productivity, flexibility and user-friendly API. At the same time, TensorFlow has emerged as a next-generation machine learning platform that is both extremely flexible and well-suited to production deployment.\nNot surprisingly, Keras and TensorFlow have of late been pulling away from other deep learning frameworks:\n\n\nGoogle web search interest around deep learning frameworks over time. If you remember Q4 2015 and Q1-2 2016 as confusing, you weren't alone. pic.twitter.com/1f1VQVGr8n\n\n— François Chollet (@fchollet) June 3, 2017\n\nThe good news about Keras and TensorFlow is that you don’t need to choose between them! The default backend for Keras is TensorFlow and Keras can be integrated seamlessly with TensorFlow workflows. There is also a pure-TensorFlow implementation of Keras with deeper integration on the roadmap for later this year.\nKeras and TensorFlow are the state of the art in deep learning tools and with the keras package you can now access both with a fluent R interface.\nGetting Started\nInstallation\nTo begin, install the keras R package from CRAN as follows:\n\n\ninstall.packages(\"keras\")\n\n\nThe Keras R interface uses the TensorFlow backend engine by default. To install both the core Keras library as well as the TensorFlow backend use the install_keras() function:\n\n\nlibrary(keras)\ninstall_keras()\n\n\nThis will provide you with default CPU-based installations of Keras and TensorFlow. If you want a more customized installation, e.g. if you want to take advantage of NVIDIA GPUs, see the documentation for install_keras().\nMNIST Example\nWe can learn the basics of Keras by walking through a simple example: recognizing handwritten digits from the MNIST dataset. MNIST consists of 28 x 28 grayscale images of handwritten digits like these:\n\nThe dataset also includes labels for each image, telling us which digit it is. For example, the labels for the above images are 5, 0, 4, and 1.\nPreparing the Data\nThe MNIST dataset is included with Keras and can be accessed using the dataset_mnist() function. Here we load the dataset then create variables for our test and training data:\n\n\nlibrary(keras)\nmnist <- dataset_mnist()\nx_train <- mnist$train$x\ny_train <- mnist$train$y\nx_test <- mnist$test$x\ny_test <- mnist$test$y\n\n\nThe x data is a 3-d array (images,width,height) of grayscale values. To prepare the data for training we convert the 3-d arrays into matrices by reshaping width and height into a single dimension (28x28 images are flattened into length 784 vectors). Then, we convert the grayscale values from integers ranging between 0 to 255 into floating point values ranging between 0 and 1:\n\n\n# reshape\ndim(x_train) <- c(nrow(x_train), 784)\ndim(x_test) <- c(nrow(x_test), 784)\n# rescale\nx_train <- x_train / 255\nx_test <- x_test / 255\n\n\nThe y data is an integer vector with values ranging from 0 to 9. To prepare this data for training we one-hot encode the vectors into binary class matrices using the Keras to_categorical() function:\n\n\ny_train <- to_categorical(y_train, 10)\ny_test <- to_categorical(y_test, 10)\n\n\nDefining the Model\nThe core data structure of Keras is a model, a way to organize layers. The simplest type of model is the sequential model, a linear stack of layers.\nWe begin by creating a sequential model and then adding layers using the pipe (%>%) operator:\n\n\nmodel <- keras_model_sequential() \nmodel %>% \n  layer_dense(units = 256, activation = \"relu\", input_shape = c(784)) %>% \n  layer_dropout(rate = 0.4) %>% \n  layer_dense(units = 128, activation = \"relu\") %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units = 10, activation = \"softmax\")\n\n\nThe input_shape argument to the first layer specifies the shape of the input data (a length 784 numeric vector representing a grayscale image). The final layer outputs a length 10 numeric vector (probabilities for each digit) using a softmax activation function.\nUse the summary() function to print the details of the model:\n\n\nsummary(model)\n\n\nModel\n________________________________________________________________________________\nLayer (type)                        Output Shape                    Param #     \n================================================================================\ndense_1 (Dense)                     (None, 256)                     200960      \n________________________________________________________________________________\ndropout_1 (Dropout)                 (None, 256)                     0           \n________________________________________________________________________________\ndense_2 (Dense)                     (None, 128)                     32896       \n________________________________________________________________________________\ndropout_2 (Dropout)                 (None, 128)                     0           \n________________________________________________________________________________\ndense_3 (Dense)                     (None, 10)                      1290        \n================================================================================\nTotal params: 235,146\nTrainable params: 235,146\nNon-trainable params: 0\n________________________________________________________________________________\nNext, compile the model with appropriate loss function, optimizer, and metrics:\n\n\nmodel %>% compile(\n  loss = \"categorical_crossentropy\",\n  optimizer = optimizer_rmsprop(),\n  metrics = c(\"accuracy\")\n)\n\n\nTraining and Evaluation\nUse the fit() function to train the model for 30 epochs using batches of 128 images:\n\n\nhistory <- model %>% fit(\n  x_train, y_train, \n  epochs = 30, batch_size = 128, \n  validation_split = 0.2\n)\n\n\nThe history object returned by fit() includes loss and accuracy metrics which we can plot:\n\n\nplot(history)\n\n\n\nEvaluate the model’s performance on the test data:\n\n\nmodel %>% evaluate(x_test, y_test,verbose = 0)\n\n\n$loss\n[1] 0.1149\n\n$acc\n[1] 0.9807\nGenerate predictions on new data:\n\n\nmodel %>% predict_classes(x_test)\n\n\n  [1] 7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7 2 7 1 2\n [40] 1 1 7 4 2 3 5 1 2 4 4 6 3 5 5 6 0 4 1 9 5 7 8 9 3 7 4 6 4 3 0 7 0 2 9 1 7 3 2\n [79] 9 7 7 6 2 7 8 4 7 3 6 1 3 6 9 3 1 4 1 7 6 9\n [ reached getOption(\"max.print\") -- omitted 9900 entries ]\nKeras provides a vocabulary for building deep learning models that is simple, elegant, and intuitive. Building a question answering system, an image classification model, a neural Turing machine, or any other model is just as straightforward.\nThe Guide to the Sequential Model article describes the basics of Keras sequential models in more depth.\nExamples\nOver 20 complete examples are available (special thanks to [@dfalbel](https://github.com/dfalbel) for his work on these!). The examples cover image classification, text generation with stacked LSTMs, question-answering with memory networks, transfer learning, variational encoding, and more.\nExample\nDescription\naddition_rnn\nImplementation of sequence to sequence learning for performing addition of two numbers (as strings).\nbabi_memnn\nTrains a memory network on the bAbI dataset for reading comprehension.\nbabi_rnn\nTrains a two-branch recurrent network on the bAbI dataset for reading comprehension.\ncifar10_cnn\nTrains a simple deep CNN on the CIFAR10 small images dataset.\nconv_lstm\nDemonstrates the use of a convolutional LSTM network.\ndeep_dream\nDeep Dreams in Keras.\nimdb_bidirectional_lstm\nTrains a Bidirectional LSTM on the IMDB sentiment classification task.\nimdb_cnn\nDemonstrates the use of Convolution1D for text classification.\nimdb_cnn_lstm\nTrains a convolutional stack followed by a recurrent stack network on the IMDB sentiment classification task.\nimdb_fasttext\nTrains a FastText model on the IMDB sentiment classification task.\nimdb_lstm\nTrains a LSTM on the IMDB sentiment classification task.\nlstm_text_generation\nGenerates text from Nietzsche’s writings.\nmnist_acgan\nImplementation of AC-GAN (Auxiliary Classifier GAN ) on the MNIST dataset\nmnist_antirectifier\nDemonstrates how to write custom layers for Keras\nmnist_cnn\nTrains a simple convnet on the MNIST dataset.\nmnist_irnn\nReproduction of the IRNN experiment with pixel-by-pixel sequential MNIST in “A Simple Way to Initialize Recurrent Networks of Rectified Linear Units” by Le et al.\nmnist_mlp\nTrains a simple deep multi-layer perceptron on the MNIST dataset.\nmnist_hierarchical_rnn\nTrains a Hierarchical RNN (HRNN) to classify MNIST digits.\nmnist_transfer_cnn\nTransfer learning toy example.\nneural_style_transfer\nNeural style transfer (generating an image with the same “content” as a base image, but with the “style” of a different picture).\nreuters_mlp\nTrains and evaluates a simple MLP on the Reuters newswire topic classification task.\nstateful_lstm\nDemonstrates how to use stateful RNNs to model long sequences efficiently.\nvariational_autoencoder\nDemonstrates how to build a variational autoencoder.\nvariational_autoencoder_deconv\nDemonstrates how to build a variational autoencoder with Keras using deconvolution layers.\nLearning More\nAfter you’ve become familiar with the basics, these articles are a good next step:\nGuide to the Sequential Model. The sequential model is a linear stack of layers and is the API most users should start with.\nGuide to the Functional API. The Keras functional API is the way to go for defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.\nTraining Visualization. There are a wide variety of tools available for visualizing training. These include plotting of training metrics, real time display of metrics within the RStudio IDE, and integration with the TensorBoard visualization tool included with TensorFlow.\nUsing Pre-Trained Models. Keras includes a number of deep learning models (Xception, VGG16, VGG19, ResNet50, InceptionVV3, and MobileNet) that are made available alongside pre-trained weights. These models can be used for prediction, feature extraction, and fine-tuning.\nFrequently Asked Questions. Covers many additional topics including streaming training data, saving models, training on GPUs, and more.\nKeras provides a productive, highly flexible framework for developing deep learning models. We can’t wait to see what the R community will do with these tools!\n\n\n\n",
    "preview": "posts/2017-09-06-keras-for-r/preview.png",
    "last_modified": "2024-11-21T15:49:09+00:00",
    "input_file": {},
    "preview_width": 669,
    "preview_height": 414
  },
  {
    "path": "posts/2017-08-31-tensorflow-estimators-for-r/",
    "title": "TensorFlow Estimators",
    "description": "The tfestimators package is an R interface to TensorFlow Estimators, a high-level API that provides implementations of many different model types including linear models and deep neural networks.",
    "author": [
      {
        "name": "Yuan Tang",
        "url": "https://orcid.org/0000-0001-5243-233X"
      }
    ],
    "date": "2017-08-31",
    "categories": [
      "Packages/Releases"
    ],
    "contents": "\nThe tfestimators package is an R interface to TensorFlow Estimators, a high-level API that provides implementations of many different model types including linear models and deep neural networks.\nMore models are coming soon such as state saving recurrent neural networks, dynamic recurrent neural networks, support vector machines, random forest, KMeans clustering, etc. TensorFlow estimators also provides a flexible framework for defining arbitrary new model types as custom estimators.\nThe framework balances the competing demands for flexibility and simplicity by offering APIs at different levels of abstraction, making common model architectures available out of the box, while providing a library of utilities designed to speed up experimentation with model architectures.\nThese abstractions guide developers to write models in ways conducive to productionization as well as making it possible to write downstream infrastructure for distributed training or parameter tuning independent of the model implementation.\nTo make out of the box models flexible and usable across a wide range of problems, tfestimators provides canned Estimators that are are parameterized not only over traditional hyperparameters, but also using feature columns, a declarative specification describing how to interpret input data.\nFor more details on the architecture and design of TensorFlow Estimators, please check out the KDD’17 paper: TensorFlow Estimators: Managing Simplicity vs. Flexibility in High-Level Machine Learning Frameworks.\nQuick Start\nInstallation\nTo use tfestimators, you need to install both the tfestimators R package as well as TensorFlow itself.\nFirst, install the tfestimators R package as follows:\n\n\ndevtools::install_github(\"rstudio/tfestimators\")\n\n\nThen, use the install_tensorflow() function to install TensorFlow (note that the current tfestimators package requires version 1.3.0 of TensorFlow so even if you already have TensorFlow installed you should update if you are running a previous version):\n\n\nlibrary(tfestimators)\ninstall_tensorflow()\n\n\nThis will provide you with a default installation of TensorFlow suitable for getting started. See the article on installation to learn about more advanced options, including installing a version of TensorFlow that takes advantage of NVIDIA GPUs if you have the correct CUDA libraries installed.\nLinear Regression\nLet’s create a simple linear regression model with the mtcars dataset to demonstrate the use of estimators. We’ll illustrate how input functions can be constructed and used to feed data to an estimator, how feature columns can be used to specify a set of transformations to apply to input data, and how these pieces come together in the Estimator interface.\nInput Function\nEstimators can receive data through input functions. Input functions take an arbitrary data source (in-memory data sets, streaming data, custom data format, and so on) and generate Tensors that can be supplied to TensorFlow models. The tfestimators package includes an input_fn() function that can create TensorFlow input functions from common R data sources (e.g. data frames and matrices). It’s also possible to write a fully custom input function.\nHere, we define a helper function that will return an input function for a subset of our mtcars data set.\n\n\nlibrary(tfestimators)\n\n# return an input_fn for a given subset of data\nmtcars_input_fn <- function(data) {\n  input_fn(data, \n           features = c(\"disp\", \"cyl\"), \n           response = \"mpg\")\n}\n\n\nFeature Columns\nNext, we define the feature columns for our model. Feature columns are used to specify how Tensors received from the input function should be combined and transformed before entering the model training, evaluation, and prediction steps. A feature column can be a plain mapping to some input column (e.g. column_numeric() for a column of numerical data), or a transformation of other feature columns (e.g. column_crossed() to define a new column as the cross of two other feature columns).\nHere, we create a list of feature columns containing two numeric variables - disp and cyl:\n\n\ncols <- feature_columns(\n  column_numeric(\"disp\"),\n  column_numeric(\"cyl\")\n)\n\n\nYou can also define multiple feature columns at once:\n\n\ncols <- feature_columns( \n  column_numeric(\"disp\", \"cyl\")\n)\n\n\nBy using the family of feature column functions we can define various transformations on the data before using it for modeling.\nEstimator\nNext, we create the estimator by calling the linear_regressor() function and passing it a set of feature columns:\n\n\nmodel <- linear_regressor(feature_columns = cols)\n\n\nTraining\nWe’re now ready to train our model, using the train() function. We’ll partition the mtcars data set into separate training and validation data sets, and feed the training data set into train(). We’ll hold 20% of the data aside for validation.\n\n\nindices <- sample(1:nrow(mtcars), size = 0.80 * nrow(mtcars))\ntrain <- mtcars[indices, ]\ntest  <- mtcars[-indices, ]\n\n# train the model\nmodel %>% train(mtcars_input_fn(train))\n\n\nEvaluation\nWe can evaluate the model’s accuracy using the evaluate() function, using our ‘test’ data set for validation.\n\n\nmodel %>% evaluate(mtcars_input_fn(test))\n\n\nPrediction\nAfter we’ve finished training out model, we can use it to generate predictions from new data.\n\n\nnew_obs <- mtcars[1:3, ]\nmodel %>% predict(mtcars_input_fn(new_obs))\n\n\nLearning More\nAfter you’ve become familiar with these concepts, these articles cover the basics of using TensorFlow Estimators and the main components in more detail:\nEstimator Basics\nInput Functions\nFeature Columns\nThese articles describe more advanced topics/usage:\nRun Hooks\nCustom Estimators\nTensorFlow Layers\nTensorBoard Visualization\nParsing Utilities\nOne of the best ways to learn is from reviewing and experimenting with examples. See the Examples page for a variety of examples to help you get started.\n\n\n\n",
    "preview": "posts/2017-08-31-tensorflow-estimators-for-r/tensorflow-architecture.png",
    "last_modified": "2024-11-21T15:51:36+00:00",
    "input_file": {},
    "preview_width": 1198,
    "preview_height": 796
  },
  {
    "path": "posts/2017-08-17-tensorflow-v13-released/",
    "title": "TensorFlow v1.3 Released",
    "description": "The final release of TensorFlow v1.3 is now available. This release marks the initial availability of several canned estimators including DNNClassifier and  DNNRegressor.",
    "author": [
      {
        "name": "J.J. Allaire",
        "url": "https://github.com/jjallaire"
      }
    ],
    "date": "2017-08-17",
    "categories": [
      "Packages/Releases"
    ],
    "contents": "\nThe final release of TensorFlow v1.3 is now available. This release of TensorFlow marks the initial availability of several canned estimators, including:\nDNNClassifier\nDNNRegressor\nLinearClassifier\nLinearRegressor\nDNNLinearCombinedClassifier\nDNNLinearCombinedRegressor.\nThe tfestimators package provides a high level R interface for these estimators.\nFull details on the release of TensorFlow v1.3 are available here: https://github.com/tensorflow/tensorflow/releases/tag/v1.3.0\nYou can update your R installation of TensorFlow using the install_tensorflow function:\nlibrary(tensorflow)\ninstall_tensorflow()\nNote that you should also provide any options used in your original installation (e.g. method = \"conda\", version = \"gpu\", etc. )\ncuDNN 6.0\nTensorFlow v1.3 is built against version 6.0 of the cuDNN library from NVIDIA. Previous versions were built against cuDNN v5.1, so for installations running the GPU version of TensorFlow this means that you will need to install an updated version of cuDNN along with TensorFlow v1.3.\nUpdated installation instructions are available here: https://tensorflow.rstudio.com/tensorflow/installation_gpu.html.\nVersion 1.4 of TensorFlow is expected to migrate again to version 7.0 of cuDNN.\n\n\n\n",
    "preview": "posts/2017-08-17-tensorflow-v13-released/tensorflow-logo.png",
    "last_modified": "2024-11-21T15:52:37+00:00",
    "input_file": {},
    "preview_width": 3876,
    "preview_height": 741
  }
]
